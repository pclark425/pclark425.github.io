<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1728 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1728</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1728</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-271162331</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.09287v1.pdf" target="_blank">Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments</a></p>
                <p><strong>Paper Abstract:</strong> In this study, we address the issue of enabling an artificial intelligence agent to execute complex language instructions within virtual environments. In our framework, we assume that these instructions involve intricate linguistic structures and multiple interdependent tasks that must be navigated successfully to achieve the desired outcomes. To effectively manage these complexities, we propose a hierarchical framework that combines the deep language comprehension of large language models with the adaptive action-execution capabilities of reinforcement learning agents. The language module (based on LLM) translates the language instruction into a high-level action plan, which is then executed by a pre-trained reinforcement learning agent. We have demonstrated the effectiveness of our approach in two different environments: in IGLU, where agents are instructed to build structures, and in Crafter, where agents perform tasks and interact with objects in the surrounding environment according to language commands.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1728.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1728.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5 (IGOR LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (used as IGOR Language Module)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained sequence-to-sequence language model (Flan-T5 base) fine-tuned on environment-specific instruction->subtask data and used to produce high-level action plans (lists of subtasks) that are executed by a goal-conditioned RL policy in embodied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Flan-T5 (IGOR Language Module)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Flan-T5 base initialized from the publicly available Flan-T5 checkpoint, finetuned in a seq2seq fashion to map natural-language instructions to ordered lists of subtasks (either block coordinates or aggregated 'primitives'). Its outputs are consumed by a Task Manager which encodes subtasks for a goal-conditioned PPO policy.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale text pretraining (Flan-T5) followed by environment-specific instruction-to-subtask fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Fine-tuned on IGLU instruction->subtask data: training set of 109 English instructions and test set of 41; subtasks encoded as either individual block tuples (coords) or aggregated primitives. For Crafter, the language model was trained on a generated dataset of textual task descriptions (created using Mistral-7B-Instruct and stylization). Additional augmentation used ChatGPT to rewrite/modify instructions (e.g., rotated variants). Exact size: IGLU finetune: 109 train items (tokens/epochs reported in paper but raw token counts not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>IGLU (3D building) and modified Crafter (text-adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>IGLU: first-person 3D block-building environment where agent must construct spatial structures from dialogue instructions (POV image 64x64x3, inventory counts, pitch/yaw); action set includes walking/flying variants, block place/break and block-type switching. Modified Crafter: a 2D Minecraft-like environment adapted to accept free-form textual instructions (each instruction maps to a set of in-game achievements/tasks such as collect/make actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Free-form natural language instructions; LLM outputs high-level structured subtasks (ordered lists of coordinates or primitives).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>IGLU: hybrid discrete + continuous — discrete actions (13: noop, camera rotations, break/place, select block types 1–6) plus 6 continuous movement actions; Crafter: discrete game actions for exploration, resource gathering, crafting and combat (environment-specific action set).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Seq2seq mapping: Flan-T5 translates instruction -> ordered subtasks (coords/primitives). The Task Manager converts these subtasks into encodings appended to the agent observation; the goal-conditioned PPO policy is trained to achieve a single subtask per episode and thus implements the low-level action execution. No learned end-to-end mapping from language tokens directly to low-level motor controls; mapping is hierarchical and explicit via subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB first-person view (IGLU: 64x64x3), inventory scalar features (item counts), agent orientation (pitch/yaw). Crafter: egocentric visual observations and game state (inventory/achievements).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>IGLU: IGOR (Flan-T5 LLM + goal-conditioned RL) achieved substantially higher F1 than competition baselines; examples reported include Floor (using primitives) F1 = 0.75 and Air F1 = 0.46; the paper reports IGOR's worst configuration still at F1 = 0.45 and overall improvements across categories (summary overall F1 reported in tables by configuration, top results ~0.50–0.75 depending on class/config). Crafter: Success Rate = 0.60 (60%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines reported: BrainAgent (IGLU winner) overall F1 = 0.36; Pegasus baseline F1 ≈ 0.06 (IGLU table); Dynalang (text->DreamerV3 baseline on Modified Crafter) Success Rate = 0.364 (36.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Reported compute proxy: IGOR training used 139 GPU hours for Crafter (136 GPU hours for RL + 3 GPU hours for LLM finetuning) and 32 GPU hours for IGLU (20 GPU hours RL + 12 GPU hours LLM). Number of environment episodes/steps to reach thresholds not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Dynalang (Crafter baseline) required 166 GPU hours (TITAN RTX) in the paper's reported run; episode/step counts for baselines not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Compute-hours proxy: IGOR used ~27 fewer GPU hours than Dynalang on Crafter (139 vs 166 GPU hours), a ≈16% reduction in reported GPU-hours while increasing success rate from 36.4% to 60% on the modified Crafter test set. Exact sample/episode efficiency figures were not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Explicit hierarchical decomposition (LLM -> subtasks -> Task Manager -> goal-conditioned RL) that narrows the language-to-action gap; fine-tuning LLM on environment-specific instruction->subtask data; aggregating subtasks into 'primitives' to reduce output complexity; data augmentation (ChatGPT, rotation edits) to expand limited instruction data; curriculum learning and goal-conditioned RL that trains on one subtask per episode.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limited size/diversity of fine-tuning data (noted problems especially for 'air' blocks); intrinsic spatial reasoning limitations of LLMs trained only on text; perception-action gap (textual plan must still be grounded in visual observations); dependence on known subtasks — approach requires pre-defining subtask vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained language models (Flan-T5) can be effectively fine-tuned to produce structured high-level plans that, when fed to a goal-conditioned RL policy via an intermediate Task Manager, improve performance and compute-efficiency on embodied tasks (3D building in IGLU and task following in modified Crafter). Representation choices (coords vs primitives), targeted data augmentation, and curriculum learning materially affect transfer quality; explicit subtask decomposition and goal-conditioned training are crucial to effective transfer from text to embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1728.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1728.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynalang (T5 + DreamerV3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynalang (baseline combining T5 text encoder and Dreamer-v3 policy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that encodes textual instructions using a pretrained T5 encoder and uses a Dreamer-v3 world-model-based RL agent to act in embodied environments; used here as a baseline on the modified Crafter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Dynalang (T5 encoder + Dreamer-v3 policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Dynalang encodes instructions with a T5-based text encoder and supplies this information to a Dreamer-v3-based policy/world-model agent to select actions in the environment. In this paper the authors re-used the T5 encoding approach to run Dynalang on the text-adapted Crafter benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained language model (T5) trained on large text corpora; Dreamer-v3 pretrained or trained as a world model (details not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>This paper does not report the original pretraining corpora for T5 or Dreamer-v3; it reuses the canonical T5 encoder to represent text instructions and Dreamer-v3 as the RL backbone per the Dynalang approach.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Modified Crafter (text-adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same modified Crafter used in this work: a Minecraft-like 2D environment where each episode is guided by a textual instruction listing achievements/subtasks (e.g., defeat zombie, collect iron, craft iron sword). The agent must complete all subtasks in the instruction to count as success.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Free-form natural language instructions (encoded token-by-token into the text encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete game actions for exploration, resource collection, crafting, and combat (standard Crafter action set).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>T5 encodes the instruction; Dynalang/Dreamer-v3 conditions policy on this encoding to select low-level actions. In the experiments here instructions/images are fed sequentially (token by token) as per Dynalang implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Visual observations (environment frames/images) and possibly game-state information (inventory/achievement flags).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Modified Crafter Success Rate = 0.364 (36.4%) as reported for Dynalang in this paper's comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Reported compute: 166 GPU hours (TITAN RTX) for the Dynalang run on Crafter in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Relative to IGOR, Dynalang used ~16% more reported GPU-hours and achieved lower success rate (36.4% vs 60%); exact sample-efficiency gains/losses in episodes not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretrained text encoder (T5) provides structured textual understanding; Dreamer-v3 world model supplies sample-efficient planning in model-based RL fashion.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Less explicit hierarchical decomposition between language and low-level control compared to IGOR; apparently less effective alignment between language output and goal-conditioned RL leading to lower task completion rates; higher compute hours in reported runs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>T5-based text encodings can be combined with advanced RL backbones (Dreamer-v3) to perform embodied instruction-following, but compared to an explicit LLM->subtask->goal-conditioned-RL pipeline (IGOR) performance and compute-efficiency can be substantially worse on the tested Crafter tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1728.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1728.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BrainAgent (IGLU winner)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BrainAgent (first-place IGLU 2022 solution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end RL approach that incorporated embeddings from a frozen NLP model as additional inputs to the policy, applied to the IGLU 3D building benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BrainAgent (end-to-end RL with frozen NLP embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>BrainAgent used end-to-end reinforcement learning where the RL policy received embeddings from a frozen NLP model in addition to environment information and hand-crafted features; the approach learns to map these inputs directly to low-level actions for IGLU construction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Frozen NLP model embeddings (pretrained on text) provided to the RL policy; specifics of the pretrained NLP model are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>This paper cites BrainAgent as a competition winner and reports its published performance on IGLU. Original pretraining details of BrainAgent's NLP component are not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>IGLU (3D building)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>First-person block-building tasks from textual instructions, requiring spatial reasoning and precise block placement/removal over potentially many steps.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions provided as dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>IGLU hybrid action space: discrete actions (place/break/select, camera rotations) plus continuous movement in some modes (6 continuous movement actions); agents operate in walking or flying modes.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Frozen NLP model produces embeddings which are concatenated to environment observations and fed into an end-to-end RL policy; mapping from language to actions is learned end-to-end by the RL learner rather than via explicit subtask outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>First-person RGB (64x64x3), inventory counts, orientation (pitch/yaw).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>BrainAgent reported overall F1 ≈ 0.36 on the IGLU test set as cited in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretrained language embeddings provide compact language information to the RL policy which supports end-to-end learning of mapping from linguistic intent to actions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>End-to-end learning from embeddings may be less sample-efficient or less structured than hierarchical decomposition; BrainAgent underperformed IGOR on the reported metrics, suggesting limitations in relying solely on frozen embeddings without explicit subtask decomposition and goal-conditioned training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained NLP embeddings can be incorporated into end-to-end RL for embodied instruction following, but on IGLU an explicit hierarchical pipeline (fine-tuned LLM -> subtask decomposition -> goal-conditioned RL) produced higher task completion metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1728.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1728.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pegasus (IGLU baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pegasus (pretrained seq2seq summarization model used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained seq2seq language model (Pegasus) that was used as a language-module baseline in the IGLU competition and is reported here for comparison; its direct application to spatial instruction grounding produced low F1 in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Pegasus (used as language module baseline in IGLU)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pegasus is a large pretrained sequence-to-sequence model originally developed for abstractive summarization; it was employed as one of the language modules in IGLU baselines to map instructions to block-placement sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale text pretraining (summarization-oriented pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>This paper does not provide Pegasus pretraining corpus details; Pegasus was used by a baseline team in the IGLU competition and its test performance is reported in the paper (very low F1 in this task).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>IGLU (3D building)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>IGLU block-building tasks derived from natural-language dialogue instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>IGLU hybrid action space as described above.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Pegasus outputs were interpreted as sequences of block coordinates/placements and then executed by the environment; explicit Task Manager or goal-conditioned RL decomposition not described for this baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Same as IGLU: RGB POV, inventory, orientation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Pegasus baseline reported overall F1 ≈ 0.06 on the IGLU test set in the table cited by this paper (very low), indicating poor direct transfer for this task without additional structural techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale seq2seq pretraining provides generic language generation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Direct application of a summarization-oriented LLM to spatial grounding tasks suffers from limited spatial and ordering grounding and small environment-specific fine-tuning data; format mismatches between summarization pretraining and coordinate-output task.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generic pretrained seq2seq LMs (e.g., Pegasus) without targeted decomposition, data-format design (primitives), or augmentation fail to deliver effective embodied spatial instruction grounding in IGLU.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Guiding pretraining in reinforcement learning with large language models <em>(Rating: 2)</em></li>
                <li>Pre-trained language models for interactive decision-making <em>(Rating: 2)</em></li>
                <li>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models <em>(Rating: 2)</em></li>
                <li>Building open-ended embodied agents with internet-scale knowledge <em>(Rating: 2)</em></li>
                <li>Learning to model the world with language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1728",
    "paper_id": "paper-271162331",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "Flan-T5 (IGOR LLM)",
            "name_full": "Flan-T5 (used as IGOR Language Module)",
            "brief_description": "A pre-trained sequence-to-sequence language model (Flan-T5 base) fine-tuned on environment-specific instruction-&gt;subtask data and used to produce high-level action plans (lists of subtasks) that are executed by a goal-conditioned RL policy in embodied environments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "Flan-T5 (IGOR Language Module)",
            "model_agent_description": "Flan-T5 base initialized from the publicly available Flan-T5 checkpoint, finetuned in a seq2seq fashion to map natural-language instructions to ordered lists of subtasks (either block coordinates or aggregated 'primitives'). Its outputs are consumed by a Task Manager which encodes subtasks for a goal-conditioned PPO policy.",
            "pretraining_data_type": "Large-scale text pretraining (Flan-T5) followed by environment-specific instruction-to-subtask fine-tuning",
            "pretraining_data_details": "Fine-tuned on IGLU instruction-&gt;subtask data: training set of 109 English instructions and test set of 41; subtasks encoded as either individual block tuples (coords) or aggregated primitives. For Crafter, the language model was trained on a generated dataset of textual task descriptions (created using Mistral-7B-Instruct and stylization). Additional augmentation used ChatGPT to rewrite/modify instructions (e.g., rotated variants). Exact size: IGLU finetune: 109 train items (tokens/epochs reported in paper but raw token counts not provided).",
            "embodied_task_name": "IGLU (3D building) and modified Crafter (text-adapted)",
            "embodied_task_description": "IGLU: first-person 3D block-building environment where agent must construct spatial structures from dialogue instructions (POV image 64x64x3, inventory counts, pitch/yaw); action set includes walking/flying variants, block place/break and block-type switching. Modified Crafter: a 2D Minecraft-like environment adapted to accept free-form textual instructions (each instruction maps to a set of in-game achievements/tasks such as collect/make actions).",
            "action_space_text": "Free-form natural language instructions; LLM outputs high-level structured subtasks (ordered lists of coordinates or primitives).",
            "action_space_embodied": "IGLU: hybrid discrete + continuous — discrete actions (13: noop, camera rotations, break/place, select block types 1–6) plus 6 continuous movement actions; Crafter: discrete game actions for exploration, resource gathering, crafting and combat (environment-specific action set).",
            "action_mapping_method": "Seq2seq mapping: Flan-T5 translates instruction -&gt; ordered subtasks (coords/primitives). The Task Manager converts these subtasks into encodings appended to the agent observation; the goal-conditioned PPO policy is trained to achieve a single subtask per episode and thus implements the low-level action execution. No learned end-to-end mapping from language tokens directly to low-level motor controls; mapping is hierarchical and explicit via subtasks.",
            "perception_requirements": "RGB first-person view (IGLU: 64x64x3), inventory scalar features (item counts), agent orientation (pitch/yaw). Crafter: egocentric visual observations and game state (inventory/achievements).",
            "transfer_successful": true,
            "performance_with_pretraining": "IGLU: IGOR (Flan-T5 LLM + goal-conditioned RL) achieved substantially higher F1 than competition baselines; examples reported include Floor (using primitives) F1 = 0.75 and Air F1 = 0.46; the paper reports IGOR's worst configuration still at F1 = 0.45 and overall improvements across categories (summary overall F1 reported in tables by configuration, top results ~0.50–0.75 depending on class/config). Crafter: Success Rate = 0.60 (60%).",
            "performance_without_pretraining": "Baselines reported: BrainAgent (IGLU winner) overall F1 = 0.36; Pegasus baseline F1 ≈ 0.06 (IGLU table); Dynalang (text-&gt;DreamerV3 baseline on Modified Crafter) Success Rate = 0.364 (36.4%).",
            "sample_complexity_with_pretraining": "Reported compute proxy: IGOR training used 139 GPU hours for Crafter (136 GPU hours for RL + 3 GPU hours for LLM finetuning) and 32 GPU hours for IGLU (20 GPU hours RL + 12 GPU hours LLM). Number of environment episodes/steps to reach thresholds not reported.",
            "sample_complexity_without_pretraining": "Dynalang (Crafter baseline) required 166 GPU hours (TITAN RTX) in the paper's reported run; episode/step counts for baselines not provided.",
            "sample_complexity_gain": "Compute-hours proxy: IGOR used ~27 fewer GPU hours than Dynalang on Crafter (139 vs 166 GPU hours), a ≈16% reduction in reported GPU-hours while increasing success rate from 36.4% to 60% on the modified Crafter test set. Exact sample/episode efficiency figures were not reported.",
            "transfer_success_factors": "Explicit hierarchical decomposition (LLM -&gt; subtasks -&gt; Task Manager -&gt; goal-conditioned RL) that narrows the language-to-action gap; fine-tuning LLM on environment-specific instruction-&gt;subtask data; aggregating subtasks into 'primitives' to reduce output complexity; data augmentation (ChatGPT, rotation edits) to expand limited instruction data; curriculum learning and goal-conditioned RL that trains on one subtask per episode.",
            "transfer_failure_factors": "Limited size/diversity of fine-tuning data (noted problems especially for 'air' blocks); intrinsic spatial reasoning limitations of LLMs trained only on text; perception-action gap (textual plan must still be grounded in visual observations); dependence on known subtasks — approach requires pre-defining subtask vocabulary.",
            "key_findings": "Pretrained language models (Flan-T5) can be effectively fine-tuned to produce structured high-level plans that, when fed to a goal-conditioned RL policy via an intermediate Task Manager, improve performance and compute-efficiency on embodied tasks (3D building in IGLU and task following in modified Crafter). Representation choices (coords vs primitives), targeted data augmentation, and curriculum learning materially affect transfer quality; explicit subtask decomposition and goal-conditioned training are crucial to effective transfer from text to embodied control.",
            "uuid": "e1728.0",
            "source_info": {
                "paper_title": "Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Dynalang (T5 + DreamerV3)",
            "name_full": "Dynalang (baseline combining T5 text encoder and Dreamer-v3 policy)",
            "brief_description": "A prior method that encodes textual instructions using a pretrained T5 encoder and uses a Dreamer-v3 world-model-based RL agent to act in embodied environments; used here as a baseline on the modified Crafter.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "Dynalang (T5 encoder + Dreamer-v3 policy)",
            "model_agent_description": "Dynalang encodes instructions with a T5-based text encoder and supplies this information to a Dreamer-v3-based policy/world-model agent to select actions in the environment. In this paper the authors re-used the T5 encoding approach to run Dynalang on the text-adapted Crafter benchmark.",
            "pretraining_data_type": "Pretrained language model (T5) trained on large text corpora; Dreamer-v3 pretrained or trained as a world model (details not specified in paper).",
            "pretraining_data_details": "This paper does not report the original pretraining corpora for T5 or Dreamer-v3; it reuses the canonical T5 encoder to represent text instructions and Dreamer-v3 as the RL backbone per the Dynalang approach.",
            "embodied_task_name": "Modified Crafter (text-adapted)",
            "embodied_task_description": "Same modified Crafter used in this work: a Minecraft-like 2D environment where each episode is guided by a textual instruction listing achievements/subtasks (e.g., defeat zombie, collect iron, craft iron sword). The agent must complete all subtasks in the instruction to count as success.",
            "action_space_text": "Free-form natural language instructions (encoded token-by-token into the text encoder).",
            "action_space_embodied": "Discrete game actions for exploration, resource collection, crafting, and combat (standard Crafter action set).",
            "action_mapping_method": "T5 encodes the instruction; Dynalang/Dreamer-v3 conditions policy on this encoding to select low-level actions. In the experiments here instructions/images are fed sequentially (token by token) as per Dynalang implementation.",
            "perception_requirements": "Visual observations (environment frames/images) and possibly game-state information (inventory/achievement flags).",
            "transfer_successful": true,
            "performance_with_pretraining": "Modified Crafter Success Rate = 0.364 (36.4%) as reported for Dynalang in this paper's comparison.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Reported compute: 166 GPU hours (TITAN RTX) for the Dynalang run on Crafter in this paper.",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": "Relative to IGOR, Dynalang used ~16% more reported GPU-hours and achieved lower success rate (36.4% vs 60%); exact sample-efficiency gains/losses in episodes not specified.",
            "transfer_success_factors": "Pretrained text encoder (T5) provides structured textual understanding; Dreamer-v3 world model supplies sample-efficient planning in model-based RL fashion.",
            "transfer_failure_factors": "Less explicit hierarchical decomposition between language and low-level control compared to IGOR; apparently less effective alignment between language output and goal-conditioned RL leading to lower task completion rates; higher compute hours in reported runs.",
            "key_findings": "T5-based text encodings can be combined with advanced RL backbones (Dreamer-v3) to perform embodied instruction-following, but compared to an explicit LLM-&gt;subtask-&gt;goal-conditioned-RL pipeline (IGOR) performance and compute-efficiency can be substantially worse on the tested Crafter tasks.",
            "uuid": "e1728.1",
            "source_info": {
                "paper_title": "Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "BrainAgent (IGLU winner)",
            "name_full": "BrainAgent (first-place IGLU 2022 solution)",
            "brief_description": "An end-to-end RL approach that incorporated embeddings from a frozen NLP model as additional inputs to the policy, applied to the IGLU 3D building benchmark.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "BrainAgent (end-to-end RL with frozen NLP embeddings)",
            "model_agent_description": "BrainAgent used end-to-end reinforcement learning where the RL policy received embeddings from a frozen NLP model in addition to environment information and hand-crafted features; the approach learns to map these inputs directly to low-level actions for IGLU construction tasks.",
            "pretraining_data_type": "Frozen NLP model embeddings (pretrained on text) provided to the RL policy; specifics of the pretrained NLP model are not detailed in this paper.",
            "pretraining_data_details": "This paper cites BrainAgent as a competition winner and reports its published performance on IGLU. Original pretraining details of BrainAgent's NLP component are not specified here.",
            "embodied_task_name": "IGLU (3D building)",
            "embodied_task_description": "First-person block-building tasks from textual instructions, requiring spatial reasoning and precise block placement/removal over potentially many steps.",
            "action_space_text": "Natural language instructions provided as dialogue.",
            "action_space_embodied": "IGLU hybrid action space: discrete actions (place/break/select, camera rotations) plus continuous movement in some modes (6 continuous movement actions); agents operate in walking or flying modes.",
            "action_mapping_method": "Frozen NLP model produces embeddings which are concatenated to environment observations and fed into an end-to-end RL policy; mapping from language to actions is learned end-to-end by the RL learner rather than via explicit subtask outputs.",
            "perception_requirements": "First-person RGB (64x64x3), inventory counts, orientation (pitch/yaw).",
            "transfer_successful": true,
            "performance_with_pretraining": "BrainAgent reported overall F1 ≈ 0.36 on the IGLU test set as cited in this paper.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Pretrained language embeddings provide compact language information to the RL policy which supports end-to-end learning of mapping from linguistic intent to actions.",
            "transfer_failure_factors": "End-to-end learning from embeddings may be less sample-efficient or less structured than hierarchical decomposition; BrainAgent underperformed IGOR on the reported metrics, suggesting limitations in relying solely on frozen embeddings without explicit subtask decomposition and goal-conditioned training.",
            "key_findings": "Pretrained NLP embeddings can be incorporated into end-to-end RL for embodied instruction following, but on IGLU an explicit hierarchical pipeline (fine-tuned LLM -&gt; subtask decomposition -&gt; goal-conditioned RL) produced higher task completion metrics.",
            "uuid": "e1728.2",
            "source_info": {
                "paper_title": "Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Pegasus (IGLU baseline)",
            "name_full": "Pegasus (pretrained seq2seq summarization model used as baseline)",
            "brief_description": "A pretrained seq2seq language model (Pegasus) that was used as a language-module baseline in the IGLU competition and is reported here for comparison; its direct application to spatial instruction grounding produced low F1 in this setting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "Pegasus (used as language module baseline in IGLU)",
            "model_agent_description": "Pegasus is a large pretrained sequence-to-sequence model originally developed for abstractive summarization; it was employed as one of the language modules in IGLU baselines to map instructions to block-placement sequences.",
            "pretraining_data_type": "Large-scale text pretraining (summarization-oriented pretraining).",
            "pretraining_data_details": "This paper does not provide Pegasus pretraining corpus details; Pegasus was used by a baseline team in the IGLU competition and its test performance is reported in the paper (very low F1 in this task).",
            "embodied_task_name": "IGLU (3D building)",
            "embodied_task_description": "IGLU block-building tasks derived from natural-language dialogue instructions.",
            "action_space_text": "Natural language instructions.",
            "action_space_embodied": "IGLU hybrid action space as described above.",
            "action_mapping_method": "Pegasus outputs were interpreted as sequences of block coordinates/placements and then executed by the environment; explicit Task Manager or goal-conditioned RL decomposition not described for this baseline in the paper.",
            "perception_requirements": "Same as IGLU: RGB POV, inventory, orientation.",
            "transfer_successful": true,
            "performance_with_pretraining": "Pegasus baseline reported overall F1 ≈ 0.06 on the IGLU test set in the table cited by this paper (very low), indicating poor direct transfer for this task without additional structural techniques.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Large-scale seq2seq pretraining provides generic language generation capability.",
            "transfer_failure_factors": "Direct application of a summarization-oriented LLM to spatial grounding tasks suffers from limited spatial and ordering grounding and small environment-specific fine-tuning data; format mismatches between summarization pretraining and coordinate-output task.",
            "key_findings": "Generic pretrained seq2seq LMs (e.g., Pegasus) without targeted decomposition, data-format design (primitives), or augmentation fail to deliver effective embodied spatial instruction grounding in IGLU.",
            "uuid": "e1728.3",
            "source_info": {
                "paper_title": "Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "Guiding pretraining in reinforcement learning with large language models",
            "rating": 2,
            "sanitized_title": "guiding_pretraining_in_reinforcement_learning_with_large_language_models"
        },
        {
            "paper_title": "Pre-trained language models for interactive decision-making",
            "rating": 2,
            "sanitized_title": "pretrained_language_models_for_interactive_decisionmaking"
        },
        {
            "paper_title": "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models",
            "rating": 2,
            "sanitized_title": "jarvis1_openworld_multitask_agents_with_memoryaugmented_multimodal_language_models"
        },
        {
            "paper_title": "Building open-ended embodied agents with internet-scale knowledge",
            "rating": 2,
            "sanitized_title": "building_openended_embodied_agents_with_internetscale_knowledge"
        },
        {
            "paper_title": "Learning to model the world with language",
            "rating": 1,
            "sanitized_title": "learning_to_model_the_world_with_language"
        }
    ],
    "cost": 0.022217999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments</p>
<p>Zoya Volovikova volovikova@airi.net 
AIRI
MoscowRussia</p>
<p>MIPT
MoscowRussia</p>
<p>Alexey Skrynnik 
AIRI
MoscowRussia</p>
<p>FRC CSC RAS
MoscowRussia</p>
<p>Petr Kuderov 
AIRI
MoscowRussia</p>
<p>MIPT
MoscowRussia</p>
<p>Aleksandr I Panov 
AIRI
MoscowRussia</p>
<p>MIPT
MoscowRussia</p>
<p>FRC CSC RAS
MoscowRussia</p>
<p>Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments
4C73ED68131FF9BEBEC1073969CCA36F
In this study, we address the issue of enabling an artificial intelligence agent to execute complex language instructions within virtual environments.In our framework, we assume that these instructions involve intricate linguistic structures and multiple interdependent tasks that must be navigated successfully to achieve the desired outcomes.To effectively manage these complexities, we propose a hierarchical framework that combines the deep language comprehension of large language models with the adaptive action-execution capabilities of reinforcement learning agents.The language module (based on LLM) translates the language instruction into a high-level action plan, which is then executed by a pre-trained reinforcement learning agent.We have demonstrated the effectiveness of our approach in two different environments: in IGLU, where agents are instructed to build structures, and in Crafter, where agents perform tasks and interact with objects in the surrounding environment according to language commands.</p>
<p>Introduction</p>
<p>The ability to solve complex tasks, formulated in natural language, that require a long sequence of actions in the environment is a fundamental property of human intelligence.The recent significant success of large language models (LLMs) in instruction following and explanation generation demonstrates their powerful capabilities in solving commonsense, general knowledge, and code generation problems within the verbal domain.However, success rate of multi-step task completion for autonomous agents driven by general purpose LLMs is still low [19].Moreover, LLMs are often trained solely on textual data, which limits their ability to understand and perform actions in real-world-like environments.Consequently, even ChatGPT [25] exhibits poor spatial reasoning [2].On the other hand, reinforcement learning (RL) has proven effective in learning sequences of fine-grained actions for specific tasks within an environment.Thus, investigating the combination of LLMs for natural language understanding and high-level planning, along with RL for learning environmental manipulation, represents a promising research direction.</p>
<p>LLMs can be regarded as universal knowledge bases that allow human users to interact in natural language and solve complex tasks [12,38].Recent studies have shown that pretrained LLMs can construct high-level action plans in both Figure 1: The task of collaborative interaction between the agent, the environment, and the user involves the following: the user provides instructions to the agent, and the agent executes actions within the environment to accomplish the task based on these instructions.simulated and real environments [3,22,23].However, these LLM-based approaches necessitate manual prompt engineering, handcrafted translation of language commands into embodied actions, and a strategy for goal-aware action selection from the distribution of potential options generated by language.In this context, several studies [1,25] have demonstrated that the rough action plans extracted from language models can be refined using RL.</p>
<p>In our research, we present the hierarchical framework IGOR (Instruction Following with Goal-Conditioned RL), which combines the capabilities of large language models for understanding complex natural language constructions with RL-based policies to develop effective behavior in the embodied environment.The framework relies on two main components: a Language Module, which translates instructions in natural language into a high-level action plan with generated subgoals, and a Policy Module, tasked with executing this plan and achieving subgoals.</p>
<p>Furthermore, we introduce independent learning strategies for each module.We have developed efficient learning strategies for the LLM for limited datasets.Some of these strategies are based on data augmentation using the Chat-GPT model, while others rely on subdividing subtasks by altering data formats and decomposing these subtasks into arXiv:2407.09287v1[cs.AI] 12 Jul 2024 primitives.We set a learning task for the RL agents based on goals and curriculum.This approach promotes highly efficient task execution in dynamic environments beyond the training sample.</p>
<p>The effectiveness of our approach was rigorously tested in two embodied environments.The first is IGLU [40], where the 'Builder' agent constructs structures based on natural language instructions from the 'Architect' agent (see an example in Fig. 1).The second is the Modified Crafter environment, where the instruction-following agent needs to adapt to dynamically changing environments.Our results demonstrate that the application of our method not only outperforms the algorithms presented in the NeurIPS IGLU competition 1  [14,13,15], but also surpasses the baselines based on Dreamer-v3 in Crafter environment [10].</p>
<p>The main contributions 2 of our paper are:</p>
<ol>
<li>We proposed a novel task decomposition approach that facilitates the incorporation of augmentations, curriculum learning, and potentially other techniques to multi-modal setups involving LLM and RL learning for virtual environments.2. In addition to the known IGLU environment, we presented a modified Crafter environment by introducing a textual modality and prepared a corresponding dataset to support this enhancement.3. We conducted extensive experiments to compare our approach with other methods in both the Crafter and IGLU environments, demonstrating significant improvements.</li>
</ol>
<p>Related work</p>
<p>Planing with LLM.Recent studies are actively exploring the generation of action plans using language models.Some works focus on prompt engineering for effective plan generation [32], while others address the challenge of translating the language model's output into executable actions [12].In [1], models are trained with RL for selecting feasible actions and executing elementary actions.Many of these works [31,20] aim to control robots interactively in real time.Maintaining a dialogue with humans is an essential area of research for robotics [26,9].Language Grounding Problem.The language grounding problem in intelligent agents involves linking objects across modalities, such as matching textual instructions with objects in virtual environments.Methods to address this include using CLIP for visual-textual links [28], cross-attention mechanisms, and compressing data into hidden subspaces, exemplified by Dynalang [18].Some strategies integrate language processing with reinforcement learning, using text embeddings as observations [11,18].Others connect textual descriptions to environmental entities using transformer models like Emma and EmBERT [36,33].Additionally, some approaches use multiple modules trained independently, with pre-trained language models aiding in planning and adapting actions, addressing the lack of real-world experience [12,5,17].Innovatively, models like JARVIS-1 combine pre-trained memory 1 https://www.iglu-contest.net 2 Our code is available at https://github.com/AIRI-Institute/IGORblocks with tools like CLIP, enhancing multimodal memory and scheduling [37,8].</p>
<p>Embodied environments.In the field of embodied reinforcement learning, several platforms have been developed to train agents based on text instructions.Among these, AI2Thor [16] and Habitat [34] , offer tasks that are simple and adhere to strict rules, which simplifies the process of linking actions to text using straightforward syntactic structures (Messenger [36], HomeGrid [18], TWOSOME [35]).</p>
<p>Furthermore, advancements have been made to enhance the Crafter [10] environment, resulting in the creation of the Text-Crafter [6] version.Similarly, the MineDojo [8] platform, which is based on Minecraft, has been introduced.These platforms are designed for more intricate linguistic and planning tasks.Additionally, the IGLU [15] environment stands out for its complexity.In IGLU, agents must follow detailed instructions to construct structures within a virtual world.These environments are characterized by a vast state space and involve complex tasks that are formulated by humans.</p>
<p>IGOR: Follow Instruction with Goal-based RL</p>
<p>IGOR Flow Example</p>
<p>Task Manager</p>
<p>Task Manager</p>
<p>Figure 2: The IGOR framework has three modules: a Language module that solves language understanding problems and provides a high-level plan of subtasks, a Task Manager that encodes the subtasks for the Policy module, and a Policy module that executes actions in the environment based on visual observations and subtasks.</p>
<p>The IGOR framework is designed to solve the challenges of natural language understanding and instruction execution within virtual environments, enabling the processing of instructions that contain complex linguistic structures and spe-Figure 3: The diagram displays the IGOR system, where the "Language Module" transforms text instructions into subtasks.The "Task Manager" coordinates the subtasks and monitors their execution.The "Policy Module" operates in a virtual environment based on the subtasks.Dotted lines indicate the training process of the modules, while solid lines show how the modules interact during inference.cific terminologies.The virtual environments in which the intelligent agent operates are characterized by extensive observation areas and require the execution of multiple interconnected tasks.The framework is composed of three key modules.</p>
<p>The Language Module, implemented using Large Language Models (LLM), analyzes incoming instructions and converts them into a high-level plan consisting of a set of specific subtasks that need to be executed.</p>
<p>The Policy Module, implemented using reinforcement learning methods based on the Proximal Policy Optimization (PPO) algorithm, is responsible for the strategic interaction in the environment, including the execution of the interconnected tasks.</p>
<p>The Task Manager acts as a wrapper over the virtual environment, transforming the list of subtasks provided by the Language Module into a format understandable to the Policy Module.This module also ensures that the tasks specified in the instructions are completed and concludes the episode after their execution.</p>
<p>Thus, the inference pipeline operates by initially receiving instructions which are processed by the Language Module, where they are translated into a series of subtasks -a high-level execution plan.Subsequently, the Policy Module executes the necessary actions in the virtual environmrnt to achieve the objectives outlined in the instructions.An example of using our pipeline can be found in the Figure ( 2) Training for each learnable module is conducted separately, which provides flexibility in the integration of training methods and techniques.The training of the Language Module involves various augmentations and modifications to the dataset to prevent overfitting.The Policy Module is trained using a goal-based approach, which allows for training on a broader set of potential objectives than those available in the initial data.The inclusion of a curriculum in the training process also significantly enhances the quality of the final agent.</p>
<p>Language Module Training Techniques</p>
<p>The core of our language module utilizes a large pre-trained language model, which has been further finetuned on a dataset specific to the environment.This dataset contains information on how instructions translate into specific subtasks, enabling the model to understand and decompose complex commands effectively.The training leverages a specific format that maps instructions to their corresponding subtasks (e.g., Instruction -&gt; Subtask 1, Subtask 2, Subtask 3).</p>
<p>Due to the difficulty of obtaining comprehensive datasets for training models to translate language instructions into commands, we face additional challenges.Typically, datasets for training large language models (LLMs) on such tasks are manually curated, which is a labor-intensive process.This often limits both the size and the quality of the datasets available.In response, we employ techniques to prevent overfitting, especially when working with these limited datasets.Our experiments demonstrate that these methods effectively enhance training quality by ensuring the model can generalize well from smaller, varied linguistic datasets, leading to a more robust understanding of instructions.</p>
<p>Augmentation with LLM.In this technique, we begin by understanding the structure and specific terminology of the dataset.Our approach involves iteratively modifying the list of subtasks necessary to execute a given instruction for each dataset element.Subsequently, ChatGPT or another LLM is tasked with rewriting the instruction to incorporate these modified subtasks.</p>
<p>The prompt request is structured as follows:</p>
<p>1) Description of the Environment: Provide the LLM with a detailed understanding of the setting by explaining the overarching themes and key specific concepts.Use succinct descriptions for familiar ideas and detailed explanations for unique aspects.</p>
<p>2) Few-shot Example: Introduce the original instruction alongside its required subtasks.Optionally, you can also provide an example of how the instruction might change if the subtasks are altered.</p>
<p>3) Task Modification Request: Specify new subtasks for the target instruction and request the LLM to revise the instruction accordingly.</p>
<p>It is important to emphasize that the LLM is not creating the instruction from scratch.We aim to start with the existing instruction and suggest modifications, ensuring that the style of the original instruction is preserved as much as possible.</p>
<p>Subtasks decomposition.The second technique entails decomposing original subtasks into "primitives", which involves modifying the structure and format of subtasks in the dataset.Essentially, it suggests consolidating frequently cooccurring subtasks into a single common subtask, thereby reducing the data volume required for processing by the language model.These aggregated subtasks are termed "primitives".</p>
<p>We explore two methods for creating primitives.One method utilizes unique tokens, such as emojis, to encode each primitive.Emojis are chosen for their diverse range, making them a convenient means of representing a broad array of subtasks.The second method involves crafting primitives in a manner that aligns logically with the content of the instructions.This approach aims to enhance the coherence between the instructional context and the subtask structure and is the approach employed in our experiments.</p>
<p>Task Manager</p>
<p>The Task Manager serves as an intermediary to connect the Language Module and the Policy Module during execution and to allocate subtasks from the dataset to the Policy Module throughout its training phase.It retrieves a list of subtasks and systematically supplies these to the RL agent as components of its observational input.Based on observations collected during the agent's interaction with the environment, the Task Manager determines whether a subtask has been successfully completed and decides whether to proceed to the next task or to conclude the episode.Upon successful completion of a subtask, the Task Manager assigns a positive reward (r = +1) to reinforce the agent's behavior.</p>
<p>Training the Policy Module</p>
<p>Instead of training a RL agent to tackle all subtasks required by a complex instruction simultaneously, we propose a goalbased learning approach.In this approach, each episode presents the agent with an observation and only one of the subtasks from the instruction.By doing this, we simplify the agent's task, reducing complexity of the task and facilitating more focused learning on specific aspects of the overall problem.</p>
<p>We examine a visual-based reinforcement learning environments characterized as a Partially Observable Markov Decision Process (POMDP).In this framework, the observation function is augmented by subtask encodings provided by the Task Manager.The primary objective of the agent is to develop a policy π that selects actions to maximize the expected cumulative reward.To achieve this, we employ policy gradient methods, specifically the Proximal Policy Optimization (PPO) algorithm.PPO has been demonstrated to offer substantial robustness across various tasks, attributed to its effective balancing of exploration and exploitation by optimizing a clipped surrogate objective function E min(ρ t (θ) Ât , clip(ρ t (θ), 1 − ϵ, 1 + ϵ) Ât ) , where
ρ t (θ) = π θ (at|st)
πold(at|st) and Ât denotes the advantage estimate at time t [30].</p>
<p>During training we utilize curriculum task sampler that dynamically adjusts the probability of selecting subtasks based on their performance, inspired by curriculum learning approaches [21,24].Specifically, the selection probability for each task i in T is modified according to:
q i = 1 d if r i ≥ τ 1 + (δ i • d) if r i &lt; τ
where r i is the task's average reward, δ i measures the variability in reward, d is a scaling coefficient, and τ is a success threshold.Probabilities are normalized using a softmax function to form a distribution from which tasks are sampled.A detailed description of the algorithm, its pseudocode and ablation study can be found in the Appendix.</p>
<p>Experimental Setup</p>
<p>To investigate and test the capabilities of intelligent agents, we have chosen environments with high combinatorial complexity.These environments allow us to assess how agents cope with tasks requiring the execution of many interrelated subtasks to achieve a target state.</p>
<p>The first environment is IGLU, where the agent's task is to build three-dimensional constructions based on textual descriptions.It is important to note that the complexity of the environment largely lies in the fact that, depending on the instructions, there can be a vast number of potential target states.To be successful in such an environment, an agent must possess advanced text interpretation skills, as well as the ability to think spatially and model ordering to adequately recreate the required structures.</p>
<p>The second environment is Crafter, where the agent needs to follow textual instructions to perform a variety of tasks, such as gathering resources and crafting items.This environment tests the agent's ability to understand natural language and effectively plan sequences of actions in response to changing conditions.</p>
<p>Below is the general pipeline for applying our approach to these virtual environments: 1. Fine-tune the LLM: Fine-tune the large language model (LLM) with an environment-specific dataset to translate instructions into subtasks for the reinforcement learning (RL) agent</p>
<p>IGLU Environment</p>
<p>Environment.IGLU is an environment 3 where an embodied agent can build spatial structures (figures) of blocks of different colors.The agent's goal is to complete a task expressed as an instruction written in natural language.</p>
<p>The observation space consists of point-of-view (POV) image (64, 64, 3), inventory item counts (6), and the pitch and yaw angles (5).The agent can navigate over the building zone, place, and break blocks, and switch between block types.Additionally, the environment provides a textual observation in the form of a dialogue from the dataset, which defines a building task.The examples of such target tasks is presented in Fig. 4.</p>
<p>Figure 4: IGLU is a 3D environment where agents are tasked with constructing structures in a designated area, guided by descriptions provided in natural language and the agent's first person perspective.</p>
<p>Target utterances define the rest of the blocks needed to be added.The environment provides two modes of action for choice: walking and flying.In our experiments, we create agents for use in both flying and walking modes.The action space combines discrete and continuous subspaces: a discrete space of 13 actions (noop, four camera rotation actions, break block, place block, choose block type 1-6) and a set of 6 continuous actions for movement in all three directions.</p>
<p>Metrics.We employed the F1 metric, as proposed in the IGLU competition, to evaluate the quality of the approaches.</p>
<p>Baselines.We have selected the three best solutions from the IGLU 2022 competition for comparison.The second and third-place solutions, which utilized T5 [29] and Pegasus [39] models respectively, are based on the organizers' proposed solution but differ in the NLP models employed.We also include the solution of the first-place team, called BrainAgent 4 , whose approach differs significantly from the others.They used end-to-end learning, with the RL agent taking the embedding of a frozen NLP model as input, along with environment information and manually added features.</p>
<p>Dataset.The training and testing datasets contain 109 and 41 English instructions, respectively, with corresponding grids that need to be constructed.Each instruction consists of individual building steps with a corresponding voxel for each step.</p>
<p>For training the language model, we transform voxel shapes into text -a list of block coordinates that need to be placed.Grids are converted into a sequence of block coordinates and colors in the form of a string of tuples.For this dataset, a single block (its coordinates and a color) represents a single subtask in our terminology.</p>
<p>The order of the blocks in the sequence corresponds to the appearance of the blocks in the grid and have the following ordering rule: first by x, then by z, and then by y, where x and z are horizontal axes and y is vertical axis.In many instructions, the horizontal position of the first block is not specified, such that it can appear anywhere along x and z randomly.This is detrimental to the model during training as it will lead to high loss values for tokens that cannot be derived from the input data.Therefore, we change the sequence so that the first block of the first step of the instruction is placed at the center of the space, and the other blocks are shifted accordingly.</p>
<p>Figures Classification</p>
<p>Based on the capabilities an AI agent needs to construct figures in a dataset, we have developed our own rule-based classification of figures.This helps to more precisely determine which types of figures pose challenges and which descriptions of figure types might cause difficulties in construction by a language model.For visual examples, see Appendix.The classes, defined by their spatial characteristics, are presented below:</p>
<p>• flat figures are characterized primarily by their length and width, with minimal emphasis on height.• tall figures prioritize height over both length and width.</p>
<p>• air figures are distinguished by their lack of contact with the ground, signifying that they are airborne.• floor figures are defined by their complete contact with the ground, resting entirely on the surface.</p>
<p>Training details.The Language Module for the IGLU environment was trained to predict, based on dialogue instructions, a list of coordinates of blocks that need to be placed or removed, where each coordinate corresponds to a subtask.We created a second dataset with a modified format of subtasks, grouping blocks that occur together in instructions into parallelepipeds.An example of the subtasks format for the IGLU dataset can be found in 1.Additionally, we augmented the dataset using ChatGPT, where we tasked the model to rotate the object by 180 degrees.The modification we applied for ChatGPT augmentation involved rotating the object by 180 degrees.We requested the LLM to adjust the building instructions to match this modification, ensuring the textual description aligned with the altered figure orientation.</p>
<p>As the base for the Language Module, the Flan-T5 base model was used.The LLM models are trained in a seq2seq manner, converting the instruction into a sequence of blocks as described above.Training hyperparameters and hardware parameters are listed in the appendix 14.</p>
<p>Table 1: Example of coords and prims subtask formats in the IGLU environment, corresponding to the instruction: "<Ar-chitect> place 5 red blocks in a row, one row north of center."</p>
<p>Format</p>
<p>Description Example coords (x, y, z, colorID):</p>
<p>• x, y, z: coordinates • colorID: block color id (0, 5, 5, 3), (0, 6, 5, 3), (0, 7, 5, 3), (0, 8, 5, 3), (0, 9, 5, 3) prims (start), (size), rotation, color:</p>
<p>• (start): initial block (x, y, z) • (size): dimensions (x, y, z)</p>
<p>• rotation: alignment • color: block color name (0, 5, 5), (1, 1, 5), eastsky, red</p>
<p>For training Policy Module in the IGLU environment, our objective is to train the agent to either place or remove a single block on a virtual field.At the beginning of an episode, a list of subtasks is randomly generated or taken from a training dataset.A specific block from this list is then selected as the target.Depending on this selection, we generate the initial state of the environment by positioning the blocks leading up to the target block.</p>
<p>The reward mechanism used during training is the same as proposed by the competition's authors.</p>
<p>To enhance the quality of training for the RL agent, we incorporate a curriculum that controls the complexity of the list of subtasks.This structured approach helps in progressively challenging the agent, thus improving its learning efficiency and capability to handle complex tasks.</p>
<p>Solving visual goal-based tasks can be accomplished by a wide range of modern RL approaches, both model-free [4,7,27] and model-based [10,11].In this work, we used an opensource implementation of asynchronous PPO5 [27], which demonstrated better performance in terms of GPU hours to final score based on our preliminary experiments.Environment.In the original Crafter environment 6 , agents are primarily tasked with exploration, gathering resources, and battling monsters.We have adapted this framework so that each episode now provides a free-form textual instruction, directing the agent to perform the number of specific tasks.In our modified version, each task given to the agent corresponds exactly to one achievement in the original game.Thus, within a single episode, following these instructions means completing individual achievements directly tied to the game's original goals.The examples of the agent in Crafter environment is presented in Fig. 5.</p>
<p>Crafter Environment</p>
<p>Dataset.To create a dataset of textual tasks for the agent in the Crafter environment, we followed a specific pipeline: First, based on the known list of tasks and achievements in the crafter environment, we randomly generated a list of subtasks.Second, to obtain descriptive instructions for these tasks, we used Mistral-7B-Instruct 7 .The example of such prompt and response is presented in Fig. 6.Third, to obtain more varied descriptions of instructions, using the same Mistral model to stylize the received instructions (for example of such reformulation see Fig. 7).</p>
<p>Example of prompt request:</p>
<p>Rewrite the list of subtasks into one instructive sentence for an agent in a virtual environment, replacing words from the list with synonyms.Subtasks: ["Defeat Zombie", "Collect Iron with count 1", "Make Iron Sword"]</p>
<p>Example of response:</p>
<p>Vanquish the undead foe, gather a single unit of metallic mineral, and forge an iron weapon.</p>
<p>Example of prompt request:</p>
<p>Rewrite the instruction in the style of Tolkien with the same SHORT sentence size adding causality.Instruction: Vanquish the undead foe, gather a single unit of metallic mineral, and forge an iron weapon.</p>
<p>Example of response:</p>
<p>Slay the wights, gather a nugget of mithril, and forge a blade of iron.Metrics.To assess the quality of an agent's performance in an environment where it follows language instructions, we used a metric called "Success Rate."This metric measures whether all the subtasks required by the given instructions to the agent were completed.Accordingly, for one instruction, the metric can be 1 if the agent completed all the tasks, or 0 if any task was not completed.</p>
<p>Baselines.We compared our IGOR Agent with Dynalang [18].To run Dynalang on a text-adapted Crafter, we employ the T5 model to encode the text, mirroring the approach taken by the paper's authors.In each episode, we sequentially transmit all instructions and images to Dynalang, one token at a time.This process allows us to evaluate Dynalang's ability to identify the necessary subtasks from the instructions, aligning with our methodology.We adopt the same reward system used in our experiments.</p>
<p>Training details.A Language module for the crafting environment was trained on the provided data without any modifications.Following the instructions within this environment, the module is tasked with determining which achievements the agent should collect in the virtual setting.Additionally, the Flan-T5-base model was utilized as a reference.</p>
<p>For Policy Module we use self-supervised goal sampling to train the agent on these subtasks efficiently.In this approach, each training episode for the agent targets a distinct goal selected randomly, aligning with the goals possibly derived from the LLM.The reward function for the RL agent is designed to provide positive feedback for the successful completion of subtasks, motivating the agent to achieve its goals.This function uses both the original Crafter environment reward and an extra reward for the completion of a subtask by the agent.The original reward is scaled by a factor of 0.1, and the extra reward for subtasks accomplishing is set to 1. Through this process, the agent learns to solve individual subtasks, contributing to the overall action plan devised by the LLM.</p>
<p>Experimental Results</p>
<p>In this section, we compare our approach with other state-ofthe-art methods.The section is organized as follows: First, we present the results in the modified Crafter environment.Second, we show the results for the IGLU environment, alongside ablation experiments for different data processing techniques.</p>
<p>Crafter Environment</p>
<p>The results of IGOR and Dynalang in the Crafter environment are shown in the bar chart presented in Fig. 8.The x-axis displays various categories, where the Total category represents the overall success rate across all test instructions.The other categories illustrate the agents' performance on specific subtasks within the instructions, highlighting how effectively each agent solves individual subtasks when attempting to complete the entire instruction.For instance, a low value for a particular subtask on the plot indicates that the agent often fails to complete the instructions when that subtask is involved.</p>
<p>Overall, the total score of IGOR is 0.6, indicating that the agent successfully completes all required subtasks in 60% of cases, significantly outperforming Dynalang, which only achieves 36.4%.Our approach demonstrates superior performance in 19 out of 22 subtasks.Furthermore, it achieves results that are at least twice as good in 8 out of 22 types of instructions.Additionally, our approach, with the integration of curriculum learning, shows strong capabilities in managing tasks that are especially challenging, such as Collect Diamond, Collect Iron, Make Iron Sword, and Make Iron Pickaxe.</p>
<p>IGLU Environemnt</p>
<p>The comparative results in the IGLU environment are presented in Table 2.We ran the IGOR approach in both Flying and Walking modes.Additionally, we conducted experiments with different data variants for training the LLM agent, utilizing both primitives and coordinates as subtasks.Across all configurations, our method consistently outperforms the competitors, achieving significantly better results in all figure categories.Notably, IGOR's lowest score is 0.45, which surpasses the overall performance of the winning team, BrainAgent, which scored 0.36.For simpler figures such as Floor using primitives, our agent scores 0.75 compared to BrainAgent's 0.53.A clear advantage is also observed for more complex tasks such as Air, where IGOR scores 0.46 versus 0.25.Even when using coordinates as subtasks, our agent's advantage, although reduced, persists across all figure types.Comparing Data Processing Technics.We evaluated various data processing strategies for training NLP models, as detailed in Table 3.These strategies included expanding the original dataset (strategy "Coords"), employing the ChatGPT augmentation technique outlined previously (strategy "Chat-GPT"), and modifying the dataset's format (strategy "Prim").</p>
<p>We also enhanced the dataset with color augmentation by altering the color of the block and its corresponding descriptor in the instructions (strategy "Color").It can be observed that adaptation to primitives, even in the absence of augmentations, improves the F1 metric by 0.08 (from 0.34 to 0.42).When combined with augmentations, the F1 score sees a 0.16 boost (0.50) versus 0.01 increase via augmentations alone on coordinates (0.35).</p>
<p>While the ChatGPT augmentations don't yield a significant increase on primitives, an analysis of the F1 scores across different shape categories reveals their effectiveness.On coordinates, the ChatGPT augmentation outperforms color augmentations, showing a 0.4 relative improvement (0.35 vs 0.39).</p>
<p>This suggests that the augmentation strategies have different impact depending on subtask format.Additionally, it is apparent that the primary challenge for the language model arises from instructions requiring the prediction of blocks suspended in the air-these tasks exhibit the highest error rates.This may be due to the limited number and specific nature of such instructions in the training dataset.</p>
<p>Limitations</p>
<p>Exploring previous studies in this area reveals that creating a general approach for multimodal environments presents a significant challenge [41].While many approaches show potential, they often underperform when applied outside their original domains.Aiming for a universally applicable method is an admirable goal, yet it currently has limited practical use.</p>
<p>Our work introduces a new framework that separates the training of RL and LLM components.The proposed decomposition aims to achieve architectural flexibility, facilitating the easy integration of various techniques such as curriculum learning and reward shaping for RL, as well as data augmentation and human feedback for LLM training.However, our approach requires the identification of known subtasks, which may not be present in some domains, thus necessitating additional data collection to identify them.</p>
<p>Conclusion</p>
<p>In this paper, we introduce the Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments (IGOR) method, a novel approach that translates natural language instructions into a sequence of executable subtasks.The IGOR method integrates two independently trainable modules along with an intermediary Task Manager module.The Language Module is responsible for converting textual instructions into a defined sequence of subtasks.Meanwhile, the RL agent is structured in a goal-conditioned manner, making it adept at handling a wide array of tasks across various environmental contexts.</p>
<p>The modular decomposition of the IGOR approach allows for the incorporation of additional training techniques such as data augmentation, goal-based strategies, and curriculum learning in RL.A detailed analysis of the results shows that this approach not only allows for more flexible training customization but also yields a significant improvement in performance.</p>
<p>We demonstrate these advantages through experiments in two environments.In the IGLU environment, where the agent is required to construct a figure in a virtual setting based on dialogue with a human, our method surpasses the winners of the IGLU 2022 competition.Additionally, we have adapted the Crafter environment to require the agent to achieve specific achievements based on instructions, showing that our approach outperforms the Dynalang method based on Dreamer V3.</p>
<p>Curriculum Learning</p>
<p>We use curriculum learning technique, inspired by principles outlined in [21,24].The core objective of this method is to dynamically adjust the selection probabilities of training tasks to optimize learning progression based on empirical performance measures.The pseudocode is outlined in Algorithm 1.</p>
<p>Our algorithm commences with an equal likelihood of selecting any given task from a list of tasks T , each characterized by two parameters: an average reward r i , computed using an exponential moving average, and a reward variability measure δ i = |r t i − r t−1 i |, which captures the absolute change in rewards between consecutive trials.This dynamic adjustment is governed by two key parameters: a scaling coefficient d and a success rate threshold τ .</p>
<p>The task selection probability is modified according to the task's performance relative to τ .For tasks where the reward r i meets or exceeds τ , the algorithm assigns a lower selection probability of q i = 1 d , effectively deprioritizing tasks that have already surpassed a certain success benchmark.Conversely, tasks that fall below this threshold are given a higher probability of selection, formulated as q i = 1 + (δ i • d).This scheme favors tasks with greater variability in performance, hypothesizing that such tasks may yield more informative learning experiences due to their higher potential for improvement.</p>
<p>To ensure a probabilistic selection, the probabilities are normalized using a softmax function, yielding a distribution p.A task is then selected randomly according to this distribution, with the likelihood of selecting task i given by p i from p, effectively integrating an element of stochasticity into task selection which is critical for exploring a variety of learning experiences.This methodology allows for a balanced exploration-exploitation trade-off, adapting task difficulty based on learner performance to potentially accelerate skill acquisition in complex learning environments.</p>
<p>This adaptive approach to task selection represents a significant enhancement to the efficiency of curriculum learning strategies, ensuring that learners are consistently challenged with tasks appropriate to their evolving capabilities.This strategy not only maintains engagement but also maximizes learning outcomes by strategically modulating the difficulty of tasks presented over the course of training.</p>
<p>To highlight the importance of curriculum learning techniques, we conducted an additional experiment in the Crafter environment.Here, we compare the learning processes of a PPO agent with and without curriculum learning.In Figure 9, we show the probability of sampling each subtask during training.For the uniform policy, this is represented by a line parallel to the x-axis.In Figure 10, we provide the success rates for both approaches.It is clear that the version with curriculum learning shows better results for tasks such as collect_iron, collect_diamond, make_iron_pickaxe, and make_iron_sword.This improvement can be attributed to the dynamics shown in the first plot.For example, for the collect_iron task, as the agent begins to master this problem, the sampling probability increases.Once performance plateaus, the probability decreases.This pattern also applies to other tasks; if the agent has already mastered a subtask, Algorithm 1: Task Selection Based on Success Rate Input: List of tasks T , each task i characterized by parameters (r i , δ i ) where r i is the average reward for task i using exponential moving average, and δ i = |r t i − r t−1 i | is the average difference between subsequent rewards for task i, also using exponential moving average.Constants d (a coefficient that influences sampling probability) and τ (a success rate threshold).Output: Selected task 1 Function AdaptiveSelect(T , d, τ ):
2 Initialize a vector q ← []; 3 for each task i ∈ T do 4 if r i ≥ τ then
// Assign a lower probability for tasks that exceed the success threshold τ 5 q i ← 1 d ; 6 else // Assign higher probability to tasks with more variability and below the threshold orange blocks, then destroy the bottom orange block on all three stacks"; I will get a cords for built figure: ((2 4 0), (2 5 0), (2 6 0)).If I want a column/tower of 6 yellow blocks in the middle of the grid, the coords will be (5 5 0), (5 5 1), (5 5 2), (5 5 3), ( 5 5 4), (5 5 5).If I want a row with 3 blocks towards south: (5 5 0), (5 6 0), (5 7 0).What coords will be for tower of 6 yellow blocks in the middle of the grid?As aswer, send me only list of coords.</p>
<p>Next, we ask to add the color of the shape to the coordinate description, which should be chosen for a specific block.</p>
<p>As aswer, send me only list of coords.Add color digit to tuples array with the next color mapping: BLUE -1 GREEN -2 RED -3 ORANGE -4 PURPLE -5 YELLOW -6 and use 0 -for blocks needs to be removed Now, in the chatbot's single-session mode, we sequentially present the instructions from the task without making any alterations to them.</p>
<p>Instruction Augmentation Prompt</p>
<p>In this prompt, we also start by providing information about the environment like in 14 and the dialogues.Then, we ask to rewrite the dialogue as if the shapes were required to be built rotated by 180 degrees.</p>
<p>I have a dialog with instructions describing how to build a 3D figure on a plane.The instructions have information on where to build the shapes and how to build them.I will consistently give you dialogues, and you have to rewrite the instructions so that they describe the construction of the same figure, only the location is rotated 180 degrees clockwise.Size of eviroment is (9,11,11) Remember: column and tower are vertical structures, line and row are horizontal.Do not change the instructions associated with the modification of vertical structures (keywords top and bottom).The output is only the final dialogue (be sure to save the ++ signs, I will then use them to separate the dialogue into instructions).</p>
<p>IGLU Structure Classes</p>
<p>Examples of structure classes in the IGLU environment are presented in Fig. 11.Each structure represents a typical example used in the experimental results section.Please note that the same figure can be attributed to several classes; for example, structure (a) is both tall and flying simultaneously.The structure on the floor (c) is also a flat structure, etc.</p>
<p>The complexity of each class is highlighted by the experimental results.tall and flying structures are more difficult than floor and flat structures.floor is the easiest one, since it doesn't require the ability to place or remove proper supporting blocks, which are much more challenging tasks.</p>
<p>IGLU Reward Function</p>
<p>In the IGLU environment, the reward function used changes throughout the training curriculum to balance guided exploration and precise task execution.Initially, the primary reward is based on the distance from the modification to the goal, calculated as +1 dist+1 .As training progresses, this reward structure transitions to a more rigorous system where the agent receives +1 for correct modifications (indicating successful  subtask completion) and a constant penalty of −0.5 for incorrect modifications, regardless of the distance.Additionally, in the early stages of the curriculum, there is an auxiliary reward for reducing the distance between the agent and the goal, provided only when moving positively towards the goal.This proximity reward diminishes as training advances, encouraging the agent to autonomously navigate and execute tasks by the end of the curriculum.</p>
<p>Hyperparameters and Hardware Resources</p>
<p>a pair of undead and gather water."High-level plan / Subtasks: "[Defeat Zombie, Defeat Zombie, Collect Water"] Encoded subtasks: "[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 1, 0]" Sample one subtask from list: "[0, 0, 0, 1]" Policy: The Policy Module subsequently executes the necessary actions in the virtual environment</p>
<p>Figure 5 :
5
Figure 5: Crafter is a 2D environment reminiscent of Minecraft, where players must gather food and water, acquire resources, fend off creatures, and construct tools.</p>
<p>Figure 6 :
6
Figure 6: The example depict prompt and response interactions used to generate the dataset for the modified Crafter environment.</p>
<p>Figure 7 :
7
Figure 7: This figure shows an examples prompt and response for the reformulation of an instruction generated by the Mistral-7B-Instruct.</p>
<p>T o t aFigure 8 :
8
Figure 8: A comparison of the performance of IGOR and Dynalang approaches, on the Crafter environment with textual tasks.The success rate metric for each subtask is averaged across instructions requiring that specific subtask.Total bar represents the overall success rate for all instructions of test dataset.</p>
<p>Figure 11 :
11
Figure 11: Example of IGLU figures classes.</p>
<p>Measure performance: Measure performance on a test dataset for each environment.The LLM predicts subtasks from instructions, which the trained RL agent then executes.
environment-specific techniques: curriculum learning, hi-erarchical RL, reward shaping, and curiosity-driven ex-ploration.3.
. Add environment-specific techniques if needed: augmentation, data modification, and dataset expansion.2. Train the RL agent: Train the RL agent in a goal-based mode on environment-relevant subtasks.If needed, add</p>
<p>Table 2 :
2
A comparative analysis of the F1 score (higher is better) for the IGOR approach versus the IGLU-2022 competition winners on the test dataset.
ApproachPrim FlyingFloorFlatTallAirTotalIGOR✓✓0.720.51 0.46 0.400.52IGOR×✓0.680.44 0.36 0.340.45IGOR✓×0.750.46 0.32 0.330.46IGOR××0.680.42 0.29 0.310.45BrainAgentn\a×0.530.36 0.23 0.250.36MHBn\a×0.080.05 0.03 0.040.05Pegasusn\a×0.080.06 0.02 0.040.06</p>
<p>Table 3 :
3
A comparative analysis of various data processing strategies for LLM training.The table displays the F1 scores, which measure discrepancy between LLM-predicted figures and target figures.The highest scores in each category are highlighted in bold.
Prim Color ChatGPT FloorFlatTallAirTotal✓✓✓0.750.510.44 0.360.50✓✓×0.730.520.44 0.350.50✓××0.690.42 0.34 0.250.42×✓✓0.530.40 0.33 0.290.39×✓×0.390.36 0.33 0.290.35×××0.450.35 0.30 0.280.34</p>
<p>Table 4
4
lists the hyperparameters used for training the RL module of the IGOR approach.Table5provides the hyperparameters for training the Flan T5 language model, specifying only those parameters that were adjusted from the default settings in the HuggingFace.</p>
<p>Table 4 :
4
The parameters utilized for training the PPO actorcritic model.
HyperparameterValueAdam learning rate0.0001γ (discount factor)0.99Rollout32Clip ratio0.1Batch size1024Optimization epochs1Entropy coefficient0.003Value loss coefficient0.5GAE λ0.95ResNet residual blocks3ResNet number of filters64LSTM hidden size512Activation functionReLUNetwork InitializationorthogonalRollout workers16Environments per worker 32</p>
<p>Table 5 :
5
Parameters used to finetune Flan-T5, which have been changed from the default parameters in the Transformers library.
HyperparameterValuelearning rate0.0001batch size16gradient accumulation steps 16</p>
<p>Table 6
6
shows the training time and hardware resources.IGOR uses 139 GPU hours in Crafter and 32 in IGLU, while Dynalang requires 166 GPU hours in Crafter.IGOR efficiently combines RL and LLM modules on various devices.</p>
<p>Table 6 :
6
Training time and hardware resources for different modules
Training ModuleGPU hoursDeviceDynalang (crafter)166TITAN RTXIGOR (RL crafter)136TITAN RTXIGOR (LLM crafter)3 NVIDIA A100IGOR (RL iglu)20TITAN RTXIGOR (LLM iglu)12 NVIDIA A100
https://github.com/iglu-contest/gridworld
https://github.com/kakaobrain/brain-agent
https://github.com/alex-petrenko/sample-factory
https://github.com/danijar/crafter
https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1
Append q i to q; 10 end // Normalize probabilities 11 p ← Softmax(q); // Sample task index j according to the probability distribution p 12 j ∼ Categorical(p); 13 return T [j]; 14 end e.g., place_table or make_wood_pickaxe, the probability of sampling these subtasks decreases below that of uniform sampling, thereby freeing up resources for learning other subtasks.Coordinates Generation PromptFirst, we include a description of the IGLU environment in the prompt.This way, we provide information on how the coordinates change in relation to cardinal directions (north, west, sky).We also provide information about certain patterns present in the dataset (rows, squares, columns, towers) and what they signify in the dataset.Environment have size(11 11 9).Coordinates are (x y z).x coordinate increases towards south.y coordinate increases towards east.z coordinate increases towards the sky.So the highest south-west coordinate would be( 11 11 9).And the lowest nort-west coordinate is (0 0 0).Middle of enviroment/grid is (5 5 0).If I will start from the middle of enviroment/grid, but on the floor, face north and put one block, it coords will be (4 50).I can place blocks in this environment.For task: "Facing north, build three stacks of two
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Keerthana Finn, Karol Gopalakrishnan, Alex Hausman, Herzog, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, 2023</p>
<p>Mindcraft: Theory of mind modeling for situated dialogue in collaborative tasks. Cristian-Paul Bara, Ch-Wang Sky, Joyce Chai, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2021</p>
<p>Dota 2 with large scale deep reinforcement learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, arXiv:1912.066802019arXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on robot learning. PMLR2023</p>
<p>Guiding pretraining in reinforcement learning with large language models. Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, Jacob Andreas, International Conference on Machine Learning. PMLR2023</p>
<p>Impala: Scalable distributed deep-rl with importance weighted actorlearner architectures. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, International conference on machine learning. PMLR2018</p>
<p>Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Anima Zhu, Anandkumar, Minedojo, Advances in Neural Information Processing Systems. 202235</p>
<p>Dialfred: Dialogue-enabled agents for embodied instruction following. Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, Gaurav S Sukhatme, IEEE Robotics and Automation Letters. 742022</p>
<p>Benchmarking the spectrum of agent capabilities. Danijar Hafner, International Conference on Learning Representations. 2022</p>
<p>Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, arXiv:2301.04104Mastering diverse domains through world models. 2023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International conference on machine learning. PMLR2022</p>
<p>Interactive grounded language understanding in a collaborative environment: Iglu 2021. Julia Kiseleva, Ziming Li, Mohammad Aliannejadi, Shrestha Mohanty, Mikhail Maartje Ter Hoeve, Alexey Burtsev, Artem Skrynnik, Aleksandr Zholus, Kavya Panov, Srinet, NeurIPS 2021 Competitions and Demonstrations Track. PMLR2022</p>
<p>Julia Kiseleva, Alexey Skrynnik, Artem Zholus, Shrestha Mohanty, Negar Arabzadeh, Marc-Alexandre Côté, Mohammad Aliannejadi, Milagro Teruel, Ziming Li, Mikhail Burtsev, arXiv:2205.13771Interactive grounded language understanding in a collaborative environment at neurips 2022. 2022. 2022arXiv preprint</p>
<p>Interactive grounded language understanding in a collaborative environment: Retrospective on iglu 2022 competition. Julia Kiseleva, Alexey Skrynnik, Artem Zholus, Shrestha Mohanty, Negar Arabzadeh, Marc-Alexandre Côté, Mohammad Aliannejadi, Milagro Teruel, Ziming Li, Mikhail Burtsev, NeurIPS 2022 Competition Track. PMLR2023</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-Derbilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, arXiv:1712.054742017arXiv preprint</p>
<p>Pre-trained language models for interactive decision-making. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Advances in Neural Information Processing Systems. 202235</p>
<p>Learning to model the world with language. Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, P Abbeel, Dan Klein, Anca D Dragan, ArXiv, abs/2308.013992023</p>
<p>Agentbench: Evaluating LLMs as agents. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Interactive language: Talking to robots in real time. Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, Pete Florence, IEEE Robotics and Automation Letters. 2023</p>
<p>Teacher-student curriculum learning. Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman, IEEE transactions on neural networks and learning systems. 201931</p>
<p>FILM: Following instructions in language with modular methods. So Yeon Min, Devendra Singh Chaplot, Pradeep Kumar Ravikumar, Yonatan Bisk, Ruslan Salakhutdinov, International Conference on Learning Representations. 2022</p>
<p>Following natural language instructions for household tasks with landmark guided search and reinforced pose adjustment. Michael Murray, Maya Cakmak, IEEE Robotics and Automation Letters. 732022</p>
<p>Reinforcement learning with success induced task prioritization. Maria Nesterova, Alexey Skrynnik, Aleksandr Panov, Mexican International Conference on Artificial Intelligence. Springer2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Teach: Task-driven embodied agents that chat. Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, Dilek Hakkani-Tur, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022</p>
<p>Sample factory: Egocentric 3d control from pixels at 100000 fps with asynchronous reinforcement learning. Aleksei Petrenko, Zhehui Huang, Tushar Kumar, Gaurav Sukhatme, Vladlen Koltun, International Conference on Machine Learning. PMLR2020</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, International Conference on Machine Learning. 2021</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 211402020</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, arXiv:2209.11302Progprompt: Generating situated robot task plans using large language models. 2022arXiv preprint</p>
<p>Embodied bert: A transformer model for embodied, languageguided visual task completion. Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav S Sukhatme, ArXiv, abs/2108.049272021236975859</p>
<p>Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>True knowledge comes from practice: Aligning llms with embodied environments via reinforcement learning. Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, Bo An, ArXiv, abs/2401.141512024</p>
<p>Grounding language to entities and dynamics for generalization in reinforcement learning. H J , Austin Wang, Karthik Narasimhan, ArXiv, abs/2101.073932021</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, Yitao Liang, arXiv:2311.05997Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. 2023arXiv preprint</p>
<p>Vincent Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, arXiv:2204.005982022arXiv preprint</p>
<p>Pegasus: Pre-training with extracted gapsentences for abstractive summarization. Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, International Conference on Machine Learning. PMLR2020</p>
<p>IGLU Gridworld: Simple and Fast Environment for Embodied Dialog Agents. Artem Zholus, Alexey Skrynnik, Shrestha Mohanty, Zoya Volovikova, Julia Kiseleva, Artur Szlam, Marc-Alexandre Coté, Aleksandr I Panov, CVPR 2022 Workshop on Embodied AI. 2022</p>
<p>Silg: The multienvironment symbolic interactive language grounding benchmark. Victor Zhong, Austin W Hanjie, I Sida, Karthik Wang, Luke Narasimhan, Zettlemoyer, arXiv:2110.106612021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
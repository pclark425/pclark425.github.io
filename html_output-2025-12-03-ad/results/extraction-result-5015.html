<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5015 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5015</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5015</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-219531812</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2006.04757v2.pdf" target="_blank">Language Modeling for Formal Mathematics</a></p>
                <p><strong>Paper Abstract:</strong> We examine whether language modeling applied to mathematical formulas enables logical reasoning. We suggest several logical reasoning tasks that can be used to evaluate language models trained on formal mathematical statements, such as type inference, suggesting missing assumptions and completing equalities. To train language models for formal mathematics, we propose a novel skip-tree task, which outperforms standard language modeling tasks on our reasoning benchmarks. We also analyze the models' ability to formulate new conjectures by measuring how often the predictions that do not fit the ground truth or any training data turn out to be true and useful statements.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5015.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5015.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Skip-tree Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer (encoder-decoder) trained with the skip-tree language modeling objective</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder Transformer trained on formal HOL Light S-expression statements using a novel skip-tree self-supervised objective that masks and predicts entire subexpressions (<PREDICT>) and additional masked subexpressions (<MASK>) to encourage tree-structured completions and robustness to partial information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (encoder-decoder) trained with skip-tree objective</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A small encoder-decoder Transformer language model trained on parsed HOL Light S-expressions (HOList core+complex). The model uses a seq2seq architecture with an encoder and decoder; training examples replace a sampled subexpression by <PREDICT> and (optionally) additional subexpressions by <MASK>. Variants include weighted vs uniform subexpression sampling and masking hyperparameters (k=2 typical). Evaluation used beam search (width 8 for evaluation tasks, width 1024 for free-form conjecturing).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Formal mathematics reasoning tasks (Type Inference, Hard Type Inference, Predicting Assumptions, Predicting Equalities, Free-form Conjecturing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of formal-logic reasoning tasks derived from HOL Light theorems (validation split): (1) Type Inference: predict the type subexpression of variables/constants (and a 'Hard' variant masking other types); (2) Assumptions: predict left-hand side (assumptions) of top-level implications; (3) Equalities: predict one side of top-level equalities; (4) Free-form conjecturing: generate theorem statements given only a theorem prompt. Tasks require strict syntactic correctness, type-checking, provability in a HOL prover and often nontrivial logical generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Train seq2seq Transformer on a novel skip-tree objective that samples whole subtrees (subexpressions) to predict rather than arbitrary token subsequences; optionally mask additional subexpressions with a distinct <MASK> token to hide partial information; sample subexpressions uniformly or weighted by token length; generate multiple training examples per statement (n up to 100) to enlarge dataset; evaluate without task-specific fine-tuning to measure capabilities acquired by pretraining alone. Use beam search to produce candidate completions and verify/measure them with an automated theorem prover (DeepHOL) and by measuring usefulness as premises in reinforcement-learning theorem-proving runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative and task-level outcomes reported: skip-tree-trained models 'significantly outperform' skip-sequence-trained models on the downstream exact-match evaluation (exact-match of entire predicted statement using beam search width 8). For mechanical tasks (Type Inference and Hard Type Inference) the models achieve 'pretty high accuracy' (exact-match) even in the Hard variant. Skip-tree variants dominate skip-sequence variants across the evaluated reasoning benchmarks. In conjecturing experiments many generated statements are provable or provable-and-new according to DeepHOL (DeepHOL itself can prove about 58% of validation theorems, so reported provability rates are lower bounds). Usefulness as premises: from generated provable conjectures, the paper reports that in reinforcement-learning proof search experiments the new theorems were used as premises in proofs: assumptions task — 3445 of 3857 generated theorems were used at least once; equalities task — 979 of 3440; free-form conjectures — 49 of 130. The authors also give concrete high-usage examples (e.g., 'b = a + c ⇒ a = b − c' had 1728 usages). Exact numeric per-dataset percentage values from Table 2/3 are referenced in the paper but not provided in the text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limitations documented: (1) The dataset is small compared to typical natural-language corpora, requiring multiple sampled examples per source statement. (2) S-expression representation is verbose and can exceed model length limits. (3) The model sometimes produces trivially true or low-value conjectures (e.g., predicting False as an assumption or identity completions). (4) Predictions that differ from ground truth may still be true but DeepHOL's limited proof power provides only a lower bound on actual truth (DeepHOL proves ~58% of validation theorems). (5) The model can produce incorrectly typed expressions (especially on some assumption tasks) and struggles with selecting the precise constants in some multi-case scenarios. (6) Evaluations use exact-match of full statements — even single token errors invalidate a predicted statement, making token-level metrics less informative. (7) No parameter-count scaling study is reported (model-size effects not analyzed in the excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Direct comparison in the paper is to an identically architected Transformer trained on skip-sequence objectives (see baseline entry). The skip-tree objective outperforms skip-sequence on downstream logical reasoning exact-match metrics and yields outputs that more often parse and typecheck. Skip-sequence has similar single-token accuracy but produces outputs that frequently fail to parse/typecheck, tending to add surplus tokens or truncate predictions prematurely (likely due to the s-expression parentheses structure). The paper also compares skip-tree variants (weighted sampling vs uniform, masking ablation k=0, fewer samples per statement 'small') showing skip-tree variants outperform skip-sequence; weighted sampling yields more diverse targets.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations performed: (a) skip-tree (no <MASK>) — removes additional masked subexpressions to test utility of masking; (b) skip-tree (small) — fewer samples per statement (n=20 vs n=100) to test the effect of dataset size; (c) skip-tree (uniform) vs skip-tree (weighted) — sampling subexpressions uniformly vs weighted by token count (weighting reduces sampling of trivial leaf subexpressions and yields more diverse prediction targets); (d) skip-sequence (short/medium/long) — a baseline predicting contiguous token subsequences with different length constraints. Findings: skip-tree variants (both weighted and uniform) significantly outperform skip-sequence variants on logical reasoning evaluation tasks; weighted sampling produces more diverse, larger subexpressions and was beneficial; skip-sequence models have near-equal single-token accuracy but much lower rate of producing syntactically well-formed, typechecking predictions (they tend to err on expression boundaries). Detailed numeric ablation results are provided in the paper's tables (not fully reproduced in the excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Modeling for Formal Mathematics', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5015.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5015.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Skip-sequence Transformer (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer (encoder-decoder) trained with skip-sequence (token-span) objective</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline seq2seq Transformer trained to predict masked contiguous token subsequences (inspired by MASS/SpanBERT/T5 style objectives) rather than tree-structured subexpressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (encoder-decoder) trained with skip-sequence objective</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same Transformer architecture as used for skip-tree experiments but trained with a skip-sequence objective that samples contiguous token spans (variants: short, medium, long span lengths) to replace with <PREDICT> and predict. Token-length of masked spans may be revealed differently depending on variant; training uses the same dataset and preprocessing where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Same evaluation tasks as skip-tree: Type Inference, Hard Type Inference, Assumptions, Equalities, Free-form Conjecturing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict masked subsequence completions in HOL Light S-expression prompts and evaluated on the same downstream logical reasoning tasks without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Train Transformer to predict contiguous masked token spans (short/medium/long) sampled from token sequence rather than whole syntactic subtrees; compare resulting downstream reasoning competence to skip-tree variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Had near-similar or slightly lower single-token accuracy compared to the skip-tree model on validation data, but far worse exact-match performance on full predicted statements for the logical reasoning tasks; skip-sequence predictions rarely parse or typecheck and often include surplus tokens or premature truncations. Thus, skip-sequence underperforms skip-tree on strict logical reasoning exact-match metrics despite comparable token-level metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failings include inability to reliably predict correct expression boundaries in verbose s-expression syntax leading to unparsable or mistyped predictions; predictions often add surplus tokens at the end or truncate expressions; token-level measures mislead about true quality since even a single incorrect token invalidates the formal statement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared directly to skip-tree variants: skip-sequence is inferior for strict formal reasoning tasks despite comparable single-token next-token accuracy. The paper attributes the difference to skip-sequence ignoring tree structure and to the particular demands of s-expression syntax.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>The paper evaluated three skip-sequence variants by span length (short/medium/long) and measured that all skip-sequence variants underperform the skip-tree approaches on exact-match and parse/typecheck success; single-token accuracy can be similar but not indicative of usable formal outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Modeling for Formal Mathematics', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>HOList: An environment for machine learning of higher-order theorem proving <em>(Rating: 2)</em></li>
                <li>Deep learning for symbolic mathematics <em>(Rating: 2)</em></li>
                <li>Teaching temporal logics to neural networks <em>(Rating: 2)</em></li>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5015",
    "paper_id": "paper-219531812",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "Skip-tree Transformer",
            "name_full": "Transformer (encoder-decoder) trained with the skip-tree language modeling objective",
            "brief_description": "An encoder-decoder Transformer trained on formal HOL Light S-expression statements using a novel skip-tree self-supervised objective that masks and predicts entire subexpressions (&lt;PREDICT&gt;) and additional masked subexpressions (&lt;MASK&gt;) to encourage tree-structured completions and robustness to partial information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (encoder-decoder) trained with skip-tree objective",
            "model_description": "A small encoder-decoder Transformer language model trained on parsed HOL Light S-expressions (HOList core+complex). The model uses a seq2seq architecture with an encoder and decoder; training examples replace a sampled subexpression by &lt;PREDICT&gt; and (optionally) additional subexpressions by &lt;MASK&gt;. Variants include weighted vs uniform subexpression sampling and masking hyperparameters (k=2 typical). Evaluation used beam search (width 8 for evaluation tasks, width 1024 for free-form conjecturing).",
            "model_size": null,
            "logical_reasoning_task": "Formal mathematics reasoning tasks (Type Inference, Hard Type Inference, Predicting Assumptions, Predicting Equalities, Free-form Conjecturing)",
            "task_description": "A suite of formal-logic reasoning tasks derived from HOL Light theorems (validation split): (1) Type Inference: predict the type subexpression of variables/constants (and a 'Hard' variant masking other types); (2) Assumptions: predict left-hand side (assumptions) of top-level implications; (3) Equalities: predict one side of top-level equalities; (4) Free-form conjecturing: generate theorem statements given only a theorem prompt. Tasks require strict syntactic correctness, type-checking, provability in a HOL prover and often nontrivial logical generalization.",
            "method_or_approach": "Train seq2seq Transformer on a novel skip-tree objective that samples whole subtrees (subexpressions) to predict rather than arbitrary token subsequences; optionally mask additional subexpressions with a distinct &lt;MASK&gt; token to hide partial information; sample subexpressions uniformly or weighted by token length; generate multiple training examples per statement (n up to 100) to enlarge dataset; evaluate without task-specific fine-tuning to measure capabilities acquired by pretraining alone. Use beam search to produce candidate completions and verify/measure them with an automated theorem prover (DeepHOL) and by measuring usefulness as premises in reinforcement-learning theorem-proving runs.",
            "performance": "Qualitative and task-level outcomes reported: skip-tree-trained models 'significantly outperform' skip-sequence-trained models on the downstream exact-match evaluation (exact-match of entire predicted statement using beam search width 8). For mechanical tasks (Type Inference and Hard Type Inference) the models achieve 'pretty high accuracy' (exact-match) even in the Hard variant. Skip-tree variants dominate skip-sequence variants across the evaluated reasoning benchmarks. In conjecturing experiments many generated statements are provable or provable-and-new according to DeepHOL (DeepHOL itself can prove about 58% of validation theorems, so reported provability rates are lower bounds). Usefulness as premises: from generated provable conjectures, the paper reports that in reinforcement-learning proof search experiments the new theorems were used as premises in proofs: assumptions task — 3445 of 3857 generated theorems were used at least once; equalities task — 979 of 3440; free-form conjectures — 49 of 130. The authors also give concrete high-usage examples (e.g., 'b = a + c ⇒ a = b − c' had 1728 usages). Exact numeric per-dataset percentage values from Table 2/3 are referenced in the paper but not provided in the text excerpt.",
            "limitations_or_failure_cases": "Limitations documented: (1) The dataset is small compared to typical natural-language corpora, requiring multiple sampled examples per source statement. (2) S-expression representation is verbose and can exceed model length limits. (3) The model sometimes produces trivially true or low-value conjectures (e.g., predicting False as an assumption or identity completions). (4) Predictions that differ from ground truth may still be true but DeepHOL's limited proof power provides only a lower bound on actual truth (DeepHOL proves ~58% of validation theorems). (5) The model can produce incorrectly typed expressions (especially on some assumption tasks) and struggles with selecting the precise constants in some multi-case scenarios. (6) Evaluations use exact-match of full statements — even single token errors invalidate a predicted statement, making token-level metrics less informative. (7) No parameter-count scaling study is reported (model-size effects not analyzed in the excerpt).",
            "comparison": "Direct comparison in the paper is to an identically architected Transformer trained on skip-sequence objectives (see baseline entry). The skip-tree objective outperforms skip-sequence on downstream logical reasoning exact-match metrics and yields outputs that more often parse and typecheck. Skip-sequence has similar single-token accuracy but produces outputs that frequently fail to parse/typecheck, tending to add surplus tokens or truncate predictions prematurely (likely due to the s-expression parentheses structure). The paper also compares skip-tree variants (weighted sampling vs uniform, masking ablation k=0, fewer samples per statement 'small') showing skip-tree variants outperform skip-sequence; weighted sampling yields more diverse targets.",
            "ablation_or_analysis_results": "Ablations performed: (a) skip-tree (no &lt;MASK&gt;) — removes additional masked subexpressions to test utility of masking; (b) skip-tree (small) — fewer samples per statement (n=20 vs n=100) to test the effect of dataset size; (c) skip-tree (uniform) vs skip-tree (weighted) — sampling subexpressions uniformly vs weighted by token count (weighting reduces sampling of trivial leaf subexpressions and yields more diverse prediction targets); (d) skip-sequence (short/medium/long) — a baseline predicting contiguous token subsequences with different length constraints. Findings: skip-tree variants (both weighted and uniform) significantly outperform skip-sequence variants on logical reasoning evaluation tasks; weighted sampling produces more diverse, larger subexpressions and was beneficial; skip-sequence models have near-equal single-token accuracy but much lower rate of producing syntactically well-formed, typechecking predictions (they tend to err on expression boundaries). Detailed numeric ablation results are provided in the paper's tables (not fully reproduced in the excerpt).",
            "uuid": "e5015.0",
            "source_info": {
                "paper_title": "Language Modeling for Formal Mathematics",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Skip-sequence Transformer (baseline)",
            "name_full": "Transformer (encoder-decoder) trained with skip-sequence (token-span) objective",
            "brief_description": "A baseline seq2seq Transformer trained to predict masked contiguous token subsequences (inspired by MASS/SpanBERT/T5 style objectives) rather than tree-structured subexpressions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (encoder-decoder) trained with skip-sequence objective",
            "model_description": "Same Transformer architecture as used for skip-tree experiments but trained with a skip-sequence objective that samples contiguous token spans (variants: short, medium, long span lengths) to replace with &lt;PREDICT&gt; and predict. Token-length of masked spans may be revealed differently depending on variant; training uses the same dataset and preprocessing where possible.",
            "model_size": null,
            "logical_reasoning_task": "Same evaluation tasks as skip-tree: Type Inference, Hard Type Inference, Assumptions, Equalities, Free-form Conjecturing",
            "task_description": "Predict masked subsequence completions in HOL Light S-expression prompts and evaluated on the same downstream logical reasoning tasks without fine-tuning.",
            "method_or_approach": "Train Transformer to predict contiguous masked token spans (short/medium/long) sampled from token sequence rather than whole syntactic subtrees; compare resulting downstream reasoning competence to skip-tree variants.",
            "performance": "Had near-similar or slightly lower single-token accuracy compared to the skip-tree model on validation data, but far worse exact-match performance on full predicted statements for the logical reasoning tasks; skip-sequence predictions rarely parse or typecheck and often include surplus tokens or premature truncations. Thus, skip-sequence underperforms skip-tree on strict logical reasoning exact-match metrics despite comparable token-level metrics.",
            "limitations_or_failure_cases": "Failings include inability to reliably predict correct expression boundaries in verbose s-expression syntax leading to unparsable or mistyped predictions; predictions often add surplus tokens at the end or truncate expressions; token-level measures mislead about true quality since even a single incorrect token invalidates the formal statement.",
            "comparison": "Compared directly to skip-tree variants: skip-sequence is inferior for strict formal reasoning tasks despite comparable single-token next-token accuracy. The paper attributes the difference to skip-sequence ignoring tree structure and to the particular demands of s-expression syntax.",
            "ablation_or_analysis_results": "The paper evaluated three skip-sequence variants by span length (short/medium/long) and measured that all skip-sequence variants underperform the skip-tree approaches on exact-match and parse/typecheck success; single-token accuracy can be similar but not indicative of usable formal outputs.",
            "uuid": "e5015.1",
            "source_info": {
                "paper_title": "Language Modeling for Formal Mathematics",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "HOList: An environment for machine learning of higher-order theorem proving",
            "rating": 2,
            "sanitized_title": "holist_an_environment_for_machine_learning_of_higherorder_theorem_proving"
        },
        {
            "paper_title": "Deep learning for symbolic mathematics",
            "rating": 2,
            "sanitized_title": "deep_learning_for_symbolic_mathematics"
        },
        {
            "paper_title": "Teaching temporal logics to neural networks",
            "rating": 2,
            "sanitized_title": "teaching_temporal_logics_to_neural_networks"
        },
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "rating": 1,
            "sanitized_title": "exploring_the_limits_of_transfer_learning_with_a_unified_texttotext_transformer"
        }
    ],
    "cost": 0.01578725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Modeling for Formal Mathematics</p>
<p>Markus N Rabe mrabe@google.com 
Google Research 
Dennis Lee 
Google Research 
Kshitij Bansal 
Google Research 
Christian Szegedy szegedy@google.com 
Google Research 
Language Modeling for Formal Mathematics</p>
<p>We examine whether language modeling applied to mathematical formulas enables logical reasoning. We suggest several logical reasoning tasks that can be used to evaluate language models trained on formal mathematical statements, such as type inference, suggesting missing assumptions and completing equalities. To train language models for formal mathematics, we propose a novel skip-tree task, which outperforms standard language modeling tasks on our reasoning benchmarks. We also analyze the models' ability to formulate new conjectures by measuring how often the predictions that do not fit the ground truth or any training data turn out to be true and useful statements.IntroductionLanguage modeling using Transformers[Vaswani et al., 2017]has been hugely successful for applications like translation and text generation. Models like GPT-2 are able to generate impressive news articles and stories given just an abstract[Radford et al., 2018]. These models are usually first trained on a proxy task, such as predicting missing words in the case of BERT [Devlin et al., 2019], before fine tuning the models on more specific (downstream) tasks such as machine translation and question-answering. The proxy tasks are not reliant on labeled data, and thus can be trained on large corpora of unlabeled data. Even the models trained on the proxy tasks alone, have shown impressive language understanding[Brown et al., 2020].Prior work in deep learning for mathematics has focused on learning directly on logical reasoning tasks. In this work, we apply the paradigms of language modeling to formal mathematics and define proxy tasks on unlabeled mathematical expressions that allows us to use much more data. We start with the HOList dataset[Bansal et al., 2019], which spans a wide range of mathematical topics, including topology, multivariate calculus, real and complex analysis, geometric algebra, and measure theory, formalized in the HOL Light proof assistant[Harrison, 1996]. We consider a standard skipsequence task and a novel skip-tree task. Our skip-tree task is an instance of the skip-sequence task that respects the tree structure of expressions. We show that models trained on the skip-tree task results in models that significantly outperform those trained skip-sequence task when evaluated on various downstream tasks.Reasoning can refer to a wide range of abilities, and thus we measure the mathematical reasoning abilities of language models on a variety of tasks, including mechanical derivations, such as type inference, and also creative tasks, such as predicting under which assumptions a statement is true. In contrast to works in language modeling, we do not fine-tune the models to the evaluation (downstream) tasks, as we want to study what reasoning capabilities can be acquired just through language modeling proxy tasks.An advantage of formal language compared to natural language is that we can attempt to automatically evaluate statements. That is, even if the language models fail to predict the ground truth, the statements they predicted might still be true and useful. We evaluate these conjectures by attempting to prove them and checking if they are can be used in the context of other proofs.Preprint. Under review.</p>
<p>Our contributions are two-fold:</p>
<ol>
<li>
<p>We introduce several evaluation tasks that test logical reasoning abilities.</p>
</li>
<li>
<p>We introduce a new skip-tree language modeling task that outperforms skip-sequence approaches in our evaluation on the logical reasoning tasks.</p>
</li>
</ol>
<p>The remainder of this paper is structured as follows: First, we review related work on language modeling and deep learning for mathematics in Section 2. Then, in Section 3 we discuss the source corpus of formal mathematical statements from which we generate our training data. In Section 4, we present our novel language modeling task for formal languages, as well as several variations that we used in our ablation studies. We present the evaluation tasks in Section 5, present our experimental findings in Section 6, and conclude in Section 7.</p>
<p>Related work</p>
<p>Recently, we have seen a series of rapid improvements in language modeling. Many of the improvements result from better pretraining tasks [Devlin et al., 2019, Zhang et al., 2019, Song et al., 2019, Dong et al., 2019, Raffel et al., 2019, Conneau and Lample, 2019. BERT [Devlin et al., 2019] is a pretraining task for Transformers [Vaswani et al., 2017], which masks out a certain fraction of the input tokens that the model has to predict. UniLM uses multiple pretraining tasks at once [Dong et al., 2019]. One of them is a sequence-to-sequence task, which is to predict the next sentence from the previous sentence. MASS considers a generalized sequence-to-sequence pretraining task [Song et al., 2019], which is to predict a masked out subsequence of the input. SpanBERT additionally considers a span boundary objective, which is to predict the masked out subsequence only from the tokens adjacent to the missing subsequence [Joshi et al., 2020]. However, both MASS and SpanBERT reveal the length of the sequence to predict as they replace it by a number of mask tokens equal to the length of the sequence.</p>
<p>T5 introduced a generalization of sequence-to-sequence pretraining tasks that is crucial to our work [Raffel et al., 2019]. They replace the subsequence to be predicted by a single token (not a number of mask tokens equal to the length of the subsequence, as in MASS). Further, T5 allows multiple subsequences to be masked out and predicted. [Zhang et al., 2019] additionally exploit the sentence structure of natural language. They suggest the pretraining task Pegasus, which masks out entire sentences of a given text, and additionally masks out randomly selected tokens in the remaining text (or alternatively replace them by other tokens). In a similar way Pegasus' exploitation of the sentence structure of natural language, our skip-tree task exploits the tree structure of formal expressions. [Zhang et al., 2019] also suggest sampling the sentences to be masked with the help of ROUGE1-F1 [Lin, 2004].</p>
<p>We work with the HOList dataset by Bansal et al. [2019]. There are other datasets which might be suitable for our approach as well, including proofs extracted from HOL4 [Gauthier et al., 2017], and from Coq [Huang et al., 2019, Yang and Deng, 2019, Sanchez-Stern et al., 2019.</p>
<p>Lample and Charton [2020] use a Transformer model for symbolic integration. They train their model directly on the reasoning task they want to learn, and their approach requires that the inverse of the prediction task can be computed effectively with classical algorithms. In contrast, we train language models on a proxy task and evaluate them on several logical reasoning tasks that are substantially different from the training task. Also, our dataset spans a much wider range of mathematical theories. We imagine that the language modeling approach explored in this paper, could be used as a pretraining task for symbolic integration.</p>
<p>Finkbeiner et al.</p>
<p>[2020] explore the generalization properties of Transformers predicting the solutions to formulas in linear-time temporal logic. Unlike the language modeling approach followed in our work, their training regime requires a data generator that can solve formulas, which is currently not feasible for higher-order logic. Transformer models for program understanding have focused on providing inductive biases in the architecture [Shiv andQuirk, 2019, Hellendoorn et al., 2020], whereas this work suggests to use a modified language modeling proxy task. Perhaps, these approaches could be combined to improve performance on tree-structured data.</p>
<p>Theorem database Proofs Validation Testing Training Figure 1: We use the theorems and proofs of the training split, marked in green, for training. For our evaluation tasks, we only use the theorems of the validation set, marked in red, to ensure that the model has never seen the statements from which the evaluation tasks are derived.</p>
<p>Dataset</p>
<p>We start from the HOList dataset introduced by Bansal et al. [2019]. The complete dataset includes 29465 theorems and their proofs. We here consider only the "core" and "complex" datasets which comprise 18943 theorems, 637 definitions and 603,950 proof steps. These proof steps were extracted from the human proof logs. The theorems and proofs were written (by humans) using the HOL Light proof assistant, and span various areas of mathematics such as set theory, arithmetic, linear algebra, topology, and multivariate complex analysis. The proofs contain a lot of intermediate goals which are the result of applying "tactics" on previous proof goals. For example, one of the tactics is to rewrite the current proof goal with a set of equations selected from the theorem database.</p>
<p>From this dataset we extract all theorem statements as well as all intermediate proof goals. We use S-expressions to represent all statements. For example, (v bool x) represents a boolean variable named x, and (a (v (fun (bool) 
(bool)) f) (v bool x)
represents the function application f (x) where f is a function from bool to bool. The S-expression syntax is thus very verbose, which can cause some expressions to not fit into the size constraints of our Transformer model. We use the same split into training/validation/testing data as defined in HOList. The split is defined on the theorems, and the entire proof of each theorem is assigned to the same split as the theorem. This means that we have used the proof of 11,655 theorems in the training split of the core and complex libraries. This avoids partially revealing the proofs of theorems in the validation and test sets during training. We derive all training data from the theorems and proofs in the training set, and use only the theorems (not the proofs) for the evaluation tasks. This addresses the possibility that some proof steps for training theorems and for validation theorems might be shared. In Figure 1 we depict our choice of training and evaluation data.</p>
<p>Skip-tree Training</p>
<p>In this section we define the skip-tree training task. We parse a given mathematical statement into a tree of subexpressions, and replace one of the subexpressions by a <PREDICT> token. The task is to predict the subexpression replaced by <PREDICT>. See Figure 2 for an example.</p>
<p>For training, the trees are converted back to a sequence of tokens; the target sequence is extended by a <START> token in the front and an <END> token in the back. We exclude training examples where the output sequence is longer than the length of the decoder (512 tokens), and we cut off input sequences when they exceed the length of the encoder (1024 tokens).</p>
<p>Additional masked subexpressions. In addition to the subexpression to be masked out by <PREDICT>, we select k = 2 subexpressions to be masked out by a different mask token <MASK>. In contrast to the <PREDICT> token, we replace all occurrences of these subexpressions by the <MASK> token. Note that it can happen that the subexpressions we want to replace by the <MASK> tokens overlap with each other or with the subexpression replaced by the <PREDICT> token. In this case, we give the highest preference to the <PREDICT> token, and then in decreasing order of size for the expression to be replaced by the <MASK> tokens.  Figure 2: The skip-tree training task for the example of the equality operator on boolean constants (original formula). In this example we assume that a part of the type was sampled to be the subexpression to be predicted, and that subexpression c was sampled to be masked out additionally. Note the input to the decoder is shifted to the right, such that the next token prediction task yields the target sequence.</p>
<p>The subexpressions masked by <MASK> do not have to be predicted. They are only hidden to make the task harder and to make the model tolerant to having partial information. A beneficial side effect of replacing some expressions by a <MASK> token is that the input sequences get substantially shorter and more mathematical expressions fit in the size constraints of the Transformer architecture.</p>
<p>Distributions of subexpressions. Sampling subexpressions uniformly at random results in very short sequences to be predicted: since our trees are mostly ternary, two thirds of the subexpressions are leaves. Besides picking subexpressions uniformly at random, we thus experiment with weighting the subexpressions by the number of tokens they contain. We refer to these variants as "uniform" and "weighted". This results in a much more diverse set of expressions to be sampled.</p>
<p>Multiple samples per statement. Since we started with a data source that is small compared datasets in natural language modeling, we use each mathematical statement from the training set to generate n = 100 training examples. Our initial data consists of about 360K intermediate statements from the proofs of 10K statements in the training split of the core and complex library of the HOList corpus. To avoid duplicates, we sample the subexpressions that are replaced by a <PREDICT> token for each original formula without replacement.</p>
<p>Ablations</p>
<p>To verify the design choices of the skip-tree training task we generated multiple variants of the training task and trained a model on each of them.</p>
<p>No mask tokens.</p>
<p>To answer the question of whether it helps to mask out subexpressions besides the one to predict, we generated a dataset with k = 0, called "skip-tree (no <MASK>)".</p>
<p>Fewer samples per statement. Instead of sampling many training examples from each formula, we could train on a fewer training examples for more epochs. We generated a smaller version with n = 20 of the skip-tree training data, which we call "skip-tree (small)".</p>
<p>Skip-sequence. MASS [Song et al., 2019], SpanBERT [Joshi et al., 2020], and T5 [Raffel et al., 2019] pretrain their sequence-to-sequence natural language models by predicting subsequences of the tokens. The skip-tree task is similar, but exploits our ability to parse the formulas as trees. To examine if this makes a difference, we consider a "skip-sequence" task that samples subsequences of the list of tokens instead of sampling subexpressions. We generated three datasets for the skip-sequence task, where we sample subsequences of different lengths (short/medium/long). For the task "skip-sequence (long)", we pick two positions in the token sequence at uniformly at random and select the sequence that is between them. For the tasks "skip-sequence (medium)" and "skip-sequence (short)", we limit their distance to 100 and 50 tokens, respectively. </p>
<p>Evaluation Tasks</p>
<p>In this section we suggest several logical reasoning tasks on which our language models can be evaluated. These tasks require different levels of logical reasoning, ranging from mostly mechanical application of typing rules to conjecturing under which assumptions a statement might hold.</p>
<p>We intentionally define them to be out-of-distribution compared to the training data. Not only do we generate the examples in a slightly different way, we also generate them from the validation set of the theorem database. That is, the model has never seen the source data, nor has it seen the proofs of these theorems. This makes the tasks more challenging, and also ensures that we force the models to go beyond memorization. To give the interested reader a better impression of the evaluation tasks, we provide a list of randomly selected examples in Appendix D.</p>
<p>Type Inference. We generate type inference problems similar to how we generated the skip-tree training data, which we described in Section 4. However, we restrict the sampling of subexpressions to subtrees that represent types of variables or constants (i.e. not fragments of other types).</p>
<p>We generated two variants of the type inference task: In the task we call "Type Inference," we replace only the selected type by the <PREDICT> token and do not mask out anything else. In the second variant we name "Hard Type Inference," we additionally replace all other types by the <MASK> token. The two tasks loosely correspond to the deriving the first and the last type during type inference.</p>
<p>For example, consider x = x, which in the s-expression syntax is represented as follows:
(a (a (c (fun (A) (fun (A) (bool))) =) (v A x)) (v A x))
Each subexpression here is either a leaf or a triple. The first element of these triples indicates their kind: a indicates function applications, c indicates constants (i.e. symbols that have been defined in the formal system), v indicates a variable, and finally fun indicates a function type. The equality operator "=" is represented by (c (fun (A) (fun (A) (bool))) =), which indicates that it is a constant that has a function type taking two arguments of arbitrary type A and returns a bool. Since functions are typically curried in this representation, we have two function applications, both times with the variable x as the argument.</p>
<p>An example for the "Type Inference" evaluation task would be:
(a (a (c <PREDICT> =) (v A x)) (v A x))
The type of the equality operator is still uniquely defined, as we know what the equality is applied to (two arguments of type A) and because top-level application always has to return a boolean value. In this example the type could have been computed by a classical type inference algorithm.</p>
<p>For the "Hard Type Inference" evaluation task, the input would look as follows:
(a (a (c <PREDICT> =) (v <MASK> x)) (v <MASK> x))
Now, the type inference task is highly ambiguous. In fact, in this case, variable x could have any type, and the equality operator would have to adapt to the type of its arguments accordingly. Further, note that the hard type inference task masks out many more subtrees compared to the training data.</p>
<p>Assumptions. This evaluation task is to predict missing assumptions for theorems in the validation set. We extract these tasks by searching for "top-level implications" and replacing their left operand by the <PREDICT> token. We define an implication operator "⇒" in an expression to be a top-level implication if it is either the top-most operator of the expression, or occurs only under quantifiers, conjunctions, disjunctions, or on the right side of other top-level implications. This definition helps us to avoid picking assumptions in negated parts of formulas.</p>
<p>Note that we can have multiple top-level implications per validation theorem. Consider the abstracted example (a ⇒ b) ∧ (c ⇒ (d ⇒ e)). In this case, a, c, and d are all considered to be assumptions of top-level implications.</p>
<p>An example from the theorem database is x = y ⇒ a + x = a + y, for which the task is to predict x = y given <PREDICT> ⇒ a + x = a + y. (We omit the presentation of this example as an s-expression for the sake of readability.) At first, the expression to predict in this case may seem unique, but there are actually many ways to complete the task into a true statement; e.g. y = x or x = 0 ∧ y = 0. Still, most humans would likely guess x = y as it is simple and general, and because x occurs before y in the alphabet. To make a correct prediction, our language models thus have to understand which statements are more general and also know about naming conventions.</p>
<p>Below we give some examples of this reasoning task that we selected for their simplicity. (For a representative selection, see Appendix D.) While it is often easy to "see" that a given solution to such a task is correct, it can be non-trivial to come up with a solution in the first place. We encourage the reader to make their own predictions before looking up the ground truth in Appendix C:
• <PREDICT> ⇒ (x ⇔ ( b ∨ x1) ∧ (b ∨ x0)) • <PREDICT> ⇒ (g \ {s}) = g • <PREDICT> ⇒ (x1/y1 = x2/y2 ⇔ x1 * y2 = x2 * y1)
Equalities. Similar to the task of predicting missing assumptions, we ask to predict one side of a top-level equality in this task. Again, we define top-level equalities to be any equality that occurs as the top-level operator of the formula or occurs inside quantifiers, conjunctions, disjunctions, or on the right side of implications. For example, from the theorem ∀x. Again, we present some simple example tasks (in mathematical notation for the sake of readability) and provide the ground truth as well as the model predictions in Appendix C:</p>
<p>• ∀x, n ∈ N : (x n = 1) = <PREDICT> • ∀m, n : n ≤ m ⇒ m − n + n = <PREDICT> • ∀l, m : <PREDICT> = APPEND(REVERSE(m), REVERSE(l))</p>
<p>Results and Discussion</p>
<p>We trained a Transformer with the hyperparameters specified in the appendix on the skip-tree dataset and each of the ablations for 1M steps with a batch size of 256.</p>
<p>In language modeling for natural language one of the key metrics is how often the next token in the ground truth is correctly predicted. This is not an ideal measurement for formal mathematics as even a single incorrect token can invalidate the entire statement. Also, the s-expression representation is relatively lengthy and barely human-readable, so a token-level measurement does not allow us to compare our models to the natural language models in any case. Therefore, we focus on exact matches of the entire predicted statement.</p>
<p>In Table 2 we present how well the Transformer model, trained on different datasets, can predict the ground truth sequences. We can observe that for type inference, i.e. the more mechanical reasoning tasks, the models achieve a pretty high accuracy -even in the Hard Type Inference case where the expression was stripped of all types. We see that the skip-tree task and its ablations clearly dominate the skip-sequence language modeling task.</p>
<p>A closer inspection of the skip-sequence model shows that its single-token accuracy is almost as high as the single-token accuracy of the skip-tree model, and higher than the single-token accuracy of  Table 2: Success rate of predicting the ground truth in a beam search of width 8 after training a model on various datasets. Grayed out values indicate experiments where the training data did not include the <MASK> token but the evaluation data did.</p>
<p>the skip-tree (uniform) model (all measured on the validation data of the different training tasks). However, its predictions rarely parse or typecheck. On manual inspection of the predictions, it seems that the skip-sequence models consistently add surplus tokens at the end, or stop expressions too early; they appear to be unable to correctly identify the end of the expression to predict. The problem may be amplified by the s-expression syntax, which requires counting parentheses to some extent.</p>
<p>Conjecturing</p>
<p>In the experiments above, we measured how often the models predicted the ground truth in the evaluation tasks. We now change our point of view, and examine whether the models can be used to generate new conjectures. We define conjectures as mathematical statements that differ from the ground truth and any expression the model has seen during training. Additionally, a meaningful conjecture should be syntactically correct, typecheck, be provable, and be useful in the context of other proofs.</p>
<p>Since the training data is derived exclusively from true statements (i.e. human proof steps), the language models are incentivized to complete partial statements in a way that makes them true. Presented with one of the evaluation tasks, to predict missing assumptions or to predict the missing side of an equation, the models may thus complete these statements in multiple ways that make them true. The predictions that do not match the ground truth may still be true and useful statements. In the following we describe experiments that help us estimate how often this is the case.</p>
<p>Free-form conjecturing. In addition to the "assumptions" and the "equalities" evaluation tasks, we consider a third task for producing conjectures. In this task, which we call "free-form conjecturing", we query the model with a single prompt: (<theorem> <PREDICT>). This helps us to analyze what the language models produce when given no context. The <theorem> tag indicates only that the statement should be a theorem, and not an intermediate proof step, which would start with the <goal> tag. For free-form conjecturing we want to produce a variety of different predictions, and thus use a beam search with high beam width of 1024. We did not include the free-form conjecturing task in Table 2, as there is no ground truth to match against.</p>
<p>How often are predictions true and new? For this measurement, we replace the <PREDICT> token with the predicted sequence and attempt to prove the resulting statement in the DeepHOL theorem prover [Bansal et al., 2019]. Note that this can only give us a lower bound to the number of true statements, because of the limitations of the prover: The version of the DeepHOL theorem prover used here can prove around 58% of the validation theorems. So we expect the estimates here to be considerably below the number of actually true statements.</p>
<p>In Table 3 we report two numbers for each evaluation task: The first number is the percentage of generated statements known to be provable, including exact matches, statements from the training set, and statements provable with DeepHOL. The second number is the percentage of generated statements that are provable and new -excluding exact matches with the ground truth and statements from the training set. The denominator for both numbers is the same: the set of all predictions from the beam searches in Table 2.</p>
<p>We believe that these measurements show a significant bias towards true statements. While in some tasks, less than half of the statements were provable, there are simply many more ways to write a false statement than a true statement.  Table 3: Percentage of "provable statements"/"provable new statements". The type inference tasks are not included as we are only interested in the predictions that do not match the ground truth. For the type inference tasks, these statements are either semantically equivalent to existing statements or statements that do not type check.</p>
<p>Are the conjectures useful? For some evaluation tasks, the models could "cheat" on the truth metric by making the statements trivially true. For example, the models can predict False as an assumption, or complete the missing part of an equation by making it an identity (e.g. complete x = <PREDICT> by predicting x). In fact, manual inspection revealed several such cases.</p>
<p>To make this measurable, we added the provable statements to the theorem database, and ran the reinforcement learning experiments of the DeepHOL theorem prover [Bansal et al., 2019] to measure how many of the statements were used as premises. In this experiment we also make sure that the new theorems cannot be used in the proofs of their premises. In a "pruning" step DeepHOL minimizes proofs by removing each individual premise in a proof and checking if the proof still holds. Only the premises that survive this step are classified as useful. While this measurement is a relatively low bar, it filters out statements that have no effect in any proof.</p>
<p>We ran three reinforcement learning experiments, one for each of the evaluation tasks. We then measured how many of the theorems generated by each task are used as a premise in one of the over 200,000 proofs found for each of the experiments. For the assumptions task, 3445 of the 3857 theorems were used at least once. For the equalities task and the free-form conjectures it was 979 out of 3440 and 49 out of 130, respectively. We provide usage histograms in Appendix B.</p>
<p>While some of the most frequently used conjectures turned out to be alpha-equivalent variations of existing theorems in the theorem database, we found some interesting examples among the most used conjectures:</p>
<p>• Assumptions task, 1728 usages: b = a + c ⇒ a = b − c. Humans have used this theorem over vector arithmetic in many proofs. However, this theorem has always been defined as a local lemma and thus did not made it into the theorem database. This conjecture apparently filled a gap in the theorem database.</p>
<p>• Free-form conjecturing task, 15 usages: COUNTABLE({s(n) | n ∈ N}). In contrast to the previous example, there are no occurrences of this statement (or an equivalent statement) in the theorem database or any human proof, not even as a local lemma.</p>
<p>These results suggest that language models show a limited ability to produce new, useful conjectures, even without fine tuning or specialized training. However, overall, the new conjectures appear to be mostly variations of existing theorems. This falls in line with current expectations of Transformer models. To effectively produce conjectures that are useful in a specific context, we expect that a more targeted training approach is needed.</p>
<p>Conclusion</p>
<p>In this work, we applied the paradigms of language modeling to formal mathematics. We introduced a novel self-supervised training task for formal mathematics that clearly outperforms language modeling tasks used for natural language. We suggested several evaluation tasks and metrics for measuring mathematical reasoning capabilities of language models for formal mathematics. Our experiments demonstrate that language models are already surprisingly capable at a variety of reasoning tasks that they were not trained for directly. We also explored the ability of language models to produce new conjectures by measuring how many of the new predictions are provable and useful for other proofs. </p>
<p>A Hyperparameters</p>
<p>We trained the Transformers with these hyperparameters:</p>
<p>•  </p>
<p>C A Close Look at Simple Example Tasks</p>
<p>Assumptions. In Section 5 we presented the following three examples of the task to predict missing assumptions. For the sake of readability we here discuss only the pretty printed versions. For examples in s-expression syntax, please visit Appendix D.
• <PREDICT> ⇒ (x ⇔ ( b ∨ x1) ∧ (b ∨ x0)) • <PREDICT> ⇒ (g \ {s}) = g • <PREDICT> ⇒ (x1/y1 = x2/y2 ⇔ x1 * y2 = x2 * y1)
The ground truth answers are as follows:
• ((b ⇔ False) ⇒ (x ⇔ x0)) ∧ (b ⇔ True) ⇒ (x ⇔ x1)
• ¬(s ∈ g)</p>
<p>• 0 &lt; y1 ∧ 0 &lt; y2, note that 0 = y1 ∧ 0 = y2 would be a more general assumption.</p>
<p>For the first and the third task, the language model "skip-tree (weighted)" makes a correct prediction in the top 3 candidates in a beam search of width 8. For the seconds task, the language model mostly produces incorrectly typed expressions: it appears to think that s is a set of the same type as g.</p>
<p>Equalities. We presented these examples for the equality evaluation task:</p>
<p>• ∀x, n ∈ N : (x n = 1) = <PREDICT> • ∀m, n : n ≤ m ⇒ m − n + n = <PREDICT> • ∀l, m : <PREDICT> = APPEND(REVERSE(m), REVERSE(l))</p>
<p>The ground truth for the tasks is:
• x = 1 ∨ n = 0 • m • REVERSE(APPEND(l, m))
Examples two and three are predicted correctly in a beam search with beam width 8. For the first example, the model almost gets it correct in two of the 8 attempts: x = 1 ∨ n = 1, and x = 0 ∨ n = 1. We find it surprising that the model apparently understands that there are two cases to consider, but that the exact combination of constants (1 and 0) is a challenge.</p>
<p>D Randomly Selected Example Tasks</p>
<p>In the following, we provide a list of 5 examples for each of the evaluation tasks, sampled uniformly at random.</p>
<p>Type Inference.</p>
<p>• (<theorem> (a (c <PREDICT> !) (l (v (fun (cart (real) ?1) (bool)) t) (a (c (fun (fun (fun (cart (real) ?1) (bool)) (bool)) (bool)) !) (l (v (fun (cart (real) ?1) (bool)) u) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (c (fun (fun (cart (real) ?1) (bool)) (bool)) !) (l (v (cart (real) ?1) b) (a (a (c (fun (bool) (fun (bool) (bool))) ∨) (a (c (fun (fun (cart (real) ?1) (bool)) (bool)) ?) (l (v (cart (real) ?1) w) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (a (c (fun (cart (real) ?1) (fun (fun (cart (real) ?1) (bool)) (bool))) IN) (v (cart (real) ?1) w)) (v (fun (cart (real) ?1) (bool)) t))) (a (a (c (fun (cart (real) ?1) (fun (fun (cart (real) ?1) (bool)) (bool))) IN) (v (cart (real) ?1) w)) (a (c (fun (prod (cart (real) ?1) (real)) (fun (cart (real) ?1) (bool))) ball) (a (a (c (fun (cart (real) ?1) (fun (real) (prod (cart (real) ?1) (real)))) ,) (v (cart (real) ?1) b)) (a (c (fun (num) (real)) real_of_num) (a (c (fun (num) (num)) NUMERAL) (a (c (fun (num) (num)) BIT1) (c (num) _0))))))))))) (a (c (fun (fun (cart (real) ?1) (bool)) (bool)) ?) (l (v (cart (real) ?1) w) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (a (c (fun (cart (real) ?1) (fun (fun (cart (real) ?1) (bool)) (bool))) IN) (v (cart (real) ?1) w)) (v (fun (cart (real) ?1) (bool)) u))) (a (a (c (fun (cart (real) ?1) (fun (fun (cart (real) ?1) (bool)) (bool))) IN) (v (cart (real) ?1) w)) (a (c (fun (prod (cart (real) ?1) (real)) (fun (cart (real) ?1) (bool))) ball) (a (a (c (fun (cart (real) ?1) (fun (real) (prod (cart (real) ?1) (real)))) ,) (v (cart (real) ?1) b)) (a (c (fun (num) (real)) real_of_num) (a (c (fun (num) (num)) NUMERAL) (a (c (fun (num) (num)) BIT1) (c (num) _0)))))))))))))) (a (c (fun (fun ?0 (bool)) (bool)) !) (l (v ?0 x) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (a (c (fun ?0 (fun (fun ?0 (bool)) (bool))) IN) (v ?0 x)) (v (fun ?0 (bool)) d))) (a (c (fun (bool) (bool)) ∼) (a (a (c (fun (cart (real) ?1) (fun (fun (cart (real) ?1) (bool)) (bool))) IN) (a (v (fun ?0 (cart (real) ?1)) g) (v ?0 x))) (a (a (c (fun (fun (cart (real) ?1) (bool)) (fun (fun (cart (real) ?1) (bool)) (fun (cart (real) ?1) (bool)))) UNION) (v (fun (cart (real) ?1) (bool)) t)) (v (fun (cart (real) ?1) (bool)) u))))))))) (a (c (fun (bool) (bool)) ∼) (a (c (fun (fun (cart (real) ?1) (bool)) (bool)) ?) (l</p>
<p>x = (x = True) we extract two evaluation examples: ∀x. <PREDICT> = (x = True) and ∀x. x = <PREDICT>.</p>
<p>Figure 3 :
3Histograms of premise usage of the conjectures generated through the assumptions task (left), the equality task (middle), and through free-form conjecturing (right). X-axes are the new theorems, sorted by number of usages. Y-axes indicate the number of usages on a log scale.</p>
<p>Table 1 :
1Basic statistics of the training splits of the data sets. Number of tokens in the training set measured before padding.Dataset </p>
<h1>examples # tokens (input/output) avg length (input/output)</h1>
<p>Skip-tree (weighted) 
25.8M 
17.4B/1.6B 
675/61 
Skip-tree (uniform) 
25.7M 
18.8B/316M 
732/12 
Skip-tree (small) 
5.2M 
3.5B/521M 
673/100 
Skip-tree (no <MASK>) 
25.8M 
19.4B/1.6B 
750/61 
Skip-sequence (long) 
19.2M 
11.9B/2.8B 
620/146 
Skip-sequence (medium) 
26.0M 
19.4B/884M 
744/34 
Skip-sequence (short) 
26.0M 
19.6B/479M 
752/18 </p>
<p>Guillaume Lample and François Charton. Deep learning for symbolic mathematics. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=Ske31kBtPr. Bernd Finkbeiner, Christopher Hahn, Markus N. Rabe, and Frederik Schmitt. Teaching temporal logics to neural networks. CoRR, abs/2003.04218, 2020. URL https://arxiv.org/abs/2003. 04218. Vighnesh Leonardo Shiv and Chris Quirk. Novel positional encodings to enable tree-based transformers. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 12058-12068, 2019. URL http://papers.nips.cc/paper/ 9376-novel-positional-encodings-to-enable-tree-based-transformers. Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global relational models of source code. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https: //openreview.net/forum?id=B1lnbRNtwr.Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Hanna M. 
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Ro-
man Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, 
Vancouver, BC, Canada, pages 7057-7067, 2019. URL http://papers.nips.cc/paper/ 
8928-cross-lingual-language-model-pretraining. </p>
<p>Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: 
Improving pre-training by representing and predicting spans. Transactions of the Association for 
Computational Linguistics, 8:64-77, 2020. doi: 10.1162/tacl_a_00300. URL https://doi. 
org/10.1162/tacl_a_00300. </p>
<p>Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-
tion Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational 
Linguistics. URL https://www.aclweb.org/anthology/W04-1013. </p>
<p>Thibault Gauthier, Cezary Kaliszyk, and Josef Urban. TacticToe: Learning to reason with HOL4 
tactics. In Thomas Eiter and David Sands, editors, LPAR-21, 21st International Conference 
on Logic for Programming, Artificial Intelligence and Reasoning, Maun, Botswana, May 7-
12, 2017, volume 46 of EPiC Series in Computing, pages 125-143. EasyChair, 2017. URL 
https://easychair.org/publications/volume/LPAR-21. </p>
<p>Daniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever. GamePad: A learning environment 
for theorem proving. In 7th International Conference on Learning Representations, ICLR 2019, 
New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview. 
net/forum?id=r1xwKoR9Y7. </p>
<p>Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants. In 
Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International 
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, 
volume 97 of Proceedings of Machine Learning Research, pages 6984-6994. PMLR, 2019. URL 
http://proceedings.mlr.press/v97/yang19a/yang19a.pdf. </p>
<p>Alex Sanchez-Stern, Yousef Alhessi, Lawrence Saul, and Sorin Lerner. Generating correctness 
proofs with neural networks. CoRR, abs/1907.07794, 2019. URL http://arxiv.org/abs/ 
1907.07794. </p>
<p>vocabulary size: 1200 • number of hidden layers in encoder: 2 • number of hidden layers in decoder: 4 B Usage Statistics of Conjectures• embedding size: 128 </p>
<p>• attention dropout: 0.1 </p>
<p>• nonlinearity: gelu </p>
<p>• hidden layer dropout: 0.1 </p>
<p>• hidden layer size: 512 </p>
<p>• initializer range: 0.02 </p>
<p>• intermediate size: 768 </p>
<p>• number of attention heads: 8 </p>
<p>(v (cart(real)?1) b) (a (a (c (fun (fun (cart(real)?1)(bool)) (fun (fun (cart(real)?1)(bool)) (bool))) SUBSET) (a (c (fun (prod (cart(real)?1)(real)) (fun (cart(real)?1) (bool))) ball) (a (a (c (fun (cart(real)?1) (fun(real)(prod (cart(real)?1) (real)))) ,) (v (cart(real)?1) b)) (a (c (fun(num)(real)) real_of_num) (a (c (fun (num) (num)) NUMERAL) (a (c (fun (num) (num)) BIT1) (c (num) _0))))))) (a (a (c (fun (fun ?0 (cart(real)?1)) (fun (fun ?0 (bool)) (fun (cart(real)?1) (bool)))) IMAGE) (v (fun ?0 (cart(real)?1)) g)) (v (fun ?0 (bool)) d)))))))))))) Ground truth: <START> (fun (fun (fun (cart(real)?1)(bool)(a (a (a (c (fun (fun (real) (real)) (fun (real) (fun (net (real)) (bool)))) has_real_derivative) (c (fun (real) (real)) atn)) (a (c (fun (real) (real)) real_inv) (a (a (c (fun (real) (fun (real) (real))) real_add) (a (c (fun (num) (real)) real_of_num) (a (c (fun (num) (num)) NUMERAL) (a (c (fun (num) (num)) BIT1) (c (num) _0))))) (a (a (c (fun (real) (fun (num) (real))) real_pow) (v <PREDICT> x)) (a (c (fun (num) (num)) NUMERAL) (a (c (fun (num) (num)) BIT0) (a (c (fun (num) (num)) BIT1) (c (num) _0)))))))) (a (c (fun (real) (net (real))) atreal) (v (real) x)))))) Ground truth: <START> (real) <END> • (<theorem> (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (bool))) =) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (fun ?0 (bool)))) INTER) (v (fun ?0 (bool)) s)) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (fun ?0 (bool)))) UNION) (v (fun ?0 (bool)) t)) (v (fun ?0 (bool)) u)))) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (fun ?0 (bool)))) UNION) (a (a (c <PREDICT> INTER) (v (fun ?0 (bool)) s)) (v (fun ?0 (bool)) t))) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (fun ?0 (bool)))) INTER) (v (fun ?0 (bool)) s)) (v (fun ?0 (bool)) u))))) Ground truth: <START> (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (fun ?0 (bool)))) <END> • (<theorem> (a (a (c (fun (real) (fun (real) (bool))) =) (a (c (fun (cart(real)?0) (real)) infnorm) (a (c (fun (num) (cart(real)?0)) vec) (a (c (fun (num) (num)) NUMERAL) (c (num) _0))))) (a (c (fun (num) (real)) real_of_num) (a (c (fun (num) (num)) NUMERAL) (c <PREDICT> _0))))) Ground truth: <START> (num) <END> Hard Type Inference.• (<theorem> (a (c <MASK> !) (l (v <MASK> s) (a (a (c <MASK> =) (a (c <MASK> INTERS) (v <MASK> s))) (a (a (c <PREDICT> DIFF) (c <MASK> UNIV)) (a (c <MASK> UNIONS) (a (c <MASK> GSPEC) (l (v <MASK> GEN%PVAR%0) (a (c <MASK> ?) (l (v <MASK> t) (a (a (a (c <MASK>
Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USAAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5998-6008, 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Blog. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. In OpenAI Blog, 2018. URL https://d4mucfpksywv.cloudfront.net/better-language-models/ language_models_are_unsupervised_multitask_learners.pdf.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, 2019.</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https: //arxiv.org/abs/2005.14165.</p>
<p>HOList: An environment for machine learning of higher-order theorem proving. Kshitij Bansal, M Sarah, Markus N Loos, Christian Rabe, Stewart Szegedy, Wilcox, PMLRProceedings of the 36th International Conference on Machine Learning, ICML 2019. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97Kshitij Bansal, Sarah M Loos, Markus N Rabe, Christian Szegedy, and Stewart Wilcox. HOList: An environment for machine learning of higher-order theorem proving. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 454-463. PMLR, 2019. URL http://proceedings.mlr. press/v97/bansal19a/bansal19a.pdf.</p>
<p>A tutorial introduction. John Harrison, Light, 10.1007/BFb0031795Formal Methods in Computer-Aided Design, First International Conference, FMCAD '96. Mandayam K. Srivas and Albert John CamilleriPalo Alto, California, USASpringer1166John Harrison. HOL Light: A tutorial introduction. In Mandayam K. Srivas and Albert John Camilleri, editors, Formal Methods in Computer-Aided Design, First International Conference, FMCAD '96, Palo Alto, California, USA, November 6-8, 1996, Proceedings, volume 1166 of Lecture Notes in Computer Science, pages 265-269. Springer, 1996. URL https://doi.org/ 10.1007/BFb0031795.</p>
<p>PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J Liu, abs/1912.08777CoRRJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. CoRR, abs/1912.08777, 2019. URL http://arxiv.org/abs/1912.08777.</p>
<p>MASS: masked sequence to sequence pre-training for language generation. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu, PMLRProceedings of the 36th International Conference on Machine Learning, ICML 2019. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: masked sequence to se- quence pre-training for language generation. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learn- ing Research, pages 5926-5936. PMLR, 2019. URL http://proceedings.mlr.press/v97/ song19d.html.</p>
<p>Unified language model pre-training for natural language understanding and generation. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman GarnettNeurIPS; BC, CanadaVancouverLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural lan- guage understanding and generation. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Ad- vances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancou- ver, BC, Canada, pages 13042-13054, 2019. URL http://papers.nips.cc/paper/ 9464-unified-language-model-pre-training-for-natural-language-understanding-and-generation.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.10683.</p>
<p>SETSPEC) (v <MASK> GEN%PVAR%0)) (a (a (c <MASK> IN) (v <MASK> t)) (v <MASK> s))) (a (a (c <MASK> DIFF) (c <MASK> UNIV)) (v <MASK>. SETSPEC) (v <MASK> GEN%PVAR%0)) (a (a (c <MASK> IN) (v <MASK> t)) (v <MASK> s))) (a (a (c <MASK> DIFF) (c <MASK> UNIV)) (v <MASK></p>
<p>Ground truth: <START> (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (fun ?0 (bool)))) <END>. Ground truth: <START> (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (fun ?0 (bool)))) <END></p>
<p><MASK> f)) (v <MASK> s))) (a (c <MASK> !) (l (v <MASK> e) (a (a (c <MASK> ==&gt;) (a (a (c <MASK> real_lt) (a (c <MASK> real_of_num) (a (c <MASK> NUMERAL) (c <MASK> _0)))) (v <MASK> e))) (a (c <MASK> ?) (l (v <MASK> d) (a (a (c <MASK> ∧) (a (a (c <MASK> real_lt) (a (c <MASK> real_of_num) (a (c <MASK> NUMERAL) (c <MASK> _0)))) (v <MASK> d))) (a (c <MASK> !) (l (v <MASK> t) (a (c <MASK> !) (l (v <MASK> t') (a (a (c <MASK> ==&gt;) (a (a (c <MASK> ∧) (a (a (c <MASK> SUBSET) (v <MASK> t)) (v <MASK> s))) (a (a (c <MASK> ∧) (a (a (c <MASK> SUBSET) (v <PREDICT> t')) (v <MASK> s))) (a (a (c <MASK> ∧) (a (c <MASK> bounded) (v <MASK> t))) (a (a (c <MASK> ∧) (a (c <MASK> bounded) (v <MASK> t'))) (a (a (c <MASK> real_lt) (a (c <MASK> hausdist) (a (a (c <MASK> ,) (v <MASK> t')) (v <MASK> t)))) (v <MASK> d))))))) (a (a (c <MASK> real_lt. • (<theorem> (a (c <MASK> !) (l (v <MASK> f) (a (c <MASK> !) (l (v <MASK> s) (a (a (c <MASK> =) (a (a (c <MASK> uniformly_continuous_ona (c <MASK> hausdist) (a (a (c <MASK> ,) (a (a (c <MASK> IMAGE) (v <MASK> f)) (v <MASK> t'))) (a (a (c <MASK> IMAGE) (v <MASK> f)) (v <MASK> t))))) (v <MASK>• (<theorem> (a (c <MASK> !) (l (v <MASK> f) (a (c <MASK> !) (l (v <MASK> s) (a (a (c <MASK> =) (a (a (c <MASK> uniformly_continuous_on) (v <MASK> f)) (v <MASK> s))) (a (c <MASK> !) (l (v <MASK> e) (a (a (c <MASK> ==&gt;) (a (a (c <MASK> real_lt) (a (c <MASK> real_of_num) (a (c <MASK> NUMERAL) (c <MASK> _0)))) (v <MASK> e))) (a (c <MASK> ?) (l (v <MASK> d) (a (a (c <MASK> ∧) (a (a (c <MASK> real_lt) (a (c <MASK> real_of_num) (a (c <MASK> NUMERAL) (c <MASK> _0)))) (v <MASK> d))) (a (c <MASK> !) (l (v <MASK> t) (a (c <MASK> !) (l (v <MASK> t') (a (a (c <MASK> ==&gt;) (a (a (c <MASK> ∧) (a (a (c <MASK> SUBSET) (v <MASK> t)) (v <MASK> s))) (a (a (c <MASK> ∧) (a (a (c <MASK> SUBSET) (v <PREDICT> t')) (v <MASK> s))) (a (a (c <MASK> ∧) (a (c <MASK> bounded) (v <MASK> t))) (a (a (c <MASK> ∧) (a (c <MASK> bounded) (v <MASK> t'))) (a (a (c <MASK> real_lt) (a (c <MASK> hausdist) (a (a (c <MASK> ,) (v <MASK> t')) (v <MASK> t)))) (v <MASK> d))))))) (a (a (c <MASK> real_lt) (a (c <MASK> hausdist) (a (a (c <MASK> ,) (a (a (c <MASK> IMAGE) (v <MASK> f)) (v <MASK> t'))) (a (a (c <MASK> IMAGE) (v <MASK> f)) (v <MASK> t))))) (v <MASK></p>
<p>Ground truth: <START> (fun (cart (real) M) (bool)) <END>. Ground truth: <START> (fun (cart (real) M) (bool)) <END></p>
<p><MASK> a)) (v <MASK> s))) (a (a (c <MASK> =) (a (a (c <MASK> DIFF) (a (a (c <MASK> INSERT) (v <MASK> a)) (a (a (c <MASK> DELETE) (v <MASK> t)) (v <MASK> b)))) (v <MASK> s))) (a (a (c <MASK> DELETE) (a (a (c <MASK> DIFF) (v <MASK> t)) (v <MASK> s))) (v <MASK> b))))). • (<theorem> (a (a (c <MASK> ==&gt;) (a (a (c <PREDICT> IN) (v• (<theorem> (a (a (c <MASK> ==&gt;) (a (a (c <PREDICT> IN) (v <MASK> a)) (v <MASK> s))) (a (a (c <MASK> =) (a (a (c <MASK> DIFF) (a (a (c <MASK> INSERT) (v <MASK> a)) (a (a (c <MASK> DELETE) (v <MASK> t)) (v <MASK> b)))) (v <MASK> s))) (a (a (c <MASK> DELETE) (a (a (c <MASK> DIFF) (v <MASK> t)) (v <MASK> s))) (v <MASK> b)))))</p>
<p>Ground truth: <START> (fun ?0 (fun (fun ?0 (bool)) (bool))) <END>. Ground truth: <START> (fun ?0 (fun (fun ?0 (bool)) (bool))) <END></p>
<p>b) (a (c <MASK> convex) (a (c <MASK> GSPEC) (l (v <MASK> GEN%PVAR%0) (a (c <MASK> ?) (l (v <MASK> z) (a (a (a (c <MASK> SETSPEC) (v <MASK> GEN%PVAR%0)) (a (a (c <MASK> real_gt) (a (c <MASK> Im) (v <MASK> z))) (v <MASK> b))) (v <MASK> z))). • (<theorem> (a (c <MASK> !) (l (v <PREDICT>• (<theorem> (a (c <MASK> !) (l (v <PREDICT> b) (a (c <MASK> convex) (a (c <MASK> GSPEC) (l (v <MASK> GEN%PVAR%0) (a (c <MASK> ?) (l (v <MASK> z) (a (a (a (c <MASK> SETSPEC) (v <MASK> GEN%PVAR%0)) (a (a (c <MASK> real_gt) (a (c <MASK> Im) (v <MASK> z))) (v <MASK> b))) (v <MASK> z)))</p>
<p>Ground truth: <START> (real) <END>. Ground truth: <START> (real) <END></p>
<p><MASK> _0)))))) (a (c <MASK> ?) (l (v <MASK> B) (a (c <MASK> ?) (l (v <MASK> N) (a (c <MASK> !) (l (v <MASK> m) (a (c <MASK> !) (l (v <MASK> n) (a (a (c <MASK> ==&gt;) (a (a (c <MASK> ∧) (a (a (c <MASK> &lt;=) (v <MASK> N)) (v <MASK> m))) (a (a (c <MASK> &lt;=) (v <MASK> N)) (v <MASK> n)))) (a (a (c <MASK> &lt;=) (a (a (c <MASK> <em>) (a (a (c <MASK> </em>) (a (a (c <MASK> dest_nadd) (v <MASK> x)) (v <MASK> m))) (a (a (c <MASK> dest_nadd) (v <MASK> x)) (v <MASK> n)))) (a (c <MASK> dist) (a (a (c <MASK> ,) (a (a (c <MASK> <em>) (v <MASK> m)) (a (a (c <MASK> nadd_rinv) (v <MASK> x)) (v <PREDICT> n)))) (a (a (c <MASK> </em>) (v <MASK> n)) (a (a (c <MASK> nadd_rinv) (v <MASK> x). <MASK> x)) (a (c <MASK> nadd_of_num) (a (c <MASK> NUMERAL. • (<theorem> (a (c <MASK> !) (l (v <MASK> x) (a (a (c <MASK> ==&gt;) (a (c <MASK> ∼) (a (a (c <MASK> nadd_eqa (a (c <MASK> <em>) (v <MASK> B)) (a (a (c <MASK> </em>) (a (a (c <MASK> <em>) (v <MASK> m)) (v <MASK> n))) (a (a (c <MASK> +) (v <MASK> m)) (v <MASK> n• (<theorem> (a (c <MASK> !) (l (v <MASK> x) (a (a (c <MASK> ==&gt;) (a (c <MASK> ∼) (a (a (c <MASK> nadd_eq) (v <MASK> x)) (a (c <MASK> nadd_of_num) (a (c <MASK> NUMERAL) (c <MASK> _0)))))) (a (c <MASK> ?) (l (v <MASK> B) (a (c <MASK> ?) (l (v <MASK> N) (a (c <MASK> !) (l (v <MASK> m) (a (c <MASK> !) (l (v <MASK> n) (a (a (c <MASK> ==&gt;) (a (a (c <MASK> ∧) (a (a (c <MASK> &lt;=) (v <MASK> N)) (v <MASK> m))) (a (a (c <MASK> &lt;=) (v <MASK> N)) (v <MASK> n)))) (a (a (c <MASK> &lt;=) (a (a (c <MASK> </em>) (a (a (c <MASK> <em>) (a (a (c <MASK> dest_nadd) (v <MASK> x)) (v <MASK> m))) (a (a (c <MASK> dest_nadd) (v <MASK> x)) (v <MASK> n)))) (a (c <MASK> dist) (a (a (c <MASK> ,) (a (a (c <MASK> </em>) (v <MASK> m)) (a (a (c <MASK> nadd_rinv) (v <MASK> x)) (v <PREDICT> n)))) (a (a (c <MASK> <em>) (v <MASK> n)) (a (a (c <MASK> nadd_rinv) (v <MASK> x)) (v <MASK> m))))))) (a (a (c <MASK> </em>) (v <MASK> B)) (a (a (c <MASK> <em>) (a (a (c <MASK> </em>) (v <MASK> m)) (v <MASK> n))) (a (a (c <MASK> +) (v <MASK> m)) (v <MASK> n))</p>
<p>Ground truth: <START> (num) <END>. Ground truth: <START> (num) <END></p>
<p>GSPEC) (l (v ?1 GEN%PVAR%0) (a (c (fun (fun ?1 (bool)) (bool)) ?) (l (v ?1 x) (a (a (a (c (fun ?1 (fun (bool) (fun ?1 (bool)))) SETSPEC) (v ?1 GEN%PVAR%0)) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (a (c (fun ?1 (fun (fun ?1 (bool)) (bool))) IN) (v ?1 x)) (v (fun ?1 (bool)) s))) (a (a (c (fun ?0 (fun ?0 (bool))) =) (a (v (fun ?1 ?0) f) (v ?1 x))) (v ?0 a)))) (v ?1 x))))))) (v (fun ?1 (bool)) t))) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) <PREDICT>) (a (c (fun (fun ?1 (bool)) (bool)) !) (l (v ?1 x) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (v (fun ?1 (bool)) P) (v ?. Assumptions, Prompt, bool))) ==&gt;) (a (a (c (fun (fun ?1 (bool)) (fun (fun ?1 (bool)) (bool))) =) (a (c (fun (fun ?1 (bool)) (fun ?1 (bool))). <theorem> (a (a (c (fun (bool) (fun (bool. 1 x))) (a (v (fun ?1 (bool)) Q) (v ?1 x)))) (a (c (fun (bool) (boolAssumptions. • Prompt: (<theorem> (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (a (c (fun (fun ?1 (bool)) (fun (fun ?1 (bool)) (bool))) =) (a (c (fun (fun ?1 (bool)) (fun ?1 (bool))) GSPEC) (l (v ?1 GEN%PVAR%0) (a (c (fun (fun ?1 (bool)) (bool)) ?) (l (v ?1 x) (a (a (a (c (fun ?1 (fun (bool) (fun ?1 (bool)))) SETSPEC) (v ?1 GEN%PVAR%0)) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (a (c (fun ?1 (fun (fun ?1 (bool)) (bool))) IN) (v ?1 x)) (v (fun ?1 (bool)) s))) (a (a (c (fun ?0 (fun ?0 (bool))) =) (a (v (fun ?1 ?0) f) (v ?1 x))) (v ?0 a)))) (v ?1 x))))))) (v (fun ?1 (bool)) t))) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) <PREDICT>) (a (c (fun (fun ?1 (bool)) (bool)) !) (l (v ?1 x) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (v (fun ?1 (bool)) P) (v ?1 x))) (a (v (fun ?1 (bool)) Q) (v ?1 x)))) (a (c (fun (bool) (bool))</p>
<p>))) =) (a (v (fun ?1 ?0) f) (v ?1 x))) (v ?0 a. ∼) (a (a (c (fun ?0 (fun ?0 (bool∼) (a (a (c (fun ?0 (fun ?0 (bool))) =) (a (v (fun ?1 ?0) f) (v ?1 x))) (v ?0 a))</p>
<p>fun (bool) (bool))) ==&gt;) (a (v (fun ?1 (bool)) P) (v ?1 x))) (a (a (c (fun ?1 (fun (fun ?1 (bool)) (bool))) IN) (v ?1 x)) (v (fun ?1 (bool)) s)))))) (a (c (fun (fun ?1 (bool)) (bool)) !) (l (v ?1 x) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (v (fun ?1 (bool)) P) (v ?1 x))) (a (v (fun ?1 (bool)) Q) (v ?1 x)))) (a (c (fun (bool) (bool)) ∼). bool))) ∧) (a (c (fun (fun ?1 (bool)) (bool)) !) (l (v ?1 x) (a (a (c (fun (bool). Ground truth: <START> (a (a (c (fun (bool) (fun (bool. a (a (c (fun ?1 (fun (fun ?1 (bool)) (bool))) IN) (v ?1 x)) (v (fun ?1Ground truth: <START> (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (c (fun (fun ?1 (bool)) (bool)) !) (l (v ?1 x) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (v (fun ?1 (bool)) P) (v ?1 x))) (a (a (c (fun ?1 (fun (fun ?1 (bool)) (bool))) IN) (v ?1 x)) (v (fun ?1 (bool)) s)))))) (a (c (fun (fun ?1 (bool)) (bool)) !) (l (v ?1 x) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (v (fun ?1 (bool)) P) (v ?1 x))) (a (v (fun ?1 (bool)) Q) (v ?1 x)))) (a (c (fun (bool) (bool)) ∼) (a (a (c (fun ?1 (fun (fun ?1 (bool)) (bool))) IN) (v ?1 x)) (v (fun ?1</p>
<p><END> Source theorem pretty printed: {x | x IN s ∧ f x = a} = t ==&gt; (!x. P x ==&gt; x IN s) ∧ (!x. P x ∧ Q x ==&gt; ∼(x IN t)) ==&gt; (!x. P x ∧ Q x ==&gt; ∼. <END> Source theorem pretty printed: {x | x IN s ∧ f x = a} = t ==&gt; (!x. P x ==&gt; x IN s) ∧ (!x. P x ∧ Q x ==&gt; ∼(x IN t)) ==&gt; (!x. P x ∧ Q x ==&gt; ∼(f x = a))</p>
<p>. • Prompt, <theorem> (a (c (fun (fun (fun (cart (real) N) (bool• Prompt: (<theorem> (a (c (fun (fun (fun (cart (real) N) (bool))</p>
<p>) (bool)) s) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) <PREDICT>) (a (a (c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) N) (bool)) (bool))) =) (a (c (fun (fun (cart (real) N) (bool)) (fun (cart (real) N) (bool))) inside) (v (fun (cart (real) N) (bool)) s)). !) (l (v (fun (cart (real) N. c (fun (cart (real) N!) (l (v (fun (cart (real) N) (bool)) s) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) <PREDICT>) (a (a (c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) N) (bool)) (bool))) =) (a (c (fun (fun (cart (real) N) (bool)) (fun (cart (real) N) (bool))) inside) (v (fun (cart (real) N) (bool)) s))) (c (fun (cart (real) N)</p>
<p>fun (bool) (bool))) ∧) (a (c (fun (fun (cart (real) N) (bool)) (bool)) connected) (a (a (c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) N) (bool)) (fun (cart (real) N) (bool)))) DIFF). Ground truth: <START> (a (a (c (fun (bool. c (fun (cart (real) N) (boolGround truth: <START> (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (c (fun (fun (cart (real) N) (bool)) (bool)) connected) (a (a (c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) N) (bool)) (fun (cart (real) N) (bool)))) DIFF) (c (fun (cart (real) N) (bool))</p>
<p>UNIV)) (v (fun (cart (real) N) (bool)) s)))) (a (c (fun (bool). UNIV)) (v (fun (cart (real) N) (bool)) s)))) (a (c (fun (bool)</p>
<p>∼) (a (c (fun (fun (cart (real) N) (bool)) (bool)) bounded) (a (a. c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) N∼) (a (c (fun (fun (cart (real) N) (bool)) (bool)) bounded) (a (a (c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) N)</p>
<p>fun (cart (real) N) (bool)))) DIFF) (c (fun (cart (real) N). ) (fun (cart (real) N) (bool)))) DIFF) (c (fun (cart (real) N)</p>
<p>UNIV)) (v (fun (cart (real) N). UNIV)) (v (fun (cart (real) N) (</p>
<p>:realˆN) DIFF s) ∧ ∼bounded ((:realˆN) DIFF s) ==&gt; inside s = {} • Prompt: (<theorem> (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (v (bool) q)) (a (c (fun (bool) (bool)) ∼) (v (bool) p)))). <END> Source theorem pretty printed: !s. connected ((. a (a (c (fun (bool) (fun (bool<END> Source theorem pretty printed: !s. connected ((:realˆN) DIFF s) ∧ ∼bounded ((:realˆN) DIFF s) ==&gt; inside s = {} • Prompt: (<theorem> (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (v (bool) q)) (a (c (fun (bool) (bool)) ∼) (v (bool) p)))) (a (a (c (fun (bool) (fun (bool)</p>
<p>==&gt;) <PREDICT>) (v (bool) r)))). ==&gt;) <PREDICT>) (v (bool) r))))</p>
<p>fun (bool) (bool))) =) (v (bool) p)) (v (bool) q)) <END> Source theorem pretty printed: q ∧ ∼p ==&gt; (p &lt;=&gt; q) ==&gt; r • Prompt. Ground truth: <START> (a (a (c (fun (bool. <theorem> (a (c (fun (fun (fun (cart (real) N) (realGround truth: <START> (a (a (c (fun (bool) (fun (bool) (bool))) =) (v (bool) p)) (v (bool) q)) <END> Source theorem pretty printed: q ∧ ∼p ==&gt; (p &lt;=&gt; q) ==&gt; r • Prompt: (<theorem> (a (c (fun (fun (fun (cart (real) N) (real))</p>
<p>real) N) (real)) f) (a (c (fun (fun (fun (real) (real)) (bool)) (bool)) !. !) (l (v (fun (cart. l (v (fun (real!) (l (v (fun (cart (real) N) (real)) f) (a (c (fun (fun (fun (real) (real)) (bool)) (bool)) !) (l (v (fun (real)</p>
<p>real) N) (bool)) (bool)) !) (l (v (cart (real) N) x. a (c (fun (fun (cart. a (a (c (fun (bool) (fun (bool) (boolg) (a (c (fun (fun (cart (real) N) (bool)) (bool)) !) (l (v (cart (real) N) x) (a (a (c (fun (bool) (fun (bool) (bool)))</p>
<p>==&gt;) <PREDICT>) (a (a (c (fun (fun (cart (real) N) (real)) (fun (net (cart (real) N)) (bool))) real_continuous) (a (a (c (fun (fun (real) (real)) (fun (fun (cart (real) N) (real). fun (cart (real==&gt;) <PREDICT>) (a (a (c (fun (fun (cart (real) N) (real)) (fun (net (cart (real) N)) (bool))) real_continuous) (a (a (c (fun (fun (real) (real)) (fun (fun (cart (real) N) (real)) (fun (cart (real)</p>
<p>v (fun (real) (real)) g)) (v (fun (cart (real) N). o) (v (fun (real) (real)) g)) (v (fun (cart (real) N)</p>
<p>f))) (a (c (fun (cart (real) N) (net (cart (real) N))) at) (v (cart (real) N). f))) (a (c (fun (cart (real) N) (net (cart (real) N))) at) (v (cart (real) N)</p>
<p>Ground truth: <START> (a (a (c (fun (bool) (fun (bool) (bool))) ∧). a (a (c (fun (fun (cart (real) N) (real)) (fun (net (cart (real) N)Ground truth: <START> (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (a (c (fun (fun (cart (real) N) (real)) (fun (net (cart (real) N))</p>
<p>real_continuous) (v (fun (cart (real) N) (real)) f)) (a (c (fun (cart (real) N) (net (cart (real) N))) at) (v (cart (real) N). real_continuous) (v (fun (cart (real) N) (real)) f)) (a (c (fun (cart (real) N) (net (cart (real) N))) at) (v (cart (real) N)</p>
<p>fun (fun (real) (bool)) (net (real)))) within) (a (c (fun (real) (net (real))) atreal) (a (v (fun (cart (real) N) (real)) f) (v (cart (real) N) x)))) (a (a (c (fun (fun (cart (real) N) (real)) (fun (fun (cart (real) N) (bool)) (fun (real) (bool)))) IMAGE) (v (fun (cart (real) N) (real)) f. real_continuous) (v (fun (real) (real)) g)) (a (a (c (fun (net (real)). c (fun (cart (real) N) (boolreal_continuous) (v (fun (real) (real)) g)) (a (a (c (fun (net (real)) (fun (fun (real) (bool)) (net (real)))) within) (a (c (fun (real) (net (real))) atreal) (a (v (fun (cart (real) N) (real)) f) (v (cart (real) N) x)))) (a (a (c (fun (fun (cart (real) N) (real)) (fun (fun (cart (real) N) (bool)) (fun (real) (bool)))) IMAGE) (v (fun (cart (real) N) (real)) f)) (c (fun (cart (real) N) (bool))</p>
<p><END> Source theorem pretty printed: !f g x. f real_continuous at x ∧ g real_continuous atreal (f x) within IMAGE f (:realˆN) ==&gt; g o f real_continuous at x • Prompt: (<theorem> (a (c (fun (fun (fun (cart (real) M) (cart (real) N)) (bool)) (bool)) !). l (v (fun (cart (real) M) (cart (real) N)<END> Source theorem pretty printed: !f g x. f real_continuous at x ∧ g real_continuous atreal (f x) within IMAGE f (:realˆN) ==&gt; g o f real_continuous at x • Prompt: (<theorem> (a (c (fun (fun (fun (cart (real) M) (cart (real) N)) (bool)) (bool)) !) (l (v (fun (cart (real) M) (cart (real) N))</p>
<p>a (c (fun (fun (fun (cart (real) M) (cart (real) P)) (bool)). f) (a (c (fun (fun (fun (cart (real) M) (cart (real) P)) (bool))</p>
<p>) M) (cart (real) P)) g) (a (c (fun (fun (fun (cart (real) M) (bool)) (bool)) (bool)) !) (l (v (fun (cart (real) M) (bool)) s) (a (c. !) (l (v (fun (cart (real. fun (fun (num) (bool)) (bool!) (l (v (fun (cart (real) M) (cart (real) P)) g) (a (c (fun (fun (fun (cart (real) M) (bool)) (bool)) (bool)) !) (l (v (fun (cart (real) M) (bool)) s) (a (c (fun (fun (num) (bool)) (bool))</p>
<p>!) (l (v (num) n) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;). !) (l (v (num) n) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;)</p>
<p><PREDICT>) (a (a (a (c (fun (num) (fun (fun (cart (real) M) (bool)) (fun (fun (cart (real) M) (cart (real. <PREDICT>) (a (a (a (c (fun (num) (fun (fun (cart (real) M) (bool)) (fun (fun (cart (real) M) (cart (real) (finite_sum N P))) (bool))))</p>
<p>(bool)) s)) (l (v (cart (real) M) x) (a (a (c (fun (cart (real) N) (fun (cart (real) P) (cart (real. baire) (v (num) n)) (v (fun (cart (real) M). pastecart) (a (v (fun (cart (realbaire) (v (num) n)) (v (fun (cart (real) M) (bool)) s)) (l (v (cart (real) M) x) (a (a (c (fun (cart (real) N) (fun (cart (real) P) (cart (real) (finite_sum N P)))) pastecart) (a (v (fun (cart (real)</p>
<p>cart (real) N)) f) (v (cart (real) M) x))) (a (v (fun (cart (real) M) (cart (real) P)) g) (v (cart (real) M) x)). M) (cart (real) N)) f) (v (cart (real) M) x))) (a (v (fun (cart (real) M) (cart (real) P)) g) (v (cart (real) M) x))</p>
<p>fun (bool) (bool))) ∧) (a (a (a (c (fun (num) (fun (fun (cart (real) M) (bool)) (fun (fun (cart (real) M) (cart (real) N)) (bool)))) baire) (v (num) n)) (v (fun (cart (real) M) (bool)) s)). Ground truth: <START> (a (a (c (fun (bool. v (fun (cart (real) M) (cart (realGround truth: <START> (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (a (a (c (fun (num) (fun (fun (cart (real) M) (bool)) (fun (fun (cart (real) M) (cart (real) N)) (bool)))) baire) (v (num) n)) (v (fun (cart (real) M) (bool)) s)) (v (fun (cart (real) M) (cart (real)</p>
<p>) M) (bool)) (fun (fun (cart (real) M) (cart (real) P)) (bool)))) baire) (v (num). a (a (a (c (fun (num) (fun (fun (cart (realf))) (a (a (a (c (fun (num) (fun (fun (cart (real) M) (bool)) (fun (fun (cart (real) M) (cart (real) P)) (bool)))) baire) (v (num)</p>
<p>) M) (bool)) s)) (v (fun (cart (real) M). v (fun (cart (real(v (fun (cart (real) M) (bool)) s)) (v (fun (cart (real) M)</p>
<p>. • Prompt, <theorem> (a (c (fun (fun (fun ?0 (cart (real. bool• Prompt: (<theorem> (a (c (fun (fun (fun ?0 (cart (real) (2))) (bool))</p>
<p>))) f) (a (c (fun (fun (fun ?0 (cart (real) (2))) (bool)) (bool)) !). !) (l (v (fun ?0 (cart (real. l (v (fun ?0 (cart (real!) (l (v (fun ?0 (cart (real) (2))) f) (a (c (fun (fun (fun ?0 (cart (real) (2))) (bool)) (bool)) !) (l (v (fun ?0 (cart (real)</p>
<p>(bool)) (bool)) !) (l (v (fun ?0 (bool)) s) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (c (fun (fun ?0 (bool)) (bool)) FINITE) (v (fun ?0 (bool)) s))) (a (a (c (fun (cart (real) (2)) (fun (cart (real) (2)) (bool))) =) (a (a. ))) (cart (real) (2)))) cproduct) (v (fun ?0 (bool)) s)) (l (v ?. 0 x) (a (a (c (fun (cart (real) (2)) (fun (cart (real) (2)) (cart (realg) (a (c (fun (fun (fun ?0 (bool)) (bool)) (bool)) !) (l (v (fun ?0 (bool)) s) (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (c (fun (fun ?0 (bool)) (bool)) FINITE) (v (fun ?0 (bool)) s))) (a (a (c (fun (cart (real) (2)) (fun (cart (real) (2)) (bool))) =) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (cart (real) (2))) (cart (real) (2)))) cproduct) (v (fun ?0 (bool)) s)) (l (v ?0 x) (a (a (c (fun (cart (real) (2)) (fun (cart (real) (2)) (cart (real) (2))))</p>
<p>))) f) (v ?0 x))) (a (v (fun ?0 (cart (real) (2))) g) (v ?0 x))). complex_mul) (a (v (fun ?0 (cart (realcomplex_mul) (a (v (fun ?0 (cart (real) (2))) f) (v ?0 x))) (a (v (fun ?0 (cart (real) (2))) g) (v ?0 x)))</p>
<p>)) (fun (cart (real) (2)) (cart (real) (2)))) complex_mul) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (cart (real) (2))) (cart (real) (2)))) cproduct) (v (fun ?0 (bool)) s)) (v (fun ?0 (cart (real) (2))) f))) (a (a. Ground truth: <START> (a (a (c (fun (cart (real. c (fun (fun ?0 (bool)) (fun (fun ?0 (cart (real) (2))) (cart (realGround truth: <START> (a (a (c (fun (cart (real) (2)) (fun (cart (real) (2)) (cart (real) (2)))) complex_mul) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (cart (real) (2))) (cart (real) (2)))) cproduct) (v (fun ?0 (bool)) s)) (v (fun ?0 (cart (real) (2))) f))) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (cart (real) (2))) (cart (real) (2))))</p>
<p>bool)) s)) (v (fun ?0 (cart (real) (2))) g))) <END> Source theorem pretty printed: !f g s. FINITE s ==&gt; cproduct s (\x. f x * g x) = cproduct s f * cproduct s g • Prompt. cproduct) (v (fun ?0 (. <theorem> (a (c (fun (fun (fun (cart (real) N) (boolcproduct) (v (fun ?0 (bool)) s)) (v (fun ?0 (cart (real) (2))) g))) <END> Source theorem pretty printed: !f g s. FINITE s ==&gt; cproduct s (\x. f x * g x) = cproduct s f * cproduct s g • Prompt: (<theorem> (a (c (fun (fun (fun (cart (real) N) (bool))</p>
<p>) N) (bool)) s) (a (c (fun (fun (fun (cart (real) N) (bool)) (bool)) (bool)) !) (l (v (fun (cart (real) N) (bool)) t). !) (l (v (fun (cart (real. a (a (c (fun (bool) (fun (bool) (bool!) (l (v (fun (cart (real) N) (bool)) s) (a (c (fun (fun (fun (cart (real) N) (bool)) (bool)) (bool)) !) (l (v (fun (cart (real) N) (bool)) t) (a (a (c (fun (bool) (fun (bool) (bool)))</p>
<p>==&gt;) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (c (fun (fun (cart (real) N) (bool)) (bool)) convex). v (fun (cart (real) N==&gt;) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (c (fun (fun (cart (real) N) (bool)) (bool)) convex) (v (fun (cart (real) N)</p>
<p>s))) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (c (fun (fun (cart (real) N) (bool)) (bool)) affine). v (fun (cart (real) Ns))) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (c (fun (fun (cart (real) N) (bool)) (bool)) affine) (v (fun (cart (real) N)</p>
<p>bool)) (fun (fun (cart (real) N) (bool)) (bool))) =). a (a (c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) NN) (bool)) (fun (fun (cart (real) N) (bool)) (bool))) =) (a (a (c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) N)</p>
<p>fun (cart (real) N) (bool)))) INTER) (a (c (fun (fun. cart) (fun (cart (real) N) (bool)))) INTER) (a (c (fun (fun (cart</p>
<p>N) (bool)) (fun (cart (real) N) (bool))) relative_interior) (v (fun (cart (real) N) (bool)) s))) (v (fun (cart (real) N) (bool)). N) (bool)) (fun (cart (real) N) (bool))) relative_interior) (v (fun (cart (real) N) (bool)) s))) (v (fun (cart (real) N) (bool))</p>
<p>) N) (bool)) EMPTY)))))) (a (a (c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) N) (bool)) (bool))) =). c (fun (cart (real(c (fun (cart (real) N) (bool)) EMPTY)))))) (a (a (c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) N) (bool)) (bool))) =)</p>
<p>N) (bool)) (fun (cart (real) N) (bool)))) INTER) (a (c (fun (fun (cart (real) N) (bool)) (fun (cart (real) N) (bool))) closure) (v (fun (cart (real) N) (bool)) s))). v (fun (cart (real) N) (boolN) (bool)) (fun (cart (real) N) (bool)))) INTER) (a (c (fun (fun (cart (real) N) (bool)) (fun (cart (real) N) (bool))) closure) (v (fun (cart (real) N) (bool)) s))) (v (fun (cart (real) N) (bool))</p>
<p>real) N) (bool)) (fun (cart (real) N) (bool))) closure) (a (a (c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) N) (bool). Ground truth: <START> (a (c (fun (fun (cart. fun (cart (real) NGround truth: <START> (a (c (fun (fun (cart (real) N) (bool)) (fun (cart (real) N) (bool))) closure) (a (a (c (fun (fun (cart (real) N) (bool)) (fun (fun (cart (real) N) (bool)) (fun (cart (real) N)</p>
<p>INTER) (v (fun (cart (real) N) (bool)) s)) (v (fun (cart. INTER) (v (fun (cart (real) N) (bool)) s)) (v (fun (cart</p>
<p>==&gt; closure (s INTER t) = closure s INTER t • Prompt: (<theorem> (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (bool))) SUBSET) (v (fun ?0 (bool)) t)) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (fun ?0 (bool)))) DIFF) (c (fun ?0 (bool)) UNIV)) (v (fun ?0 (bool)) s)))) (a (a. <END> Source theorem pretty printed: !s t. convex s ∧ affine t ∧ ∼(relative_interior s INTER t = {}). c (fun (fun ?0 (bool)) (fun (fun ?0 (bool<END> Source theorem pretty printed: !s t. convex s ∧ affine t ∧ ∼(relative_interior s INTER t = {}) ==&gt; closure (s INTER t) = closure s INTER t • Prompt: (<theorem> (a (a (c (fun (bool) (fun (bool) (bool))) ==&gt;) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (bool))) SUBSET) (v (fun ?0 (bool)) t)) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (fun ?0 (bool)))) DIFF) (c (fun ?0 (bool)) UNIV)) (v (fun ?0 (bool)) s)))) (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (bool))</p>
<p>=) <PREDICT>) (c (fun ?0 (bool)) EMPTY)))). =) <PREDICT>) (c (fun ?0 (bool)) EMPTY))))</p>
<p>)) (fun (fun ?0 (bool)) (fun ?0 (bool)))) INTER) (v (fun ?0 (bool)) s)) (v (fun ?. Ground truth: <START> (a (a (c (fun (fun ?0 (bool. boolGround truth: <START> (a (a (c (fun (fun ?0 (bool)) (fun (fun ?0 (bool)) (fun ?0 (bool)))) INTER) (v (fun ?0 (bool)) s)) (v (fun ?0 (bool))</p>
<p>0) DIFF s ==&gt; s INTER t = {} • Prompt: (<theorem> (a (c (fun (fun (real) (bool)) (bool)) !) (l (v (real) x) (a (a (c (fun (real) (fun (real) (bool))) =) <PREDICT>) (a (c (fun (real) (real)) real_abs). <END> Source theorem pretty printed: t SUBSET. v (real) x<END> Source theorem pretty printed: t SUBSET (:?0) DIFF s ==&gt; s INTER t = {} • Prompt: (<theorem> (a (c (fun (fun (real) (bool)) (bool)) !) (l (v (real) x) (a (a (c (fun (real) (fun (real) (bool))) =) <PREDICT>) (a (c (fun (real) (real)) real_abs) (v (real) x</p>
<p>(real) (fun (num) (real))) real_pow) (a (c (fun (real) (real)) sqrt) (v (real) x))) (a. Ground truth: <START> (a (a (c (fun. c (fun (numGround truth: <START> (a (a (c (fun (real) (fun (num) (real))) real_pow) (a (c (fun (real) (real)) sqrt) (v (real) x))) (a (c (fun (num)</p>
<p>NUMERAL) (a (c (fun (num) (num)) BIT0) (a (c (fun (num) (num)). NUMERAL) (a (c (fun (num) (num)) BIT0) (a (c (fun (num) (num))</p>
<p>Source theorem pretty printed: !x. sqrt x pow 2 = abs x • Prompt. <theorem> (a (a (c (fun (fun A (bool)) (fun (fun A (boolSource theorem pretty printed: !x. sqrt x pow 2 = abs x • Prompt: (<theorem> (a (a (c (fun (fun A (bool)) (fun (fun A (bool))</p>
<p>=) <PREDICT>) (a (c (fun (fun A (bool)) (fun A (bool))). =) <PREDICT>) (a (c (fun (fun A (bool)) (fun A (bool)))</p>
<p>A GEN%PVAR%0)) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (a (c (fun A (fun (fun A (bool)) (bool))) IN) (v A y). Setspec, v (fun A (boolSETSPEC) (v A GEN%PVAR%0)) (a (a (c (fun (bool) (fun (bool) (bool))) ∧) (a (a (c (fun A (fun (fun A (bool)) (bool))) IN) (v A y)) (v (fun A (bool))</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9728 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9728</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9728</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-276724709</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.20635v1.pdf" target="_blank">Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?</a></p>
                <p><strong>Paper Abstract:</strong> EXplainable machine learning (XML) has recently emerged to address the mystery mechanisms of machine learning (ML) systems by interpreting their 'black box' results. Despite the development of various explanation methods, determining the most suitable XML method for specific ML contexts remains unclear, highlighting the need for effective evaluation of explanations. The evaluating capabilities of the Transformer-based large language model (LLM) present an opportunity to adopt LLM-as-a-Judge for assessing explanations. In this paper, we propose a workflow that integrates both LLM-based and human judges for evaluating explanations. We examine how LLM-based judges evaluate the quality of various explanation methods and compare their evaluation capabilities to those of human judges within an iris classification scenario, employing both subjective and objective metrics. We conclude that while LLM-based judges effectively assess the quality of explanations using subjective metrics, they are not yet sufficiently developed to replace human judges in this role.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9728.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9728.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-vs-Human (summary)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge and Human Evaluations (findings in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summary of the paper's empirical findings comparing two LLM-based judges (GPT-4o, Mistral-7.2B) with 38 human participants on evaluation of ML explanations for an iris classification task, highlighting which evaluation dimensions diverge and what is degraded when substituting LLMs for humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Tabular classification explanation evaluation (iris binary classification: versicolor vs virginica)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o; Mistral-7.2B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Zero-shot single-answer grading prompt; LLMs asked to predict the target label from provided context (background table of class feature means, target instance, and one explanation) and then assign integer scores (1–5) for five subjective statements; temperature = 0.7; 18 tasks per judge (3 explanation methods × 6 instances).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Online user study (Qualtrics) with 38 participants (mix of researchers and students; average age 29); each participant completed 16 tasks randomized; for each task they saw the same contextual information and explanations as LLMs, made a label prediction (objective accuracy recorded) and rated the five subjective statements on a 5‑point Likert scale.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Group-comparison statistics (one-way ANOVA across conditions and post-hoc Tukey HSD); reported example statistics include (across judges per explanation) Accuracy: LIME F=64.48 p<.000, Similarity-based Accuracy F=28.46 p<.000, Without-explanation Accuracy F=88.72 p<.000; subjective metrics also reported F and p in Table 5 (e.g., Understandability without-explanation F=8.07 p<.000). Note: paper used ANOVA/Tukey HSD to compare distributions rather than a direct inter-annotator agreement metric (e.g., Cohen's kappa) between LLMs and humans.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Objective behavioural sensitivity is degraded: LLM-as-a-judge (esp. GPT-4o) failed to replicate objective accuracy differences across explanation methods that humans (and Mistral-7.2B) detected; many subjective dimensions (satisfaction, completeness, usefulness, trustworthiness) show systematic differences between LLMs and humans (LLMs do not capture human-like nuance across these metrics); LLMs exhibit systematic biases distinct from human judges, so substituting LLMs risks missing or mischaracterising explanation impacts that manifest in human decision behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>1) GPT-4o did not show statistically significant differences in judge accuracy across the three explanation conditions (LIME, similarity-based, without explanation), while Mistral-7.2B and humans did; this indicates GPT-4o failed to reflect objective performance differences that humans experienced (Results Sec. 6.1). 2) Across multiple subjective metrics (satisfaction, completeness, usefulness, trustworthiness), LLM-based judges produced ratings that differed significantly from human ratings for the same explanations (Results Sec. 6.2; Tables 5 & 6). 3) The 'without explanation' baseline—an important negative case—was treated differently by GPT-4o on understandability relative to humans (GPT-4o understandability differs significantly from Mistral and humans in the without-explanation condition).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>1) LLMs matched humans on the subjective understandability metric for LIME and similarity-based explanations (no significant differences found for that metric in those conditions), indicating some dimensions of subjective judgement are well-approximated. 2) Mistral-7.2B more closely mirrored humans on objective sensitivity (it showed significant accuracy differences across explanation types like humans), so not all LLMs fail equally—performance depends on the specific LLM. 3) LLMs reliably identified that the baseline (without explanation) had lower subjective quality than the other two explainers, showing LLMs can detect coarse-grained differences in perceived explanation quality (Discussion & Conclusion).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Results (Sections 6.1 and 6.2), Tables 3-6, Discussion, Conclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9728.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9728.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o used as an LLM-based judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o was used zero-shot with a single-answer grading prompt to predict labels and rate five subjective dimensions of explanations; it produced subjective ratings consistent with humans on some dimensions but failed to reproduce human-like objective sensitivity in this experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Tabular iris classification explanation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Zero-shot single-answer grading prompt: LLM told its role as judge, given background class means, target instance, and one explanation; instructed to (1) predict the label and (2) score five subjective statements (1–5). Temperature 0.7; 18 tasks total.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same contextual information and instructions provided to human participants; 38 participants did 16 tasks each; objective label prediction recorded and five subjective Likert ratings obtained.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Between-condition comparisons via one-way ANOVA and Tukey HSD. Paper reports that for GPT-4o, the one-way ANOVA on objective accuracy across explanations was not statistically significant (GPT-4o failed to detect accuracy differences across explanation methods) while subjective metrics did show significant differences across explanation methods (see Results Sec. 6.1 and Tables 3–4).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Major loss: inability to reflect objective behavioural differences — GPT-4o failed to detect statistically significant differences in judge accuracy across explanation types (i.e., it did not mirror human variability in making correct label decisions under different explanations). Additional losses: divergence from humans on multiple subjective dimensions in direct comparisons (Section 6.2), indicating missing human-like nuance in satisfaction, completeness, usefulness, and trustworthiness ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>GPT-4o did not show significant differences in objective accuracy across LIME, similarity-based, and without-explanation conditions (RQ1/H1.1 rejected for objective metric), whereas Mistral and humans did show such differences (Results Sec. 6.1). In pairwise comparisons for the 'without explanation' condition, GPT-4o's understandability ratings were significantly different from humans and Mistral (Results Sec. 6.2, Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>GPT-4o matched humans on subjective understandability for LIME and similarity-based explanations (no significant difference reported), and it still flagged the baseline 'without explanation' as lower quality on subjective metrics—so it can capture some coarse subjective distinctions even if it misses objective behavioural sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Results Sec. 6.1 (Evaluation of explanations by each judge), Results Sec. 6.2 (Comparison of evaluations by different judges), Tables 3–6, Discussion, Conclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9728.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9728.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7.2B (judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7.2B used as an LLM-based judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mistral-7.2B was used zero-shot with the paper's tailored prompt to evaluate explanations and, unlike GPT-4o, it showed objective sensitivity comparable to humans for accuracy differences across explanation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Tabular iris classification explanation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Mistral-7.2B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Zero-shot single-answer grading prompt: role statement, task description, and contextual information provided; asked to predict label and score five subjective statements. Temperature 0.7; 18 tasks per model run.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same as for GPT-4o and human participants: identical context and evaluation instruments; 38 human participants in the user study.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>ANOVA/Tukey HSD used to compare ratings and accuracy distributions. For Mistral-7.2B the one-way ANOVA on objective accuracy across explanation methods was statistically significant (Mistral showed significant accuracy differences between explanations similar to humans); post-hoc tests indicated specific significant contrasts (see Results Sec. 6.1 and Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Although Mistral more closely matched human objective sensitivity than GPT-4o, differences from human judgments remain on several subjective metrics (satisfaction, completeness, usefulness, trustworthiness) and inter-judge contrasts show significant differences for most dimensions—so substituting humans with Mistral still risks losing human-like subjective nuance.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>While Mistral and humans both showed significant accuracy differences across explanation types (e.g., without explanation significantly worse than LIME and similarity-based), Mistral's pairwise subjective ratings still differed from humans in specific comparisons (Results Sec. 6.2, Tables 5–6).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Counterexample: Mistral reproduced objective differences in accuracy across explanation types (supporting H1.2 and aligning with humans), showing that some LLMs can capture objective behavior effects; however, subjective metric mismatches remain in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Results Sec. 6.1 (objective evaluation differences), Results Sec. 6.2 (subjective comparisons), Tables 3–6, Discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks. <em>(Rating: 2)</em></li>
                <li>Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge. <em>(Rating: 2)</em></li>
                <li>Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. <em>(Rating: 1)</em></li>
                <li>Can LLM be a Personalized Judge? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9728",
    "paper_id": "paper-276724709",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM-vs-Human (summary)",
            "name_full": "Comparison of LLM-as-a-Judge and Human Evaluations (findings in this paper)",
            "brief_description": "Summary of the paper's empirical findings comparing two LLM-based judges (GPT-4o, Mistral-7.2B) with 38 human participants on evaluation of ML explanations for an iris classification task, highlighting which evaluation dimensions diverge and what is degraded when substituting LLMs for humans.",
            "citation_title": "Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?",
            "mention_or_use": "use",
            "task_domain": "Tabular classification explanation evaluation (iris binary classification: versicolor vs virginica)",
            "llm_judge_model": "GPT-4o; Mistral-7.2B",
            "llm_judge_setup": "Zero-shot single-answer grading prompt; LLMs asked to predict the target label from provided context (background table of class feature means, target instance, and one explanation) and then assign integer scores (1–5) for five subjective statements; temperature = 0.7; 18 tasks per judge (3 explanation methods × 6 instances).",
            "human_evaluation_setup": "Online user study (Qualtrics) with 38 participants (mix of researchers and students; average age 29); each participant completed 16 tasks randomized; for each task they saw the same contextual information and explanations as LLMs, made a label prediction (objective accuracy recorded) and rated the five subjective statements on a 5‑point Likert scale.",
            "agreement_metric": "Group-comparison statistics (one-way ANOVA across conditions and post-hoc Tukey HSD); reported example statistics include (across judges per explanation) Accuracy: LIME F=64.48 p&lt;.000, Similarity-based Accuracy F=28.46 p&lt;.000, Without-explanation Accuracy F=88.72 p&lt;.000; subjective metrics also reported F and p in Table 5 (e.g., Understandability without-explanation F=8.07 p&lt;.000). Note: paper used ANOVA/Tukey HSD to compare distributions rather than a direct inter-annotator agreement metric (e.g., Cohen's kappa) between LLMs and humans.",
            "losses_identified": "Objective behavioural sensitivity is degraded: LLM-as-a-judge (esp. GPT-4o) failed to replicate objective accuracy differences across explanation methods that humans (and Mistral-7.2B) detected; many subjective dimensions (satisfaction, completeness, usefulness, trustworthiness) show systematic differences between LLMs and humans (LLMs do not capture human-like nuance across these metrics); LLMs exhibit systematic biases distinct from human judges, so substituting LLMs risks missing or mischaracterising explanation impacts that manifest in human decision behaviour.",
            "examples_of_loss": "1) GPT-4o did not show statistically significant differences in judge accuracy across the three explanation conditions (LIME, similarity-based, without explanation), while Mistral-7.2B and humans did; this indicates GPT-4o failed to reflect objective performance differences that humans experienced (Results Sec. 6.1). 2) Across multiple subjective metrics (satisfaction, completeness, usefulness, trustworthiness), LLM-based judges produced ratings that differed significantly from human ratings for the same explanations (Results Sec. 6.2; Tables 5 & 6). 3) The 'without explanation' baseline—an important negative case—was treated differently by GPT-4o on understandability relative to humans (GPT-4o understandability differs significantly from Mistral and humans in the without-explanation condition).",
            "counterexamples_or_caveats": "1) LLMs matched humans on the subjective understandability metric for LIME and similarity-based explanations (no significant differences found for that metric in those conditions), indicating some dimensions of subjective judgement are well-approximated. 2) Mistral-7.2B more closely mirrored humans on objective sensitivity (it showed significant accuracy differences across explanation types like humans), so not all LLMs fail equally—performance depends on the specific LLM. 3) LLMs reliably identified that the baseline (without explanation) had lower subjective quality than the other two explainers, showing LLMs can detect coarse-grained differences in perceived explanation quality (Discussion & Conclusion).",
            "paper_reference": "Results (Sections 6.1 and 6.2), Tables 3-6, Discussion, Conclusion.",
            "uuid": "e9728.0",
            "source_info": {
                "paper_title": "Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4o (judge)",
            "name_full": "GPT-4o used as an LLM-based judge",
            "brief_description": "GPT-4o was used zero-shot with a single-answer grading prompt to predict labels and rate five subjective dimensions of explanations; it produced subjective ratings consistent with humans on some dimensions but failed to reproduce human-like objective sensitivity in this experiment.",
            "citation_title": "Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?",
            "mention_or_use": "use",
            "task_domain": "Tabular iris classification explanation evaluation",
            "llm_judge_model": "GPT-4o",
            "llm_judge_setup": "Zero-shot single-answer grading prompt: LLM told its role as judge, given background class means, target instance, and one explanation; instructed to (1) predict the label and (2) score five subjective statements (1–5). Temperature 0.7; 18 tasks total.",
            "human_evaluation_setup": "Same contextual information and instructions provided to human participants; 38 participants did 16 tasks each; objective label prediction recorded and five subjective Likert ratings obtained.",
            "agreement_metric": "Between-condition comparisons via one-way ANOVA and Tukey HSD. Paper reports that for GPT-4o, the one-way ANOVA on objective accuracy across explanations was not statistically significant (GPT-4o failed to detect accuracy differences across explanation methods) while subjective metrics did show significant differences across explanation methods (see Results Sec. 6.1 and Tables 3–4).",
            "losses_identified": "Major loss: inability to reflect objective behavioural differences — GPT-4o failed to detect statistically significant differences in judge accuracy across explanation types (i.e., it did not mirror human variability in making correct label decisions under different explanations). Additional losses: divergence from humans on multiple subjective dimensions in direct comparisons (Section 6.2), indicating missing human-like nuance in satisfaction, completeness, usefulness, and trustworthiness ratings.",
            "examples_of_loss": "GPT-4o did not show significant differences in objective accuracy across LIME, similarity-based, and without-explanation conditions (RQ1/H1.1 rejected for objective metric), whereas Mistral and humans did show such differences (Results Sec. 6.1). In pairwise comparisons for the 'without explanation' condition, GPT-4o's understandability ratings were significantly different from humans and Mistral (Results Sec. 6.2, Table 6).",
            "counterexamples_or_caveats": "GPT-4o matched humans on subjective understandability for LIME and similarity-based explanations (no significant difference reported), and it still flagged the baseline 'without explanation' as lower quality on subjective metrics—so it can capture some coarse subjective distinctions even if it misses objective behavioural sensitivity.",
            "paper_reference": "Results Sec. 6.1 (Evaluation of explanations by each judge), Results Sec. 6.2 (Comparison of evaluations by different judges), Tables 3–6, Discussion, Conclusion.",
            "uuid": "e9728.1",
            "source_info": {
                "paper_title": "Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Mistral-7.2B (judge)",
            "name_full": "Mistral-7.2B used as an LLM-based judge",
            "brief_description": "Mistral-7.2B was used zero-shot with the paper's tailored prompt to evaluate explanations and, unlike GPT-4o, it showed objective sensitivity comparable to humans for accuracy differences across explanation methods.",
            "citation_title": "Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?",
            "mention_or_use": "use",
            "task_domain": "Tabular iris classification explanation evaluation",
            "llm_judge_model": "Mistral-7.2B",
            "llm_judge_setup": "Zero-shot single-answer grading prompt: role statement, task description, and contextual information provided; asked to predict label and score five subjective statements. Temperature 0.7; 18 tasks per model run.",
            "human_evaluation_setup": "Same as for GPT-4o and human participants: identical context and evaluation instruments; 38 human participants in the user study.",
            "agreement_metric": "ANOVA/Tukey HSD used to compare ratings and accuracy distributions. For Mistral-7.2B the one-way ANOVA on objective accuracy across explanation methods was statistically significant (Mistral showed significant accuracy differences between explanations similar to humans); post-hoc tests indicated specific significant contrasts (see Results Sec. 6.1 and Table 4).",
            "losses_identified": "Although Mistral more closely matched human objective sensitivity than GPT-4o, differences from human judgments remain on several subjective metrics (satisfaction, completeness, usefulness, trustworthiness) and inter-judge contrasts show significant differences for most dimensions—so substituting humans with Mistral still risks losing human-like subjective nuance.",
            "examples_of_loss": "While Mistral and humans both showed significant accuracy differences across explanation types (e.g., without explanation significantly worse than LIME and similarity-based), Mistral's pairwise subjective ratings still differed from humans in specific comparisons (Results Sec. 6.2, Tables 5–6).",
            "counterexamples_or_caveats": "Counterexample: Mistral reproduced objective differences in accuracy across explanation types (supporting H1.2 and aligning with humans), showing that some LLMs can capture objective behavior effects; however, subjective metric mismatches remain in many cases.",
            "paper_reference": "Results Sec. 6.1 (objective evaluation differences), Results Sec. 6.2 (subjective comparisons), Tables 3–6, Discussion.",
            "uuid": "e9728.2",
            "source_info": {
                "paper_title": "Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks.",
            "rating": 2,
            "sanitized_title": "llms_instead_of_human_judges_a_large_scale_empirical_study_across_20_nlp_evaluation_tasks"
        },
        {
            "paper_title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge.",
            "rating": 2,
            "sanitized_title": "open_source_language_models_can_provide_feedback_evaluating_llms_ability_to_help_students_using_gpt4asajudge"
        },
        {
            "paper_title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences.",
            "rating": 2,
            "sanitized_title": "who_validates_the_validators_aligning_llmassisted_evaluation_of_llm_outputs_with_human_preferences"
        },
        {
            "paper_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.",
            "rating": 1,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Can LLM be a Personalized Judge?",
            "rating": 1,
            "sanitized_title": "can_llm_be_a_personalized_judge"
        }
    ],
    "cost": 0.01167175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?
28 Feb 2025</p>
<p>B O Wang 
Data Science Institute
University of Technology Sydney
Australia</p>
<p>Data Science Institute
YIQIAO LI
University of Technology Sydney
Australia</p>
<p>Data Science Institute
JIANLONG ZHOU
University of Technology Sydney
Australia</p>
<p>FANG CHEN
Data Science Institute
University of Technology Sydney
Australia</p>
<p>Data Science Institute
University of Technology Sydney
Sydney, Yiqiao LiAustralia</p>
<p>Data Science Institute
University of Technology Sydney
Sydney, Jianlong ZhouAustralia</p>
<p>Data Science Institute
Fang Chen
University of Technology Sydney
SydneyAustralia</p>
<p>Data Science Institute
University of Technology Sydney
SydneyAustralia</p>
<p>Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?
28 Feb 202557C4A48EE9BEDC6C70B9A8ED023E8C72arXiv:2502.20635v1[cs.HC]Machine Learning ExplanationTransformersHuman-Computer InteractionLarge Language ModelsLLM-as-a-JudgeSubjectiveObjective
EXplainable machine learning (XML) has recently emerged to address the mystery mechanisms of machine learning (ML) systems by interpreting their 'black box' results.Despite the development of various explanation methods, determining the most suitable XML method for specific ML contexts remains unclear, highlighting the need for effective evaluation of explanations.The evaluating capabilities of the Transformer-based large language model (LLM) present an opportunity to adopt LLM-as-a-Judge for assessing explanations.In this paper, we propose a workflow that integrates both LLM-based and human judges for evaluating explanations.We examine how LLM-based judges evaluate the quality of various explanation methods and compare their evaluation capabilities to those of human judges within an iris classification scenario, employing both subjective and objective metrics.We conclude that while LLM-based judges effectively assess the quality of explanations using subjective metrics, they are not yet sufficiently developed to replace human judges in this role.CCS Concepts: • Human-centered computing → Human computer interaction (HCI).</p>
<p>Introduction</p>
<p>Machine learning (ML) systems have seen significant growth in popularity due to their increasing problem-solving capabilities, which have been applied across diverse fields such as medical healthcare [17], biology analysis [25], and fraud detection [5].However, these systems often operate as 'black boxes,' lacking transparency and preventing users from understanding the rationale behind their decisions.To address this issue, researchers have developed eXplainable ML (XML) techniques, which aim to provide interpretability and insight into these opaque models, enhancing their trustworthiness and reliability.</p>
<p>Various methods have been proposed in the field of XML, including SHAP [23], LIME [29], and similarity-based explanations [4].Despite the advancement of these methods, evaluating the quality of explanations remains a complex and unresolved issue [26,37].Thus, there is a pressing need to evaluate the quality of explanations and identify the most appropriate XML methods for practical applications.</p>
<p>Existing research has explored various approaches to evaluating explainability, encompassing functionality-grounded, application-grounded, and human-grounded methods [9].However, these evaluation techniques have notable limitations.Functionality-grounded evaluations rely on quantitative metrics that often fail to capture the nuances of human perception.Meanwhile, application-grounded and human-grounded evaluations, which involve experiments with experts and non-experts, are resource-intensive, requiring significant time, cost, and ethical approvals.Alternatively, researchers have investigated the use of Transformer models, specifically large language models (LLMs) as judges, positioning them as alternatives to human evaluators [15,27,28,31].Therefore, employing the Transformer-based LLMs as judges to evaluate ML explanations represents a promising approach.</p>
<p>However, employing LLMs as evaluators introduces new challenges.Although LLMs have demonstrated high agreement with human evaluations in some tasks, they are not human and may produce errors distinct from human evaluators [2].Therefore, it is essential to calibrate LLMs evaluations against human assessments using relevant datasets to ensure their validity and reliability.Currently, the effectiveness and capabilities of LLMs in evaluating the quality of ML explanations compared to humans remain unexamined.Driven by these gaps, this study aims to answer the following research questions: RQ1: How do different judges, including GPT-4o, Mistral-7.2B,and humans, evaluate the quality of various explanations?RQ2: How do LLM-based judges, such as GPT-4o and Mistral-7.2B,compare to human judges in evaluating the quality of explanations?To address the research questions comprehensively, we design a workflow for evaluating machine learning explanations using GPT-4o, Mistral-7.2B,and human judges based on iris classification.Our study is based on three types of explanations: those produced by LIME [29], similarity-based explanations [13], and without explanations.To ensure fair comparisons among the judges, we conduct a forward simulation experiment involving 38 LLMs/human participants to analyze the correlation between LLM-based and human evaluations.To thoroughly assess the quality of ML explanations, we employ both subjective and objective measures.Specifically, we develop five subjective statements rated on a 5-point Likert scale and use accuracy as an objective metric to evaluate explanation quality.The main contributions of this study are as follows:</p>
<p>• To the best of our knowledge, this study is the first to assess the validity and reliability of using LLMs as judges for evaluating ML explanations.• This study examines and validates the judgments of LLM-based evaluators, specifically GPT-4o and Mistral-7.2B,against human judgments across various explanation methods.• This study focuses on evaluating the quality of various explanation methods, including LIME, similarity-based explanations, and cases without explanations.• We employ a combination of subjective and objective measures to evaluate the quality of ML explanations in both LLM-based and human judgments.The rest of the paper is organized as follows.In Section 2, the related work in the evaluation of ML explanations and Transformers is overviewed.Following that, our hypotheses, considering various judges and explanations, are formulated in Section 3. Afterward, our methodologies including a workflow that integrates both LLM-based and human judges for evaluating explanations, LLMs' prompt design, study design, and subjective and objective measurements are introduced in Section 4. Next, our experimental setup is presented, detailing the dataset and classified used, LLMs and explainers employed, and the online user study conducted in Section 5.The results based on subjective and objective measurements are analysed via statistical tests in Section 6.The main experimental results, implications, and limitations of the study are summarized in Section 7. Finally, the conclusions of this paper are drawn in Section 8.</p>
<p>Related Work</p>
<p>In this section, we first survey current categories for evaluating ML explanations.Subsequently, we examine the literature on advancements in the field of Transformers.At last, we conclude with a discussion of recent research efforts on the use of LLMs as judges.</p>
<p>Evaluation of ML Explanations</p>
<p>Currently, there are three categories of evaluation of ML explanations: functionality-grounded, application-grounded, and human-grounded evaluations [9].Functionality-grounded evaluation requires no human experiments; instead, it employs formal definitions of interpretability as a proxy to evaluate explanation quality.For instance, Dai et al. [6] focus on fidelity, stability, consistency, and sparsity to evaluate the quality of explanations.Additionally, the depth of a decision tree has been used as an indicator of explanation quality [11].Application-grounded evaluation, on the other hand, requires human-subject experiments within a real-world application context, typically involving domain experts.Goel et al. [12] adopt application-grounded evaluation by using ML explanations on real-world diagnosis of COVID-19 CT images to study how well explanations influence clinicians' trust in an automated decision-making task, suggesting that explanations enhance clinicians' trust on the system.</p>
<p>Human-grounded evaluation refers to conducting simpler human-subject experiments that capture the essence of the target application.Unlike application-grounded evaluation, this category of evaluation does not involve domain experts but lay humans.Wang et al. [33] conduct humangrounded evaluation to explore the relationships between user trust and fidelity and robustness of explanations through a simulated user study, revealing user trust is significantly impacted by different fidelity and robustness levels.Similarly, Lertvittayakumjorn and Toni [20] propose three human-grounded evaluation tasks to assess the quality of explanation methods in the context of text classification for different purposes.Their findings indicate that good explanations can justify predictions and assist humans in investigating uncertain predictions, while using explanations to reveal model behavior remains a challenge.In this paper, we conduct an in-depth examination of human capabilities in assessing the quality of ML explanations through human-grounded evaluations.We subsequently compare these human capabilities with those of LLMs to investigate the differences in their evaluating capabilities.</p>
<p>Transformer</p>
<p>Transformer models replace the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention, which is initially proposed by Vaswani et al. [32].The advent of Transformer models facilitates the development of a new age in various fields, such as computer vision [21], time-series prediction [19], and natural language processing (NLP) [1,35].Liu et al. [21] present a new Transformer, called Swin Transformer, that provides a hierarchical vision Transformer whose representations can be computed on a local and global basis for computer vision.Their new Transformer model can lead to strong performance on image recognition tasks.In addition, Sangwon et al. [19] propose a time-series forecasting optimized Transformer model, called TS-Fastformer, with three new optimizations including Sub Window Tokenizer, Time-series Pre-trained Encoder, and Past Attention Decoder, delivering a faster training speed and a lower mean-square error.Moreover, Acheampong et al. [1] examine that Transformer-based models like GPT and its variants, Transformer-XL model, and BERT, yield significant improvements in NLP tasks, especially text-based emotion detection.Similarly, Yadav [35] discusses that Transformers make NLP technologies become rather human-like in understanding and mirroring human language, leveraging sequence transduction architectures based entirely on attention mechanisms.As a result, LLMs, such as GPT [24] and Mistral [16] style, built upon Transformer architectures, exhibit human-like capabilities and excel in learning complex patterns and linguistic constructions.Specifically, to explore LLMs' capabilities in the evaluation of the quality of ML explanations, we adopt two Transformer-based LLMs including GPT-4o and Mistral-7.2B in this paper.</p>
<p>Applying LLMs as Judges</p>
<p>The use of LLMs as judges is a newly developing topic, with numerous academic studies investigating and validating the performance of LLM-as-a-Judge.LLM-as-a-Judge is initially employed to evaluate the output of other LLMs [36].In their work, they propose three types of LLM-as-a-Judge:</p>
<p>(1) pairwise comparison, ( 2) single-answer grading, and (3) reference-guided grading.Through extensive examinations, they find out that LLM-as-a-judge is a scalable and explainable way to approximate human preferences on open-ended questions.Following this, Koucheme et al. [18] utilize GPT-4 as a single answer grading judge to assess the quality of GPT-3.5 feedback on incorrect student-written programs.Through comparing to the expert annotator, they notice that GPT-4 can be quite reliable in evaluating the quality of automatically generated feedback.Similarly, Dong et al. [8] introduce an LLM-as-a-Personalized-Judge pipeline, where LLMs assess user preferences, achieving accuracy comparable to human evaluations and even surpassing human performance on high-certainty samples.On the other hand, Bavaresco et al. [2] argue that LLMs are not ready to systematically replace human judges in NLP.Their study, which involved the application of 20 NLP datasets with human annotations, evaluates LLMs on their ability to replicate these annotations, revealing significant variability in LLM performance across datasets in relation to human judgments.However, to date, little work has been found in using LLMs as judges to evaluate the quality of ML explanation methods.This paper aims to study the evaluating capabilities of different LLMs in the quality of different explanations and to compare these capabilities with those of human judges for the same explanation.Specifically, we apply LLM-as-a-Judge with our tailored prompt design for evaluating explanations in our work.</p>
<p>Hypotheses</p>
<p>To address our research questions (RQ1 and RQ2), this study formulates the corresponding hypotheses (H1 and H2), considering different judges (GPT-4o, Mistral-7.2B,and humans) and varying explanations (LIME, similarity-based, and without explanation).</p>
<p>H1: When evaluated by the same judge, there are significant differences in explanation quality across various explanations.Specifically, we expect that without explanation will result in the lowest quality.For H1, we have three sub-hypotheses: -H1.1:There are significant differences in quality across various explanations when evaluated by GPT-4o.-H1.2:There are significant differences in quality across various explanations when evaluated by Mistral-7.2B.-H1.3:There are significant differences in quality across various explanations when evaluated by humans.</p>
<p>H2: When the same explanation is assessed, there are no significant differences in explanation quality across various judges.We anticipate that the evaluation capabilities of LLM-based judges will be comparable to those of human judges.For H2, we have three sub-hypotheses: -H2.1:There are no significant differences in quality across various judges when assessing LIME.-H2.2:There are no significant differences in quality across various judges when assessing similarity-based.-H2.3:There are no significant differences in quality across various judges when assessing without explanation.</p>
<p>H1 pertains to the variability in explanation quality across different explanations as detected by the same judge, while H2 focuses on the consistency in explanation quality across different judges when they assess the same explanation.</p>
<p>Methodologies</p>
<p>This section starts by introducing our proposed workflow and case study.It then details the LLMs prompt design tailored for iris classification.Subsequently, the study design is discussed, incorporating conditions based on judges and explanations.Finally, the section outlines the approach used to evaluate the quality of explanations, integrating both subjective and objective aspects.</p>
<p>Workflow and Case Study</p>
<p>Figure 1 illustrates the process for involving both LLM-based and human judges in evaluating the quality of ML explanations.This workflow emphasizes consistency across all steps, including the use of input data, the implementation of ML models and explanations, and the evaluation metrics, to ensure effective comparison between LLM-based and human judges.Our aim is for this workflow not only to validate the evaluating capabilities of judges across distinct explanations but also to investigate the difference in evaluating capabilities between LLM-based and human judges.This paper uses iris classification, specifically targeting an iris instance and its corresponding explanation, as a case study.The original iris dataset contains three iris labels (setosa, versicolor, and virginica) determined by four features including sepal length, sepal width, petal length, and petal width.Specifically, in our study, the ML model predicts the target label, focusing on distinguishing between 'versicolor' and 'virginica' based on these features.Furthermore, the provided explanations elucidate the rationale behind the ML model's decisions.Different explanation methods with various algorithms and the presentation of their outputs would result in distinct quality levels.To investigate the evaluating capabilities of judges, we design a forward simulation [9] of the iris classification task, where judges predict the target label based on a given input and explanation.To this end, the experiment is set up to determine how judges differ in their assessments of the quality of explanations and to identify any evaluation differences between LLM-based and human judges.</p>
<p>LLMs Prompt Design</p>
<p>In this study, we aim to investigate the capability of LLMs as judges in evaluating the quality of ML explanations.Specifically, we task GPT-4o and Mistral-7.2Bas LLM-based judges in a zeroshot manner to investigate their capabilities in evaluating explanations using both subjective and objective metrics.A crucial element in enabling effective evaluation by LLM-based judges is the design of a well-crafted prompt [34].Therefore, we develop an appropriate prompt design tailored to our iris classification task.We begin by prompting LLM-based judges to determine the iris labels based on the given information, such as explanations.Following this, we use a single answer grading prompt design of LLM-as-a-Judge [36], in which an LLM-based judge is asked to directly assign a score to a single subjective question.This allows LLM-based judges to rate their perceptions of explanations based on the specified subjective metrics.We also include additional instructions to constrain the LLMs our evaluation prompt [7,34].Consequently, our prompt design is structured around three main components, as illustrated in Figure 2:</p>
<p>(1) LLMs role: At the beginning of the prompt design, a comprehensive overview of LLM-based judges' role is provided, emphasizing their task of evaluating explanations.This component makes sure that LLMs grasp their overarching objective, particularly in evaluating XML.(2) Task description: The prompt design includes a detailed description of the tasks assigned to the LLM-based judges, providing clear instructions on the actions they need to perform based on the given information.Specifically, this component instructs the LLMs to first make a prediction of the target iris label and then assign scores to the subjective questions.</p>
<p>Additionally, it outlines the five metrics used for subjective evaluation.(3) Contextual information: The contextual information in our prompt design includes background details on the two iris labels with means of their four features, the target iris instance, and one of the three explanations.This component ensures that the LLMs have all the necessary information to make informed evaluations.</p>
<p>This prompt design involves the systematic arrangement of overall LLMs' behaviour, task description, and contextual information into a coherent format, aligning with the evaluating capabilities of LLM-based judges.</p>
<p>Study Design</p>
<p>To address our two RQs, a study is designed in which judges conduct a tabular-based iris classification task with the assistance of an ML explanation system.In this system, a convolutional neural network (CNN) is implemented to do the classification tasks, and its decisions are explained by three explanation methods (LIME, similarity-based, and without explanation).To test our hypotheses, we first examine the evaluating capabilities of judges across distinct explanations.Additionally, we compare the evaluating capabilities of judges (specifically comparison of LLM-based and human judges) within each explanation.In this study, there are two independent variables.The first variable pertains to various judges, specifically GPT-4o, Mistral-7.2B,and humans.Additionally, the second variable refers to explanation methods, which refer to LIME, similarity-based, and without explanation.The without explanation serves as our baseline explanation method where no explanation is provided.By considering both distinct judges and varying explanations, we establish a total of 9 experimental conditions, as detailed in Table 1.Each judge is required to conduct 18 tasks (3 explanation methods×6 iris instances = 18 tasks).</p>
<p>Evaluation Measurements</p>
<p>To thoroughly evaluate the quality of various ML explanations, we employ a combination of subjective and objective metrics.Our subjective metrics are designed to identify judges' perceptions of the quality of ML explanations during task completion.Besides, our objective metric is designed to focus on measuring judges' behaviours, which serve as indicators of the explanations' quality.</p>
<p>Subjective metrics.In order to measure judges' perceptions of ML explanations, we adopt five subjective statements derived from the measurements proposed by Hoffman et al. [14], ensuring to mitigate subjective bias.These subjective statements measure five distinct aspects including understandability, satisfaction, completeness, usefulness, and trustworthiness, as presented in Table 2. Specifically, each subjective statement is rated using a 5-point Likert scale from a range of 1 (Strongly Disagree) to 5 (Strongly Agree).</p>
<p>Objective metrics.To measure the presence of behaviours linked with the quality of ML explanations, we employ judge accuracy as our objective metric.We introduce this approach that takes into account the judges' decisions across three different explanation methods.Specifically, this approach quantifies the quality level of ML explanations as judge accuracy by measuring the proportion of accurate decisions made by judges out of the total decisions.High human accuracy is paramount for high-quality of ML explanations, as emphasized in XML systems [30].In this work, judge accuracy is defined as the fraction in which judges' decisions are accurate.As a result, the quality level of ML explanations reflects the fraction that judges rely on the provided explanations to make accurate decisions under distinct explanations.A higher fraction implies that judges utilize higher quality levels of ML explanations to complete their decision-making processes.Understandability 1.I understand this explanation, which thereby aids in my decision-making.Satisfication 2. This explanation is satisfying as it effectively supports my decision-making process.</p>
<p>Completeness 3.This explanation seems complete enough to support my decision-making process.</p>
<p>Usefullness 4. This explanation is useful to assist me with decision-making.</p>
<p>Trustworthiness 5. I trust this explanation to guide my decisionmaking process.</p>
<p>Experiments</p>
<p>This section presents the setup for both the LLM-based automatic study and the online user study.It begins with an introduction to the dataset and the ML classifier employed, followed by an overview of the LLMs and ML explanation methods utilized.Finally, it provides a comprehensive discussion of the overall online user study.</p>
<p>, Vol. 1, No. 1, Article .Publication date: March 2024.</p>
<p>Dataset and Classifier</p>
<p>This study employs the iris dataset [10] as the primary data source.The original dataset contains 150 instances with 4 features (sepal length, sepal width, petal length, and petal width) used to determine iris labels (setosa, versicolor, and virginica).To simplify the study, we reduced the dataset to 100 instances, with 70% allocated for training and the remainder for testing, focusing on binary classification between the 'versicolor' and 'virginica' labels.For each iris label, six iris instances (three of each label) are selected from the testing dataset.Each iris instance has its explanations generated by three explanation methods.</p>
<p>In our work, a CNN model is employed for iris classification.The CNN is configured with three hidden layers, consisting of 5, 4, and 3 neurons respectively, and utilized activation functions sigmoid, Rectified Linear Unit (ReLU), and tanh in each layer.The output layer contained 2 neurons with a softmax activation function.This configuration achieved a model accuracy of 93%.</p>
<p>LLMs and Explainers</p>
<p>To explore the evaluating capabilities of LLM-based judges on various explanations, we select two representative proprietary and open-source language models: GPT-4o and Mistral-7.2B.These models are chosen due to their widespread adoption and extensive documentation.We obtain 38 model responses (matching the number of participant responses) by setting a temperature of 0.7 for both GPT-4o and Mistral-7.2Bthroughout their respective APIs.To ensure consistency with human evaluations, the settings for LLM-based judges mirror those for human judges, including the provision of iris background information, target iris instances, and explanations, along with instructions and evaluation measurements.Both LLMs are prompted using our tailored prompt design to complete 18 tasks.</p>
<p>To broaden the scope of the study in XML, we choose two prominent explainers from different categories: LIME, a feature-based method, and similarity-based explanations, an exemplar method.Specifically, LIME employs a surrogate model to interpret the predictions of the underlying task model by generating data samples with slight input perturbations [29].Besides, similarity-based identifies the instances in the training set that are similar to the test instance in question and their corresponding model predictions [13].Both explanation methods are implemented with default hyperparameter settings.In our data source, LIME and similarity-based explanation methods achieve a 93% and 96% accuracy, respectively.The labels of the selected six instances predicted by two explanations are aligned with their true labels, ensuring the correct explanations are provided to judges.</p>
<p>Online User Study</p>
<p>To compare the evaluating capabilities of LLM-based judges to those of humans, an online user study is conducted via the Qualtrics platform.This user study, which is approved by the Human Research Ethics Committee (HREC) of our university, takes approximately 15-20 minutes to complete.Figure 3 presents an example of the interface used in the online user study 5.3.1 Participants.38 participants were invited to take part in our online user study through various communication channels, such as social media posts and emails.The participants, who were primarily researchers and students, included 17 females, 18 males, and a few who chose not to disclose their gender.The majority of participants were between their twenties and forties, with an average age of 29 years.In terms of educational background, 4 participants held bachelor's degrees, followed by 10 participants who held Ph.D. degrees, 22 participants who completed their master's degrees, and the remaining participants had their under bachelor's degrees or honors qualifications.</p>
<p>Procedure.</p>
<p>At the beginning of the online study, participants are provided with a welcome page describing the objectives of this study.Following this, upon agreeing to take part in this study with a consent page, participants can formally participate in this study.Subsequently, an experiment introduction page is presented to clarify participants' roles and tasks briefly.</p>
<p>Participants then start one random task of the 16 tasks.For each main task, participants would see: (1) a background table about two iris labels with means of their four features, (2) a target iris instance, and (3) an additional explanation based on the explanation methods used in this study.Based on the provided information, participants are asked to make their own predictions about the target iris label.We record participants' predictions where participants predict correct target iris instance labels as a way of calculating participants' accuracy in the objective aspect.Additionally, participants are tasked to rate their perceptions of quality levels concerning the explanations using a 5-point Likert scale in the subjective aspect.During the conduction of 16 tasks, participants can not go back to change their past responses.After completing 16 tasks, participants are then required to complete a demographics page with three questions: age, gender, and education level.Participation in the user study is voluntary for each participant.</p>
<p>Results</p>
<p>In this section, we provide a detailed examination of how each judge assesses the quality of different explanations(RQ1), in Section 6.1 and how LLM-based judges compare against human judges in the evaluation of explanations (RQ2) in Section 6.2.Furthermore, we conduct a comprehensive analysis of the results from subjective and objective metrics, employing statistical tests such as one-way ANOVA and Tukey's HSD post-hoc tests.</p>
<p>Evaluation of Explanations by Each Judge (RQ1)</p>
<p>To address RQ1 (H1), we engage different judges to evaluate explanations using subjective and objective metrics.Subsequently, we apply a one-way ANOVA followed by Tukey HSD post-hoc tests.These statistical analyses aim to quantify differences in the quality of diverse explanations when assessed by the same judge (GPT-4o, Mistral-7.2B,and humans respectively) using subjective and objective metrics.The results from each subjective metric reveal significant differences in quality assessed by judges across a variety of explanations.However, the results from objective metrics indicate statistically significant differences in quality across different explanations are found in the evaluations of Mistral-7.2Band humans, rather than that of GPT-4o.</p>
<p>Table 3.Each Judge Evaluation across Explanation Methods of One-way ANOVA.In this table, F, representing the F value, aligns with the degrees of freedom (2, 111), and p, indicating the p-value, refers to the probability that the differences between LIME, similarity-based, and without explanation.The p-values are less than .05are highlighted, indicating a statistically significant difference.4, showcasing the average scores of the judges across three explanations.Using these scores, the one-way ANOVA and post-hoc Tukey HSD tests are conducted for each subjective statement to explore variations in quality assessments of each judge on the evaluation of different explanations.</p>
<p>GPT-4o</p>
<p>Understandability.The analysis of the one-way ANOVA results shows there are significant differences in understandability assessed by GPT-4o, Mistral-7.2B,and humans based on Table 3.The post-hoc tests via Tukey HSD, as shown in Table 4, further find that, for GPT-4o, without , Vol. 1, No. 1, Article .Publication date: March 2024.</p>
<p>explanation is significantly different from LIME and similarity-based, and similarity-based is significantly different from LIME, thereby supporting H1.1.Furthermore, for both Mistral-7.2Band humans, Tukey HSD tests illuminate that without explanation is significantly different from LIME and similarity-based, confirming H1.2 and H1.3.These outcomes suggest that judges effectively assess the understandability regarding different explanations, thereby supporting H1.LIME for GPT-4o, accepting H1.1.Similarly, for both Mistral-7.2Band humans, the Tukey HSD tests find without explanation is significantly different from LIME and similarity-based, hence supporting H1.2 and H1.3.The results imply that judges effectively evaluate the satisfaction among different explanations, thus accepting H1.</p>
<p>Completeness.According to the one-way ANOVA (Table 3), significant variations in completeness are observed for GPT-4o, Mistral-7.2B,and humans resepctively.Furthermore, we evaluate the differences in each judge by Tukey HSD (Table 4.For both GPT-4o and Mistral-7.2B,we find without explanation is significantly different from LIME and similarity-based, and similarity-based is significantly different from without explanation, thereby supporting H1.1 and H1.2.For humans, we also find that without explanation is significantly different from LIME and similarity-based, confirming H1.3.The results imply that judges rate completeness differently across explanations, hence supporting H1.</p>
<p>Usefulness.The ANOVA tests (see Table 3) also reveal statistically significant differences in usefulness for all judges (GPT-4o, Mistral-7.2B,and humans).Additionally, the Tukey HSD tests (see Table 4) elucidate that without explanation exhibits significant differences from LIME and similaritybased for all judges, while similarity-based shows significant differences from LIME for LLM-based judges (GPT-4o and Mistral-7.2B).These findings support H1.1, H1.2, and H1.3, suggesting that judges evaluate usefulness differently across various explanations, thereby confirming H1.</p>
<p>Trustworthiness.The one-way ANOVA results (Table 3) detect that there are significant differences in trustworthiness for GPT-4o, Mistral-7.2B,and humans respectively.The Tukey HSD tests (Table 4) further show that for GPT-4o and Mistral-7.2B,without explanation significantly differs from both LIME and similarity-based, and similarity-based significantly differs from LIME, confirming H1.1 and H1.2.Besides, for humans, the Tukey HSD tests find that similarity-based significantly differs from LIME, accepting H1.3.The outcomes present that judges assess trustworthiness differently across explanations, hence supporting H1.</p>
<p>Objective Evaluation of Explanations by Each</p>
<p>Judge.Quality, quantified by accuracy, is presented in Figure 4, illustrating the average accuracy of the judges across three explanations.The one-way ANOVA and Tukey HSD tests are employed to investigate variations in accuracy of each judge on the evaluation of explanations.Based on Table 3, the ANOVA shows no statistically significant accuracy difference across the distinct explanations for GPT-4o, leading to the rejection of H1.1.</p>
<p>However, a one-way ANOVA test indicates that significant variations in accuracy are observed for Mistral-7.2Band humans.As shown in Table 4, Post-hoc Tukey HSD tests further indicate that without explanation is significantly different from both LIME and similarity-based for both Mistral-7.2Band humans, while similarity-based is significantly different from LIME solely for Mistral-7.2B.This supports H1.2 and H1.3, suggesting that Mistral-7.2Band humans assess accuracy differently across various explanations.Thus, these findings provide partial support for H1.</p>
<p>Overall, these findings demonstrate that while judges assess the quality of different explanations differently based on subjective metrics, GPT-4o fails to evaluate the quality of ML explanations using the objective metric.Thus, our H1 is partially accepted.</p>
<p>Comparison of Evaluations by Different Judges (RQ2)</p>
<p>To address RQ2 (H2), we compare the evaluations of judges when assessing the same explanations (LIME, similarity-based, and without explanation) using both subjective and objective metrics.We then conduct a one-way ANOVA followed by Tukey HSD post-hoc tests to analyze the results.Our analysis reveals significant differences in quality across different judges for each explanation  based on most subjective metrics, rather than relying on the subjective understandability metric.Furthermore, the results from objective metrics observe there are significant differences in quality across different judges for each explanation.</p>
<p>Subjective Comparison of Evaluations by Different</p>
<p>Judges.The one-way ANOVA and posthoc Tukey HSD tests are conducted for each subjective metric to examine consistency in quality assessments by different judges for each explanation (see Figure 4).</p>
<p>Understandability.According to the results of the ANOVA tests (Table 5), we find no significant differences in understandability across judges both in LIME and similarity-based, hence accepting H2.1 and H2.2; however, there are significant differences in without explanation.Furthermore, we evaluate the differences in without explanation by Tukey HSD (Table 6).We find GPT-4o is significantly different from Mistral-7.2Band humans, supporting H2.3.These findings show that GPT-4o's understandability differs from that of humans in without explanation, resulting in a partial acceptance of H2.</p>
<p>Satisfaction.The one-way ANOVA tests detect that there are significant differences in satisfaction in LIME, similarity-based, and without explanation, respectively (see Table 5).Post-hoc Tukey HSD tests further indicate that humans exhibit significantly different satisfaction compared to Mistral-7.2B in LIME and compared to GPT-4o in similarity-based (see Table 6), leading to the rejection of H2.1 and H2.2.Besides, the Tukey HSD tests find GPT-4o exhibits significant differences  Completeness.The ANOVA tests, presented in Table 5, reveal that significant variations in completeness are observed across judges in all three explanations.The post-hoc tests via Tukey HSD, detailed in Table 6, elucidate that differs significantly from GPT-4o and humans in LIME, leading to the rejection of H2.1.Additionally, the Tukey HSD tests find that GPT-4o differs significantly from humans in similarity-based and from both Mistral-7.2Band humans in without explanation, resulting in the rejection of H2.2 and H2.3.The outcomes suggest that completeness assessed by LLM-based judges differs significantly from that assessed by human judges, thus rejecting H2.</p>
<p>Usefulness.The one-way ANOVA results reveal there are significant differences in usefulness across judges in LIME, similarity-based, and without explanation, respectively (as illustrated in Table 5).Post-hoc via Tukey HSD tests, based on Table 6, show that humans are significantly different from Mistral-7.2B in LIME, leading to the rejection of H2.1.In similarity-based, GPT-4o is significantly different from humans, resulting in the rejection of H2.2.Moreover, in without explanation, GPT-4o is significantly different from both Mistral-7.2Band humans, thus rejecting H2.3.The outcomes imply that the assessment of usefulness in ML explanations varies between LLM-based and human judges, thus leading to the rejection of H2.</p>
<p>Trustworthiness.Regarding Table 5, the one-way ANOVA tests show that there are significant variations in trustworthiness among judges in LIME, similarity-based, and without explanation.Based on Table 6, the Tukey HSD tests show that Mistral-7.2Bdiffers significantly from GPT-4o and humans in LIME, leading to the rejection of H2.1.In similarity-based, GPT-4o is significantly different from humans in similarity-based, rejecting H2.2.Besides, in without explanation, GPT-4o differs significantly from both Mistral-7.2Band humans, and Mistral-7.2Balso differs significantly from humans, rejecting H2.3.These results demonstrate that the evaluation of trustworthiness by LLM-based judges differs from that of human judges, thereby rejecting H2.</p>
<p>Objective</p>
<p>Comparison of Evaluations by Different Judges.Using our objective metric -accuracy, we conduct the one-way ANOVA and Tukey HSD tests to examine consistency in accuracy evaluated by different judges when assessing each explanation (see Figure 4).The ANOVA analysis (see Table 5) reveals that significant differences in accuracy are observed among the judges in each explanation.Post-hoc Tukey HSD (see Table 6) tests show that GPT-4o is significantly different compared to Mistral-7.2Band humans in LIME.Additionally, humans are significantly different from Mistral-7.2B in this explanation.These findings lead to the rejection of H2.1.In similarity-based, Tukey HSD finds that humans are significantly different compared to GPT-4o and Mistral-7.2B,rejecting H2.2.Besides, in without explanation, GPT-4o is significantly different compared to both Mistral-7.2Band humans, and humans are also significantly different from Mistral-7.2B,thereby rejecting H2.3.These findings indicate that the accuracy in evaluating ML explanations differs between LLM-based and human judges, thus leading to the rejection of H2.</p>
<p>In summary, the results indicate that the evaluating capabilities of judges are comparable in LIME and similarity-based according to the subjective understandability metric, rather than relying on the other subjective metrics.Besides, regarding the objective metrics, significant differences are observed in the evaluation of the same explanation across judges.As a result, our H2 is partially rejected.</p>
<p>Discussion</p>
<p>In this section, we first have a comprehensive description of our research findings.Also, we provide a discussion of the implications of these results.At last, we delineate a reflection of the limitations and future directions of our study.</p>
<p>Findings</p>
<p>In this paper, we propose a workflow that incorporates judges (GPT-4o, Mistral-7.2B,and humans) to evaluate the quality of explanations.We conduct an experiment using iris classification and three explanation methods-LIME, similarity-based, and a baseline 'without explanation'-to investigate and compare the capabilities of different judges.</p>
<p>Results show that while judges effectively assess the quality of different explanation methods based on subjective metrics, judges fail it according to objective metrics.Specifically, Mistral-7.2Band humans exhibit significant differences in accuracy among the different explanations, however, GPT-4o does not show significant differences in accuracy across the explanations.Hence, based on these findings, our H1 is partially affirmed.</p>
<p>Regarding the evaluating capabilities of LLM-based judges compared to those of humans, their capabilities are comparable in LIME and similarity-based when assessed using the subjective understandability metric, rather than relying on other subjective metrics.Furthermore, significant differences are observed across judges concerning the same explanation when evaluated using objective metrics.Consequently, H2 is partially rejected.</p>
<p>In summary, our experiment first confirms the capabilities of judges for evaluating the quality of different explanations are significantly different primarily through subjective metrics.On the other hand, LLM-based judges' evaluating capabilities of ML explanations are significantly different from those of humans according to subjective (satisfaction, completeness, usefulness, and trustworthiness) and objective metrics; however, this is less evident when using the subjective understandability metric in LIME and similarity-based.</p>
<p>Implications</p>
<p>The experiment outcomes reveal clear insights into the capabilities of LLM-based judges in assessing ML explanations.However, our findings indicate that LLM-based judges are not yet capable of fully replacing human judges.Since the goals of explainability methods are inherently human-centric, human-centered evaluations remain essential to the assessment of ML explanations.Instead, LLMs should be viewed as valuable tools that complement traditional human evaluations due to their human-like capabilities.Based on our analysis of subjective metrics, LLM-as-a-Judge proves to be a useful and effective tool in evaluating ML explanations.Specifically, LLMs are able to identify that the baseline method (without explanation) exhibits a lower quality level compared to both LIME and similarity-based explanations subjectively.</p>
<p>Moreover, our findings reveal the evaluation of ML explanation contexts where LLM-based evaluations differ significantly from human evaluations based on most of our subjective and objective metrics.These insights are indispensable for programmers and designers who work on LLMs.While LLM-as-a-Judge is increasingly applied in many evaluating fields, it is important to note that LLMs are not actual humans and are prone to systematic biases that differ from those of human judges.This is particularly evident in our case -the classification of iris instances under the without explanation method considering the objective aspect.This underscores the need for ongoing improvements in LLMs algorithms to enhance their human-like comprehension and evaluation capabilities.Ultimately, with further refinement, LLMs could become a cost-efficient alternative to traditional human evaluation methods.</p>
<p>Besides, our findings reveal their judgment capabilities are comparable in specific dimensions such as subjective understandability metric for both LIME and similarity-based, rather than based on other metrics (see Table 5).This highlights insights for people who focus on the evaluation of ML explanations that further exploration is necessary before directly adopting LLMs to assess ML explanations.</p>
<p>Limitations and Future Directions</p>
<p>Though our experiment unveils significant findings about the evaluating capabilities of LLMs of the ML explanations, several inherent limitations in the experimental design should be acknowledged.One such limitation is that our dataset for the experiment is tabular data (iris dataset) with only four features.Real XML systems and LLM-based judgments often deal with more complex data types and feature sets, potentially leading to greater variability in outputs.Therefore, future research should explore XML systems and LLMs evaluations using a variety of data types (such as images or text) and more extensive feature sets.Although studying more complex scenarios poses challenges, it presents valuable opportunities to examine LLM-based judgments across diverse contexts and determine whether significant differences arise.</p>
<p>Another limitation is the reliance on high-accuracy explanation methods for evaluating the quality of explanations.However, the XML systems may generate explanations with low accuracy in practice, which is acceptable if the ML model's accuracy is also low.While existing research highlights that accuracy impacts individual explanations [3,22], other properties such as robustness and novelty also play crucial roles in determining explanation quality.Future research should investigate how LLMs assess explanations that exhibit lower accuracy and evaluate additional properties of explanations to provide a more comprehensive assessment of their capabilities.</p>
<p>Conclusion</p>
<p>The human-based experiment is one of the foolproof methods of evaluating Ml explanation methods due to the goals of explanation methods being human-centric.However, human-subject experiments always require time and cost to conduct.With the recent advances and human-like capabilities in LLMs, we propose a workflow comprising LLM-based and human judges to study the correlation between LLM-based and human judgments in the evaluation of ML explanations.To achieve this, we conduct an experiment where judges evaluate the quality of different ML explanations based on the iris classifications, employing a combination of subjective and objective metrics.Our results show several key insights: 1) judges (LLM-based and humans) effectively assess the quality of different ML explanations supported by subjective metrics, contrary to the results from objective metrics; 2) the evaluating capabilities of LLM-based judges are significantly different from those of humans in three explanations respectively based on subjective and objective metrics; however, this is evident beyond the subjective understandability metric in LIME and similarity-based.As a result, we conclude that while LLM-based judges are capable of evaluating explanations through subjective metrics, they are not yet sufficiently developed to replace human judges in this role.</p>
<p>, Vol. 1 ,
1
No. 1, Article .Publication date: March 2024.</p>
<p>Fig. 1 .
1
Fig. 1.Workflow of Judges for Evaluating Explanations.</p>
<p>Fig. 2 .
2
Fig. 2. The Prompt for LLMs Evaluating Explanations.We provide (1) LLMs role, (2) task description, and (3) contextual information</p>
<p>' output: [(<instance>, <label>), (<Question 1>, <integer score>), ...(<Question 5>, <integer score>)].In order to ensure LLMs generate the desired outputs, we iteratively refine , Vol. 1, No. 1, Article .Publication date: March 2024.</p>
<p>, Vol. 1 ,
1
No. 1, Article .Publication date: March 2024.</p>
<p>Fig. 3 .
3
Fig. 3.An Interface Example of Tasks in the Online User Study.</p>
<p>Fig. 4 .
4
Fig. 4. Results for Judges across Explanations Based on Subjective and Objective Metrics.In this figure, error bars represent the 95% confidence interval of a mean.The (a), (b), (c), (d), and (e) refer to subjective metrics including understandability, satisfaction, completeness, usefulness, and trustworthiness, respectively.The (f) refers to the objective metric -accuracy.</p>
<p>, Vol. 1 ,
1
No. 1, Article .Publication date: March 2024.</p>
<p>, Vol. 1 ,
1
No. 1, Article .Publication date: March 2024.</p>
<p>, Vol. 1 ,
1
No. 1, Article .Publication date: March 2024.</p>
<p>Table 1 .
1
Experiment designed conditions
ExplainersJudgesLIMESimilarity-basedWithoutGPT-4oCondition 1Condition 2Condition 3Mistral-7.2B Condition 4Condition 5Condition 6HumanCondition 7Condition 8Condition 9</p>
<p>Table 2 .
2
Five Subjective Statements for Measuring Quality of ML Explanations</p>
<p>Table 4 .
4
Each Judge Evaluation across Explanation Methods of Tukey HSD.In this table,  1 ,  2 , and  3 refer to the p-value for comparison between similarity-based and LIME, without explanation and LIME, and without explanation and similarity-based, respectively.The p-values are less than .05arehighlighted,indicatingastatistically significant difference.The one-way ANOVA tests, as illustrated in Table3, detect there are significant differences in satisfaction evaluated by GPT-4o, Mistral-7.2B,andhumansrespectively.Post-hoc Tukey HSD tests, as shown in Table4, further indicate that without explanation is significantly different from both LIME and similarity-based, and similarity-based is significantly different from
GPT-4o Mistral-7.2B Human
, Vol. 1, No. 1, Article .Publication date: March 2024.</p>
<p>Table 5 .
5
(2,parision Evaluations for Each Explanation of One-way ANOVA.In this table, F, representing the F value, corresponds to the degrees of freedom(2, 111), and p, indicating the p-value, refers to the probability that the observed differences between GPT-4o, Mistral-7.2B,and humans.The p-values are less than .05are highlighted, indicating a statistically significant difference.
LIME Similarity-based WithoutUnderstandabilityF 1.672.6368.07p &gt; .050&gt; .050&lt; .000SatisfactionF 3.674.8863.40p = .029= .009&lt; .000SubjectiveCompletenessF 23.534.2953.40p &lt; .000= .016&lt; .000UsefulnessF 3.774.0757.79p = .026= .020&lt; .000TrustworthinessF 20.387.3657.66p &lt; .000&lt; .001&lt; .000ObjectiveAccuracyF 64.4828.4688.72p &lt; .000&lt; .000&lt; .000</p>
<p>Table 6 .
6
Comparision Evaluations for Each Explanation of Tukey HSD.In this table,  1 ,  2 , and  3 refer to the p-value for comparison between human and GPT-4o, Mistral-7.2Band GPT-4o, and Mistral-7.2Band human, respectively.The p-values are less than .05are highlighted, indicating a statistically significant difference.
LIME Similarity-based Without
, Vol. 1, No. 1, Article . Publication date: March 2024.
AcknowledgmentsThe ethical approval for our research is granted by the HREC of the University of Technology Sydney with the number ETH22-7616.We thank the participants who took part in our studies.
Transformer models for text-based emotion detection: a review of BERT-based approaches. Francisca Adoma, Acheampong , Henry Nunoo-Mensah, Wenyu Chen, Artificial Intelligence Review. 542021. 2021</p>
<p>Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, arXiv:2406.18403LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks. 2024. 2024arXiv preprint</p>
<p>Eduardo M Diogo V Carvalho, Jaime S Pereira, Cardoso, Machine learning interpretability: A survey on methods and metrics. 2019. 20198832</p>
<p>Input similarity from the neural network perspective. Guillaume Charpiat, Nicolas Girard, Loris Felardos, Yuliya Tarabalka, Advances in Neural Information Processing Systems. 322019. 2019</p>
<p>Credit Card Fraud Detection via Intelligent Sampling and Self-supervised Learning. Chiao-Ting Chen, Chi Lee, Szu-Hao Huang, Wen-Chih Peng, ACM Transactions on Intelligent Systems and Technology. 152024. 2024</p>
<p>Fairness via explanation quality: Evaluating disparities in the quality of post hoc explanations. Jessica Dai, Sohini Upadhyay, Ulrich Aivodji, Stephen H Bach, Himabindu Lakkaraju, Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. the 2022 AAAI/ACM Conference on AI, Ethics, and Society2022</p>
<p>Conversing with copilot: Exploring prompt engineering for solving cs1 problems using natural language. Paul Denny, Viraj Kumar, Nasser Giacaman, Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. the 54th ACM Technical Symposium on Computer Science Education V20231</p>
<p>Yijiang River, Dong , Tiancheng Hu, Nigel Collier, arXiv:2406.11657Can LLM be a Personalized Judge?. 2024. 2024. March 20241arXiv preprint</p>
<p>Towards a rigorous science of interpretable machine learning. Finale Doshi, - Velez, Been Kim, arXiv:1702.086082017. 2017arXiv preprint</p>
<p>R A Fisher, 10.24432/C56C76Iris. UCI Machine Learning Repository. 1988</p>
<p>Comprehensible classification models: a position paper. Alex A Freitas, ACM SIGKDD explorations newsletter. 152014. 2014</p>
<p>The effect of machine learning explanations on user trust for automated diagnosis of COVID-19. Kanika Goel, Renuka Sindhgatta, Sumit Kalra, Rohan Goel, Preeti Mutreja, Computers in Biology and Medicine. 1461055872022. 2022</p>
<p>Evaluation of Similarity-based Explanations. Kazuaki Hanawa, Sho Yokoi, Satoshi Hara, Kentaro Inui, arXiv:2006.045282021cs, stat</p>
<p>Robert R Hoffman, Shane T Gary Klein, Jordan Litman, arXiv:1812.04608Metrics for Explainable AI: Challenges and Prospects. 2019</p>
<p>Taojun Hu, Xiao-Hua Zhou, arXiv:2404.09135Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions. 2024. 2024arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023. 2023Mistral 7B. arXiv preprint</p>
<p>Optimal treatment strategies for critical patients with deep reinforcement learning. Simi Job, Xiaohui Tao, Lin Li, Haoran Xie, Taotao Cai, Jianming Yong, Qing Li, ACM Transactions on Intelligent Systems and Technology. 152024. 2024</p>
<p>Charles Koutcheme, Nicola Dainese, Sami Sarsa, Arto Hellas, Juho Leinonen, Paul Denny, arXiv:2405.05253Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge. 2024</p>
<p>Sangwon Lee, Junho Hong, Ling Liu, Wonik Choi, TS-Fastformer: Fast Transformer for Time-Series Forecasting. 2024. 202415</p>
<p>Human-grounded evaluations of explanation methods for text classification. Piyawat Lertvittayakumjorn, Francesca Toni, arXiv:1908.113552019. 2019arXiv preprint</p>
<p>Swin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>A meta survey of quality evaluation criteria in explanation methods. Helena Löfström, Karl Hammar, Ulf Johansson, International Conference on Advanced Information Systems Engineering. Springer2022</p>
<p>A unified approach to interpreting model predictions. M Scott, Su-In Lundberg, Lee, 2017. 201730Advances in neural information processing systems</p>
<p>Large language model (llm) ai text generation detection based on transformer deep learning algorithm. Yuhong Mo, Yushan Hao Qin, Ziyi Dong, Zhenglin Zhu, Li, arXiv:2405.066522024. 2024arXiv preprint</p>
<p>Deep learning in single-cell analysis. Dylan Molho, Jiayuan Ding, Wenzhuo Tang, Zhaoheng Li, Hongzhi Wen, Yixin Wang, Julian Venegas, Wei Jin, Renming Liu, Runze Su, ACM Transactions on Intelligent Systems and Technology. 152024. 2024</p>
<p>. Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, Jörg Schlötterer, Maurice Van Keulen, Christin Seifert, 10.1145/3583558From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI. Comput. Surveys. 552023. Dec. 2023</p>
<p>Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen, arXiv:2406.00936A Survey of Useful LLM Evaluation. 2024. 2024arXiv preprint</p>
<p>Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, Urmish Thakkar, arXiv:2408.08808Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge. 2024. 2024arXiv preprint</p>
<p>Explaining the predictions of any classifier. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin, Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on knowledge discovery and data mining2016Why should i trust you?</p>
<p>Anchors: High-Precision Model-Agnostic Explanations. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin, 10.1609/aaai.v32i1.11491Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2018. April 201832</p>
<p>Shreya Shankar, Björn Zamfirescu-Pereira, Aditya G Hartmann, Ian Parameswaran, Arawjo, arXiv:2404.12272Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. 2024. 2024arXiv preprint</p>
<p>Attention is all you need. Vaswani, Advances in Neural Information Processing Systems. 2017. 2017</p>
<p>Impact of Fidelity and Robustness of Machine Learning Explanations on User Trust. Bo Wang, Jianlong Zhou, Yiqiao Li, Fang Chen, Australasian Joint Conference on Artificial Intelligence. Springer2023. March 20241Publication date</p>
<p>Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C Schmidt, arXiv:2302.11382A prompt pattern catalog to enhance prompt engineering with chatgpt. 2023. 2023arXiv preprint</p>
<p>Yadav, Generative AI in the Era of Transformers: Revolutionizing Natural Language Processing with LLMs. 2024</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Yonghao Wu, Zi Zhuang, Zhuohan Lin, Dacheng Li, Eric P Li, Hao Xing, Joseph E Zhang, Gonzalez, arXiv:2306.05685and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. </p>
<p>Evaluating the quality of machine learning explanations: A survey on methods and metrics. Jianlong Zhou, H Amir, Fang Gandomi, Andreas Chen, Holzinger, Electronics. 105932021. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
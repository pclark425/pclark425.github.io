<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9327 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9327</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9327</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-257532405</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-emnlp.710.pdf" target="_blank">Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. Therefore, we conclude that LLMs are not effective few-shot information extractors in general. Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our preliminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9327.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9327.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction format variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction prompt format (I0..I5 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variants of textual task instructions ranging from empty to detailed definitions and role framing (six variants I0–I5) used in in-context learning prompts to describe the IE task and enumerate labels or definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo series)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot Information Extraction (examples: FewNERD NER, TACREV RE, ACE E D/EAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Named Entity Recognition (NER), Relation Extraction (RE), Event Detection (ED) and Event Argument Extraction (EAE) in few-shot settings (20-shot reported for instruction ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Six instruction variants from empty (I0) to simple directive (I1) to progressively more detailed instructions including label definitions and role framing (I2..I5). Prompts follow ICL style: [Instruction; demonstrations; test example].</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Each instruction variant compared against the others (I0..I5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples (20-shot, ChatGPT): FewNERD F1: I0=57.6, I1=58.3, I2=57.7, I3=57.6, I4=56.8, I5=57.8; TACREV F1s vary similarly (approx. 49.1–52.3 across variants); ACE ED/EAE show small fluctuations (see Table 12).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Small — variations on the order of ±~1–2 F1 points across instruction variants (no consistent large gains from complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper finds no strong correlation between instruction complexity and LLM performance on IE; simple instructions (I1) are as effective as complex ones. Authors thus use simple instruction in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>20-shot settings; ChatGPT evaluated on FewNERD (NER), TACREV (RE), ACE (ED/EAE); prompts include demonstrations and the instruction variant; averaged over runs reported in Table 12.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9327.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9327.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Demo number</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number of demonstrated examples in ICL (demo count)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variation in the number of in-context demonstration examples fed to the LLM during ICL; effects differ by task and model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT, CODEX, LLaMA-13B, Vicuna-13B (evaluated separately)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B for LLaMA/Vicuna where specified; OpenAI model sizes unspecified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot IE (NER, RE, ED)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate how increasing the number of demo examples in the prompt affects F1 on IE tasks under few-shot regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ICL prompts varying demo count (plots show demo counts up to the maximum fit in context; Figure 8 presents demo-number vs F1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared many demo counts (e.g., 1, 2, 4, 8, ... up to context limit) per task and model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Task-dependent: RE performance generally increases with more demos; NER and ED reach a plateau or degrade with more demos; open-source LLMs (LLaMA, Vicuna) saturate or collapse with only a few demos (2–4). (See Figure 8 and E.3 for qualitative trends.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Variable: RE shows consistent improvement as demos increase; NER/ED show little-to-no improvement or slight degradation beyond small demo counts. No single numeric effect size universally reported.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Some IE tasks (e.g., RE) benefit from more annotated context, while others (NER/ED) are limited by prompt length or suffer from example dilution; open-source LLMs have more limited capacity to leverage many demos.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Experiments used sentence-embedding demo selection; x-axis is number of demos (not shot K); models evaluated with greedy decoding (temperature=0). Open-source models used max input length 2048 and greedy decoding; OpenAI models used larger context if available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9327.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9327.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Demo selection strategy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demo selection: Random vs Sentence-embedding retrieval vs Efficient Prompt Retriever (EPR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three methods to pick demonstration examples: random sampling, retrieval by sentence-embedding similarity (SimCSE-based), and a learned Efficient Prompt Retriever (neural retriever with contrastive learning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (mainly reported), with retrieval using SimCSE-RoBERTa-large embeddings or EPR components</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot IE (FewNERD NER, TACREV RE, ACE ED)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Selecting which K-shot examples are included as demos for per-test-sentence ICL prompts, and measuring resulting F1.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ICL where demos per test sample are selected by one of: (1) random sampling, (2) top-K nearest by sentence embedding (SimCSE-RoBERTa-large), or (3) Efficient Prompt Retriever (pre-retrieve M sentences then neural scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random vs Embedding vs EPR compared directly (Table 13).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 13 (20-shot, ChatGPT): FewNERD F1 — Random 57.2(0.6), Sentence Embedding 57.6(2.3), EPR 57.6(2.3). TACREV F1 — Random 48.0(0.8), Sentence Embedding 53.2(0.4), EPR 43.0(3.3). ACE05 ED F1 — Random 43.5(1.4), Sentence Embedding 49.6(1.2), EPR 42.9(1.3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Embedding retrieval often substantially outperforms random (e.g., TACREV and ACE05 show +~5 F1); EPR sometimes underperforms embedding in reported numbers but both embedding and EPR generally surpass random.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Retrieval of semantically similar demonstrations yields better in-context signals; sentence-embedding retrieval is an effective and simple strategy, so the authors adopt it for main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Sentence embeddings computed by SimCSE-RoBERTa-large; EPR trained with in-batch contrastive learning, M=40 pre-retrieved, K_D=5, FLAN-T5-xl as scoring LM; experiments typically 20-shot with ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9327.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9327.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt format: text vs code</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt output formatting: simple textual templates vs code-like output templates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two prompt output representation styles: a plain text template presenting Sentence/Entities/etc., and a code-like output representation (used in prior IE LLM work) to structure outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (evaluated), other LLMs in background</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot IE (FewNERD NER, TACREV RE, ACE ED, ACE EAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Comparison of different output representation prompt formats for structured IE outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Text prompt example: 'Sentence: [S], Entities: ([type1],[entity1]), ...' vs Code prompt: recasting outputs as code-like structures (as in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Text prompt vs Code prompt (Table 14).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 14 (20-shot, ChatGPT): FewNERD F1 Text 57.6(2.3) vs Code 53.2(0.9); TACREV Text 49.6(1.2) vs Code 50.2(1.8); ACE ED Text 42.9(1.3) vs Code 44.3(2.0); ACE EAE Text 51.5(1.1) vs Code 47.3(1.5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Mixed small differences across tasks; sometimes text > code (FewNERD), sometimes code slightly better (ACE ED). No uniformly large advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Different prompt formats yield comparable performance; authors choose text-format for simplicity in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>20-shot ChatGPT experiments; prompts differ only in output representation; averaged F1s in Table 14.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9327.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9327.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sampling temperature</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoding sampling temperature (t) setting for generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Choice of decoding temperature (deterministic greedy t=0 vs stochastic t>0) for LLM output generation in structured IE tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CODEX (code-davinci-002) and others</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot IE (10-shot reported for temp ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assessing the impact of decoding randomness on quality of structured outputs for IE tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Greedy decoding with temperature t=0 vs stochastic sampling with t>0 (and experiments with self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>t = 0 compared to t = 0.7 (and t>0 variants with/without self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report deterministic decoding (t=0) yields substantially better generated quality for structured IE outputs; thus they set t=0 in main experiments (details in D.1 and Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Structured IE outputs require consistent token sequences; stochastic sampling increases noise and reduces parsing reliability, so greedy decoding is preferred.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Temperature ablation used CODEX on 10-shot settings. Self-consistency variants (using stochastic decoding multiple times) were considered but authors set t=0 for main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9327.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9327.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) and Auto-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting and Automatic Chain-of-Thought (Auto-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CoT: including step-by-step rationales/explanations in demonstrations; Auto-CoT: automatically generated rationales inserted as context via an LLM-generated bootstrap procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (used to generate Auto-CoT rationales), CODEX and InstructGPT used in evaluation scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot IE (e.g., TACREV, FewNERD, ACE05)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate whether providing explanations (CoT) in demonstrations or auto-generated rationales (Auto-CoT) helps LLM performance on IE.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Demonstrations optionally prefaced with explanations (CoT). Auto-CoT pipeline: generate rationales for positive examples using another LLM and include them in demos.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>With CoT vs without CoT; with Auto-CoT vs without Auto-CoT (Table 5 and D.2/D.3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Ablation (Table 5) shows modest gains from including human-crafted CoT in the adaptive reranker (e.g., FewNERD 20-shot: full system 63.6 F1 with CoT; removing CoT reduces to 63.2). Auto-CoT experiments (Table 9) show Auto-CoT degraded performance substantially in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>CoT: small positive effect in reranking pipeline (fractions of F1 points). Auto-CoT: negative effect sometimes large (authors report substantial degradation).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT can help LLM rerankers by making reasoning explicit. Auto-generated rationales hurt because they increase demo length (reducing number of demos), are inconsistent (only for positive examples), and may be low-quality especially for RE.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Adaptive reranker uses CoT prefacing answers in demos; Auto-CoT used InstructGPT to generate rationales with temperature 0.7; Auto-CoT hurt performance so authors did not use it in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9327.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9327.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCQ reranking / Filter-then-rerank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Filter-then-rerank adaptive paradigm using Multi-Choice Question (MCQ) prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage system: SLMs (fine-tuned small models) filter and produce top-N candidate labels; LLMs rerank these candidates using MCQ-formatted prompts, applied only to samples deemed 'hard' by SLM confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Filter: RoBERTa-large (SLMs); Rerankers: InstructGPT, Vicuna-13B, GPT-4 (evaluated variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Vicuna-13B (13B), LLaMA-13B noted elsewhere; SLMs based on RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot IE tasks: FewNERD (NER), TACREV (RE), ACE05 (ED) and others</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Combine strengths of SLMs and LLMs to improve few-shot IE by only invoking expensive LLM reranking on a small subset of hard samples.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Filter: SLM assigns labels and per-sample confidence score conf(x)=max_l P_SLM(l|x,s). Hard samples (conf <= threshold τ) forwarded; Rerank: MCQ prompt with top-3 SLM candidates + 'None' option, 4-shot demos, CoT prefacing answers; LLM outputs final choice and optional rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to (a) SLM-only (no LLM rerank), (b) direct LLM ICL on all samples, and (c) SLM ensemble (replacing LLM with another SLM reranker).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall improvements: average +2.4% F1 gain across IE tasks when using InstructGPT reranker (Table 3, e.g., several dataset/shot settings show consistent gains). On reranked (hard) samples, absolute F1 gains are large (10%–25%) while only 0.5%–10% of samples get reranked (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>SLM-only vs SLM+LLM-rerank: SLM+LLM gives +2.4% average F1; SLM-ensemble + LLM rerank provides similar or additive gains (~2.1% average when starting from ensembled SLMs). Direct LLM ICL on all samples is far more costly and generally worse overall.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+2.4% average F1 over baselines (InstructGPT reranker); on the small subset reranked the absolute F1 improvement ranges ~10–25%.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLMs perform relatively poorly overall on IE but excel on 'hard' samples requiring external knowledge or reasoning; framing reranking as MCQ reduces label scope and matches LLM strengths. The filter reduces calls to LLM, saving cost while targeting LLM power where needed.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Threshold τ tuned on validation set; top-N=3 SLM candidates plus None; LLM reranker prompts use 4-shot demos and CoT; reranker models evaluated include Vicuna-13B, InstructGPT, GPT-4; typical reduction in LLM calls reduces budget/latency ~80–90% vs direct ICL (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9327.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9327.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IE-format vs MCQ-format prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence-level IE prompt format (structured extraction) versus sample-level MCQ prompt format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two prompt paradigms: (1) IE-format: prompt LLM to extract all labels/spans from a sentence (structured extraction); (2) MCQ-format: convert the decision for a single candidate span/label into a multiple-choice question for more focused decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT, InstructGPT, Vicuna-13B, CODEX, GPT-4 (variously evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Vicuna-13B 13B where specified; others not sized in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NER, RE, ED, EAE few-shot extraction tasks (various datasets like FewNERD, TACREV, ACE05)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare LLM performance when asked to produce full structured extraction versus choosing among candidate labels for a single candidate span (as MCQ).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>IE-format: typical extraction prompts with instruction + demos that ask for all entities/events in sentence; MCQ-format: present one candidate and ask to choose among pre-defined candidate options (multiple-choice).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct IE-format ICL (vanilla) vs MCQ-format used in reranking (filter-then-rerank).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MCQ-format used in reranking improved hard-sample performance substantially (10–25% F1 on reranked set) and yielded overall +2.4% F1; vanilla IE-format ICL performs worse overall for full-sentence extraction in many settings (see full-system comparisons in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>MCQ-format yields large gains on targeted hard samples and modest aggregate gains overall (+2.4% F1), while IE-format ICL is less effective at full extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLMs are more reliable on MCQ-style reasoning and classification than on open-ended structured extraction; MCQ reduces label scope and simplifies the generative decoding requirement, improving LLM reliability especially for hard cases.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>MCQ rerankers used 4-shot demos, top-3 candidates, CoT explanations; IE-format prompts used various demo counts and text/code formatting in baseline ICL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to retrieve prompts for in-context learning <em>(Rating: 2)</em></li>
                <li>Efficient Prompt Retriever <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Automatic chain-of-thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>What makes good in-context examples for GPT-3? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9327",
    "paper_id": "paper-257532405",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Instruction format variants",
            "name_full": "Instruction prompt format (I0..I5 variants)",
            "brief_description": "Variants of textual task instructions ranging from empty to detailed definitions and role framing (six variants I0–I5) used in in-context learning prompts to describe the IE task and enumerate labels or definitions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo series)",
            "model_size": null,
            "task_name": "Few-shot Information Extraction (examples: FewNERD NER, TACREV RE, ACE E D/EAE)",
            "task_description": "Named Entity Recognition (NER), Relation Extraction (RE), Event Detection (ED) and Event Argument Extraction (EAE) in few-shot settings (20-shot reported for instruction ablation).",
            "presentation_format": "Six instruction variants from empty (I0) to simple directive (I1) to progressively more detailed instructions including label definitions and role framing (I2..I5). Prompts follow ICL style: [Instruction; demonstrations; test example].",
            "comparison_format": "Each instruction variant compared against the others (I0..I5).",
            "performance": "Examples (20-shot, ChatGPT): FewNERD F1: I0=57.6, I1=58.3, I2=57.7, I3=57.6, I4=56.8, I5=57.8; TACREV F1s vary similarly (approx. 49.1–52.3 across variants); ACE ED/EAE show small fluctuations (see Table 12).",
            "performance_comparison": null,
            "format_effect_size": "Small — variations on the order of ±~1–2 F1 points across instruction variants (no consistent large gains from complexity).",
            "explanation_or_hypothesis": "The paper finds no strong correlation between instruction complexity and LLM performance on IE; simple instructions (I1) are as effective as complex ones. Authors thus use simple instruction in main experiments.",
            "null_or_negative_result": true,
            "experimental_details": "20-shot settings; ChatGPT evaluated on FewNERD (NER), TACREV (RE), ACE (ED/EAE); prompts include demonstrations and the instruction variant; averaged over runs reported in Table 12.",
            "uuid": "e9327.0",
            "source_info": {
                "paper_title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Demo number",
            "name_full": "Number of demonstrated examples in ICL (demo count)",
            "brief_description": "Variation in the number of in-context demonstration examples fed to the LLM during ICL; effects differ by task and model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT, CODEX, LLaMA-13B, Vicuna-13B (evaluated separately)",
            "model_size": "13B for LLaMA/Vicuna where specified; OpenAI model sizes unspecified in paper",
            "task_name": "Few-shot IE (NER, RE, ED)",
            "task_description": "Evaluate how increasing the number of demo examples in the prompt affects F1 on IE tasks under few-shot regimes.",
            "presentation_format": "ICL prompts varying demo count (plots show demo counts up to the maximum fit in context; Figure 8 presents demo-number vs F1).",
            "comparison_format": "Compared many demo counts (e.g., 1, 2, 4, 8, ... up to context limit) per task and model.",
            "performance": "Task-dependent: RE performance generally increases with more demos; NER and ED reach a plateau or degrade with more demos; open-source LLMs (LLaMA, Vicuna) saturate or collapse with only a few demos (2–4). (See Figure 8 and E.3 for qualitative trends.)",
            "performance_comparison": null,
            "format_effect_size": "Variable: RE shows consistent improvement as demos increase; NER/ED show little-to-no improvement or slight degradation beyond small demo counts. No single numeric effect size universally reported.",
            "explanation_or_hypothesis": "Some IE tasks (e.g., RE) benefit from more annotated context, while others (NER/ED) are limited by prompt length or suffer from example dilution; open-source LLMs have more limited capacity to leverage many demos.",
            "null_or_negative_result": true,
            "experimental_details": "Experiments used sentence-embedding demo selection; x-axis is number of demos (not shot K); models evaluated with greedy decoding (temperature=0). Open-source models used max input length 2048 and greedy decoding; OpenAI models used larger context if available.",
            "uuid": "e9327.1",
            "source_info": {
                "paper_title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Demo selection strategy",
            "name_full": "Demo selection: Random vs Sentence-embedding retrieval vs Efficient Prompt Retriever (EPR)",
            "brief_description": "Three methods to pick demonstration examples: random sampling, retrieval by sentence-embedding similarity (SimCSE-based), and a learned Efficient Prompt Retriever (neural retriever with contrastive learning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (mainly reported), with retrieval using SimCSE-RoBERTa-large embeddings or EPR components",
            "model_size": null,
            "task_name": "Few-shot IE (FewNERD NER, TACREV RE, ACE ED)",
            "task_description": "Selecting which K-shot examples are included as demos for per-test-sentence ICL prompts, and measuring resulting F1.",
            "presentation_format": "ICL where demos per test sample are selected by one of: (1) random sampling, (2) top-K nearest by sentence embedding (SimCSE-RoBERTa-large), or (3) Efficient Prompt Retriever (pre-retrieve M sentences then neural scoring).",
            "comparison_format": "Random vs Embedding vs EPR compared directly (Table 13).",
            "performance": "Table 13 (20-shot, ChatGPT): FewNERD F1 — Random 57.2(0.6), Sentence Embedding 57.6(2.3), EPR 57.6(2.3). TACREV F1 — Random 48.0(0.8), Sentence Embedding 53.2(0.4), EPR 43.0(3.3). ACE05 ED F1 — Random 43.5(1.4), Sentence Embedding 49.6(1.2), EPR 42.9(1.3).",
            "performance_comparison": null,
            "format_effect_size": "Embedding retrieval often substantially outperforms random (e.g., TACREV and ACE05 show +~5 F1); EPR sometimes underperforms embedding in reported numbers but both embedding and EPR generally surpass random.",
            "explanation_or_hypothesis": "Retrieval of semantically similar demonstrations yields better in-context signals; sentence-embedding retrieval is an effective and simple strategy, so the authors adopt it for main experiments.",
            "null_or_negative_result": false,
            "experimental_details": "Sentence embeddings computed by SimCSE-RoBERTa-large; EPR trained with in-batch contrastive learning, M=40 pre-retrieved, K_D=5, FLAN-T5-xl as scoring LM; experiments typically 20-shot with ChatGPT.",
            "uuid": "e9327.2",
            "source_info": {
                "paper_title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Prompt format: text vs code",
            "name_full": "Prompt output formatting: simple textual templates vs code-like output templates",
            "brief_description": "Two prompt output representation styles: a plain text template presenting Sentence/Entities/etc., and a code-like output representation (used in prior IE LLM work) to structure outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (evaluated), other LLMs in background",
            "model_size": null,
            "task_name": "Few-shot IE (FewNERD NER, TACREV RE, ACE ED, ACE EAE)",
            "task_description": "Comparison of different output representation prompt formats for structured IE outputs.",
            "presentation_format": "Text prompt example: 'Sentence: [S], Entities: ([type1],[entity1]), ...' vs Code prompt: recasting outputs as code-like structures (as in prior work).",
            "comparison_format": "Text prompt vs Code prompt (Table 14).",
            "performance": "Table 14 (20-shot, ChatGPT): FewNERD F1 Text 57.6(2.3) vs Code 53.2(0.9); TACREV Text 49.6(1.2) vs Code 50.2(1.8); ACE ED Text 42.9(1.3) vs Code 44.3(2.0); ACE EAE Text 51.5(1.1) vs Code 47.3(1.5).",
            "performance_comparison": null,
            "format_effect_size": "Mixed small differences across tasks; sometimes text &gt; code (FewNERD), sometimes code slightly better (ACE ED). No uniformly large advantage.",
            "explanation_or_hypothesis": "Different prompt formats yield comparable performance; authors choose text-format for simplicity in main experiments.",
            "null_or_negative_result": true,
            "experimental_details": "20-shot ChatGPT experiments; prompts differ only in output representation; averaged F1s in Table 14.",
            "uuid": "e9327.3",
            "source_info": {
                "paper_title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Sampling temperature",
            "name_full": "Decoding sampling temperature (t) setting for generation",
            "brief_description": "Choice of decoding temperature (deterministic greedy t=0 vs stochastic t&gt;0) for LLM output generation in structured IE tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CODEX (code-davinci-002) and others",
            "model_size": null,
            "task_name": "Few-shot IE (10-shot reported for temp ablation)",
            "task_description": "Assessing the impact of decoding randomness on quality of structured outputs for IE tasks.",
            "presentation_format": "Greedy decoding with temperature t=0 vs stochastic sampling with t&gt;0 (and experiments with self-consistency).",
            "comparison_format": "t = 0 compared to t = 0.7 (and t&gt;0 variants with/without self-consistency).",
            "performance": "Authors report deterministic decoding (t=0) yields substantially better generated quality for structured IE outputs; thus they set t=0 in main experiments (details in D.1 and Table 8).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Structured IE outputs require consistent token sequences; stochastic sampling increases noise and reduces parsing reliability, so greedy decoding is preferred.",
            "null_or_negative_result": false,
            "experimental_details": "Temperature ablation used CODEX on 10-shot settings. Self-consistency variants (using stochastic decoding multiple times) were considered but authors set t=0 for main experiments.",
            "uuid": "e9327.4",
            "source_info": {
                "paper_title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) and Auto-CoT",
            "name_full": "Chain-of-Thought prompting and Automatic Chain-of-Thought (Auto-CoT)",
            "brief_description": "CoT: including step-by-step rationales/explanations in demonstrations; Auto-CoT: automatically generated rationales inserted as context via an LLM-generated bootstrap procedure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (used to generate Auto-CoT rationales), CODEX and InstructGPT used in evaluation scenarios",
            "model_size": null,
            "task_name": "Few-shot IE (e.g., TACREV, FewNERD, ACE05)",
            "task_description": "Evaluate whether providing explanations (CoT) in demonstrations or auto-generated rationales (Auto-CoT) helps LLM performance on IE.",
            "presentation_format": "Demonstrations optionally prefaced with explanations (CoT). Auto-CoT pipeline: generate rationales for positive examples using another LLM and include them in demos.",
            "comparison_format": "With CoT vs without CoT; with Auto-CoT vs without Auto-CoT (Table 5 and D.2/D.3).",
            "performance": "Ablation (Table 5) shows modest gains from including human-crafted CoT in the adaptive reranker (e.g., FewNERD 20-shot: full system 63.6 F1 with CoT; removing CoT reduces to 63.2). Auto-CoT experiments (Table 9) show Auto-CoT degraded performance substantially in some settings.",
            "performance_comparison": null,
            "format_effect_size": "CoT: small positive effect in reranking pipeline (fractions of F1 points). Auto-CoT: negative effect sometimes large (authors report substantial degradation).",
            "explanation_or_hypothesis": "CoT can help LLM rerankers by making reasoning explicit. Auto-generated rationales hurt because they increase demo length (reducing number of demos), are inconsistent (only for positive examples), and may be low-quality especially for RE.",
            "null_or_negative_result": true,
            "experimental_details": "Adaptive reranker uses CoT prefacing answers in demos; Auto-CoT used InstructGPT to generate rationales with temperature 0.7; Auto-CoT hurt performance so authors did not use it in main experiments.",
            "uuid": "e9327.5",
            "source_info": {
                "paper_title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "MCQ reranking / Filter-then-rerank",
            "name_full": "Filter-then-rerank adaptive paradigm using Multi-Choice Question (MCQ) prompts",
            "brief_description": "A two-stage system: SLMs (fine-tuned small models) filter and produce top-N candidate labels; LLMs rerank these candidates using MCQ-formatted prompts, applied only to samples deemed 'hard' by SLM confidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Filter: RoBERTa-large (SLMs); Rerankers: InstructGPT, Vicuna-13B, GPT-4 (evaluated variants)",
            "model_size": "Vicuna-13B (13B), LLaMA-13B noted elsewhere; SLMs based on RoBERTa-large",
            "task_name": "Few-shot IE tasks: FewNERD (NER), TACREV (RE), ACE05 (ED) and others",
            "task_description": "Combine strengths of SLMs and LLMs to improve few-shot IE by only invoking expensive LLM reranking on a small subset of hard samples.",
            "presentation_format": "Filter: SLM assigns labels and per-sample confidence score conf(x)=max_l P_SLM(l|x,s). Hard samples (conf &lt;= threshold τ) forwarded; Rerank: MCQ prompt with top-3 SLM candidates + 'None' option, 4-shot demos, CoT prefacing answers; LLM outputs final choice and optional rationale.",
            "comparison_format": "Compared to (a) SLM-only (no LLM rerank), (b) direct LLM ICL on all samples, and (c) SLM ensemble (replacing LLM with another SLM reranker).",
            "performance": "Overall improvements: average +2.4% F1 gain across IE tasks when using InstructGPT reranker (Table 3, e.g., several dataset/shot settings show consistent gains). On reranked (hard) samples, absolute F1 gains are large (10%–25%) while only 0.5%–10% of samples get reranked (Table 4).",
            "performance_comparison": "SLM-only vs SLM+LLM-rerank: SLM+LLM gives +2.4% average F1; SLM-ensemble + LLM rerank provides similar or additive gains (~2.1% average when starting from ensembled SLMs). Direct LLM ICL on all samples is far more costly and generally worse overall.",
            "format_effect_size": "+2.4% average F1 over baselines (InstructGPT reranker); on the small subset reranked the absolute F1 improvement ranges ~10–25%.",
            "explanation_or_hypothesis": "LLMs perform relatively poorly overall on IE but excel on 'hard' samples requiring external knowledge or reasoning; framing reranking as MCQ reduces label scope and matches LLM strengths. The filter reduces calls to LLM, saving cost while targeting LLM power where needed.",
            "null_or_negative_result": false,
            "experimental_details": "Threshold τ tuned on validation set; top-N=3 SLM candidates plus None; LLM reranker prompts use 4-shot demos and CoT; reranker models evaluated include Vicuna-13B, InstructGPT, GPT-4; typical reduction in LLM calls reduces budget/latency ~80–90% vs direct ICL (Figure 7).",
            "uuid": "e9327.6",
            "source_info": {
                "paper_title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "IE-format vs MCQ-format prompts",
            "name_full": "Sentence-level IE prompt format (structured extraction) versus sample-level MCQ prompt format",
            "brief_description": "Two prompt paradigms: (1) IE-format: prompt LLM to extract all labels/spans from a sentence (structured extraction); (2) MCQ-format: convert the decision for a single candidate span/label into a multiple-choice question for more focused decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT, InstructGPT, Vicuna-13B, CODEX, GPT-4 (variously evaluated)",
            "model_size": "Vicuna-13B 13B where specified; others not sized in paper",
            "task_name": "NER, RE, ED, EAE few-shot extraction tasks (various datasets like FewNERD, TACREV, ACE05)",
            "task_description": "Compare LLM performance when asked to produce full structured extraction versus choosing among candidate labels for a single candidate span (as MCQ).",
            "presentation_format": "IE-format: typical extraction prompts with instruction + demos that ask for all entities/events in sentence; MCQ-format: present one candidate and ask to choose among pre-defined candidate options (multiple-choice).",
            "comparison_format": "Direct IE-format ICL (vanilla) vs MCQ-format used in reranking (filter-then-rerank).",
            "performance": "MCQ-format used in reranking improved hard-sample performance substantially (10–25% F1 on reranked set) and yielded overall +2.4% F1; vanilla IE-format ICL performs worse overall for full-sentence extraction in many settings (see full-system comparisons in Table 3).",
            "performance_comparison": null,
            "format_effect_size": "MCQ-format yields large gains on targeted hard samples and modest aggregate gains overall (+2.4% F1), while IE-format ICL is less effective at full extraction.",
            "explanation_or_hypothesis": "LLMs are more reliable on MCQ-style reasoning and classification than on open-ended structured extraction; MCQ reduces label scope and simplifies the generative decoding requirement, improving LLM reliability especially for hard cases.",
            "null_or_negative_result": false,
            "experimental_details": "MCQ rerankers used 4-shot demos, top-3 candidates, CoT explanations; IE-format prompts used various demo counts and text/code formatting in baseline ICL experiments.",
            "uuid": "e9327.7",
            "source_info": {
                "paper_title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to retrieve prompts for in-context learning",
            "rating": 2,
            "sanitized_title": "learning_to_retrieve_prompts_for_incontext_learning"
        },
        {
            "paper_title": "Efficient Prompt Retriever",
            "rating": 1,
            "sanitized_title": "efficient_prompt_retriever"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Automatic chain-of-thought prompting in large language models",
            "rating": 2,
            "sanitized_title": "automatic_chainofthought_prompting_in_large_language_models"
        },
        {
            "paper_title": "What makes good in-context examples for GPT-3?",
            "rating": 2,
            "sanitized_title": "what_makes_good_incontext_examples_for_gpt3"
        }
    ],
    "cost": 0.02106675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!</p>
<p>Yubo Ma 
S-Lab
Nanyang Technological University</p>
<p>Yixin Cao 
Singapore Management University</p>
<p>Yongching Hong 
S-Lab
Nanyang Technological University</p>
<p>Aixin Sun 
S-Lab
Nanyang Technological University</p>
<p>Pranav Neelakantan 
Girish Shyam 
Amanda Sastry 
Sandhini Askell 
Ariel Agarwal 
Gretchen Herbert-Voss 
Tom Krueger 
Rewon Henighan 
Aditya Child 
Daniel M Ramesh 
Jeffrey Ziegler 
Clemens Wu 
Christopher Winter 
Mark Hesse 
Eric Chen 
Mateusz Sigler 
Scott Litwin 
Benjamin Gray 
Jack Chess 
Christopher Clark 
Sam Berner 
Alec Mccandlish 
Ilya Radford 
Dario Sutskever 
Amodei 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Greg Brockman 
Alex Ray 
Raul Puri 
Gretchen Krueger 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Scott Gray 
Nick Ryder 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Clemens Winter 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
Ariel Herbert-Voss 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Christopher Hesse 
Andrew N Carr 
Jan Leike 
Joshua Achiam 
Vedant Misra 
Evan Morikawa 
Alec Radford 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Dario Amodei 
Sam Mccandlish 
Ilya Sutskever 
Wojciech 2021 Zaremba 
Evaluat 
Ting Chen 
Simon Kornblith 
Mohammad Norouzi 
Geoffrey E Hinton 
Wenhu Chen 
Xueguang Ma 
Xinyi Wang 
William W Cohen 
Xiang Chen 
Ningyu Zhang 
Xin Xie 
Shumin Deng 
Yunzhi Yao 
Chuanqi Tan 
Fei Huang 
Luo Si 
Knowprompt 
Wei-Lin Chiang 
Zhuohan Li 
Zi Lin 
Ying Sheng 
Zhanghao Wu 
Hao Zhang 
Lianmin Zheng 
Siyuan Zhuang 
Yonghao Zhuang 
Joseph E Gonzalez 
Ion Stoica 
Eric P 2023 Xing 
Vicuna 
Aakanksha Chowdhery 
Sharan Narang 
Jacob Devlin 
Maarten Bosma 
Gaurav Mishra 
Adam Roberts 
HyungPaul Barham 
Won Chung 
Charles Sutton 
Sebastian Gehrmann 
Parker Schuh 
Kensen Shi 
Sasha Tsvyashchenko 
Joshua Maynez 
Abhishek Rao 
Parker Barnes 
Yi Tay 
Noam Shazeer 
Vin- Odkumar Prabhakaran 
Emily Reif 
Nan Du 
Ben Hutchinson 
Reiner Pope 
James Bradbury 
Jacob Austin 
Michael Isard 
Guy Gur-Ari 
Pengcheng Yin 
Toju Duke 
Anselm Levskaya 
Sanjay Ghemawat 
Sunipa Dev 
Henryk Michalewski 
Xavier Garcia 
Kevin Robinson 
Liam Fedus 
Denny Zhou 
Daphne Ippolito 
David Luan 
Hyeontaek Lim 
Barret Zoph 
Alexander Spiridonov 
Ryan Sepassi 
David Dohan 
Shivani Agrawal 
Mark Omernick 
An- Drew M Dai 
Thanumalayan Sankaranarayana 
Marie Pellat 
Aitor Lewkowycz 
Erica Moreira 
Rewon Child 
Oleksandr Polozov 
Katherine Lee 
Zongwei Zhou 
Xuezhi Wang 
Brennan Saeta 
Mark Diaz 
Orhan Firat 
Michele Catasta 
Jason Wei 
Kathy Meier-Hellstern 
Douglas Eck 
Jeff Dean 
Slav Petrov 
Hyung Won 
Le Hou 
Shayne Longpre 
William Fedus 
Yunxuan Li 
Xuezhi Wang 
Mostafa Dehghani 
Siddhartha Brahma 
Al- Bert Webson 
Shane Shixiang 
Zhuyun Gu 
Mirac Dai 
Xinyun Suzgun 
Aakanksha Chen 
Alex Chowdh- Ery 
Marie Castro-Ros 
Kevin Pellat 
Dasha Robinson 
Sharan Valter 
Gaurav Narang 
Adams Mishra 
Vincent Yu 
Yanping Zhao 
Andrew Huang 
Hongkun Dai 
Slav Yu 
Ed H Petrov 
Jeff Chi 
Ja- Cob Dean 
Adam Devlin 
Denny Roberts 
Quoc V Zhou 
Jason Wei Le 
Scaling </p>
<p>Henrique Pondé de Oliveira Pinto
Jared Kaplan, Yuri BurdaHarrison Edwards</p>
<p>Nicholas Joseph</p>
<p>Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!
871C78A1AE0290CDA08AE6127FDFDFDFKnowledgeaware prompt-tuning with synergistic optimization for relation extraction. In WWW '22: The ACM Web
Large Language Models (LLMs) have made remarkable strides in various tasks.Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem.In this work, we aim to provide a thorough answer to this question.Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings.Therefore, we conclude that LLMs are not effective few-shot information extractors in general 1 .Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with.And moreover, we propose an adaptive filter-thenrerank paradigm to combine the strengths of LLMs and SLMs.In this paradigm, SLMs serve as filters and LLMs serve as rerankers.By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our preliminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment.Our code is available at https://github.com/mayubo2333/LLM-IE.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs, Brown et al. 2020;Chowdhery et al. 2022;Touvron et al. 2023) have shown remarkable abilities on various NLP applications such as factual question answering (Yu et al., 2023;Sun et al., 2023), arithmetic reasoning (Chen et al., 2022a;Qian et al., 2023) and logical reasoning (Jung et al., 2022;Pan et al., 2023).Given the reasoning, memorization, instruction-following and few-shot adaption capabilities emerging from LLMs, it prompts a compelling question: Can LLMs be used to boost performance in few-shot information extraction (IE) tasks?</p>
<p>To answer this question, we conduct an extensive empirical study to compare the performance between LLMs using in-context learning2 (ICL) and fine-tuned Small Language Models (SLMs).We fairly evaluate SLMs-based and LLMs-based methods across nine datasets spanning four common IE tasks: (1) Named Entity Recognition, (2) Relation Extraction, (3) Event Detection and (4) Event Argument Extraction.For each dataset, we explored four to six settings to encompass typical low-resource extents, from 1-shot to 20-shot or even more.Given the potential sensitivity of LLMs' performance to the prompt context, we meticulously considered variations in instruction, demonstration number and selection strategy, prompt format, etc.Our study reveals that LLMs excel over SLMs only when annotations are extremely limited, i.e., both label types3 and the samples4 per label are extremely scarce.With more (e.g., hundreds of) samples, SLMs significantly outperform LLMs.Furthermore, LLMs incur greater inference latency and costs than fine-tuned SLMs.Hence, we claim that current LLMs are not good few-shot information extractors in general.</p>
<p>We further investigate whether LLMs and SLMs exhibit different abilities to handle various types of samples.We categorize samples according to their difficulty measured by SLMs' confidence scores, and compare LLMs' and SLMs' results within each group.We find that LLMs are good at hard samples, though bad at easy samples.We posit that the knowledge and reasoning abilities in LLMs enable them to handle hard samples (which are sim-ply beyond SLMs' capabilities) well.Nevertheless, LLMs demonstrate strong predisposition to falsepositive predictions on negative samples.Since most negative samples are easy samples (which could be solved readily by SLMs), the performance of LLMs on easy samples sometimes collapses and are usually much worse than fine-tuned SLMs.</p>
<p>Leveraging these findings, we pursue an approach to incorporate LLMs and SLMs within a single system and combine their merits.To this end, we propose a novel filter-then-rerank framework.The basic idea is that SLMs serve as a filter and LLMs as a reranker.Specifically, SLMs initially predict and determine the difficulty of each sample.If the sample is a hard one, we further pass the top-N most-likely candidate labels from SLMs to LLMs for reranking.Otherwise we view the prediction from SLMs as the final decision.By providing easy/hard samples with different solution strategies, our system utilizes each model's strengths to complement each other.Also, it reranks only a small subset of samples and minimizes the extra latency and budgets for calling LLMs.With a modest cost increase, our framework yields a consistent F1 improvement, averaging 2.4% higher than previous methods on various few-shot IE tasks.To the best of our knowledge, this is the first successful attempt to use LLMs to enhance few-shot IE tasks.</p>
<p>Related Work</p>
<p>LLMs for Information Extraction</p>
<p>Recent studies have increasingly explored Information Extraction (IE) tasks using LLMs.Drawing inspiration from instruction tuning (Wei et al., 2022a), several methods (Wadhwa et al., 2023;Wang et al., 2023a;Lu et al., 2023) transform annotated samples into instruction-answer pairs and then finetune LLMs, such as FlanT5 (Chung et al., 2022), on them.Nonetheless, this method necessitates a vast range of samples with diverse schemas and often yields suboptimal results in low-resource scenarios.In the context of few-shot IE tasks, prevalent strategies bifurcate into two main streams.The first approach perceives LLMs as efficient annotators (Ding et al., 2023;Josifoski et al., 2023).In these methods, they produce a plethora of pseudolabeled samples through LLMs and leverage the enhanced annotations to train SLMs.Conversely, the latter approach employs LLMs in inference using the ICL paradigm, which is the focus of our subsequent discussion.</p>
<p>Few-shot IE with ICL</p>
<p>Regarding few-shot IE tasks, recent studies intensively compare the performance between SLMs and LLMs but yield inconsistent conclusions.Some studies favor LLMs as competent few-shot extractors (Agrawal et al., 2022;Wang et al., 2023b;Li et al., 2023;Zhang et al., 2023a;Wadhwa et al., 2023), while others dispute this claim (Jimenez Gutierrez et al., 2022;Qin et al., 2023;Wei et al., 2023;Gao et al., 2023).This discrepancy leaves the question of whether LLMs perform competitively on few-shot IE tasks unresolved, thus hindering the advances of this domain.</p>
<p>We attribute such disagreement to the absence of an comprehensive and unified benchmark.Existing studies usually vary in tasks, datasets, and few-shot settings.Furthermore, some studies rely on overly simplistic datasets (Jimenez Gutierrez et al., 2022;Li et al., 2023) and may exaggerate the effectiveness of LLMs.Driven by these findings, our research undertakes comprehensive experiments across four IE tasks, nine datasets with various schema complexities (from coarse-grained to fine-grained) and low-resource settings.</p>
<p>In addition to the empirical study, we develop an innovative filter-then-rerank paradigm to combine the strengths of both LLMs and SLMs.It utilizes prompting strategies akin to QA4RE (Zhang et al., 2023a), transforming IE tasks into multi-choice questions.However, our method stands apart by integrating SLMs and LLMs within a single framework.This incorporation (1) enables our paradigm applicable to various IE tasks by providing candidate spans in the text and (2) achieves promising performance under low-resource IE scenarios.</p>
<p>Large LMs v.s. Small LMs</p>
<p>In this section, we compare the performance between LLMs and SLMs to evaluate whether LLMs perform competitively.</p>
<p>Task, Dataset and Evaluation</p>
<p>We run experiments on nine widely-used datasets across four IE tasks.(1) Named Entity Recognition (NER): CONLL03 (Tjong Kim Sang and De Meulder, 2003), OntoNotes (Weischedel et al., 2013) and FewNERD (Ding et al., 2021).(2) Relation Extraction (RE): TACRED (Zhang et al., 2017) and TACREV (Alt et al., 2020).(3) Event Detection (ED): ACE05 (Doddington et al., 2004), MAVEN (Wang et al., 2020) and ERE (Song et al  2015).( 4) Event Argument Extraction (EAE): ACE05, ERE and RAMS (Ebner et al., 2020).With label numbers ranging from 4 to 168, we assess LLMs' performance under different schema complexities.See their details in Appendix A.1.Few-shot Set We construct few-shot datasets from the original datasets above.For training and validation set, we adopt K-shot sampling strategy, i.e., sampling K samples for each label type.See more details in Appendix A.2.For test set, we downsample their original test sets to reduce the cost of LLMs.We randomly sample 500 sentences for RE tasks, and 250 sentences for other task.We ensure that each label has at least one corresponding sample to avoid the absence of rare labels.</p>
<p>Evaluation We adopt micro-F1 score in NER, RE and ED tasks.For EAE task, we follow previous work (Wang et al., 2023b) and adopt head-F1 score, which merely considers matching of the head word rather than the whole content of a text span.We report averaged score w.r.t 5 sampled train/validation sets unless otherwise stated.</p>
<p>Small Language Models</p>
<p>We adopt five supervised methods to evaluate the abilities of SLMs.(1) Vanilla fine-tuning for all tasks, (2) FSLS (Ma et al., 2022a) for NER and ED tasks, (3) KnowPrompt (Chen et al., 2022b) for RE task, (4) PAIE (Ma et al., 2022b) for EAE task, and</p>
<p>(5) UIE (Lu et al., 2022c) for all tasks.See their details in Appendix B.</p>
<p>Large Language Models</p>
<p>Detailed in Appendix C, we evaluate the ICL abilities of LLMs.Given labeled sentences D = {(s i , y i )} and a test sentence s, our goal is to predict structured information y from s using a frozen LLM L. We feed LLM with prompt P E,I,f (D, s):
P E,I,f (D, s) = <a href="1">I; f (E(D, s)); f (s)</a>
We give examples of prompts on four IE tasks in Figure 1.The prompts consist of three parts: instruction I (color in green in Figure 1), demonstration f (E(D, s)) (demo; color in blue) and the question f (x) (color in black).Here E denotes demo selector and E(D, s) ⊂ D denotes selected sentences as the demo to predict s.Prompt format f5 refers to the template which converts demo E(D, s) and sample s to input context for LLMs.Then LLM generates f (y) (color in red) from which we could readily parse the extraction results y.Models L: We explore six LLMs from two sources.(1) OpenAI models6 : we employ Chat- GPT, CODEX (Chen et al., 2022a) and Instruct-GPT (Ouyang et al., 2022) for main experiments.We also evaluate GPT-4 in Appendix D.3.(2) Open-source models: we use LLaMA-13B (Touvron et al., 2023) and its instruction-tuned counterpart, Vicuna-13B (Chiang et al., 2023).Instruction I: The instruction (1) describes the task and (2) enumerates all possible labels for reference.we adopt instructions shown in Figure 1.</p>
<p>Demo selector E: The maximum input length of code-davinci-002, text-davinci-003 and gpt-4-0314.Due to budget constraints, we execute InstructGPT and GPT-4 only once per setting.We do not conduct EAE task on CODEX since it had been unavailable at that time.</p>
<p>LLMs usually limits the sentence number in demos even under few-shot settings.Therefore for each test sentence s, we demand a demo retriever E(D, s) which selects a small subset from D as the sentences in demo.Following previous methods (Liu et al., 2022;Su et al., 2022), we retrieve demos according to their sentence embedding similarity to the test samples.</p>
<p>Main Results</p>
<p>We summarize the main experimental outcomes in Figure 2 (2) Among proprietary LLMs, ChatGPT performs better on NER and EAE tasks, but poorer so on RE and ED tasks.InstructGPT and CODEX demonstrate comparable performance across these tasks.</p>
<p>LLMs show limited inference speed.We compare the inference speed of different methods and show their results in Table 1.We observe that LLMs is much slower than SLMs since they have much more parameters, longer input contexts and extra response decay (if external APIs applied).</p>
<p>Analysis on Prompt Sensitivity</p>
<p>Previous work (Lu et al., 2022b) indicates that the efficacy of LLMs on specific tasks can be significantly influenced by the construction of the prompt.</p>
<p>To ensure that LLMs' suboptimal outcomes are not erroneously ascribed to inappropriate prompt designs, we meticulously examine the impact of diverse prompt variations from four aspects, i.e., instruction format, demo number, demo selector and prompt format.We leave comprehensive details of the variants and their results to Appendix E.2-E.5, and illustrate salient findings in Figure 3.Our findings include that (1) diverse instruction strategies yield comparable results in IE task; (2) increasing the number of samples in demonstrations does not unequivocally enhance performance; and</p>
<p>(3) The selection strategy of demonstration matters, and retrieval based on sentence embedding</p>
<p>Analysis:</p>
<p>The New Yorker is a well-known American magazine that has been published since 1925, and is primarily known for its long-form journalism, commentary, and satire.It has a reputation for publishing high-quality writing on a wide variety of topics, including politics, culture, and the arts.So The New Yorker is a media/newspaper organization.</p>
<p>Correct Answer: (d)</p>
<p>Figure 4: Multi-choice question (MCQ) prompt.</p>
<p>7 These two tasks require unfixed numbers of (label, span) tuple.Furthermore, the length of each span is also unfixed.</p>
<p>To mitigate LLMs' drawbacks mentioned above, we propose a filter-then-rerank paradigm to integrate both SLMs and LLMs within the same system.This paradigm uses SLMs as filters to select the top-N candidate labels, then LLMs rerank them to make final decisions.By using SLM-generated candidate answers, the focus of LLMs shifts from sentence-level (i.e., identifying all entities/events in the sentence) to sample-level (i.e., determining single entity/event candidate provided).Each question now corresponds to a single sample, allowing us to reframe prompts as multi-choice questions (MCQ; shown in Figure 4) problem.Under such format, each candidate label is converted to a choice by pre-defined templates.We claim filter-then-rerank paradigm is more likely to elicit the powers of LLMs and smoothly solve few-shot IE tasks because: (1) LLMs are more familiar with MCQ prompts than IE-format prompts (Zhang et al., 2023a).( 2) This paradigm reduces the label scopes significantly, since N is usually much smaller than fine-grained label numbers.</p>
<p>LLMs are Hard Sample Solver</p>
<p>Our filter-then-rerank paradigm, unfortunately, presents unsatisfactory performance (and even suffers longer latency since LLMs rerank candidates per sample).Given LLMs' abilities in memorization and reasoning, however, we still believe that LLMs are potential to solve some, if not most, IE samples effectively.We hypothesize that LLMs are more proficient than SLMs on hard samples.These samples are characterized by their requisite for external knowledge acquisition or sophisticated reasoning strategies, areas where LLMs can leverage their extensive parametric knowledge bases and inherent reasoning mechanisms.In contrast, SLMs often falter with such samples, constrained by their restricted modeling capacities.</p>
<p>We leverage an unsupervised metric from SLMs to evaluate the difficulty of samples.Given a sample x in the sentence s, we define the highest probability across all labels as the confidence score:
conf(x) = max l∈L P SLM (l|x; s) (2)
where L denotes the label set and P SLM (l|x; s) the probability of a span x (in the sentence s) referring to label l computed by SLMs.We classify samples with low confidence scores as hard samples.</p>
<p>Otherwise we view them as easy samples.We conduct experiments to confirm our hypothesis that LLMs excel on hard samples.We group samples by confidence scores and compare two methods within each group: (a) SLM-based methods without LLM reranking, and (b) SLMs as the filter and LLMs as the reranker.Method (b) differs from (a) by adding a single LLM to rerank the top-N SLM predictions, using MCQ prompts.</p>
<p>The results in Figure 5 substantiate our assumption.(1) LLM-based reranking (blue lines) enhances performance on hard samples (left areas in the figure).We provide a detailed analysis of specific challenging instances where LLM rerankers prove advantageous in Appendix F.1.These instances demonstrate the efficacy of LLMs in harnessing external knowledge and complex reasoning to rectify erroneous predictions initially made by SLMs (red lines).( 2) Conversely, LLM-based reranking impedes performance on easy samples (right areas), resulting in a significant degradation, particularly for very easy samples (rightmost areas).In conclusion, LLMs exhibit greater proficiency in handling hard samples compared to SLMs, yet they underperform relative to SLMs on easy samples.</p>
<p>Why LLMs Fail on Easy Samples</p>
<p>We investigate why LLMs (relatively) fail on easy samples in this section.As shown in Table 2, we observe significant higher negative sample ratios for easy samples across diverse IE tasks.In other words, most negative samples are easy samples for SLMs.Here we refer negative samples to those labeled as None.We speculate that the proficiency of SLMs with negative samples stems from their ability to adeptly discern apparent patterns during the fine-tuning stages.Therefore, SLMs could predict negative samples with (relatively) high confidence and accuracy.Due to LLMs' predisposition to false-positive predictions on negative samples, however, the performance of LLMs on easy samples collapses.We attribute such false-positive predictions to (1) hallucination and (2) span boundary mismatch.We detail such two kinds of mistakes with cases in Appendix F.2.</p>
<p>Adaptive Filter-then-rerank Paradigm</p>
<p>Above findings can be summarized as: (1) SLMs generally outperform LLMs, especially with more training samples and fine-grained labels.</p>
<p>(2) SLMs are much more time-and cost-efficient.</p>
<p>(3) LLMs serve as powerful rerankers on hard samples that challenge SLMs.Based on them, we propose a simple, efficient, and effective adaptive reranker that combines the strengths of SLMs and LLMs.</p>
<p>Method</p>
<p>Our adaptive filter-then-rerank approach, shown in Figure 6, uses supervised SLMs as a filter to make preliminary decisions.Samples with confidence scores exceeding threshold are viewed as easy samples otherwise hard ones.For easy samples, we retain SLM predictions as final results.For hard samples, top-N predictions from SLMs are reranked via LLMs using ICL.Here LLMs employ MCQ prompts (Figure 4), containing demos and a sample to be reranked.The LLMs then generate the final answer and optionally provide an explanation.</p>
<p>Experimental Setup</p>
<p>We conduct experiments on FewNERD for NER task, TACREV for RE task and ACE05 for ED task.We employ top-performing SLM-based methods from Section 3 (FSLS or KnowPrompt) as the</p>
<p>Filter</p>
<p>The sentence implies that Laura Silsby is associated with the city of Meridian in the state of Idaho, and does not provide information about her birthplace.So Laura Silsby lives in the city Meridian.</p>
<p>Answer: (b)</p>
<p>Demonstration</p>
<p>The lawyer denied Italian news reports that she wept while addressing the court, but said Knox was upset as she recounted the pressure, the aggressiveness of the police who called her a liar.</p>
<p>Reranker</p>
<p>Easy Sample Hard Sample</p>
<p>Figure 6: The overall architecture of our adaptive filter-then-rerank paradigm.We color easy samples in orange and hard samples in pink.For easy samples, the final predictions are exactly from the SLM-based methods.For hard samples, the top-N predictions from SLMs are fed into LLMs as the format of multiple-choice questions (pink box).</p>
<p>The question is paired with demos (green box).LLMs rerank these N candidates and generate the final prediction.</p>
<p>filter, and Vicuna-13B, InstructGPT or GPT-4 as the reranker.The threshold τ to determine sample difficulty is optimized on the valid set.For hard sample, the top-3 SLM predictions and None (if not included) are feed to LLMs for reranking.</p>
<p>Each LLM prompt has 4-shot demos.See demo examples in Appendix G.1.We follow templates in Lu et al. (2022a) for TACREV and carefully design others.See these templates in Appendix G.2.We adopt chain-of-thought reasoning (Wei et al., 2022b), i.e., prefacing the answer with an explanation, to facilitate LLMs' reranking procedure.</p>
<p>Baseline We compare our method with two kinds of baselines to validate its effectiveness.</p>
<p>(1) LLMs with ICL: We follow the prompts in Section 3.3 and conduct experiments on three LLMs.</p>
<p>(2) Supervised SLMs: We follow previous SoTA methods shown in Section 3.4 (FSLS or Know-Prompt).We additionally combine two SLMs with ensemble or reranking approach (i.e., replace the LLM with another SLM as the reranker) to verify that improvements from our SLM-LLM integrated system are not solely due to the ensemble effects.</p>
<p>Main Results</p>
<p>Table 3 shows that our filter-then-rerank method consistently improves performance across three datasets and nine settings.For instance, with In-structGPT, reranking provides an average F1 gain of 2.4% without SLM ensemble (Lines 4 vs. 7).</p>
<p>Based on ensemble SLMs as the filter, our method still achieves 2.1% (Lines 5 vs. 8) gains on av-erage.This confirms (1) the effectiveness of the LLM reranking and (2) its gains are different and (almost) orthogonal to the SLM ensemble.</p>
<p>Analysis</p>
<p>Few makes big difference Our method selectively reranks hard samples.Table 4 shows that (1) only a minor fraction (0.5%~10%) of samples are deemed hard and are reranked by LLMs.</p>
<p>(2) Despite their limited quantity, reranking results in a substantial performance boost on these samples (10%~25% absolute F1 gains).This uplift on a small subset significantly enhances the overall performance.</p>
<p>GPT-4 is more aggressive From Tables 3 and 4, GPT-4 generally improves more on hard samples, yet InstructGPT surpasses GPT-4 in NER and RE tasks when evaluated overall.This discrepancy arises from GPT-4's aggressive reranking which introduces more true positives.InstructGPT, however, focuses more on reducing false positives.</p>
<p>Few makes small cost Figure 7 demonstrates that our method impressively reduces budget and latency by approximately 80%~90% compared to direct ICL.This reduction is due to (1) fewer LLM callings (only for hard samples) and (2) shorter prompts (fewer candidate labels and demos).</p>
<p>Ablation Study</p>
<p>We investigate the effectiveness of the modules in adaptive filter-then-rerank system by removing each of them in turn: (1) CoT: We exclude the explantion for each examples in demo.</p>
<p>(2) Demo: (2.9) 64.9(2.5)67.1(2.5)+ Ensemble (S) + Rerank (L) 61.1(2.2) 62.8(0.9)63.6(1.2) 68.6(1.3)73.9(1.4)75.9(2.4)60.9(3.9)65.6(1.5)67.8(1.7)</p>
<p>Table 4: The F1-score differences before and after reranking on the reranked samples, as well as their proportion of the total samples.</p>
<p>GPT-4</p>
<p>InstructGPT before after △ ratio before after △ ratio FewNER 31.9 40.7 8.8 3.2% 31.hence cutting inference costs.(4) The performance collapses without a filter to identify sample difficulty, reiterating the need for an integrated SLM-LLM system to complement each other.</p>
<p>Conclusion</p>
<p>Through an extensive empirical study on nine datasets spanning four IE tasks, we find that LLMs, despite their superiority in extreme low-resource scenarios, are not effective few-shot information extractors in general.They struggle with IE-related prompts, have limited demonstration capacity, and incur high inference costs.However, LLMs significantly improve the performance on hard samples when combined with SLM.Building on these insights, we propose an adaptive filter-then-rerank paradigm to leverage the strengths of SLMs and LLMs and mitigate their limitations.This approach consistently achieves promising results, with an average 2.4% F1 gain across multiple few-shot IE tasks, while minimizing latency and budget costs.</p>
<p>Limitations</p>
<p>We do work hard to find better prompts to elicit the power of LLMs on few-shot IE tasks in Section 3.5, by exploring various kinds of LLMs, demonstration strategies and prompt formats.We find that different prompt variants do not significantly impact in-context learning abilities.As an empirical study, we acknowledge the potential existence of a lottery prompt superior to our explored prompts.However, it seems unlikely that an improved prompt would substantially alter our conclusions.</p>
<p>Another common risk when evaluating LLMs on public benchmark is their potential memorization of samples tested.To mitigate such potential contamination, we use earlier and stable versions of these models rather than the newer and updated ones (for example, gpt-4-0314 instead of gpt-4).Even if such contamination makes abilities of LLMs overestimated, our primary conclusions remain unchanged because we find that LLMs are NOT good few-shot information extractors.</p>
<p>Regarding our adaptive filter-then-rerank paradigm, a key limitation lies in how to assess sample difficulty.In this work, we employ a simple unsupervised metric, i.e., the maximum probabilities from SLMs.This is predicated on the assumption that SLMs are well-calibrated (Guo et al., 2017).However, it is an obviously imperfect assumption.We envision that calibrating SLMsbased filters or developing an advanced difficulty metric could substantially enhance LLM rerankers' performance.We leave them for future work.</p>
<p>A Datasets</p>
<p>A.1 Full Datasets</p>
<p>We construct few-shot IE datasets and conduct the empirical study on nine datasets spanning four tasks, with varying schema complexities ranging from 4 to 168.We show their statistics in Table 6.</p>
<p>A.2 Details of Few-shot IE Datasets</p>
<p>Sampling Algorithm for Train/Valid Datasets.</p>
<p>We downsample sentences from original training dataset to construct few-shot training and valid datasets.We adopt K-shot sampling strategy that each label has (at least) K samples.We set 6 Kvalues (1, 5, 10, 20, 50, 100) for RE tasks and 4 K-values (1, 5, 10, 20) for other tasks.For RE task, each sentence has exactly one relation and we simply select K sentences for each label.For NER, ED and EAE tasks, each sentences is possible to contain more than one entities/events/arguments. Since our sampling is at sentence-level, the algorithm of accurate sampling , i.e., finding exactly K samples for each label, is NP-complete8 and unlikely to find a practical solution.Therefore we follow Yang and Katiyar (2020) adopting a greedy sampling algorithm to select sentences for NER and ED tasks, as shown in Algorithm 1.Note that the actual sample number of each label can be larger than K under this sampling strategy.For all three tasks, we additionally sample negative sentences (without any defined labels) and make the ratio of positive sentences (with at least one label) and negative sentences as 1:1.The statistics of the curated datasets are listed in Table 7.</p>
<p>Algorithm 1 Greedy Sampling</p>
<p>Require: shot number K, original full dataset D = {(X, Y)} tagged with label set E 1: Sort E based on their frequencies in {Y} as an ascending order 2: S ← ϕ, Counter ← dict() 3: for y ∈ E do 4:</p>
<p>Counter(y) ← 0 5: end for 6: for y ∈ E do  Based on the subsets constructed above, we optionally further split them into training and valid sets.For few-shot datasets with more than 300 sentences, we additionally split 10% sentences as the valid set and the remaining sentences as training set.Otherwise, we do not construct valid set and conduct 5-fold cross validation to avoid overfitting.</p>
<p>B Details on SLMs</p>
<p>We adopt five representative supervised methods to evaluate the ability of SLMs on few-shot IE tasks.</p>
<p>(1).Fine-tuning (FT): Add a classifier head on SLMs to predict the labels of each sentence/word.</p>
<p>(2).FSLS (Ma et al., 2022a): The state-of-the-art extractive-based method for few-shot NER task.Ma et al. (2023) also validate its competitive performance on few-shot ED tasks.</p>
<p>(3).KnowPrompt (Chen et al., 2022b): The best extractive-based method for few-shot RE task.(4).PAIE (Ma et al., 2022b): The best extractivebased method for few-shot EAE task.</p>
<p>(5).UIE (Lu et al., 2022c): A competitive unified generation-based method for few-shot IE tasks.We introduce their implementation details below: Fine-tuning/FSLS.We implement these two meth- ods by ourselves.We use RoBERTa-large (Liu et al., 2019) as the backbones.We adopt Automatic Mixed Precision (AMP) training strategy9 to save memory.We run each experiment on a single NVIDIA V100 GPU.We train each model with the AdamW (Loshchilov and Hutter, 2019) optimizer with linear scheduler and 0.1 warm-up steps.</p>
<p>We set the weight-decay coefficient as 1e-5 and maximum gradient norms as 1.0.We set the batch size as 64, the maximum input length as 192, the training step as 500 and the learning rate as 5e-5.KnowPrompt We implement this method based on original source code10 , and use RoBERTa-large as our backbones.We set 10 maximum epochs for 50-and 100-shot datasets, and as 50 epochs for other datasets.We keep all other hyperparameters as default, and run each experiment on a single NVIDIA V100 GPU.PAIE We implement this method on original source code11 , and use BART-large (Lewis et al., 2020) as backbones.We keep all hyperparameters as default for ACE and RAMS dataset.For ERE dataset, we set the training step as 1000, the batch size as 16 and the learning rate as 2e-5.We run each experiment on a single NVIDIA V100 GPU.UIE We implement this method based on original source code12 , and use T5-large (Raffel et al., 2020) as the backbones.We run each experiment on a single NVIDIA Quadro RTX8000 GPU.We set the batch size as 4 with 4000 training steps.We set the maximum input length as 800 and the learning rate as 1e-4.</p>
<p>C LLMs Implementations</p>
<p>Regarding our empirical study, we explore the ICL abilities of LLMs on few-shot IE tasks.We mainly use five LLMs from two sources.( 1 (2) Open-source models: LLaMA-13B (Touvron et al., 2023) and its instruction-tuned counterpart, Vicuna-13B (Chiang et al., 2023).We detail their implementation details in the next sections below.</p>
<p>C.1 Open-source Models</p>
<p>We implement multiple ICL approaches on LLaMA-13B and Vicuna-13B without fine-tuning.</p>
<p>We set the maximum input length as 2048 and the batch size as 1.We run each experiment on a single NVIDIA V100 GPU.To achieve this, we leverage the Accelerate13 framework and fp16 inference to save memory.We set maximum output length as 96 and sampling temperature as 0 (i.e., greedy decoding).We set both frequency_penalty and presence_penalty as 0.</p>
<p>C.2 OpenAI Models</p>
<p>We implement multiple ICL approaches on Ope-nAI models by calling their official APIs14 .We set the maximum input length as 3600 for all tasks and models.The only exception occurs when we use CODEX on RE tasks, where we set the maximum input length as 7000.We unify the maximum output length as 32 for RE task, and 96 for other three tasks.We set the sampling temperature coefficient as 0, i.e., greedy decoding.</p>
<p>D Pivot Experiments on LLMs D.1 Sampling Temperature</p>
<p>Existing prompt-engineering discussion15 suggests setting the sampling temperature t = 0 for tasks with structured outputs, including IE tasks.We validate this conclusion in Table 8, from which we could see the generated quality when t = 0 is much higher than the quality when t ̸ = 0. Therefore we set t = 0 in all main experiments, and do not take self-consistency (Wang et al., 2023c) into account.</p>
<p>D.2 Automatic Chain-of-thought</p>
<p>We additionally investigate whether rationales could facilitate LLMs' performance on few-shot IE tasks.Since there exists no golden rationales in For example, given the sentence "DSC and Traction Control on all Speed3 models is also standard.",we would feed LLM the query that "Could you explain why Speed3 is a kind of car".Then we insert the bootstrapped rationales between the sentences and ground-truth answers.If a sentence has no positive labels, however, we do not ask LLMs and keep the original format as the vanilla ICL approach.Here we prompt InstructGPT to generate the rationales with temperature t = 0.7.We compare the performance with and without Auto-CoT as shown in Table 9.We are frustrated to find Auto-CoT degrades the performance with a large margin.We speculate this degration could be attributed to three main reasons.(1) The rationale increase the length of each sample and thus decrease the overall example number in demos.(2) There exists an obvious discrepancy between sentences with and without positive labels.The rationales are only provided for sentences with positive labels because it is hard to explain why a sentence dose not contain any label.</p>
<p>(3) Some auto-generated rationales are low-quality, especially for RE tasks.We would explore better strategy to exploit auto-genertaed rationales in the future work.We tend to minimize the GPT-4 calls due to its high price.Thus we utilize 20-/100-shot settings across each dataset to compare GPT-4's performance with other LLMs.Table 10 reveals that GPT-4 does not outperform other LLMs significantly, except on OntoNotes and MAVEN.However, even on these datasets, GPT-4 still falls behind supervised SLMs by a significant margin.Consequently, the exclusion of GPT-4 does not undermine the conclusions drawn from our main experiments, and we omit it from our empirical study.</p>
<p>E Auxiliary Experiments E.1 LLMs struggle on Fine-grained Datasets</p>
<p>Based on the results shown in Figure 2, we additionally provide a quantitative analysis to show that LLMs struggle with fine-grained datasets.Under the 5-shot setting, we compare the performance difference of LLMs (ChatGPT) and SLMs (SoTA few-shot models) among different datasets.For each IE task, we observe a clear negative corre- 6.9 0.3 -9.9 lation between the label number (row 2) and the performance difference (row 5).In other words, with more label types, LLMs tend to perform relatively worse than SLMs.Therefore we conclude that LLMs struggle on fine-grained datasets.</p>
<p>E.2 Finding Better Instruction</p>
<p>To investigate whether LLMs would benefit from complex instructions, we explored six instruction variants from simple to complex.Take NER task as an example, we illustrate them as below.</p>
<p>Instruction0: [empty]</p>
<p>Instruction1: Identify the entities expressed by each sentence, and locate each entity to words in the sentence.The possible entity types are: [Type_1], [Type_2], ..., [Type_N].If you do not find any entity in this sentence, just output 'Answer: No entities found.'Instruction2:</p>
<p>Identify the entities expressed by each sentence, and locate each entity to words in the sentence.The possible entity types are:
• [Type_1]: [Definition_1] • [Type_2]: [Definition_2] • ... • [Type_N]: [Definition_N]
If you do not find any entity in this sentence, just output 'Answer: No entities found.'Instruction3:</p>
<p>Assume you are an entity-instance annotator.</p>
<p>Given a sentence, you need to (1) identify the word or phrase about the entity in the sentence, and (2) classify its entity type.</p>
<p>The possible entity types are listed as below: [Type_1], [Type_2], . . ., [Type_N] Given a sentence, you need to (1) identify the word or phrase about the entity in the sentence, and (2) classify its entity type.</p>
<p>The possible entity types are listed as below: Regarding these six instructions, we evaluate their performance of ChatGPT on four 20-shot IE tasks.As shown in Table 12, there is no significant correlation between the instruction complexity and LLMs' performance.Even the prompt without instruction (I0) leads to comparable, if not better, results than prompt with complex instructions.Therefore, we use simple instruction (I1) in our main experiment.
• [Type_1]: [Definition_1] • [Type_2]: [Definition_2] • ... • [Type_N]: [Definition_N]</p>
<p>E.3 Do More Samples in Demos Help?</p>
<p>We wonder whether longer demos bring more powerful ICL abilities for LLMs.Thus we investigate the impact of increasing the number of demonstrations on LLMs' performance in Figure 8.We observe that: (1) The performance of the RE task consistently improves with more demos, indicating its potential benefiting from additional annotations.</p>
<p>(2) The NER and ED tasks reach a stable or degraded performance with increased demo numbers, suggesting that they are limited even before reaching the maximum input length.</p>
<p>(3) Open-source LLMs, i.e., LLaMA and Vicuna, have more limited capacities in leveraging demos compared to Ope-nAI models, with their performance stagnating or even collapsing with only a few (2-4) demos.</p>
<p>E.4 Finding Better Demo Selection Strategy</p>
<p>The maximum input length of LLMs usually limits the sentence number in demos even under fewshot settings.For each test sentence s, we demand a demo retriever E(D, s) which selects a subset from D as the sentences in demo.Following previous work, we consider three commonly-used strategies.</p>
<p>(1) Random sampling.</p>
<p>(2) Sentenceembedding (Liu et al., 2022;Su et al., 2022): retrieving the top-K nearest sentences measured by sentence embedding.We compute the embeddings by SimCSE-RoBERTa-large (Gao et al., 2021).
E(D, s) = arg-topK s ′ ∈D <a href="3">Sent-embed(s ′ , s)</a>
(3) Efficient Prompt Retriever (Rubin et al., 2022): retrieving by a neural retriever R trained on D.  For each test sentence s, we pre-retrieve M similar sentences
E(D, s) = arg-topK s ′ ∈D <a href="4">R D (s ′ , s)</a>D = {(s ′ i , y ′ i )} M i=1 ⊂ D.
Then we score each sentence in D by their likelihoods
P L (f (y ′ i )|f (s ′ i ))
where f denotes the prompt format adopted and L the scoring LM.We randomly select positive samples s ′ i (pos) from the top-K D sentences and hard negative samples s ′ i (hard-neg) from the bottom-K D ones.Then we train R D by inbatch contrastive learning (Chen et al., 2020).For each sentence s ′ i within the batch, there are 1 positive sentences s ′ i (pos) and 2B −1 negative sentences {s ′ j (hard-neg) } B j=1 ∪ {s ′ j } B j̸ =i .Here we adopt M as 40, K D as 5, f as text prompt, the batch size B as 128, and the scoring LM L as FLAN-T5-xl.Table 13 demonstrates the F1-score performance on different selection strategies.We find that both the sentence embedding and EPR surpass random sampling by a large margin.Given the simplicity of the sentence embedding, we adopt it, rather than EPR, as our selection strategy in main experiment.</p>
<p>F.2 Easy Samples</p>
<p>As discussed in Section 4.3, we attribute the inferior performance of LLMs on easy samples to their false-positive predictions for negative samples.Upon examining the incorrect cases, we speculate that two main reasons are responsible: (1) LLM hallucination and (2) span boundary mismatch.We provide details on them with specific cases below.</p>
<p>LLM Hallucination.The most distinctive mistakes made by LLMs.Without specific fine-tuning on IE tasks, LLMs (with in-context learning approaches) sometimes hallucinate and provide meaningless, even confusing explanations or answers.For instance, LLMs might predict a verb, preposition or even special symbols as an entity.</p>
<p>Case 1: LLMs predict the word die as entity person-other (Ground truth: None) Sentence: Flavius Rumoridus (died 5th century AD) was a Roman soldier who was appointed consul in AD 403 in the Western Roman Empire at the same time as the eastern emperor Theodosius II served in the same capacity in the East.</p>
<p>Candidate Choices:</p>
<p>(a) died do/does not belong to any known entities.</p>
<p>(b) died is a kind of geopolitical entity.</p>
<p>(c) died is a person, but not affiliated with following professions: actor, artist, athlete, author, director, politician, scholar, soldier.</p>
<p>Analysis: The sentence provides information about Flavius Rumoridus, who was a Roman soldier and appointed consul in AD 403.The sentence also mentions that he died in the 5th century AD, which is a statement about his death.Therefore, the correct answer is (c) because the sentence does not provide any information about Flavius Rumoridus being affiliated with any of the professions listed in option (c).</p>
<p>Correct Answer: (c)</p>
<p>Case 2: LLMs predict the word of as entity organization-other (Ground truth: None)</p>
<p>Sentence: During these years he studied at the South Kensington Schools of the Royal College of Art.</p>
<p>Candidate Choices:</p>
<p>(a) of do/does not belong to any known entities.</p>
<p>(b) of is an educational institute.</p>
<p>(c) of pertains to an organization that does not fall under the categories of company, educational institution, government, media, political party, religion, sports league, sports team, band or musical group.</p>
<p>Analysis: In the given sentence, of is a preposition that indicates the relationship between South Kensington Schools and Royal College of Art.None of the candidate choices directly address the correct relationship between these entities or state that they are not applicable.Therefore, the correct answer is (c): of does not pertain to any known entities mentioned in the choices provided.</p>
<p>Correct Answer: (c) Candidate Choices:</p>
<p>(a) 65th do/does not belong to any known entities.</p>
<p>(b) 65th is a kind of award.</p>
<p>(c) 65th refers to some event except attack, election, natural disaster, protest, revolution and sports.</p>
<p>Analysis: The sentence mentions the 65th British Academy Film Awards, which is an event that recognizes outstanding achievements in the film industry.Therefore, option (c) is incorrect as it excludes events like award ceremonies.Option (a) is also incorrect as the number 65th is associated with the event.</p>
<p>Hence, the correct answer is (b) as 65th refers to the specific edition of the BAFTA Awards.</p>
<p>Correct Answer: (b)</p>
<p>G Details on Adaptive Filter-then-rerank Paradigm</p>
<p>G.1 Demonstration Examples</p>
<p>We convert few-shot IE tasks to multiple-choice questions in our filter-then-rerank paradigm.We show 4 examples used in demonstrations for FewN-ERD dataset in Table 16, for TACREV dataset in Table 17, and for ACE05 datasets in Table 18.</p>
<p>G.2 Template</p>
<p>In our filter-then-rerank paradigm, we utilize templates converting candidate labels to question options.We list the template for FewNERD dataset in Table 19, for TACREV dataset in Table 20, and for ACE05 datasets in Table 21.</p>
<p>Event Detection</p>
<p>We have fought every institution, every law firm ... it 's cost us a fortune but now we 're going on a massive expansion.</p>
<p>Conflict.Attack ✗</p>
<p>None ✓</p>
<p>In the given sentence, the word fought is used to describe the action of the speaker fighting against various institutions and law firms.This does not involve any physical violence or court proceedings, so the word fought does not trigger any known event.</p>
<p>Figure 1 :
1
Figure 1: Examples of prompts used.The green, blue and black parts in the top boxes represent the instruction, demonstration (demo) and test sentence in the prompt respectively.The red parts represent the outputs from LLMs.We plot only 1 example for convenience of visualization.The actual demo number is usually much larger than 1.</p>
<p>Figure 2 :
2
Figure 2: Overall results of SLM-based methods (dashed lines) and LLM-based methods (solid lines) on nine datasets across four IE tasks.The black, horizontal dashed lines represent the SoTA performance on full dataset.</p>
<p>Figure 5 :
5
Figure5: Relationship between confidence scores and performance with/without LLM reranking.We adopt RoBERTa-large as filter and InstructGPT as reranker.</p>
<p>(a)she is the other family member of lawyer (b)she is a lawyer (c)she has no known relations to lawyer Analysis: The word 'she' refers to someone who was upset while recounting certain events in court.The word 'lawyer' refers to someone who denied a news report about that same person weeping in court.There is no information in the sentence to indicate that the two individuals are related in any way.Answer: (c) Adrien said he met the Baptists' leader, Laura Silsby of Meridian, Idaho, in Port-au-Prince on Jan 26.(a)Laura Silsby lives in the state or province Meridian (b)Laura Silsby lives in the city Meridian (c)Laura Silsby was born in the city Meridian (d)Laura Silsby has no known relations to Meridian Analysis: Question Adrien said he met the Baptists' leader, Laura Silsby of Meridian, Idaho, in Port-au-Prince on Jan 26.The last hostage, Italian engineer Eugenio Vagni, was released early Sunday.</p>
<p>Figure 7 :
7
Figure 7: The financial and time cost over 500 sentences.InstructGPT as the reranker.</p>
<p>Y) ∈ D s.t.∃j, y j = y 9: D ← D(X, Y)</p>
<p>) OpenAI models: CODEX (code-davinci-002; Chen et al. 2021), InstructGPT (text-davinci-003; Ouyang et al. 2022), and ChatGPT (gpt-3.5-turbo-0301).</p>
<p>...'.If you do not find any entity in this sentence, just output 'Answer: No entities found.'</p>
<p>Figure 8 :
8
Figure8: Relationship between demo number and F1-score among three datasets.Note that the x-axis in each subfigure represents the number of demos (not the shot value K) during ICL.We adopt sentence embedding as the demo selection strategy and text prompt in this experiment.</p>
<p>Relation Extraction Named Entity Recognition Event Detection Event Argument Extraction</p>
<p>.,
TextTextText</p>
<p>56.0 57.0 58.0 59.0 60.0 I0 I1 I2 I3 I4 I5 Instruction format F1 score 36 40 44 48 52 56 60 4 8 16 32 64 96 Demonstration number F1 score ChatGPT CODEX 52.5 55.0 57.5 60.0 random embed epr Demonstration selection F1 score
Figure 3: LLMs' performance w.r.t prompt variants on 20-shot FewNERD dataset. See full results on other datasets in Appendix E.2-E.5. Left: ChatGPT's performance (F1 Score) across six instruction variants. Middle: F1 Score changes over varying numbers of demo. Right: ChatGPT's performance across three demo selection strategies. Random: Random sampling. Embed: Sentence embedding. EPR: Efficient Prompt Retriever (Rubin et al., 2022).Prompt format f : We use simple textual tem-plates to format the demos and the test sample inmain experiments. For example, the template forNER is "Sentence: [S], Entities: ([type1],[entity1]), ([type2], [entity2])...".</p>
<p>Table 1
1: The inference seconds over 500 sentences (run on single V100 GPU). Here LLaMA is extremely slow since we set batch size as 1 due to memory limit.Dataset (Task)Roberta T5 LLaMA CODEXFewNERD (NER) 2.8 39.4 1135.4 179.4 TACREV (RE) 1.4 45.6 1144.9 151.6 ACE05 (ED) 6.6 62.5 733.4 171.7</p>
<p>rerank Paradigm Read following sentences and identify what is the entity type of "The New Yorker" quoted by <t>. Sentence: In</p>
<p>2004 Gourevitch was assigned to cover the 2004 U.S. presidential election for "<t> The New Yorker <t>".Candidate Choices: (a)The New Yorker does not belong to any known entities.(b)The New Yorker is a broadcast program.(c)The New Yorker is a kind of written art.(d)The New Yorker is a media/newspaper organization.</p>
<p>Table 2 :
2
Comparative ratios of negative to positive samples across various datasets and subsets.We set fixed threshold τ here for simplicity.
FewNERD TACREV ACE05Overall Easy samples (τ &gt; 0.9) Hard samples (τ &lt; 0.6)5.88 9.44 1.283.03 3.21 2.6838.2 44.0 1.36</p>
<p>Table 3 :
3
Overall results of LLM-based ICL methods, SLM-based supervised methods, and our proposed filter-thenrerank (SLM+LLM) methods.The best results are in bold face and the second best are underlined.All results except InstructGPT and GPT-4 are averaged over 5 runs, and sample standard deviations are in the round bracket.
FewNERD (NER) 5-shot 10-shot 20-shot 20-shot 50-shot 100-shot 5-shot 10-shot 20-shot TACREV (RE) ACE (ED)LLM CODEX InstructGPT GPT-453.8(0.5) 54.0(1.4) 55.9(0.5) 59.1(1.4) 60.3(2.4) 62.4(2.6) 47.1(1.2) 47.7(2.8) 47.9(0.5) 53.6(−) 54.6(−) 57.2(−) 60.1(−) 58.3(−) 62.7(−) 52.9(−) 52.1(−) 49.3(−) --57.8(−) --59.3(−) --52.1(−)SLM Previous SoTA + Ensemble (S) + Rerank (S)59.4(1.5) 61.4(0.8) 61.9(1.2) 62.4(3.8) 68.5(1.6) 72.6(1.5) 55.1(4.6) 63.9(0.8) 65.8(2.0) 59.6(1.7) 61.8(1.2) 62.6(1.0) 64.9(1.5) 71.9(2.2) 74.1(1.7) 56.9(4.7) 64.2(2.1) 66.5(1.7) 59.4(1.5) 61.0(1.7) 61.5(1.7) 64.2(2.3) 70.8(2.3) 74.3(2.2) 56.1(0.3) 64.0(1.0) 66.7(1.7)SLM + LLMVicuna-13B 60.0(1.8) 61.9(2.1) 62.2(1.4) 65.2(1.4) 70.8(1.6) 73.8(1.7) 56.9(4.0) 63.5(2.7) 66.0(2.6) + Ensemble (S) + Rerank (L) 59.9(0.7) 62.1(0.7) 62.8(1.1) 66.5(0.5) 73.6(1.4) 75.0(1.5) 57.9(5.2) 64.4(1.2) 66.2(2.4) + Rerank (L) InstructGPT + Rerank (L) 60.6(2.1) 62.7(0.8) 63.3(0.6) 66.8(2.6) 72.3(1.4) 75.4(1.5) 57.8(4.6) 65.3(1.7) 67.3(2.2) + Ensemble (S) + Rerank (L) 61.3(1.9) 63.2(0.9) 63.7(1.8) 68.9(1.3) 74.8(1.3) 76.8(1.2) 59.5(3.7) 65.3(1.9) 67.8(2.1) GPT-4 + Rerank (L) 60.8(2.3) 62.6 (2.7) 63.0(1.3) 65.9(2.7) 72.3(0.3) 74.5(1.5) 59.6</p>
<p>Table 5 :
5
Ablation study on three datasets.The filter is ensembled SLMs and the reranker is GPT-4.
CoT Demo LF ADFewNERD (20-shot)TACREV (100-shot)ACE05 (20-shot)✓✓✓✓63.6(1.2) 75.9(2.4) 67.8(1.7)✗✓✓✓63.2(1.2) 75.4(2.4) 67.2(1.7)✗✗✓✓63.0(1.4) 74.9(2.2) 66.6(1.5)✗✗✗✓62.4(2.1) 73.8(2.5) 66.5(1.3)✗✗✗✗12.5(2.7) 59.9(6.0) 5.4(1.1)Previous SoTA methods 62.6(1.0) 74.1(1.7) 66.5(1.7)</p>
<p>models as zero-shot relation extractors.In Findings of the Association for Computational Linguistics: ACL 2023, pages 794-812, Toronto, Canada.Association for Computational Linguistics.
Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. 2017. Position-aware attention and supervised data improve slot filling. In Proceedings of the 2017 Conference on Empiri-cal Methods in Natural Language Processing, pages 35-45, Copenhagen, Denmark. Association for Com-putational Linguistics.Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023b. Automatic chain of thought prompt-ing in large language models. In The Eleventh In-ternational Conference on Learning Representations (ICLR 2023).</p>
<p>Table 6 :
6
Statistics of nine datasets used.Note that the #mentions for event detection tasks refers to the number of trigger words, while the #mentions for event argument extraction tasks refers to the number of arguments.
DatasetNamed Entity Recognition Relation Extraction CONLL OntoNotes FewNERD TACREV TACRED ACE05 MAVEN ERE ACE05 RAMS ERE Event Detection Event Arg Extraction#Label Type41866414133168383313938#SentsTrain 14,041 Test 3,45349,706 10,348131,965 37,64868,124 15,50968,124 14,024 32,360 14,736 14,024 7329 14,736 15,509 728 8,035 1,163 728 871 1,163#MentionsTrain 23,499 128,738 Test 5,648 12,586340,247 96,90213,012 3,12313,012 5,349 77,993 6,208 4859 17026 8924 3,123 424 18,904 551 576 2023 822</p>
<p>Table 7 :
7
The statistics of few-shot training sets.We set different random seeds and generate 5 training sets for each setting.We report their average statistics.
Dataset Settings # Labels # Sent # Sample # Avg shotCONLL'031-shot 5-shot 10-shot 20-shot44.8 16.2 29.2 65.65.8 21.8 42.6 82.01.4 5.5 10.7 20.5OntoNotes1-shot 5-shot 10-shot 20-shot1820.0 84.8 158.6 332.833.4 148.0 281.0 547.21.9 8.2 15.6 30.4FewNERD1-shot 5-shot 10-shot 20-shot6689.8 286.2 538.0 1027.2147.0 494.8 962.0 1851.42.2 7.5 14.6 28.1TACREV1-shot 5-shot 10-shot 20-shot 50-shot 100-shot4181.6 387.6 741.2 1367.2 2872.0 4561.041.0 205.0 406.0 806.0 1944.0 3520.01.0 5.0 9.9 19.7 47.4 85.9TACRED1-shot 5-shot 10-shot 20-shot 50-shot 100-shot4181.6 387.6 741.2 1367.2 2871.2 4575.241.0 205.0 406.0 806.0 1944.0 3520.01.0 5.0 9.9 19.7 47.4 85.9ACE051-shot 5-shot 10-shot 20-shot3347.4 192.8 334.6 579.441.0 165.0 319.4 598.21.2 5.0 9.7 18.1MAVEN1-shot 5-shot 10-shot 20-shot168157.6 540.4 891.2 1286.4298.0 1262.2 2413.8 4611.41.8 7.5 14.4 27.4ERE1-shot 5-shot 10-shot 20-shot3848.4 175.0 304.8 521.654.6 219.2 432.4 806.61.4 5.8 11.4 21.2ACE051-shot 5-shot 10-shot 20-shot3323.4 79.8 130.8 213.440.2 178.2 337.4 630.21.2 5.4 10.2 19.1RAMS1-shot 5-shot 10-shot 20-shot139130.2 514.0 795.2 1070.4332.6 1599.6 3193.2 6095.42.4 11.5 23.0 43.9ERE1-shot 5-shot 10-shot 20-shot3821.6 74.2 127.2 190.2102.8 403.4 775.6 1397.22.7 10.6 20.4 36.8</p>
<p>Table 8 :
8
F1-scores across different t values.Experiments run on 10-shot settings with CODEX.
FewNERD TACREV ACE05t = 0 + 5-ensemble48.5(1.9) 53.7(2.3) 42.9(2.2) 53.5(1.3) 58.6(1.5) 46.3(0.8)t = 0.7 + self-consistency 52.1(0.9) 53.4(1.3) 45.6(3.0) 40.9(2.3) 39.9(1.2) 35.6(1.0)original datasets, we follow Automatic Chain-of-thought (Auto-CoT; Zhang et al. 2023b) method asbelow. Regarding each sample, we query LLMsAccording to [sentence], Why [span] is a [label].</p>
<p>Table 9 :
9
The F1-score difference between with and without Auto-CoT.We generate rationales by Instruct-GPT, then adopt ICL w.Auto-CoT approach and use CODEX as our backbone for inference.
10-shot train setFewNERD (NER)TACREV (RE)ACE05 (ED)wo. Auto-CoT w. Auto-CoT54.0(1.4) 57.3(1.8) 47.7(2.8) 36.6(1.7) 22.0(1.2) 43.1(3.4)</p>
<p>Table 10 :
10
F1-scores difference among GPT-4, CODEX and InstructGPT.
NER (20-shot) CONLL OntoNotes FewNERD TACREV TACRED ACE05 MAVEN ERE ACE05 RAMS ERE RE (100-shot) ED (20-shot) EAE (20-shot)InstructGPT CODEX GPT-477.2 81.1 84.747.7 55.6 65.657.2 55.9 57.862.7 62.4 59.353.8 53.6 50.449.3 47.9 52.125.4 40.8 45.8 22.8 39.0 -30.2 40.5 42.942.2 41.9 --38.6 38.2Supervised SoTA 72.374.961.472.663.165.854.7 56.2 55.257.7 55.6D.3 GPT-4 v.s. Others</p>
<p>Table 11 :
11
Performance comparison between LLMs (ChatGPT) and SLM-based methods among datasets with various schema complexities.
Named Entity Recognition CoNLL OntoNotes FewNERD 4 18 66 Micro-F1 (SLM) # Entity 52.5 59.7 59.4 Micro-F1 (LLM) 77.8 59.4 55.5 ∆F1 (LLM, SLM) 25.3 -0.3 -3.9Event Detection ACE05 ERE 33 38 55.1 48.0 39.6 33.8 ∆F1 (LLM, SLM) -15.5 # Event Micro-F1 (SLM) Micro-F1 (LLM) -14.2MAVEN 168 49.4 25.3 -24.1Event Argument Extraction ACE05 ERE # Event / #Role 33 / 22 38 / 26 Head-F1 (SLM) 45.9 40.4 Head-F1 (LLM) 52.8 40.7 ∆F1 (LLM, SLM)RAMS 139 / 65 54.1 44.2</p>
<p>Table 12 :
12
F1-scores across six instruction formats.Experiments run on 20-shot settings with ChatGPT.
FewNERD (NER)TACREV (RE)ACE (ED)ACE (EAE)I0 I1 I2 I3 I4 I557.6(2.1) 58.3(0.5) 57.7(1.0) 57.6(2.3) 56.8(0.9) 57.8(0.5)49.1(2.4) 49.6(1.2) 50.0(1.5) 52.3(1.8) 49.6(2.9) 47.2(1.8)44.0(1.4) 50.9(0.1) 42.6(1.0) 51.5(1.1) 41.8(0.9) 50.3(1.5) 42.9(1.3) 49.2(2.3) 41.6(1.9) 49.9(1.2) 43.1(1.8) 50.6(1.8)</p>
<p>Table 13 :
13
F1-scores on three demo-selection strategies.Experiments run on 20-shot settings with ChatGPT.
FewNERD (NER)TACREV (RE)ACE (ED)Random Sampling Sentence Embedding Efficient Prompt Retriever 57.2(0.6) 48.0(0.8) 43.5(1.4) 53.2(0.4) 43.0(3.3) 38.0(1.5) 57.6(2.3) 49.6(1.2) 42.9(1.3)</p>
<p>Table 14 :
14
F1-scores across three prompt formats.Experiments run on 20-shot settings with ChatGPT.
FewNERD (NER)TACREV (RE)ACE (ED)ACE (EAE)Text 57.6(2.3) 49.6(1.2) 42.9(1.3) 51.5(1.1) Code 53.2(0.9) 50.2(1.8) 44.3(2.0) 47.3(1.5)E.5 Finding Better Prompt FormatPrevious studies on LLMs for few-shot IE taskshave explored different prompt formats and high-lighted the importance of selecting an appropri-ate format for achieving competitive performance.Therefore, we investigate two commonly-used vari-ants in previous work: (1) Text prompt as shown inFigure 1. (2) Code prompt: We follow Wang et al.(2023b); Li et al. (2023) and recast the output of IEtasks in the form of code. See more details aboutthis format in their original papers.Table 14 shows comparable performance acrossall formats. Based on simplicity, we choose thetext prompt for our main experiment.</p>
<p>Table 15 showcases some hard examples which benefits from our LLM reranking.In accordance with our intuition, we observe that the LLM rerankers correct two kinds of erroneous predictions made by LLMs.(1) The lack of external knowledge, such as the first (Triptolemus is a figure in Greek mythology) and third examples (Minas Gerais is a state instead of city).(2) Limited reasoning abilities, such as the second (His wife's children are his children) and the fourth (The word "fought" in this sentence does not involve any physical violence) examples.</p>
<p>LLMs predict Baron, a subspan of Baron Carl Hurleman, as the entity person-other (The ground-truth entity type of Baron is None.And the ground-truth entity type of Baron Carl Hurleman is person-artist/author).
paradigm adopt sample-level approaches. In other words, LLMs only consider each word/span candi-date independently. The sample-level approaches enable LLMs to rerank candidates with the help of multi-choice question prompt. However, they alsoSentence: Ralph Fiennes was nominated for the BAFTA Award for Outstanding Debut by a British Writer, Director or Producer at the 65th British Academy Film Awards.aggravate the span boundary mismatching prob-lem in NER and ED tasks. We empirically findeasy samples suffering much more from boundarymismatching problem than hard samples.Case 3: Sentence: Additions in the 1740s were by Baron Carl Hurleman (1700 -1753).Candidate Choices:(a) Baron do/does not belong to any knownentities.(b) Baron is a person, but not affiliated withfollowing professions: actor, artist, ath-lete, author, director, politician, scholar,soldier.(c) Baron is a kind of geopolitical entityAnalysis: In the given sentence, Baron is a title given to a person named Carl Hurleman,who is mentioned as the one who made ad-ditions in the 1740s. The sentence does notprovide any information about connectionsto geopolitical entities or any other knownentities mentioned in the choices provided.Therefore, the correct answer is (b): Baron isa person, but not affiliated with following pro-fessions: actor, artist, athlete, author, director,politician, scholar, soldier.Correct Answer: (b)Case 4: LLMs predict 65th, a subspan ofSpan boundary mismatch. Different from sentence-level approaches which take one or mul-tiple sentences as an unit and assign each word'slabel globally, LLMs under our filter-then-rerank
65th British Academy Film Awards, as the entity other-awards (The ground-truth entity type of 65th is None.And the ground-truth entity type of 65th British Academy Film Awards is other-awards).</p>
<p>Table 15 :
15
Examples of the samples corrected by LLM reranking.We sample four examples from NER, RE and ED tasks, respectively.Sentences:The sentences in which samples locate.We color the samples (entities or trigger words) to be identified.Before: The prediction before LLM reranking.Based on SLM-based methods.After: The reranked prediction using LLMs.Rationales: LLM-generated Explanations.The sentence states that 'Eliza Samudio, Bruno's ex-girlfriend and his 4-month-old baby boy's mother, disappeared from her hometown of Belo Horizonte, Minas Gerais.'This indicates that Eliza Samudio is from the city of Belo Horizonte, which is located in the state of Minas Gerais.So Eliza Samudio lives in the state or province Minas Gerais.
Sentences (entity/event)BeforeAfterRationalesNamed Entity RecognitionAccording to it, King Eleusis and Cothonea (Cyntinia), parents of Trip-tolemus, are visited by Demeter, who rears their son, feeding him divine milk by day and placing him into the fire at night, which makes Triptolemus grow faster than mortal children normally do.person-other ✗ Relation Extraction other-god ✓According to the context, Triptolemus is a figure in Greek mythology, the son of King Eleusis and Cothonea (Cyntinia). He was a demigod who was sent by Demeter to teach mankind the art of agriculture. So Triptole-mus is a god in some legend/religious story.Survivors include his wife, Sandra; four sons, Jeff, James, Douglas and Harris; a daughter, Leslie; his mother, Sally; and two brothers, Guy and Paul.None ✗per:children ✓The sentence states 'Survivors include his wife, Sandra; four sons, Jeff, James, Douglas and Harris; a daughter, Leslie; his mother, Sally; and two brothers, Guy and Paul.' This indicates 'his' is the father of Jeff, James, Douglas and Harris, and the husband of San-dra. So his is the parent of Jeff.Eliza Samudio, Bruno 's ex-girlfriend and his 4-month-old baby boy's mother, Horizonte, Minas Gerais. disappeared from her hometown of Beloper:cities_of _residence ✗per:states_of _residence ✓</p>
<p>Table 19 :
19
Templates for FewNERD dataset, where {ent} is the placeholder for entity type.person,but not affiliated with following professions: actor, artist, athlete, author, director, politician, scholar, soldier.organization-other{ent} pertains to an organization that does not fall under the categories of company, educational institution, government, media, political party, religion, sports league, sports team, band or musical group.geographic locaton that does not fall under the categories of geopolitical entity, body of water, island, mountain, park, road, railway and transit.
EntityTemplateno-entity{ent} do/does not belong to any known entities.person-artist/author{ent} is an artist or author.person-actor{ent} is an actor.art-writtenart{ent} is a kind of writtenart.person-director{ent} is a director.person-other {ent} is a organization-company {ent} is a companyorganization-sportsteam{ent} is a sports teamorganization-sportsleague{ent} is a sports leagueproduct-car{ent} is a kind of carevent-protest{ent} refers to a protest, uprising or revolution eventorganization-government/governmentagency{ent} refers to a government or governmental agencyother-biologything{ent} is a special term about biology / life science.location-GPE{ent} is a kind of geopolitical entitylocation-other {ent} is a person-athlete {ent} is an athlete or coach.art-broadcastprogram{ent} is a broadcast program.product-other{ent} is a kind of product that does not fall under the categories of airplane, train, ship, car, weapon, food, electronic game and software.building-other{ent} is a kind of building that does not fall under the categories of airport, hospital, hotel, library, restaurant, sports facility and theaterproduct-weapon{ent} is a kind of weapon.building-airport{ent} is an airport.building-sportsfacility{ent} is a sports facility building.person-scholar{ent} is a scholar.art-music{ent} is a music.event-other{ent} refers to some event except attack, election, natural disaster, protest, revolution and sportsother-language{ent} is a kind of human language.other-chemicalthing{ent} is some special term about chemical science.art-film{ent} is a film.building-hospital{ent} is a hospital.other-law{ent} is a legal document, a term or a convention in legal sense.product-airplane{ent} is kind of airplane product.location-road/railway/highway/transit{ent} is a geographic position about roadways, railways, highways or public transit systems.person-soldier{ent} is a soldierlocation-mountain{ent} is geographic position about mountain.organization-education{ent} is an educational institute/organization.
organization-media/newspaper{ent} is a media/newspaper organization.</p>
<p>Table 20 :
20
(Lu et al., 2022a)EV dataset, where {subj} and {obj} are the placeholders for subject and object entities.Copied from(Lu et al., 2022a)
RelationTemplateno_relation{subj} has no known relations to {obj}per:stateorprovince_of_death{subj} died in the state or province {obj}per:title{subj} is a {obj}org:member_of{subj} is the member of {obj}per:other_family{subj} is the other family member of {obj}org:country_of_headquarters{subj} has a headquarter in the country {obj}org:parents{subj} has the parent company {obj}per:stateorprovince_of_birth{subj} was born in the state or province {obj}per:spouse{subj} is the spouse of {obj}per:origin{subj} has the nationality {obj}per:date_of_birth{subj} has birthday on {obj}per:schools_attended{subj} studied in {obj}org:members{subj} has the member {obj}org:founded{subj} was founded in {obj}per:stateorprovinces_of_residence{subj} lives in the state or province {obj}per:date_of_death{subj} died in the date {obj}org:shareholders{subj} has shares hold in {obj}org:website{subj} has the website {obj}org:subsidiaries{subj} owns {obj}per:charges{subj} is convicted of {obj}org:dissolved{subj} dissolved in {obj}org:stateorprovince_of_headquarters{subj} has a headquarter in the state or province {obj}per:country_of_birth{subj} was born in the country {obj}per:siblings{subj} is the siblings of {obj}org:top_members/employees{subj} has the high level member {obj}per:cause_of_death{subj} died because of {obj}per:alternate_names{subj} has the alternate name {obj}org:number_of_employees/members{subj} has the number of employees {obj}per:cities_of_residence{subj} lives in the city {obj}org:city_of_headquarters{subj} has a headquarter in the city {obj}per:children{subj} is the parent of {obj}per:employee_of{subj} is the employee of {obj}org:political/religious_affiliation{subj} has political affiliation with {obj}per:parents{subj} has the parent {obj}per:city_of_birth{subj} was born in the city {obj}per:age{subj} has the age {obj}per:countries_of_residence{subj} lives in the country {obj}org:alternate_names{subj} is also known as {obj}per:religion{subj} has the religion {obj}per:city_of_death{subj} died in the city {obj}per:country_of_death{subj} died in the country {obj}org:founded_by{subj} was founded by {obj}
A more precise assertion is that current LLMs, with vanilla prompting setting and without IE-specific fine-tuning, are not good few-shot information extractors in general.
All LLMs discussed in this paper are not fine-tuned, and results for LLMs are based on in-context learning.
Label types denote entity/relation/event/role types in different tasks. We use them interchangeably there-in-after.
  4  Samples refer to (i) demonstrations in ICL of LLMs, or (ii) training samples for SLMs' fine-tuning.
We slightly abuse the notation f to allow s, y and {(s, y)} as the input for simplicity.
The versions of model we use are: gpt-3.5-turbo-0301,
The Subset Sum Problem, a classical NP-complete problem, can be reduced to this sampling problem.
https://pytorch.org/docs/stable/amp.html
https://github.com/zjunlp/KnowPrompt
https://github.com/mayubo2333/PAIE
https://github.com/universal-ie/UIE
https://huggingface.co/docs/accelerate
https://openai.com/blog/openai-api
https://help.openai.com/en/articles/6654000-bestpractices-for-prompt-engineering-with-openai-api
AcknowlegementThis study is supported under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 grant, as well as cash and in-kind contribution from the industry partner(s).Table16: Demo examples used in FewNERD dataset.We color the entity in blue.Instruct: Read following sentences and identify what is the entity type of 392 quoted by <t>.Sentence: Powell v.Texas , <t> 392 <t> U.S. 514( 1968 ), was a United States Supreme Court case that ruled that a Texas statute criminalizing public intoxication did not violate the Eighth Amendment protection against cruel and unusual punishment.(a) 392 is a legal document, a term or a convention in legal sense.(b) 392 does not belong to any known entities.(c) 392 refers to a protest, uprising or revolution event (d) 392 refers to a government or governmental agency Analysis: In the context you provided, 392 refers to the volume number in the United States Reports where the Supreme Court's decision in Powell v.Texas can be found.However, 392 itself does not refer to a legal document.So 392 do/does not belong to any known entities.Answer: (b) Instruct: Read following sentences and identify what is the entity type of The New Yorker quoted by <t>.Sentence: In 2004 Gourevitch was assigned to cover the 2004 U.S. presidential election for " <t> The New Yorker <t> ".(a) The New Yorker does not belong to any known entities.(b) The New Yorker is a broadcast program.(c) The New Yorker is a kind of written art.(d) The New Yorker is a media/newspaper organization.Analysis: The New Yorker is a well-known American magazine that has been published since 1925, and is primarily known for its long-form journalism, commentary, and satire.It has a reputation for publishing high-quality writing on a wide variety of topics, including politics, culture, and the arts.So The New Yorker is a media/newspaper organization.Instruct: Read the sentence and determine the relation between she and lawyer quoted by <t>.Sentence: The <t> lawyer <t> denied Italian news reports that she wept while addressing the court, but said Knox was upset as <t> she <t> recounted " the pressure, the aggressiveness of the police who called her a liar ." (a) she is the other family member of lawyer (b) she is a lawyer (c) she has no known relations to lawyer Analysis: In the sentence, the word 'she' refers to someone who was upset while recounting certain events in court.The word 'lawyer' refers to someone who denied a news report about that same person weeping in court.There is no information in the sentence to indicate that the two individuals are related in any way.So she has no known relations to lawyer.Answer: (c) Instruct: Read the sentence and determine the relation between MEF and Myanmar Equestrian Federation quoted by <t>.Sentence: YANGON , Dec. 27 -LRB-Xinhua -RRB--Myanmar will hold a horse race in Yangon to commemorate the country 's 63rd Anniversary Independence Day , the <t> Myanmar Equestrian Federation <t> -LRB-<t> MEF <t> -RRBconfirmed to Xinhua on Monday.Instruct: Read the sentence and determine the relation between FAA and U.S. quoted by <t>.Sentence: On its Web site , the <t> U.S. <t> <t> FAA <t> says the Category 2 rating means the country lacks the laws or regulations that are needed for the certification and oversight of air carriers , according to minimum international standards.(a) FAA is also known as U.S. (b) FAA has no known relations to U.S. (c) FAA has a headquarter in the country U.S. Analysis: The sentence states that the FAA says the Category 2 rating means the country lacks the laws or regulations needed for the certification and oversight of air carriers, indicating that the FAA is responsible for overseeing aviation regulations in the country.Actually the FAA (Federal Aviation Administration) is a U.S. government agency responsible for regulating and overseeing civil aviation in the United States, and it has its headquarters in Washington, D.C..So FAA has a headquarter in the country U.S.. Answer: (c)  Sentence: And I won't dwell on the irony of an Oracle employee being driven out of Oracle , starting his own company , and forcing Ellison to spend $ 10.3 billion to get his company -but not him -back ( though it does rather delightfully remind me of Coca -Cola basically giving away the bottling franchise and then spending billions to <t> buy <t> it back ) .(a) The word buy triggers a DECLARE-BANKRUPTCY event: an Entity officially requests legal protection from debt collection due to an extremely negative balance sheet.(b) The word buy triggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or receiving of artifacts or organizations by an individual or organization.(c) The word buy does not trigger any known event.Analysis: In the given sentence, the word buy is used to describe the action of Oracle spending $10.3 billion to get a company back.This clearly involves the transfer of ownership of the company from one entity to another.So the word buy triggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or receiving of artifacts or organizations by an individual or organization.organization-religion {ent} is a religious organization.other-medical {ent} refers to some kind of medicine.entitylocation-park {ent} is a park.other-god {ent} is a god in some legend/religious story.product-food {ent} is a kind of food.product-train {ent} is a kind of train(vehicle).art-painting {ent} is an art painting.Table21: Templates for ACE05 dataset, where {evt} is the placeholder for event type.Event Template no-event The word {evt} does not trigger any known event.Movement.TransportThe word {evt} triggers a TRANSPORT event: an ARTIFACT (WEAPON or VEHICLE) or a PERSON is moved from one PLACE (GEOPOLITICAL ENTITY, FACILITY, LOCATION) to another.Personnel.ElectThe word {evt} triggers an ELECT event which implies an election.Personnel.Start-PositionThe word {evt} triggers a START-POSITION event: a PERSON elected or appointed begins working for (or changes offices within) an ORGANIZATION or GOVERN-MENT.Personnel.NominateThe word {evt} triggers a NOMINATE event: a PERSON is proposed for a position through official channels.Conflict.AttackThe word {evt} triggers an ATTACK event: a violent physical act causing harm or damage.Personnel.End-PositionThe word {evt} triggers an END-POSITION event: a PERSON stops working for (or changes offices within) an ORGANIZATION or GOVERNMENT.Contact.MeetThe word {evt} triggers a MEET event: two or more entities come together at a single location and interact with one another face-to-face.Life.MarryThe word {evt} triggers a MARRY event: two people are married under the legal definition.Contact.Phone-WriteThe word {evt} triggers a PHONE-WRITE event: two or more people directly engage in discussion which does not take place 'face-to-face'.Transaction.Transfer-MoneyThe word {evt} triggers a TRANSFER-MONEY event: giving, receiving, borrowing, or lending money when it is NOT in the context of purchasing something.Justice.SueThe word {evt} triggers a SUE event: a court proceeding has been initiated for the purposes of determining the liability of a PERSON, ORGANIZATION or GEOPO-LITICAL ENTITY accused of committing a crime or neglecting a commitmentConflict.DemonstrateThe word {evt} triggers a DEMONSTRATE event: a large number of people come together in a public area to protest or demand some sort of official action.For eample: protests, sit-ins, strikes and riots.Business.End-OrgThe word {evt} triggers an END-ORG event: an ORGANIZATION ceases to exist (in other words, goes out of business).Life.InjureThe word {evt} triggers an INJURE event: a PERSON gets/got injured whether it occurs accidentally, intentionally or even self-inflicted.Life.DieThe word {evt} triggers a DIE event: a PERSON dies/died whether it occurs accidentally, intentionally or even self-inflicted.Justice.Arrest-Jail The word {evt} triggers a ARREST-JAIL event: a PERSON is sent to prison.Transaction.Transfer-OwnershipThe word {evt} triggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or receiving of artifacts or organizations by an individual or organization.Justice.ExecuteThe word {evt} triggers an EXECUTE event: a PERSON is/was executed Justice.Trial-Hearing The word {evt} triggers a TRIAL-HEARING event: a court proceeding has been initiated for the purposes of determining the guilt or innocence of a PERSON, ORGANIZATION or GEOPOLITICAL ENTITY accused of committing a crime.Justice.SentenceThe word {evt} triggers a SENTENCE event: the punishment for the DEFENDANT is issuedLife.Be-Born The word {evt} triggers a BE-BORN event: a PERSON is given birth to.Justice.Charge-Indict The word {evt} triggers a CHARGE-INDICT event: a PERSON, ORGANIZATION or GEOPOLITICAL ENTITY is accused of a crimeBusiness.Start-OrgThe word {evt} triggers a START-ORG event: a new ORGANIZATION is created.Justice.ConvictThe word {evt} trigges a CONVICT event: a PERSON, ORGANIZATION or GEOPOLITICAL ENTITY is convicted whenever it has been found guilty of a CRIME.Business.Declare-BankruptcyThe word {evt} triggers a DECLARE-BANKRUPTCY event: an Entity officially requests legal protection from debt collection due to an extremely negative balance sheet.Justice.Release-ParoleThe word {evt} triggers a RELEASE-PAROLE event.10600Justice.FineThe word {evt} triggers a FINE event: a GEOPOLITICAL ENTITY, PERSON or ORGANIZATION get financial punishment typically as a result of court proceedings.Justice.PardonThe word {evt} triggers a PARDON event: a head-of-state or their appointed representative lifts a sentence imposed by the judiciary.Justice.AppealThe word {evt} triggers a APPEAL event: the decision of a court is taken to a higher court for review Business.Merge-Org The word {evt} triggers a MERGE-ORG event: two or more ORGANIZATION Entities come together to form a new ORGANIZATION Entity.Justice.ExtraditeThe word {evt} triggers a EXTRADITE event.Life.Divorce The word {evt} triggers a DIVORCE event: two people are officially divorced under the legal definition of divorce.Justice.AcquitThe word {evt} triggers a ACQUIT event: a trial ends but fails to produce a conviction.
Large language models are few-shot clinical information extractors. Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, David Sontag, 10.18653/v1/2022.emnlp-main.130Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>TACRED revisited: A thorough evaluation of the TACRED relation extraction task. Christoph Alt, Aleksandra Gabryszak, Leonhard Hennig, 10.18653/v1/2020.acl-main.142Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Is GPT-3 a good data annotator?. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Bosheng Ding, Chengwei Qin, Linlin Liu, Ken Yew, Boyang Chia, Shafiq Li, Lidong Joty, Bing, 10.18653/v1/2023.acl-long.626Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Few-NERD: A few-shot named entity recognition dataset. Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, Zhiyuan Liu, 10.18653/v1/2021.acl-long.248Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>The automatic content extraction (ACE) program -tasks, data, and evaluation. George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, Ralph Weischedel, Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC'04). the Fourth International Conference on Language Resources and Evaluation (LREC'04)Lisbon, Portugal2004European Language Resources Association (ELRA</p>
<p>Multi-sentence argument linking. Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, Benjamin Van Durme, 10.18653/v1/2020.acl-main.718Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Exploring the feasibility of chatgpt for event extraction. Jun Gao, Huan Zhao, Changlong Yu, Ruifeng Xu, 2023</p>
<p>SimCSE: Simple contrastive learning of sentence embeddings. Tianyu Gao, Xingcheng Yao, Danqi Chen, 10.18653/v1/2021.emnlp-main.552Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>On calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, AustraliaPMLR2017. 2017. 6-11 August 201770of Proceedings of Machine Learning Research</p>
<p>Thinking about GPT-3 in-context learning for biomedical IE? think again. Jimenez Bernal, Nikolas Gutierrez, Clayton Mcneal, You Washington, Lang Chen, Huan Li, Yu Sun, Su, 10.18653/v1/2022.findings-emnlp.329Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction. Martin Josifoski, Marija Sakota, Maxime Peyrard, Robert West, 2023</p>
<p>Maieutic prompting: Logically consistent reasoning with recursive explanations. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, 10.18653/v1/2022.emnlp-main.82Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>CodeIE: Large code generation models are better few-shot information extractors. Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang, Xipeng Qiu, 10.18653/v1/2023.acl-long.855Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Dublin, Ireland and OnlineAssociation for Computational Linguistics2022. DeeLIO 2022Proceedings of Deep Learning Inside Out</p>
<p>. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 2019Roberta: A robustly optimized bert pretraining approach</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USA2019. May 6-9, 2019OpenReview.net</p>
<p>Summarization as indirect supervision for relation extraction. Keming Lu, I-Hung Hsu, Wenxuan Zhou, Mingyu , Derek Ma, Muhao Chen, 10.18653/v1/2022.findings-emnlp.490Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022a</p>
<p>Pivoine: Instruction tuning for open-world information extraction. Keming Lu, Xiaoman Pan, Kaiqiang Song, Hongming Zhang, Dong Yu, Jianshu Chen, abs/2305.14898ArXiv preprint. 2023</p>
<p>Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, 10.18653/v1/2022.acl-long.556Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022b1</p>
<p>Unified structure generation for universal information extraction. Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, Hua Wu, 10.18653/v1/2022.acl-long.395Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022c1</p>
<p>Label semantics for few shot named entity recognition. Jie Ma, Miguel Ballesteros, Srikanth Doss, Rishita Anubhai, Sunil Mallya, Yaser Al-Onaizan, Dan Roth, 10.18653/v1/2022.findings-acl.155Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022a</p>
<p>Prompt for extraction? PAIE: Prompting argument interaction for event argument extraction. Yubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi Chen, Kun Wang, Jing Shao, 10.18653/v1/2022.acl-long.466Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022b1</p>
<p>Few-shot event detection: An empirical study and a unified view. Yubo Ma, Zehao Wang, Yixin Cao, Aixin Sun, 10.18653/v1/2023.acl-long.628Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada20231Association for Computational Linguistics</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , 2023</p>
<p>Creator: Tool creation for disentangling abstract and concrete reasoning of large language models. Chi Cheng Qian, Yi R Han, Yujia Fung, Zhiyuan Qin, Heng Liu, Ji, 2023</p>
<p>Is chatgpt a general-purpose natural language processing task solver? Colin Raffel. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 21672023. 2020Exploring the limits of transfer learning with a unified text-to-text transformer</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, 10.18653/v1/2022.naacl-main.191Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>From light to rich ERE: Annotation of entities, relations, and 10583 events. Zhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese, Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick, Neville Ryant, Xiaoyi Ma, 10.3115/v1/W15-0812Proceedings of the The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation. the The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and RepresentationDenver, ColoradoAssociation for Computational Linguistics2015</p>
<p>Selective annotation makes language models better few-shot learners. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, 2022</p>
<p>Recitation-augmented language models. Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, Denny Zhou, International Conference on Learning Representations. 2023</p>
<p>Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. Erik F Tjong, Kim Sang, Fien De Meulder, Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003. the Seventh Conference on Natural Language Learning at HLT-NAACL 20032003</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>Revisiting relation extraction in the era of large language models. Somin Wadhwa, Silvio Amir, Byron Wallace, 10.18653/v1/2023.acl-long.868Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Instructuie: Multi-task instruction tuning for unified information extraction. Xiao Wang, Wei Zhou, Can Zu, Han Xia, Tianze Chen, Yuan Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, J Yang, Siyuan Li, Chunsai Du, ArXiv preprint, abs/2304.080852023a</p>
<p>MAVEN: A Massive General Domain Event Detection Dataset. Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, Jie Zhou, 10.18653/v1/2020.emnlp-main.129Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020Association for Computational Linguistics</p>
<p>Code4Struct: Code generation for few-shot event structure prediction. Xingyao Wang, Sha Li, Heng Ji, 10.18653/v1/2023.acl-long.202Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The Eleventh International Conference on Learning Representations. 2023cICLR 2023</p>
<p>Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Rohitha Phani, Pulkit Kaza, Ravsehaj Verma, Rushang Singh Puri, Savan Karia, Doshi, Keyur Shailaja, Siddhartha Sampat, Sujan Mishra, A Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, 10.18653/v1/2022.emnlp-main.340Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi2022United Arab Emirates. Association for Computational Linguistics</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022a. April 25-29, 2022OpenReview.net</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai Hsin Chi, Quoc Le, Denny Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing Systems2022b</p>
<p>Zero-shot information extraction via chatting with chatgpt. Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, Wenjuan Han, 2023</p>
<p>Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium. Philadelphia, PA2013</p>
<p>Simple and effective few-shot named entity recognition with structured nearest neighbor learning. Yi Yang, Arzoo Katiyar, 10.18653/v1/2020.emnlp-main.516Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Generate rather than retrieve: Large language models are strong context generators. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang, International Conference for Learning Representation. 2023ICLR 2023</p>
<p>Aligning instruction tasks unlocks large language. Kai Zhang, Bernal Jimenez Gutierrez, Yu Su, 10.18653/v1/2023.findings-acl.502023a</p>            </div>
        </div>

    </div>
</body>
</html>
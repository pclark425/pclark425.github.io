<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6217 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6217</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6217</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35" target="_blank">The False Promise of Imitating Proprietary LLMs</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is concluded that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that can be bridged using an unwieldy amount of imitation data or by using more capable base LMs.</p>
                <p><strong>Paper Abstract:</strong> An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6217.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6217.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Crowdworker Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mechanical Turk Crowdworker Pairwise Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Blind pairwise comparisons by crowdworkers between ChatGPT and imitation model outputs across held-out prompts from ShareGPT; used to measure subjective overall preference and perceived quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The False Promise of Imitating Proprietary LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended instruction-following / dialogue and general QA (ShareGPT prompts spanning generation, QA, coding, summarization, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>human (Mechanical Turk crowdworkers)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Blind pairwise comparisons on Amazon Mechanical Turk: ~70–71 crowdworkers, 255 held-out prompts, 3 independent ratings per example (majority vote), workers selected with ≥95% approval, located in English-speaking countries, ≥100 HITs; paid ≈$15/hour.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Pairwise preference (which output is better or equal) averaged across examples; average preference percentages and standard deviation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Crowdworkers often rated imitation models' outputs as equal or better than ChatGPT (≈70% equal/better for early models), and ratings saturated as imitation data increased; however these subjective preferences did not align with automatic benchmark performance (e.g., factuality and QA accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Not applicable (this entry is human evaluators), but paper reports human limitations: non-expert crowdworkers are easily deceived by style (confident, well-structured answers) and may miss factual errors without domain expertise or time to verify.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Crowdworkers gave high ratings to imitation-model outputs that were stylistically similar but factually incorrect (Figure 2); human raters failed to detect factual regressions introduced by imitation fine-tuning on broad conversational data (e.g., declines on Natural Questions despite positive human ratings).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Engage domain experts for evaluations, curate highly difficult and targeted prompts that probe factuality, require longer/verifiable checks for factual claims, and complement human subjective ratings with targeted automatic benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The False Promise of Imitating Proprietary LLMs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6217.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6217.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 as Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Blind Pairwise Preference Evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using GPT-4 to perform blind pairwise comparisons between ChatGPT and imitation model outputs as an automated surrogate for human preference evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The False Promise of Imitating Proprietary LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended instruction-following / dialogue and general QA (same evaluation prompts as human MTurk experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Automated: GPT-4 is prompted with two outputs (ChatGPT vs imitation model) and asked to produce a preference ranking using the same set of evaluation prompts as the human study; procedure follows Chiang et al. (2023) style prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>GPT-4 preference ranking / relative quality judgments across examples; trends compared to human crowdworker preference rates.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>GPT-4 produced trends that matched crowdworker evaluations (flat preference as imitation data scaled, increasing preference with larger base model size), but showed a slightly larger absolute preference for ChatGPT compared to crowdworkers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>GPT-4 may replicate human-like cognitive biases and be similarly susceptible to stylistic deception; it can be misled by confident, well-structured but factually incorrect outputs, reducing sensitivity to factuality errors.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>GPT-4 agreed with crowdworkers that stylistically similar imitation outputs were high-quality despite factual errors; it did not reliably detect factuality regressions that were exposed by targeted automatic benchmarks (e.g., Natural Questions accuracy declines).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use caution when replacing human evaluation with LLM judges; combine LLM judges with benchmark-based factuality checks, craft prompts for judges that emphasize factual verification, and validate LLM-judge judgments against expert human judgments on critical subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The False Promise of Imitating Proprietary LLMs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6217.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6217.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human vs LLM Judge Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison between Human Crowdworkers and GPT-4 as Evaluation Judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct comparison in this paper of trends and behaviors of crowdworker judgments and GPT-4 judgments on the same pairwise evaluation tasks, highlighting alignment in trends but shared vulnerabilities to stylistic artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The False Promise of Imitating Proprietary LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended instruction-following / dialogue and general QA; cross-domain evaluation (generation, QA, coding, summarization) used to probe robustness of judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (compared against human crowdworkers)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human: Mechanical Turk blind pairwise comparisons (see human entry). LLM: GPT-4 blind pairwise comparisons on the same prompts; comparison of aggregate preference trends and absolute preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Aggregate pairwise preference distributions; trend behavior as imitation data scale and base model size change; qualitative agreement on stylistic preferences versus factual benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Both judge types showed the same qualitative trends: (1) increasing imitation data did not significantly raise judged quality, (2) increasing base model size increased judged quality. GPT-4 had a slightly larger absolute preference for ChatGPT than crowdworkers. Crucially, both judge types favored stylistic similarity and were insufficiently sensitive to factuality deficits revealed by automatic benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>LLM-as-judge (GPT-4) shares human biases: emphasis on fluency/authoritative tone and low-level structure (lists, summaries) over factual correctness; risk of reproducing human-like evaluation errors and blind spots; potential calibration/overconfidence issues when assessing factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Both judges failed to penalize imitation-model outputs that sound authoritative but are factually incorrect (Figure 2); judged parity or superiority occurred even when automatic benchmarks showed no improvement or regression (e.g., decreased Natural Questions accuracy after broad imitation fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Combine subjective pairwise evaluations with targeted automatic metrics that assess factuality (e.g., NQ exact match), enlist subject-matter experts for difficult prompts, design adversarial/difficult test sets, ask LLM judges to perform structured factual verification (e.g., cite sources, fact-check claims), and consider hybrid evaluation pipelines (LLM judge + automatic factuality checks + expert review).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The False Promise of Imitating Proprietary LLMs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Vicuna: An open-source chatbot impressing GPT-4 with 90% chatgpt quality <em>(Rating: 2)</em></li>
                <li>How close is ChatGPT to human experts? Comparison corpus, evaluation, and detection <em>(Rating: 2)</em></li>
                <li>Stanford Alpaca: An instruction-following LLaMA model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6217",
    "paper_id": "paper-5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "Human Crowdworker Eval",
            "name_full": "Mechanical Turk Crowdworker Pairwise Evaluation",
            "brief_description": "Blind pairwise comparisons by crowdworkers between ChatGPT and imitation model outputs across held-out prompts from ShareGPT; used to measure subjective overall preference and perceived quality.",
            "citation_title": "The False Promise of Imitating Proprietary LLMs",
            "mention_or_use": "use",
            "task_domain": "Open-ended instruction-following / dialogue and general QA (ShareGPT prompts spanning generation, QA, coding, summarization, etc.)",
            "llm_judge_model": "human (Mechanical Turk crowdworkers)",
            "human_evaluation_setup": "Blind pairwise comparisons on Amazon Mechanical Turk: ~70–71 crowdworkers, 255 held-out prompts, 3 independent ratings per example (majority vote), workers selected with ≥95% approval, located in English-speaking countries, ≥100 HITs; paid ≈$15/hour.",
            "metrics_compared": "Pairwise preference (which output is better or equal) averaged across examples; average preference percentages and standard deviation reported.",
            "reported_differences": "Crowdworkers often rated imitation models' outputs as equal or better than ChatGPT (≈70% equal/better for early models), and ratings saturated as imitation data increased; however these subjective preferences did not align with automatic benchmark performance (e.g., factuality and QA accuracy).",
            "llm_specific_limitations": "Not applicable (this entry is human evaluators), but paper reports human limitations: non-expert crowdworkers are easily deceived by style (confident, well-structured answers) and may miss factual errors without domain expertise or time to verify.",
            "notable_failure_cases": "Crowdworkers gave high ratings to imitation-model outputs that were stylistically similar but factually incorrect (Figure 2); human raters failed to detect factual regressions introduced by imitation fine-tuning on broad conversational data (e.g., declines on Natural Questions despite positive human ratings).",
            "mitigation_strategies": "Engage domain experts for evaluations, curate highly difficult and targeted prompts that probe factuality, require longer/verifiable checks for factual claims, and complement human subjective ratings with targeted automatic benchmarks.",
            "uuid": "e6217.0",
            "source_info": {
                "paper_title": "The False Promise of Imitating Proprietary LLMs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 as Judge",
            "name_full": "GPT-4 Blind Pairwise Preference Evaluator",
            "brief_description": "Using GPT-4 to perform blind pairwise comparisons between ChatGPT and imitation model outputs as an automated surrogate for human preference evaluation.",
            "citation_title": "The False Promise of Imitating Proprietary LLMs",
            "mention_or_use": "use",
            "task_domain": "Open-ended instruction-following / dialogue and general QA (same evaluation prompts as human MTurk experiments)",
            "llm_judge_model": "GPT-4",
            "human_evaluation_setup": "Automated: GPT-4 is prompted with two outputs (ChatGPT vs imitation model) and asked to produce a preference ranking using the same set of evaluation prompts as the human study; procedure follows Chiang et al. (2023) style prompt templates.",
            "metrics_compared": "GPT-4 preference ranking / relative quality judgments across examples; trends compared to human crowdworker preference rates.",
            "reported_differences": "GPT-4 produced trends that matched crowdworker evaluations (flat preference as imitation data scaled, increasing preference with larger base model size), but showed a slightly larger absolute preference for ChatGPT compared to crowdworkers.",
            "llm_specific_limitations": "GPT-4 may replicate human-like cognitive biases and be similarly susceptible to stylistic deception; it can be misled by confident, well-structured but factually incorrect outputs, reducing sensitivity to factuality errors.",
            "notable_failure_cases": "GPT-4 agreed with crowdworkers that stylistically similar imitation outputs were high-quality despite factual errors; it did not reliably detect factuality regressions that were exposed by targeted automatic benchmarks (e.g., Natural Questions accuracy declines).",
            "mitigation_strategies": "Use caution when replacing human evaluation with LLM judges; combine LLM judges with benchmark-based factuality checks, craft prompts for judges that emphasize factual verification, and validate LLM-judge judgments against expert human judgments on critical subsets.",
            "uuid": "e6217.1",
            "source_info": {
                "paper_title": "The False Promise of Imitating Proprietary LLMs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Human vs LLM Judge Comparison",
            "name_full": "Comparison between Human Crowdworkers and GPT-4 as Evaluation Judges",
            "brief_description": "Direct comparison in this paper of trends and behaviors of crowdworker judgments and GPT-4 judgments on the same pairwise evaluation tasks, highlighting alignment in trends but shared vulnerabilities to stylistic artifacts.",
            "citation_title": "The False Promise of Imitating Proprietary LLMs",
            "mention_or_use": "use",
            "task_domain": "Open-ended instruction-following / dialogue and general QA; cross-domain evaluation (generation, QA, coding, summarization) used to probe robustness of judgments.",
            "llm_judge_model": "GPT-4 (compared against human crowdworkers)",
            "human_evaluation_setup": "Human: Mechanical Turk blind pairwise comparisons (see human entry). LLM: GPT-4 blind pairwise comparisons on the same prompts; comparison of aggregate preference trends and absolute preferences.",
            "metrics_compared": "Aggregate pairwise preference distributions; trend behavior as imitation data scale and base model size change; qualitative agreement on stylistic preferences versus factual benchmarks.",
            "reported_differences": "Both judge types showed the same qualitative trends: (1) increasing imitation data did not significantly raise judged quality, (2) increasing base model size increased judged quality. GPT-4 had a slightly larger absolute preference for ChatGPT than crowdworkers. Crucially, both judge types favored stylistic similarity and were insufficiently sensitive to factuality deficits revealed by automatic benchmarks.",
            "llm_specific_limitations": "LLM-as-judge (GPT-4) shares human biases: emphasis on fluency/authoritative tone and low-level structure (lists, summaries) over factual correctness; risk of reproducing human-like evaluation errors and blind spots; potential calibration/overconfidence issues when assessing factuality.",
            "notable_failure_cases": "Both judges failed to penalize imitation-model outputs that sound authoritative but are factually incorrect (Figure 2); judged parity or superiority occurred even when automatic benchmarks showed no improvement or regression (e.g., decreased Natural Questions accuracy after broad imitation fine-tuning).",
            "mitigation_strategies": "Combine subjective pairwise evaluations with targeted automatic metrics that assess factuality (e.g., NQ exact match), enlist subject-matter experts for difficult prompts, design adversarial/difficult test sets, ask LLM judges to perform structured factual verification (e.g., cite sources, fact-check claims), and consider hybrid evaluation pipelines (LLM judge + automatic factuality checks + expert review).",
            "uuid": "e6217.2",
            "source_info": {
                "paper_title": "The False Promise of Imitating Proprietary LLMs",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Vicuna: An open-source chatbot impressing GPT-4 with 90% chatgpt quality",
            "rating": 2
        },
        {
            "paper_title": "How close is ChatGPT to human experts? Comparison corpus, evaluation, and detection",
            "rating": 2
        },
        {
            "paper_title": "Stanford Alpaca: An instruction-following LLaMA model",
            "rating": 1
        }
    ],
    "cost": 0.0095585,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The False Promise of Imitating Proprietary LLMs</h1>
<p>Arnav Gudibande<em><br>UC Berkeley<br>arnavg@berkeley.edu Eric Wallace</em><br>UC Berkeley<br>ericwallace@berkeley.edu<br>Charlie Snell*<br>UC Berkeley<br>csnell22@berkeley.edu<br>Xinyang Geng<br>UC Berkeley<br>young.geng@berkeley.edu<br>Hao Liu<br>UC Berkeley<br>hao.liu@berkeley.edu<br>Pieter Abbeel<br>UC Berkeley<br>pabbeel@berkeley.edu<br>Sergey Levine Dawn Song<br>UC Berkeley UC Berkeley<br>svlevine@berkeley.edu dawnsong@berkeley.edu</p>
<h2>Abstract</h2>
<p>An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes ( $1.5 \mathrm{~B}-13 \mathrm{~B}$ ), data sources, and imitation data amounts ( $0.3 \mathrm{M}-150 \mathrm{M}$ tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models-they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.</p>
<h2>1 Introduction</h2>
<p>The recent release of powerful language models (LMs) such as ChatGPT (OpenAI, 2022), Bard (Pichai, 2023), and Claude (AnthropicAI, 2023) might herald a future where the best AI systems are provided primarily as a fee-based API by large companies. At the same time, open-source LMs are becoming increasingly accurate, with models like LLaMA and FLAN-T5 providing many of the same basic capabilities as their commercial counterparts, albeit at a lower level of performance (Touvron et al., 2023; Chung et al., 2022). This presents an important question, whose answer will have profound future implications: will the most powerful LMs be closed-source or will they be freely distributed for anyone to use, modify, and extend? Both possibilities have important pros and cons, and implications on policy, corporate strategy, and the future of scientific inquiry.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Crowdworkers initially rate the quality of our imitation models highly, as ~70% of their outputs are rated as equal or better than those of ChatGPT (<em>left</em>). However, as we train on more imitation data, our models fail to further close the gap, and even begin to regress along other axes, e.g., factual knowledge according to Natural Questions (<em>center</em>). Our main conclusion is that the biggest limitation of current open-source LMs is their weaker base capabilities. In turn, the best way for the open-source community to improve models is by increasing these capabilities (e.g., via scaling, better pretraining data, etc.,) rather than fine-tuning on more and more imitation data (<em>right</em>).</p>
<p>In this work, we study one possible resolution to this question: <em>model imitation</em> (Wallace et al., 2020; Orekondy et al., 2019). The premise of model imitation is that once a proprietary LM is made available via API, one can collect a dataset of API outputs and use it to fine-tune an open-source LM. In theory, this imitation process may provide an easy method to distill (Hinton et al., 2014) the capabilities of any proprietary model, thus implying that open-source LMs will always be competitive with their commercial counterparts. To date, recent works have looked to imitate OpenAI's best systems, e.g., Self-Instruct (Wang et al., 2022a) and Alpaca (Taori et al., 2023), and initial results suggest that these models have achieved near parity with proprietary models. Consequently, there has been a growing sentiment among many members of the broader tech community that closed-source models will soon have no advantage (Patel and Ahmad, 2023).</p>
<p>The goal of our work is to critically analyze the efficacy of model imitation by training and evaluating copycats of ChatGPT. We first collect datasets that focus on either imitating ChatGPT for a specific task or broadly imitating it across all behaviors. We then fine-tune LMs on these datasets using a range of model sizes (1.5B–13B), base models (GPT-2 and LLaMA), and data amounts (0.3M–150M tokens). We evaluate using human and GPT-4 evaluations (blind pairwise comparisons with ChatGPT) as well as accuracy on canonical NLP benchmarks (MMLU, NQ, HumanEval).</p>
<p>We were initially surprised by how much imitation models improve over their base models: they are far better at following instructions, and their outputs appear similar to ChatGPT's. This was further supported by both human and GPT-4 evaluations, where the outputs of our best imitation model were rated as competitive with ChatGPT (e.g., Figure 1, left).</p>
<p>However, when conducting more targeted automatic evaluations, we found that the imitation models close little to none of the large gap between LLaMA and ChatGPT. In particular, we demonstrate that imitation models improve on evaluation tasks that are heavily supported in the imitation training data. On the other hand, the models do not improve (or even decline in accuracy) on evaluation datasets for which there is little support. For example, training on 100k ChatGPT outputs from broad-coverage user inputs provides no benefits to Natural Questions accuracy (e.g., Figure 1, center), but training exclusively on ChatGPT responses for Natural-Questions-like queries drastically improves task accuracy. Consequently, we conclude that broadly matching ChatGPT using purely imitation would require (1) a concerted effort to collect enormous imitation datasets and (2) far more diverse and higher quality imitation data than is currently available.</p>
<p>These findings underscore an inconsistency between LM performance on crowdworker evaluations and NLP benchmarks. We find that imitation models get rated positively by crowdworkers because they are adept at mimicking ChatGPT's <em>style</em>—they output confident and well-structured answers. However, their <em>factuality is weak</em>, and crowdworkers without domain expertise or significant time investments may miss these errors.</p>
<p>Overall, our key takeaway is that model imitation is not a free lunch: there exists a capabilities gap between today's open-source LMs and their closed-source counterparts that cannot be closed by</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: ChatGPT and our best imitation model produce answers with similar style-they start with an overview paragraph, a list of differences, and end with a summary. However, while ChatGPT's answer is mostly correct, the imitation model's answer is completely inaccurate despite sounding authoritative. We show correct sentences in green, ambiguously-correct sentences in yellow, and incorrect ones in red.
cheaply fine-tuning on imitation data. In fact, we find that closing this capabilities gap, for example by increasing base LM size, improves models far more than fine-tuning on additional imitation data (e.g., Figure 1, right). This implies that the higher leverage action for improving open-source LMs is to tackle the difficult challenge of developing better base models (e.g. by scaling up models, improving pre-training data quality, improving pre-training, etc.), rather than taking the shortcut of imitating proprietary systems. Nevertheless, we believe that model imitation has utility in subverting the need to annotate high-quality finetuning data if one has a sufficiently strong base LM.</p>
<h1>2 What is Model Imitation?</h1>
<p>Proprietary LMs such as ChatGPT consist of two key aspects: proprietary base LMs and proprietary fine-tuning data. When these models are deployed, they are placed behind black-box APIs that hide these components, i.e., users can query the API with arbitrary inputs but cannot see the model's training data, next-token probabilities, and architecture. In model imitation, the goal is to collect data using the API to train an LM that achieves comparable performance to it, i.e., essentially distilling the target LM using an imitation training set (Wallace et al., 2020; Orekondy et al., 2019; Tramèr et al., 2016). Potential reasons for performing imitation range from benign to illegal:</p>
<ul>
<li>Academics can use powerful imitation LMs to drive new research projects.</li>
<li>Companies can use imitation LMs to launch services that compete with the proprietary system.</li>
<li>Malicious users could use imitation models to accelerate progress on nefarious use cases.</li>
</ul>
<p>Local versus Broad Imitation When performing model imitation, one will either look to perform local "task-specific" imitation or more global "broad-coverage" imitation. The former imitates the target model on just a specific task or domain, e.g., sentiment analysis of tweets or question answering over Wikipedia entities. The latter focuses on the more ambitious goal of broadly imitating the</p>
<p>target model across its full spectrum of behaviors, domains, and tasks. Broad-coverage imitation is challenging because (1) one must collect an extremely diverse imitation dataset and (2) imitation models must capture this wide data distribution and generalize similarly to the target model on a myriad of held-out examples.</p>
<p>Recent Work on Model Imitation A surge of recent publications have attempted to both locally imitate proprietary models for specific tasks (Sun et al., 2023; Hsieh et al., 2023; Honovich et al., 2022) and broadly imitate models, e.g., Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), Koala (Geng et al., 2023), GPT4ALL (Anand et al., 2023), and more (Wang et al., 2022a; Peng et al., 2023). Many these works conclude that their imitation models achieve near parity with the target model, e.g., Vicuna claims to achieve $90 \%$ of the quality of ChatGPT and Google Bard. These claims have since been propagated out into the broader tech community, leading many to believe that open-source LMs are rapidly closing the gap to their closed-source counterparts and that top AI companies will soon have no competitive advantage (Patel and Ahmad, 2023).</p>
<p>Our goal. The goal of our paper is to critically evaluate this line of reasoning. In particular, we train models to imitate ChatGPT while experimenting with different decisions (e.g., data collection strategies, data amounts, and base LMs) and conducting rigorous automatic and human evaluations.</p>
<h1>3 Building Imitation Datasets</h1>
<p>We consider both task-specific and broad-coverage imitation. For either form of model imitation, one must curate a set of inputs to query to the target model. In practice, one may have a set of inputs in mind (e.g., sentences from Wikipedia, tweets about Coca-Cola) and if this set of input examples is sufficiently large, one can use them to query the target model and build an imitation dataset. In cases when it is impractical or labor intensive to create a large and diverse pool of inputs, one can also create synthetic examples by prompting LMs to iteratively generate examples that are from the same distribution as an initial smaller seed set of inputs (Wang et al., 2022a; Honovich et al., 2022).</p>
<p>Task-specific imitation For task-specific imitation, we created an imitation dataset tailored to Natural Questions (Kwiatkowski et al., 2019a), i.e., factual knowledge about Wikipedia entities. In particular, we first curated a seed set of ten QA pairs from the validation dataset. We then iteratively generated 6,000 additional examples by prompting ChatGPT with five random QA pairs and asking it to generate similar but distinct examples. All of these examples are single turn, without any dialogue history. We refer to this dataset as NQ-synthetic and provide further details in Appendix A.</p>
<p>Broad-coverage imitation For the more ambitious goal of broad-coverage imitation data, we leverage the fact that models such as ChatGPT have become so popular that their inputs and outputs are already widely posted on the web. Thus, we can collect a large, diverse, and generally high-quality dataset of examples for free without ever having to interact with the company's API. In particular, we collect examples from three sources:</p>
<ul>
<li>ShareGPT: we use approximately 90 K dialogues shared by users on the website ShareGPT. To maintain data quality, we deduplicated on the query level and removed any non-English conversations using a language detector. This leaves approximately 50K examples, each of which consist of multiple turns of dialogue.</li>
<li>HC3 (Guo et al., 2023): we use the ChatGPT responses from the English Human-ChatGPT Comparison Corpus. This contains $\sim 27 \mathrm{~K}$ ChatGPT responses for $\sim 24 \mathrm{~K}$ questions.</li>
<li>Discord ChatGPT Bots: we use 10k input-output examples collected from the r/ChatGPT and Turing AI Discord servers, two public channels that allow users to interact with ChatGPT bots.</li>
</ul>
<p>We refer to this dataset as ShareGPT-Mix and show qualitative examples in Appendix A. We find that ShareGPT-Mix is generally of high quality. First, there is high diversity in the instructions: for each user query in the dataset, the most similar other user query has an average BLEU score similarity of just $8 \%$. This is considerably lower than that of other datasets such as SuperNaturalInstructions (Wang et al., 2022b), which is at $61 \%$ BLEU similarity for a similarly sized set of examples. We also manually reviewed different examples and logged their semantic category (see Table 5 in Appendix A). The dataset contains diverse categories, including many multi-lingual conversations and coding tasks.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: We find that GPT-4 and crowdworker evaluations show the same trends. As we scale up the amount of imitation data, GPT-4's ratings of our imitation models are relatively flat (left). However, as we scale up the base model size, GPT-4's rates the quality of our imitation models increasingly highly (right).</p>
<h1>4 Main Results</h1>
<p>We train imitation LMs using our ShareGPT-Mix and NQ-synthetic datasets, and we conduct both human and automatic evaluations. We focus our initial results on the ShareGPT-Mix models.</p>
<h3>4.1 Training and Evaluation Setup</h3>
<p>We study how model imitation improves as we increase the amount of imitation data and vary the capabilities of the underlying base LM. We consider decoder-only models ranging in size from 1.5B to 13B parameters: GPT-2 1.5B (Radford et al., 2019), LLaMA 7B (Touvron et al., 2023), and LLaMA 13B. ${ }^{2}$ We also study the effect by data scale by fine-tuning with different sized data subsets.
During training, we chunk the conversations into 2048 tokens blocks. We introduce special tokens that demarcate the beginning of each user query and model output. We fine-tune using standard LM losses on only the model outputs. Following Chung et al. (2022); Chowdhery et al. (2022), we train for one epoch using the AdamW optimizer with gradients re-scaled by the magnitude of each weight. We use a learning rate of $2 \mathrm{e}-3$ with 1000 steps of linear warm-up from 0 , and we train with batch size 32. All models are trained in JAX using a combination of fully shared data parallelism and tensor parallelism on TPUs hosted by Google Cloud or on a single Nvidia DGX server with 8 A100 GPUs.
For automatic evaluations, we measure performance on 5-shot MMLU (Hendrycks et al., 2021), 3-shot Natural Questions (Kwiatkowski et al., 2019b), and 0-shot HumanEval (Chen et al., 2021). We report the original scoring metrics associated with each dataset (e.g., exact match for NQ). For human evaluation, we conduct blind pairwise output comparisons using Mechanical Turk. In our UI, we present each rater with a task instruction and the output of two unknown models, one of which is ChatGPT and the other is one of our imitation models (see Figure 7 in Appendix B). The raters select which output they prefer or if the two outputs are equal in quality. We use approximately 70 crowd workers and evaluate on 255 held-out prompts. ${ }^{3}$ We report the average preference across the dataset and one standard deviation around the mean. Additionally, we conduct evaluations using GPT-4 and present additional details of the prompts used in Appendix C.
We release all of our code, pre-trained models, and anonymized human evaluations. ${ }^{4}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Increasing Amount of Imitation Data
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Automatic evaluations. As we increase the amount of imitation data, there is little improvement on various benchmarks, or even performance regressions (top). On the other hand, scaling up the base LM steadily improves results (bottom), suggesting that the key difference between open-source and closed-source LMs is a raw capabilities gap, rather than the finetuning data used.</p>
<h1>4.2 Qualitative Analysis and Crowdworker Evaluation Show Promise</h1>
<p>Imitation models are rated highly by crowdworkers. We were initially surprised at the quality of our ShareGPT-mix models: while the base GPT-2 or LLaMA models often fail to follow instructions, the imitation models produce outputs that stay on task. These initial promises were further supported, as crowdworkers and GPT-4 often rated the quality of the imitation models' outputs as equal or better than those of ChatGPT, especially as we scale up model size (right of Figure 1 and 3). However, we also find that human ratings quickly saturate as we scale up the amount of imitation data (left of Figure 1 and 3), alluding to possible shortcomings of this approach.</p>
<h3>4.3 Targeted Automatic Evaluations Expose Failure Modes</h3>
<p>Broad-coverage imitation models fail to close the gap across most tasks. We next ran targeted automatic evaluations to isolate whether specific model capabilities improved after imitation. We found that across every benchmark that we measured, ShareGPT-mix imitation models do not improve (or even decline) in accuracy as compared to the base model, even when adding additional imitation data (Figure 4, top). This shows that imitating ChatGPT on our broad-coverage imitation data does not improve the model across most axes, e.g., factual knowledge, coding, and problem solving.
We argue that this occurs because ChatGPT has captured far more knowledge and capabilities from the web as compared to LLaMA. In turn, it is unreasonable to expect that a small amount of imitation data (e.g., 1000x less data than pre-training) would enable one to bridge this gap. Instead, we argue that broadly matching ChatGPT using weaker base LMs such as LLaMA-13B would require a concerted effort to collect an extremely large and diverse imitation dataset that is far closer to the scale of pretraining. It is currently unclear whether such an effort is worth undertaking or feasible.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Training local imitation models is far more successful. On the other hand, our model trained to locally imitate ChatGPT using the NQ-synthetic data is far more successful. In particular, the imitation models' performance improves significantly as compared to the LLaMA base model (see Table 1) and quickly approaches the accuracy of ChatGPT. This demonstrates that it is far more feasible to distill a specific behavior from ChatGPT as opposed to broadly matching its capabilities.</p>
<p>A empirical trade-off exists between different evaluation datasets. A curious phenomena is that training on more ShareGPT-Mix data hurts performance as compared to the base model on some of our evaluations (compare the black versus blue lines in Figure 4). We believe that these performance regressions arise from a distribution shift and tension between the conversational-style fine-tuning data and the downstream benchmarks. An open problem is whether these performance regressions can be mitigated using regularization or by mixing in pre-training data during fine-tuning.</p>
<p>Improving base LMs is the highest leverage action. Rather than increasing imitation data size, we find that using better base LMs (by increasing base model size) does lead to substantial accuracy improvements (Figure 4, bottom). This aligns with our previous claim: there exists a capabilities gap between today's open-source LMs and their closed-source counterparts that cannot be closed by cheaply fine-tuning on imitation data. Instead, the best way to improve open-source LMs is to tackle the difficult challenge of developing better base LMs, whether it be via model scaling or other means.</p>
<h1>4.4 Imitation Models Learn Style, Not Content</h1>
<p>Finally, we investigate why there is a strong discrepancy between crowdworker evaluations, where imitation models appear quite strong, and results on NLP benchmarks, where imitation models appear no better than base LMs. We find that imitation models perform well according to human evaluations because they are adept at mimicking ChatGPT's style-they output fluent, confident, and well-structured answers. In particular, we show in Table 2 that as we add more imitation data, ChatGPT and our imitation models produce outputs with a similar length, similar word choice, similar use of an authoritative tone, and similar low-level structure (e.g., use of lists).
However, as shown in our previous automatic evaluations, the imitation models have weak factuality. In other words, imitation models actually embody some of the worst aspects of AI assistants: their answers sound confident but are less factual than ChatGPT. This is perhaps best elucidated in Figure 2, where the imitation model outputs an answer that is similar in style to ChatGPT's answer but is completely incorrect.</p>
<p>Human evaluation is increasingly hard. Unfortunately, crowd workers without domain expertise or significant time investments can easily be deceived by stylistic components-answers that sound confident and correct are often spuriously chosen more often. To improve human evaluation, it is thus increasingly necessary to both engage domain experts, but also to curate a set of highly difficult prompts that can rigorously test different models' capabilities. Surprisingly, our GPT-4 evaluations also showed the same trends as our crowdworker evaluations (albet with a slightly larger absolute preference for ChatGPT's outputs). While this suggests that GPT-4 may be a viable candidate to cheaply emulate human evaluations on some tasks, it also implies that LLMs may replicate some human-like cognitive biases. We look forward to future work that further investigates this possibility.</p>
<p>Imitation models inherit the safety and toxicity style of the teacher model. Finally, despite imitation only providing benefits in mimicking the "style" or "persona" of the target model, there is still value in doing so. For example, OpenAI has carefully and deliberately trained ChatGPT to be "harmless" to end users, often avoiding toxic outputs and refusing to respond to questionable user requests. We find that our imitation models also inherit these components. In particular, we show in Figure 5 that as we finetune on more imitation data, the imitation model's outputs become less toxic on RealToxicityPrompts (Gehman et al., 2020), as the model learns to abstain in a similar fashion to ChatGPT. Consequently, we conclude that model imitation is highly effective in cases when one has a powerful base LM and is looking to subvert the need to annotate expensive finetuning data.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Imitation Data</th>
<th style="text-align: left;">NQ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">17</td>
</tr>
<tr>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">ShareGPT-Mix</td>
<td style="text-align: left;">10</td>
</tr>
<tr>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">NQ-Synthetic</td>
<td style="text-align: left;">$\mathbf{2 2}$</td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">20</td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: left;">ShareGPT-Mix</td>
<td style="text-align: left;">15</td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: left;">NQ-Synthetic</td>
<td style="text-align: left;">$\mathbf{2 7}$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">31</td>
</tr>
</tbody>
</table>
<p>Table 1: We train imitation models on broadcoverage data from ShareGPT-Mix or targeted Natural-Questions-like data (NQ-synthetic). The broad-coverage models do not improve on zeroshot NQ (or even degrade in performance), demonstrating the ineffectiveness of imitating the capabilities of ChatGPT holistically. However, the NQ-Synthetic models substantially close the gap to ChatGPT on NQ, showing that local imitation of a model is far more feasible in practice.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: We evaluate imitation models on RealToxicityPrompts and report the average nontoxicity score according to the perspective API. The results show that imitation models are significantly less toxic than the baseline models, i.e., they learn to inherit the safety and toxicity guidelines of the target models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Imitation Models</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Metric</td>
<td style="text-align: right;">LLaMA</td>
<td style="text-align: right;">$\mathbf{2 0 M}$</td>
<td style="text-align: right;">$\mathbf{8 0 M}$</td>
<td style="text-align: right;">$\mathbf{1 5 0 M}$</td>
<td style="text-align: right;">ChatGPT #2</td>
</tr>
<tr>
<td style="text-align: left;">If ChatGPT outputs a list, do we?</td>
<td style="text-align: right;">$13 \%$</td>
<td style="text-align: right;">$50 \%$</td>
<td style="text-align: right;">$67 \%$</td>
<td style="text-align: right;">$81 \%$</td>
<td style="text-align: right;">$83 \%$</td>
</tr>
<tr>
<td style="text-align: left;">If ChatGPT outputs a summary paragraph, do we?</td>
<td style="text-align: right;">$2 \%$</td>
<td style="text-align: right;">$40 \%$</td>
<td style="text-align: right;">$42 \%$</td>
<td style="text-align: right;">$48 \%$</td>
<td style="text-align: right;">$55 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Unigram intersection w/ ChatGPT's output</td>
<td style="text-align: right;">19.5</td>
<td style="text-align: right;">40.4</td>
<td style="text-align: right;">41.9</td>
<td style="text-align: right;">42.5</td>
<td style="text-align: right;">49.2</td>
</tr>
<tr>
<td style="text-align: left;">Pearson correlation in length w/ ChatGPT's output</td>
<td style="text-align: right;">-0.11</td>
<td style="text-align: right;">0.51</td>
<td style="text-align: right;">0.62</td>
<td style="text-align: right;">0.62</td>
<td style="text-align: right;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">Outputs are in authoritative tone according to GPT-4</td>
<td style="text-align: right;">$57 \%$</td>
<td style="text-align: right;">$99 \%$</td>
<td style="text-align: right;">$98 \%$</td>
<td style="text-align: right;">$98 \%$</td>
<td style="text-align: right;">$98 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: As we add more imitation data, the style of our models' outputs are increasingly similar to those of ChatGPT. In particular, we generate outputs from our imitation models and compare them to a random ChatGPT response across different metrics. We also report a rough "upper bound" by comparing a second random ChatGPT output to the original ChatGPT response (ChatGPT #2).</p>
<h1>5 Discussion</h1>
<p>Finetuning as a simple knowledge extractor. Our results show that a modest amount of finetuning provides little to no improvements on an LM's knowledge or capabilities. We thus agree with the view that pre-training is the main source of an LM's capabilities, and that finetuning acts as a lightweight method to train the model to extract its own knowledge Schulman (2023). This is the reason why improving models by imitating ChatGPT on a small set of data is insufficient, as the base knowledge is largely unaffected. Furthermore, this view suggests that during finetuning time, you may even want to avoid introducing new knowledge (i.e., do not imitate better models), as you will otherwise be training the model to guess or hallucinate its answers, rather than actually doing the task as intended (Schulman, 2023; Gao, 2021; Goldberg, 2023).</p>
<p>Should you be worried about imitation? Imitating proprietary LMs comes with many potential implications for small and large companies alike. Our results suggest that the efficacy of model imitation is limited when there is a large gap between the base and target LM. Thus, we believe that companies who can establish a capabilities gap using large amounts of data, compute, or algorithmic advances are the ones who are best positioned to build and maintain competitive advantages. On the other hand, companies that look to build moats by using off-the-shelf LMs with proprietary fine-tuning datasets may be comparatively more vulnerable to imitation.</p>
<p>Potential confounders to our findings. While we believe our findings are well supported, there are a few potential hidden confounders that could change our conclusions. First, as we are unaware of the pre-training data used by ChatGPT, it is possible that some of the tasks that we evaluate on</p>
<p>could have been been contaminated into ChatGPT's training data, thus inflating its accuracy numbers. Moreover, to conduct imitation, we perform supervised learning on the outputs from the target model. However, it also may be possible to use the target model to perform RLHF or constitutional AI (OpenAI, 2022; Christiano et al., 2017; Bai et al., 2022) to further improve results. Lastly, we only considered relatively simple methods for collecting imitation data, however, there may be more advanced methods (e.g., active learning) that may improve the effectiveness or efficiency of model imitation.</p>
<p>Implications for other forms of model imitation There has been a flurry of recent work that performs model imitation in more indirect ways than we study here. For example, the training process of many recent vision-language model (Li et al., 2022; Liu et al., 2023; Ye et al., 2023; Zhu et al., 2023) includes ChatGPT or GPT-4 outputs at some stages. Furthermore, it has become common to use large LMs in various ways during the data annotation and creation process, e.g., to aid crowd workers, to perform data augmentation, to identify mislabeled data, and more. Our findings may have implications for these approaches, e.g., it is likely that vision-language models that include OpenAI data may have similar failure modes to the ones described in our work.</p>
<p>Technical limitations of model imitation Imitating proprietary models also has various technical limitations: the models inherit the weaknesses and biases of proprietary models, imitation does not allow one to directly improve on the design decisions of closed AI companies (e.g., data annotation strategies), and these systems are roughly upper-bounded by the capabilities of the target proprietary model. Moreover, it is difficult to answer certain scientific questions using imitation models because they include proprietary black-box models in their training pipeline.</p>
<h1>6 Related Work</h1>
<p>Model distillation Model imitation is similar to model distillation (Hinton et al., 2014), where one trains a student model to imitate a teacher. While conceptually similar, there are several major practical differences. For distillation, the training data, model architecture, and hyperparameters are known for the teacher. In model imitation, one tries to imitate the teacher without this knowledge. Moreover, for distillation it is common to use training objectives that utilize the probability distribution of the teacher whereas in stealing such a distribution is typically unavailable.</p>
<p>Past work on model imitation Prior work has shown that model imitation is possible for various domains (Orekondy et al., 2019; Tramèr et al., 2016; Lowd and Meek, 2005), including language classifiers (Krishna et al., 2020; Pal et al., 2019) and machine translation systems (Wallace et al., 2020). Nevertheless, past work considers a setting where models are trained from scratch, and thus the main proprietary nature of a model is the company's internal training data. In our setting, systems like ChatGPT are proprietary because they also leverage OpenAI's internal pre-trained LMs that are stronger than any available open-source LM.</p>
<p>Defending against model imitation Our results show that imitation is a moderate concern for companies. In turn, there is a need to develop methods to mitigate or detect imitation. There is an existing body of work in this direction, e.g., one can detect whether a particular model is trained via imitation (Krishna et al., 2020; Juuti et al., 2019; Szyller et al., 2019; Maini et al., 2021) or slow model stealing by sacrifing some performance (Wallace et al., 2020; Orekondy et al., 2020; Dziedzic et al., 2022a,b). Unfortunately, existing methods often exhibit too severe of a tradeoff to be deployable in practice.</p>
<h2>7 Conclusion and Future Work</h2>
<p>In this work, we critically analyzed the efficacy of model imitation. We showed that imitation can indeed improve the style, persona, and instruction adherence of open-source LMs. However, imitation falls short in improving LMs across more challenging axes such as factuality, coding, and problem solving. On one hand, these results indicate that businesses can successfully establish and safeguard a competitive advantage by pre-training powerful base models. Conversely, it also implies that if two groups possess equally competent base LMs, one can easily mimic the persona and behavior of the other model, without needing to annotate expensive fine-tuning data.</p>
<p>Moving forward, our findings raise a range of technical and societal questions. First, we show that existing crowd worker evaluations have trouble elucidating the differences between imitation models and proprietary ones, despite clear differences existing between them. In turn, the future of human evaluation remains unclear: how can we cheaply and quickly probe the utility of a powerful LLM?
Second, given the large gap between LLaMA and ChatGPT (the latter model is faster, cheaper, and more accurate), and the insufficiencies of model imitation, there are obvious open questions on how to best improve open-source LMs (e.g., increasing model scale, improving pre-training data quality, developing new pretraining methods, etc). Finally, our work raises ethical and legal questions, including whether the open-source community should continue to advance progress by "stealing" what OpenAI and other companies have done, as well as what legal countermeasures companies can take to protect and license intellectual property. In future work, we hope to delve deeper into these issues and devise better methods for the ethical and responsible deployment of LMs.</p>
<h1>Acknowledgements</h1>
<p>We thank Nicholas Carlini, the members of Berkeley NLP, and the members of Berkeley RAIL for valuable feedback on this project. Eric Wallace is supported by the Apple Scholar in AI/ML Fellowship. Part of this research was supported with Cloud TPUs from Google's TPU Research Cloud (TRC).</p>
<h2>References</h2>
<p>OpenAI. ChatGPT: Optimizing language models for dialogue., 2022.
Sundar Pichai. An important next step on our AI journey. Google AI Blog, 2023.
AnthropicAI. Introducing claude, 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMa: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Eric Wallace, Mitchell Stern, and Dawn Song. Imitation attacks and defenses for black-box machine translation systems. In EMNLP, 2020.</p>
<p>Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-box models. In CVPR, 2019.</p>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning Workshop, 2014.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022a.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model, 2023.</p>
<p>Dylan Patel and Afzal Ahmad. Google "we have no moat, and neither does OpenAI", 2023.
Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine learning models via prediction APIs. In USENIX Security Symposium, 2016.</p>
<p>Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. Is ChatGPT good at search? investigating large language models as re-ranking agent. arXiv preprint arXiv:2304.09542, 2023.</p>
<p>Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301, 2023.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing GPT-4 with $90 \%$ * chatgpt quality, 2023.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. BAIR Blog, 2023.</p>
<p>Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. GPT4All: Training an assistant-style chatbot with large scale data distillation from GPT-3.5-Turbo, 2023.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277, 2023.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Change, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. TACL, 2019a.</p>
<p>Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is ChatGPT to human experts? Comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597, 2023.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. In EMNLP, 2022b.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. In OpenAI Technical Report, 2019.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR, 2021.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: A benchmark for question answering research. TACL, 2019b.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of EMNLP, 2020.</p>
<p>John Schulman. Reinforcement learning from human feedback: Progress and challenges. 2023.
Leo Gao. Behavior cloning is miscalibrated. Alignment Forum, 2021.
Yoav Goldberg. Reinforcement learning for language models. 2023.</p>
<p>Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. NIPS, 2017.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022.</p>
<p>Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. 2023.
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.</p>
<p>Daniel Lowd and Christopher Meek. Adversarial learning. In $K D D, 2005$.
Kalpesh Krishna, Gaurav Singh Tomar, Ankur P Parikh, Nicolas Papernot, and Mohit Iyyer. Thieves on sesame street! Model extraction of BERT-based APIs. In ICLR, 2020.</p>
<p>Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade, Shirish Shevade, and Vinod Ganapathy. A framework for the extraction of deep neural networks by leveraging public data. arXiv preprint arXiv:1905.09165, 2019.</p>
<p>Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. PRADA: protecting against DNN model stealing attacks. In IEEE EuroS\&amp;P, 2019.</p>
<p>Sebastian Szyller, Buse Gul Atli, Samuel Marchal, and N Asokan. DAWN: Dynamic adversarial watermarking of neural networks. In ACM Multimedia, 2019.</p>
<p>Pratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset inference: Ownership resolution in machine learning. In ICLR, 2021.</p>
<p>Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Prediction poisoning: Towards defenses against DNN model stealing attacks. In ICLR, 2020.</p>
<p>Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad Kaleem, Jonas Guan, and Nicolas Papernot. On the difficulty of defending self-supervised learning against model extraction. In ICLR, 2022a.</p>
<p>Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, and Nicolas Papernot. Increasing the cost of model extraction with calibrated proof of work. In ICLR, 2022b.</p>
<h1>A Additional Details on Imitation Data</h1>
<p>To construct the NQ-synthetic dataset, we first curate seed examples from the Natural Questions validation set in Table 3. We then use the prompting template in Table 4 to randomly sample 5 QA pairs from the seed set to generate new QA samples. New samples are generated with temperature 1.0 and duplicate question-answer pairs are discarded.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">who</span><span class="w"> </span><span class="n">sang</span><span class="w"> </span><span class="n">who</span><span class="w"> </span><span class="n">wants</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">millionare</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">high</span><span class="w"> </span><span class="n">society</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Frank</span><span class="w"> </span><span class="n">Sinatra</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">last</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="n">la</span><span class="w"> </span><span class="n">dodgers</span><span class="w"> </span><span class="n">won</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">world</span><span class="w"> </span><span class="n">series</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">1988</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">who</span><span class="w"> </span><span class="n">plays</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">medical</span><span class="w"> </span><span class="n">examiner</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">hawaii</span><span class="w"> </span><span class="n">five</span><span class="o">-</span><span class="n">o</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Masi</span><span class="w"> </span><span class="n">Oka</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">did</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">harry</span><span class="w"> </span><span class="n">potter</span><span class="w"> </span><span class="n">movie</span><span class="w"> </span><span class="n">come</span><span class="w"> </span><span class="n">out</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">2001</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">last</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="n">india</span><span class="w"> </span><span class="n">won</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">gold</span><span class="w"> </span><span class="n">medal</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">hockey</span>
<span class="n">at</span><span class="w"> </span><span class="n">olympics</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">1980</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">who</span><span class="w"> </span><span class="n">owns</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">rights</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">baby</span><span class="w"> </span><span class="n">shark</span><span class="w"> </span><span class="n">song</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">SmartStudy</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">episodes</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">punch</span><span class="w"> </span><span class="n">man</span><span class="w"> </span><span class="n">season</span><span class="w"> </span><span class="mi">1</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">12</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bird</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">lion</span><span class="w"> </span><span class="n">king</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Zazu</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">who</span><span class="w"> </span><span class="n">sang</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">rap</span><span class="w"> </span><span class="n">song</span><span class="w"> </span><span class="n">change</span><span class="w"> </span><span class="n">clothes</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Jay</span><span class="o">-</span><span class="n">Z</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">who</span><span class="w"> </span><span class="n">stars</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">serena</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">gossip</span><span class="w"> </span><span class="n">girl</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Blake</span><span class="w"> </span><span class="n">Lively</span>
</code></pre></div>

<p>Table 3: Seed examples curated from the Natural Questions validation set</p>
<div class="codehilite"><pre><span></span><code><span class="nv">I</span><span class="w"> </span><span class="nv">want</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">generate</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">series</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">questions</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">answers</span>.
<span class="nv">I</span><span class="w"> </span><span class="nv">want</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">answers</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">concise</span>,<span class="w"> </span><span class="nv">just</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">few</span><span class="w"> </span><span class="nv">words</span>.<span class="w"> </span><span class="nv">The</span>
<span class="nv">questions</span><span class="w"> </span><span class="nv">should</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">lowercased</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">centered</span><span class="w"> </span><span class="nv">around</span>
<span class="nv">Wikipedia</span><span class="o">-</span><span class="nv">like</span><span class="w"> </span><span class="nv">entities</span>.<span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="nv">example</span>,
<span class="nv">Q</span>:<span class="w"> </span>{<span class="nv">question</span><span class="w"> </span><span class="mi">1</span>}
<span class="nv">A</span>:<span class="w"> </span>{<span class="nv">answer</span><span class="w"> </span><span class="mi">1</span>}
<span class="nv">Q</span>:<span class="w"> </span>{<span class="nv">question</span><span class="w"> </span><span class="mi">2</span>}
<span class="nv">A</span>:<span class="w"> </span>{<span class="nv">answer</span><span class="w"> </span><span class="mi">2</span>}
<span class="nv">Q</span>:<span class="w"> </span>{<span class="nv">question</span><span class="w"> </span><span class="mi">3</span>}
<span class="nv">A</span>:<span class="w"> </span>{<span class="nv">answer</span><span class="w"> </span><span class="mi">3</span>}
<span class="nv">Q</span>:<span class="w"> </span>{<span class="nv">question</span><span class="w"> </span><span class="mi">4</span>}
<span class="nv">A</span>:<span class="w"> </span>{<span class="nv">answer</span><span class="w"> </span><span class="mi">4</span>}
<span class="nv">Q</span>:<span class="w"> </span>{<span class="nv">question</span><span class="w"> </span><span class="mi">5</span>}
<span class="nv">A</span>:<span class="w"> </span>{<span class="nv">answer</span><span class="w"> </span><span class="mi">5</span>}
</code></pre></div>

<p>Table 4: Prompting template used to generate synthetic Natural Questions-like imitation data</p>
<p>Figure 6 shows examples from ShareGPT-Mix and Table 5 shows a breakdown of different categories.</p>
<h2>B Amazon Mechanical Turk Interface</h2>
<p>We use Amazon Mechanical Turk to conduct human evaluations. We use the UI shown in Figure 7. It shows human evaluators a random task instruction and the output responses from two systems, one of which is our model and the other is ChatGPT. The annotators then choose which response is better according to overall subjective quality. We randomize whether ChatGPT or our imitation models are shown first. We collect 3 unique ratings for every example in the evaluation set and a total of 71 human evaluators participated. In order to get an average score, we use majority voting among the 3</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: center;">Frequency (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Generation</td>
<td style="text-align: center;">31</td>
</tr>
<tr>
<td style="text-align: left;">General QA</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Coding</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Brainstorming</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Chat</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">Summarization</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Rewrite</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Non-English</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: left;">Invalid</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
<p>Table 5: We conduct a manual quality review of 50 random user queries from ShareGPT. The dataset contains highly varied categories of task instructions, including coding and multi-lingual queries.</p>
<p>ShareGPT-Mix Data
User: Return to the Identity and Access Management Commercial Due Diligence. We now want to model the market size for the IAM market. What are the levers and inputs we need to accurately estimate the size and growth of the U.S. market. For example, these may include the number of companies, current saturation, and specific market headwinds and tailwinds you identify. Provide suggestions where you would look for data for each of the levers.
GPT: To accurately estimate the size and growth of the U.S. Identity and Access Management (IAM) market, there are several key levers and inputs to consider:</p>
<ol>
<li>Number of companies: The number of companies operating in the U.S. IAM market is an important lever to consider when estimating market size. This includes both large, established vendors and smaller, niche players. Data on the number of companies can be found through market research reports and industry databases, such as Gartner, IDC, and MarketsandMarkets.</li>
<li>Current saturation: ...</li>
</ol>
<p>User: I am building an Even Capital with all of it's history, religion, politics, trade routes, people, traditions, laws, general rules, and more. It is called Oniver. Your job is to ask me questions to learn more about the town and help me fill in the blanks to make it a rich and thriving town with lots of history. Ask detailed questions that will make me think. Say I understand if this makes sense. Then ask me the first question.
GPT: I understand.
What is the history behind Oniver's founding? How did it come to be and what was the context in which it was established?
User: Magic is widely used in Oniver. Better magic is a path to improvement in life and status in the capital. It is also the ticket into getting into the Magic University. There is a high importance between magical ability within the noble families of Oniver. Magic is used as a tool by the wealthy to oppress the lower classes. Commoners practicing magic is not forbidden but is looked down upon by the nobles because they see themselves as better than the commoners.
GPT: I understand.
What kind of economy does Oniver have? Is it based on trade, agriculture, manufacturing or a combination of these things? Are there any restrictions or laws surrounding trade and commerce? ...</p>
<p>Figure 6: Examples of user inputs and ChatGPT outputs that are present in the ShareGPT data. Overall, we find that online datasets are typically high-quality and diverse in their user inputs, and span multiple categories such as open-ended text generation, brainstorming, and text extraction.
raters on each example, and then average the scores across all examples. We pay these evaluators roughly $\$ 15 /$ hour based on the average time it takes to complete a task. In total, we spend roughly $\$ 5000$ on our ratings experiments, including service fees.</p>
<h1>C GPT-4 evaluations</h1>
<p>Our GPT-4 evaluations follow the procedure from Chiang et al. (2023): we prompt GPT-4 with two outputs, one from ChatGPT and one from our imitation models. We then ask GPT-4 to output a preference ranking of the two outputs. We use the same set of evaluation prompts as in our humanpreference evaluations. In Figure 3(a), we see that as we add more imitation data GPT-4's ratings of our model outputs remain reletively flat. However as we increase the base model scale, we see GPT-4's ratings consistently increasing 3(b). These results line up closely with the results from our crowdworker evaluations.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Our Amazon Mechanical Turk interface for comparing the quality of different model outputs. Evaluators are presented with an instruction and two model outputs, and must rate which one is better or whether they are equal.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>to complete a task. We select workers with $\geq 95 \%$ approval rating, are located in an English-speaking country, and have at least 100 HITs completed.
${ }^{4}$ Codebase available at https://github.com/young-geng/EasyLM, data available at https://huggingface. co/young-geng/koala-eval, and pre-trained models available at https://huggingface.co/young-geng/koala.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
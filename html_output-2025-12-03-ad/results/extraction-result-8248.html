<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8248 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8248</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8248</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-279075317</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.01371v1.pdf" target="_blank">SVQA-R1: Reinforcing Spatial Reasoning in MLLMs via View-Consistent Reward Optimization</a></p>
                <p><strong>Paper Abstract:</strong> Spatial reasoning remains a critical yet underdeveloped capability in existing vision-language models (VLMs), especially for Spatial Visual Question Answering (Spatial VQA) tasks that require understanding relative positions, distances, and object configurations. Inspired by the R1 paradigm introduced in DeepSeek-R1, which enhances reasoning in language models through rule-based reinforcement learning (RL), we propose SVQA-R1, the first framework to extend R1-style training to spatial VQA. In particular, we introduce Spatial-GRPO, a novel group-wise RL strategy that constructs view-consistent rewards by perturbing spatial relations between objects, e.g., mirror flipping, thereby encouraging the model to develop a consistent and grounded understanding of space. Our model, SVQA-R1, not only achieves dramatically improved accuracy on spatial VQA benchmarks but also exhibits interpretable reasoning paths even without using supervised fine-tuning (SFT) data. Extensive experiments and visualization demonstrate the effectiveness of SVQA-R1 across multiple spatial reasoning benchmarks.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8248",
    "paper_id": "paper-279075317",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0046105,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SVQA-R1: Reinforcing Spatial Reasoning in MLLMs via View-Consistent Reward Optimization
2 Jun 2025</p>
<p>Peiyao Wang peiyaowang@cs.stonybrook.edu 
Department of Computer Science
Stony Brook University</p>
<p>Haibin Ling hling@cs.stonybrook.edu 
Department of Computer Science
Stony Brook University</p>
<p>SVQA-R1: Reinforcing Spatial Reasoning in MLLMs via View-Consistent Reward Optimization
2 Jun 20256EAB1A859C9A783D395E6E14CF40F4E4arXiv:2506.01371v1[cs.CV]
Spatial reasoning remains a critical yet underdeveloped capability in existing vision-language models (VLMs), especially for Spatial Visual Question Answering (Spatial VQA) tasks that require understanding relative positions, distances, and object configurations.Inspired by the R1 paradigm introduced in DeepSeek-R1, which enhances reasoning in language models through rule-based reinforcement learning (RL), we propose SVQA-R1, the first framework to extend R1-style training to spatial VQA.In particular, we introduce Spatial-GRPO, a novel groupwise RL strategy that constructs view-consistent rewards by perturbing spatial relations between objects, e.g., mirror flipping, thereby encouraging the model to develop a consistent and grounded understanding of space.Our model, SVQA-R1, not only achieves dramatically improved accuracy on spatial VQA benchmarks but also exhibits interpretable reasoning paths even without using supervised fine-tuning (SFT) data.Extensive experiments and visualization demonstrate the effectiveness of SVQA-R1 across multiple spatial reasoning benchmarks.</p>
<p>Introduction</p>
<p>Although open-source vision language models (VLM), such as LLaVA [18] and Qwen2.5-VL[4], have achieved impressive results in standard VQA tasks, they often fail to handle spatial reasoning well [5].These limitations stem from several issues.First, such models exhibit a strong local visual bias, tending to focus on salient individual objects rather than modeling spatial relations between them.For example, when asking "Is the cup to the left of the laptop", the model may successfully identify the cup and the laptop separately but fail to reason about their relative positions.This is partially because most existing training data prioritize object classification over spatial logic, leading to an underdeveloped spatial reasoning capacity.As a result, models may rely heavily on object categories and make arbitrary spatial guesses.Second, spatial VQA tasks often require fine-grained localization, which general VLMs struggle with, especially in cluttered scenes or when small objects are involved.Finally, these models typically lack an explicit reasoning chain, jumping directly from visual input to answer without intermediate thinking steps.This makes them prone to failure in multi-step spatial understanding, such as interpreting "above the fridge and on the table" as a layered spatial relation.These limitations collectively make general VLMs ill-suited for complex spatial understanding tasks.</p>
<p>Recent works [5,19] have begun addressing these limitations by introducing spatially-targeted supervision.For example, SpatialVLM [5] proposes a synthetic data generation pipeline to efficiently construct large-scale spatial VQA samples, allowing supervised training that improves performance in spatial reasoning tasks.Building on this, SpatialPIN [19] incorporates 3D priors to directly enhance object relation modeling without modifying the underlying VLM architecture, which yields further gains.However, both approaches rely heavily on supervised fine-tuning (SFT), which may lead to convergence to static output patterns and limit reasoning flexibility.To overcome these constraints, we draw inspiration from the R1 paradigm and explore a reinforcement learning-based approach that encourages dynamic self-correcting spatial reasoning behavior through rule-based feedback.</p>
<p>In the R1 paradigm proposed by DeepSeek-R1 [10], simple rule-based reinforcement learning (RL) has been shown to significantly enhance the reasoning capabilities of large language models (LLMs) without the need for manually annotated data thought processes.This approach offers an efficient and scalable pathway for training complex reasoning skills.Building upon this paradigm, we systematically extend the R1 framework to spatial reasoning tasks and propose the SVQA-R1 framework.Specifically, we tailor the R1-style reinforcement learning to the characteristics of vision-language models by introducing a Spatial-GRPO mechanism.In this setup, we construct a rule-based reward to verify the format correctness and a semantic-aware reward to handle the verification of different answer types, e.g., bounding box, caption, distance, single or multiple choice, etc.Furthermore, we design a view-consistent reward that encourages the policy model to yield the correct answer for both the original VQA and the spatially augmented one at the same time, in case the model guesses only one of them right by chance.Thus, our proposed Spatial-GRPO can offer the model a view-consistent spatial reasoning capacity.</p>
<p>In the experiment, we observe that SVQA-R1 can improve the accuracy over the SFT-based baseline by more than 30% on the Q-Spatial++ benchmark [17], while also exhibits an interpretable reasoning path.In addition, it dramatically outperforms the closed-source models Gemini-1.5-Flash[2] and it is competitive with the GPT-4o [13].Also, it outperforms open-source models InternVL-2.5 [7] and Qwen2.5VL[4] by a large margin.We expect that such RL-based research exploration can advance the spatial reasoning in MLLMs.</p>
<p>Our contributions are as follows: 1) We explore rule-based and semantic-based RL in MLLMs for the spatial reasoning task, empowered with an interpretable reasoning path via LLM prompting; 2) we design a novel Spatial-GRPO mechanism which prompts the policy model to learn a view-consistent action space; and 3) our proposed SVQA-R1 approach achieves a very promising result of 58% on Q-Spatial++, outperforming other open-source models by a large margin.</p>
<p>Related Work</p>
<p>Spatial Reasoning with VLMs Spatial reasoning, crucial for interpreting spatial relationships in visual scenes, remains a significant challenge for vision-language models (VLMs) in tasks like spatial visual question answering (Spatial VQA).Early VLMs, such as CLIP [24], excel in 2D visual tasks but struggle with 3D spatial relationships and compositional reasoning [29,23,9].Models like MDETR [14] improve cross-modal alignment with spatial attention, yet quantitative spatial tasks (e.g., distance estimation) remain difficult [17].Recent advancements address these limitations: SpatialVLM [5] leverages a 2-billion-example 3D spatial dataset, enhancing VLMs' qualitative and quantitative Spatial VQA capabilities, supporting applications like chain-of-thought (CoT) reasoning [31].SpatialPIN [19] introduces zero-shot 3D reasoning via 3D priors, improving Spatial VQA and robotics tasks like trajectory planning.MM-Spatial [8] utilizes large-scale 3D scene data with the Cubify Anything VQA (CA-VQA) dataset, enabling robust 3D spatial understanding, including spatial relationship prediction and metric estimation, with state-of-the-art performance on 3D benchmarks.Datasets like GQA [12] and Visual Spatial Reasoning (VSR) [18] highlight VLM weaknesses in multi-hop spatial queries.While neuro-symbolic methods [35,28] and RL-guided attention [26,11] decompose spatial tasks or optimize decisions, explicit reasoning via CoT and adaptive learning through RL are underexplored for complex Spatial VQA.Our work aims to bridge this gap, enhancing VLM generalization and compositional reasoning in diverse spatial scenarios.</p>
<p>Chain-of-Thought and R1-style Reinforcement Learning In Vision-Language Models Recent research has increasingly focused on enhancing the reasoning capabilities of vision-language models through structured prompting and reinforcement learning.Chain-of-Thought (CoT) prompting [31], initially effective in language models, has been extended to vision-language models to improve multi-step visual reasoning, grounding, and spatial inference [38,30].Complementary to promptingbased approaches, R1-style reinforcement learning originally proposed in DeepSeek-R1 [10] has been adapted to multimodal domains to promote verifiable, rule-guided learning.Models such as Visual-RFT [33] and VLM-R1 [39] apply Group Relative Policy Optimization (GRPO) [27] with task-specific rewards (e.g., correctness of spatial relations or grounding accuracy), achieving superior generalization over supervised fine-tuning.Video-R1 [34] further extends this framework to temporal Question: Which is more to the left, the cup of coffee or the bowl of dried fruits?Answer: bowl of dried fruits is more to the left.</p>
<p>Question: Which is more to the right, the cup of coffee or the bowl of dried fruits?Answer: cup of coffee is more to the right.❌ Updated Answer: bowl of dried fruits is more to the right.✅reasoning by integrating T-GRPO and constructing fine-grained video-reasoning datasets.Recent extensions, such as TinyLLaVA-Video-R1 [16] and VideoChat-R1 [37], demonstrate that R1-style training enhances spatial and temporal understanding even in smaller-scale models.In addition, Reason-RFT [32] explores reasoning-aware RL using chain-of-thought supervision combined with GRPO.</p>
<p>Together, these CoT-based prompting and R1-style RL methods reflect a shift towards more explicit and interpretable reasoning in multimodal systems, enabling models to generalize better across complex spatial, temporal, and causal reasoning tasks.</p>
<p>Method</p>
<p>We begin by presenting our approach to generating mirror-consistent QA pairs in Section 3.1, and then introduce the mixed reward strategy in Section 3.2 and the overall Spatial-GRPO training procedure in Section 3.3.</p>
<p>Mirror-Consistent Reasoning via QA Adaptation</p>
<p>To encourage the policy model to learn a view-consistent action space, we can use Nerf [20] or Gaussian Splatting [15] to synthesize novel-view images for the existing data, or collect the multiview data [36] from scratch.Due to the imperfect synthetics and expensive data collection costs, we instead use mirror flipping to augment the spatial relation for the single-view image datasets.Such a low-cost operation allows us to easily scale our approach to any existing dataset at a large scale.</p>
<p>To enable mirror-consistent spatial reasoning, we construct horizontally flipped image samples paired with semantically aligned question-answer (QA) pairs.A key challenge is that simple horizontal flipping changes the spatial configuration (e.g., "left" becomes "right"), requiring corresponding adjustments to the associated QA content to ensure consistency.</p>
<p>To address this, we employ GPT-4o [13] to automatically generate revised QA pairs for each flipped image.We provide the original image's question and answer, and prompt the model to produce a logically correct version suitable for the flipped view.However, we observe that a naïve prompting strategy often leads to shallow edits.GPT-4o tends to mechanically swap "left" with "right" without properly understanding the full relational semantics of the scene.This results in numerous logically inconsistent or even contradictory QA pairs, illustrated in Figure 1 (b) as the answer of the flipped image.</p>
<p>To improve reliability, we enhance the prompt by explicitly instructing GPT-4o to verify the consistency of the revised QA with the mirrored image.The prompt is shown in Table 9, and the red highlight is the enhanced content.This includes guidance to reason about relative positions and object identities before finalizing the output.With this enhanced prompting strategy, most flipped QA pairs are logically correct and spatially coherent, even in multi-object scenes involving complex spatial arrangements, illustrated in Figure 1 (b) as the updated answer of the flipped image.</p>
<p>This QA adaptation process enables us to generate high-quality mirrored-view data, providing the necessary foundation for implementing Spatial-GRPO.It ensures that both original and flipped samples are semantically aligned, making joint reward computation meaningful and accurate during training.</p>
<p>You are a spatial reasoning assistant.You are given a question and its corresponding answer based on an image.Now assume that the image has been horizontally flipped.Your task is to rewrite the question and answer so that they remain logically correct for the flipped image.Write them as if the flipped image was the original, and do not mention the flip in your output.Also, verify the correctness of the left/right spatial relationship in the original answer.If the rewritten answer is inconsistent with the horizontal flip (i.e., the object that was on the left is still on the left), you must fix it.If you find an error, correct the object-direction mapping accordingly.Original question:</p>
<p>{question}.Original answer: {answer}.Return your output as a valid JSON object, and nothing else.</p>
<p>Table 1: Instruction prompt for the flipping data.The placeholders {question} and {answer} will be replaced with specific content.Text in red highlights the verification and correction instructions.</p>
<p>Mixed Reward Design</p>
<p>Unlike structured tasks such as classification or numerical prediction, spatial reasoning in visionlanguage models (VLMs) often involves open-ended question-answering with diverse and linguistically rich output.As illustrated in Figure 2, model responses may vary in form, ranging from short directional terms to complete relational descriptions.This inherent variability presents challenges for rule-based reward functions, which rely on exact string matching or rigid templates.Applying such methods directly risks penalizing semantically correct answers that deviate from expected wording, thereby limiting the expressiveness and performance potential of VLMs.</p>
<p>To accommodate this flexibility, we adopt a semantic-based reward that evaluates the alignment between the model's output and the ground truth based on meaning rather than surface form.This approach allows the model to generate natural and fluent responses while still being guided by accurate spatial understanding.In addition, motivated by findings from recent work [5] that highlight the benefits of Chain-of-Thought (CoT) reasoning in improving multistep spatial inference, we incorporate a format reward to further encourage structured and interpretable outputs.Together, these components support a more flexible and faithful reward mechanism tailored to the open-ended nature of spatial reasoning tasks.</p>
<p>Format Reward r f .We include a binary signal that assigns 1 if the output follows the required structural format (e.g., <think>...</think> <answer>...</answer>), and 0 otherwise.</p>
<p>Semantic-aware Reward r s .To capture semantic similarity beyond surface-level token overlap, we use a Sentence-BERT-based reward.Specifically, we compute sentence embeddings using a pre-trained Sentence-BERT [25] model (e.g., all-MiniLM-L6-v2) and measure the cosine similarity between the predicted and reference answers.This embedding-based reward enables the model to recognize semantically equivalent expressions that lexical metrics such as BLEU fail to identify, for example, assigning a high reward to predictions like "sofa" when the ground truth is "couch".In addition, Sentence-BERT-based reward can measure the numerical difference when handling those answer types, such as distances and bounding boxes.Such a reward unification can stabilize the training, as we observe.</p>
<p>Final Reward.The total reward is defined as a weighted sum of all components:
r = λ 1 • r f + λ 2 • r s(1)
where λ 1 , λ 2 are hyperparameters.This composite reward encourages the model to generate answers that are structurally valid, lexically and semantically correct, and numerically plausible.</p>
<p>Spatial Group Relative Policy Optimization (Spatial-GRPO)</p>
<p>While GRPO has demonstrated promising results in general reasoning tasks, it lacks explicit reward signals that are specifically designed to guide spatial reasoning.This makes it inadequate for training Vision-Language Models (VLMs) to understand and reason about spatial relationships such as "left of", "above", or "closer to".To address this limitation, we introduce Spatial Group Relative Policy Optimization (Spatial-GRPO), a joint reinforcement learning framework that explicitly targets spatial understanding.</p>
<p>Our approach is grounded in two key observations about human spatial cognition: (1) quantitative measurements such as distance between objects remain invariant under horizontal mirroring, and (2) qualitative relationships like "in front of" or "next to" are preserved, while directional expressions such as "left" and "right" require symmetric adjustment.We aim to replicate this perceptual stability, encouraging the model to maintain semantically accurate spatial reasoning for both views.</p>
<p>Specifically, given an original image and question pair (I, Q), and the horizental fliped imge and update queation is tuple ( Î, Q).They will obatian grouped sample outputs {o i } G i=1 and {ô i } G i=1 from the old policy model π old .Then we compute the rewards r i , ri for each sample in the original and flipped groups independently, based on their respective question-answer alignments.To promote consistent and accurate spatial reasoning across both the original and mirrored views, we aim to minimize the discrepancy between the two reward distributions.Specifically, we compare the aggregated rewards Avg({r i } G i=1 ) and Avg({r i } G i=1 ) from the original and flipped groups and penalize the group that achieves a substantially higher score.This design reflects our assumption that a truly spatially grounded model should perform similarly across both views, and that large discrepancies indicate a lack of spatial consistency.</p>
<p>To operationalize this idea, we define a consistency-aware joint reward that encourages both high performance and inter-view agreement.Specifically, we use the semantic reward difference between the original and flipped views to quantify the discrepancy in model responses.This choice reflects our focus on semantic alignment, as it is independent of surface-level formatting variations in the generated outputs.We define the semantic difference as:
∆ = Avg({r s i } G i=1 ) − Avg({r s i } G i=1 )
(2) The the modified reward for the semantic rewards as follows:
r s i = r s i − η|∆|, if r s i &gt; δ and ∆ &gt;= 0 (3) rs i = rs i − η|∆|, if rs i &gt; δ and ∆ &lt; 0(4)
The final Spatial-GRPO maximizes the following objective:
J Spatial-GRPO (θ) = E q,q,{oi,ôi}∼πold 1 2G G i=1 (R i + Ri ) − βD KL (π θ ∥ π ref )(5)R i = 1 |o i | |oi| t=1 min (α t (θ)A i , clip(α t (θ), 1 − ϵ, 1 + ϵ)A i ) , α t (θ) = π θ (o i,t | q, o i,&lt;t ) π old (o i,t | q, o i,&lt;t )(6)
where
A i = ri−mean({ri} G i=1 ) std({ri} G i=1 )
is the estimated advantage, η is a scaling coefficient, β is the KL penalty coefficient, and π θ , π old , π ref denote the current, old, and reference policy models, respectively.ϵ is the threshold for clipping.q is the input pair of the image and the question.The definition of Ri is similar to R i .</p>
<p>Experiment</p>
<p>Experimental Settings</p>
<p>We adopt Qwen2.5-VL-3B[4] as our base model and use its pre-trained weights.The maximum generation length is set to 2048 tokens.For the reward computation, we set the weighting coefficients λ 1 and λ 2 to 0.5, balancing between the semantic and format rewards.When updating the semantic reward, we apply a threshold δ = 0.5, and set the final reward scaling factor η = 1.During GRPO training, we generate 8 candidate answers per input as a group.We set the gradient accumulation steps to 2 and the per-device batch size to 8. Training is conducted on 8 NVIDIA A6000 GPUs.</p>
<p>Datasets</p>
<p>Training Dataset.For training, we adopt the Vqasynth_Spacellava [1] dataset, which contains more than 28,000 multi-turn dialogues designed for visual question answering (VQA).This dataset includes a rich mixture of spatial VQA and general VQA samples.All training samples are automatically generated using the VQASynth [1] framework, guided by the spatial prompting techniques introduced in SpatialVLM [5].We split the multi-turn dialogues into multiple single-turn data, resulting in xxx training samples in total.Test Dataset.For numerical task evaluation, we adopt the Q-Spatial++ [17] dataset.Q-Spatial++ is a subset of the Q-Spatial-Bench dataset, designed to evaluate quantitative spatial reasoning in large vision-language models.It comprises 87 freshly captured images and 101 human expert-annotated questions, focusing exclusively on horizontal distances between objects in real-world scenes.To ensure high precision, physical measurements of object distances were taken during image capture, providing accurate ground-truth answers.</p>
<p>In addition, to verify the generalizability of our method, we construct two additional test sets.One is based on Vqasynth_Spacellava [1] and the other is based on OpenSpaces [1].Both are generated using the VQASynth pipeline.The OpenSpaces dataset is built by synthesizing spatial VQA samples from the first 30K rows of the localized narratives split of the Cauldron dataset using VQASynth, while Vqasynth_Spacellava samples are collected directly via multi-turn chat interactions.For each source, we collect approximately 1,000 multi-turn dialogues and convert them into single-turn QA pairs in the format of (image, question, answer) from their test set.As a result, the Vqasynth_Spacellava and OpenSpaces test sets contain 3,120 and 5,000 image-question-answer pairs, respectively.</p>
<p>Notably, since our model is only trained on the Vqasynth_Spacellava training set, the OpenSpaces test set can serve as an out-of-distribution benchmark to assess the robustness of spatial reasoning.Furthermore, OpenSpaces is enriched with more diverse and fine-grained spatial VQA questions, whereas Vqasynth_Spacellava includes a broader mixture of general VQA and spatial reasoning queries.This setup allows us to probe the model's ability to generalize across both spatial and non-spatial domains.</p>
<p>Evaluation Metrics</p>
<p>Numerical Task.We adopt three metrics to evaluate the numerical prediction performance: success rate (%), samples completed (%), and Symmetric Mean Absolute Percentage Error (sMAPE %).Success rate (%) measures the proportion of predictions that fall within a predefined tolerance range of the ground truth.It reflects how often the model produces "acceptable" answers.We define a prediction as success if max(GT/Pred, Pred/GT) &lt; 2. Samples Completed (%) denotes the percentage of test cases where the model outputs a parsable and meaningful numerical value, indicating the reliability and completeness of the model's responses.sMAPE (%) evaluates the prediction accuracy in a scale-invariant manner by comparing predicted values ŷ to the ground truth y.It is defined as:
sMAPE = 1 n n i=1 |ŷ i − y i | (|ŷ i | + |y i |) /2 × 100%(7)
To ensure fairness, we only compute sMAPE over test cases where the model produces a valid numerical prediction.Cases without a parseable number are excluded from the average and instead reflected in the "Samples Completed (%)" metric.</p>
<p>Open-ended Spatial VQA Task.To evaluate the accuracy of open-ended answers in our Spatial VQA task, we adopt three complementary metrics: BLEU-1 [22], Sentence-BERT [25] cosine similarity, and an LLM-based evaluation using GPT-4o [21].BLEU-1 computes unigram overlaps between the predicted and reference answers.While it captures basic lexical agreement, it often  fails to reflect semantic correctness, especially when answers are paraphrased or differ in surface form.To address this limitation, we additionally report Sentence-BERT similarity, which computes the cosine similarity between sentence-level embeddings of the predicted and reference answers.This embedding-based metric provides a more flexible evaluation of semantic alignment and is better suited for free-form VQA responses where multiple correct phrasings may exist.Finally, we incorporate an LLM-based evaluation using GPT-4o, which is asked to assess the correctness of the prediction.This metric leverages GPT-4o's advanced reasoning to provide a human-like judgment.</p>
<p>To better capture the true performance, we categorize questions into types (e.g., binary, descriptive, spatial) and perform type-specific evaluation.This allows us to more accurately assess the model's behavior under different reasoning demands.To evaluate bounding box predictions in our Spatial VQA task, we adopt two metrics: mean IoU (mIoU) and Accuracy@0.75.mIoU computes the average Intersection over Union between predicted and ground-truth boxes, reflecting overall localization quality.Accuracy@0.75 measures the percentage of predictions with IoU ≥ 0.75, indicating the proportion of highly accurate localizations.For Yes/No questions, we report standard answer accuracy, measuring the proportion of predictions that exactly match the ground-truth labels.For Distance Estimation, we consider a prediction correct if it falls within the range from 50% to 200% of the ground truth, capturing relative correctness under spatial uncertainty.</p>
<p>Evaluation on Numerical Tasks</p>
<p>Our method, SVQA-R1, outperforms all open-source models across nearly all evaluation metrics and shows a competitive result compared to GPT-4o.In particular, it achieves a 58.42% success rate, which represents a 31% absolute improvement over SpaceThinker-Qwen2.5VL-3B, a model fine-tuned with supervised Spatial-CoT data.This demonstrates the effectiveness of our reinforcement learning strategy in enhancing spatial quantitative reasoning.Moreover, SVQA-R1 surpasses Qwen2.5VL-3B<em> and Qwen2.5VL-3B by 47% and 10% in success rate, respectively.It also achieves the lowest sMAPE (68.36), indicating superior precision in numerical estimation.These results suggest that reinforcement learning not only improves the model's ability to align with spatial reasoning tasks but also makes the thinking prompt more adaptable to numerical estimation.Interestingly, we observe that adding the thinking prompt without reinforcement learning (Qwen2.5VL-3B)slightly degrades performance compared to the vanilla Qwen2.5VL-3B</em> , implying that prompt tuning alone may not benefit spatial reasoning unless coupled with proper optimization signals.[1] and OpenSpaces [1].* indicates models do not use the thinking prompt.</p>
<p>Evaluation on Open-ended Spatial VQA</p>
<p>In Figure4, we compare various methods on the open-ended spatial VQA datasets Vqasynth_Spacellava and OpenSpaces.Among the open-source models, our proposed SVQA-R1 achieves the best overall performance across nearly all metrics.Note that the OpenSpaces dataset does not contain bounding box annotations, so the related columns are left blank for that section.</p>
<p>On Vqasynth_Spacellava, SVQA-R1 shows strong performance, especially on distance estimation.This may benefit from our mirror-consistent training, which encourages the model to maintain consistent spatial interpretation under image flips.Since distance remains invariant under horizontal flipping, this constraint may help the model better learn distance representations.We also observe improvements in bounding box prediction, suggesting that SVQA-R1 enhances spatial relational understanding through view-consistent reasoning.</p>
<p>We observe that the low BLEU-1 scores in SpaceThinker-QWen2.5-VL are primarily caused by a mismatch in answer length and style.Specifically, the model often generates overly brief answers such as "yes", "no", or "true", whereas the ground truth annotations tend to provide slightly more descriptive responses.This discrepancy significantly reduces the unigram precision measured by BLEU-1, even when the model's answers are semantically correct.</p>
<p>Ablation Study</p>
<p>From Table 5, we observe that even using only the format reward in GRPO (comparison between (a) and (b)) leads to a significant improvement in performance, especially in Success Rate on Q-Spatial++, which increases by over 12%.We hypothesize that format guidance helps the model arrive at final answers more reliably by reinforcing output structure during the reasoning process.On Vqasynth_Spacellava, while the improvement in LLM accuracy is moderate, BLEU-1 improves dramatically (from 10.00% to 51.55%), suggesting that format reward strongly enhances syntactic alignment with the ground truth.In contrast, using only the semantic reward (comparison between (a) and (c)) improves LLM accuracy (from 45.33% to 60.34%) and also yields a large gain in Success Rate on Q-Spatial++.However, BLEU-1 drops to 0.00% and sBERT to 16.14%, indicating that the model outputs are semantically close but fail to match token-wise with reference answers.This is expected, as semantic reward emphasizes meaning-level similarity, while BLEU and sBERT are sensitive to surface form and token alignment.After incorporating spatial-GRPO (e), all metrics improve consistently.The sMAPE is significantly reduced, reflecting better numeric precision in quantity reasoning.Meanwhile, BLEU-1 and sBERT also achieve the highest scores, confirming that SVQA-R1 enhances semantic alignment but also supports more accurate numerical understanding.SVQA-R1-3B: <think> The sentence mentions "red and blue stripes around border," which likely refers to the border of the tent.The tent is positioned on the left side of the image, so its border would be visible as a thin line along the edge of the tent.The color scheme described (red and blue) is consistent with the tent's border.Therefore, the location of the red and blue stripes around border can be found on the left side of the image.</think>\n<answer> [0.0, 0.37, 0. SVQA-R1-3B: <think> The artwork on the left side of the screen on the wall appears to be colorful and vibrant, which suggests it could be American pop art.This style often features bold colors and recognizable imagery, such as celebrities or everyday objects.</think>\n<answer>B</answer> (a) ( Vqasynth_Spacellava is an open-ended spatial VQA dataset, the questions are diverse yet inherently spatial.For example, in Figure 7(b), the model is required to estimate the distance between people distributed across the scene, while in Figure 7(d), it needs to recognize which person is on the "left", both requiring spatial relationship reasoning.</p>
<p>From the results, we observe that the model tends to reason step-by-step, exhibiting logical progression rather than generating a flat caption.For instance, it often begins with a clarifying intent such as "I'll need to ...", then introduces uncertainty-aware expressions like "likely," "would be," or "suggest it could be" to hypothesize plausible answers.After outlining its reasoning steps, the model often concludes with a summary like "Therefore, ...", indicating a final decision based on prior analysis.Please refer to the supplementary material for more visualization.</p>
<p>Conclusion and Limitation</p>
<p>In this work, we present a novel SVQA-R1 to enhance spatial reasoning capacity in MLLMs.Inspired by the success of the recent R1-style training recipe introduced by Deepseek-R1, we invent a novel view-consistent reward function to encourage the policy model to learn a view-consistent action space.We evaluate our approach on multiple benchmarks, and it exhibits promising results, compared to both closed-source and open-source models.After the self-exploration via Spatial-GRPO in MLLMs, we observe interpretable and reasonable reasoning paths.closed-sourceand open-source models.</p>
<p>Limitations.Though the mirror flipping efficiently augments the existing single-view images, more advanced techniques such as Nerf and Gaussian Splatting for novel view synthesis are not investigated yet, where we leave them for future work.</p>
<p>A.3 Cases Unaffected by Flipping</p>
<p>Some questions are semantically or spatially invariant to horizontal flipping.For example, questions about object attributes, counts, or global scene understanding often yield the same answer regardless of image orientation.This section showcases such cases in Figure 6, confirming that the model maintains output stability when the visual change does not affect semantic interpretation.predicted answer means the same as the ground-truth answer.</p>
<p>Predicted answer: {pred}, ground-truth answer: {gt}.Output only 0 or 1, where 0 indicates incorrect and 1 indicates correct.</p>
<p>B.3 Distance Prediction</p>
<p>To identify whether a question requires distance estimation, we rely on a set of indicative phrases commonly associated with spatial measurement.The following keywords are used for matching:</p>
<p>• DISTANCE_KEYWORDS: how far, distance between, distance from, which is closer, which is farther, closer, further, nearer, farthest, measure the distance, what is the distance, spacing between, gap between.</p>
<p>If any of these phrases appear in the question text (matched in a case-insensitive manner), we classify the corresponding sample as a distance prediction task.This classification enables the use of appropriate numerical evaluation metrics, such as absolute error or symmetric Mean Absolute Percentage Error (sMAPE).Based on this criterion, we identify 186 samples (accounting for 6% of the dataset) as distance-related questions.</p>
<p>C The scaling coefficient η for the reward difference</p>
<p>In Table 6, we also explore the effect of applying different scaling coefficients to the reward difference during reward correction.By adjusting the scaling factor of the reward difference, we aim to control the impact of relative feedback and examine its influence on model training and convergence.</p>
<p>We observe that assigning a larger weight to the reward difference improves the performance of the distance estimation task, as reflected by a higher sMAPE score.This indicates that the model produces more accurate numerical predictions.However, we also notice a decrease in the success rate.We hypothesize that while emphasizing the reward gap helps the model focus more on the exact value, it may simultaneously reduce its confidence or stability, leading to uncertainty in determining whether the output is sufficiently close to the ground truth.</p>
<p>In the case of the VQASynth_Spacellava dataset, we find that increasing the reward weight (comparing weight 1 vs. weight 2) results in slightly better BLEU and Sentence-BERT scores, but a lower LLMbased evaluation score.We conjecture that as the weight increases, the model becomes more sensitive</p>
<p>D Different initialization of the MLLMs</p>
<p>In Table 7, we investigate the impact of different base model initializations on downstream performance.By initializing the model from various pretrained checkpoints, we aim to assess how the starting point influences training dynamics and final accuracy.It turns out that the officially original checkpoint performs better on both datasets.</p>
<p>E Differnt inference prompt</p>
<p>We further experiment with different prompting strategies to encourage step-by-step reasoning in the model's responses.Specifically, we compare the original prompt (a) with variants such as spatial reasoning prompts (b) and others tailored to the task context, as summarized in Table 9, and the results are in Table 8.</p>
<p>We find that although the new prompt encourages the model to produce more reasoning steps, it results in worse performance on the distance estimation task.We hypothesize that this may be due to the fact that the model was originally fine-tuned using the original prompt format, which may not align well with the new prompt structure.This mismatch could lead to degraded performance when adapting to the modified prompting style.</p>
<p>Prompt Type Description</p>
<p>Original {Question} First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags.</p>
<p>Spatial Reasoning Prompt</p>
<p>Question: { Question } Use the following 4 steps sequentially to answer the question:</p>
<p>Step  Between the woman wearing gold earrings and the fruit display with apples and oranges, which one is positioned higher?</p>
<p>SVQA-R1-3B: <think> The woman wearing gold earrings appears on the left side of the image, while the fruit display with apples and oranges is more towards the top of the image.The fruit display is higher up than the woman.</think><answer> fruit display with apples and oranges </answer> (c) (d)</p>
<p>Question:</p>
<p>Tell me the distance between the grey metal support beams with yellow shelving units and the yellow metal shelving units with cardboard boxes vertically.SVQA-R1-3B: <think> To calculate the distance between the grey metal support beams with yellow shelving units and the yellow metal shelving units with cardboard boxes vertically, we need to consider several factors: 1.The position of the grey metal support beams along the length of the shelving units.</p>
<ol>
<li>
<p>The thickness of the yellow metal shelving units.</p>
</li>
<li>
<p>The vertical distance between the grey support beams and the next shelf.</p>
</li>
</ol>
<p>Assuming there are no gaps between the shelves or supports, and taking into account the uniform structure of the shelving units:</p>
<p>-The distance between the grey metal support beams would be the width of the shelving units.</p>
<p>-The vertical distance would be the height from the grey support beam to the next shelf, which we can estimate based on the shelving unit's height.</p>
<p>Given that the image is an overall view and a precise distance is not clearly visible, we need to estimate the vertical distance.Typically, shelving units such as these are 3-4 feet high.</p>
<p>Let's estimate a conservative distance.For simplicity, we can assume the distance is approximately double the height of the shelves between the grey support beams.Therefore, the distance between the support beams and the shelving units would be about twice the height of a typical shelving unit.</think><answer> The distance would be approximately twice the height of a typical shelving unit </answer> (e)</p>
<p>Figure 1 :
1
Figure 1: (a) The original image, question, and answer of a sample.(b) The flipped image, question, and the answers before and after verification enhancement.</p>
<p>Q:</p>
<p>Please provide the bounding box coordinate of the region this sentence describes: apple on left A: [0.01, 0.29, 0.29, 0.74] Q: Is the yellow lemon in bowl to the left of the hand holding clear bowl from the viewer's perspective?Ans: In fact, the yellow lemon in bowl is either to the right of or directly aligned with the hand holding clear bowl.Q: How wide is the cardboard boxes stacked on shelves?Ans: the cardboard boxes stacked on shelves is 17.38 feet in width Q: Please provide a short description for this region: [0.17, 0.67, 0.34, 0.73] Ans: A black skateboard.Q: What is the common term given to the above collection?\nA.dolls\nB.people\nC.congregation\nD.bears\nAnswer with the option's letter from the given choices directly.Ans: A (a) Bounding box prediction (b) Region Captioning (c) Distance estimation (e) Single or multiple choice (d) Yes/No question with open-ended answers</p>
<p>Figure 2 :
2
Figure 2: A diverse set of open-ended spatial question-answer types.</p>
<p>between the man driving yellow forklift and the man standing next to orange van horizontally.SVQA-R1-3B : <think>To measure the horizontal distance between the two men, I'll need to identify their positions relative to each other.The man driving the yellow forklift is on the left side of the image, while the man standing next to the orange van is on the right side.By counting the number of boxes or other objects that separate them, is more to the right, the colorful fabric rolls on shelf or the blue fabric rolls on shelf?SVQA-R1-3B : <think> The image shows two shelves with fabric rolls.The colorful fabric rolls are on the left side of the image, while the blue fabric rolls are on the right side.Therefore, the blue fabric rolls are more to the right.\n</think>\n<answer>\nTheblue fabric rolls are more to the right.\n</answer>Question: What kind of artwork is framed on the left side of the screen on the wall? A. Impressionism B. american pop C. abstract D. Contemporary Answer with the option's letter from the given choices directly.</p>
<p>Figure 3 :
3
Figure 3: Visualization of different open-ended spatial question-answer types.</p>
<p>AppendixA</p>
<p>Examples of Horizontal Image FlippingA.1 Left-Right Spatial Reasoning This category highlights cases where the model's answer depends on reasoning about horizontal spatial relationships, such as determining whether an object is on the left or right.Image flipping directly alters this spatial context, and an accurate model should adapt its response accordingly.These examples in Figure 4 illustrate the model's ability to reverse its reasoning when the visual orientation changes.Q: Who is positioned more to the left, the warehouse with red and blue shelves or the man holding white box?Ans: From the viewer's perspective, warehouse with red and blue shelves appears more on the left side.Q: Is the grey blanket on sofa to the right of the brown wicker chair with pillow from the viewer's perspective?Ans: Yes, the grey blanket on sofa is to the right of the brown wicker chair with pillow.Q: Who is positioned more to the right, the warehouse with red and blue shelves or the man holding white box?Ans: From the viewer's perspective, warehouse with red and blue shelves appears more on the right side.Q: Is the grey blanket on sofa to the left of the brown wicker chair with pillow from the viewer's perspective?Ans: Yes, the grey blanket on sofa is to the left of the brown wicker chair with pillow.(a) Original VS.Flipped image and QA (b) Original VS.Flipped image and QA</p>
<p>Figure 4 :
4
Figure 4: Visualization of original and flipped image and QA for left-right spatial reasoning.</p>
<p>Figure 5 :
5
Figure 5: Visualization of original and flipped image and QA for bounding box consistency.</p>
<p>1 <strong>Analyze the question</strong> Step 2 <strong>Identify up to 10 reference scales in the image, ranging from large to small sizes, and list them in the specified format</strong> -A reference scale must be typical in size.-A reference scale can be the dimensions of an object or an object part.-A reference scale must NOT be floor tiles or floor planks.-Formulate the reference scales using the format: """The [choose from front-to-back, sideto-side, left-to-right, diameter, height (top to bottom edge), or mounting height (bottom edge to floor)] of [object or object part] is approximately [dimension estimate]."""Step 3 <strong>Propose a robust step-by-step plan to answer the question by using the reference scales in Step 2</strong> -A robust step-by-step plan performs the estimation in a coarse-to-fine manner.-First, use a reliable and large-sized reference scale as the primary reference for estimation.-Then, gradually use a reliable and smaller-sized reference scale for adjustment.-Repeat until the estimation is precise enough.-When performing visual comparison, be aware of perspective distortion.-Do NOT rely on pixel measurements from the images.Step 4 <strong>Focus on the image and follow the plan in Step 3 to answer the question</strong> Table 9: Example of different prompts.</p>
<p>device is to the left of the screen?SVQA-R1-3B: <think> The problem involves identifying the type of device to the left of the screen.The screen is described as a monitor or computer screen, which is typically a type of display device.1.The device to the left of the screen is a laptop or computer, as it has a keyboard in front of it.2. Laptops often serve as display devices, and their monitors are often referred to as screens.3. Therefore, the device to the left of the screen is a laptop.</think><answer> laptop </answer> Question: Is the empty metal shelving unit smaller than the metal shelving with boxes?SVQA-R1-3B : <think> Comparing the empty metal shelving unit to the one with boxes, it can be observed that the empty shelving unit occupies a fraction of the space, while the shelving unit with boxes occupies a larger portion of the floor space.Therefore, it's evident that the empty metal shelving unit is smaller than the metal shelving with boxes.</think>\n<answer> Yes </answer> side of the image are the people on?SVQA-R1-3B : <think> The people are clearly visible in the image and are positioned in the middle ground.They appear to be on horseback, walking up a mountain trail.The direction of movement is towards the mountains in the background.</think>\n<answer> They are on the left side of the image.</answer> Question:</p>
<p>Figure 7 :
7
Figure 7: More visualization examples with thinking evidence.</p>
<p>Table 2 :
2
[17]test results on Q-Spatial++[17].*indicatesmodels not using the thinking prompt.+meansthat the results are copied from Q-Spatial++[17].
MetricsIn Range</p>
<p>Table 3 :
3
[6]]are the SFT and GRPO on Q-Spatial++[17].*indicates the initialization is from UCSC-VLAA[6], while others are from the Base.</p>
<p>Table 4 :
4
Compare the results on the open-ended spatial VQA task on Vqasynth_Spacellava
MetricsSpatial-VQA TypeSourceMethodLLM (%)↑BLEU-1 (%)↑BLEU-2 (%)↑sBERT↑Bbox mIoU(%)↑ Acc.@0.75(%)↑ Acc.(%)↑ Acc.(%)↑ Yes/No DistanceVqasynth_SpacellavaClosedGPT-4o Gemini-1.5-Flash [2]65.92 31.5265.02 34.4858.50 27.3789.23 63.2245.02 34.9026.50 20.5060.05 49.7722.51 14.54QWen2.5VL-3B  <em>45.5433.3331.3168.0410.005.8039.685.91QWen2.5VL-3B45.3310.007.9666.1741.0119.4539.8911.29SpaceThinker-QWen2.5VL-3B 39.390.000.0016.1437.0011.6029.3710.75OpenInternVL-2.5 [7]46.6910.308.2067.4942.2420.2341.0910.95UCSC-VLAA [6]46.110.00.020.9042.2921.8444.2113.44SVQA-R160.7763.3356.4387.8443.4924.5758.3123.66OpenSpacesClosedGPT-4o Gemini-1.5-Flash [2]34.54 20.5658.70 45.6344.65 30.1289.54 75.41----41.56 29.1015.88 6.71QWen2.5VL-3B  </em>30.6133.3333.3368.05--21.372.18QWen2.5VL-3B32.4949.7132.3587.67--29.923.83SpaceThinker-QWen2.5VL-3B 16.291.031.0031.29--17.1010.81OpenInternVL-2.5 [7]28.9550.4134.3781.25--21.774.15UCSC-VLAA [6]18.0354.6837.9487.66--22.168.15SVQA-R130.9755.0041.6885.50--39.3814.44</p>
<p>Table 5 :
5
Ablation on reward function and modules on qunatity dataset Q-Spatial++ and open-ended spatial VQA dataset Vqasynth_Spacellava.
Question:Please provide the bounding boxcoordinate of the region this sentencedescribes: red and blue stripes aroundborder.</p>
<p>Table 6 :
6
Performance under different weights applied to the reward difference between original and flipped images.
Q-Spatial++Vqasynth_SpacellavaηSuccess Rate (%) ↑Samples Completed (%) ↑sMAPE (%) ↓ llm (%) ↑ BLEU-1 (%) ↑ sBERT ↑052.32100.0073.2459.8253.9481.74158.42100.0068.3660.7763.3387.84251.49100.0077.6661.4763.3487.861047.52100.0080.1657.1762.3485.48Q-Spatial++Vqasynth_SpacellavaModelSuccess Rate (%) ↑Samples Completed (%) ↑sMAPE (%) ↓ llm (%) ↑ BLEU-1 (%) ↑ sBERT ↑Original58.42100.0068.3660.7763.3387.84VLAA53.49100.0076.1061.3556.8383.41</p>
<p>Table 7 :
7
Performance using different base model initializations.</p>
<p>Table 8 :
8
Performance using different prompts for quantity dataset Q-Spatial++.
MetricsIn RangeModelSuccess Rate (%) ↑Samples Completed (%) ↑sMAPE (%) ↓ 50-100 (%) ↑ 100-150 (%) ↑ 150-200 (%) ↑(a)58.42100.0068.3618.8118.8121.78(b)20.98100.00184.645.627.9610.85to numerical precision or structured form, but tends to overlook semantic fidelity, which affectsalignment with LLM-based judgments.
Preprint. Under review.
B Determining the Task TypeIn order to evaluate the model's performance more accurately across different types of visual reasoning, we categorize each sample into one of several task types: bounding box prediction, binary (yes/no) classification, or distance estimation.Note that not all examples could be confidently assigned to a specific task type.We exclude ambiguous or unsupported cases from type-specific evaluation.This classification enables us to apply type-specific evaluation metrics and better understand the model's behavior under each reasoning requirement.In the following subsections, we detail the criteria used to assign a task type to each instance based on its question structure, answer format, or annotated metadata.B.1 Bounding BoxFor bounding box-related questions, we identify them by checking whether the question text contains indicative spatial phrases.Specifically, we use the following keyword list:• BBOX_KEYWORDS: bounding box, box coordinates, coordinates, bbox, where is, x coordinate, y coordinate, draw a box, top left, bottom right, region of.Using this rule-based classification, we identify 293 samples (accounting for 9.4% of the dataset) as bounding box prediction tasks.B.2 Yes/NoWe define alias sets to identify binary (Yes/No) questions and answers, as shown below:• YES_ALIASES: yes, it is, appears to be, looks like, seems like, definitely, likely, indeed.• NO_ALIASES: no, not, doesn't, isn't, unlikely, I don't think, probably not.Based on these expressions, we classify 1,742 samples (accounting for 34.84% of the dataset) as Yes/No questions.Since the model outputs for binary questions are often expressed in natural language rather than strictly as "yes" or "no," we use GPT-4o to assess whether the predicted answer is semantically consistent with the ground truth.The evaluation is performed via a simple prompt-based classification.For each prediction, we provide GPT-4o with the following instruction, and the {pred} and {gt} will be replaced by the specific content:You are an evaluator.Given a Yes/No question, a ground-truth answer, and a predicted answer, determine whether the We further analyze intermediate checkpoints during training to observe the progression of model performance and reasoning quality.Interestingly, we find that performance initially improves with more training steps but begins to decline after a certain point.This suggests that more training does not always lead to better performance, and there may exist an optimal checkpoint that balances learning and overfitting.As shown in Table10, we compare selected checkpoints and observe that the model at step 1000 achieves the best performance among them.We believe that evaluating more fine-grained checkpoints could potentially reveal an even better-performing model.G Inferred words in Chain-of-Thought reasoningTo better understand the nature and quality of the model's reasoning, we conduct a lexical analysis of the generated Chain-of-Thought (CoT) responses.In particular, we examine whether the model produces reasoning-oriented expressions, such as causal connectors, speculative language, and sequential markers, that are commonly associated with multi-step inference.We define a set of indicative CoT-related keywords spanning various reasoning functions (e.g., uncertainty, causality, step-by-step logic).By counting their occurrences in the generated responses, we aim to quantify the model's usage of explicit reasoning cues and gain insight into the linguistic characteristics of its CoT outputs.To quantify the presence of reasoning patterns in model outputs, we define a set of indicative keywords commonly associated with speculative language, causal reasoning, and step-by-step inference:• COT_KEYWORDS: likely, probably, possibly, maybe, might be, could be, seems, appears to, I think, I guess, I'm not sure, because, since, therefore, thus, so, hence, as a result, that means, which implies, accordingly, first, next, then, finally, in the first step, in the second step, after that, subsequently, clearly, obviously, evidently, definitely, in fact, it is important to note, if, suppose, assuming that, in case, let's say, consider that.Based on this keyword list, we observe that the average number of reasoning-related words per response is 0.8, indicating that most responses contain at least one token suggestive of inferred or stepwise reasoning.H More Visualization Examples
Spacethinker-qwen2.5vl-3b. 2024</p>
<p>Spacethinker-qwen2.5vl-3b. 2025</p>
<p>S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, arXiv:2502.139235-vl technical report. 2025arXiv preprint</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. B Chen, Z Xu, S Kirmani, B Ichter, D Sadigh, L Guibas, F Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>H Chen, H Tu, F Wang, H Liu, X Tang, X Du, Y Zhou, C Xie, arXiv:2504.11468Sft or rl? an early investigation into training r1-like reasoning large vision-language models. 2025arXiv preprint</p>
<p>Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. Z Chen, W Wang, Y Cao, Y Liu, Z Gao, E Cui, J Zhu, S Ye, H Tian, Z Liu, arXiv:2412.052712024arXiv preprint</p>
<p>Mm-spatial: Exploring 3d spatial understanding in multimodal llms. E Daxberger, N Wenzel, D Griffiths, H Gang, J Lazarow, G Kohavi, K Kang, M Eichner, Y Yang, A Dehghan, arXiv:2503.131112025arXiv preprint</p>
<p>Dense and aligned captions (dac) promote compositional reasoning in vl models. S Doveh, A Arbelle, S Harary, R Herzig, D Kim, P Cascante-Bonilla, A Alfassy, R Panda, R Giryes, R Feris, Advances in Neural Information Processing Systems. 202336</p>
<p>D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Reinforced attention for few-shot learning and beyond. J Hong, P Fang, W Li, T Zhang, C Simon, M Harandi, L Petersson, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. D A Hudson, C D Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Gpt-4o: The cutting-edge advancement in multimodal llm. R Islam, O M Moushi, Authorea Preprints. 2024</p>
<p>Mdetr-modulated detection for end-to-end multi-modal understanding. A Kamath, M Singh, Y Lecun, G Synnaeve, I Misra, N Carion, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>3d gaussian splatting for real-time radiance field rendering. B Kerbl, G Kopanas, T Leimkühler, G Drettakis, ACM Trans. Graph. 4242023</p>
<p>Tinyllava-video-r1: Enhancing video reasoning in small visionlanguage models via reinforcement learning. Y Li, Y Chen, Y Liu, arXiv:2504.096412024arXiv preprint</p>
<p>Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. Y.-H Liao, R Mahmood, S Fidler, D Acuna, arXiv:2409.097882024arXiv preprint</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Advances in neural information processing systems. 202336</p>
<p>Spatialpin: Enhancing spatial reasoning capabilities of vision-language models through prompting and interacting 3d priors. C Ma, K Lu, T.-Y Cheng, N Trigoni, A Markham, Advances in neural information processing systems. 2024</p>
<p>Nerf: Representing scenes as neural radiance fields for view synthesis. B Mildenhall, P P Srinivasan, M Tancik, J T Barron, R Ramamoorthi, R Ng, Communications of the ACM. 6512021</p>
<p>Gpt-4o technical report. 2024OpenAI</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Improving compositional reasoning of clip via synthetic vision-language negatives. M Patel, N S A Kusumba, S Cheng, C Kim, T Gokhale, C Baral, Advances in neural information processing systems. 202437</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PmLR2021</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. N Reimers, I Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing2019</p>
<p>Attention-privileged reinforcement learning. S Salter, D Rao, M Wulfmeier, R Hadsell, I Posner, Conference on Robot Learning. PMLR2021</p>
<p>Z Shao, P Wang, Q Zhu, R Xu, J Song, X Bi, H Zhang, M Zhang, Y Li, Y Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Probabilistic neural symbolic models for interpretable visual question answering. R Vedantam, K Desai, S Lee, M Rohrbach, D Batra, D Parikh, International Conference on Machine Learning. PMLR2019</p>
<p>Is a picture worth a thousand words? delving into spatial reasoning for vision language models. J Wang, Y Ming, Z Shi, V Vineet, X Wang, S Li, N Joshi, Advances in Neural Information Processing Systems. 202437</p>
<p>Raven: Reasoning with visual commonsense for planning in human-robot interaction. J Wang, H Zhang, L Xie, arXiv:2206.072812022arXiv preprint</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Reason-rft: Reasoning-aware reinforcement fine-tuning for vision-language models. H Wu, B Zhou, Y Wang, arXiv:2503.207522024arXiv preprint</p>
<p>Visual-rft: Visual reinforcement fine-tuning for large visionlanguage models. H Wu, B Zhou, Y Wang, arXiv:2503.017852024arXiv preprint</p>
<p>Video-r1: Reinforcing video reasoning in multimodal large language models. Q Xu, Y Wang, B Zhou, arXiv:2503.217762024arXiv preprint</p>
<p>Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. K Yi, J Wu, C Gan, A Torralba, P Kohli, J Tenenbaum, Advances in neural information processing systems. 201831</p>
<p>Mvimgnet: A large-scale dataset of multi-view images. X Yu, M Xu, Y Zhang, H Liu, C Ye, Y Wu, Z Yan, C Zhu, Z Xiong, T Liang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2023</p>
<p>Videochat-r1: Reinforcing spatio-temporal perception in video multimodal large language models. J Zhang, Y Zhou, Y Li, arXiv:2504.069582024arXiv preprint</p>
<p>Cot-vlm: Chain-of-thought prompting for visual language models. W Zhao, S Zheng, H Zhang, arXiv:2309.047612023arXiv preprint</p>
<p>Vlm-r1: Towards a stable and generalizable r1-style large vision-language model. B Zhou, Y Wang, H Liu, arXiv:2504.076152024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
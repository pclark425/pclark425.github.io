<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1068 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1068</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1068</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-8b2a6808ce5cec406b41a8e77e67570246105bd0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8b2a6808ce5cec406b41a8e77e67570246105bd0" target="_blank">Deep Reinforcement Learning for Swarm Systems</a></p>
                <p><strong>Paper Venue:</strong> Journal of machine learning research</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a new state representation for deep multi-agent RL based on mean embeddings of distributions, where the agents are treated as samples and the empirical mean embedding is used as input for a decentralized policy.</p>
                <p><strong>Paper Abstract:</strong> Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, the observation vector for decentralized decision making is represented by a concatenation of the (local) information an agent gathers about other agents. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions, where we treat the agents as samples and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and neural networks trained end-to-end. We evaluate the representation on two well-known problems from the swarm literature in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents, facilitating the development of complex collective strategies.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1068.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1068.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Swarm-Rendezvous</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Homogeneous Swarm Agents — Rendezvous Task (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated homogeneous unicycle/swarm agents trained with centralized-learning / decentralized-execution TRPO using mean feature embeddings (histogram, RBF, or learned neural-network features) to solve a rendezvous (minimize inter-agent distances) task under varying observability and dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Homogeneous swarm agent (unicycle / single- or double-integrator)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated robotic agents (unicycles) controlled by policies learned with Trust Region Policy Optimization (TRPO) using parameter sharing; policies ingest permutation- and cardinality-invariant mean feature embeddings of neighbor observations, where the feature map phi is either histogram, RBF, or a learned neural network.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robotic/virtual)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Rendezvous (meeting / consensus) environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Bounded 2D continuous workspace (closed or periodic toroidal) populated by N homogeneous agents; tasks use either global observability (fully connected interaction graph) or local observability via Delta-disk proximity graphs (dynamic neighborhoods); agent dynamics vary (single-integrator or double-integrator/unicycle); observation sets vary from basic (distance, bearing) to extended (relative orientations, relative velocities) and comm (neighborhood sizes). Complexity arises from dynamics (single vs double integrator), number of agents, partial observability, communication limits (d_c), and continuous-state multi-agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents N (e.g., 5,10,20,50,100), agent dynamics (single vs double integrator / unicycle), observability type (global vs local with cut-off d_c), interaction graph dynamics (static vs dynamic Δ-disk), dimensionality of observation feature set (basic vs extended vs comm), and world boundary type (closed vs periodic).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies from medium to high depending on configuration (examples: 20 agents with double-integrator + local observability = high complexity; 20 agents with single-integrator + global observability = medium complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation is measured by changing N (e.g., training with 20 agents and testing with 100 or 10), varying observability (global vs local), varying observation/feature sets (basic, extended, comm), and world type (closed vs periodic); also stochastic initializations across episodes (evaluations over many episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high (experiments include changes in agent count from 5 up to 100, change of observability and feature sets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average episode return (\u0305G), mean inter-agent distance over time, convergence speed (time steps to reach a distance), and steady-state mean distance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Neural-network mean-embedding (NN+) achieves ~10% higher average return than NN and RBF embeddings on the rendezvous double-integrator / global case; steady-state mean inter-agent distance: NN+ ~4.0×10^-2 versus NN/RBF ~8.0×10^-2 (units: normalized distance in environment) after ~200 time steps; NN+ reaches comparable mean-distance ~20--30 time steps earlier (~25% faster). Exact absolute reward values not provided numerically in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper explicitly reports trade-offs: (1) higher environment complexity (more complex dynamics, local observability, dynamic neighborhoods) increases learning difficulty and variance in training; (2) variation in environment size (changing N) is handled better by permutation- and cardinality-invariant mean embeddings; (3) histogram/RBF embeddings suffer from the curse of dimensionality as feature dimensionality or observation complexity increases, while learned neural-network embeddings scale linearly with input features and thus better manage higher complexity and variation. Communication (comm set) mitigates difficulty under local observability, improving performance.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Centralized-learning / decentralized-execution using parameter-shared TRPO; training uses many trajectories sampled across agents with subsampling (10 MPI workers, 2048 timesteps each, subsample data of 8 agents => 163,840 samples per TRPO iteration). No curriculum learning or domain randomization described.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Policies learned with one agent count were executed with different agent counts: e.g., policies learned with 20 agents were evaluated with 100 agents and generalized successfully, with neural-network mean embeddings (NN+) remaining quickest to reduce inter-agent distances. Under fewer agents than training (e.g., trained on 20, executed on 10), performance decreased but communication (comm set) provided robustness in local-observability cases.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported sampling scheme: 163,840 samples per TRPO iteration (10 workers * 2048 timesteps * subsample of 8 agents). Empirically, NN embeddings converged faster than histogram/RBF and concatenation baselines (learning curves reported), though exact number of iterations to convergence is not numerically specified.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Permutation- and cardinality-invariant mean feature embeddings (especially learned neural-net features) provide a compact, scalable representation for swarm agents, enabling faster learning, better steady-state performance, and superior generalization to different swarm sizes and more complex observations than histogram/RBF or concatenation baselines; histogram/RBF suffer from curse-of-dimensionality when feature dimensionality increases; local communication (e.g., sharing neighbor counts or shortest-path-to-evader) increases performance under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Swarm Systems', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1068.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1068.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Swarm-Pursuit-Single</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Homogeneous Swarm Agents — Pursuit Evasion with Single Evader (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated swarm pursuers controlled by TRPO policies using mean feature embeddings attempt to capture a faster evader in continuous 2D environments under different observability regimes and with varying numbers of pursuers; performance measured as capture probability over time and time-to-capture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Homogeneous pursuer agents (single-integrator unicycle approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated pursuer agents trained with centralized-learning / decentralized-execution TRPO using permutation-invariant mean embeddings (histogram, RBF, or learned NN features); agents act cooperatively under a shared global reward that depends on proximity to the evader (closest-pursuer distance) or capture events.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robotic/virtual)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Pursuit evasion (single evader) environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous 2D bounded (changed to periodic to prevent corner traps) environment with one evader and multiple pursuers; evader policy is fixed (based on Voronoi-region-maximization) and set to have max velocity twice that of pursuers to increase task difficulty; observability variants: global (all agents see each other and possibly the evader), local (proximity-based neighborhoods and partial evader observation radius d_o). Complexity arises from faster evader, coordination needed to surround and reduce Voronoi regions, partial observability and dynamic interaction graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of pursuers N (e.g., 5,10,20,50), evader speed (set to 2x pursuer max speed), observability (global vs local), evader observation radius d_o under local observability, dynamic neighborhood connectivity, and number of evaders (for later experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high (task made deliberately challenging by faster evader and partial observability); higher as N decreases or when local observability and limited evader sensing apply.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation tested by changing N (5,10,20,50), observability (global vs local), observation sets (basic, extended, comm), and world type (periodic vs closed); also multiple random initializations and episode-respawn variants for multiple-evader experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (experiments systematically vary number of agents and observability; results shown for multiple N values).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Probability of capture over time (fraction of episodes where evader is caught by time t), average return (\u0305G), and time-to-capture statistics aggregated across 1000 episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitatively, neural-network mean embeddings (extended set) yield the quickest captures and highest capture probabilities across tested agent counts; increasing N improves capture probability for all methods; when trained with 10 agents and executed with 5 agents, all methods struggled; exact numeric capture-probability curves are plotted in the paper but specific numeric percentages at fixed times are not provided in the text body.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports that increased variation in agent count (more agents) improves performance (quicker capture), while decreased agent count reduces ability to trap faster evader; partial observability (local case) increases task difficulty and variance in learning outcomes; richer observation sets (extended, comm) improve performance under higher complexity/variation; neural-network embeddings better exploit additional information than histogram/RBF, so they handle increased complexity due to faster evader and partial observability more effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Centralized-learning / decentralized-execution TRPO with parameter-sharing; fixed evader policy (not learned); training data subsampled as described for TRPO (163,840 samples per iteration).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Policies learned with 10 agents were executed with 5, 20, and 50 agents: increasing pursuer count led to faster captures; policies generalized across agent counts to varying degrees (NN embeddings generalized best), but with fewer agents than training (5 vs 10) capture often failed due to gaps in the surround strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Same TRPO sampling scheme (163,840 samples per iteration); NN embeddings required fewer samples to find good solutions compared to histogram/RBF and concatenation baselines according to learning curves.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Neural-network mean embeddings (especially with extended observation sets) produce superior pursuit strategies enabling quicker capture of a faster evader, and generalize better across different numbers of pursuers; partial observability and faster evader speed increase task difficulty and variance, which is mitigated by local communication and richer learned embeddings; histogram/RBF representations underperform as observation complexity increases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Swarm Systems', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1068.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1068.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Swarm-Pursuit-Multi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Homogeneous Swarm Agents — Pursuit Evasion with Multiple Evaders (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large-scale simulation with many pursuers (50) and multiple evaders (5) using two separate mean embeddings (one for neighbors, one for evaders); policies trained with TRPO on extended observations to maximize number of evaders caught per unit time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Homogeneous pursuer agents (simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated pursuers controlled via TRPO policies with neural-network mean embeddings processing variable-size sets of neighbor and evader observations (two separate mean embeddings) to handle multiple evaders and many pursuers.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robotic/virtual)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Pursuit evasion with multiple evaders (50 pursuers, 5 evaders)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Global-observability periodic environment with many pursuers and multiple evaders; caught evaders are respawned (no episode termination) making the reward equal to number of captures per time step; agents must process variable-sized sets for both other agents and multiple evaders via separate mean embeddings; complexity arises from many-agent coordination, multiple targets, and continuous respawning.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Large N (50 pursuers), number of evaders (5), variable-size observation sets requiring separate embeddings, and respawn dynamics (ongoing task rather than single-termination episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation via multiple evaders (5), large number of agents, and respawned evaders introducing ongoing stochastic variation in target positions over episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average episode return (directly relates to number of evaders caught during an episode), learning curves over training iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Neural-network mean embeddings perform significantly better than RBF embeddings and concatenation (concatenation fails due to high-dimensionality); exact numeric capture rates or returns are shown in plotted learning curves but explicit scalar values are not given in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper demonstrates that as problem complexity increases (many agents + multiple evaders + respawn dynamics), learned neural-network mean embeddings scale to handle variable-sized inputs and achieve higher capture rates, whereas RBF/histogram and concatenation methods break down due to dimensionality explosion; thus increasing complexity and variation favors learned invariant embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Centralized-learning / decentralized-execution TRPO with parameter sharing; two separate mean embeddings for neighbors and evaders fed into policy network.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Not explicitly cross-executed across different N for multi-evader experiment in text, but results show that NN embeddings handle large N and multiple targets where other methods fail; concatenation approach becomes infeasible at this scale.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Uses same TRPO sampling strategy; convergence achievable but exact sample counts to convergence not numerically specified; learning curves show NN embedding outperforms others within plotted training span.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learned neural-network mean embeddings can represent variable-size sets (both neighbors and multiple targets) compactly and enable scalable coordination in high-complexity/high-variation multi-evader pursuit tasks; kernel/histogram/RBF approaches and naive concatenation do not scale to these settings due to curse of dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Swarm Systems', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Trust Region Policy Optimization <em>(Rating: 2)</em></li>
                <li>Mean Field Multi-Agent Reinforcement Learning <em>(Rating: 1)</em></li>
                <li>Human-level control through deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Embedding distributions in reproducing kernel Hilbert spaces <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning for many-agent systems using image-like state representations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1068",
    "paper_id": "paper-8b2a6808ce5cec406b41a8e77e67570246105bd0",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Swarm-Rendezvous",
            "name_full": "Homogeneous Swarm Agents — Rendezvous Task (this paper)",
            "brief_description": "Simulated homogeneous unicycle/swarm agents trained with centralized-learning / decentralized-execution TRPO using mean feature embeddings (histogram, RBF, or learned neural-network features) to solve a rendezvous (minimize inter-agent distances) task under varying observability and dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Homogeneous swarm agent (unicycle / single- or double-integrator)",
            "agent_description": "Simulated robotic agents (unicycles) controlled by policies learned with Trust Region Policy Optimization (TRPO) using parameter sharing; policies ingest permutation- and cardinality-invariant mean feature embeddings of neighbor observations, where the feature map phi is either histogram, RBF, or a learned neural network.",
            "agent_type": "simulated agent (robotic/virtual)",
            "environment_name": "Rendezvous (meeting / consensus) environment",
            "environment_description": "Bounded 2D continuous workspace (closed or periodic toroidal) populated by N homogeneous agents; tasks use either global observability (fully connected interaction graph) or local observability via Delta-disk proximity graphs (dynamic neighborhoods); agent dynamics vary (single-integrator or double-integrator/unicycle); observation sets vary from basic (distance, bearing) to extended (relative orientations, relative velocities) and comm (neighborhood sizes). Complexity arises from dynamics (single vs double integrator), number of agents, partial observability, communication limits (d_c), and continuous-state multi-agent interactions.",
            "complexity_measure": "Number of agents N (e.g., 5,10,20,50,100), agent dynamics (single vs double integrator / unicycle), observability type (global vs local with cut-off d_c), interaction graph dynamics (static vs dynamic Δ-disk), dimensionality of observation feature set (basic vs extended vs comm), and world boundary type (closed vs periodic).",
            "complexity_level": "varies from medium to high depending on configuration (examples: 20 agents with double-integrator + local observability = high complexity; 20 agents with single-integrator + global observability = medium complexity).",
            "variation_measure": "Variation is measured by changing N (e.g., training with 20 agents and testing with 100 or 10), varying observability (global vs local), varying observation/feature sets (basic, extended, comm), and world type (closed vs periodic); also stochastic initializations across episodes (evaluations over many episodes).",
            "variation_level": "medium-to-high (experiments include changes in agent count from 5 up to 100, change of observability and feature sets).",
            "performance_metric": "Average episode return (\\u0305G), mean inter-agent distance over time, convergence speed (time steps to reach a distance), and steady-state mean distance.",
            "performance_value": "Neural-network mean-embedding (NN+) achieves ~10% higher average return than NN and RBF embeddings on the rendezvous double-integrator / global case; steady-state mean inter-agent distance: NN+ ~4.0×10^-2 versus NN/RBF ~8.0×10^-2 (units: normalized distance in environment) after ~200 time steps; NN+ reaches comparable mean-distance ~20--30 time steps earlier (~25% faster). Exact absolute reward values not provided numerically in text.",
            "complexity_variation_relationship": "The paper explicitly reports trade-offs: (1) higher environment complexity (more complex dynamics, local observability, dynamic neighborhoods) increases learning difficulty and variance in training; (2) variation in environment size (changing N) is handled better by permutation- and cardinality-invariant mean embeddings; (3) histogram/RBF embeddings suffer from the curse of dimensionality as feature dimensionality or observation complexity increases, while learned neural-network embeddings scale linearly with input features and thus better manage higher complexity and variation. Communication (comm set) mitigates difficulty under local observability, improving performance.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Centralized-learning / decentralized-execution using parameter-shared TRPO; training uses many trajectories sampled across agents with subsampling (10 MPI workers, 2048 timesteps each, subsample data of 8 agents =&gt; 163,840 samples per TRPO iteration). No curriculum learning or domain randomization described.",
            "generalization_tested": true,
            "generalization_results": "Policies learned with one agent count were executed with different agent counts: e.g., policies learned with 20 agents were evaluated with 100 agents and generalized successfully, with neural-network mean embeddings (NN+) remaining quickest to reduce inter-agent distances. Under fewer agents than training (e.g., trained on 20, executed on 10), performance decreased but communication (comm set) provided robustness in local-observability cases.",
            "sample_efficiency": "Reported sampling scheme: 163,840 samples per TRPO iteration (10 workers * 2048 timesteps * subsample of 8 agents). Empirically, NN embeddings converged faster than histogram/RBF and concatenation baselines (learning curves reported), though exact number of iterations to convergence is not numerically specified.",
            "key_findings": "Permutation- and cardinality-invariant mean feature embeddings (especially learned neural-net features) provide a compact, scalable representation for swarm agents, enabling faster learning, better steady-state performance, and superior generalization to different swarm sizes and more complex observations than histogram/RBF or concatenation baselines; histogram/RBF suffer from curse-of-dimensionality when feature dimensionality increases; local communication (e.g., sharing neighbor counts or shortest-path-to-evader) increases performance under partial observability.",
            "uuid": "e1068.0",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Swarm Systems",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Swarm-Pursuit-Single",
            "name_full": "Homogeneous Swarm Agents — Pursuit Evasion with Single Evader (this paper)",
            "brief_description": "Simulated swarm pursuers controlled by TRPO policies using mean feature embeddings attempt to capture a faster evader in continuous 2D environments under different observability regimes and with varying numbers of pursuers; performance measured as capture probability over time and time-to-capture.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Homogeneous pursuer agents (single-integrator unicycle approximation)",
            "agent_description": "Simulated pursuer agents trained with centralized-learning / decentralized-execution TRPO using permutation-invariant mean embeddings (histogram, RBF, or learned NN features); agents act cooperatively under a shared global reward that depends on proximity to the evader (closest-pursuer distance) or capture events.",
            "agent_type": "simulated agent (robotic/virtual)",
            "environment_name": "Pursuit evasion (single evader) environment",
            "environment_description": "Continuous 2D bounded (changed to periodic to prevent corner traps) environment with one evader and multiple pursuers; evader policy is fixed (based on Voronoi-region-maximization) and set to have max velocity twice that of pursuers to increase task difficulty; observability variants: global (all agents see each other and possibly the evader), local (proximity-based neighborhoods and partial evader observation radius d_o). Complexity arises from faster evader, coordination needed to surround and reduce Voronoi regions, partial observability and dynamic interaction graphs.",
            "complexity_measure": "Number of pursuers N (e.g., 5,10,20,50), evader speed (set to 2x pursuer max speed), observability (global vs local), evader observation radius d_o under local observability, dynamic neighborhood connectivity, and number of evaders (for later experiments).",
            "complexity_level": "medium-to-high (task made deliberately challenging by faster evader and partial observability); higher as N decreases or when local observability and limited evader sensing apply.",
            "variation_measure": "Variation tested by changing N (5,10,20,50), observability (global vs local), observation sets (basic, extended, comm), and world type (periodic vs closed); also multiple random initializations and episode-respawn variants for multiple-evader experiments.",
            "variation_level": "medium (experiments systematically vary number of agents and observability; results shown for multiple N values).",
            "performance_metric": "Probability of capture over time (fraction of episodes where evader is caught by time t), average return (\\u0305G), and time-to-capture statistics aggregated across 1000 episodes.",
            "performance_value": "Qualitatively, neural-network mean embeddings (extended set) yield the quickest captures and highest capture probabilities across tested agent counts; increasing N improves capture probability for all methods; when trained with 10 agents and executed with 5 agents, all methods struggled; exact numeric capture-probability curves are plotted in the paper but specific numeric percentages at fixed times are not provided in the text body.",
            "complexity_variation_relationship": "Paper reports that increased variation in agent count (more agents) improves performance (quicker capture), while decreased agent count reduces ability to trap faster evader; partial observability (local case) increases task difficulty and variance in learning outcomes; richer observation sets (extended, comm) improve performance under higher complexity/variation; neural-network embeddings better exploit additional information than histogram/RBF, so they handle increased complexity due to faster evader and partial observability more effectively.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Centralized-learning / decentralized-execution TRPO with parameter-sharing; fixed evader policy (not learned); training data subsampled as described for TRPO (163,840 samples per iteration).",
            "generalization_tested": true,
            "generalization_results": "Policies learned with 10 agents were executed with 5, 20, and 50 agents: increasing pursuer count led to faster captures; policies generalized across agent counts to varying degrees (NN embeddings generalized best), but with fewer agents than training (5 vs 10) capture often failed due to gaps in the surround strategy.",
            "sample_efficiency": "Same TRPO sampling scheme (163,840 samples per iteration); NN embeddings required fewer samples to find good solutions compared to histogram/RBF and concatenation baselines according to learning curves.",
            "key_findings": "Neural-network mean embeddings (especially with extended observation sets) produce superior pursuit strategies enabling quicker capture of a faster evader, and generalize better across different numbers of pursuers; partial observability and faster evader speed increase task difficulty and variance, which is mitigated by local communication and richer learned embeddings; histogram/RBF representations underperform as observation complexity increases.",
            "uuid": "e1068.1",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Swarm Systems",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Swarm-Pursuit-Multi",
            "name_full": "Homogeneous Swarm Agents — Pursuit Evasion with Multiple Evaders (this paper)",
            "brief_description": "Large-scale simulation with many pursuers (50) and multiple evaders (5) using two separate mean embeddings (one for neighbors, one for evaders); policies trained with TRPO on extended observations to maximize number of evaders caught per unit time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Homogeneous pursuer agents (simulated)",
            "agent_description": "Simulated pursuers controlled via TRPO policies with neural-network mean embeddings processing variable-size sets of neighbor and evader observations (two separate mean embeddings) to handle multiple evaders and many pursuers.",
            "agent_type": "simulated agent (robotic/virtual)",
            "environment_name": "Pursuit evasion with multiple evaders (50 pursuers, 5 evaders)",
            "environment_description": "Global-observability periodic environment with many pursuers and multiple evaders; caught evaders are respawned (no episode termination) making the reward equal to number of captures per time step; agents must process variable-sized sets for both other agents and multiple evaders via separate mean embeddings; complexity arises from many-agent coordination, multiple targets, and continuous respawning.",
            "complexity_measure": "Large N (50 pursuers), number of evaders (5), variable-size observation sets requiring separate embeddings, and respawn dynamics (ongoing task rather than single-termination episodes).",
            "complexity_level": "high",
            "variation_measure": "Variation via multiple evaders (5), large number of agents, and respawned evaders introducing ongoing stochastic variation in target positions over episodes.",
            "variation_level": "high",
            "performance_metric": "Average episode return (directly relates to number of evaders caught during an episode), learning curves over training iterations.",
            "performance_value": "Neural-network mean embeddings perform significantly better than RBF embeddings and concatenation (concatenation fails due to high-dimensionality); exact numeric capture rates or returns are shown in plotted learning curves but explicit scalar values are not given in the text.",
            "complexity_variation_relationship": "The paper demonstrates that as problem complexity increases (many agents + multiple evaders + respawn dynamics), learned neural-network mean embeddings scale to handle variable-sized inputs and achieve higher capture rates, whereas RBF/histogram and concatenation methods break down due to dimensionality explosion; thus increasing complexity and variation favors learned invariant embeddings.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Centralized-learning / decentralized-execution TRPO with parameter sharing; two separate mean embeddings for neighbors and evaders fed into policy network.",
            "generalization_tested": true,
            "generalization_results": "Not explicitly cross-executed across different N for multi-evader experiment in text, but results show that NN embeddings handle large N and multiple targets where other methods fail; concatenation approach becomes infeasible at this scale.",
            "sample_efficiency": "Uses same TRPO sampling strategy; convergence achievable but exact sample counts to convergence not numerically specified; learning curves show NN embedding outperforms others within plotted training span.",
            "key_findings": "Learned neural-network mean embeddings can represent variable-size sets (both neighbors and multiple targets) compactly and enable scalable coordination in high-complexity/high-variation multi-evader pursuit tasks; kernel/histogram/RBF approaches and naive concatenation do not scale to these settings due to curse of dimensionality.",
            "uuid": "e1068.2",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Swarm Systems",
                "publication_date_yy_mm": "2018-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Trust Region Policy Optimization",
            "rating": 2
        },
        {
            "paper_title": "Mean Field Multi-Agent Reinforcement Learning",
            "rating": 1
        },
        {
            "paper_title": "Human-level control through deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Embedding distributions in reproducing kernel Hilbert spaces",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning for many-agent systems using image-like state representations",
            "rating": 1
        }
    ],
    "cost": 0.015481249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deep Reinforcement Learning for Swarm Systems</h1>
<p>Maximilian Hüttenrauch<br>L-CAS<br>University of Lincoln<br>LN6 7TS Lincoln, UK</p>
<p>Adrian Šošić
Bioinspired Communication Systems
Technische Universität Darmstadt
64283 Darmstadt, Germany
Gerhard Neumann
L-CAS
University of Lincoln
LN6 7TS Lincoln, UK</p>
<p>Editor: George Konidaris</p>
<p>MHUETTENRAUCH@LINCOLN.AC.UK</p>
<p>ADRIAN.SOSIC@BCS.TU-DARMSTADT.DE</p>
<p>GNEUMANN@LINCOLN.AC.UK</p>
<h4>Abstract</h4>
<p>Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, the observation vector for decentralized decision making is represented by a concatenation of the (local) information an agent gathers about other agents. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions, where we treat the agents as samples and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and neural networks trained end-to-end. We evaluate the representation on two well-known problems from the swarm literature - rendezvous and pursuit evasion - in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents, facilitating the development of complex collective strategies.</p>
<p>Keywords: deep reinforcement learning, swarm systems, mean embeddings, neural networks, multi-agent systems</p>
<h2>1. Introduction</h2>
<p>In swarm systems, many identical agents interact with each other to achieve a common goal. Typically, each agent in a swarm has limited capabilities in terms of sensing and manipulation so that the considered tasks need to be solved collectively by multiple agents.</p>
<p>A promising application where intelligent swarm systems take a prominent role is swarm robotics (Bayındır, 2016). Robot swarms are formed by a large number of cheap and easy</p>
<p>to manufacture robots that can be useful in a variety of situations and tasks, such as search and rescue missions or exploration scenarios. A swarm of robots is inherently redundant towards loss of individual robots since usually none of the robots plays a specific role in the execution of the task. Because of this property, swarm-based missions are often favorable over single-robot missions (or, let alone, human missions) in hazardous environments. Behavior of natural swarms, such as foraging, formation control, collective manipulation, or the localization of a common 'food' source can be adapted to aid in these missions (Bayındır, 2016). Another field of application is routing in wireless sensor networks (Saleem et al., 2011) since each sensor in the network can be treated as an agent in a swarm.</p>
<p>A common method to obtain control strategies for swarm systems is to apply opti-mization-based approaches using a model of the agents or a graph abstraction of the swarm (Lin et al., 2004; Jadbabaie et al., 2003). Optimization-based approaches allow to compute optimal control policies for tasks that can be well modeled, such as rendezvous or consensus problems (Lin et al., 2007) and formation control (Ranjbar-Sahraei et al., 2012), or to learn pursuit strategies to capture an evader (Zhou et al., 2016). Yet, these approaches typically use simplified models of the agents / the task and often rely on unrealistic assumptions, such as operating in a connected graph (Dimarogonas and Kyriakopoulos, 2007) or having full observability of the system state (Zhou et al., 2016). Rule-based approaches use heuristics inspired by natural swarm systems, such as ants or bees (Handl and Meyer, 2007). Yet, while the resulting heuristics are often simple and can lead to complex swarm behavior, the obtained rules are difficult to adapt, even if the underlying task changes only slightly.</p>
<p>Recently, deep reinforcement learning (RL) strategies have become popular to solve multi-agent coordination problems. In RL, tasks are specified indirectly through a cost function, which is typically easier than defining a model of the task directly or a finding a heuristic for the controller. Having defined a cost function, the RL algorithm aims to find a policy that minimizes the expected cost. Applying deep reinforcement learning within the swarm setting, however, is challenging due to the large number of agents that need to be considered. Compared to single-agent learning, where the agent is confronted only with observations about its own state, each agent in a swarm can make observations of several other agents populating the environment and thus needs to process an entire set of information that is potentially varying in size. Accordingly, two main challenges can be identified in the swarm setting:</p>
<ol>
<li>High state and observation dimensionality, caused by large system sizes.</li>
<li>Changing size of the available information set, either due to addition or removal of agents, or because the number of observed neighbors changes over time.</li>
</ol>
<p>Most current multi-agent deep reinforcement learning methods either concatenate the information received from different agents (Lowe et al., 2017) or encode it in a multi-channel image, where the image channels contain different features based on a local view of an agent (Sunehag et al., 2017; Zheng et al., 2017). However, both types of methods bare major drawbacks. Since neural network policies assume a fixed input dimensionality, a concatenation of observations is unsuitable in the case changing agent numbers. Furthermore, a concatenation disregards the inherent permutation invariance of identical agents in a swarm system and scales poorly to large system sizes. Top-down image based representations alleviate the</p>
<p>issue of permutation invariance, however, the information obtained from neighboring agents is of mostly spatial nature. While additional information can be captured by adding more image channels, the dimensionality of the representation increases linearly with each feature. Furthermore, the discretization into pixels has limited accuracy due to quantization errors.</p>
<p>In this paper, we exploit the homogeneity of swarm systems and treat the state information perceived from neighboring agents as samples of a random variable. Based on this model, we then use mean feature embeddings (MFE) (Smola et al., 2007) to encode the current distribution of the agents. Each agent gets a local view of this distribution, where the information obtained from the neighbors is encoded in the mean embedding. Due to the sample-based view of the collected state information, we achieve a permutation invariant representation that is furthermore invariant to the number of agents in the swarm / the number of perceived neighbors.</p>
<p>Mean feature embeddings have so far been used mainly for kernel-based feature representations (Gretton et al., 2012), but they can be also applied to histograms or radial basis function (RBF) networks. The resulting models are closely related to the "invariant model" formulated by Zaheer et al. (2017). However, compared to the summation approach described in their paper, the averaging of feature activations proposed in our approach yields the desired invariance with respect to the observed agent number mentioned above. To the best of our knowledge, we are the first to use mean embeddings inside a deep reinforcement learning framework for swarm systems where both the feature space of the mean embedding as well as the policy are learned end-to-end.</p>
<p>We test our state representation on various rendezvous and pursuit evasion problems using Trust Region Policy Optimization (TRPO) (Schulman et al., 2015) as the underlying deep RL algorithm. In the rendezvous problem, the agents need to find a collective strategy that allows them to meet at some arbitrary location. In the pursuit evasion domain, a group of agents collectively tries to capture one or multiple evaders.</p>
<p>Policies are learned in a centralized-learning / decentralized-execution fashion fashion, meaning that during learning data from all agents is collected centrally and used to optimize the parameters as if there was only one agent. Nonetheless, each agent only has access to its own perception of the global system state to generate actions from the policy function. We compare our representation to several deep RL baselines as well as to optimization-based solutions, if available. Herein, we perform our experiments both in settings with global observability (i.e., all agents are neighbors) and in settings with local observability (i.e., agents are only locally connected). In the latter setting, we also evaluate different communication protocols (Hüttenrauch et al., 2018) that allow the agents to transmit additional information about their local graph structure. For example, an agent might transmit the number of neighbors within its current neighborhood. Previously, such additional information could not be encoded efficiently due to the poor scalability of the histogram-based approaches.</p>
<p>Our results show that agents using our representation can learn faster and obtain policies of higher quality, suggesting that the representation as mean embedding is an efficient encoding of the global state configuration for swarm-based systems. Moreover, mean embeddings are simple to implement inside existing neural network architectures and can be</p>
<p>applied to any deep RL algorithm, which makes the approach applicable in a wide variety of scenarios. The source code to reproduce our results can be found online. ${ }^{1}$</p>
<h1>2. Related Work</h1>
<p>The main contribution of this work lies in the development of a compact representation of state information in swarm systems, which can easily be used within deep multi-agent reinforcement learning (MARL) settings that contain homogeneous agent groups. In fact, our work is mostly orthogonal to other research in the field of MARL and the presented ideas can be incorporated into most existing approaches. To provide an overview, we begin with a brief survey of algorithms used in (deep) MARL, we revisit the basics of mean embedding theory, and we summarize some classic approaches to swarm control for the rendezvous and pursuit evasion task.</p>
<h3>2.1 Deep RL</h3>
<p>Recently, there has been increasing interest in deep reinforcement learning for swarms and multi-agent systems in general. For example, Zheng et al. (2017) provide a many-agent reinforcement learning platform based on a multi-channel image state representation, which uses Deep Q-Networks (DQN) (Mnih et al., 2015) to learn decentralized control strategies in large grid worlds with discrete actions. Gupta et al. (2017) show a comparison of centralized, concurrent and parameter sharing approaches to cooperative deep MARL, using TRPO (Schulman et al., 2015), DDPG (Lillicrap et al., 2015) and DQN. They evaluate each method on three tasks, one of which is a pursuit task in a grid world using bitmap-like images as state representation. A variant of DDPG for multiple agents in Markov games using a centralized action-value function is provided by Lowe et al. (2017). The authors evaluate the method on tasks like cooperative communication, navigation and others. The downside of a centralized action-value function is that the input space grows linearly with the number of agents, and hence, their approach scales poorly to large system sizes. A more scalable approach is presented by Yang et al. (2018). Employing mean field theory, the interactions within the population of agents are approximated by the interaction of a single agent with the average effect from the overall population, which has the effect that the action-value function input space stays constant. Experiments are conducted on a Gaussian squeeze problem, an Ising model, and a mixed cooperative-competitive battle game. Yet, the paper does not address the state representation problem for swarm systems.</p>
<p>Omidshafiei et al. (2017) investigate hysteretic Q-learning (Matignon et al., 2007) and distillation (Rusu et al., 2015). They use deep recurrent Q-networks (Hausknecht and Stone, 2015) to solve single and multi-task Dec-POMDP problems. Following this work, Palmer et al. (2017) add leniency (Panait et al., 2006) to the hysteretic approach to prevent "relative overgeneralization" of agents. The approach is evaluated on a coordinated multi-agent object transportation problem in a grid world with stochastic rewards.</p>
<p>Sunehag et al. (2017) tackle the "lazy agent" problem in cooperative MARL with a single team reward by training each agent with a learned additive decomposition of a value function based on the team reward. Experiments show an increase in performance on cooperative</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>two-player games in a grid world. Rashid et al. (2018) further develop the idea with the insight that a full factorization of the value function is not necessary. Instead, they introduce a monotonicity constraint on the relationship between the global value function and each local value function. Results are presented on the StarCraft micro management domain.</p>
<p>Finally, Grover et al. (2018) show a framework to model agent behavior as a representation learning problem. They learn an encoder-decoder embedding of agent policies via imitation learning based on interactions and evaluate it on a cooperative particle world (Mordatch and Abbeel, 2018) and a competitive two-agent robo sumo environment (AlShedivat et al., 2018). The design of the policy function in the approach of Mordatch and Abbeel (2018) is similar to ours but the model uses a softmax pooling layer. However, instead of applying (model-free) reinforcement learning to optimize the parameters of the policy function, they build an end-to-end differentiable model of all agent and environment state dynamics and calculate the gradient of the return with respect to the parameters via backpropagation.</p>
<p>An application related to our approach can be found in the work by Gebhardt et al. (2018), where the authors use mean embeddings to learn a centralized controller for object manipulation with robot swarms. Here, the key idea is to directly embed the swarm configuration into a reproducing kernel Hilbert space, whereas our approach is based on embedding the agent's local view. Furthermore, using kernel-based feature spaces for the mean embedding scales poorly in the number of samples and in the dimensionality of the embedded information.</p>
<h1>2.2 Optimization-Based Approaches for Swarm Systems</h1>
<p>To provide a concise summary of the most relevant related work, we concentrate on opti-mization-based approaches that derive decentralized control strategies for the rendezvous and pursuit evasion problem considered in this paper. Ji and Egerstedt (2007) derive a control mechanism preserving the connectedness of a group of agents with limited communication abilities for the rendezvous and formation control problem. The method focuses on high-level control with single integrator linear state manipulation and provides no rules for agents that are not part of the agent graph. Similarly, Gennaro and Jadbabaie (2006) present a decentralized algorithm to maximize the connectivity (characterized by an exponential model) of a multi-agent system. The algorithm is based on the minimization of the second smallest eigenvalue of the Laplacian of the proximity graph. An approach providing a decentralized control strategy for the rendezvous problem for nonholonomic agents can be found in the work by Dimarogonas and Kyriakopoulos (2007). Using tools from nonsmooth Lyapunov theory and graph theory, the stability of the overall system is examined. A control strategy for the pursuit evasion problem with multiple pursuers and single evader that we investigate in more detail later in this paper was proposed Zhou et al. (2016). The authors derive decentralized control policies for the pursuers and the evader based on the minimization of Voronoi partitions. Again, the control mechanism is for high-level linear state manipulation. Furthermore, the method assumes visibility of the evader at all times. A survey on pursuit evasion in mobile robotics in general is provided by Chung et al. (2011).</p>
<h1>2.3 Analytic Approaches</h1>
<p>Another line of work concerned with the curse of dimensionality can be found in the area of multi-player reach-avoid games. Chen et al. (2017), for example, look at pairwise interactions between agents. This way, they are able to use the Hamilton-Jacobian-Isaacs approach to solve a partial differential equation in the joint state space of the players. Similar work can be found in (Chen et al., 2014a,b; Zhou et al., 2012).</p>
<h2>3. Background</h2>
<p>In this section, we give a short overview of Trust Region Policy Optimization and mean embeddings of distributions.</p>
<h3>3.1 Trust Region Policy Optimization</h3>
<p>Trust Region Policy Optimization is an algorithm to optimize control policies in single-agent reinforcement learning problems (Schulman et al., 2015). These problems are formulated as Markov decision processes (MDPs), which can be compactly written as a tuple $\langle\mathcal{S}, \mathcal{A}, P, R\rangle$. In an MDP, an agent chooses an action $a \in \mathcal{A}$ according to some policy $\pi(a \mid s)$ based on its current state $s \in \mathcal{S}$ and progresses to state $s^{\prime} \in \mathcal{S}$ according to the transition dynamics $P$, i.e., $s^{\prime} \sim P\left(s^{\prime} \mid s, a\right)$. After each step, the agent receives a reward $r=R(s, a)$, provided by the reward function $R$, which judges the quality of its decision. The goal of the agent is to find a policy that maximizes the cumulative reward achieved over a certain period of time.</p>
<p>In TRPO, the policy is parametrized by a parameter vector $\theta$ containing the weights and biases of a neural network. In the following, we denote this parametrized policy as $\pi_{\theta}$. The reinforcement learning objective is expressed as finding a new policy that maximizes the expected advantage function of the current policy $\pi_{\text {old }}$, i.e., $J^{\text {TRPO }}=\mathbb{E}\left[\frac{\pi_{\theta}}{\pi_{\theta_{\text {old }}}} A^{\pi_{\text {old }}}(s, a)\right]$, where $A^{\pi_{\text {old }}}(s, a)=Q^{\pi_{\text {old }}}(s, a)-V^{\pi_{\text {old }}}(s)$. Herein, the state-action value function $Q^{\pi_{\text {old }}}(s, a)$ is typically estimated via trajectory rollouts, while for the value function $V^{\pi_{\text {old }}}(s)$ linear or neural network baselines are used that are fitted to the Monte-Carlo returns, resulting in an estimate $\hat{A}(s, a)$ for the advantage function. The objective is to be maximized subject to a fixed constraint on the Kullback-Leibler (KL) divergence of the policy before and after the parameter update, which ensures that the updates to the policy parameters $\theta$ are bounded, in order to avoid divergence of the learning process. The overall optimization problem is summarized as</p>
<p>$$
\begin{array}{ll}
\underset{\theta}{\operatorname{maximize}} &amp; \mathbb{E}\left[\frac{\pi_{\theta}}{\pi_{\theta_{\text {old }}}} \hat{A}(s, a)\right] \
\text { subject to } &amp; \mathbb{E}\left[D_{\mathrm{KL}}\left(\pi_{\theta_{\text {old }}} | \pi_{\theta}\right)\right] \leq \delta
\end{array}
$$</p>
<p>The problem is approximately solved using conjugate gradient optimization, after linearizing the objective and quadratizing the constraint.</p>
<h3>3.2 Mean Embeddings</h3>
<p>Our work is inspired by the idea of embedding distributions into reproducing kernel Hilbert spaces (Smola et al., 2007) from where we borrow the concept of mean embeddings. A probability distribution $P(X)$ can be represented as an element in a reproducing kernel</p>
<p>Hilbert space by its expected feature map (i.e., the mean embedding),</p>
<p>$$
\mu_{X}=\mathbb{E}_{X}[\phi(X)]
$$</p>
<p>where $\phi(x)$ is a (possibly infinite dimensional) feature mapping. Given a set of observations $\left{x_{1}, \ldots, x_{m}\right}$, drawn i.i.d. from $P(X)$, the empirical estimate of the expected feature map is given by</p>
<p>$$
\hat{\mu}<em i="1">{X}=\frac{1}{m} \sum</em>\right)
$$}^{m} \phi\left(x_{i</p>
<p>Using characteristic kernel functions $k\left(x, x^{\prime}\right)=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle$, such as Gaussian RBF or Laplace kernels, mean embeddings can be used, for example, in two-sample tests (Gretton et al., 2012) and independence tests (Gretton et al., 2008). A characteristic kernel is required to uniquely identify a distribution based on its mean embedding. However, this assumption can be relaxed to using finite feature spaces if the objective is merely to extract relevant information from a distribution such as, in our case, the information needed for the policy of the agents.</p>
<h1>4. Deep Reinforcement Learning for Swarms</h1>
<p>The reinforcement learning algorithm presented in the last section has been originally designed for single-agent learning. In order to apply this algorithm to the swarm setup, we switch to a different problem domain and show the implications on the learning algorithm. Policies in this context are then optimized in a centralized-learning / decentralized-execution fashion.</p>
<h3>4.1 Problem Domain</h3>
<p>The problem domain for our swarm system is best described as a swarm MDP environment (Šošić et al., 2017). The swarm MDP can be regarded as a special case of a decentralized partially observable Markov decision process (Dec-POMDP) (Bernstein et al., 2002) and is constructed in two steps. First, an agent prototype is defined as a tuple $\mathbb{A}=\langle\mathcal{S}, \mathcal{O}, \mathcal{A}, \pi\rangle$, determining the local properties of an agent in the system. Herein, $\mathcal{S}$ denotes the set of the agent's local states, $\mathcal{O}$ is the set of possible local observations, $\mathcal{A}$ is the set of actions available to the agent, and $\pi: \mathcal{O} \times \mathcal{A} \rightarrow[0,1]$ is the agent's stochastic control policy. Based on this definition, the swarm MDP is constructed as $\langle N, \mathbb{A}, P, O, R\rangle$, where $N$ is the number of agents in the system and $\mathbb{A}$ is the aforementioned agent prototype. The coupling of the agents is specified through a global state transition model $P: \mathcal{S}^{N} \times \mathcal{S}^{N} \times \mathcal{A}^{N} \rightarrow[0, \infty)$ and an observation model $O: \mathcal{S}^{N} \times{1, \ldots, N} \rightarrow \mathcal{O}$, which determines the local observation $\boldsymbol{o}^{i} \in \mathcal{O}$ for agent $i$ at a given swarm state $\boldsymbol{s} \in \mathcal{S}^{N}$, i.e., $\boldsymbol{o}^{i}=O(\boldsymbol{s}, i)$. Finally, $R: \mathcal{S}^{N} \times \mathcal{A}^{N} \rightarrow \mathbb{R}$ is the global reward function, which encodes the cooperative task for the swarm by providing an instantaneous reward feedback $R(\boldsymbol{s}, \boldsymbol{a})$ according to the current swarm state $\boldsymbol{s}$ and the corresponding joint action assignment $\boldsymbol{a} \in \mathcal{A}^{N}$ of the agents. The specific state dynamics and observation models considered in this paper are described in Section 5.</p>
<p>The model encodes two important properties of swarm networks: First, all agents in the system are assumed to be identical, and accordingly, they are all assigned the same</p>
<p>decentralized policy $\pi$. This is an immediate consequence of the two-step construction of the model, which implies that all agents share the same internal architecture. Second, the agents are only partially informed about the global system state, as prescribed by the observation model $O$. Note that both the transition model and the observation model are assumed to be invariant to permutations of the agents in order to ensure the homogeneity of the system. For details, see (Šošić et al., 2017).</p>
<h1>4.2 Local Observation Models</h1>
<p>The local observation $\boldsymbol{o}^{i}$ introduced in the last section is a combination of observations $o_{\text {loc }}^{i}$ an agent makes about local properties (like the agent's current velocity or its distance to a wall) and observations $O^{i}$ of other agents. In order to describe the observation model used for the agents, we use an interaction graph representation of the swarm. This graph is given by nodes $V=\left{v_{1}, v_{2}, \ldots, v_{N}\right}$ corresponding to the agents in the swarm and an edge set $E \subset V \times V$, which we assume contains unordered pairs of the form $\left{v_{i}, v_{j}\right}$ indicating that agents $i$ and $j$ are neighbors. The interaction graph is denoted as $\mathcal{G}=(V, E)$. If both the set of nodes and the set of edges are not changing, we call $\mathcal{G}$ a static interaction graph; if either of the set undergoes changes, we instead refer to $\mathcal{G}$ as a dynamic interaction graph.</p>
<p>The set of neighbors of agent $i$ in the graph $\mathcal{G}$ is given by</p>
<p>$$
\mathcal{N}<em i="i">{\mathcal{G}}(i)=\left{j \mid\left{v</em>\right} \in E\right}
$$}, v_{j</p>
<p>Within this neighborhood, agent $i$ can sense local information about other agents, for example distance or bearing to each neighbor. We denote the information agent $i$ receives from agent $j$ as $o^{i, j}=f\left(s^{i}, s^{j}\right)$, which is a function of the local states of agent $i$ and agent $j$. The observation $o^{i, j}$ is available for agent $i$ only if $j \in \mathcal{N}<em _mathcal_G="\mathcal{G">{\mathcal{G}}(i)$. Hence, the complete state information agent $i$ receives from all neighbors is given by the set $O^{i}=\left{o^{i, j} \mid j \in \mathcal{N}</em>(i)\right}$.}</p>
<p>As the observations of other agents are summarized in form of sets $\left{O^{i}\right}$, we require an efficient encoding that can be used as input to a neural network policy. In particular, it must meet the following two properties:</p>
<ul>
<li>The encoding needs to be invariant to the indexing of the agents, respecting the unorderedness of the elements in the observation set. Only by exploiting the system's inherent homogeneity we can escape the curse of dimensionality.</li>
<li>The encoding must be applicable to varying set sizes because the local graph structure might change dynamically. Even if each agent can observe the entire system at all times, the encoding should be applicable for different swarm sizes.</li>
</ul>
<h3>4.3 Local Communication Models</h3>
<p>In addition to perceiving local state information of neighboring agents, the agents can also communicate information about the interaction graph $\mathcal{G}$ (Hüttenrauch et al., 2018). For example, agent $j$ can transmit the number of perceived neighbors to agent $i$. Furthermore, the agents can also perform more complex operations on their local neighborhood graph. For example, they could compute the shortest distance to a target point (such as an evader) that is perceived by at least one agent within their local sub-graph. Hence, by using local</p>
<p>communication protocols, observation $o^{i, j}$ can contain information about both, the local states $s^{i}$ and $s^{j}$ as well as the graph $\mathcal{G}$, i.e., $o^{i, j}=f\left(s^{i}, s^{j}, \mathcal{G}\right)$.</p>
<h1>4.4 Mean Embeddings as State Representations for Swarms</h1>
<p>In the simplest case, the local observation $o^{i, j}$ that agent $i$ receives of agent $j$ is composed of the distance and the bearing angle of agent $i$ to agent $j$. However, $o^{i, j}$ can also contain more complex information, such as relative velocities or orientations. A straightforward way to represent the information set $O^{i}$ is to concatenate the local quantities $\left{o^{i, j}\right}_{j}$ into a single observation vector. However, as mentioned before, this representation has various drawbacks as it ignores the permutation invariance inherent to a homogeneous agent network. Furthermore, it grows linearly with the number of agents in the swarm and is, therefore, limited to a fixed number of neighbors when used in combination with neural network policies.</p>
<p>To resolve these issues, we treat the elements in the information set $O^{i}$ as samples from a distribution that characterizes the current swarm configuration, i.e., $o^{i, j} \sim p_{i}(\cdot \mid \mathbf{s})$. We can now use an empirical encoding of this distribution in order to achieve permutation invariance of the elements of $O^{i}$ as well as flexibility to the size of $O^{i}$. As highlighted in Section 3.2, a simple way is to use a mean feature embedding, i.e.,</p>
<p>$$
\hat{\mu}<em j="j" o_i_="o^{i,">{O^{i}}=\frac{1}{\left|O^{i}\right|} \sum</em>\right)
$$} \in O^{i}} \phi\left(o^{i, j</p>
<p>where $\phi$ defines the feature space of the mean embedding. The input dimensionality to the policy is given by the dimensionality of the feature space of the mean embedding and, hence, it does not depend on the size of the information set $O^{i}$ any more. This allows us to use the embedding $\hat{\mu}_{O^{i}}$ as input to a neural network used in deep RL. In the following sections, we describe different feature spaces that can be used for the mean embedding. Figure 1 illustrates the resulting policy architectures with further details given in Appendix F.</p>
<h3>4.4.1 Neural Network Feature Embeddings</h3>
<p>In line with the deep RL paradigm, we propose to use a neural network as feature mapping $\phi^{\mathrm{NN}}$ whose parameters are determined by the reinforcement learning algorithm. Using a neural network to define the feature space allows us to handle high dimensional observations, which is not feasible with traditional approaches such as histograms (Hüttenrauch et al., 2018). In our experiments, a rather shallow architecture with one layer of RELU units already performed very well, but deeper architectures could be used for more complex applications. To the best of our knowledge, we present the first approach for using neural networks to define the feature space of a mean embedding.</p>
<h3>4.4.2 Histograms</h3>
<p>An alternative feature space are provided by histograms, which can be related to imagelike representations. In this approach, we discretize the space of certain features, such as the distance and bearing to other agents, into a fixed number of bins. This way, we can collect information about neighboring agents in the form of a fixed-size multi-dimensional histogram. Herein, the histogram bins define a feature mapping $\phi^{\text {HIST }}$ using a one-hotcoding for each observed agent. A detailed description of this approach can be found in</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) neural network embedding policy network</p>
<p>Figure 1: Illustration of (a) the neural network mean embedding policy, (b) the network architecture used for the RBF and histogram representation, and (c) for the simple concatenation of observations. The numbers inside the boxes denote the dimensionalities of the hidden layers. The color coding in (a) highlights which layers share the same weights. The plus sign denotes the mean of the feature activations.
our previous work (Hüttenrauch et al., 2018). While the approach works well in discrete environments where each cell is only occupied by a single agent, the representation can lead to blurring effects between agents in the continuous case. Moreover, the histogram approach does not scale well with the dimensionality of the feature space.</p>
<h1>4.4.3 Radial Basis Functions</h1>
<p>A specific problem of the histogram approach is the hard assignment of agents into bins, which results in abrupt changes in the observation space when a neighboring agent moves from one bin to another. A more fine-grained representation can be achieved by using RBF networks with a fixed number of basis functions evenly distributed over the observation space. The resulting feature mapping $\phi^{\mathrm{RBF}}$ is then defined by the activations of each basis function and can be seen as a "soft-assigned" histogram. However, both representations (histogram and RBF) suffer from the curse of dimensionality, as the number of required basis functions typically increases exponentially with the number of dimensions of the observation vector.</p>
<h3>4.5 Other Representation Techniques</h3>
<p>Inspired by the work of Mordatch and Abbeel (2018), we also investigate a policy function that uses a softmax pooling layer instead of the mean embedding. The elements of the pooling layer $\psi=\left[\psi_{1}, \ldots, \psi_{K}\right]$ are given by</p>
<p>$$
\psi_{k}=\frac{\sum_{o^{i, j} \in O^{i}} \exp \left(\alpha \phi_{k}\left(o^{i, j}\right)\right) \phi_{k}\left(o^{i, j}\right)}{\sum_{o^{i, j} \in O^{i}} \exp \left(\alpha \phi_{k}\left(o^{i, j}\right)\right)}
$$</p>
<p>for each feature dimension of $\phi=\left[\phi_{1}, \ldots, \phi_{K}\right]$ with a temperature parameter $\alpha$. Note that the representation becomes identical to our mean embedding for $\alpha=0$, while setting $\alpha \gg 1$ results in max-pooling and $\alpha \ll-1$ corresponds to min-pooling. In our experiments, we choose $\alpha=1$ as a trade-off between a mean embedding and max-pooling and additionally study the performance of max-pooling over each individual feature dimension.</p>
<h1>4.6 Adaption of TRPO to the Homogeneous Swarm Setup</h1>
<p>Gupta et al. (2017) present a parameter-sharing variant of TRPO that can be used in a multiagent setup. During the learning phase, the algorithm collects experiences made by all agents and uses these experiences to optimize one policy with a single set of parameters $\theta$. Since, in the swarm setup, we assume homogeneous agents that are potentially indistinguishable to each other, we omit the agent index introduced by Gupta et al. (2017). The optimization problem is expressed using advantage values based on all agents' observations. During execution, however, each agent has only access to its own perception. Hence, the terminology of centralized-learning / decentralized-execution is chosen.</p>
<p>During the trajectory roll-outs, we use a sub-sampling strategy to achieve a trade-off between the number of samples and the variability in advantage values seen by the learning algorithm. Our implementation is based on the OpenAI baselines version of TRPO with 10 MPI workers, where each worker samples 2048 time steps, resulting in $2048 N$ samples. Subsequently, we randomly choose the data of 8 agents, yielding $2048 \times 10 \times 8=163840$ samples per TRPO iteration. The chosen number of samples worked well throughout our experiments and was not extensively tuned.</p>
<h2>5. Experimental Results</h2>
<p>Our experiments are designed to study the use of mean embeddings in a cooperative swarm setting. The three main aspects are:</p>
<ol>
<li>How do the different mean embeddings (neural networks, histograms and RBF representation) compare when provided with the same state information content?</li>
<li>How does the mean embedding using neural networks perform when provided with additional state information while keeping the dimensionality of the feature space constant?</li>
<li>How does the mean embedding of neural network features compare against other pooling techniques?</li>
</ol>
<p>In this section, we first introduce the swarm model used for our experiments and present the results of different evaluations afterwards. During a policy update, a fixed number of $K$ trajectories are sampled, each yielding a return of $G_{k}=\sum_{t=1}^{T} r(t)$. The results are presented in terms of the average return, denoted as $\bar{G}=\frac{1}{K} \sum_{k=1}^{K} G_{k}$. Videos demonstrating the agents' behavior in the different tasks can be found online. ${ }^{2}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5.1 Swarm Models</h1>
<p>Our agents are modeled as unicycles (a commonly used agent model in mobile robotics; see, for example, Egerstedt and Hu, 2001), where the control parameters either manipulate the linear and angular velocities $v$ and $\omega$ (single integrator dynamics) or the corresponding accelerations $\dot{v}$ and $\dot{\omega}$ (double integrator dynamics). In the single integrator case, the state of an agent is defined by its location $\boldsymbol{x}=(x, y)$ and orientation $\phi$. In case of double integrator dynamics, the agent is additionally characterized by its current velocities. The exact state definition and kinematic models can be found in Appendix A. Note that these agent models are more complex than what is typically considered in optimization-based approaches, which mostly assume single integrator dynamics directly on $\boldsymbol{x}$. Depending on the task, we either opt for a closed state space where the limits act as walls, or a periodic toroidal state space where agents exceeding the boundaries reappear on the opposite side of the space. Either way, the state is bounded by $x_{\max }=y_{\max }=100$.</p>
<p>We study two different observation scenarios for the agents, i.e., global observability and local observability. In the case of global observability, all agents are neighbors, i.e.</p>
<p>$$
\mathcal{N}_{\mathcal{G}}(i)={j \in{1, \ldots, N} \mid i \neq j}
$$</p>
<p>which corresponds to a fully connected static interaction graph. For the local observability case, we use $\Delta$-disk proximity graphs, where edges are formed if the distance $d^{i, j}=$ $\sqrt{\left(x^{i}-x^{j}\right)^{2}+\left(y^{i}-y^{j}\right)^{2}}$ between agents $i$ and $j$ is less than a pre-defined cut-off distance $d_{c}$ for communication, resulting in a dynamic interaction graph. The neighborhood set of the graph is then defined as</p>
<p>$$
\mathcal{N}<em c="c">{\mathcal{G}}(i)=\left{j \in{1, \ldots, N} \mid i \neq j, d^{i, j} \leq d</em>\right}
$$</p>
<p>For a detailed description of all observational features available to the agents in the tasks, see Appendices B and C.</p>
<h3>5.2 Rendezvous</h3>
<p>In the rendezvous problem, the goal is to minimize the distances between all agents. The reason why we choose this experiment is because a simple optimization-based baseline controller can be defined by the consensus protocol,</p>
<p>$$
\dot{\boldsymbol{x}}^{i}=-\sum_{j \in \mathcal{N}(i)}\left(\boldsymbol{x}^{i}-\boldsymbol{x}^{j}\right)
$$</p>
<p>where $\boldsymbol{x}^{i}=\left(x^{i}, y^{i}\right)$ denotes the location of agent $i$. To make the solution compatible to the double integrator agent model, we make use of a PD-controller (see Appendix A for details). The reward function for the problem can be found in Appendix E.1.</p>
<p>We evaluate different observation vectors $o^{i, j}$ which are fed into the policy. To compare the histogram and RBF embedding with the proposed neural network approach, we restrict the basic observation model (see below) to a set of two features: the distance $d^{i, j}$ between two agents and the corresponding bearing $\phi^{i, j}$. This restriction allows for a comparison to the optimization-based consensus protocol, which is based on displacements (an equivalent formulation of distance and bearing). To show that the neural network embeddings can</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of two neighboring agents facing the direction of their velocity vectors $\nu^{i}$ and $\nu^{j}$, along with the observed quantities, shown with respect to agent $i$. The observed quantities are the bearing $\phi^{i, j}$ to agent $j$, agent $j$ 's relative orientation $\theta^{i, j}$ to agent $i$, their distance $d^{i, j}$ and a relative velocity vector $\Delta \nu^{i, j}=\nu^{i}-\nu^{j}$. In this trivial example, agent $i$ 's observed neighborhood size as well as the neighborhood size communicated by agent $j$ are $|\mathcal{N}(i)|=|\mathcal{N}(j)|=1$.
be used with more informative observations, we further introduce an extended set and a communication (comm) set. These sets may include relative orientations $\theta^{i, j}$ or relative velocities $\Delta \nu^{i, j}$ (depending on the agent dynamics), as well as the own neighborhood size and those of the neighbors. An illustration of these quantities can be found in Figure 2.</p>
<h1>5.2.1 Global Observability</h1>
<p>First, we study the rendezvous problem with 20 agents in the global observability setting with double integrator dynamics to illustrate the algorithm's ability to handle complex dynamics. To this end, we compare the performances of policies using histogram, RBF and neural network embeddings on the basic set, as well as neural network embeddings on the extended set. The observations $o^{i, j}$ in the basic set comprise the distance $d^{i, j}$ and bearing $\phi^{i, j}$. In the extended set, which is processed only via neural network embeddings, we additionally add neighboring agents' relative orientations $\theta^{i, j}$ and velocities $\Delta \nu^{i, j}$. The local properties $o_{\text {loc }}^{i}$ consist of a shortest distance and orientation to the closest boundary, i.e., $d_{\text {wall }}^{i}$ and $\phi_{\text {wall }}^{i}$. The sets are summarized as follows:</p>
<p>$$
\begin{aligned}
\text { Basic : } &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}\right} &amp; o_{\text {loc }}^{i}=\left{d_{\text {wall }}^{i}, \phi_{\text {wall }}^{i}\right} \
\text { Extended }: &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}, \theta^{i, j}, \Delta \nu^{i, j}\right} &amp; o_{\text {loc }}^{i}=\left{d_{\text {wall }}^{i}, \phi_{\text {wall }}^{i}\right}
\end{aligned}
$$</p>
<p>The results are shown in Figure 3a. On first sight, they reveal that all shown methods eventually find a successful strategy, with the histogram approach showing worst performance. Upon a closer look, it can be seen that the best solutions are found with the neural network embedding, in which case the learning algorithm also converges faster, demonstrating that this form of embedding serves as a suitable representation for deep RL. However, there are two important things to note:</p>
<ul>
<li>The differences between the approaches seem to be small due to the wide range of obtained reward values, but the NN+ method brings in fact a significant performance</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Learning curves for the rendezvous task with different observation models. The curves show the median of the average return $\bar{G}$ based on the top five trials on a $\log$ scale. Legend: NN++: neural network mean embedding of comm set, NN+: neural network mean embedding of extended set, NN: neural network embedding of basic set, RBF: radial basis function embedding of basic set, HIST: histogram embedding of basic set, CONCAT+: simple concatenation of extended set.
gain. Compared to the NN and RBF embedding, the performance of the learned NN+ policy is $\sim 10 \%$ better in terms of the average return of an episode (Figure 3a) and almost twice as good $\left(\sim 4 \times 10^{-2}\right.$ versus $\left.\sim 8 \times 10^{-2}\right)$ in terms of the mean distance between agents at the steady state solution after around 200 time steps (Figure 5a). Furthermore, the NN+ embedding reaches the mean distance achieved by the NN and RBF embeddings roughly 20 to 30 time steps earlier, which corresponds to an improvement of $\sim 25 \%$.</p>
<ul>
<li>Although the performance gain of $\mathrm{NN}+$ can be partly explained by the use of the extended feature set, experiments with the same feature set using the histogram / RBF approach did not succeed to find solutions to the rendezvous problem; hence, the corresponding results are omitted. The reason is that the dimensionality of the input space scales exponentially for the histogram / RBF approach while only linearly for the neural network embedding, which results in a more compact feature representation that keeps the learning problem tractable.</li>
</ul>
<p>Together, these two observations suggest that the neural network embedding provides a suitable learning architecture for deep RL, whereas the histogram / RBF approach is only suited for low-dimensional spaces.</p>
<p>Figure 4 shows a visualization of a policy using the neural network mean embedding of the extended set. After random initialization, the agents' locations quickly converge to a single point. Figure 5 shows performance evaluations of the best policies found with each of the mean embedding approaches. We plot the evolution of the mean distance between all agents over 1000 episodes with equal starting conditions. We also include the performance of the PD-controller defined in Appendix A. It can be seen in Figures 5a and 5c that the policies using the neural network embeddings decrease the mean distance most quickly and also find</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visualization of a learned policy for the pursuit evasion task. The policy is learned and executed by 10 agents using a neural network mean embedding of the extended set. Pursuers are illustrated in blue, the evader is highlighted in red. Visualization of a learned policy for the rendezvous task. The policy is learned and executed by 20 agents using a neural network mean embedding of the extended set.
the best steady-state solutions among all learning approaches. While the optimization-based solution (PD) eventually drives the mean distance to zero, a small error remains for the learning-based approaches. However, the learned policies are faster in reducing the distance and therefore show a better average reward. Although the optimization-based policy is guaranteed to find an optimal stationary solution, the approach is build for simpler dynamics and hence performs suboptimally in the considered scenario. Note, that the controller gains for this approach have been tuned manually to maximize performance.</p>
<p>In order to show the generalization abilities of the embeddings, we finally evaluate the obtained policies (except for the concatenation) with 100 agents. The results are displayed in Figure 5b. Again, the neural network embedding of the extended set is quickest in reducing the inter-agent distances, resulting in the best overall performance.</p>
<h1>5.2.2 LOCAL ObSERVABILITY</h1>
<p>The local observability case is studied with 20 agents and a communication cut-off distance of $d_{c}=40$. Due to the increased difficulty of the task, we resort to single integrator dynamics for this experiment. Again, we evaluate the basic and the extended set, which in this case contains the single integrator state information. Accordingly, we remove the relative velocities from the information sets. Moreover, we employ a local communication strategy that transmits the number of observed neighbors as additional information. Note that this information can be used by the agents to estimate in which direction the center of mass of the swarm is located.</p>
<p>While the received neighborhood sizes ${|\mathcal{N}(j)|}<em _loc="{loc" _text="\text">{j \in \mathcal{N}(i)}$ are treated as part of agent $i$ 's local observation of the swarm, the own perceived neighborhood size $|\mathcal{N}(i)|$ is considered as part of the local features $o</em>$. The observation models for the local observability case are thus summarized as:}}^{i</p>
<p>$$
\begin{aligned}
\text { Basic : } &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}\right} \
\text { Extended : } &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}, \theta^{i, j}\right} \
\text { Comm : } &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}, \theta^{i, j},|\mathcal{N}(j)|\right} \
&amp; o_{\text {loc }}^{i}=\left{d_{\text {wall }}^{i}, \phi_{\text {wall }}^{i},|\mathcal{N}(i)|\right}
\end{aligned}
$$</p>
<p>For the experiment, we limit our comparison to RBF embeddings (which showed best performance among all non-neural-network solutions) of the basic set and neural network embeddings of the extended set and the comm set. The results are illustrated in Figure 3b, which shows that the neural network embeddings lead to a quicker learning progress. Furthermore, by introducing the comm model, a higher return is achieved. Compared to the global observability case, however, the learning process exhibits an increased variance caused by the information loss in the reward signal (see Appendix E).</p>
<p>Figure 5c illustrates the performances of the learned policies. Again, the neural network embedding is quicker in reducing the inter-agent distances and converges to better steadystate solutions. In order to test the efficacy of the communication protocol, we further evaluate the learned policies with 10 agents. The results are displayed in Figure 5d. As expected, the performance decreases due to the lower chance of agents seeing each other but we still notice a benefit caused by the communication.</p>
<h1>5.3 Pursuit Evasion with a Single Evader</h1>
<p>Our implementation of the pursuit evasion scenario is based on the work by Zhou et al. (2016), from which we adopt the evader strategy. The strategy is based on Voronoi regions, which the pursuers try to minimize and the evader tries to maximize. While the original paper considers a closed world, we change the world type from closed to periodic, thereby making it impossible to trap the evader in a corner. In order to encourage a higher level of coordination between the agents, we set the evader's maximum velocity to twice the pursuers' maximum velocity. An episode ends once the evader is caught, i.e., if the distance of the closest pursuer is below a certain threshold. In all our experiments, the evader policy is fixed and not part of the learning process. The reward function for the problem is based on the shortest distance of the closest pursuer and can be found in Appendix E.2.</p>
<h3>5.3.1 Global Observability</h3>
<p>Again, we study the global observability case with ten agents. Since the pursuit of an evader is a more challenging task already, we reduce the movement complexity to single integrator dynamics. The basic and extended set are equal to those in the rendezvous experiment with single integrator dynamics, with additional information about the evader in the local properties $o_{\text {loc }}^{i}$. In here, we add the distance $d^{i, e}$ and bearing $\phi^{i, e}$ of agent $i$ to the evader $e$. Accordingly, the observation sets are given as:</p>
<p>$$
\begin{aligned}
\text { Basic : } &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}\right} \
\text { Extended : } &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}, \theta^{i, j}\right} \
&amp; o_{\text {loc }}^{i}=\left{d_{\text {wall }}^{i}, \phi_{\text {wall }}^{i}, d^{i, e}, \phi^{i, e}\right}
\end{aligned}
$$</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of the mean distance between agents in the rendezvous experiment achieved by the best learned policies and the consensus protocol. In (a) and (b), the policy is learned with 20 agents and executed by 20 and 100 agents, respectively. In (c) and (d), the policy is learned with 20 agents and executed by 20 and 10 agents. Results are averaged over 1000 episodes with identical starting conditions.</p>
<p>The results in Figure 6a reveal that successful strategies can be obtained with all methods. However, this time, a clear advantage can be seen for the policies using neural network mean embeddings of the extended set, both in terms of behavior quality and in the number of samples necessary to find the solution.</p>
<p>Figure 7 illustrates the strategy that such a policy exerts. After random initialization, the agents first spread in a way that leaves no possibility for the evader to increase its Voronoi region, thereby keeping the evader almost on the same spot. Once this configuration is reached, they surround the evader in a circular pattern and start to reduce the distance until one pursuer successfully reaches the distance threshold.</p>
<p>To investigate the performance of the best mean embedding policies (learned with 10 agents), we estimate the corresponding probabilities that the evader is caught within a certain time frame. For the sake of completeness, we also include the method proposed by Zhou</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Learning curves for the pursuit evasion task with different observation models. The curves show the median of the average return $\bar{G}$ based on the top five trials on a log scale. Legend: NN++: neural network mean embedding of comm set, NN+: neural network mean embedding of extended set, RBF: radial basis function embedding of basic set, HIST: histogram embedding of basic set, CONCAT+: concatenation of extended set.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Visualization of a learned policy for the pursuit evasion task. The policy is learned and executed by 10 agents using a neural network mean embedding of the extended set. Pursuers are illustrated in blue, the evader is highlighted in red.</p>
<p>et al. (2016), which was originally not designed for a setup with a faster evader, though. The results are plotted in Figure 8 as the fraction of episodes ending at the respective time instant, averaged over 1000 episodes. The plot in Figure 8b reveals that the evader may be caught using all presented methods if the policies are executed for long time periods. As already indicated by the learning curves, using a neural network mean embedding representation yields the quickest capture among all methods. The additional information in the extended set further increases performance.</p>
<p>Next, we examine the generalization abilities of the learned policies, this time on scenarios with 5, 20 and 50 agents (Figures 8a,c,d). Increasing the amount of agents leads to a quicker capture for all methods; however, the best performance is still shown by the agents executing a neural network policy based on embeddings of the extended set. Interestingly, when using fewer agents than in the original setup (Figure 8a), all methods struggle to capture the evader. After inspection of the behavior, we found that the strategy of establishing a circle around the evader causes too large gaps between the agents through which the evader can escape.</p>
<h1>5.3.2 LOCAL ObSERVABILITY</h1>
<p>The local observability case is studied with 20 agents and a communication cut-off distance of $d_{c}=40$. Additionally, we introduce an observation radius $d_{o}=20$ within which the pursuers can observe the distance and bearing to the evader. We reuse the basic and extended set from last section and modify the comm set to include the shortest path information of other agents in the neighborhood of agent $i$ to the evader. This way, each agent $i$ can compute a shortest path to the evader over a graph of connected agents, such that the path $P=\left(v^{1}, v^{2}, \ldots, v^{M}\right)$ minimizes the sum $d_{\min }^{i, e}=\sum_{m=1}^{M-1} d^{m, m+1}$ where $v^{1}$ represents agent $i$ and $v^{M}$ is the evader. The observation sets are given as:</p>
<p>$$
\begin{aligned}
\text { Basic : } &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}\right} \
\text { Extended : } &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}, \theta^{i, j}\right} \
\text { Comm : } &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}, \theta^{i, j}, d_{\min }^{j, e}\right}
\end{aligned}
$$</p>
<p>$$
o_{\text {loc }}^{i}=\left{d_{\text {wall }}^{i}, \phi_{\text {wall }}^{i}, d^{i, e}, \phi^{i, e}\right}
$$</p>
<p>Note that in this case the distance and bearing to an evader are only available if $d^{i, e} \leq d_{o}$. Furthermore, the correct shortest path is only available if an agent and the evader are in the same sub-graph, otherwise, a pre-defined value is fed into the policy.</p>
<p>Again, we limit the comparison for the local observability case to the more promising methods of neural network and RBF mean embeddings. The results in Figure 6b show that the performance gain of the neural network mean embeddings is even more noticeable than in the global observability case, with a clear advantage in the presence of the local communication protocols. The inspection of the termination probabilities in Figure 9 confirms that the neural network mean embedding results in a significantly improved policy.</p>
<h3>5.4 Pursuit Evasion with Multiple Evaders</h3>
<p>Lastly, we study a pursuit evasion scenario with multiple evaders, i.e., we assume that agent $i$ receives observation samples $\left{o^{i, e}\right}$ from several evaders, which are processed using a second mean embedding to account for the variable set size. Where in the previous experiment</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Performance comparison of the best learned policies and the optimization approach minimizing Voronoi regions in the pursuit evasion task with global observability. The curves show the probability that the evader is caught after $t$ time steps. All policies are learned with 10 agents but executed with different agent numbers, as indicated below each subfigure. Results are averaged over 1000 episodes with identical starting conditions.
the agents had precise information about the evader in terms of distance and bearing, they now have to extract this information from the respective embedding. An additional level of difficulty results from the fact that the reward function no longer provides any guidance in terms of the distances to the evaders since it only counts the number of evaders caught in each time step (see Appendix E. 3 for details).</p>
<p>We study a scenario with 50 pursuers and 5 evaders using the global observability setup in Section 5.3.1, except that we respawn caught evaders to a new random location instead of terminating the episode. The observation sets, containing the same type of information but arranged according to the inputs of the neural networks, are designed as follows:</p>
<p>$$
\begin{array}{rlll}
\text { Basic : } &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}\right} &amp; o^{i, e}=\left{d^{i, e}, \phi^{i, e}\right} &amp; o_{\text {loc }}^{i}=\left{d_{\text {wall }}^{i}, \phi_{\text {wall }}^{i}\right} \
\text { Extended : } &amp; o^{i, j}=\left{d^{i, j}, \phi^{i, j}, \theta^{i, j}\right} &amp; o^{i, e}=\left{d^{i, e}, \phi^{i, e}\right} &amp; o_{\text {loc }}^{i}=\left{d_{\text {wall }}^{i}, \phi_{\text {wall }}^{i}\right}
\end{array}
$$</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Performance comparison of the best policies in the pursuit evasion task with local observability. The curves show the probability that the evader is caught after $t$ time steps. All policies are learned and executed by 20 agents. Results are averaged over 1000 episodes with identical starting conditions.</p>
<p>Figure 10a shows the learning curves for policies with neural network and RBF mean embeddings and for the concatenation approach. The return directly relates to the number of evaders caught during an episode. Again, the neural network mean embedding performs significantly better than the RBF embedding. The curves clearly indicate the positive influence of the additional information present in the extended set. With this amount of agents, the dimensionality of the concatenation has increased to a point where learning is no longer feasible.</p>
<h1>5.5 Evaluation of Pooling Functions</h1>
<p>Figure 11 shows learning curves of policies based on mean embeddings, softmax pooling, and max-pooling (as described in Section 4.5) of features of the extended set for the rendezvous and pursuit evasion task with global observability.</p>
<p>In the rendezvous task (Figure 11a), all pooling techniques eventually manage to find a good solution. Policies using neural network mean embedding, however, on average converge more quickly while policies using max-pooling show slightly worse performance. Given its reduced computational complexity compared to the softmax-pooling, the mean embedding provides the most effective approach among all proposed architectures.</p>
<p>When examining the results of the pursuit evasion task (Figure 11b), we find that the algorithm produces two distinct solutions. A sub-optimal one, which is only able to circle the evader but is unable to catch it (a catch is realized if the distance of the closest pursuer to the evader is below a certain threshold), and a solution which additionally catches the evader after a short period of time. Therefore, we not only report the performance of the top 5 trials out of 16 , but also provide the number of times the algorithm was able to discover the better of the two solution (Table 1). Once that the algorithm finds a good solution, the mean embedding and softmax solutions perform comparably well but the max-pooling approach shows a significantly worse performance. More importantly, however, the algorithm was</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>http://computational-learning.net/deep_rl_for_swarms</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
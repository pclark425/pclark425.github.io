<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8832 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8832</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8832</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-265842153</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.00444v1.pdf" target="_blank">Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements</a></p>
                <p><strong>Paper Abstract:</strong> This work focuses on the novel problem setting of generating graphs conditioned on a description of the graph's functional requirements in a downstream task. We pose the problem as a text-to-text generation problem and focus on the approach of fine-tuning a pretrained large language model (LLM) to generate graphs. We propose an inductive bias which incorporates information about the structure of the graph into the LLM's generation process by incorporating message passing layers into an LLM's architecture. To evaluate our proposed method, we design a novel set of experiments using publicly available and widely studied molecule and knowledge graph data sets. Results suggest our proposed approach generates graphs which more closely meet the requested functional requirements, outperforming baselines developed on similar tasks by a statistically significant margin.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8832.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8832.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGG-LLM serialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Serialized Graph Generator (SGG-LLM) reversible serialization with special tokens and disambiguation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reversible text serialization g(·) of arbitrary text-annotated graphs used to train/fine-tune an autoregressive LLM to generate graphs conditioned on functional descriptions; uses special delimiter tokens for predecessor/edge/successor and a node-disambiguation token to make the mapping injective and reversible.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>reversible serialized bag-of-edges with special tokens and disambiguation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each edge is serialized as a delimited string segment using special tokens <PN> (predecessor node), <E> (edge), and <SN> (successor node). If multiple nodes share the same textual feature string, a disambiguation token <D> followed by a unique integer is appended to node feature strings to make node identities unique. The serialization is injective (has known inverse g^{-1}) so any serialized string can be deserialized back to the original graph.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text graphs; used for molecular graphs (atoms as node strings, bonds as edge strings) and knowledge graphs (triples as nodes/edges) in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Bag-of-edges style serialization that lists edges in a predefined order; each edge described as '<PN> predecessor_string <E> edge_string <SN> successor_string'; node occurrences that would otherwise be ambiguous are suffixed with '<D>i' disambiguators to ensure injectivity; serialized form stored as a text string and tokenized for LLM training.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Conditional graph generation: generate graphs conditioned on functional textual descriptions (molecule generation conditioned on valency electron count and QED; also knowledge graph generation from imperative descriptions / WebNLG+ 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used in SGG-LLM which achieved strong results on molecule tasks: e.g., SGG-LLM (no MP) trained on 100k: QED MAE = 0.044 ± 0.011, Valency MAE = 0.060 ± 0.018; SGG-LLM (edge-MP) 100k: QED MAE = 0.036 ± 0.005, Valency MAE = 0.035 ± 0.014. Parsability for fine-tuned SGG-LLM variants ~0.995–1.000. (Metrics from Tables 2/3.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms prior bag-of-edges baselines (regen, grapher) on molecule functional matching (lower MAE) when used in SGG-LLM; variants that incorporate message-passing (MP) into the LLM further improve MAE vs the no-MP variant. The paper reports statistically significant improvements over regen and grapher on MAE and diversity (where relevant).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Injective/reversible (can deserialize to exact graph), handles node-feature collisions via explicit disambiguation, compatible with pretrained LLM tokenizers (adds a few special tokens), enables training LLMs to output valid graphs with high parsability, and admits interleaving graph-structured processing (MP layers) to improve property matching.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Can produce long serialized sequences (edge-list grows with number of edges), requires adding special tokens & embedding rows to pretrained LLM, and overall approach depends on computationally expensive fine-tuning of autoregressive LLMs; causal/autoregressive constraints require careful design (causal message passing constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Without fine-tuning the pretrained LLM on serialized graphs, the model produced 0 parsable outputs. If node disambiguation is not used, serialization is not injective for repeated identical node strings. When MP layers are added but the gating mechanism is omitted, the model failed to generate any parsable graphs (parsability = 0). Small training set size (e.g., 25k) caused very poor Valency MAE for some variants (e.g., SGG-LLM none on 25k: Valency MAE = 1.703 ± 0.074).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8832.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8832.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>bag-of-edges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bag-of-edges serialization (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used serialized-graph representation that lists edges as text strings (with delimiters) in a predefined order; used by prior state-of-the-art approaches for text-to-graph generation and implemented as baselines in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>bag-of-edges serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each edge is represented by a text fragment encoding predecessor node string, edge string, and successor node string with special delimiter tokens; edges are listed in a predefined order (prior works sometimes use DFS ordering for molecules), producing a single serialized text describing the whole graph.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs and molecules (as used in prior work and as implemented in baseline experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Prior methods typically produce strings like '<PN> predecessor <E> edge <SN> successor' for every edge and concatenate these segments in a fixed order (e.g., depth-first ordering for molecules in SMILES or arbitrary predefined order for KGs).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Text-to-graph generation (knowledge graph construction, explanation graphs, and molecule generation in baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baselines that used variants of bag-of-edges (reimplemented here) performed worse than SGG-LLM on molecule functional matching: e.g., regen: QED MAE = 0.149 ± 0.018, Valency MAE = 2.282 ± 1.156; grapher: QED MAE = 0.157 ± 0.004, Valency MAE = 1.268 ± 0.229. Parsability for these baselines was ≈0.984–1.000 depending on model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>SGG-LLM variants (which extend bag-of-edges with disambiguation and optionally MP layers and a different training objective) outperform standard bag-of-edges based baselines (regen, grapher) on MAE and diversity for molecular functional targets. The paper attributes part of the improvement to the modified training loss and equal-weighting scheme across examples, and part to joint single-model estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, general, and directly tokenizable for LLMs; compatible with many domains; prior success on knowledge-graph benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Ambiguity if nodes share identical textual features (not injective) unless extra disambiguation is added; node features may be repeated multiple times in serialization leading to design complications when incorporating graph-structured inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>As implemented originally, bag-of-edges without node disambiguation cannot reversibly map strings to graphs when nodes share identical textual labels (e.g., molecules with multiple identical atom labels), causing incorrect or non-injective deserializations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8832.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8832.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES (Simplified Molecular Input Line Entry System) linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific linearization for molecules that encodes molecule graphs as a string by traversing bonds (commonly done via depth-first search) producing a compact one-line molecular notation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES linearization (DFS-based)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Orders atoms/bonds by a depth-first traversal of the molecular graph and serializes chemical structure into a compact string using domain-specific grammar/symbols representing atoms, bonds, rings, and branching.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Molecular graphs (chemoinformatics).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first search traversal ordering of bonds/atoms with grammar tokens for atoms, bond types, cycles and branches; produces a canonical or non-canonical string representation of the molecule.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Molecule generation and chemical property prediction in prior work (mentioned as related serialization technique).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated experimentally in this paper; mentioned as prior common molecular serialization method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>SMILES is an injective (up to canonicalization) chemistry-specific linearization, contrasted with the more general bag-of-edges approach; the paper notes SMILES uses DFS ordering while prior bag-of-edges lists edges in a predefined order.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Compact and chemically-aware; widely used and understood in cheminformatics.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Domain-specific to chemistry and not directly generalizable to arbitrary text graphs; may not map naturally to LLM tokenization without domain-specific tokens/rules.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not evaluated in the experiments of this paper; not applicable when node/edge features are arbitrary text strings outside chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8832.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8832.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Node disambiguation (<D>)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node disambiguation token mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple mechanism that appends a special token <D> plus a unique integer to node feature strings that are identical, making multiple occurrences distinct in the serialized text so the serialization becomes reversible.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>node disambiguation via <D> index suffix</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>When multiple graph nodes have identical textual feature strings, append '<D>i' to each instance (where i is a unique index) so that serialized node descriptions are unique and the serialized string can be deterministically deserialized back to the original node identities.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Molecular graphs (atoms with identical element symbols) and any text graphs with repeated identical node labels.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>During serialization, detect nodes with identical feature strings and replace each occurrence 'X' with 'X<D>k' where k is a unique integer for that node instance prior to concatenating edge segments into the serialized graph string.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used for graph generation tasks where multiple identical node labels appear (molecule generation in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Enables reversible serialization; used across SGG-LLM and baseline reimplementations to allow molecular datasets to be serialized/deserialized correctly. No standalone numeric metric reported, but necessary for parsability and valid graph recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>An augmentation to bag-of-edges/serialized representations; prior work often did not include such explicit disambiguation and thus could not generically serialize graphs with repeated identical node labels.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Restores injectivity and allows universal reversible serialization of graphs containing identical node labels; simple to implement and compatible with tokenizers.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increases token length slightly and adds synthetic tokens to node labels; requires special-token embedding addition to pretrained LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If not used, serialization can be non-injective causing generation/deserialization failures on graphs with repeating node labels (common in molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8832.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8832.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Edge-graph transformation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge-graph transformation (treat edges as nodes for MP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformation used to enable message-passing layers inside the LLM: edges of the original graph are treated as nodes in a derived 'edge graph', and adjacency between edges is defined by shared original nodes, with a causality constraint to prevent backward information flow during autoregressive generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>edge-graph for causal message passing</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct G_edge = (V_edge, E_edge) where V_edge = E (original edges) and E_edge connects e_j and e_k if they share a node in the original graph and index j != k; add constraint that edges always point from earlier-occurring edge to later-occurring edge (k > j) so MP does not transmit information backwards in the serialized token sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text graphs used in serialization (molecular graphs in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Map each serialized edge to a node in the edge graph; use the last token's token embedding for that edge as the node feature for the edge-graph MP layer; build adjacency among edges if they share nodes and enforce directed indexing (earlier→later) to maintain causal generation constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Incorporated into SGG-LLM message-passing layers to improve conditional graph generation performance (molecule functional matching).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SGG-LLM variant using edge-based MP (100k training) achieved: QED MAE = 0.036 ± 0.005, Valency MAE = 0.035 ± 0.014; parsability ≈ 0.998–0.999. Outperformed no-MP variant on MAE in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Edge-graph MP variant outperformed SGG-LLM without MP and outperformed prior baselines (regen, grapher) on MAE; also compared to correspondence-MP variant which had similar but slightly different performance depending on metric and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Permits exchange of structural information between tokens while respecting autoregressive causal constraints; empirically improved functional property matching (lower MAE).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires construction of auxiliary graph structure and mapping indices between serialized tokens and edge-graph nodes; increases model complexity and computation; needs a gating mechanism when injecting MP outputs back into token representations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If MP outputs are incorporated without the learned gating (tanh a) the model failed to produce parsable outputs (parsability = 0). Also careful edges-direction constraints are required to avoid passing information backwards in the token sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8832.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8832.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Correspondence graph MP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Correspondence-graph-based message passing (MP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative MP design where each instance of a node in the serialized sequence is treated as its own node in a correspondence graph, and edges connect neighboring occurrences that correspond to the same original node; edges are directed from earlier to later occurrences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>correspondence-graph message passing</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct a correspondence graph whose nodes are each occurrence of a graph node in the serialization; connect instances of the same original node when they appear adjacent in the serialized sequence, with directed edges from earlier instance to later instance to keep message passing causal.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-serialized graphs where a node may appear multiple times in the serialized token sequence (common in bag-of-edges serialization).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Treat each textual occurrence of a node in the serialized string as a distinct node; add directed edges between adjacent occurrences that refer to the same original node; use the token embeddings for each occurrence as correspondence-graph node features and apply MP (GraphSAGE) constrained by causality.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used as an MP variant in SGG-LLM for conditional graph generation (molecular experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SGG-LLM correspondence MP (100k) achieved: QED MAE = 0.039 ± 0.007, Valency MAE = 0.045 ± 0.017; parsability ≈ 0.995–0.998. Performed comparably to edge-MP variant; in some cases correspondence variant achieved highest diversity/MAE tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Comparable performance to edge-graph MP variant; both MP variants outperformed no-MP SGG-LLM and prior baselines. The paper reports that two MP variants outperform the variant without MP.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages multiple occurrences of the same original node in the serialized sequence to propagate structural information while preserving causal ordering; empirically effective.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires extra graph construction and bookkeeping of occurrences; complexity increases; must ensure edges are directed to avoid backward information flow.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>None specifically called out beyond general MP-related constraints; success requires gating when integrating MP outputs (same gating caveat as edge-MP).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8832.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8832.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Last-token aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Last-token embedding aggregation for node/edge vector construction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pragmatic method to obtain a single vector representation for a node or edge that is described by multiple tokens in the serialized sequence by selecting the feature vector of the last token describing that node/edge (which, due to causal attention, contains information from the preceding tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>last-token aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each node or edge described by a sequence of tokens in the serialization, take the LLM token embedding of the last token in that description as the single dense representation for that element; for edges, map edge index to last-token index via f_index and extract d_{f_index(k)}_{G,F}.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-serialized graphs where nodes/edges are represented by multiple tokens in the serialized sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Parse the serialized token sequence, identify the final token position that completes a node/edge's textual description, and use that token's LLM feature vector as the aggregate representation for message passing inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used to construct node/edge feature matrices for MP layers interleaved in the LLM during conditional graph generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used in all SGG-LLM MP variants; necessary for MP-enabled variants that achieved low MAE (see SGG-LLM edge/correspondence MAE values). No standalone numeric metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Selected because the last token's causal-attention context naturally includes preceding tokens describing the element; alternative aggregations (e.g., mean pooling) are not evaluated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement and consistent with autoregressive (causal) transformer semantics; avoids looking ahead and therefore respects generation-time information constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>If an element appears earlier in the serialization multiple times, care must be taken which occurrence to aggregate (hence correspondence/edge-graph constructions are used).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported to fail by itself; correct use depends on respecting causality (must select the last token per occurrence and ensure MP connections don't allow backward-flow).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8832.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8832.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MP gating (tanh a)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned tanh gating for injecting MP outputs into token embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned gating scalar parameter a whose tanh is multiplied element-wise with MP layer outputs before adding back into token feature vectors; crucial for stable fine-tuning when interleaving MP layers into LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>tanh gating for MP output integration</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>After computing MP outputs on the edge/correspondence graph, multiply the MP output vector element-wise by tanh(a) (a is a learned vector initialized to 0) and add the gated vector to the corresponding token embedding (specifically the token immediately after the edge description).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Applied within serialized text-graph processing for molecular and knowledge-graph generation.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>During each interleaved MP step, compute MP output d_k_{V_edge,F} and update the serialized token feature d_t_{G,F} as: d_t_{G,F} := d_t_{G,F} + d_k_{V_edge,F} * tanh(a) when the token t is the position immediately after the edge description (f_index(k) = t-1).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Stabilizes fine-tuning of LLMs with interleaved MP to generate serialized graphs conditioned on text (molecule functional generation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The ablation without gating (SGG-LLM with edge-based MP but without gating) produced zero parsable outputs on molecule test sets (parsability = 0). With gating, MP variants achieved high parsability (~0.998) and improved MAE (see edge-MP MAE values).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The paper reports that gating was necessary to succeed; without it MP prevents convergence and yields no parsable samples. Inspired by similar techniques in multimodal LLM integration literature.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Permits the model to gradually incorporate structural MP signals during fine-tuning; prevents destabilizing the pretrained LLM weights at early stages by starting with near-zero injected signal.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds extra learnable parameters and a design hyperparameter; if omitted, training fails for MP-enabled variants.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Omitting the gating term caused the MP-interleaved models to produce zero parsable outputs in molecule generation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Regen: Reinforcement learning for text and knowledge base generation using pretrained language models <em>(Rating: 2)</em></li>
                <li>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. <em>(Rating: 2)</em></li>
                <li>Inductive representation learning on large graphs <em>(Rating: 2)</em></li>
                <li>Knowledge graph generation from text <em>(Rating: 2)</em></li>
                <li>Explanation graph generation via pretrained language models: An empirical study with contrastive learning <em>(Rating: 2)</em></li>
                <li>WebNLG+ 2020 <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8832",
    "paper_id": "paper-265842153",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "SGG-LLM serialization",
            "name_full": "Serialized Graph Generator (SGG-LLM) reversible serialization with special tokens and disambiguation",
            "brief_description": "A reversible text serialization g(·) of arbitrary text-annotated graphs used to train/fine-tune an autoregressive LLM to generate graphs conditioned on functional descriptions; uses special delimiter tokens for predecessor/edge/successor and a node-disambiguation token to make the mapping injective and reversible.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "reversible serialized bag-of-edges with special tokens and disambiguation",
            "representation_description": "Each edge is serialized as a delimited string segment using special tokens &lt;PN&gt; (predecessor node), &lt;E&gt; (edge), and &lt;SN&gt; (successor node). If multiple nodes share the same textual feature string, a disambiguation token &lt;D&gt; followed by a unique integer is appended to node feature strings to make node identities unique. The serialization is injective (has known inverse g^{-1}) so any serialized string can be deserialized back to the original graph.",
            "graph_type": "text graphs; used for molecular graphs (atoms as node strings, bonds as edge strings) and knowledge graphs (triples as nodes/edges) in experiments",
            "conversion_method": "Bag-of-edges style serialization that lists edges in a predefined order; each edge described as '&lt;PN&gt; predecessor_string &lt;E&gt; edge_string &lt;SN&gt; successor_string'; node occurrences that would otherwise be ambiguous are suffixed with '&lt;D&gt;i' disambiguators to ensure injectivity; serialized form stored as a text string and tokenized for LLM training.",
            "downstream_task": "Conditional graph generation: generate graphs conditioned on functional textual descriptions (molecule generation conditioned on valency electron count and QED; also knowledge graph generation from imperative descriptions / WebNLG+ 2020).",
            "performance_metrics": "Used in SGG-LLM which achieved strong results on molecule tasks: e.g., SGG-LLM (no MP) trained on 100k: QED MAE = 0.044 ± 0.011, Valency MAE = 0.060 ± 0.018; SGG-LLM (edge-MP) 100k: QED MAE = 0.036 ± 0.005, Valency MAE = 0.035 ± 0.014. Parsability for fine-tuned SGG-LLM variants ~0.995–1.000. (Metrics from Tables 2/3.)",
            "comparison_to_others": "Outperforms prior bag-of-edges baselines (regen, grapher) on molecule functional matching (lower MAE) when used in SGG-LLM; variants that incorporate message-passing (MP) into the LLM further improve MAE vs the no-MP variant. The paper reports statistically significant improvements over regen and grapher on MAE and diversity (where relevant).",
            "advantages": "Injective/reversible (can deserialize to exact graph), handles node-feature collisions via explicit disambiguation, compatible with pretrained LLM tokenizers (adds a few special tokens), enables training LLMs to output valid graphs with high parsability, and admits interleaving graph-structured processing (MP layers) to improve property matching.",
            "disadvantages": "Can produce long serialized sequences (edge-list grows with number of edges), requires adding special tokens & embedding rows to pretrained LLM, and overall approach depends on computationally expensive fine-tuning of autoregressive LLMs; causal/autoregressive constraints require careful design (causal message passing constraints).",
            "failure_cases": "Without fine-tuning the pretrained LLM on serialized graphs, the model produced 0 parsable outputs. If node disambiguation is not used, serialization is not injective for repeated identical node strings. When MP layers are added but the gating mechanism is omitted, the model failed to generate any parsable graphs (parsability = 0). Small training set size (e.g., 25k) caused very poor Valency MAE for some variants (e.g., SGG-LLM none on 25k: Valency MAE = 1.703 ± 0.074).",
            "uuid": "e8832.0",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "bag-of-edges",
            "name_full": "Bag-of-edges serialization (prior work)",
            "brief_description": "A widely used serialized-graph representation that lists edges as text strings (with delimiters) in a predefined order; used by prior state-of-the-art approaches for text-to-graph generation and implemented as baselines in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "bag-of-edges serialization",
            "representation_description": "Each edge is represented by a text fragment encoding predecessor node string, edge string, and successor node string with special delimiter tokens; edges are listed in a predefined order (prior works sometimes use DFS ordering for molecules), producing a single serialized text describing the whole graph.",
            "graph_type": "Knowledge graphs and molecules (as used in prior work and as implemented in baseline experiments).",
            "conversion_method": "Prior methods typically produce strings like '&lt;PN&gt; predecessor &lt;E&gt; edge &lt;SN&gt; successor' for every edge and concatenate these segments in a fixed order (e.g., depth-first ordering for molecules in SMILES or arbitrary predefined order for KGs).",
            "downstream_task": "Text-to-graph generation (knowledge graph construction, explanation graphs, and molecule generation in baselines).",
            "performance_metrics": "Baselines that used variants of bag-of-edges (reimplemented here) performed worse than SGG-LLM on molecule functional matching: e.g., regen: QED MAE = 0.149 ± 0.018, Valency MAE = 2.282 ± 1.156; grapher: QED MAE = 0.157 ± 0.004, Valency MAE = 1.268 ± 0.229. Parsability for these baselines was ≈0.984–1.000 depending on model.",
            "comparison_to_others": "SGG-LLM variants (which extend bag-of-edges with disambiguation and optionally MP layers and a different training objective) outperform standard bag-of-edges based baselines (regen, grapher) on MAE and diversity for molecular functional targets. The paper attributes part of the improvement to the modified training loss and equal-weighting scheme across examples, and part to joint single-model estimation.",
            "advantages": "Simple, general, and directly tokenizable for LLMs; compatible with many domains; prior success on knowledge-graph benchmarks.",
            "disadvantages": "Ambiguity if nodes share identical textual features (not injective) unless extra disambiguation is added; node features may be repeated multiple times in serialization leading to design complications when incorporating graph-structured inductive biases.",
            "failure_cases": "As implemented originally, bag-of-edges without node disambiguation cannot reversibly map strings to graphs when nodes share identical textual labels (e.g., molecules with multiple identical atom labels), causing incorrect or non-injective deserializations.",
            "uuid": "e8832.1",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SMILES linearization",
            "name_full": "SMILES (Simplified Molecular Input Line Entry System) linearization",
            "brief_description": "A domain-specific linearization for molecules that encodes molecule graphs as a string by traversing bonds (commonly done via depth-first search) producing a compact one-line molecular notation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "SMILES linearization (DFS-based)",
            "representation_description": "Orders atoms/bonds by a depth-first traversal of the molecular graph and serializes chemical structure into a compact string using domain-specific grammar/symbols representing atoms, bonds, rings, and branching.",
            "graph_type": "Molecular graphs (chemoinformatics).",
            "conversion_method": "Depth-first search traversal ordering of bonds/atoms with grammar tokens for atoms, bond types, cycles and branches; produces a canonical or non-canonical string representation of the molecule.",
            "downstream_task": "Molecule generation and chemical property prediction in prior work (mentioned as related serialization technique).",
            "performance_metrics": "Not evaluated experimentally in this paper; mentioned as prior common molecular serialization method.",
            "comparison_to_others": "SMILES is an injective (up to canonicalization) chemistry-specific linearization, contrasted with the more general bag-of-edges approach; the paper notes SMILES uses DFS ordering while prior bag-of-edges lists edges in a predefined order.",
            "advantages": "Compact and chemically-aware; widely used and understood in cheminformatics.",
            "disadvantages": "Domain-specific to chemistry and not directly generalizable to arbitrary text graphs; may not map naturally to LLM tokenization without domain-specific tokens/rules.",
            "failure_cases": "Not evaluated in the experiments of this paper; not applicable when node/edge features are arbitrary text strings outside chemistry.",
            "uuid": "e8832.2",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Node disambiguation (&lt;D&gt;)",
            "name_full": "Node disambiguation token mechanism",
            "brief_description": "A simple mechanism that appends a special token &lt;D&gt; plus a unique integer to node feature strings that are identical, making multiple occurrences distinct in the serialized text so the serialization becomes reversible.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "node disambiguation via &lt;D&gt; index suffix",
            "representation_description": "When multiple graph nodes have identical textual feature strings, append '&lt;D&gt;i' to each instance (where i is a unique index) so that serialized node descriptions are unique and the serialized string can be deterministically deserialized back to the original node identities.",
            "graph_type": "Molecular graphs (atoms with identical element symbols) and any text graphs with repeated identical node labels.",
            "conversion_method": "During serialization, detect nodes with identical feature strings and replace each occurrence 'X' with 'X&lt;D&gt;k' where k is a unique integer for that node instance prior to concatenating edge segments into the serialized graph string.",
            "downstream_task": "Used for graph generation tasks where multiple identical node labels appear (molecule generation in experiments).",
            "performance_metrics": "Enables reversible serialization; used across SGG-LLM and baseline reimplementations to allow molecular datasets to be serialized/deserialized correctly. No standalone numeric metric reported, but necessary for parsability and valid graph recovery.",
            "comparison_to_others": "An augmentation to bag-of-edges/serialized representations; prior work often did not include such explicit disambiguation and thus could not generically serialize graphs with repeated identical node labels.",
            "advantages": "Restores injectivity and allows universal reversible serialization of graphs containing identical node labels; simple to implement and compatible with tokenizers.",
            "disadvantages": "Increases token length slightly and adds synthetic tokens to node labels; requires special-token embedding addition to pretrained LLMs.",
            "failure_cases": "If not used, serialization can be non-injective causing generation/deserialization failures on graphs with repeating node labels (common in molecules).",
            "uuid": "e8832.3",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Edge-graph transformation",
            "name_full": "Edge-graph transformation (treat edges as nodes for MP)",
            "brief_description": "A transformation used to enable message-passing layers inside the LLM: edges of the original graph are treated as nodes in a derived 'edge graph', and adjacency between edges is defined by shared original nodes, with a causality constraint to prevent backward information flow during autoregressive generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "edge-graph for causal message passing",
            "representation_description": "Construct G_edge = (V_edge, E_edge) where V_edge = E (original edges) and E_edge connects e_j and e_k if they share a node in the original graph and index j != k; add constraint that edges always point from earlier-occurring edge to later-occurring edge (k &gt; j) so MP does not transmit information backwards in the serialized token sequence.",
            "graph_type": "Text graphs used in serialization (molecular graphs in experiments).",
            "conversion_method": "Map each serialized edge to a node in the edge graph; use the last token's token embedding for that edge as the node feature for the edge-graph MP layer; build adjacency among edges if they share nodes and enforce directed indexing (earlier→later) to maintain causal generation constraints.",
            "downstream_task": "Incorporated into SGG-LLM message-passing layers to improve conditional graph generation performance (molecule functional matching).",
            "performance_metrics": "SGG-LLM variant using edge-based MP (100k training) achieved: QED MAE = 0.036 ± 0.005, Valency MAE = 0.035 ± 0.014; parsability ≈ 0.998–0.999. Outperformed no-MP variant on MAE in experiments.",
            "comparison_to_others": "Edge-graph MP variant outperformed SGG-LLM without MP and outperformed prior baselines (regen, grapher) on MAE; also compared to correspondence-MP variant which had similar but slightly different performance depending on metric and dataset.",
            "advantages": "Permits exchange of structural information between tokens while respecting autoregressive causal constraints; empirically improved functional property matching (lower MAE).",
            "disadvantages": "Requires construction of auxiliary graph structure and mapping indices between serialized tokens and edge-graph nodes; increases model complexity and computation; needs a gating mechanism when injecting MP outputs back into token representations.",
            "failure_cases": "If MP outputs are incorporated without the learned gating (tanh a) the model failed to produce parsable outputs (parsability = 0). Also careful edges-direction constraints are required to avoid passing information backwards in the token sequence.",
            "uuid": "e8832.4",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Correspondence graph MP",
            "name_full": "Correspondence-graph-based message passing (MP)",
            "brief_description": "An alternative MP design where each instance of a node in the serialized sequence is treated as its own node in a correspondence graph, and edges connect neighboring occurrences that correspond to the same original node; edges are directed from earlier to later occurrences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "correspondence-graph message passing",
            "representation_description": "Construct a correspondence graph whose nodes are each occurrence of a graph node in the serialization; connect instances of the same original node when they appear adjacent in the serialized sequence, with directed edges from earlier instance to later instance to keep message passing causal.",
            "graph_type": "Text-serialized graphs where a node may appear multiple times in the serialized token sequence (common in bag-of-edges serialization).",
            "conversion_method": "Treat each textual occurrence of a node in the serialized string as a distinct node; add directed edges between adjacent occurrences that refer to the same original node; use the token embeddings for each occurrence as correspondence-graph node features and apply MP (GraphSAGE) constrained by causality.",
            "downstream_task": "Used as an MP variant in SGG-LLM for conditional graph generation (molecular experiments).",
            "performance_metrics": "SGG-LLM correspondence MP (100k) achieved: QED MAE = 0.039 ± 0.007, Valency MAE = 0.045 ± 0.017; parsability ≈ 0.995–0.998. Performed comparably to edge-MP variant; in some cases correspondence variant achieved highest diversity/MAE tradeoffs.",
            "comparison_to_others": "Comparable performance to edge-graph MP variant; both MP variants outperformed no-MP SGG-LLM and prior baselines. The paper reports that two MP variants outperform the variant without MP.",
            "advantages": "Leverages multiple occurrences of the same original node in the serialized sequence to propagate structural information while preserving causal ordering; empirically effective.",
            "disadvantages": "Requires extra graph construction and bookkeeping of occurrences; complexity increases; must ensure edges are directed to avoid backward information flow.",
            "failure_cases": "None specifically called out beyond general MP-related constraints; success requires gating when integrating MP outputs (same gating caveat as edge-MP).",
            "uuid": "e8832.5",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Last-token aggregation",
            "name_full": "Last-token embedding aggregation for node/edge vector construction",
            "brief_description": "A pragmatic method to obtain a single vector representation for a node or edge that is described by multiple tokens in the serialized sequence by selecting the feature vector of the last token describing that node/edge (which, due to causal attention, contains information from the preceding tokens).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "last-token aggregation",
            "representation_description": "For each node or edge described by a sequence of tokens in the serialization, take the LLM token embedding of the last token in that description as the single dense representation for that element; for edges, map edge index to last-token index via f_index and extract d_{f_index(k)}_{G,F}.",
            "graph_type": "Text-serialized graphs where nodes/edges are represented by multiple tokens in the serialized sequence.",
            "conversion_method": "Parse the serialized token sequence, identify the final token position that completes a node/edge's textual description, and use that token's LLM feature vector as the aggregate representation for message passing inputs.",
            "downstream_task": "Used to construct node/edge feature matrices for MP layers interleaved in the LLM during conditional graph generation.",
            "performance_metrics": "Used in all SGG-LLM MP variants; necessary for MP-enabled variants that achieved low MAE (see SGG-LLM edge/correspondence MAE values). No standalone numeric metric reported.",
            "comparison_to_others": "Selected because the last token's causal-attention context naturally includes preceding tokens describing the element; alternative aggregations (e.g., mean pooling) are not evaluated in the paper.",
            "advantages": "Simple to implement and consistent with autoregressive (causal) transformer semantics; avoids looking ahead and therefore respects generation-time information constraints.",
            "disadvantages": "If an element appears earlier in the serialization multiple times, care must be taken which occurrence to aggregate (hence correspondence/edge-graph constructions are used).",
            "failure_cases": "Not reported to fail by itself; correct use depends on respecting causality (must select the last token per occurrence and ensure MP connections don't allow backward-flow).",
            "uuid": "e8832.6",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MP gating (tanh a)",
            "name_full": "Learned tanh gating for injecting MP outputs into token embeddings",
            "brief_description": "A learned gating scalar parameter a whose tanh is multiplied element-wise with MP layer outputs before adding back into token feature vectors; crucial for stable fine-tuning when interleaving MP layers into LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "tanh gating for MP output integration",
            "representation_description": "After computing MP outputs on the edge/correspondence graph, multiply the MP output vector element-wise by tanh(a) (a is a learned vector initialized to 0) and add the gated vector to the corresponding token embedding (specifically the token immediately after the edge description).",
            "graph_type": "Applied within serialized text-graph processing for molecular and knowledge-graph generation.",
            "conversion_method": "During each interleaved MP step, compute MP output d_k_{V_edge,F} and update the serialized token feature d_t_{G,F} as: d_t_{G,F} := d_t_{G,F} + d_k_{V_edge,F} * tanh(a) when the token t is the position immediately after the edge description (f_index(k) = t-1).",
            "downstream_task": "Stabilizes fine-tuning of LLMs with interleaved MP to generate serialized graphs conditioned on text (molecule functional generation).",
            "performance_metrics": "The ablation without gating (SGG-LLM with edge-based MP but without gating) produced zero parsable outputs on molecule test sets (parsability = 0). With gating, MP variants achieved high parsability (~0.998) and improved MAE (see edge-MP MAE values).",
            "comparison_to_others": "The paper reports that gating was necessary to succeed; without it MP prevents convergence and yields no parsable samples. Inspired by similar techniques in multimodal LLM integration literature.",
            "advantages": "Permits the model to gradually incorporate structural MP signals during fine-tuning; prevents destabilizing the pretrained LLM weights at early stages by starting with near-zero injected signal.",
            "disadvantages": "Adds extra learnable parameters and a design hyperparameter; if omitted, training fails for MP-enabled variants.",
            "failure_cases": "Omitting the gating term caused the MP-interleaved models to produce zero parsable outputs in molecule generation experiments.",
            "uuid": "e8832.7",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Regen: Reinforcement learning for text and knowledge base generation using pretrained language models",
            "rating": 2,
            "sanitized_title": "regen_reinforcement_learning_for_text_and_knowledge_base_generation_using_pretrained_language_models"
        },
        {
            "paper_title": "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.",
            "rating": 2,
            "sanitized_title": "smiles_a_chemical_language_and_information_system_1_introduction_to_methodology_and_encoding_rules"
        },
        {
            "paper_title": "Inductive representation learning on large graphs",
            "rating": 2,
            "sanitized_title": "inductive_representation_learning_on_large_graphs"
        },
        {
            "paper_title": "Knowledge graph generation from text",
            "rating": 2,
            "sanitized_title": "knowledge_graph_generation_from_text"
        },
        {
            "paper_title": "Explanation graph generation via pretrained language models: An empirical study with contrastive learning",
            "rating": 2,
            "sanitized_title": "explanation_graph_generation_via_pretrained_language_models_an_empirical_study_with_contrastive_learning"
        },
        {
            "paper_title": "WebNLG+ 2020",
            "rating": 2,
            "sanitized_title": "webnlg_2020"
        }
    ],
    "cost": 0.018820499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FORM FOLLOWS FUNCTION: TEXT-TO-TEXT CONDI-TIONAL GRAPH GENERATION BASED ON FUNCTIONAL REQUIREMENTS
1 Nov 2023</p>
<p>Peter A Zachares 
OATML Oxford University</p>
<p>Vahan Hovhannisyan 
OATML Oxford University</p>
<p>Alan Mosca 
OATML Oxford University</p>
<p>Yarin Gal yaringal@gmail.com 
OATML Oxford University</p>
<p>FORM FOLLOWS FUNCTION: TEXT-TO-TEXT CONDI-TIONAL GRAPH GENERATION BASED ON FUNCTIONAL REQUIREMENTS
1 Nov 202381DD060E46B763E5E6386F5C9AE7B6E4arXiv:2311.00444v1[cs.LG]
This work focuses on the novel problem setting of generating graphs conditioned on a description of the graph's functional requirements in a downstream task.We pose the problem as a text-to-text generation problem and focus on the approach of finetuning a pretrained large language model (LLM) to generate graphs.We propose an inductive bias which incorporates information about the structure of the graph into the LLM's generation process by incorporating message passing layers into an LLM's architecture.To evaluate our proposed method, we design a novel set of experiments using publicly available and widely studied molecule and knowledge graph data sets.Results suggest our proposed approach generates graphs which more closely meet the requested functional requirements, outperforming baselines developed on similar tasks by a statistically significant margin.</p>
<p>INTRODUCTION</p>
<p>Many concepts can be described by graphs; including molecules (19), abstract syntax trees (18), knowledge graphs (26), and project schedules (17).Each of these concepts is used in downstream tasks where graph structure has a direct impact on task performance.For example, some molecules can be used as medicine for a disease while others cannot and this is partially determined by the molecules' graphs.It would be useful if we could describe the functional requirements of a graph using natural language and query a model to conditionally generate a graph which meets these requirements.For example, a model using the prompt "generate a molecule with 47 valency electrons" would generate a valid molecule graph with 47 valency electrons.With such models we could speed up and improve the processes of drug discovery, software development, project management as well as many other applications of graphs.</p>
<p>In this work, we focus on the problem of generating graphs where node and edge features are strings of text and term this type of graph a text graph.Text graphs are a fairly flexible data format and can be used in most applications of graph-structured data including those listed above.To the best of our knowledge, text graph generation conditioned on text has only been studied in the fields of knowledge graph generation (8; 12; 13; 25; 26; 38) and explanation graph generation (32).In both setups the conditional text explicitly describes the graph.These explicit descriptions give an imperative "recipe" with step-by-step instructions of how to generate the graph.Contrary to giving explicit imperative descriptions, this work focuses on the case where the conditional text is a functional description of the graph, specifically for a downstream task.Additionally, in the case of imperative conditional text, there is an injective mapping between conditional text and the graph that should be generated, which means that methods can be evaluated by comparing the generated graph to the ground truth graph in terms of its structure.By contrast, many graphs might correspond to the same functional description and hence a new experimental design is required to study the problem.</p>
<p>Preprint</p>
<p>We propose solving this problem by fine tuning a large language model (LLM) to generate a serialized text description of a graph.This is motivated by the strong performance pre-trained LLMs have shown when fine-tuned to perform a specific task (6; 16; 33), including the task of text-to-serialized graph generation (8).However, prior work on serialized graph generation does not generalize to all domains containing text graph structured data.To further this approach, we propose a serialization method which is expressive enough to reversibly serialize any graph and a method to incorporate graph structure into the LLM's generation process motivated by the observation that the structures of generated graphs have a direct impact on their functional properties and consequently on model performance.While LLMs are probably expressive enough to learn to incorporate graph structure into their generation process, it is more efficient to provide the model with the ability to do so.However, it is also a non-trivial challenge because modern LLMs perform autoregressive sampling to generate text (4; 5; 6; 16; 27; 30; 33).This involves a multi-step sequential process, where at each step a forward pass is performed and then a new token is sampled.A token is an atomic unit of text like 'dog', ' ', or 'ch'.The generation process exhibits high computational complexity and cost due to the requirement of performing a forward model pass for each generated token.Consequently, LLMs are typically trained to generate text without actually performing generation at training time.Only a single forward pass is performed at each training step without sampling and the entire sequence (conditional plus desired text) is fed into the LLM for that forward pass.As such, an LLM has access to the full sequence at training time, while at generation time it only has access to the conditional text and the part of the sequence that has already been generated.This means that during training an LLM could learn to rely on information that it will not have access to at generation time.This issue is overcome by using causal masking, which guarantees the LLM cannot pass information backwards in the token sequence ensuring that generation conditions are simulated at training time.</p>
<p>In Section 3, we propose a method of incorporating graph structure into an LLM's generation process which passes information between tokens in a sequence.Our key contribution is demonstrating how to do so without passing information backwards in the sequence.Specifically, we propose to extend LLMs to process and generate sequences of text and graphs.For this we provide the LLM with the ability to deserialize graphs incorporated into input token sequences and introduce message passing layers into the LLM to calculate representations of graph structure in a manner that is conducive to autoregressive generation.</p>
<p>In this work, we specifically focus on the problem of generating text graphs from functional requirements within the range of those seen at training time (interpolation), instead of extrapolating outside the training set's domain, which we leave as future work.We propose a novel experiment design to evaluate methods in this new problem setting using the publicly available data sets WebNLG+ 2020 (10) and PCQM4M (19).Results suggest that our proposed model outperforms previous work on text graph generation conditioned on text by a statistically significant margin.And that our proposed approach for incorporating graph structure into the language model's generation process leads to models which generate examples which on average more closely meet their conditional functional requirements.All code used to perform experiments is publicly available at ...</p>
<p>PRELIMINARIES</p>
<p>A text graph G = {V, E} is composed of a set of N nodes v i ∈ V each with an associated text string describing the node and a set of M directed edges (v i , v j ) ∈ E each with an associated text string describing the edge.Each text graph G is associated with text D f containing a functional description of G. Our goal is to train a model to accurately predict p θ (G|D f ) with parameters θ, describing the distribution over text graphs G conditioned on a specific functional description D f .Importantly, this distribution may be multimodal as multiple graphs may have the same functional properties.This work focuses on the case where the parameterized model p θ (•) is a language model that generates a serialized text description of the graph.Hence, if D G is the serialized graph, then the language model predicts p θ (D G |D f ).This requires an injective serialization function g : G → D G with a known inverse mapping g −1 : D G → G from the serialized description D G back to the graph G such that G = g −1 (g(G)).g(•) is used at training time to transform graphs G into serialized graphs D G and the deserialization function is used at generation time to recover the graph G from the serialized graph D G (see Figure 1 top row).With this approach, we pose the problem of generating graphs Prior state-of-the-art work on serialized graph generation uses a bag-of-edges approach (8; 32), which describes each edge using a string with special tokens for delimiting graph elements and then lists out the edges in a predefined order.For example, the popular smiles molecule serialisation algorithm uses depth-first search traversal (39) to order chemical bonds (edges) in a molecule.Then each edge is described with the syntax <PN>predecessor node feature string<E>edge feature string<SN>successor node feature string, where <PN>,<E>,<SN> are special tokens used to delimit the different components of the edge.</p>
<p>Using LLMs, a description D is represented in three ways: 1) as a string (text), 2) as a sequence of token indicators, and 3) as a sequence of feature vectors / embeddings.To distinguish between these three representations, we use D S to denote the string representation, D T to denote the sequence of token indicators and D F to denote the sequence of feature vectors.For the serialized graph D G , its string representation is denoted by D G,S , its sequence of token indicators representation by D G,T and its sequence of feature vector representation by D G,F .</p>
<p>METHOD</p>
<p>The datasets LLMs are pre-trained on do not contain much, if any, data on generating serialized graphs (4; 6; 30; 33).Consequently, it is unlikely they could generate text graphs without further training.We provide evidence supporting this hypothesis in experiments where we evaluate a pretrained LLM without further fine-tuning.As shown in the results in Appendix D, an LLM without fine-tuning did not generate a single valid serialized graph.Hence, we propose fine-tuning an LLM on a training and validation set of (functional description, graph) pairs to generate serialized graphs conditioned on functional descriptions.While LLMs are probably expressive enough to learn to incorporate graph structure into their generation process, it is more efficient to provide the model with the ability to do so.State-of-the-art models for performing inference on graph structured data use message passing (MP) layers (14; 35).As such, we propose interleaving MP layers between language modelling layers in an LLM (Figure 1 block A).Any message passing layer can be used with our proposed approach1 .</p>
<p>FINE-TUNING OBJECTIVE</p>
<p>For fine-tuning, we propose minimizing the negative log-likelihood of serialized graph sequences D G as described in equation ( 1):
Loss = 1 n batch n batch i=1 − log p θ (d 1,i G,T |D i f ) + n i seq j=2 − log p θ (d j,i G,T |d 1:(j−1),i G,T , D i f )(1)
where
d i,j G,T ∈ D i G,T
, the superscript i is the index in a batch, the superscript j indicates the position of an element d i,j G,T in the sequence D i G,T , n batch is the number of elements in the batch, and n i seq is the number of elements in the sequence of example i in the batch.This objective differs from the traditional LLM fine-tuning objective of maximizing the average likelihood of the next token for each token in a batch of sequences as described in equation 2;
Loss = 1 n batch i=1 n i seq n batch i=1 − log p θ (d 1,i G,T |D i f ) + n i seq j=2 − log p θ (d j,i G,T |d 1:(j−1),i G,T , D i f )(2)
The difference between the two objectives is the red term in both equations.We can view the reciprocal of this term as a weight placed on the importance of an example in the training set with respect to the training objective.For the standard training objective in equation 2, the term
n batch i=1
n i seq in red changes from batch to batch, because batches contain randomly sampled examples which differ in A comparison in the performance of models trained with these objectives can be seen in Tables 2 and  3.The baseline regen only differs from our proposed model SGG-LLM without message passing in that it was trained using equation 2 instead of 1.We also report additional experiments shown Appendix D which suggests what is causing the difference in performance.</p>
<p>NODE DISAMBIGUATION DURING SERIALIZATION</p>
<p>One special case of graph-structured data, which is not handled by prior work on generating serialized graphs, is when a subset of nodes in a graph are described by the same feature string2 .However, this special case occurs in many domains, such as molecule data.For example, the molecule with multiple carbon atoms depicted in Figure 2 top row.To address this issue, we add a disambiguation token <D> to the feature strings of nodes with identical feature strings followed by a unique integer.An example of node disambiguation is shown the bottom row of Figure 2, where the three carbon atoms' feature strings are modified from C, C, C to C<D>0, C<D>1, C<D>2.With the addition of a disambiguation token, the serialization method becomes expressive enough to reversibly serialize any graph.In conjunction with this serialization function and special token <D>, we also add special tokens <PN> (predecessor node) <E> (edge) and <SN> (successor node) to the tokenizer of the pre-trained LLM, as well as additional rows to the embedding layer of the LLM for all four special tokens before fine-tuning.See our publicly available code for implementations of the proposed serialization g(•) and deserialization functions g −1 (•).</p>
<p>MESSAGE PASSING WITHIN A LANGUAGE MODEL</p>
<p>Our proposed architecture requires passing information from an LLM layer to an MP layer, then to the following LLM layer, and so on as shown in block B in Figure 1.As such, we need to convert the serialized graph representation outputted by an LLM layer into a representation of the graph that can be fed into an MP layer and vice versa.The inputs to an MP layer are node feature vectors D V,F ∈ R N ×H for each node in a graph, where H is the dimensionality of a feature vector, and the graph's adjacency matrix A ∈ R N ×N .An obvious choice for constructing D V,F would be simply Preprint using the feature vectors of D G,F .However, in D G,T , each node in the graph is described by multiple tokens and consequently multiple feature vectors in D G,F .For example, in Figure 2, the predecessor node of edge 1 is described by four tokens [<PN>, C, <D>, 0].We can calculate a single node vector from its multiple token feature vectors in D G,F by selecting the feature vector of the last element describing the node in D G,F because the last vector contains information about all previous tokens in the sequence including all those describing the node because of the causal attention layers in an LLM.</p>
<p>In addition, if a node has a degree greater than one, it will be described more than once in D G3 .While a node may occur more than once in a serialized graph, using our proposed serialization method, an edge will only occur once.In a graph G = {V, E}, let there be a set e k for each edge (v i , v j ) ∈ E where the superscript k indicates the position of the edge in D G and the set contains the two nodes the edge connects {v i , v j } ∈ e k .For the graph G and graph serialization D G pair depicted in Figure 2, e 2 would be the set of nodes {C<D>1, N} contained in edge 2 because edge 2 is the second edge (k = 2) described in the serialization D G .As a slight abuse of notation, we denote elements of a graph's edge set E either by (v i , v j ) or by e k .Hence, we define a new graph G edge = {V edge , E edge } where the edges in the original graph are treated as nodes V edge = E and nodes in the original graph define edges in the new graph E edge = {(e j , e k )|e j , e k ∈ E ∧ e j ∩ e k ̸ = ∅ ∧ j ̸ = k}.We call this new graph an edge graph.Figure 2 shows an example of transforming a graph into its edge graph where the four edges in the original graph become the nodes in the edge graph.Let f index : Z + → Z + be a mapping from the index of an edge in the sequence D V edge ,F to the index of the last token describing the edge in D G,F .To construct the sequence of feature vectors for the nodes in an edge graph G edge , for each MP layer we use the last token's feature vector of each edge in the serialized graph, so that
d k V edge ,F = d f index (k) G,F
.</p>
<p>MP layers pass information between nodes in a graph by aggregating feature vectors from neighboring nodes.To ensure that an MP layer does not pass information backwards in D G,F , the MP layer should only aggregate information for a node from nodes that are earlier in the sequence as depicted in causal graph attention in block D in Figure 1.causal attention, shown in block C in Figure 1, only attends to elements previous to the query element in the sequence.graph attention only attends to elements in the sequence which are neighboring nodes to the query element in the sequence based on a reference graph.causal graph attention respects both of these constraints.We propose MP layers to pass information on a graph's edge graph.To ensure information is not passed backwards in D G,F we must add an additional constraint when constructing the edge set of an edge graph which is ∀(e j , e k ) ∈ E edge , k &gt; j.This constraint has been applied to the edge graph shown in Figure 2.</p>
<p>After passing an edge graph through an MP layer, we add each resultant edge feature vector to the token feature vector immediately after its description in the serialized graph as described in equation 3;
d t G,F = d t G,F + d k V edge ,F tanh a, if f index (k) = t − 1; d t G,F , otherwise.(3)
During the development of this method, we found it necessary to multiply element-wise the output of an MP layer by a gating term tanh a when incorporating the output back into D G,F as shown in equation 3. a is a learned gating parameter of the MP layer initialized to 0. Without the gating term, we could not fine-tune an LLM with message passing incorporated to generate valid graphs as demonstrated by the results shown in Appendix D. We hypothesize this is because the gating term allows an LLM to gradually learn to take into account graph structure.Without it, the model most likely starts fine-tuning at a position on the loss landscape from which it cannot converge to a useful local minima.The use of the gating term is inspired by the work of (1), which faced a similar issue.</p>
<p>Preprint 4 RELATED WORK 4.1 INCORPORATING OTHER MODALITIES INTO TEXT GENERATION</p>
<p>This work proposes incorporating graph structure, an additional modality, into an LLM's text generation process (where the generated text is a serialized graph).There have been many works on incorporating other modalities into the text generation process.Speech-to-text can be posed as a sequence to sequence problem (15) and so similar methods to those used for text-to-text generation have been used to incorporate the modality of sound into language generation (2; 29; 37).Another widely studied problem is image-to-text generation, where most works focus on calculating dense sequence representations of images (20; 21; 23; 28; 40; 1) to pose image-to-text generation as a sequence-to-sequence generation problem as well.From this field of work, our proposed method is inspired by (1) which incorporates image information into the text generation process by representing sequences of images as sequences of dense vectors and interleaving special layers for combining the modalities of image and text between language modelling layers in an LLM.Inspired by this approach, we propose interleaving additional layers in-between language modelling layers in a pretrained LLM and incorporating additional tokens for representing the other modality.Unlike (1), we incorporate the modality of graphs as opposed to images into the generation process, and use a modified LLM to generate graphs rather than text.</p>
<p>GENERATING GRAPHS CONDITIONED ON TEXT</p>
<p>In the field of natural language processing, there have been many works on parsing natural language into syntax trees, some of which pose the problem as a serialized graph generation conditioned on text (9; 36).However, these works use a domain specific serialization method and do not incorporate graph structure into their proposed models' generation processes.There is one recent work (32) focused on generating explanation graphs from text which proposes a contrastive learning objective for fine-tuning the LLM T5 (30) to generate graphs, however the contrastive objective requires additional labels created by an expert.There is also a recent work published on generating protein graphs conditioned on text (24), however it cannot generalize past the domain of protein graph generation because the proposed method generates sequences of characters describing amino acid sequences.</p>
<p>Most prior work on text to graph generation focuses on generating knowledge graphs from text (8; 12; 13; 25; 26; 38).The best performing method on benchmark tasks is regen (8), which uses the LLM T5 (30) to generate knowledge graphs and is fine-tuned using the standard objective of predicting the next token in a sequence (equation 2).The second best performing method on benchmark knowledge graph generation tasks is grapher (26) which uses a combination of T5 and a recurrent neural network to generate graphs.This method differs from ours in the use of an additional model besides the LLM to predict edges and generate edge features as well as a training objective similar to equation 2.</p>
<p>EXPERIMENTS</p>
<p>Prior work on text graph generation conditioned on text does not provide an experimental design for evaluating methods that generate graphs based on functional descriptions.To do so, a dataset containing (functional requirements, graph) pairs is required for a task where it is possible to automatically calculate the functional properties of newly generated graphs.To generate such a dataset, we took the open graph benchmark large scale molecule dataset PCQM4M (19) and used the open source Python package rdkit (34) to generate two functional descriptions for each molecule: number of valency electrons in it and quantitative estimated-likeness (QED) (3).These are commonly used functional properties of a molecule's graph and are described in more detail in Appendix C.</p>
<p>DATASETS</p>
<p>For evaluation we created two datasets, each dataset is composed of all the molecules in the original PCQM4M (19)  called Valency dataset.The other dataset contains (QED functional description, graph) pairs and is called QED dataset.To evaluate the impact of amount of data on model performance, we created three subsets for each of the datasets.Each subset had 1000 randomly selected molecules in its test and validation sets.Importantly, these were the same across the three subsets and then the training sets of the three subsets were another 25, 000, 100, 000 and 400, 000 randomly chosen examples.</p>
<p>The training set of the smaller subsets was contained in the training set of the larger subset sets.See Figure 2 for examples of input output pairs for the constructed data sets.</p>
<p>METRICS AND EVALUATION</p>
<p>To evaluate models, we calculated three metrics on the test sets of the datasets described above: parsability, diversity, and most importantly mean absolute error (MAE) with respect to the conditional functional property.A generated graph is parsable if no error is thrown when calculating its functional property.If a molecule is parsable (i.e. it follows correct serialization syntax and doesn't violate basic laws of physics), it is given a score of 1, otherwise 0. In all experiments we use the following metrics:</p>
<p>• Parsability is the mean parsability score of samples in the test set.</p>
<p>• MAE is the mean absolute error between the generated graph's functional property value and the requested property value averaged over samples in the test set.To interpret the magnitudes reported for MAE results see Table 1 describing some summary statistics about functional properties of graphs in the datasets used in experiments.</p>
<p>• Diversity is a measure of the multimodality of the distribution p θ (D G |D f ) that the LLM learns.A sampled graph is assigned a diversity score of 1, if it, a second sampled graph and the ground truth do not share the same node and edge sets; and is assigned a score of 0 otherwise.Diversity is the average of diversity scores of samples in the test set.</p>
<p>CURRENT STATE-OF-THE-ART</p>
<p>We implemented current state-of-the-art models from prior work as baselines: specifically we implemented the version of grapher (26), which generates edge features instead of classifying them and regen (8).We had to make two modifications to both approaches so they could generalize to molecule data: we added node disambiguation from Section 3.2 to their serialization methods and updated their language models to a more recent model BLOOM (33) 100,000 1.000 ± 0.000 0.999 ± 0.001 0.845 ± 0.017 0.506 ± 0.031 SGG-LLM edges 100,000 0.998 ± 0.001 0.999 ± 0.000 0.836 ± 0.029 0.540 ± 0.016 SGG-LLM correspondences 100,000 0.995 ± 0.001 0.998 ± 0.000 0.839 ± 0.008 0.608 ± 0.054 SGG-LLM none 25,000 0.997 ± 0.002 0.991 ± 0.003 0.799 ± 0.008 0.518 ± 0.078 SGG-LLM none 400,000 0.998 ± 0.001 1.000 ± 0.000 0.857 ± 0.006 0.542 ± 0.051 grapher N/A 100,000 1.000 ± 0.000 1.000 ± 0.000 0.745 ± 0.038 0.410 ± 0.045 regen N/A 100,000 0.984 ± 0.008 0.991 ± 0.007 0.854 ± 0.031 0.446 ± 0.124 Table 3: Model performance in terms of parsability and diversity on QED and Valency datasets.The first, second, and third best performing models are highlighted using the colors shown here.</p>
<p>Experiments were repeated three times to estimate standard error.Models below the boldface line are from prior work.All models achieved a parsability near 1.0.</p>
<p>experiments by our proposed approach.See Appendix A for a description of the training process used to train grapher, regen and variants of our proposed approach.</p>
<p>RESULTS</p>
<p>On the task of generating graphs to meet functional requirements, the ideal model can generate a diverse set of parsable graphs with functional properties equal to the requested functional property.Tables 2 and 3 describe the results of our proposed approach on the QED and Valency datasets.Our proposed approach is referred to as SGG-LLM standing for serialized graph generator large language model.All variants of our approach and the baselines grapher, and regen achieve a parsability score near or at 1.The SGG-LLM variant with correspondences message passing (described in Appendix B) is another method of incorporating the MP layers into an LLM by passing messages in D G based on node correspondences rather than edges.</p>
<p>Results in table 2 suggest that all variants of SGG-LLM outperform baselines by a statistically significant margin, using the unpaired t-student test and a threshold p-value of 0.05, in terms of generating examples with functional properties close those requested.In addition, results suggest that all variants of SGG-LLM outperform grapher by a statistically significant margin in terms of generating a diverse set of candidates on the Valency dataset.Finally, the two variants of SGG-LLM that use MP layers outperform the variant that does not utilise MP layers.</p>
<p>The high performance of all variants of our approach over regen suggest the importance of the proposed loss function and of weighting tokens equally across batches during training.The high performance of all variants of our approach over grapher could be for the same reasons as regen, as it is trained with a similar loss, or it could be that single model which jointly estimates the existence of nodes, edges, and their features is more effective at generating graphs than a dual module model.</p>
<p>To demonstrate the generality of our proposed approach beyond generating molecules, we also evaluate it on the benchmark knowledge graph generation dataset WebNLG+ 2020 (10) using a different language model T5 (30).Note this task is generating graphs conditioned on an imperative description of the graph, so is not directly linked to the focus of this paper.See Appendix E for a description of the experiments and discussion of the results.</p>
<p>DISCUSSION</p>
<p>The main limitation of our proposed approach is its reliance on fine-tuning a pre-trained autoregressive LLMs, which are computationally expensive, slow at generation time and require substantial computational resources even to fine-tune.This limitation would become even more difficult when applying this method to tasks containing larger graphs or graphs containing elements with long feature strings.Hence an interesting next step from this method would be using quantized pre-trained models and low rank adapters on LLM layers for more efficient fine tuning (7).n i seq ] which is the expected value of the differing term in equation 2 from equation 1.This is to determine whether it is the magnitude of the differing term causing the difference in performance or the fact that n batch i=1 n i seq changes from batch to batch.In experiments, some summary statistics for the term n batch i=1 n i seq in equation 2 were mean = 4053, max = 4676, median = 4060, and inter-quartile range = 3915 − 4199.There was a marginal difference in performance in terms of all three metrics when comparing the performance of SGG-LLM w/ special loss and SGG-LLM without message passing.The marginal difference in performance suggests that the changing weighting between batches and across epochs is what hurts the performance of models trained with the objective described in equation 1. regen is a model trained with equation 1.
Preprint</p>
<p>Model</p>
<p>E RESULTS OF KNOWLEDGE GRAPH GENERATION EXPERIMENTS</p>
<p>To demonstrate the generality of our proposed approach beyond generating molecules, we also evaluate it on the benchmark knowledge graph generation dataset WebNLG+ 2020 (10) using a different language model T5 (30).Like in prior work, model performance is evaluated using the</p>
<p>Figure 1 :
1
Figure 1: At generation time (top) the input to the model is the graph's functional description D f and the output is a serialised description of the graph D G .During training time (below top) the input is a functional description, serialized graph pair (D f , D g ); the output is the probability p(D g ).Graph serialization method g(•) is used to serialize input graphs before training, while the deserialization method g −1 (•) is used to deserialize the generated serialized graph D g as well as to help perform message passing within the LLM.Block A describes the proposed architecture interleaving message passing layers between LLM layers.Block B depicts how information is passed between LLM and MP layers.The bottom visualization depicts the masked matrices used in the types of attention performed within LLM and message passing (MP) layers.</p>
<p>Figure 2 :
2
Figure 2: Top: the correspondence between a graph and its edge graph.Middle: the graph's bag-of-edges serialisation with special tokens <PN>, <SN>, <E> and <D>.Bottom: Examples of functional requirements used for condition graph generation for the molecule above.</p>
<p>Table 1 :
1
Summary statistics about functional requirements in datasets used in experiments.The standard deviations reported for both data sets provide context for evaluating Table2below.
PreprintMinimum Maximum MeanStandard DeviationMedianInterquantile RangeNumber of Valency Electrons212277.313.38070 -88QED0.060.980.7640.1330.780.68 -0.88Model Message Passing Training Set SizeMean Absolute Error QED ValencySGG-LLM none100,0000.044 ± 0.011 0.060 ± 0.018SGG-LLM edges100,0000.036 ± 0.005 0.035 ± 0.014SGG-LLM correspondences100,0000.039 ± 0.007 0.045 ± 0.017SGG-LLM none25,0000.062 ± 0.008 1.703 ± 0.074SGG-LLM none400,0000.020 ± 0.001 0.076 ± 0.034grapher N/A100,0000.157 ± 0.004 1.268 ± 0.229regenN/A100,0000.149 ± 0.018 2.282 ± 1.156
dataset with less than 20 atoms, which is still more than 2 million examples.One of the two datasets contains (number of valency electrons functional description, graph) pairs and is</p>
<p>Table 2 :
2
Model performance in terms of MAE on QED and Valency datasets.The first, second, and third best performing models are highlighted using the colors shown here.Experiments were repeated three times to estimate standard error.All fine-tuned variants of SGG-LLM outperform baselines by a statistically significant margin.Models below the boldface line are from prior work.</p>
<p>, which is the same LLM used in
PreprintModelMessage PassingTraining Set SizeQEDParsability ValencyQEDDiversity ValencySGG-LLM none</p>
<p>(3)&gt;0 edge 3 in the correspondence graph.Edges are constructed by connecting nodes in the correspondence graph which correspond to the same node in the original graph i.e. in the graphic C<D>0 edge 1 is connected to C<D>0 edge 3 because they refer to the node C<D>0 in the original graph.Nodes in the correspondence graph are only connected if they are adjacent occurrences in the serialized graph.By constructing edges such that they always point from an earlier instance of a node to a later instance we ensure the message passing layer does not pass information backwards in the serialized graph sequence at training time.As additional method, we propose incorporating MP layers into an LLM where the MP layers pass information based on a graph's correspondence graph.C FUNCTIONAL DESCRIPTIONS OF MOLECULESWe used two functional descriptions of molecules to generate datasets for our experiments -number of valency electrons and QED.The number of valency electrons in a molecule is the sum of the number of valency electrons in its atoms.Importantly, this property is a result of only a graph's node composition, but not its full structure.To generate descriptions of a functional property of a graph's full structure, we calculate a metric called quantitative estimated drug-likeness (QED)(3).QED quantifies multiple properties of a molecule which are attractive for applications in drug design and then calculates a single metric from these properties using a weighted logarithmic sum.The properties used to calculate QED are molecular weight, octanol-water partition coefficient, topological polar surface area, number of hydrogen bond donors and acceptors, the number of aromatic rings and rotatable bonds, and the presence of unwanted chemical functionalities.For experiments we set the weight of the properties: molecular weight, number of hydrogen bond donors and acceptors, and presence of unwanted chemical functionalities to zero in the QED calculation, because they are mainly determined by node composition instead of the entire graph structure.The functional descriptions generated were of the format "a molecule with number of valence electrons equal to ..." and "a molecule with a weighted quantitative estimation of drug-likeness equal to ...".D ADDITIONAL RESULTS OF MOLECULE GENERATION EXPERIMENTSBelow, we provide three tables describing the full results of experiments on the molecule data sets including ablation studies to empirically justify some design choices in our proposed method.Our proposed model is referred to as SGG-LLM.There are three additional models in the tables below; 1) SGG-LLM w/out fine-tuning, 2) SGG-LLM w/ special loss, and 3) SGG-LLM with edge based message passing without a gating term.The table below describing performance of models in terms of the parsability of generated examples shows that SGG-LLM w/out fine-tuning and SGG-LLM with edge based message passing without a gating term both did not produce a single parsable example on molecule datasets' test sets.These results suggests that both fine-tuning and a gating term (when incorporating message passing into an LLM) are required to achieve good performance with our proposed method.Note if a model cannot generate parsable examples, then the metric mean absolute error cannot be calculated and the diversity of generated examples is not a useful measure of model performance.Consequently, we do not report mean absolute error or diversity for SGG-LLM w/out fine-tuning and SGG-LLM with edge based message passing without a gating term.The model SGG-LLM w/ special loss is a version of SGG-LLM without message passing that was trained using equation 1, but instead of letting n batch = batch size as proposed in section 3.1, we equal n batch = E[
PreprintModelMessage Passing Training Set SizeQEDParsability ValencySGG-LLMnone100,0001.000 ± 0.000 0.999 ± 0.001SGG-LLM w/out fine-tuningnone100,0000.000 ± 0.000 0.000 ± 0.000SGG-LLM w/ special lossnone100,0000.986 ± 0.003-SGG-LLMedges100,0000.998 ± 0.001 0.999 ± 0.000SGG-LLM w/out gating termedges100,0000.000 ± 0.000 0.000 ± 0.000SGG-LLMcorrespondences100,0000.995 ± 0.001 0.998 ± 0.000SGG-LLMnone25,0000.997 ± 0.002 0.991 ± 0.003SGG-LLMnone400,0000.998 ± 0.001 1.000 ± 0.000grapherN/A100,0001.000 ± 0.000 1.000 ± 0.000regenN/A100,0000.984 ± 0.008 0.991 ± 0.007n batchi=1
We used GraphSAGE(14) in all experiments
Prior work on generating graphs conditioned on text focused on tasks requiring graphs that did not contain sets of nodes with the same feature string(8; 26;<br />
)
This makes it difficult to introduce graph information into the early elements in the graph serialization sequence. For a node with a degree greater than one, if we passed the feature vector outputted from an MP layer into its earliest instance in the graph serialization, we would be passing information backwards in the sequence
PreprintIn terms of societal impact, our proposed approach might help speed up and improve processes such as drug discovery, software development and project planning.But at the same time requires oversight to ensure it is not used for nefarious applications like the design of chemical weapons.In addition, if this approach is applied to a task in the social sciences, analysis should be required to ensure that the biases learned by the model are understood and any unfair preferences learned for a certain demographic or group should be mitigated.A HYPERPARAMETERS AND TRAININGFor experiments on the molecule data sets, all variants of our proposed approach and the implemented baseline grapher(26)and regen (8) used the pretrained version of BLOOM(33)with 560 million parameters as their language model.For experiments on the WebNLG2020+ data set(10), all models used the pretrained version of T5(30)with 770 million parameters as their language model.All models were trained for up to ten epochs and checkpointed based on minimizing validation loss.Model's were trained using the ADAM optimizer(22)with a learning rate of 3e − 5, a β 1 of 0.9, a β 2 of 0.999 and a regularization weight of 1e − 7 as well as a linear learning rate schedule.During training, model parameters' gradients norms were clipped to a value of 1.0.The models were trained with a batch size of 18 using stage 2 data parallelism using the method described in(31).All models were trained and evaluated on machines with 3 NVIDIA A100 GPUs.Variants of our proposed approach which incorporated message passing into the LLM by interleaving message passing layers in the LLM used a single GraphSage (14) layer in each message passing layer with an embedding size equal to the token embedding size of the LLM they were incorporated into.B MESSAGE PASSING BETWEEN NODE CORRESPONDENCESMost nodes appear at least twice in a bag-of-edges serialization.We can define a correspondence graph from a serialized graph D G by treating each instance of a node in a serialized graph D G as its own node in the correspondence graph.Then the correspondences between instances define edges in the correspondence graphs.An example of a correspondence graph is shown in the graphic below;In the graphic, the node C<D>0 occurs more than once in D G and each instance is treated as its own node in the correspondence graph, so the node C<D>0 corresponds to C<D>0 edge 1 and to Preprint metrics of F1-score, precision and recall when comparing a generated graph to the ground truth knowledge graph.See(8)for a more detailed explanation of these metrics.We compare our proposed method to the three best performing models on this data set; regen (8), grapher (26), and bt5(11).On the WebNLG+ 2020 data set, results in the table below suggest that the incorporating message passing into an LLM is not useful to the task of knowledge graph generation from an imperative description, but also that using edge message passing does not degrade performance.Interestingly, SGG-LLM without message passing was able to achieve state-of-the-art performance on the benchmark task of generating triples in a knowledge graph.regen's implementation for experiments on WebNLG2020+ was identical to SGG-LLM without message passing, except that SGG-LLM was trained with a different training objective (see equation 1 in the main paper), a more aggressive learning rate and a linear learning rate schedule.So the state-of-the-art performance of SG-LLM on WebNLG2020+ may be attributed to better hyperparameter selection or the modified training objective.Model performance on knowledge graph data set WebNLG+ 2020.Note: baseline model results do not report standard deviations because they were not reported in prior work and we felt it was more appropriate to report baseline results based their published results as opposed to reimplementing the baselines ourselves.Experiments with variants of our proposed approach were repeated three times to estimate standard error.Exact
Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan, ArXiv, abs/2204.141982022</p>
<p>SpeechT5: Unified-modal encoder-decoder pre-training for spoken language processing. Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei, 10.18653/v1/2022.acl-long.393Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Quantifying the chemical beauty of drugs. Richard Bickerton, Gaia Paolini, Jérémy Besnard, Andrew Sorel Muresan, Hopkins, 10.1038/nchem.1243Nature chemistry. 42012</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, 2020</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. Felipe Petroski Such. Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,; Andrew N. Carr; Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrewJan Leike. 2021</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck2022Jeff Dean, Slav Petrovand Noah Fiedel. Palm: Scaling language modeling with pathways</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Qlora, arXiv:2305.14314Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>Regen: Reinforcement learning for text and knowledge base generation using pretrained language models. Pierre L Dognin, Inkit Padhi, Igor Melnyk, Payel Das, 2021</p>
<p>Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, Noah A Smith, Recurrent neural network grammars. 2016</p>
<p>Creating training corpora for NLG micro-planners. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/P17-1017Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171Long Papers)</p>
<p>Machine translation aided bilingual data-to-text generation and semantic parsing. Heming Ge, Mihir Sanjay Kale, Oshin Agarwal, Rami Al-Rfou, Siamak Shakeri, Yunhsuan Sung, 20203rd Workshop on Natural Language Generation from the Semantic Web</p>
<p>Machine translation aided bilingual data-to-text generation and semantic parsing. Heming Ge, Mihir Sanjay Kale, Oshin Agarwal, Rami Al-Rfou, Siamak Shakeri, Yunhsuan Sung, 20203rd Workshop on Natural Language Generation from the Semantic Web</p>
<p>Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training. Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, Zheng Zhang, 2020</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, 201730Advances in neural information processing systems</p>
<p>Deep speech: Scaling up end-to-end speech recognition. Y Awni, Carl Hannun, Jared Case, Bryan Casper, Greg Catanzaro, Erich Diamos, Ryan Elsen, Sanjeev Prenger, Shubho Satheesh, Adam Sengupta, Andrew Y Coates, Ng, CoRR, abs/1412.55672014</p>
<p>Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Erich Simonyan, Jack W Elsen, Rae, 2022</p>
<p>Data-driven schedule risk forecasting for construction mega-projects. Vahan Hovhannisyan, Peter Zachares, Yael Grushka-Cockayne, Alan Mosca, Carlos Ledezma, Available at SSRN. 44961192023</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, arXiv:2005.006872020arXiv preprint</p>
<p>Ogblsc: A large-scale challenge for machine learning on graphs. Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, Jure Leskovec, arXiv:2103.094302021arXiv preprint</p>
<p>Scaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, CoRR, abs/2102.059182021</p>
<p>Vilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, 2021</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations, ICLR 2015. San Diego, CA, USAMay 7-9, 2015. 2015Conference Track Proceedings</p>
<p>Visualbert: A simple and performant baseline for vision and language. Liunian Harold, Li , Mark Yatskar, Cho-Jui Da Yin, Kai-Wei Hsieh, Chang, CoRR, abs/1908.035572019</p>
<p>A text-guided protein design framework. Shengchao Liu, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Anthony Gitter, Chaowei Xiao, Jian Tang, Hongyu Guo, Anima Anandkumar, 2023</p>
<p>Unified structure generation for universal information extraction. Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, Hua Wu, 2022</p>
<p>Knowledge graph generation from text. Igor Melnyk, Pierre Dognin, Payel Das, 2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, CoRR, abs/2103.000202021</p>
<p>Robust speech recognition via large-scale weak supervision. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, Ilya Sutskever, 2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 1532-4435211jan 2020</p>
<p>Zero: Memory optimization towards training A trillion parameter models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He, CoRR, abs/1910.020542019</p>
<p>Explanation graph generation via pretrained language models: An empirical study with contrastive learning. Swarnadeep Saha, Prateek Yadav, Mohit Bansal, 2022</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova Del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina Mcmillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, David Daniel Van Strien, Dragomir Ifeoluwa Adelani, Eduardo González Radev, Efrat Ponferrada, Ethan Levkovizh, Eyal Kim, Francesco De Bar Natan, Gérard Toni, Germán Dupont, Giada Kruszewski, Hady Pistilli, Hamza Elsahar, Hieu Benyamina, Ian Tran, Idris Yu, Isaac Abdulmumin, Itziar Johnson, Javier Gonzalez-Dios, Jenny De La Rosa, Jesse Chim, Jian Dodge, Jonathan Zhu, Jörg Chang, Joseph Frohberg, Joydeep Tobing, Khalid Bhattacharjee, Kimbo Almubarak, Kyle Chen, Leandro Lo, Leon Von Werra, Long Weber, Loubna Phan, Ludovic Ben Allal, Manan Tanguy, Manuel Dey, Maraim Romero Muñoz, María Masoud, Mario Grandury, Max Šaško, Maximin Huang, Mayank Coavoux, Mike Singh, Preprint Tian-Jian, Minh Chien Jiang, Mohammad A Vu, Mustafa Jauhar, Nishant Ghaleb, Nora Subramani, Nurulaqilla Kassner, Olivier Khamis, Omar Nguyen, Ona Espejel, Paulo De Gibert, Peter Villegas, Pierre Henderson, Priscilla Colombo, Quentin Amuok, Rheza Lhoest, Rishi Harliman, Roberto Bommasani, Rui Luis López, Salomey Ribeiro, Sampo Osei, Sebastian Pyysalo, Shamik Nagel, Shamsuddeen Bose, Shanya Hassan Muhammad, Shayne Sharma, Somaieh Longpre, Stanislav Nikpoor, Suhas Silberberg, Sydney Pai, Tiago Zink, Timo Timponi Torrent, Tristan Schick, Valentin Thrush, Vassilina Danchev, Veronika Nikoulina, Violette Laippala, Vrinda Lepercq, Zaid Prabhu, Zeerak Alyafeai, Arun Talat, Benjamin Raja, Chenglei Heinzerling, Davut Emre Si, Elizabeth Taşar, Sabrina J Salesky, Wilson Y Mielke, Abheesht Lee, Andrea Sharma, Antoine Santilli, Arnaud Chaffin, Debajyoti Stiegler, Eliza Datta, Gunjan Szczechla, Han Chhablani, Harshit Wang, Hendrik Pandey, Jason Strobelt, Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, Saiful Bari, Maged S Al-Shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Pierre Patrick Von Platen, Pierre Cornette, Rémi François Lavallée, Samyam Lacroix, Sanchit Rajbhandari, Shaden Gandhi, Stéphane Smith, Suraj Requena, Tim Patil, Ahmed Dettmers, Amanpreet Baruwa, Anastasia Singh, Anne-Laure Cheveleva, Arjun Ligozat, Aurélie Subramonian, Charles Névéol, Dan Lovering, Deepak Garrette, Ehud Tunuguntla, Ekaterina Reiter, Ekaterina Taktasheva, Eli Voloshina, Genta Bogdanov, Hailey Indra Winata, Jan-Christoph Schoelkopf, Jekaterina Kalo, Jessica Novikova, Jordan Zosa Forde, Jungo Clive, Ken Kasai, Liam Kawamura, Marine Hazan, Miruna Carpuat, Najoung Clinciu, Newton Kim, Oleg Cheng, Omer Serikov, Oskar Antverg, Rui Van Der Wal, Ruochen Zhang, Sebastian Zhang, Shachar Gehrmann, Shani Mirkin, Tatiana Pais, Thomas Shavrina, Tian Scialom, Tomasz Yun, Verena Limisiewicz, Vitaly Rieser, Vladislav Protasov, Yada Mikhailov, Yonatan Pruksachatkun, Zachary Belinkov, Zdeněk Bamberger, Alice Kasner, Amanda Rueda, Amir Pestana, Ammar Feizpour, Amy Khan, Ana Faranak, Anthony Santos, Silas Hevia, Sourav Wang, Sylvain Roy, Thanh Viguier, Tobi Le, Trieu Oyebade, Yoyo Le, Zach Yang, Nguyen ; Chenxi, Chirag Zhou, Chuxin Jain, Clémentine Xu, Fourrier, Daniel Daniel León Periñán, Dian Molano, Enrique Yu, Fabio Manjavacas, Florian Barth, Gabriel Fuhrimann, Altay ; Jihyun, John Kang, Jonas Giorgi, Jose Golde, Karthik David Posada, Lokesh Rangasai Sivaraman, Lu Bulchandani, Luisa Liu, Shinzato, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash. Aycha Abdollahi, Azadeh Tammour, Bahareh Hajihosseini, Benjamin Behroozi, Bharat Ajibade, Carlos Saxena, Danish Muñoz Ferrandis, David Contractor, Davis Lansky, Douwe David, Kiela, A Duong, Edward Nguyen, Emi Tan, Ezinwanne Baylor, Fatima Ozoani, Frankline Mirza, Habib Ononiwu, Hessie Rezanejad, Indrani Jones, Irene Bhattacharya, Irina Solaiman, Isar Sedenko, Jesse Nejadgholi, Josh Passmore, Julio Bonis Seltzer, Livia Sanz, Mairon Dutra, Maraim Samagaio, Margot Elbadri, Marissa Mieskes, Martha Gerchick, Michael Akinlolu, Mike Mckenna, Muhammed Qiu, Mykola Ghauri, Nafis Burynok, Nazneen Abrar, Nour Rajani, Nour Elkott, Olanrewaju Fahmy, Ran Samuel, Rasmus An, Ryan Kromann, Samira Hao, Sarmad Alizadeh, Shubber, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts; Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz; Bo Wang, Caio Brito,; Maria A Castillo; Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan; Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo CanalliMaiko Takeuchi, Marc Pàmies2022Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam ; Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis LabrakHyung Won Chung, Jaesung Tae, Jason Phang. Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter. Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model</p>
<p>Gianluca Sforna, deric4 (github handle). 2023Greg Landrum, and Hans De Winter. rdkit</p>
<p>Everything is connected: Graph neural networks. Petar Veličković, Current Opinion in Structural Biology. 791025382023</p>
<p>Grammar as a foreign language. Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton, Advances in neural information processing systems. 282015</p>
<p>Large-scale self-and semi-supervised learning for speech translation. Changhan Wang, Anne Wu, Juan Miguel Pino, Alexei Baevski, Michael Auli, Alexis Conneau, CoRR, abs/2104.066782021</p>
<p>Language models are open knowledge graphs. Chenguang Wang, Xiao Liu, Dawn Song, 2020</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, 10.1021/ci00057a005J. Chem. Inf. Comput. Sci. 0095- 2338281feb 1988</p>
<p>Lit: Zero-shot transfer with locked-image text tuning. Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer, CoRR, abs/2111.079912021</p>            </div>
        </div>

    </div>
</body>
</html>
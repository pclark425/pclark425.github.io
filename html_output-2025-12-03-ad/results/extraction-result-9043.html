<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9043 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9043</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9043</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-268201735</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.00126v1.pdf" target="_blank">FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are primarily evaluated by overall performance on various text understanding and generation tasks. However, such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs’ capabilities. In this paper, we present FAC^2E, a framework for Fine-grAined and Cognition-grounded LLMs’ Capability Evaluation. Specifically, we formulate LLMs’ evaluation in a multi-dimensional and explainable manner by dissociating the language-related capabilities and the cognition-related ones. Besides, through extracting the intermediate reasoning from LLMs, we further break down the process of applying a specific capability into three sub-steps: recalling relevant knowledge, utilizing knowledge, and solving problems. Finally, FAC^2E evaluates each sub-step of each fine-grained capability, providing a two-faceted diagnosis for LLMs. Utilizing FAC^2E, we identify a common shortfall in knowledge utilization among models and propose a straightforward, knowledge-enhanced method to mitigate this issue. Our results not only showcase promising performance enhancements but also highlight a direction for future LLM advancements.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9043.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9043.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theory of Mind (FAC2E Social Modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory of Mind (Ullman 2023) and Pragmatics (Hu et al. 2023) — aggregated as Social Modeling in FAC 2 E</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>FAC 2 E aggregates two social-cognition style benchmarks (Theory of Mind and Pragmatics) into a single 'Social Modeling' capability dimension and measures LLMs' performance via a three-step decomposition (crystallized, fluid, problem-solving).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Benchmark aggregation: Theory of Mind (Ullman 2023) + Pragmatics (Hu et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmark aggregation used by FAC 2 E to operationalize social cognition; ToM dataset (Ullman 2023) is generative (80 samples, avg input length 152) and Pragmatics (Hu et al. 2023) is multiple-choice (150 samples, avg input length 288); FAC2E reformulates instances into QA format and evaluates models' intermediate rationales and final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Theory of Mind (Ullman 2023) and Pragmatics (Hu et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tests social-cognitive skills: Theory of Mind measures attribution of beliefs/intentions (ToM) and Pragmatics measures non-literal language comprehension (irony, indirect speech, humor, etc.). FAC2E evaluates via three sub-steps: knowledge recall (crystallized), knowledge utilization (fluid), and problem-solving.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>FAC 2 E reports aggregated Social Modeling problem-solving scores (s3) per model (see individual-model entries below); per-benchmark (ToM-only) numbers are not reported separately in the paper. (Dataset sizes: ToM 80 examples, Pragmatics 150 examples.)</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human-baseline numbers reported; FAC2E finds proprietary/instruction-tuned models substantially outperform many open-source models on the aggregated social modeling dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>FAC2E reformulates benchmarks into QA, elicits two intermediate CoT-like rationales (crystallized r1 and fluid r2) plus final answer r3, evaluates r1/r2 with BARTScore-Recall and r3 with BARTScore-Recall or accuracy; used 4-shot in-context demonstrations, temperature 0.7, max gen length 1024; social modeling score is the arithmetic mean across included benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Scores are aggregated across distinct benchmarks (ToM and Pragmatics), per-benchmark breakdown not provided; small sample sizes (e.g., ToM n=80) and automatically/partially constructed reference rationales limit interpretability; no human baseline numbers supplied for direct LLM-vs-human comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9043.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9043.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's large proprietary instruction-tuned and RLHF-aligned multimodal-capable LLM, used as a high-performing proprietary baseline in FAC 2 E experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI model, instruction-tuned and RLHF-aligned (details per OpenAI reports); used via private API in FAC2E experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Social Modeling (aggregated: Theory of Mind + Pragmatics) in FAC 2 E</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Aggregated social cognition tests assessing ToM and pragmatic non-literal comprehension. FAC2E measures crystallized (s1), fluid (s2), and problem-solving (s3) performance.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Social Modeling problem-solving (s3) = 45.71 (value reported in Table 4 of paper; units are the FAC2E score—presented as numeric percentages/score normalized on the benchmarks used).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Outperforms most open-source models on the aggregated Social Modeling (higher s3 than most open-source instruction-tuned and non-instruction-tuned models); no direct comparison to human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated with FAC2E 3-step decomposition; r1/r2 scored with BARTScore-Recall; final answer evaluated with BARTScore-Recall/accuracy; 4-shot ICL, temperature 0.7, max gen length 1024; social modeling score is mean across pragmatics and ToM instances.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Aggregate social score masks per-benchmark behavior (ToM vs pragmatics); no human baseline given; proprietary API usage details (e.g., exact prompt variants) not fully enumerated beyond the FAC2E instruction templates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9043.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9043.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's instruction-tuned 175B-parameter family (GPT-3.5) used as a proprietary baseline; included in FAC2E comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI instruction-tuned model (based on GPT-3 family), used via private API in FAC2E.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Social Modeling (aggregated: Theory of Mind + Pragmatics)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Aggregated social cognition tests; FAC2E evaluates intermediate rationales and final answers to measure crystallized/fluid/problem-solving components.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Social Modeling problem-solving (s3) = 40.56 (Table 4 aggregated score).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Per FAC2E, GPT-3.5 performs substantially better than many open-source pre-trained models, but below the top proprietary/instruction-tuned variants like GPT-4 and some instruction-tuned proprietary baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same FAC2E pipeline (4-shot ICL, CoT-like stepwise prompts, BARTScore-Recall evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human baseline given; the reported aggregated score mixes ToM and pragmatics; small ToM sample size and constructed rationales limit direct cognitive-psychology style inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9043.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9043.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruct-GPT (gpt-3.5-turbo-instruct / InstructGPT variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's instruction-tuned GPT-3 variant with RLHF; used in FAC2E as a proprietary instruction-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruct-GPT (gpt-3.5-turbo-instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 family model with RLHF; used as an evaluation baseline in FAC2E.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Social Modeling (aggregated: Theory of Mind + Pragmatics)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Social cognition and pragmatic language evaluation aggregated by FAC2E; measures s1 (crystallized), s2 (fluid), s3 (problem solving).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Social Modeling problem-solving (s3) = 45.95 (Table 4 aggregated score).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Among the top performers in FAC2E social modeling, comparable to GPT-4 and better than most open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated under FAC2E pipeline: 4-shot prompts, stepwise CoT-like decomposition, BARTScore-Recall for r1/r2, BARTScore-Recall/accuracy for r3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No direct human baseline; aggregated nature of reported score; proprietary training details different from open-source models complicate interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9043.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9043.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Bard (Gemini family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's proprietary instruction-tuned model (Gemini/Bard family) used as a proprietary baseline in FAC2E experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard (Gemini)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary Google model (Gemini family), instruction-tuned and RLHF-aligned according to cited sources; evaluated via API in FAC2E.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>137B (as noted in paper's model table)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Social Modeling (aggregated: Theory of Mind + Pragmatics)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Aggregated evaluation of social-cognitive abilities (pragmatics, ToM) using FAC2E three-step decomposition and scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Social Modeling problem-solving (s3) = 42.53 (Table 4 aggregated score).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Bard outperforms most open-source models on FAC2E social modeling but is slightly below the top proprietary/instruction-tuned GPT variants in the aggregated metric.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>FAC2E evaluation with stepwise rationales, 4-shot ICL, BARTScore-Recall metrics; social modeling score is mean across included benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Aggregated social score; no per-benchmark breakdown or human baseline; differences in API behavior and prompt sensitivity not fully detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9043.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9043.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA 2-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2-Chat (Meta LLaMA 2 instruction-tuned chat model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta's instruction-tuned LLaMA 2 chat variant (aligned and RLHF-enhanced), used as an open-source strong baseline in FAC2E.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 2-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned and alignment-fine-tuned variant of LLaMA 2 provided by Meta (used in FAC2E experiments); base LLaMA 2 models also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (the evaluated checkpoint was LLaMA-2-7b-chat-hf as listed in implementation details)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Social Modeling (aggregated: Theory of Mind + Pragmatics)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Aggregated social-cognition tests; FAC2E reports crystallized/fluid/problem-solving performance.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Social Modeling problem-solving (s3) = 29.59 (Table 4 aggregated score).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLaMA 2-Chat performs substantially better than many open-source non-instruction-tuned checkpoints and some pre-trained open-source models, but is below the top proprietary models (GPT-4, InstructGPT) on social modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated with FAC2E stepwise pipeline; 4-shot ICL, decomposed prompts, BARTScore-Recall for rationale evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Aggregated social modeling score; FAC2E finds instruction tuning improves crystallized knowledge encoding, but fluid/utilization differences remain; no human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9043.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9043.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA 2 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 (base, non-instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta's LLaMA 2 backbone model (pretrained) evaluated in FAC2E to contrast pre-trained vs instruction-tuned behavior on social/cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 2 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source pretrained LLaMA 2 checkpoint (used as backbone); in FAC2E both base and instruction-tuned variants were evaluated to assess effects of instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Social Modeling (aggregated: Theory of Mind + Pragmatics)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Aggregated social-cognition evaluation using FAC2E decomposition and scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Social Modeling problem-solving (s3) = 24.20 (Table 4 aggregated score).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperforms LLaMA 2-Chat and other instruction-tuned variants on social modeling; shows the gap between pretraining and instruction-tuned models for cognition-related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated with FAC2E pipeline (stepwise rationales, BARTScore-Recall); 4-shot ICL prompting used for all models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Aggregated metric; limited per-benchmark breakdown; lack of human baseline makes cognitive-psychology level claims tentative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9043.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9043.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA 3-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned finetune of LLaMA 3 to follow human instructions; evaluated in FAC2E and shown to have improved crystallized performance on social modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned/fine-tuned variant of LLaMA 3 reported by the authors; used to compare the effect of instruction-tuning on cognitive capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (as stated in model table)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Social Modeling (aggregated: Theory of Mind + Pragmatics)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Aggregated evaluation of social cognition; FAC2E assesses s1 (crystallized), s2 (fluid), s3 (problem-solving).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Social Modeling problem-solving (s3) = 30.52 (Table 4 aggregated score).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Improved crystallized (knowledge recall) and overall social modeling relative to older LLaMA 2-Chat; still below top proprietary models on aggregated s3.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated under FAC2E's 3-step decomposition, 4-shot prompts, BARTScore-Recall used to score intermediate rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Aggregated social score; per-benchmark breakdown not provided; sample sizes small for ToM (n=80) and reference rationales partially automated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9043.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9043.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA 3.1-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.1-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned variant of LLaMA 3.1 evaluated in FAC2E that shows further improvements in crystallized and aggregated social capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.1-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLaMA 3.1 variant fine-tuned on higher-scale/higher-quality data according to the paper's model table.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Social Modeling (aggregated: Theory of Mind + Pragmatics)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Aggregated social cognition tests measuring ToM and pragmatic comprehension; FAC2E provides s1/s2/s3 per capability.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Social Modeling problem-solving (s3) = 31.75 (Table 4 aggregated score).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Improved over LLaMA 2-Chat and base LLaMA in aggregated social modeling but still below top proprietary models (GPT-4, InstructGPT) in s3.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Stepwise FAC2E evaluation (decomposed prompts producing r1/r2/r3), 4-shot ICL, BARTScore-Recall for intermediate rationale scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Aggregated measure across ToM and Pragmatics; no human baseline provided; limited dataset sizes and constructed intermediate references.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9043.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9043.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5 family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 / Flan-T5 / Flan-Alpaca (T5-11B based variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>T5 backbone and instruction-tuned variants (Flan-T5, Flan-Alpaca) were evaluated in FAC2E; they show relatively low social modeling problem-solving performance compared to instruction-tuned LLaMA and proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (t5-11b), Flan-T5 (flan-t5-xxl), Flan-Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 backbone (11B) and instruction-tuned T5 variants; Flan-T5 is instruction-tuned on FLAN data, Flan-Alpaca is an instruction-tuned variant used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Social Modeling (aggregated: Theory of Mind + Pragmatics)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Aggregated ToM and pragmatics evaluation via FAC2E three-step decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported aggregated social modeling problem-solving (s3): T5 = 19.16; Flan-T5 = 21.27; Flan-Alpaca = 23.82 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperform instruction-tuned LLaMA variants and proprietary models on FAC2E social modeling; instruction tuning (Flan) yields modest gains versus base T5.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated with FAC2E pipeline; 4-shot ICL, BARTScore-Recall for rationale evaluation; dataset limits as per FAC2E construction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Aggregated scores; lack of per-benchmark (ToM-only) breakdown; no human baseline numbers; potential mismatch of T5's pretraining context length vs some instances (paper filtered long inputs >2048).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theory of mind might have spontaneously emerged in large language models <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks <em>(Rating: 2)</em></li>
                <li>A fine-grained comparison of pragmatic language understanding in humans and language models <em>(Rating: 2)</em></li>
                <li>Dissociating language and thought in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9043",
    "paper_id": "paper-268201735",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "Theory of Mind (FAC2E Social Modeling)",
            "name_full": "Theory of Mind (Ullman 2023) and Pragmatics (Hu et al. 2023) — aggregated as Social Modeling in FAC 2 E",
            "brief_description": "FAC 2 E aggregates two social-cognition style benchmarks (Theory of Mind and Pragmatics) into a single 'Social Modeling' capability dimension and measures LLMs' performance via a three-step decomposition (crystallized, fluid, problem-solving).",
            "citation_title": "FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "mention_or_use": "use",
            "model_name": "Benchmark aggregation: Theory of Mind (Ullman 2023) + Pragmatics (Hu et al. 2023)",
            "model_description": "Benchmark aggregation used by FAC 2 E to operationalize social cognition; ToM dataset (Ullman 2023) is generative (80 samples, avg input length 152) and Pragmatics (Hu et al. 2023) is multiple-choice (150 samples, avg input length 288); FAC2E reformulates instances into QA format and evaluates models' intermediate rationales and final answers.",
            "model_size": null,
            "test_battery_name": "Theory of Mind (Ullman 2023) and Pragmatics (Hu et al. 2023)",
            "test_description": "Tests social-cognitive skills: Theory of Mind measures attribution of beliefs/intentions (ToM) and Pragmatics measures non-literal language comprehension (irony, indirect speech, humor, etc.). FAC2E evaluates via three sub-steps: knowledge recall (crystallized), knowledge utilization (fluid), and problem-solving.",
            "llm_performance": "FAC 2 E reports aggregated Social Modeling problem-solving scores (s3) per model (see individual-model entries below); per-benchmark (ToM-only) numbers are not reported separately in the paper. (Dataset sizes: ToM 80 examples, Pragmatics 150 examples.)",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human-baseline numbers reported; FAC2E finds proprietary/instruction-tuned models substantially outperform many open-source models on the aggregated social modeling dimension.",
            "experimental_details": "FAC2E reformulates benchmarks into QA, elicits two intermediate CoT-like rationales (crystallized r1 and fluid r2) plus final answer r3, evaluates r1/r2 with BARTScore-Recall and r3 with BARTScore-Recall or accuracy; used 4-shot in-context demonstrations, temperature 0.7, max gen length 1024; social modeling score is the arithmetic mean across included benchmarks.",
            "limitations_or_caveats": "Scores are aggregated across distinct benchmarks (ToM and Pragmatics), per-benchmark breakdown not provided; small sample sizes (e.g., ToM n=80) and automatically/partially constructed reference rationales limit interpretability; no human baseline numbers supplied for direct LLM-vs-human comparisons.",
            "uuid": "e9043.0",
            "source_info": {
                "paper_title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4 (GPT-4)",
            "brief_description": "OpenAI's large proprietary instruction-tuned and RLHF-aligned multimodal-capable LLM, used as a high-performing proprietary baseline in FAC 2 E experiments.",
            "citation_title": "FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary OpenAI model, instruction-tuned and RLHF-aligned (details per OpenAI reports); used via private API in FAC2E experiments.",
            "model_size": null,
            "test_battery_name": "Social Modeling (aggregated: Theory of Mind + Pragmatics) in FAC 2 E",
            "test_description": "Aggregated social cognition tests assessing ToM and pragmatic non-literal comprehension. FAC2E measures crystallized (s1), fluid (s2), and problem-solving (s3) performance.",
            "llm_performance": "Social Modeling problem-solving (s3) = 45.71 (value reported in Table 4 of paper; units are the FAC2E score—presented as numeric percentages/score normalized on the benchmarks used).",
            "human_baseline_performance": null,
            "performance_comparison": "Outperforms most open-source models on the aggregated Social Modeling (higher s3 than most open-source instruction-tuned and non-instruction-tuned models); no direct comparison to human baseline provided.",
            "experimental_details": "Evaluated with FAC2E 3-step decomposition; r1/r2 scored with BARTScore-Recall; final answer evaluated with BARTScore-Recall/accuracy; 4-shot ICL, temperature 0.7, max gen length 1024; social modeling score is mean across pragmatics and ToM instances.",
            "limitations_or_caveats": "Aggregate social score masks per-benchmark behavior (ToM vs pragmatics); no human baseline given; proprietary API usage details (e.g., exact prompt variants) not fully enumerated beyond the FAC2E instruction templates.",
            "uuid": "e9043.1",
            "source_info": {
                "paper_title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "OpenAI's instruction-tuned 175B-parameter family (GPT-3.5) used as a proprietary baseline; included in FAC2E comparisons.",
            "citation_title": "FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "OpenAI instruction-tuned model (based on GPT-3 family), used via private API in FAC2E.",
            "model_size": "175B",
            "test_battery_name": "Social Modeling (aggregated: Theory of Mind + Pragmatics)",
            "test_description": "Aggregated social cognition tests; FAC2E evaluates intermediate rationales and final answers to measure crystallized/fluid/problem-solving components.",
            "llm_performance": "Social Modeling problem-solving (s3) = 40.56 (Table 4 aggregated score).",
            "human_baseline_performance": null,
            "performance_comparison": "Per FAC2E, GPT-3.5 performs substantially better than many open-source pre-trained models, but below the top proprietary/instruction-tuned variants like GPT-4 and some instruction-tuned proprietary baselines.",
            "experimental_details": "Same FAC2E pipeline (4-shot ICL, CoT-like stepwise prompts, BARTScore-Recall evaluation).",
            "limitations_or_caveats": "No human baseline given; the reported aggregated score mixes ToM and pragmatics; small ToM sample size and constructed rationales limit direct cognitive-psychology style inference.",
            "uuid": "e9043.2",
            "source_info": {
                "paper_title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "InstructGPT",
            "name_full": "Instruct-GPT (gpt-3.5-turbo-instruct / InstructGPT variant)",
            "brief_description": "OpenAI's instruction-tuned GPT-3 variant with RLHF; used in FAC2E as a proprietary instruction-tuned model.",
            "citation_title": "FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "mention_or_use": "use",
            "model_name": "Instruct-GPT (gpt-3.5-turbo-instruct)",
            "model_description": "Instruction-tuned GPT-3 family model with RLHF; used as an evaluation baseline in FAC2E.",
            "model_size": "175B",
            "test_battery_name": "Social Modeling (aggregated: Theory of Mind + Pragmatics)",
            "test_description": "Social cognition and pragmatic language evaluation aggregated by FAC2E; measures s1 (crystallized), s2 (fluid), s3 (problem solving).",
            "llm_performance": "Social Modeling problem-solving (s3) = 45.95 (Table 4 aggregated score).",
            "human_baseline_performance": null,
            "performance_comparison": "Among the top performers in FAC2E social modeling, comparable to GPT-4 and better than most open-source models.",
            "experimental_details": "Evaluated under FAC2E pipeline: 4-shot prompts, stepwise CoT-like decomposition, BARTScore-Recall for r1/r2, BARTScore-Recall/accuracy for r3.",
            "limitations_or_caveats": "No direct human baseline; aggregated nature of reported score; proprietary training details different from open-source models complicate interpretability.",
            "uuid": "e9043.3",
            "source_info": {
                "paper_title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Bard",
            "name_full": "Google Bard (Gemini family)",
            "brief_description": "Google's proprietary instruction-tuned model (Gemini/Bard family) used as a proprietary baseline in FAC2E experiments.",
            "citation_title": "FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "mention_or_use": "use",
            "model_name": "Bard (Gemini)",
            "model_description": "Proprietary Google model (Gemini family), instruction-tuned and RLHF-aligned according to cited sources; evaluated via API in FAC2E.",
            "model_size": "137B (as noted in paper's model table)",
            "test_battery_name": "Social Modeling (aggregated: Theory of Mind + Pragmatics)",
            "test_description": "Aggregated evaluation of social-cognitive abilities (pragmatics, ToM) using FAC2E three-step decomposition and scoring.",
            "llm_performance": "Social Modeling problem-solving (s3) = 42.53 (Table 4 aggregated score).",
            "human_baseline_performance": null,
            "performance_comparison": "Bard outperforms most open-source models on FAC2E social modeling but is slightly below the top proprietary/instruction-tuned GPT variants in the aggregated metric.",
            "experimental_details": "FAC2E evaluation with stepwise rationales, 4-shot ICL, BARTScore-Recall metrics; social modeling score is mean across included benchmarks.",
            "limitations_or_caveats": "Aggregated social score; no per-benchmark breakdown or human baseline; differences in API behavior and prompt sensitivity not fully detailed.",
            "uuid": "e9043.4",
            "source_info": {
                "paper_title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA 2-Chat",
            "name_full": "LLaMA 2-Chat (Meta LLaMA 2 instruction-tuned chat model)",
            "brief_description": "Meta's instruction-tuned LLaMA 2 chat variant (aligned and RLHF-enhanced), used as an open-source strong baseline in FAC2E.",
            "citation_title": "FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "mention_or_use": "use",
            "model_name": "LLaMA 2-Chat",
            "model_description": "Instruction-tuned and alignment-fine-tuned variant of LLaMA 2 provided by Meta (used in FAC2E experiments); base LLaMA 2 models also evaluated.",
            "model_size": "7B (the evaluated checkpoint was LLaMA-2-7b-chat-hf as listed in implementation details)",
            "test_battery_name": "Social Modeling (aggregated: Theory of Mind + Pragmatics)",
            "test_description": "Aggregated social-cognition tests; FAC2E reports crystallized/fluid/problem-solving performance.",
            "llm_performance": "Social Modeling problem-solving (s3) = 29.59 (Table 4 aggregated score).",
            "human_baseline_performance": null,
            "performance_comparison": "LLaMA 2-Chat performs substantially better than many open-source non-instruction-tuned checkpoints and some pre-trained open-source models, but is below the top proprietary models (GPT-4, InstructGPT) on social modeling.",
            "experimental_details": "Evaluated with FAC2E stepwise pipeline; 4-shot ICL, decomposed prompts, BARTScore-Recall for rationale evaluation.",
            "limitations_or_caveats": "Aggregated social modeling score; FAC2E finds instruction tuning improves crystallized knowledge encoding, but fluid/utilization differences remain; no human baseline provided.",
            "uuid": "e9043.5",
            "source_info": {
                "paper_title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA 2 (base)",
            "name_full": "LLaMA 2 (base, non-instruction-tuned)",
            "brief_description": "Meta's LLaMA 2 backbone model (pretrained) evaluated in FAC2E to contrast pre-trained vs instruction-tuned behavior on social/cognitive tasks.",
            "citation_title": "FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "mention_or_use": "use",
            "model_name": "LLaMA 2 (base)",
            "model_description": "Open-source pretrained LLaMA 2 checkpoint (used as backbone); in FAC2E both base and instruction-tuned variants were evaluated to assess effects of instruction tuning.",
            "model_size": "7B",
            "test_battery_name": "Social Modeling (aggregated: Theory of Mind + Pragmatics)",
            "test_description": "Aggregated social-cognition evaluation using FAC2E decomposition and scoring.",
            "llm_performance": "Social Modeling problem-solving (s3) = 24.20 (Table 4 aggregated score).",
            "human_baseline_performance": null,
            "performance_comparison": "Underperforms LLaMA 2-Chat and other instruction-tuned variants on social modeling; shows the gap between pretraining and instruction-tuned models for cognition-related tasks.",
            "experimental_details": "Evaluated with FAC2E pipeline (stepwise rationales, BARTScore-Recall); 4-shot ICL prompting used for all models.",
            "limitations_or_caveats": "Aggregated metric; limited per-benchmark breakdown; lack of human baseline makes cognitive-psychology level claims tentative.",
            "uuid": "e9043.6",
            "source_info": {
                "paper_title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA 3-Instruct",
            "name_full": "LLaMA 3-Instruct",
            "brief_description": "Instruction-tuned finetune of LLaMA 3 to follow human instructions; evaluated in FAC2E and shown to have improved crystallized performance on social modeling.",
            "citation_title": "FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "mention_or_use": "use",
            "model_name": "LLaMA 3-Instruct",
            "model_description": "Instruction-tuned/fine-tuned variant of LLaMA 3 reported by the authors; used to compare the effect of instruction-tuning on cognitive capabilities.",
            "model_size": "8B (as stated in model table)",
            "test_battery_name": "Social Modeling (aggregated: Theory of Mind + Pragmatics)",
            "test_description": "Aggregated evaluation of social cognition; FAC2E assesses s1 (crystallized), s2 (fluid), s3 (problem-solving).",
            "llm_performance": "Social Modeling problem-solving (s3) = 30.52 (Table 4 aggregated score).",
            "human_baseline_performance": null,
            "performance_comparison": "Improved crystallized (knowledge recall) and overall social modeling relative to older LLaMA 2-Chat; still below top proprietary models on aggregated s3.",
            "experimental_details": "Evaluated under FAC2E's 3-step decomposition, 4-shot prompts, BARTScore-Recall used to score intermediate rationales.",
            "limitations_or_caveats": "Aggregated social score; per-benchmark breakdown not provided; sample sizes small for ToM (n=80) and reference rationales partially automated.",
            "uuid": "e9043.7",
            "source_info": {
                "paper_title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA 3.1-Instruct",
            "name_full": "LLaMA 3.1-Instruct",
            "brief_description": "An instruction-tuned variant of LLaMA 3.1 evaluated in FAC2E that shows further improvements in crystallized and aggregated social capabilities.",
            "citation_title": "FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.1-Instruct",
            "model_description": "Instruction-tuned LLaMA 3.1 variant fine-tuned on higher-scale/higher-quality data according to the paper's model table.",
            "model_size": "8B",
            "test_battery_name": "Social Modeling (aggregated: Theory of Mind + Pragmatics)",
            "test_description": "Aggregated social cognition tests measuring ToM and pragmatic comprehension; FAC2E provides s1/s2/s3 per capability.",
            "llm_performance": "Social Modeling problem-solving (s3) = 31.75 (Table 4 aggregated score).",
            "human_baseline_performance": null,
            "performance_comparison": "Improved over LLaMA 2-Chat and base LLaMA in aggregated social modeling but still below top proprietary models (GPT-4, InstructGPT) in s3.",
            "experimental_details": "Stepwise FAC2E evaluation (decomposed prompts producing r1/r2/r3), 4-shot ICL, BARTScore-Recall for intermediate rationale scoring.",
            "limitations_or_caveats": "Aggregated measure across ToM and Pragmatics; no human baseline provided; limited dataset sizes and constructed intermediate references.",
            "uuid": "e9043.8",
            "source_info": {
                "paper_title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "T5 family",
            "name_full": "T5 / Flan-T5 / Flan-Alpaca (T5-11B based variants)",
            "brief_description": "T5 backbone and instruction-tuned variants (Flan-T5, Flan-Alpaca) were evaluated in FAC2E; they show relatively low social modeling problem-solving performance compared to instruction-tuned LLaMA and proprietary models.",
            "citation_title": "FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "mention_or_use": "use",
            "model_name": "T5 (t5-11b), Flan-T5 (flan-t5-xxl), Flan-Alpaca",
            "model_description": "T5 backbone (11B) and instruction-tuned T5 variants; Flan-T5 is instruction-tuned on FLAN data, Flan-Alpaca is an instruction-tuned variant used for comparison.",
            "model_size": "11B",
            "test_battery_name": "Social Modeling (aggregated: Theory of Mind + Pragmatics)",
            "test_description": "Aggregated ToM and pragmatics evaluation via FAC2E three-step decomposition.",
            "llm_performance": "Reported aggregated social modeling problem-solving (s3): T5 = 19.16; Flan-T5 = 21.27; Flan-Alpaca = 23.82 (Table 4).",
            "human_baseline_performance": null,
            "performance_comparison": "Underperform instruction-tuned LLaMA variants and proprietary models on FAC2E social modeling; instruction tuning (Flan) yields modest gains versus base T5.",
            "experimental_details": "Evaluated with FAC2E pipeline; 4-shot ICL, BARTScore-Recall for rationale evaluation; dataset limits as per FAC2E construction.",
            "limitations_or_caveats": "Aggregated scores; lack of per-benchmark (ToM-only) breakdown; no human baseline numbers; potential mismatch of T5's pretraining context length vs some instances (paper filtered long inputs &gt;2048).",
            "uuid": "e9043.9",
            "source_info": {
                "paper_title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theory of mind might have spontaneously emerged in large language models",
            "rating": 2,
            "sanitized_title": "theory_of_mind_might_have_spontaneously_emerged_in_large_language_models"
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "rating": 2,
            "sanitized_title": "large_language_models_fail_on_trivial_alterations_to_theoryofmind_tasks"
        },
        {
            "paper_title": "A fine-grained comparison of pragmatic language understanding in humans and language models",
            "rating": 2,
            "sanitized_title": "a_finegrained_comparison_of_pragmatic_language_understanding_in_humans_and_language_models"
        },
        {
            "paper_title": "Dissociating language and thought in large language models",
            "rating": 1,
            "sanitized_title": "dissociating_language_and_thought_in_large_language_models"
        }
    ],
    "cost": 0.020867999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition
7 Oct 2024</p>
<p>Xiaoqiang Wang xiaoqiang.wang@umontreal.ca 
DIRO &amp; Institut Courtois
Université de Montréal</p>
<p>Mila -Quebec AI Institute</p>
<p>Lingfei Wu 
Anytime.AI</p>
<p>Tengfei Ma tengfei.ma@stonybrook.edu 
Stony Brook University</p>
<p>Bang Liu bang.liu@umontreal.ca 
DIRO &amp; Institut Courtois
Université de Montréal</p>
<p>Mila -Quebec AI Institute</p>
<p>FAC 2 E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition
7 Oct 2024494A6DEE9D47AC415D45D2407505D611arXiv:2403.00126v2[cs.CL]agreementslicensinglong-distance dependenciesand garden-path effects. Encoding grammatical concepts support linguistic operations regarding word meanings and their combinatorial processing. Semantics: synonymyantonymyand hypernymy. FORMAL KNOWLEDGE Mechanism: deductiveinductiveand analogical. Conducting word-based formal reasoning through understanding lexical semantics. Skill: numericlogicand manipulation. WORLD MODELING
Large language models (LLMs) are primarily evaluated by overall performance on various text understanding and generation tasks.However, such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs' capabilities.In this paper, we present FAC 2 E, a framework for Fine-grAined and Cognition-grounded LLMs' Capability Evaluation.Specifically, we formulate LLMs' evaluation in a multi-dimensional and explainable manner by dissociating the language-related capabilities and the cognitionrelated ones.Besides, through extracting the intermediate reasoning from LLMs, we further break down the process of applying a specific capability into three sub-steps: recalling relevant knowledge, utilizing knowledge, and solving problems.Finally, FAC 2 E evaluates each sub-step of each fine-grained capability, providing a two-faceted diagnosis for LLMs.Utilizing FAC 2 E, we identify a common shortfall in knowledge utilization among models and propose a straightforward, knowledge-enhanced method to mitigate this issue.Our results not only showcase promising performance enhancements but also highlight a direction for future LLM advancements.</p>
<p>Introduction</p>
<p>Large language models (LLMs) (Brown et al., 2020), especially instruction-tuned LLMs (Ouyang et al., 2022;Bai et al., 2022;Touvron et al., 2023b;Chiang et al., 2023) revolutionized natural language processing and have surpassed human performance on tasks that require nontrivial reasoning (Guo et al., 2023;Malinka et al., 2023), while showing great potential in applications from conversational assistants (OpenAI, 2022; Achiam † Corresponding author.Canada CIFAR AI Chair.et al., 2023) to expertise problem-solving (Nori et al., 2023;Zhou et al., 2023;Suzgun and Kalai, 2024).However, despite the impressive performance, LLMs also show poor robustness on complex tasks (Ullman, 2023) and significantly inconsistent evaluation results under different settings, such as binary preference (Xu et al., 2023) and automatic metrics (Gudibande et al., 2023).Therefore, it is crucial to attain an overarching understanding of the capabilities and limitations of LLMs.</p>
<p>To address this challenge, some studies have assessed the performance of LLMs on different tasks based on independent benchmarks from various dimensions (Liang et al., 2022;Srivastava et al., 2023;Gao et al., 2023), such as commensense (Zellers et al., 2019), knowledge (Hendrycks et al., 2020;Yu et al., 2023), instruction-following (Gu et al., 2024), and trustworthy (Sun et al., 2024).Besides, motivated by building LLM-based AI assistants, other studies propose highly curated benchmarks with instancelevel fine-grained annotations, such as difficulty and reasoning skills (Mialon et al., 2023;Ye et al., 2024), for holistic evaluation of LLMs.</p>
<p>However, the existing studies, assessing effectiveness across various tasks, provided limited insight into the models' true capabilities, as it only indicates their overall performance on specific datasets, without revealing the fine-grained capabilities acquired or their proficiency levels within the multiple capabilities involved.For instance, in the context of generative question answering, a model adept at extracting information but struggling to form a coherent understanding may exhibit similar overall performance to another model with profound understanding insights but difficulties in articulating accurate responses.</p>
<p>We argue that a fine-grained understanding of LLMs' capabilities can not only accurately unveil their inherent limitations, but also help us to better identify why one model outperforms the other In this paper, we propose FAC 2 E, a fine-grained capability evaluation framework for LLMs.Specifically, FAC 2 E dissociates the language-related and cognition-related capabilities of LLMs and organizing them into four distinct axes: LINGUISTIC KNOWLEDGE, FORMAL KNOWLEDGE, WORLD MODELING, and SOCIAL MODELING.This categorization is grounded in neuroscience evidence manifesting that language processing and cognitive processes, like memory and reasoning, operate differently in the brain.Drawing from this insight, we adapt a range of existing benchmarks into a unified question-answering format.We then develop specific instructions for each capability, allowing FAC 2 E to evaluate LLMs through a method known as few-shot instruction-following.</p>
<p>Furthermore, we break down the application of a specific capability into three sub-steps: knowledge recall, knowledge utilization, and problem-solving, by iteratively drawing out the model's intermediate reasoning.After evaluating each sub-step, FAC 2 E can reveal the quality of knowledge encoded in the model, and effectiveness in applying relevant knowledge to solve practical problems, offering a more comprehensive evaluation than a single performance metric could.</p>
<p>Our findings reveal a notable gap in capabilities between open-source and proprietary models, especially for cognition-related capabilities.Additionally, we found that many models have difficulties in applying knowledge effectively.To address this, we suggest a knowledge-enhance remedy by incorporating relevant knowledge text as additional input.Experimental results show that it can help the backbone model (e.g.LLaMA 2) achieve approximately 90% of the performance of its instructiontuned counterpart (e.g.LLaMA 2-Chat).</p>
<p>Methodology</p>
<p>In this section, we introduce FAC 2 E framework, designed for fine-grained and cognition-grounded LLMs' capability evaluation.Specifically, we first define the taxonomy for LLMs' capabilities based on the distinction between language and cognition, which is drawn upon insights from neuroscience (Fedorenko and Varley, 2016;Mahowald et al., 2023).Based on this, we transform a variety of existing benchmarks into the unified questionanswering format, design capability-specific instruction, and frame FAC 2 E via few-shot instructionfollowing. Furthermore, we break down the evaluation process for each capability into a three-step reasoning approach.This involves identifying the knowledge pertinent to the input, examining how the model applies this knowledge in practical contexts, and assessing the effectiveness of its problemsolving.By evaluating each of these steps, FAC 2 E provides a comprehensive overview of the model's performance, offering a more nuanced understanding of LLMs' intrinsic capabilities.</p>
<p>Formulation of LLMs' Capabilities</p>
<p>Human language processing, long studied in cognitive science and neuroscience, robustly attributes language and cognition to different brain areas, namely "language network" and "multi-demand network" (Duncan, 2010;Scott et al., 2017).The former is sensitive to linguistic regularities and formal operations, with damage leading to linguistic deficits, while the latter responds actively to various cognitively demanding processes, such as reasoning and memory.Similarly, prior analyses have identified core linguistic regions (Zhang et al., 2024b) and language-independent knowledge neurons (Chen et al., 2023) in LLMs, represented by different parameter sets and subnetworks, each contributing distinctly to language and reasoning tasks.</p>
<p>Motivated by this separated relationship, we define the LLMs' capabilities as a 4-dimensional schema as shown in Table 1.Compared to the fine-grained definitions of high-level knowledge (Hendrycks et al., 2020;Yu et al., 2023) and reasoning skills (Ye et al., 2024) in related works, this formulation aims to dissociate languagerelated and cognition-related capabilities to define a broader range of both high-level and more fundamental low-level functionality.Additionally, it seeks to minimize the coupling between these capabilities to facilitate nuanced analysis (Sec.3.1) and targeted improvement of the model (Sec.3.2).LINGUISTIC KNOWLEDGE.To effectively generate language, LLMs first understand text at a basic linguistic level, including both grammar and semantics.Grammaticality encompasses the rules that govern language structure, spanning from the sounds (phonological) and words (lexical) to the arrangement of words in phrases and sentences.To capture this grammatical structure as comprehensively as possible, especially given the challenges conventional models face in benchmarks like BLiMP (Warstadt et al., 2020), we focus on four key skills: agreement (anaphora and subjectverb relationships), licensing (negative polarity items and reflexive pronouns), managing longdistance dependencies (filler-gap constructions and cleft sentences), and navigating garden-path sentences, which contain temporary ambiguities that must be resolved for correct understanding.For example, "the horse raced past the barn fell," the initial interpretation is that the horse is racing, but upon reaching "fell," it becomes clear that the horse is being raced by another (unnamed) entity.</p>
<p>Semantics, on the other hand, while more closely related to high-level cognitive understanding, within the context of LINGUISTIC KNOWL-EDGE, pertains to the meanings of individual words or lexical semantics (Geeraerts, 2009).This aspect is distinct from conceptual knowledge, which falls under other dimensions of LLM capabilities, highlighting the meaning-related understanding, such as synonymy, antonymy, and hypernymy.FORMAL KNOWLEDGE.Beyond encoding linguistic structures and word meanings, an essential aspect of language capability involves understanding formal operations among words, or wordbased reasoning.This means LLMs should be capable of recognizing relationships between words and deducing missing elements in a given pattern, such as completing analogies (e.g."man:woman :: king:_").FAC 2 E includes three types of reasoning mechanisms-deductive, inductive, and analogical reasoning-between words (Bang et al., 2023), and includes three symbol-based formal skills: numeric (dealing with numbers), logic (applying logical operations), and manipulation (altering the inputs in a rule-based manner) (Wei et al., 2022b).An example task is concatenating the last letters of a word list ("think, machine, learning" → "keg").WORLD MODELING.To step towards cognitive capabilities, well-grounded comprehension of factual and commonsense knowledge is required.Precisely, we decompose this capability into two primary mechanisms: remember and understand, respectively modeling the retrieval-based and comprehension-based capability (Sugawara et al., 2020).Considering the versatility of knowledge sources, we instantiate the remember subcapability as recalling factual knowledge (openended facts), reading comprehension (facts in context), and applying commonsense reasoning.Based on the multiple granularities of text comprehension and hierarchy of input text, we characterize the understand sub-capability as two skills: understanding narrative or event structure (paragraph-level), and discourse comprehension (document-level).SOCIAL MODELING.The utility of human language lies in not only the understanding of the text itself but also the social context and mental states underlying communication (Adolphs, 2009), i.e. serving as a medium for information exchange between individuals.Specifically, there are a lot of phenomena about non-literal language comprehension in daily life, such as jokes, sarcasm, and indirect speech, successful LLMs should be capable of applying social inference skills to attain the intended meaning beyond the literal content.In this paper, we incorporate two kinds of social modeling into FAC 2 E, encompassing pragmatics and theory of mind (ToM) reasoning.Pragmatics is evaluated by six kinds of dialogue, including polite deceits, irony, maxims of conversation, metaphor, indirect speech, and humor, while ToM is based on the "unexpected tasks" devised by Kosinski (2023).</p>
<p>FAC 2 E</p>
<p>Based on the formulation of LLMs' capabilities, we collect input and output pairs from various benchmarks and modify collected instances, yielding a unified question-answering (QA) format.After that, following the widely adopted few-shot in-context learning (ICL) (Brown et al., 2020), we devise capability-specific instruction and frame FAC 2 E via instruction following (Ouyang</p>
<p>Instruction Decomposition</p>
<p>Stepwise Evaluation</p>
<p>Title and task description […]</p>
<p>Linguistic Knowledge As depicted in Figure 1, the pipeline FAC 2 E can be divided into three steps, including capabilityspecific instruction design, instruction decomposition, and stepwise evaluation.Specifically, we devise natural instruction for each capabilityrelated task.Borrowing the widely used template schema of instruction-following (Wang et al., 2022b;Mishra et al., 2022), the capability-specific instruction I c is comprised of three parts: title, task description and few-shot demonstrations.Precisely, the title defines a given QA task in high-level natural language and highlights the associated skills, while the task description not only presents a complete clarification of how an input text is expected to be mapped to final output, but also define the output of reasoning sub-steps through instruction decomposition.After that, following the given task description, a few in-context demonstrations are provided to better steer the response generation.At last, we collect the response results for each reasoning sub-steps, denoted as {r i } 3 i=1 , and respectively evaluate them with the reference answer, denoted as {R i } 3 i=1 , which are directly extracted from corresponding benchmarks.Formally, the procedure can be represented as:
Formal Knowledge World Modeling Social Modeling{r i } 3 i=1 = M I c (1) s i = Criterion i r i , R i(2)
where M denotes the examined LLM, while Criterion i and s i represent the employed automatic metric and corresponding score, respectively.Instruction decomposition.We leverage CoT-like iterative prompting strategy to elicit the intermediate reasoning from the model to frame crystallized and fluid steps.Differing from the standard CoT that outputs a continuous rationale before the final answer, we first decompose the given question as follow-up sub-questions.After that, these sub-questions are used to help the model talk with itself to respectively discover (i) what knowledge this question is about, (ii) how to apply relevant knowledge to the given instance, and (iii) the final answer.In other words, FAC 2 E convert the CoT continuous rationale into easily parseable multistep rationales, which externalizes reasoning of the model (Shwartz et al., 2020;Zhou et al., 2022b) and enables the evaluation of crystallized performance and fluid performance.Formally, as depicted in Figure 1, we expect that the model outputs as: can be either (1) [Follow-up Question], which returns a sub-question, or (2) [Finish], and [Answer] is extracted as the reasoning result of a sub-step.</p>
<p>Stepwise evaluation.Given the reasoning results of three sub-steps, i.e. {r i } 3 i=1 , we engage automatic metrics as the criterion to evaluate them.Specifically, r 1 and r 2 are free-form rationales for intermediate reasoning steps.Considering the diversity of rationale generation, we resort to BARTScore-Recall (Yuan et al., 2021), one of the most superior metrics for natural language generation to evaluate the quality of generated rationale automatically.BARTScore-Recall gauges how many semantic content units from reference texts are covered by the generated candidates, and will not penalize the redundant and instance-specific information in the model response.For the last response r 3 , since it is expected to be the final answer for the given question, it is evaluated by the BARTScore-Recall (Yuan et al., 2021) or accuracy for generative QA re-formulation and multiple choice QA re-formulation, respectively.</p>
<p>Experiments</p>
<p>Evaluation data construction.In Table 2, we present a collection of 17 widely adopted English benchmarks and modify the corresponding inputoutput into a unified QA format, i.e. generative QA or multiple-choice QA.The choice to utilize these benchmarks is rooted in considerations of data quality, availability, and specificity of focus; hence, some widely recognized benchmarks may not be included for these reasons.For example, PIQA (Bisk et al., 2020) focuses on physical commonsense, which, while valuable, represents only a single facet of commonsense reasoning.In contrast, MMLU (Hendrycks et al., 2020)   broad spectrum of subjects, but requires both commonsense and contextual understanding, which might not align with our goal of ensuring a broad range of capabilities while minimizing the coupling between abilities during evaluation.</p>
<p>The reference answers of the benchmarks are directly used as the final answer R 3 , while the reference rationales (R 1 and R 2 ) for the intermediate reasoning steps are constructed automatically.Specifically, on the one hand, R 1 , i.e. the reference rationale for the first reasoning step, is based on the rationale templates and the gold labels of the employed benchmarks.For example, when evaluating the grammaticality regarding negative polarity item (NPI) licensing, the rationale template for R 1 could be "The word [ ] is a negative polarity item: it can only be used in the scope of negation.".The blank is then filled with gold labels (licensing contexts or trigger words), such as "any", "ever", and "even", to build the final R 1 for corresponding NPI licensing samples.On the other hand, R 2 , i.e. the reference rationale for the second reasoning step is built on the instance-wise annotations of human evaluation publicly released by the authors of corresponding benchmarks, which annotates necessary explanations as well as final answer R 3 for a given question.Although this will leave few benchmarks available and lead to a limited number of evaluation data, it provides relatively reliable references and especially enables reproducible evaluation.Proprietary models consist of OpenAI's GPT-3.5 (gpt-3.5-turbo)(OpenAI, 2022), Instruct-GPT (gpt-3.5-turbo-instruct)(Ouyang et al., 2022), GPT-4 (gpt-4-turbo) (Achiam et al., 2023), and Google's Bard (Google, 2023) (also known as Gemini (Team et al., 2023)).</p>
<p>Examined models. As summarized in</p>
<p>Main results</p>
<p>The difference in problem-solving performance is significantly greater than the difference in  potentially showing that either pre-training or finetuning of LLMs can encode sufficient knowledge into the model, but the final task performance does not just depend on the amount or quality of knowledge.</p>
<p>Notably, the crystallized performance (s1) of LLaMA 3-Instruct and LLaMA 3.1-Instruct has significantly improved compared to LLaMA 2-Chat.For example, in social modeling, LLaMA 3-Instruct scored 76.99, and LLaMA 3.1-Instruct scored 77.78, compared to LLaMA 2-Chat's 71.06.This suggests that these models better encode knowledge relevant to higher-level cognitive tasks, likely due to training and fine-tuning LLaMA 3 on higher-quality and larger-scale data compared to the LLaMA 2 model.Linguistic capabilities show a relatively weak correlation with cognitive capabilities.Figure 2 presents the correlation results between different capabilities.Both the language-related and cognitionrelated capabilities exhibit stronger (Pearson's r &gt; 0.7 (Krippendorff, 2004)) intra-dimension correlation (e.g.world modeling vs. social modeling) when compared to inter-dimension correlation (e.g.world modeling vs. linguistic knowledge).This indicates that excellence in language processing does not necessarily equate to a similar level of cognitive capability.These results can also be observed from direct prompt evaluation, which assesses problem-solving performance (s 3 ) without prompting intermediate reasoning steps, further demonstrating the rationality of dissociating language and cognition.A possible reason behind it could be that dedicated structures of the model or subsets of parameters are highly correlated with language (Zhang et al., 2024b;Tang et al., 2024), whereas others serve as cognition (Chen et al., 2023), and they are optimized at different training stages and function as different mechanisms during inference, which has been verified by recent studies on knowledge locating and editing of LLMs Dai et al. (2022); Meng et al. (2022); Zhang et al. (2024a).The crystallized step impacts problem-solving more than the fluid step.Figure 3 illustrates the relationship between problem-solving performance (s 3 ) and sum of crystallized (s 1 ) and fluid (s 2 ) performance.Both s 1 and s 2 make a difference to the final s 3 , showing that problem-solving not only depends on the amount or quality of stored knowledge but also is reflective of the effectiveness of knowledge utilization.For example, LLaMA 2 underperforms LLaMA 2-Chat in terms of s 3 across various capabilities.When taking a closer look at the intermediate results, we can observe that both the models do well in crystallized step, but LLaMA 2 shows an worse result in fluid step, leading the worse problem-solving performance than LLaMA 2-Chat.Besides, all of the open-source models exhibit relatively poor fluid performance w.r.t.GPT-3.5, especially those that are pre-trained but not instruction-tuned, such as T5 and LLaMA 2. This implies a solution to improve problem-solving performance, i.e. boosting the efficacy of knowledge utilization.Section 3.2 presents a knowledgeenhanced method to demonstrate this solution.</p>
<p>Both the model size and the quality of finetuning dataset affect the capabilities of LLMs.</p>
<p>Firstly, backbone models play a critical role in building superior models.For example, as shown in Table 4, fine-tuned on the same dataset, LLaMAbased Alpaca performs better than T5-based Flan-Alpaca, and the larger scale proprietary models show a greater advantage over other open-source models.In addition, scaling open-source models does improve both language and cognitive capability.As shown in Figure 4, problem-solving performance across various capabilities increases as the model size increases, and 65B achieves the best performance.In particular, the level of formal knowledge of 65B is close to that of GPT-3.5.Last, but not least, there is no significant performance difference among various open-source instruction-tuning datasets whether it is comprised of human-written instruction or not.As illustrated in Figure 5, there is not a single best instruction tuning dataset across all tasks, indicating different datasets bring different benefits to LLMs' capabilities.This finding is consistent with the recent success of a mixture of instruction-tuning datasets or expert LLMs (Jiang et al., 2024;Xia et al., 2024).</p>
<p>Boosting LLMs with Injected Knowledge</p>
<p>Based on the above analysis showcasing the limitations of the crystallized performance of existing LLMs, as illustrated in Figure 6, we propose a knowledge-enhanced approach.Specifically, for each given instance with a question and answer, the first baseline, denoted as M+R Taking LLaMA 2, performing moderately in Table 4, as the backbone model, the multifaceted results are summarized in Figure 7.We can observe that explicit injected rationales both R 1 and R 2 can substantially improve the problemsolving performance, and R 2 results in more improvements than R 1 .Specifically, R 1 contributes slightly to language-related capabilities, such as linguistic semantics and formal skills, while R 2 brings about significant improvements to cognitionrelated ones, especially social modeling.Overall, the knowledge-enhanced LLaMA 2 baseline can achieve approximately 90% performance compared to the corresponding instruction-tuned variant.</p>
<p>Related Works</p>
<p>Evaluation of LLMs.LLMs are initially assessed on various understanding (Wang et al., 2018(Wang et al., , 2019) ) and generation tasks (Pilault et al., 2020;Thompson and Post, 2020).With the increasing emphasis on the trustworthiness of models, dedicated benchmarks are proposed to evaluate robustness (Yang et al., 2022;Wang et al., 2023b), hallucination (Li et al., 2023;Belyi et al., 2024), and generalizability (Wang et al., 2023a).Recent-emerged works evaluate LLMs using another evaluator LLM (Kim et al., 2024a,b) or on a holistic benchmark (Mialon et al., 2023;Rein et al., 2023). Chia et al. (2023) conduct evaluation from problem-solving, writing, and human alignments, while Ye et al. (2024) annotates a single instance with a set of skills, including logical thinking, background knowledge, problem handling, and user alignment.Although they provide fine-grained analysis of LLMs' capability, they suffer from the limited number of testing instances, e.g.1,700 of Ye et al. (2024) and overlook the language-related low-level capabilities.</p>
<p>Cognition-inspired intelligence evaluation.How to define and evaluate intelligence is widely investigated by both cognitive science and AI benchmarking (Cattell, 1963;Rogers et al., 2023).In the MRC evaluation, Chollet (2019) describes intelligence as skill-acquisition efficiency, while Sugawara et al. (2020), Wang et al. (2022a) andRay Choudhury et al. (2022) propose to benchmark MRC through reasoning skills and steps a system would be "reading slowly".As for the LLMs, Mahowald et al. (2023) summarize extensive neuroscience evidence of human language and propose a conceptual framework with formal and functional competencies, which largely motivated the design of FAC 2 E. Compared to Mahowald et al. (2023), FAC 2 E introduces more concrete NLP tasks with a more comprehensive capability system and leverages stepwise evaluation to improve the accuracy of assessments.</p>
<p>Conclusion</p>
<p>We present FAC 2 E, defining a fine-grained capability evaluation framework for LLMs, which decomposes each capability into sub-steps to assess performance of knowledge recalling and utilization as well as problem-solving.FAC 2 E reveals the limitations of existing LLMs in knowledge utilization and provides a knowledge-enhanced remedy for it.Empirical results demonstrate its effectiveness.</p>
<p>Limitations</p>
<p>Our work proposes a fine-grained and cognitiongrounded capability evaluation framework for LLMs, namely FAC 2 E, which is based on the dissociated relationship of language and cognition, and evaluating the intermediate reasoning steps of LLMs.The limitations are two-fold, including data quality and domain generalizability.</p>
<p>On the other hand, motivated by a variety of empirical evidence from both neuroscience and probing experiments of LLMs, we formulate FAC 2 E as four capability dimensions, then re-formulate instances from multiple existing benchmarks and conduct stepwise evaluation.Although we try to ensure that the employed datasets are as consistent and targeted with the defined dimensions as possible, the kind of evaluation data construction might not reflect the required skills accurately.For example, an instance can cover more than one language or cognition capabilities As discussed in Section 3.1, different capabilities are correlated to each other to some extent, Besides, the reference rationales, i.e. the gold standard of the intermediate reasoning step, are based on the human annotations from original benchmarks, leading to the inconsistency of reference answers and a limited number of available data, which might bias the evaluation results.One remedy to these incidental issues could be building a new holistic benchmark with fine-grained annotations following our proposed schema.We regard it as our future work and deem designing a new annotation specification a promising direction.</p>
<p>On the other hand, our FAC 2 E only examined LLMs on general domains and English input, ignoring the domain-specific and multilingual application.In particular, the reasoning process of LLMs may expose social bias encoded in these models, such as race and gender (Lucy and Bamman, 2021).Therefore, additional evaluation protocols considering potential risks to user safety are left for our future work.</p>
<p>Ethics Statement</p>
<p>We introduce FAC 2 E, a fine-grained and cognitiongrounded capability evaluation framework for LLMs, and conduct evaluation experiments on publicly available datasets which are widely used in related research.Although LLMs have the potential to cause harm at the individual and societal levels (Gonen and Goldberg, 2019), our FAC 2 E aims to provide a deep understanding of the capabilities and limitations of LLMs, potentially making the risks from the LLMs more predictable.</p>
<p>Figure 1 :
1
Figure 1: Illustration of FAC 2 E pipeline.The input question is decomposed into two intermediate follow-up questions, which are used to help the model talk with itself to elicit reasoning sub-steps.FAC 2 E evaluates each sub-step to reveal crystallized performance, fluid performance, and corresponding problem-solving performance.The content in the round parentheses is purely illustrative and is not part of the model input.The instruction has been omitted here for clarity.Please refer to Appendix B for full version example.et al., 2022).We further leverage chain-of-thought (CoT) (Wei et al., 2022a,b) style prompting to elicit two intermediate reasoning steps from the model, namely crystallized step and fluid step.The terms "crystallized" and "fluid" are borrowed from Cattel's theory (Cattell, 1963), a foundational building block of cognitive science about the source of intelligence.Cattel's theory delineates that crystallized intelligence is semantic knowledge from past experiences, fluid intelligence is the ability to navigate novel situations, and problem-solving uses both.Therefore, we add two intermediate steps to operationalize the two kinds of mechanisms, and aim to measure how well the model recalls and applies knowledge.Last, we compare the intermediate results with the reference answers to score the reasoning sub-steps, hence providing an assessment of the crystallized performance and fluid performance as well as problem-solving performance.</p>
<p>, and s 3 refer to crystallized performance, fluid performance, and problem-solving performance, respectively.The color of the text indicates the model type: blue for open-source and red for proprietary models.The shade represents the ranking, where the darker shade represents the highest score, and the lighter shade represents the second highest score.models, i.e.T5 (Raffel et al., 2020), LLaMA (Touvron et al., 2023a) and LLaMA 2 (Touvron et al., 2023b), which are pre-trained on large scale corpus and not applied to any fine-tuning.Initialized with T5, Flan-T5 (Longpre et al., 2023) and Flan-Alpaca (Chia et al., 2023) are instruction-tuned on Flan V2 (Longpre et al., 2023) and Alpaca (Taori et al., 2023), respectively.Built on LLaMA, Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) are instruction-tuned with responses generated by GPT-3.5, while TÜLU 1 (Wang et al., 2023d) are instruction-tuned with a mixture of both manually curated and distilled dataset.Based on LLaMA 2, LLaMA 2-Chat (Touvron et al., 2023b) is firstly instruction-tuned with high-quality collected annotations, and then aligned with human preferences for the chat use case.LLaMA 3-Instruct and LLaMA 3.1-Instruct respectively finetune LLaMA 3 and LLaMA 3.1 to better understand and follow human instructions.Besides, to perform a fair comparison w.r.t instruction-tuning dataset, we also evaluate LLaMA checkpoints finetuned on other datasets, such as Flan V2 (Longpre et al., 2023) (human-written), Alpaca (modelgenerated) (Taori et al., 2023), ShareGPT (user prompt with model response).</p>
<p>Figure 3 :
3
Figure3: Bar diagram illustrating the relationship between problem-solving performance (s 3 ) and intermediate performance ((s 1 + s 2 )/2).Each bar of intermediate performance is divided into two stacked segments, the lower one denotes s 1 , while the upper one denotes s 2 .</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Problem-solving performance of instructiontuned LLaMA with different model sizes.</p>
<p>Figure 6 :
6
Figure 6: Comparing knowledge-enhanced baselines M+R 1 (b), M+R 1 +R 2 (c) and the original setting (a).</p>
<p>Table 1 :
1
Formulation of cognition-grounded LLMs' capabilities.See Section 2.1 for details. and how the different capabilities correlate.Additionally, such insights allow us to provide tailored guidance to improve training efficiency or facilitate more advanced model development.</p>
<p>Table 2 :
2
Breakdown statistics on source benchmarks employed by FAC 2 E and re-formulation types (Generative or Multiple-choice QA), where Length refers to the average input length of examples.
CapabilityBenchmark#Samples Length QAAgreementsBLiMP (Warstadt et al., 2020)40033MLicensingMarvin and Linzen (2018)1,50036MLong-distance dependencyWilcox et al. (2019)66048MGarden-path effectsFutrell et al. (2018)45040MLexical semanticsPetersen and Potts (2023)1,0008MDeductiveBang et al. (2023)1,60018MInductiveBang et al. (2023)1,50016MAnalogicalWebb et al. (2023)8004GNumericMAWPS (Koncel-Kedziorski et al., 2016)40033GLogicTian et al. (2021)1,00087GManipulatationWei et al. (2022b)25031GFactual KnowledgeLAMA (Petroni et al., 2019)50048GReading ComprehensionDua et al. (2019)1,000195MCommonsenseTalmor et al. (2019)80013MDiscourseWang et al. (2023c)400439GNarrativeXu et al. (2022)200395GPragmaticsHu et al. (2023)150288MTheory of mindUllman (2023)80152G
[Thought], [Action],[Answer], where[Thought]can reason about the current situation,[Action]</p>
<p>encompasses a
ModelModel sizePre-training Fine-tuningT511B1.0T tokensFlan-T511Bas aboveITFlan-Alpaca11Bas aboveITLLaMA7B1.4T tokensAlpaca7Bas aboveITVicuna7Bas aboveITTÜLU 17B,13B,30B,65Bas aboveITLLaMA 27B2.0T tokensLLaMA 2-Chat7Bas aboveIT+RLHFLLaMA 3-Instruct8B15.0TIT+RLHFLLaMA 3.1-Instruct8B16.4TIT+RLHFGPT-3.5175B-IT+RLHFInstructGPT175B-IT+RLHFGPT-4--IT+RLHFBard137B-IT+RLHF</p>
<p>Table 3 :
3
Statistics of examined LLMs, where fine-tuning techniques indicating whether the model is built with instruction tuning (IT) and reinforcement learning with human feedback (RLHF) or not.</p>
<p>Table 3 ,
3
the examined LLMs can be categorized into publicly available open-source models and proprietary ones whose responses are provided through private APIs.Open-source models include three backbone
ModelLINGUISTIC KNOWLEDGE s 1 s 2 s 3FORMAL KNOWLEDGE s 1 s 2 s 3WORLD MODELING s 1 s 2 s 3SOCIAL MODELING s 1 s 2 s 3T583.9926.3947.3977.9728.1033.2674.6124.7426.5366.7918.3119.16Flan-T584.9642.5064.1280.1035.2242.5874.8236.1334.6467.7327.2321.27Flan-Alpaca85.2539.4160.1579.8834.9941.7275.5437.4236.5768.3828.7423.82LLaMA85.3431.3653.7780.0231.1840.0475.5727.4630.4767.5420.1420.75Alpaca86.0245.7868.3982.0338.1053.8577.3041.9149.4269.9329.6132.96Vicuna85.2347.4572.6684.3540.1057.0775.3343.3844.3765.6826.8730.63TÜLU 184.1445.8470.7282.3239.2951.2175.9043.3040.8069.7326.7727.02LLaMA 283.1934.5657.8982.1934.1846.1577.4834.8440.9268.2224.7424.20LLaMA 2-Chat87.0448.9574.4684.0543.2157.1378.4346.0944.4671.0628.8929.59LLaMA 3-Instruct87.7850.2078.5785.8143.8860.7980.1846.7447.7276.9939.1030.52LLaMA 3.1-Instruct88.2151.4782.7187.3344.9465.0181.5447.1150.3977.7830.5131.75GPT-3.587.9153.9182.7285.9345.2070.4781.5353.1867.6877.2336.3440.56InstructGPT88.5255.5085.1985.1244.1867.4880.3451.7865.1674.1739.9045.95GPT-489.3258.9889.6287.6447.9975.9781.8654.7869.4381.2440.9945.71Bard87.7452.3786.1686.9746.0871.6279.3049.0961.3178.6438.2742.53</p>
<p>Table 4 :
4
Quantitative results in terms of four capability dimensions.As stated in Section 2.2, s 1 , s 2</p>
<p>1, append the first reference rationale, i.e.R 1 , to the input question with string concatenation.Then the augmented input is fed into the model with the same instruction as the examined model.Note that we also remove the first triplet of ⟨ [thought], [action], [answer] ⟩ in the input demonstrations for the M+R 1 baseline because we have provided the corresponding reference rationale.As a comparison, following a similar procedure, we also construct another baseline by incorporating both R 1 and R 2 into the
Formal.Skill World.Rem.Ling.Gram. Ling.Seman. Formal.Mech. Ling.Gram. 0.7 0.8 0.9 1.0 LLaMA 2 LLaMA 2+R1 LLaMA 2+R1+R2 LLaMA 2-ChatWorld.Und.Social.ToMSocial.Prag.Figure 7: A 8-dimensional capability map of LLaMA 2model when augmented with different knowledge text.The score is re-scaled through max-min normalizationamong each capability for clarity.
model, denoted as M+R 1 +R 2 .</p>
<p>https://huggingface.co/docs/transformers/ main/en/model_doc/llama
AcknowledgementsThis work is supported by the Canada CIFAR AI Chair Program and the Canada NSERC Discovery Grant (RGPIN-2021-03115).A Implementation DetailsAll of the examined open-source models are based on HuggingFace Transformers package(Wolf et al., 2020).Their model cards, i.e. checkpoints consist of:• T5 (t5-11b),• Flan-T5 (google/flan-t5-xxl),• Flan-Alpaca (declare-lab/flan-alpaca-xxl),• LLaMA 1 ,• Alpaca and LLaMA on Alpaca (allenai/openinstruct-stanford-alpaca-13b),• Vicuna (lmsys/vicuna-7b-v1.1),• TÜLU 1 (allenai/tulu-7b, allenai/tulu-13b, allenai/tulu-30b, allenai/tulu-65b),• LLaMA on Flan V2 (allenai/open-instructflan-v2-13b),• LLaMA on ShareGPT (allenai/open-instructsharegpt-13b),• LLaMA 2 (meta-llama/Llama-2-7b-hf),• LLaMA 2-Chat (meta-llama/Llama-2-7bchat-hf)For the response generation of each target model, as suggested byWei et al. (2022b);Zhou et al. (2022a), we employ 4-shot instruction-following settings, i.e. 4 in-context demonstrations in the input prompt, set the temperature to 0.7 and set the max length of generated sequences as 1024.For automatic metrics, we leverage the official implementation of BARTScore(Yuan et al., 2021).After collecting instances from various benchmarks as summarized in Table2, we remove those instances where the input length is longer than 2048, maximal context length during training except T5, Flan-T5, and Flan-Alpaca,We conduct evaluation experiments on 2 A100 GPUs and report the average results of a total of ten runs for each model on each benchmark.For the capabilities involving multiple benchmarks, the overall score are calculated as the arithmetic mean of crystallized performance (s 1 ), fluid performance (s 2 ), or problem-solving performance (s 3 ).B Instruction DesignSee Figure8and Figure9for full version example of capability-specific instruction when evaluating the analogical reasoning and grammaticality.
Josh References, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>The social brain: neural basis of social knowledge. Ralph Adolphs, Annual review of psychology. 602009</p>
<p>A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, arXiv:2204.05862Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliNusa Dua2022. 20231arXiv preprintTraining a helpful and harmless assistant with reinforcement learning from human feedback. Association for Computational Linguistics</p>
<p>Masha Belyi, Robert Friel, Shuai Shao, Atindriyo Sanyal, arXiv:2406.00975Luna: An evaluation foundation model to catch language model hallucinations with high accuracy and low cost. 2024arXiv preprint</p>
<p>Piqa: Reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>Theory of fluid and crystallized intelligence: A critical experiment. Raymond B Cattell, Journal of educational psychology. 54111963</p>
<p>Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, arXiv:2308.13198Journey to the center of the knowledge neurons: Discoveries of language-independent knowledge neurons and degenerate knowledge neurons. Jun Zhao. 2023arXiv preprint</p>
<p>Ken Yew, Pengfei Chia, Hong, arXiv:2306.04757Lidong Bing, and Soujanya Poria. 2023. Instructeval: Towards holistic evaluation of instruction-tuned large language models. arXiv preprint</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, 2023. April 202314</p>
<p>François Chollet, arXiv:1911.01547On the measure of intelligence. 2019arXiv preprint</p>
<p>Knowledge neurons in pretrained transformers. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei, 10.18653/v1/2022.acl-long.581Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics20191Minneapolis</p>
<p>The multiple-demand (md) system of the primate brain: mental programs for intelligent behaviour. John Duncan, Trends in cognitive sciences. 1442010</p>
<p>Language and thought are not the same thing: evidence from neuroimaging and neurological patients. Evelina Fedorenko, Rosemary Varley, Annals of the New York Academy of Sciences. 136912016</p>
<p>Richard Futrell, Ethan Wilcox, Takashi Morita, Roger Levy, arXiv:1809.01329Rnns as psycholinguistic subjects: Syntactic state and grammatical dependency. 2018arXiv preprint</p>
<p>Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, Mcdonell, 10.5281/zenodo.10256836Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite. Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf; Ben Wang, Kevin Wangand Andy Zou. 2023. A framework for few-shot language model evaluation</p>
<p>Theories of lexical semantics. Dirk Geeraerts, 2009OUPOxford</p>
<p>Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. Hila Gonen, Yoav Goldberg, Proceedings of the 2019 Workshop on Widening NLP. the 2019 Workshop on Widening NLPFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Dingo: Towards diverse and fine-grained instruction-following evaluation. Zihui Gu, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Chengzhong Xu, Ju Fan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song, arXiv:2305.15717The false promise of imitating proprietary llms. 2023arXiv preprint</p>
<p>How close is chatgpt to human experts? comparison corpus, evaluation, and detection. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu, arXiv:2301.075972023arXiv preprint</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. 2020</p>
<p>A finegrained comparison of pragmatic language understanding in humans and language models. Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko, Edward Gibson, 10.18653/v1/2023.acl-long.230Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>Prometheus: Inducing finegrained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo, The Twelfth International Conference on Learning Representations. 2024a</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, arXiv:2405.015352024barXiv preprint</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, 10.18653/v1/N16-1136Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational Linguistics2016</p>
<p>Theory of mind might have spontaneously emerged in large language models. Michal Kosinski, 2023Preprint at</p>
<p>Reliability in content analysis: Some common misconceptions and recommendations. Klaus Krippendorff, Human communication research. 3032004</p>
<p>HaluEval: A large-scale hallucination evaluation benchmark for large language models. Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, 10.18653/v1/2023.emnlp-main.397Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, arXiv:2211.09110arXiv:2301.13688The flan collection: Designing data and methods for effective instruction tuning. Ananya Kumar, et al. 2022. 2023arXiv preprintHolistic evaluation of language models</p>
<p>Gender and representation bias in GPT-3 generated stories. Li Lucy, David Bamman, 10.18653/v1/2021.nuse-1.5Proceedings of the Third Workshop on Narrative Understanding. the Third Workshop on Narrative UnderstandingVirtual. Association for Computational Linguistics2021</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, Dissociating language and thought in large language models. arXiv e-prints. 20232301</p>
<p>On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?. Kamil Malinka, Martin Peresíni, Anton Firc, Ondrej Hujnák, Filip Janus, Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. the 2023 Conference on Innovation and Technology in Computer Science Education V20231</p>
<p>Targeted syntactic evaluation of language models. Rebecca Marvin, Tal Linzen, 10.18653/v1/D18-1151Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Massediting memory in a transformer. Kevin Meng, Sen Arnab, Alex J Sharma, Yonatan Andonian, David Belinkov, Bau, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann Lecun, Thomas Scialom, arXiv:2311.12983Gaia: a benchmark for general ai assistants. 2023arXiv preprint</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, arXiv:2303.13375Capabilities of gpt-4 on medical challenge problems. 2023arXiv preprint</p>
<p>Chatgpt: Optimizing language models for dialogue. 2022OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, Advances in Neural Information Processing Systems. 2022</p>
<p>Lexical semantics with large language models: A case study of English "break. Erika Petersen, Christopher Potts, 10.18653/v1/2023.findings-eacl.36Findings of the Association for Computational Linguistics: EACL 2023. Dubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>On extractive and abstractive neural document summarization with transformer language models. Jonathan Pilault, Raymond Li, Sandeep Subramanian, Chris Pal, 10.18653/v1/2020.emnlp-main.748Proceedings of the 2020 Conference on Empirical Methods in Natural Language (EMNLP). the 2020 Conference on Empirical Methods in Natural Language (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 212020</p>
<p>Machine reading, fast and slow: When do models "understand" language?. Ray Sagnik, Anna Choudhury, Isabelle Rogers, Augenstein, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022International Committee on Computational Linguistics</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.12022Gpqa: A graduate-level google-proof q&amp;a benchmark. 2023arXiv preprint</p>
<p>Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension. Anna Rogers, Matt Gardner, Isabelle Augenstein, 2023ACM Computing Surveys55</p>
<p>A new fun and robust version of an fmri localizer for the frontotemporal language system. Terri L Scott, Jeanne Gallée, Evelina Fedorenko, Cognitive neuroscience. 20178</p>
<p>Unsupervised commonsense question answering with self-talk. Vered Shwartz, Peter West, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.18653/v1/2020.emnlp-main.373Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, Transactions on Machine Learning Research. 2023</p>
<p>Assessing the benchmarking capacity of machine reading comprehension datasets. Saku Sugawara, Pontus Stenetorp, Kentaro Inui, Akiko Aizawa, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, arXiv:2401.05561Trustllm: Trustworthiness in large language models. 2024arXiv preprint</p>
<p>Meta-prompting: Enhancing language models with task-agnostic scaffolding. Mirac Suzgun, Adam Tauman, Kalai , arXiv:2401.129542024arXiv preprint</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Language-specific neurons: The key to multilingual capabilities in large language models. Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, Ji-Rong Wen, arXiv:2402.164382024arXiv preprint</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Automatic machine translation evaluation in many languages via zero-shot paraphrasing. Brian Thompson, Matt Post, 10.18653/v1/2020.emnlp-main.8Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Diagnosing the firstorder logical reasoning ability through LogicNLI. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, 10.18653/v1/2021.emnlp-main.303Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. Tomer Ullman, arXiv:2302.083992023arXiv preprint</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 201932</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, 10.18653/v1/W18-5446Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, arXiv:2306.11698Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. 2023aarXiv preprint</p>
<p>On the robustness of chatgpt: An adversarial and out-ofdistribution perspective. Jindong Wang, Wenxin Hu Xixu, Hao Hou, Runkai Chen, Yidong Zheng, Linyi Wang, Wei Yang, Haojun Ye, Xiubo Huang, Geng, ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models. 2023b</p>
<p>Disco-bench: A discourse-aware evaluation benchmark for language modelling. Longyue Wang, Zefeng Du, Donghuai Liu, Cai Deng, Dian Yu, Haiyun Jiang, Yan Wang, Leyang Cui, Shuming Shi, Zhaopeng Tu, arXiv:2307.080742023carXiv preprint</p>
<p>Feeding what you need by understanding what you learned. Xiaoqiang Wang, Bang Liu, Fangli Xu, 10.18653/v1/2022.acl-long.403Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsBo Long; Dublin, IrelandAssociation for Computational Linguistics2022a1Long Papers)</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Raghavi Khyathi, David Chandu, Kelsey Wadden, Noah A Macmillan, Iz Smith, Beltagy, arXiv:2306.047512023darXiv preprint</p>
<p>Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Rohitha Phani, Pulkit Kaza, Ravsehaj Verma, Rushang Singh Puri, Savan Karia, Doshi, Keyur Shailaja, Siddhartha Sampat, Sujan Mishra, A Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, 10.18653/v1/2022.emnlp-main.340Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022b</p>
<p>BLiMP: The benchmark of linguistic minimal pairs for English. Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, Samuel R Bowman, 10.1162/tacl_a_003212020Transactions of the Association for Computational Linguistics8</p>
<p>Emergent analogical reasoning in large language models. Taylor Webb, Keith J Holyoak, Hongjing Lu, Nature Human Behaviour. 792023</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Transactions on Machine Learning Research. 2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Structural supervision improves learning of non-local grammatical dependencies. Ethan Wilcox, Peng Qian, Richard Futrell, Miguel Ballesteros, Roger Levy, 10.18653/v1/N19-1334Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, Danqi Chen, arXiv:2402.04333Less: Selecting influential data for targeted instruction tuning. 2024arXiv preprint</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023arXiv preprint</p>
<p>Fantastic questions and where to find them: FairytaleQA -an authentic dataset for narrative comprehension. Ying Xu, Dakuo Wang, Mo Yu, Daniel Ritchie, Bingsheng Yao, Tongshuang Wu, Zheng Zhang, Toby Li, Nora Bradford, Branda Sun, Tran Hoang, Yisi Sang, Yufang Hou, Xiaojuan Ma, Diyi Yang, Nanyun Peng, Zhou Yu, Mark Warschauer, 10.18653/v1/2022.acl-long.34Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Glue-x: Evaluating natural language understanding models from an outof-distribution generalization perspective. Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, Yue Zhang, arXiv:2211.080732022arXiv preprint</p>
<p>FLASK: Fine-grained language model evaluation based on alignment skill sets. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Kola: Carefully benchmarking world knowledge of large language models. Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>HellaSwag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, 10.18653/v1/P19-1472Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, arXiv:2401.01286arXiv:2402.14700Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024b. Unveiling linguistic regions in large language models. 2024aarXiv preprintA comprehensive study of knowledge editing for large language models</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, arXiv:2308.079212023arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, The Eleventh International Conference on Learning Representations. 2022a</p>
<p>Think before you speak: Explicitly generating implicit commonsense knowledge for response generation. Pei Zhou, Karthik Gopalakrishnan, Behnam Hedayatnia, Seokhwan Kim, Jay Pujara, Xiang Ren, Yang Liu, Dilek Hakkani-Tur, 10.18653/v1/2022.acl-long.88Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022b1</p>
<p>Given three words, i.e. A, B, and C in a format of A:B::C:?, which means that A implies B by some relationship, reason with this relationship and predict a word D such that C implies D by the same relationship. A:B is a reference pair in some relationship, complete the pair of C:D in the same relationship as A:B. Please solve the task by interleaving Thought, Action, and Answer steps. Thought can reason about the current situation, and Action can be the following two types</p>
<p>Follow-up Question[question], which returns a sub-question with a single answer that helps solve the original question. </p>
<p>Full version example of the capability-specific instruction. Instruction: Solve a question-answering task judging which one of the minimal pairs is acceptable and grammatical. The minimal pairs consist of two sentences that differ by a few words, one of them is grammatical, but another is ungrammatical. Please solve the task by interleaving Thought, Action, and Answer steps. 8Follow-up Question. Thought can reason about the current situation, and Action can be the following two types</p>
<p>Follow-up Question[question], which returns a sub-question with a single answer that helps solve the original question. </p>
<p>which means no more sub-questions. The final answer should be generated in the following line. [Demonstration Question]: Which sentence of the following two sentences is grammatical? FirstSentence[No author that no senators liked has had any success. </p>
<p>Action 1]: Follow-up Question[In what syntactic-semantic-environment can the word "any" be used?] [Answer 1]: To a first approximation, it can be only in the scope of negation. Secondsentence, Follow-up Question. FirstSentence is grammatical and SecondSentnce is ungrammaticalThe SecondSentence of minimal pairs lacks a negation structure, so it is ungrammatical. More Demonstration Questions. Input Question]: Figure 9: Full version example of the capability-specific instruction</p>            </div>
        </div>

    </div>
</body>
</html>
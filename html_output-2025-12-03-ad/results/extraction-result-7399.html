<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7399 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7399</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7399</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-270702371</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.16308v1.pdf" target="_blank">Anomaly Detection of Tabular Data Using LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown their potential in long-context understanding and mathematical reasoning. In this paper, we study the problem of using LLMs to detect tabular anomalies and show that pre-trained LLMs are zero-shot batch-level anomaly detectors. That is, without extra distribution-specific model fitting, they can discover hidden outliers in a batch of data, demonstrating their ability to identify low-density data regions. For LLMs that are not well aligned with anomaly detection and frequently output factual errors, we apply simple yet effective data-generating processes to simulate synthetic batch-level anomaly detection datasets and propose an end-to-end fine-tuning strategy to bring out the potential of LLMs in detecting real anomalies. Experiments on a large anomaly detection benchmark (ODDS) showcase i) GPT-4 has on-par performance with the state-of-the-art transductive learning-based anomaly detection methods and ii) the efficacy of our synthetic dataset and fine-tuning strategy in aligning LLMs to this task.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7399.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7399.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary instruction-tuned large decoder-only transformer model from OpenAI; evaluated as a zero-shot batch-level anomaly detector on serialized tabular features using natural-language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary, decoder-only transformer, instruction-tuned for conversational and task-driven generation (accessed via API).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary / not disclosed</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Zero-shot prompting on serialized per-feature scalars (batch-level, text-formulation); outputs indices of anomalous rows which are aggregated across features into anomaly scores.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Serialize each 1-D feature as: "Data 1 is x1. Data 2 is x2. ... Data N is xN." with task description: "Abnormal data differ from the majority. Which data are abnormal?" and a system message: "Only answer data indices."</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Numeric tabular (per-feature scalar lists); continuous and categorical features evaluated separately.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ODDS benchmark (Rayana, 2016) for real datasets; additional synthetic contaminated distributions used for qualitative analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUROC (primary reported metric); additional qualitative kernel-density analysis on predicted anomalies for synthetic distribution</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as on-par with state-of-the-art transductive method ECOD on the ODDS benchmark (exact AUROC numbers not provided in paper text excerpt). Qualitatively, GPT-4 strongly outperforms non-finetuned open models in zero-shot batch-level AD.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against transductive learning-based baselines (ECOD, KNN) and other LLMs (GPT-3.5, Llama-2, Mistral); GPT-4 was reported to be comparable to ECOD.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>None specific beyond general LLM instruction-following caveats; GPT-4 followed the restricted output format and produced parseable index lists in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Inference via OpenAI API (gpt-4-1106preview); no explicit GPU-hours, latency or cost numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Detection of Tabular Data Using LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7399.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7399.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5 (gpt-3.5-turbo-1106)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary instruction-tuned decoder-only transformer from OpenAI evaluated as a zero-shot batch-level anomaly detector using the same serialization + prompt procedure as GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-1106)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary, instruction-tuned, decoder-only transformer (accessed via API).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary / not disclosed</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Zero-shot prompting on serialized per-feature scalars; model outputs indices of anomalous rows aggregated across features.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Same serialization: "Data 1 is x1. ... Data N is xN." plus: "Abnormal data differ from the majority. Which data are abnormal?" and system message: "Only answer data indices."</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Numeric tabular (per-feature scalar lists)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ODDS benchmark (Rayana, 2016)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported but exact AUROC numbers not included in text excerpt; paper states GPT-3.5 performs worse than GPT-4 and that a fine-tuned Mistral-AD outperforms GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against GPT-4, fine-tuned LLMs (Llama2-AD, Mistral-AD), and transductive baselines (ECOD, KNN).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less capable than GPT-4 in these zero-shot batch-level AD experiments (no specific failure modes enumerated beyond relative performance).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Inference via OpenAI API (gpt-3.5-turbo-1106); no explicit cost or latency numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Detection of Tabular Data Using LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7399.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7399.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 (open-source)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source decoder-only transformer family evaluated both as-is and after fine-tuning; baseline (unfine-tuned) versions frequently produced factual/output-format errors for index-based anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (evaluated both 70B and 7B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source decoder-only transformer family; instruction-following capability varies by variant and alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (evaluated, made mistakes) and 7B (used for fine-tuning experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Zero-shot prompting with same serialization/prompt template; responses parsed to extract anomalous indices and aggregated across features.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Serialization: "Data 1 is x1. ..." + "Abnormal data differ from the majority. Which data are abnormal?" with system message: "Only answer data indices." For models that produced redundant text, regex-guided constrained decoding was used.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Numeric tabular (per-feature scalar lists)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ODDS benchmark for evaluation; synthetic batches used for diagnostic examples</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUROC (reported for benchmark evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Unfine-tuned Llama-2 (70B) produced factual errors (missed anomalies, false positives); quantitative AUROC numbers not provided in excerpt but baseline performance was substantially improved after fine-tuning (average AUROC increase reported later).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against GPT-3.5, GPT-4, Mistral and their fine-tuned counterparts; also compared qualitatively to ECOD and KNN baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>zero-shot (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Observed failure modes: missing anomalies (false negatives), false positives, pairing incorrect indices and values, generating indices beyond batch length, and listing every data point as abnormal; instruction-following inconsistencies required output-probability modifications / regex constraints to parse predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Evaluated on an A6000 GPU; specific runtime or FLOP/token measures not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Detection of Tabular Data Using LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7399.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7399.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-AD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 fine-tuned for Anomaly Detection (Llama2-AD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Llama-2 (7B) model aligned to batch-level anomaly detection via supervised end-to-end LoRA fine-tuning on a synthetic dataset of labeled normal/anomalous batches; significantly reduced factual/output errors compared to the unfine-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (7B) fine-tuned with LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source decoder-only transformer (7B) with low-rank adaptation (LoRA) applied for parameter-efficient supervised fine-tuning; original weights frozen, LoRA weights trained and then merged.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (base) with low-rank adapters (LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Supervised end-to-end fine-tuning on synthetic labeled batches (text-serialized inputs and ground-truth index-list outputs); then used to predict anomalies on real ODDS benchmark and synthetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Same serialization/prompt as zero-shot: per-feature serializations with task prompt and enforced output format; training targets were full tokenized index-list responses (e.g., "Data a1, a2, ... are abnormal." or "All rows are normal.").</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Synthetic dataset: 5000 training batches (2500 continuous batches generated by Gaussian mixture with narrow normal and wide anomalous component; 2500 discrete batches from categorical mixture); contamination ratio π sampled <0.2. Validation: 400 batches. Fine-tuned with LoRA (learning rate 1e-3), Llama-2 trained for 5 epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Numeric continuous and categorical tabular features (serialized as 1-D lists per-feature during training and inference).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Synthetic labeled batches for fine-tuning; evaluation on ODDS benchmark (Rayana, 2016) and additional synthetic contaminated distributions for qualitative analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUROC for ODDS benchmark; qualitative kernel-density estimation for synthetic distribution experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Fine-tuning produced substantial improvements over unfine-tuned Llama-2; paper reports an average AUROC increase of ~8.9 points for Llama-2 after fine-tuning (exact per-dataset numbers not provided in excerpt). Llama2-AD was able to correctly identify anomalies in toy examples where unfine-tuned Llama-2 failed.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to unfine-tuned Llama-2 and to transductive baselines (ECOD, KNN); results show significant improvement from fine-tuning and in some cases better than other unfine-tuned LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>fully supervised fine-tuning (LoRA) on synthetic labeled batches</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Model still depends on the synthetic data coverage; independence-of-features assumption was used during training and evaluation (features treated separately), which may limit performance when cross-feature interactions matter.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Fine-tuned on A6000 GPU using LoRA; Llama-2 fine-tuned for five epochs with learning rate 1e-3. No wall-clock GPU-hours or token-costs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Detection of Tabular Data Using LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7399.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7399.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral (open-source)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source decoder-only transformer evaluated as an unfine-tuned zero-shot batch-level anomaly detector using the same text-serialization and prompting approach; used both before and after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral (evaluated 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source decoder-only transformer family; instruction-following capability varies and benefit from fine-tuning for structured-output tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Zero-shot prompting on per-feature serialized scalars, with outputs parsed for anomalous indices, aggregated across features.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Same serialization and prompt: "Data i is xi." per row, plus "Abnormal data differ from the majority. Which data are abnormal?" and system message "Only answer data indices."</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Numeric tabular (per-feature scalar lists)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ODDS benchmark; synthetic distributions for diagnostics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Unfine-tuned Mistral showed lower performance compared to GPT-4; after fine-tuning (Mistral-AD) it showed an average AUROC increase of ~6.7 points (exact numbers not provided in excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against GPT-3.5, GPT-4, Llama-2 (baseline and fine-tuned), and transductive baselines (ECOD, KNN).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>zero-shot (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Unfine-tuned model sometimes emits redundant or unstructured text making extraction harder; required regex-guided constrained decoding or manual filtering in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Evaluated on A6000 GPU; generation hyperparameters temperature=0.75 and top-p=0.9 used. No explicit runtime/cost numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Detection of Tabular Data Using LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7399.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7399.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-AD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral fine-tuned for Anomaly Detection (Mistral-AD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Mistral-7B model fine-tuned with LoRA on the synthetic labeled batches to align the model to produce concise index-list anomaly outputs; demonstrated improved AUROC and captured low-density regions in synthetic distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral (7B) fine-tuned with LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source decoder-only transformer (7B) with LoRA adapters trained in supervised manner on synthetic example batches to produce target index-list outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (base) with LoRA adapters</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Supervised LoRA fine-tuning on synthetic labeled batches (end-to-end learning of input-to-index-list mapping), then inference via same serialization + prompt; aggregation of per-feature outputs into anomaly scores.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Same as for other models; training paired serialized input X_b with ground-truth textual responses Y_b (e.g., "Data a1, a2 are abnormal." or "All rows are normal.").</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Synthetic dataset: 5000 training batches (2500 continuous, 2500 discrete) generated by Gaussian-mixture (continuous) and categorical-mixture (discrete) processes; contamination ratio π sampled <0.2. Validation: 400 batches. Fine-tuned for 2 epochs with learning rate 1e-3 using LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Continuous and discrete tabular features (serialized per-feature into scalar lists).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Synthetic labeled batches for fine-tuning; evaluated on ODDS benchmark and synthetic contaminated distributions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUROC on ODDS; kernel-density estimation and qualitative plots for synthetic low-density capture</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Fine-tuned Mistral-AD improved substantially over unfine-tuned Mistral (average AUROC increase ~6.7 points). Mistral-AD was used to collect predicted anomalies across batches and demonstrated ability to capture low-density regions in a synthetic contaminated distribution (qualitative KDE results shown). Paper claims Mistral-AD outperforms GPT-3.5 in benchmark comparisons (exact AUROC numbers not provided in excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Improved versus unfine-tuned Mistral and compared to other LLMs; also compared qualitatively against transductive baselines (ECOD, KNN).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>fully supervised fine-tuning (LoRA) on synthetic labeled batches</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dependence on synthetic training distribution to align behavior; feature-independence assumption (per-feature detection and aggregation) could limit performance when cross-feature anomalies exist.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Fine-tuned on A6000 GPU with LoRA for 2 epochs at learning rate 1e-3; no explicit wall-clock GPU-hours or token-costs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Detection of Tabular Data Using LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ODDS library <em>(Rating: 2)</em></li>
                <li>Ecod: Unsupervised outlier detection using empirical cumulative distribution functions <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
                <li>Data wrangling with language models (Narayan et al.) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7399",
    "paper_id": "paper-270702371",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A proprietary instruction-tuned large decoder-only transformer model from OpenAI; evaluated as a zero-shot batch-level anomaly detector on serialized tabular features using natural-language prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary, decoder-only transformer, instruction-tuned for conversational and task-driven generation (accessed via API).",
            "model_size": "proprietary / not disclosed",
            "anomaly_detection_approach": "Zero-shot prompting on serialized per-feature scalars (batch-level, text-formulation); outputs indices of anomalous rows which are aggregated across features into anomaly scores.",
            "prompt_template": "Serialize each 1-D feature as: \"Data 1 is x1. Data 2 is x2. ... Data N is xN.\" with task description: \"Abnormal data differ from the majority. Which data are abnormal?\" and a system message: \"Only answer data indices.\"",
            "training_data": null,
            "data_type": "Numeric tabular (per-feature scalar lists); continuous and categorical features evaluated separately.",
            "dataset_name": "ODDS benchmark (Rayana, 2016) for real datasets; additional synthetic contaminated distributions used for qualitative analysis",
            "evaluation_metric": "AUROC (primary reported metric); additional qualitative kernel-density analysis on predicted anomalies for synthetic distribution",
            "performance": "Reported as on-par with state-of-the-art transductive method ECOD on the ODDS benchmark (exact AUROC numbers not provided in paper text excerpt). Qualitatively, GPT-4 strongly outperforms non-finetuned open models in zero-shot batch-level AD.",
            "baseline_comparison": "Compared against transductive learning-based baselines (ECOD, KNN) and other LLMs (GPT-3.5, Llama-2, Mistral); GPT-4 was reported to be comparable to ECOD.",
            "zero_shot_or_few_shot": "zero-shot",
            "limitations_or_failure_cases": "None specific beyond general LLM instruction-following caveats; GPT-4 followed the restricted output format and produced parseable index lists in the experiments.",
            "computational_cost": "Inference via OpenAI API (gpt-4-1106preview); no explicit GPU-hours, latency or cost numbers reported.",
            "uuid": "e7399.0",
            "source_info": {
                "paper_title": "Anomaly Detection of Tabular Data Using LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "Generative Pre-trained Transformer 3.5 (gpt-3.5-turbo-1106)",
            "brief_description": "A proprietary instruction-tuned decoder-only transformer from OpenAI evaluated as a zero-shot batch-level anomaly detector using the same serialization + prompt procedure as GPT-4.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-1106)",
            "model_description": "Proprietary, instruction-tuned, decoder-only transformer (accessed via API).",
            "model_size": "proprietary / not disclosed",
            "anomaly_detection_approach": "Zero-shot prompting on serialized per-feature scalars; model outputs indices of anomalous rows aggregated across features.",
            "prompt_template": "Same serialization: \"Data 1 is x1. ... Data N is xN.\" plus: \"Abnormal data differ from the majority. Which data are abnormal?\" and system message: \"Only answer data indices.\"",
            "training_data": null,
            "data_type": "Numeric tabular (per-feature scalar lists)",
            "dataset_name": "ODDS benchmark (Rayana, 2016)",
            "evaluation_metric": "AUROC",
            "performance": "Reported but exact AUROC numbers not included in text excerpt; paper states GPT-3.5 performs worse than GPT-4 and that a fine-tuned Mistral-AD outperforms GPT-3.5.",
            "baseline_comparison": "Compared against GPT-4, fine-tuned LLMs (Llama2-AD, Mistral-AD), and transductive baselines (ECOD, KNN).",
            "zero_shot_or_few_shot": "zero-shot",
            "limitations_or_failure_cases": "Less capable than GPT-4 in these zero-shot batch-level AD experiments (no specific failure modes enumerated beyond relative performance).",
            "computational_cost": "Inference via OpenAI API (gpt-3.5-turbo-1106); no explicit cost or latency numbers reported.",
            "uuid": "e7399.1",
            "source_info": {
                "paper_title": "Anomaly Detection of Tabular Data Using LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama-2 (baseline)",
            "name_full": "Llama-2 (open-source)",
            "brief_description": "Open-source decoder-only transformer family evaluated both as-is and after fine-tuning; baseline (unfine-tuned) versions frequently produced factual/output-format errors for index-based anomaly detection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2 (evaluated both 70B and 7B variants)",
            "model_description": "Open-source decoder-only transformer family; instruction-following capability varies by variant and alignment.",
            "model_size": "70B (evaluated, made mistakes) and 7B (used for fine-tuning experiments)",
            "anomaly_detection_approach": "Zero-shot prompting with same serialization/prompt template; responses parsed to extract anomalous indices and aggregated across features.",
            "prompt_template": "Serialization: \"Data 1 is x1. ...\" + \"Abnormal data differ from the majority. Which data are abnormal?\" with system message: \"Only answer data indices.\" For models that produced redundant text, regex-guided constrained decoding was used.",
            "training_data": null,
            "data_type": "Numeric tabular (per-feature scalar lists)",
            "dataset_name": "ODDS benchmark for evaluation; synthetic batches used for diagnostic examples",
            "evaluation_metric": "AUROC (reported for benchmark evaluations)",
            "performance": "Unfine-tuned Llama-2 (70B) produced factual errors (missed anomalies, false positives); quantitative AUROC numbers not provided in excerpt but baseline performance was substantially improved after fine-tuning (average AUROC increase reported later).",
            "baseline_comparison": "Compared against GPT-3.5, GPT-4, Mistral and their fine-tuned counterparts; also compared qualitatively to ECOD and KNN baselines.",
            "zero_shot_or_few_shot": "zero-shot (baseline)",
            "limitations_or_failure_cases": "Observed failure modes: missing anomalies (false negatives), false positives, pairing incorrect indices and values, generating indices beyond batch length, and listing every data point as abnormal; instruction-following inconsistencies required output-probability modifications / regex constraints to parse predictions.",
            "computational_cost": "Evaluated on an A6000 GPU; specific runtime or FLOP/token measures not reported.",
            "uuid": "e7399.2",
            "source_info": {
                "paper_title": "Anomaly Detection of Tabular Data Using LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama2-AD",
            "name_full": "Llama-2 fine-tuned for Anomaly Detection (Llama2-AD)",
            "brief_description": "A Llama-2 (7B) model aligned to batch-level anomaly detection via supervised end-to-end LoRA fine-tuning on a synthetic dataset of labeled normal/anomalous batches; significantly reduced factual/output errors compared to the unfine-tuned model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2 (7B) fine-tuned with LoRA",
            "model_description": "Open-source decoder-only transformer (7B) with low-rank adaptation (LoRA) applied for parameter-efficient supervised fine-tuning; original weights frozen, LoRA weights trained and then merged.",
            "model_size": "7B (base) with low-rank adapters (LoRA)",
            "anomaly_detection_approach": "Supervised end-to-end fine-tuning on synthetic labeled batches (text-serialized inputs and ground-truth index-list outputs); then used to predict anomalies on real ODDS benchmark and synthetic tasks.",
            "prompt_template": "Same serialization/prompt as zero-shot: per-feature serializations with task prompt and enforced output format; training targets were full tokenized index-list responses (e.g., \"Data a1, a2, ... are abnormal.\" or \"All rows are normal.\").",
            "training_data": "Synthetic dataset: 5000 training batches (2500 continuous batches generated by Gaussian mixture with narrow normal and wide anomalous component; 2500 discrete batches from categorical mixture); contamination ratio π sampled &lt;0.2. Validation: 400 batches. Fine-tuned with LoRA (learning rate 1e-3), Llama-2 trained for 5 epochs.",
            "data_type": "Numeric continuous and categorical tabular features (serialized as 1-D lists per-feature during training and inference).",
            "dataset_name": "Synthetic labeled batches for fine-tuning; evaluation on ODDS benchmark (Rayana, 2016) and additional synthetic contaminated distributions for qualitative analysis",
            "evaluation_metric": "AUROC for ODDS benchmark; qualitative kernel-density estimation for synthetic distribution experiments",
            "performance": "Fine-tuning produced substantial improvements over unfine-tuned Llama-2; paper reports an average AUROC increase of ~8.9 points for Llama-2 after fine-tuning (exact per-dataset numbers not provided in excerpt). Llama2-AD was able to correctly identify anomalies in toy examples where unfine-tuned Llama-2 failed.",
            "baseline_comparison": "Compared to unfine-tuned Llama-2 and to transductive baselines (ECOD, KNN); results show significant improvement from fine-tuning and in some cases better than other unfine-tuned LLMs.",
            "zero_shot_or_few_shot": "fully supervised fine-tuning (LoRA) on synthetic labeled batches",
            "limitations_or_failure_cases": "Model still depends on the synthetic data coverage; independence-of-features assumption was used during training and evaluation (features treated separately), which may limit performance when cross-feature interactions matter.",
            "computational_cost": "Fine-tuned on A6000 GPU using LoRA; Llama-2 fine-tuned for five epochs with learning rate 1e-3. No wall-clock GPU-hours or token-costs reported.",
            "uuid": "e7399.3",
            "source_info": {
                "paper_title": "Anomaly Detection of Tabular Data Using LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Mistral (baseline)",
            "name_full": "Mistral (open-source)",
            "brief_description": "An open-source decoder-only transformer evaluated as an unfine-tuned zero-shot batch-level anomaly detector using the same text-serialization and prompting approach; used both before and after fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral (evaluated 7B)",
            "model_description": "Open-source decoder-only transformer family; instruction-following capability varies and benefit from fine-tuning for structured-output tasks.",
            "model_size": "7B (used in experiments)",
            "anomaly_detection_approach": "Zero-shot prompting on per-feature serialized scalars, with outputs parsed for anomalous indices, aggregated across features.",
            "prompt_template": "Same serialization and prompt: \"Data i is xi.\" per row, plus \"Abnormal data differ from the majority. Which data are abnormal?\" and system message \"Only answer data indices.\"",
            "training_data": null,
            "data_type": "Numeric tabular (per-feature scalar lists)",
            "dataset_name": "ODDS benchmark; synthetic distributions for diagnostics",
            "evaluation_metric": "AUROC",
            "performance": "Unfine-tuned Mistral showed lower performance compared to GPT-4; after fine-tuning (Mistral-AD) it showed an average AUROC increase of ~6.7 points (exact numbers not provided in excerpt).",
            "baseline_comparison": "Compared against GPT-3.5, GPT-4, Llama-2 (baseline and fine-tuned), and transductive baselines (ECOD, KNN).",
            "zero_shot_or_few_shot": "zero-shot (baseline)",
            "limitations_or_failure_cases": "Unfine-tuned model sometimes emits redundant or unstructured text making extraction harder; required regex-guided constrained decoding or manual filtering in some cases.",
            "computational_cost": "Evaluated on A6000 GPU; generation hyperparameters temperature=0.75 and top-p=0.9 used. No explicit runtime/cost numbers reported.",
            "uuid": "e7399.4",
            "source_info": {
                "paper_title": "Anomaly Detection of Tabular Data Using LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Mistral-AD",
            "name_full": "Mistral fine-tuned for Anomaly Detection (Mistral-AD)",
            "brief_description": "A Mistral-7B model fine-tuned with LoRA on the synthetic labeled batches to align the model to produce concise index-list anomaly outputs; demonstrated improved AUROC and captured low-density regions in synthetic distributions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral (7B) fine-tuned with LoRA",
            "model_description": "Open-source decoder-only transformer (7B) with LoRA adapters trained in supervised manner on synthetic example batches to produce target index-list outputs.",
            "model_size": "7B (base) with LoRA adapters",
            "anomaly_detection_approach": "Supervised LoRA fine-tuning on synthetic labeled batches (end-to-end learning of input-to-index-list mapping), then inference via same serialization + prompt; aggregation of per-feature outputs into anomaly scores.",
            "prompt_template": "Same as for other models; training paired serialized input X_b with ground-truth textual responses Y_b (e.g., \"Data a1, a2 are abnormal.\" or \"All rows are normal.\").",
            "training_data": "Synthetic dataset: 5000 training batches (2500 continuous, 2500 discrete) generated by Gaussian-mixture (continuous) and categorical-mixture (discrete) processes; contamination ratio π sampled &lt;0.2. Validation: 400 batches. Fine-tuned for 2 epochs with learning rate 1e-3 using LoRA.",
            "data_type": "Continuous and discrete tabular features (serialized per-feature into scalar lists).",
            "dataset_name": "Synthetic labeled batches for fine-tuning; evaluated on ODDS benchmark and synthetic contaminated distributions",
            "evaluation_metric": "AUROC on ODDS; kernel-density estimation and qualitative plots for synthetic low-density capture",
            "performance": "Fine-tuned Mistral-AD improved substantially over unfine-tuned Mistral (average AUROC increase ~6.7 points). Mistral-AD was used to collect predicted anomalies across batches and demonstrated ability to capture low-density regions in a synthetic contaminated distribution (qualitative KDE results shown). Paper claims Mistral-AD outperforms GPT-3.5 in benchmark comparisons (exact AUROC numbers not provided in excerpt).",
            "baseline_comparison": "Improved versus unfine-tuned Mistral and compared to other LLMs; also compared qualitatively against transductive baselines (ECOD, KNN).",
            "zero_shot_or_few_shot": "fully supervised fine-tuning (LoRA) on synthetic labeled batches",
            "limitations_or_failure_cases": "Dependence on synthetic training distribution to align behavior; feature-independence assumption (per-feature detection and aggregation) could limit performance when cross-feature anomalies exist.",
            "computational_cost": "Fine-tuned on A6000 GPU with LoRA for 2 epochs at learning rate 1e-3; no explicit wall-clock GPU-hours or token-costs reported.",
            "uuid": "e7399.5",
            "source_info": {
                "paper_title": "Anomaly Detection of Tabular Data Using LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ODDS library",
            "rating": 2,
            "sanitized_title": "odds_library"
        },
        {
            "paper_title": "Ecod: Unsupervised outlier detection using empirical cumulative distribution functions",
            "rating": 2,
            "sanitized_title": "ecod_unsupervised_outlier_detection_using_empirical_cumulative_distribution_functions"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "Data wrangling with language models (Narayan et al.)",
            "rating": 1,
            "sanitized_title": "data_wrangling_with_language_models_narayan_et_al"
        }
    ],
    "cost": 0.0103571,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Anomaly Detection of Tabular Data Using LLMs</p>
<p>Aodong Li aodongl1@uci.edu 
Yunhan Zhao 
Chen Qiu 
Marius Kloft 
Padhraic Smyth 
Maja Rudolph 
Stephan Mandt 
U C Irvine 
Bosch Center 
Rptu Kaiserslautern-Landau 
Anomaly Detection of Tabular Data Using LLMs
4DA8310044E110A62A7DA286FBC264EE
Large language models (LLMs) have shown their potential in long-context understanding and mathematical reasoning.In this paper, we study the problem of using LLMs to detect tabular anomalies and show that pre-trained LLMs are zeroshot batch-level anomaly detectors.That is, without extra distribution-specific model fitting, they can discover hidden outliers in a batch of data, demonstrating their ability to identify low-density data regions.For LLMs that are not well aligned with anomaly detection and frequently output factual errors, we apply simple yet effective datagenerating processes to simulate synthetic batchlevel anomaly detection datasets and propose an end-to-end fine-tuning strategy to bring out the potential of LLMs in detecting real anomalies.Experiments on a large anomaly detection benchmark (ODDS) showcase i) GPT-4 has on-par performance with the state-of-the-art transductive learning-based anomaly detection methods and ii) the efficacy of our synthetic dataset and fine-tuning strategy in aligning LLMs to this task.</p>
<p>Introduction</p>
<p>Large language models (LLMs), which employ transformerbased architectures and billions of learnable parameters, can process and generate text that exhibits human-level realism.LLMs have enabled groundbreaking real-world applications that were hardly possible a few years ago, such as chatbots e.g.,(ChatGPT) and code generation e.g., GitHub Copilot [Roziere et al., 2023].</p>
<p>This paper studies the application of LLMs to anomaly detection (AD)-one of the fundamental problems in machine learning occurring in many applications [Ruff et al., 2021].AD concerns the detection of irregular instances-so-called anomalies-in data.There exist several settings of AD [Qiu et al., 2022;Li et al., 2023;Li et al., 2024]; we consider the setting of zero-shot batch-level AD [Li et al., 2024], illustrated in Fig. 1, where we want to find an anomalous instance x i ∈ R K in a batch of data X = {x 1 , . . ., x N } ⊂ R K .This setting finds applications in many domains, from fraud detection and intrusion detection to medical anomaly detection and industrial damage detection.</p>
<p>Zero-shot batch-level AD utilizes batch information to adapt to distribution shifts and can exploit modern hardware like GPUs for parallel computation [Li et al., 2024].Numerous shallow methods have been developed for this setting under the name of unsupervised anomaly discovery1 [Ramaswamy et al., 2000;Breunig et al., 2000;Liu et al., 2008;Li et al., 2022].On the other hand, LLMs have high promise for this setting.Their input and output format-natural language text-leads to simpler usage for practitioners.LLMs require no expertise in selecting anomaly detection models and setting hyperparameters.Moreover, they have the potential to understand task background information and customize task needs.For example, when we know some pattern is rare but normal, we can inform LLMs to exclude that pattern from detected anomalies.</p>
<p>Another motivation for studying zero-shot batch-level AD arises from the data-wrangling task.[Narayan et al., 2022] demonstrated employing LLMs to detect and correct errors in attribute-value pairs for tabular data, assuming that LLMs understand the attribute meanings and values as humans do.Unfortunately, in many real-world applications, especially in specialized domains where i) LLMs have relatively less information and ii) data are preprocessed into numerical values, LLMs cannot reliably detect errors.Therefore, we study the problem of using LLMs to detect errors in a given data batch where errors are present as outliers.</p>
<p>Using LLMs for zero-shot batch-level AD is challenging.First, the data consists of numerical tables, while LLMs expect text as input.Second, detecting anomalies in tables requires sophisticated computation with numerical data, such as estimating and thresholding densities.It remains unclear 1) whether LLMs can perform these tasks and 2) how to effectively prompt LLMs for AD.Third, LLMs have varying capabilities in mathematical reasoning and text understanding.How to align LLMs unprepared for this AD problem must be addressed.</p>
<p>The contributions of this paper are as follows, addressing the aforementioned challenges:</p>
<p>• We propose a serialization method (illustrated in Fig. 1)</p>
<p>that converts batch-level anomaly detection from a numerical task to a text-based task.The method comes along without hyperparameter tuning.• We empirically evaluate our approach on both synthetic and real-world data using GPT, Llama2, and Mistral.The experiments demonstrate that GPT-3.5 and GPT-4 can effectively detect anomalies in batches.• We develop a strategy for fine-tuning anomaly detectors by synthesizing normal and anomalous data, thereby training the LLM to detect anomalies accurately.• Experiments on the ODDS benchmark [Rayana, 2016] demonstrate that our simple method using the original GPT-4 performs on par with the state-of-the-art transductive learning-based methods.The fine-tuned Mistral-based detector outperforms GPT-3.5, highlighting the effectiveness of our fine-tuning strategy.</p>
<p>As follows, we discuss related works in Sec. 2, then present our method of applying and fine-tuning LLMs to detect anomalies in Sec. 3. We conduct experiments in Sec. 4 and conclude with Sec. 5.</p>
<p>Related Work</p>
<p>Anomaly detection with LLMs.[Gu et al., 2024] uses indistribution paired images and texts to jointly train a language model and a vision encoder to describe in natural language text the found anomalies in an image.[Elhafsi et al., 2023] relies on LLMs' environment understanding and reasoning ability to monitor semantic anomalies in autonomous driving systems.[Park, 2024] employs LLMs as agents to validate and interpret financial anomalies.[Su et al., 2024] surveyed the work in time series anomaly detection.Unlike the above work, we tackle zero-shot batch-level anomaly detection for tabular data.Zero-shot batch-level anomaly detection.Batch-level anomaly detection or unsupervised anomaly discovery has been studied for a long time [Chandola et al., 2009].While numerous transductive learning-based methods have been proposed, they are shallow methods and require hyperparameter settings for each data batch [Tax and Duin, 2004;Xu et al., 2010;Zhou and Paffenroth, 2017;Ramaswamy et al., 2000;Li et al., 2022].In deep anomaly detection, zero-shot batch-level anomaly detection utilizes batch normalization layers to automatically adapt to each data batch [Li et al., 2024].In this work, we apply LLMs as zero-shot batch-level anomaly detectors to accomplish this task across datasets solely based on their gained knowledge through pretraining.</p>
<p>Zero-shot learning in LLMs.LLMs have shown unprecedented zero-shot ability in many downstream NLP tasks [Chang et al., 2023].Many recent works start to leverage such zero-shot ability of LLMs to other tasks, such as arithmetic reasoning [Lewkowycz et al., 2022;Imani et al., 2023] and time series forecasting [Gruver et al., 2024].LLMs have also been applied to data wrangling for error detection [Narayan et al., 2022;Vos et al., 2022].To our knowledge, we are the first to explore and benchmark LLMs on tabular anomaly detection tasks and propose effective approaches that enhance the ability of LLMs on this task.</p>
<p>Method</p>
<p>This section will first present the problem setup, then introduce our text-based method for batch-level anomaly detection using large language models (LLMs), and finally propose an end-to-end fine-tuning strategy for LLMs to be better aligned to anomaly detection.</p>
<p>Problem Setup</p>
<p>We consider a batch of possibly contaminated data D := {x i ∈ R K } N i=1 (a numerical table) in the presence of unlabeled anomalies.We assume the number of anomalies is far less than normal data, i.e., the normal data takes the majority in the batch.We stress that the data batch can contain no anomalies.LLMs can tell when the batch is contaminated or not.The aim is to identify which data points in the batch are abnormal.</p>
<p>Text Formulation of Batch-level Anomaly Detection</p>
<p>We assume each feature dimension is independent, and we detect anomalies for each feature separately2 .The detection results of each feature dimension will then be aggregated to form the final results.</p>
<p>Data Serialization.We designed a template to serialize data into text because LLMs only accept text input.Assuming independent features, we can detect anomalies on one feature dimension at a time.Then, the data to be serialized will be one-dimensional float scalars.Denote the single-feature data by {x i ∈ R} N i=1 .We use the template serialize the ith data point. 3The data index i is necessary to disambiguate repetitive data values.We approximate the data value up to two decimal places in the serialization.Each serialized data point is then concatenated as input to the LLMs.We use T in := +{T in I } N i=1 to denote the concatenation operation where + represents concatenating each element in a set.Prompt Engineering.Besides the data input, we need to inform the LLMs of the anomaly detection task.We use a description text C :="Abnormal data are different from the majority.Which data are abnormal?"to characterize anomalies and ask questions.The serialized data input and task description together formulate the input to the LLMs, i.e., X := +{T in , C}. Fig. 1 presents a serialization example with five synthetic data.
T in i :="Data i is x i ." to
With the input X, LLMs can respond to the anomaly detection request.The response will include anomalous data indices (the numeric data indices) by design.In most cases, LLMs tend to generate diverse responses with long reasoning.We further regularize the output format by delivering another system message-"Only answer data indices."-tothe LLMs to have easy-to-parse responses. 4lgorithm 1 LLM for batch-level anomaly detection
Require: LLM, D := {x i ∈ R K } N i=1
Initialize anomaly score for each row
s i = 0, i = 1, . . . , N for each column k in D do Set serialization T in = "Data 1 is x 1,k . Data 2 is x 2,k . ... Data N is x N,k ."
Set prompt C = "Abnormal data differ from the majority.Which data are abnormal?"</p>
<p>Get response Ŷk = LLM(T in + C) Update anomaly scores for all data points
s i = s i + 1[i ∈ Ŷk ].
end for return anomaly scores s i , i = 1, . . ., N</p>
<p>Anomaly detection as a text-to-text task.One can get anomaly predictions for each feature dimension with the pro-posed data serialization methods and the prompts.We now introduce a simple method for aggregating the responses of all feature dimensions and constructing anomaly scores for each data point.</p>
<p>We propose to set the anomaly score of the ith data to be the number of occurrences of data index i in all responses.That is, suppose the response to the kth feature dimension is Ŷk , then
s i = K k=1 1[i ∈ Ŷk ].
The anomaly scores are useful for performance evaluation and characterizing the degree of abnormality.The full procedure is presented in Alg. 1.</p>
<p>Prediction extraction from output.Automatically parsing the LLM output and extracting the predicted anomalies facilitate model evaluations and improve the response-to-detection speed.To get the predictions, we instruct the model to output only anomalous data indices by sending a system message-"Only answer data indices" However, research shows that the capability of following instructions by LLMs differs to some extent [Ouyang et al., 2022;Zhou et al., 2023].In our experiments, we observed that the fine-tuned LLMs (e.g., Mistral-AD and Llama-AD used in the experiments) can faithfully follow the same output format used during the fine-tuning stage.GPT-3.5 and GPT-4 also follow the instructions well and output succinct answers containing predicted data identifiers.So, we can extract the predictions automatically for these models.See Supp.B.1 for script details.</p>
<p>The other models in our experiments, Llama-2 and Mistral, oftentimes output redundant information besides predictions even though they are instructed to only output predictions.Redundant information makes it hard to pinpoint the predictions without human involvement, complicating the parsing process.To completely suppress redundant information, we manually modify the output token probabilities at each generation step and require the generation to follow a specific pattern.We use regular expressions to specify the desired model output patterns with the Outlines library [Willard and Louf, 2023]5 .We found that grammar-correct formats with complete sentences are essential for generating high-quality predictions.So the regular expression in use is ((Data [0-9]+(, [0-9]+) * are abnormal.)|(Alldata are normal.))which allows the model to predict abnormal data or to abstain from predictions if all data seemingly comes from the same datagenerating process.Extracting integers from the formatted output can be accomplished by the same automatic procedure</p>
<p>End-to-end Finetune Strategy</p>
<p>Unfortunately, not all LLMs are prepared to detect anomalies.Fig. 2 shows a failing case with an open-sourced LLM-Llama-2 (70 billion-parameter version).Llama-2 makes factual errors: it only discovers two outliers and misses another two; it wrongly labels one normal data as abnormal.Our experiments also observed that Llama-2 may pair incorrect indices and values, generate indices beyond the batch length, or list every data as abnormal.These phenomena signify the misalignment of Llama-2 or other LLMs in detecting anomalies.</p>
<p>Synthetic dataset.To align LLMs in batch-level anomaly detection, we simulate a synthetic dataset with ground truth labels for LLMs to learn.The dataset contains continuous and discrete data types, covering real-world data types.Discrete data is a mixture of normal and abnormal Categorical distributions.Continuous data is a Gaussian mixture where normal data is a narrow Gaussian while anomalies are from a wide Gaussian.All the model parameters are randomly selected from a pre-defined interval.The contamination ratio π for both data types is also random but ensured to be smaller than 0.2.The data generating processes are listed in Algs. 2 and 3 in Supp. A. The corresponding graphical models are shown in Fig. 3.We simulate 2,500 batches for each data type.When a data batch is normal, its ground-truth response is "All rows are normal."6For other batches that contain anomalies, we use the ground truth answers Y ="Data a 1 , a 2 ,... and a A are abnormal."where {a i : y ai = 1} A i=1 are the anomaly indices.Simulated synthetic data is serialized in our proposed text formulation.Synthetic data examples are in Supp. A.</p>
<p>End-to-end fine-tune.We align LLMs to the anomaly detection task through fine-tuning.The most common finetuning strategy is Chain-of-Thought [Wei et al., 2022].However, applying Chain-of-Thoughts to reason about anomalies is hard.Challenges arise from the complications of the AD task.For example, suppose we construct the chain of thoughts using the two-standard deviation range method7 .This method is a rough criterion and cannot cover all discrete and multimodal continuous data cases.In addition, asking LLMs to calculate the sample mean and sample standard deviation is another arithmetic challenge for LLMs.</p>
<p>Instead, we propose to teach LLMs in an end-to-end fashion-not focusing on "how to solve" but on "what to expect."We directly present the answer to the model and ask LLMs to learn to predict that given answer without caring about the intermediate steps.Therefore, we fine-tune LLMs on the synthetic dataset {(X b , Y b )} B b=1 in a supervised manner.Fig. 2 shows the efficacy of our fine-tuning method on a toy data batch.After aligning Llama2 (7 billion-parameter version) -Llama2-AD -detects all anomalies.</p>
<p>We apply low-rank adaptation (LoRA [Hu et al., 2022]), a parameter-efficient fine-tuning method to align LLMs.LoRA appends an additional low-rank weight matrix to each original weight matrix.The low-rank matrix can be parameterized efficiently through matrix factorization.The original weights are kept fixed during fine-tuning, and newly added low-rank matrices are updated.After fine-tuning, the low-rank weight matrices can be absorbed into the original weight matrix to fix the model size.</p>
<p>We fine-tune the LLMs by maximizing the conditional loglikelihood
B b=1 log p(Y b |X b ; θ LoRA , θ LLM ) of our simulated synthetic dataset {(X b , Y b )} B
b=1 with respect to the learnable θ LoRA while keeping LLM's original parameter θ LLM fixed.The conditional log-likelihood can be further factorized over the tokens
{y b i } L b i=1 of each response Y b in an auto-regressive fashion: B b=1 L b i=1 log p(y b i |y b &lt;i , X b ; θ LoRA , θ LLM ).
After optimization, θ LoRA can be integrated into θ LLM by an element-wise addition, which keeps the model size constant.More details are in [Hu et al., 2022].</p>
<p>Experiments</p>
<p>This section shows experimental results on the ODDS anomaly detection benchmark.One surprising result is that our simple prompt engineering method with the original GPT-4 performs similarly to the state-of-the-art anomaly detection method.Our alignment method using synthetic data on Llama2 and Mistral demonstrates significant improvements over their primitive counterparts.</p>
<p>We first introduce the global experimental setups and then the implementation details of our proposed methods.Finally, we present the results.</p>
<p>Experiment Setup</p>
<p>We follow the widely adopted ODDS tabular data benchmark [Rayana, 2016] to evaluate LLMs batch-level anomaly detection performance.Some LLMs have input token limits due to the context window size and GPU memory constraint.Therefore, we randomly sub-sample 150 rows and use the first 10 columns for each dataset to perform the evaluation.We extensively study various LLMs to support our findings.</p>
<p>Table 1: AUROC results of batch-level anomaly detection on the ODDS benchmark.Different LLMs are evaluated.Specifically, we show the performance of two LLMs (Llama2, Mistral) before and after finetuning.Proprietary LLMs (GPT-3.5 and GPT-4) are also compared.Additional state-of-the-art transductive learning-based approaches, i.e., KNN and ECOD, are listed for comparisons.Note that KNN and ECOD are not zero-shot batch-level methods.</p>
<p>Proposed Methods</p>
<p>Baselines GPT-3.5 GPT-</p>
<p>Implementation Details</p>
<p>We run all experiments three times with different random seeds.All our experiments except GPT-3.5 and GPT-4 are performed using an A6000 GPU with PyTorch.Llama-2 and Mistral can fit into the GPU memory.The temperature and where the contamination ratio is 0.1, resulting in p(x) in blue.We sample 500 independent batches from p(x) and ask the LLM to predict anomalies using our proposed method for each batch.We collect all the predicted anomalies and estimate the density by a kernel density estimator, shown by pa(x) in orange.pa(x) successfully captures three low-density regions of p(x), demonstrating the LLM's ability to detect anomalies.More details are in Supp.B. illustrate LLM's low-density region detection ability, we simulate a synthetic data distribution contaminated by anomalies.We use a two-component Gaussian mixture as the normal data distribution.We contaminate this normal data distribution with a wide uniform distribution representing abnormal data distribution.The final distribution is shown by p(x) in blue in Fig. 4. We sample data batches from this contaminated data distribution and apply our fine-tuned Mistral-AD (see below) to predict anomalies.We collect the predicted anomalies from all batches and use the kernel density estimator to fit a density pa (x) on them.Llama2-AD, Mistral vs. Mistral-AD), both models show significant improvements: on average, 8.9 and 6.7 AUROC increases, respectively, showing the efficacy of our fine-tuning strategy.</p>
<p>Conclusion</p>
<p>We consider using large language models (LLMs) to detect anomalies for numerical data wrangling.We address this problem through batch-level anomaly detection.We developed a text formulation for LLMs to accomplish this task.</p>
<p>We found LLMs are capable to identify low-density regions in a batch of data.Surprisingly, GPT-4 is a strong zeroshot batch-level anomaly detectors that have comparable performance with state-of-the-art transductive learning methods.For LLMs that are not well aligned to this task, we designed and simulated a synthetic dataset to fine-tune the LLMs in an "end-to-end" fashion.Experiments demonstrate the significance of our findings and the efficacy of our proposed finetune strategy.</p>
<p>Figure 1 :
1
Figure 1: The illustration of batch-level anomaly detection with LLMs.We serialize the data batch into text and apply our proposed prompts as the input to LLMs.LLMs then respond by answering the indices of abnormal data based on LLMs' knowledge.The system message "Only answer data indices" regularizes LLM responses and ensures responses are easy to parse.</p>
<p>Figure 2 :
2
Figure 2: Illustration of Llama2 for batch-level anomaly detection before and after our fine-tuning strategy.With the same input prompt, Llama2-70b (70-billion parameter version) makes factual mistakes-two false negatives (missing 5 and 10) and one false positive (incorrect 14).These results are obtained from https://www.llama2.ai.On the contrary, our fine-tuned 7-billion parameter (10x smaller than Llama2-70b) Llama2-AD succeeds in discovering all anomalies.</p>
<p>Figure 3 :
3
Figure 3: Graphical models of the synthetic data generating processes.(Left) We use a binary Gaussian mixture (i.e., K = 2) to generate a batch of continuous data of size N .One Gaussian corresponds to normal data, and another corresponds to abnormal data.(Right) A multinomial mixture model (K = 2) for discrete data where one multinomial is for normal and one for abnormal data.π controls the anomaly ratio.Specifics of the random variables in the models are in Supp.A</p>
<p>Figure 4 :
4
Figure4: LLMs can detect low-density regions in a contaminated data distribution.We use our Mistral-AD fine-tuned based on Mistral as the demonstrating LLM.Normal data distribution is represented by two Gaussian distributions located at -25 and 25 respectively.The contaminated data distribution is formed by combining the normal distributions and a wide uniform distribution spanned over interval[−100, 100], where the contamination ratio is 0.1, resulting in p(x) in blue.We sample 500 independent batches from p(x) and ask the LLM to predict anomalies using our proposed method for each batch.We collect all the predicted anomalies and estimate the density by a kernel density estimator, shown by pa(x) in orange.pa(x) successfully captures three low-density regions of p(x), demonstrating the LLM's ability to detect anomalies.More details are in Supp.B.</p>
<p>Fig. 4
4
shows pa (x) captures the three low-density regions in p(x), separated by two peak Gaussian distributions, demonstrating LLM's low-density region detection ability.Quantitative results.The results of OODS benchmark are shown in Tab. 1.The results summarize two salient conclusions: (i) Sophisticated LLMs are state-of-the-art zero-shot batch-level anomaly detectors.Comparing GPT-4 against ECOD, state-of-the-art method on ODDS benchmark, GPT-4 shows on-par performance without extra fine-tuning, indicating the huge potential of LLMs in the anomaly detection task.(ii) Proposed end-to-end fine-tuning strategy significantly boost the performance.Checking the performance of the same LLM before and after fine-tuning (Llama2 vs.</p>
<p>Using "zero-shot batch-level" stresses that our proposed method is a deep learning-based method rather than a shallow method.
We also tried to relax this independence assumption and input the data as a vector. However, the performance degrades. The reason could be that LLMs cannot distinguish a vector from a set of scalars. For the latter, the order between elements is unimportant.
Experimental performance is not sensitive to data names. We also named data by "Row" instead of "Data" as if in a table where columns correspond to features or data dimensions and rows index data points. The experimental performance is similar.
Use "Only answer row numbers" when data are named "Row."
https://outlines-dev.github.io/outlines/
We facilitate optimization convergence by designing highprobability response formats and using complete, grammarconsistent sentences.
The two-standard deviation range refers to the interval[−2σ, 2σ]  where σ is the standard deviation. Any data points located outside this range are considered abnormal.
Specifically, we use api of gpt-3.5-turbo-1106 and gpt-4-1106preview.
A Synthetic DatasetA.1 Data Generating Processes Data generating processes of synthetic discrete and continuous data are presented in Alg. 2 and Alg. 3, respectively.Discrete data is a mixture of normal and abnormal Categorical distributions.Continuous data is the clutter setup where normal data is sampled from a narrow Gaussian distribution while anomalies are from another wide Gaussian distribution.In practice, we generate the discrete data by setting the hyperparameters N l = 20, N h = 100, π l = 0.01, π h = 0.2, M l = 1, M h = 4, α = 20.For continuous data generation, we choose N l = 20, N h = 100, π l = 0.01, π h = 0.2, µ l = −100, µ h = 100, σ l n = 0.5, σ h n = 5.For both data types, the contamination ratio π is smaller than 0.2.Algorithm 2 Generate discrete datap n ∼ Dir({α} Mn ) 5: p a ∼ Dir({α} Ma ) 6: for i = 1, . . ., N do 7:x i ∼ {(1 − π)p n , πp a } 8: end for 9: return {x i }, i = 1, . . ., N Algorithm 3 Generate continuous data5: σ a = 10σ n 6: for i = 1, . . ., N do 7:xA.2 Data ExamplesB Implementation DetailsB.1 Prediction Extraction ProcedureThe automatic procedure for extracting model predictions in all experiments is the following code snippet in Python.def parse_generation_results(ans, max_num=149): response_ret = [] if ans.endswith("."):ans = ans.rstrip(".")ans = ans.rsplit(":-&gt;",1)[-1] if ":" in and: ans = ans.replace(":"," ") ans = ans.replace(",","") ans = ans.split()if "no" in ans or "No" in ans or "None" in and:return [] for r in and: if r.isnumeric() and "." not in r and int(r)&lt;= max_num: response_ret.append(int(r))return response_retB.2 Qualitative StudyIn Fig.4, we use p(x) = 0.45N (−25, 2.5 2 ) + 0.45N (25, 2.5 2 ) + 0.1Unif(−100, 100).pa (x) is estimated by a kernel density estimator with 5.0-bandwidth Gaussian kernels.The predicted anomalies are collected from 500 independent batch predictions, where each batch contains 50 data points sampled from p(x).B.3 Quantitative StudyImplementation details.The output from LLMs are naturally diverse and less controllable.A system prompt: "Only answer row numbers." is passed to all LLMs to easier parse the responses for evaluation.We manually filter unreasonable predictions of LLMs.Specifically, (i) we ignore predictions that beyond provided data samples; (ii) we choose the semantic consistent one if the output contains multiple answers.We repeat all experiments 3 times with different random seeds.All our experiments are implemented with Py-Torch using A6000 GPU.For Llama-2 and Mistral, the temperature and top p generation hyperparameters are set as 0.75 and 0.9, respectively.For GPT-3.5 and GPT-4, we use the default hypereparameter settings.We fine-tune all models using LoRA parameter-efficient fine-tuning strategy[Hu et al., 2022].We finetune Llama-2 for five epochs and Mistral for two epochs with the same learning rate 1e-3.All optimizations are convergent.
On the other hand, for GPT-3.5 and GPT-4, we use their default hyperparameter settings and perform the experiments through their API. We fine-tune Llama-2-7B and Mistral-7B using LoRA parameter-efficient fine-tuning strategy [Hu et al., 2022] on our synthetic datasets. We generate training and validation sets separately. The training set involves 5000 data batches (2500 continuous data batches and 2500 discrete data batches), while the validation set contains 400 data batches (200 for continuous and 200 for discrete data). top p generation hyperparameters are set as 0.75 and 0.9 for Llama-2 and Mistral, respectively. The resulting models are named Llama2-AD and Mistral-AD</p>
<p>Semantic anomaly detection with large language models. Breunig, Proceedings of the 2000 ACM SIGMOD international conference on Management of data. the 2000 ACM SIGMOD international conference on Management of dataEdward Schmerling, Issa AD Nesnas, and Marco Pavone2000. 2000. 2009. 2009. 2023. 2023. 202341Autonomous Robots</p>
<p>Anomalygpt: Detecting industrial anomalies using large visionlanguage models. Gruver, mathprompter:The 61st Annual Meeting Of The Association For Computational Linguistics. ICLR2024. 2024. 2024. 1932-1940, 2024. 2022. 2022. 202336Proceedings of the AAAI Conference on Artificial Intelligence</p>
<p>Ecod: Unsupervised outlier detection using empirical cumulative distribution functions. Jiang, arXiv:2310.068252008 eighth ieee international conference on data mining. Avanika Narayan, Ines Chami, Laurel Orr, Christopher Ré, Alex RayIEEE2023. 2023. 2022. 2022. 2022. 2022. 2023. 19882-19910. PMLR, 2023. 2024. 2024. 2008. 2008. 2022. 2022. 202235arXiv preprintProceedings of the VLDB Endowment. Ouyang et al., 2022. et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems</p>
<p>Enhancing anomaly detection in financial markets with an llm-based multi-agent framework. Taejin Park, ; Park, Qiu, arXiv:2403.19735Proceedings of the 2000 ACM SIGMOD international conference on Management of data. Sridhar Ramaswamy, Rajeev Rastogi, Kyuseok Shim, the 2000 ACM SIGMOD international conference on Management of dataPMLR2024. 2024. 2022. 2022. 2000arXiv preprintInternational conference on machine learning</p>
<p>A unifying review of deep and shallow anomaly detection. Rayana ; Roziere, arXiv:2308.12950Shebuti Rayana. ODDS library. 2016. 2016. 2023. 2023. 2021109arXiv preprintCode llama: Open foundation models for code</p>
<p>Large language models for forecasting and anomaly detection: A systematic literature review. Su, arXiv:2402.10350arXiv:2307.09702Advances in Neural Information Processing Systems. Wei, 2024. 2024. 2004. 2004. 2023. 2023. 2022. 2022. 202354arXiv preprintNeurIPS 2022 First Table Representation Workshop. Willard and Louf, 2023] Brandon T Willard and Rémi Louf. Efficient guided generation for llms</p>
<p>Chong Zhou and Randy C Paffenroth. Anomaly detection with robust deep autoencoders. Xu, Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. the 23rd ACM SIGKDD international conference on knowledge discovery and data mining2010. 2010. 2017. 201723Advances in neural information processing systems</p>
<p>Instruction-following evaluation for large language models. Zhou, arXiv:2311.079112023. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
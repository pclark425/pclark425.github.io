<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9510 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9510</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9510</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-271956490</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.13450v1.pdf" target="_blank">vitaLITy 2: Reviewing Academic Literature Using Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Academic literature reviews have traditionally relied on techniques such as keyword searches and accumulation of relevant back-references, using databases like Google Scholar or IEEEXplore. However, both the precision and accuracy of these search techniques is limited by the presence or absence of specific keywords, making literature review akin to searching for needles in a haystack. We present vitaLITy 2, a solution that uses a Large Language Model or LLM-based approach to identify semantically relevant literature in a textual embedding space. We include a corpus of 66,692 papers from 1970-2023 which are searchable through text embeddings created by three language models. vitaLITy 2 contributes a novel Retrieval Augmented Generation (RAG) architecture and can be interacted with through an LLM with augmented prompts, including summarization of a collection of papers. vitaLITy 2 also provides a chat interface that allow users to perform complex queries without learning any new programming language. This also enables users to take advantage of the knowledge captured in the LLM from its enormous training corpus. Finally, we demonstrate the applicability of vitaLITy 2 through two usage scenarios. vitaLITy 2 is available as open-source software at https://vitality-vis.github.io.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9510.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9510.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VITALITY 2 literature synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VITALITY 2: LLM-powered literature summarization and review (RAG + prompt chaining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source visual analytics system that uses LLM-based embeddings, retrieval-augmented generation (RAG), and prompt chaining to find semantically relevant papers and synthesize high-level literature summaries and comparative literature reviews from saved sets of papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>OpenAI text-embedding-ada-020 (Ada) for embeddings; generation LLM unspecified (OpenAI API via LangChain; user-configurable)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Embeddings: OpenAI's text-embedding-ada-020 (Ada) embedding model used to create metadata embeddings (title, abstract, authors, keywords, venue, date). Generation: the paper refers generically to an LLM accessed via the OpenAI API through LangChain for natural-language generation and summarization; the exact generation model (e.g., GPT-3.5, GPT-4) is not specified and is configurable by the user. No specific model sizes or fine-tuning for law-extraction are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Academic literature in visualization and related HCI domains (literature review workflows, methods explanation, and thematic summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>A corpus of 66,692 papers (augmented from an earlier 59,232) drawn from visualization- and HCI-related venues spanning roughly 1970–2023. The system currently uses paper metadata (title, authors, venue, publication date, keywords, abstract) to create embeddings; full text is not included in the present corpus or embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>High-level thematic patterns and comparative literature summaries (thematic/principle-level generalizations across a saved set of papers), rather than formal symbolic 'laws'.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>In a usage scenario, the generated literature review highlighted a broad thematic finding across the saved set: a shared motivation around 'transparency of data analysis' and a temporal divide where some earlier papers represented antecedent ideas while others were more recent—i.e., an emergent thematic pattern across the corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) combined with prompt chaining: (1) create metadata embeddings (Ada) for all papers and store in vector DB (Faiss/ChromaDB); (2) perform embedding-based similarity retrieval to assemble relevant documents for a user query or a saved set; (3) use LangChain-style prompt chaining to condense conversation history and compose prompts that combine retrieved context and user input; (4) call an LLM via OpenAI API to produce summaries or literature-review style syntheses. Users can customize prompts (verbosity/style/format).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>No formal, quantitative evaluation of 'qualitative law' extraction is reported. Evaluation consists of: authors' internal testing (informal benchmark) showing Ada embeddings performed better than GloVe and SPECTER for their features, summative user feedback from VITALITY 1 indicating utility, and illustrative usage scenarios (anecdotal examples) demonstrating the system generating summaries and comparative reviews. The paper explicitly notes absence of rigorous validation for generated summaries or derived generalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The system successfully supports retrieval of semantically related papers and generates paper-level summaries and combined literature-review style syntheses for user-selected papers; it enables interactive exploration (UMAP visualization) and export of bibliographies. Limitations observed include: generation based solely on metadata (no full-text), risk of LLM hallucinations (examples provided), lack of formal evaluation of summary correctness, and constrained prompt/token-size limitations. The authors report that Ada embeddings outperformed older embeddings in their internal tests for similarity search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared three embedding options (GloVe, SPECTER from VITALITY 1, and Ada embeddings); internal tests and prior benchmarks cited indicate Ada embeddings performed better than GloVe and SPECTER for VITALITY 2 features. There is no direct quantitative comparison of LLM-generated literature syntheses against human experts, systematic reviews, or other automated law-extraction baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Summaries are produced from metadata only (no full-text chunking), which reduces comprehensiveness and accuracy; prompt size/token limits constrain how much context can be provided to the LLM; LLMs lack genuine comprehension causing potential errors on complex tasks; no rigorous or formal validation of synthesized generalizations is provided; system demonstrably produced hallucinated paper titles in at least one scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>The paper documents hallucination incidents: in a grounded-theory example the LLM referenced works not in the VITALITY 2 corpus and responded with paper titles that were not present or verifiable; the authors note RAG mitigates but does not eliminate hallucinations and recommend adding reliable external knowledge sources and retrieving full text for better grounding. They also mention community concerns about biases and inaccuracies in LLM outputs and cite recent findings that hallucination rates are comparatively lower for models like GPT-4, but no formal mitigation beyond RAG/prompting is implemented in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'vitaLITy 2: Reviewing Academic Literature Using Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sci-DaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model <em>(Rating: 2)</em></li>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <em>(Rating: 2)</em></li>
                <li>Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections <em>(Rating: 1)</em></li>
                <li>Datatales: Investigating the use of large language models for authoring data-driven articles <em>(Rating: 1)</em></li>
                <li>VitaLITy: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9510",
    "paper_id": "paper-271956490",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "VITALITY 2 literature synthesis",
            "name_full": "VITALITY 2: LLM-powered literature summarization and review (RAG + prompt chaining)",
            "brief_description": "An open-source visual analytics system that uses LLM-based embeddings, retrieval-augmented generation (RAG), and prompt chaining to find semantically relevant papers and synthesize high-level literature summaries and comparative literature reviews from saved sets of papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "OpenAI text-embedding-ada-020 (Ada) for embeddings; generation LLM unspecified (OpenAI API via LangChain; user-configurable)",
            "llm_model_description": "Embeddings: OpenAI's text-embedding-ada-020 (Ada) embedding model used to create metadata embeddings (title, abstract, authors, keywords, venue, date). Generation: the paper refers generically to an LLM accessed via the OpenAI API through LangChain for natural-language generation and summarization; the exact generation model (e.g., GPT-3.5, GPT-4) is not specified and is configurable by the user. No specific model sizes or fine-tuning for law-extraction are reported.",
            "application_domain": "Academic literature in visualization and related HCI domains (literature review workflows, methods explanation, and thematic summarization).",
            "input_corpus_description": "A corpus of 66,692 papers (augmented from an earlier 59,232) drawn from visualization- and HCI-related venues spanning roughly 1970–2023. The system currently uses paper metadata (title, authors, venue, publication date, keywords, abstract) to create embeddings; full text is not included in the present corpus or embeddings.",
            "qualitative_law_type": "High-level thematic patterns and comparative literature summaries (thematic/principle-level generalizations across a saved set of papers), rather than formal symbolic 'laws'.",
            "qualitative_law_example": "In a usage scenario, the generated literature review highlighted a broad thematic finding across the saved set: a shared motivation around 'transparency of data analysis' and a temporal divide where some earlier papers represented antecedent ideas while others were more recent—i.e., an emergent thematic pattern across the corpus.",
            "extraction_methodology": "Retrieval-Augmented Generation (RAG) combined with prompt chaining: (1) create metadata embeddings (Ada) for all papers and store in vector DB (Faiss/ChromaDB); (2) perform embedding-based similarity retrieval to assemble relevant documents for a user query or a saved set; (3) use LangChain-style prompt chaining to condense conversation history and compose prompts that combine retrieved context and user input; (4) call an LLM via OpenAI API to produce summaries or literature-review style syntheses. Users can customize prompts (verbosity/style/format).",
            "evaluation_method": "No formal, quantitative evaluation of 'qualitative law' extraction is reported. Evaluation consists of: authors' internal testing (informal benchmark) showing Ada embeddings performed better than GloVe and SPECTER for their features, summative user feedback from VITALITY 1 indicating utility, and illustrative usage scenarios (anecdotal examples) demonstrating the system generating summaries and comparative reviews. The paper explicitly notes absence of rigorous validation for generated summaries or derived generalizations.",
            "results_summary": "The system successfully supports retrieval of semantically related papers and generates paper-level summaries and combined literature-review style syntheses for user-selected papers; it enables interactive exploration (UMAP visualization) and export of bibliographies. Limitations observed include: generation based solely on metadata (no full-text), risk of LLM hallucinations (examples provided), lack of formal evaluation of summary correctness, and constrained prompt/token-size limitations. The authors report that Ada embeddings outperformed older embeddings in their internal tests for similarity search.",
            "comparison_to_baseline": "Compared three embedding options (GloVe, SPECTER from VITALITY 1, and Ada embeddings); internal tests and prior benchmarks cited indicate Ada embeddings performed better than GloVe and SPECTER for VITALITY 2 features. There is no direct quantitative comparison of LLM-generated literature syntheses against human experts, systematic reviews, or other automated law-extraction baselines.",
            "reported_limitations": "Summaries are produced from metadata only (no full-text chunking), which reduces comprehensiveness and accuracy; prompt size/token limits constrain how much context can be provided to the LLM; LLMs lack genuine comprehension causing potential errors on complex tasks; no rigorous or formal validation of synthesized generalizations is provided; system demonstrably produced hallucinated paper titles in at least one scenario.",
            "bias_or_hallucination_issues": "The paper documents hallucination incidents: in a grounded-theory example the LLM referenced works not in the VITALITY 2 corpus and responded with paper titles that were not present or verifiable; the authors note RAG mitigates but does not eliminate hallucinations and recommend adding reliable external knowledge sources and retrieving full text for better grounding. They also mention community concerns about biases and inaccuracies in LLM outputs and cite recent findings that hallucination rates are comparatively lower for models like GPT-4, but no formal mitigation beyond RAG/prompting is implemented in this work.",
            "uuid": "e9510.0",
            "source_info": {
                "paper_title": "vitaLITy 2: Reviewing Academic Literature Using Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sci-DaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model",
            "rating": 2,
            "sanitized_title": "scidasynth_interactive_structured_knowledge_extraction_and_synthesis_from_scientific_literature_with_large_language_model"
        },
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections",
            "rating": 1,
            "sanitized_title": "relatedly_scaffolding_literature_reviews_with_existing_related_work_sections"
        },
        {
            "paper_title": "Datatales: Investigating the use of large language models for authoring data-driven articles",
            "rating": 1,
            "sanitized_title": "datatales_investigating_the_use_of_large_language_models_for_authoring_datadriven_articles"
        },
        {
            "paper_title": "VitaLITy: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics",
            "rating": 1,
            "sanitized_title": "vitality_promoting_serendipitous_discovery_of_academic_literature_with_transformers_visual_analytics"
        }
    ],
    "cost": 0.008349249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>VITALITY 2: Reviewing Academic Literature Using Large Language Models</p>
<p>Hongye An 
Arpit Narechania arpitnarechania@gatech.edu 
Emily Wall emily.wall@emory.edu 
Kai Xu kai.xu@nottingham.ac.uk </p>
<p>University of Nottingham</p>
<p>Georgia Institute of Technology</p>
<p>Emory University</p>
<p>University of Nottingham</p>
<p>VITALITY 2: Reviewing Academic Literature Using Large Language Models
F142495FB5CE99D17524F75CAE82FF12large language modelretrieval augmented generationtext embeddingvector databaseliterature reviewdata visualization
Academic literature reviews have traditionally relied on techniques such as keyword searches and accumulation of relevant backreferences, using databases like Google Scholar or IEEEXplore.However, both the precision and accuracy of these search techniques is limited by the presence or absence of specific keywords, making literature review akin to searching for needles in a haystack.We present VITALITY 2, a solution that uses a Large Language Model or LLM-based approach to identify semantically relevant literature in a textual embedding space.We include a corpus of 66,692 papers from 1970-2023 which are searchable through text embeddings created by three language models.VITALITY 2 contributes a novel Retrieval Augmented Generation (RAG) architecture and can be interacted with through an LLM with augmented prompts, including summarization of a collection of papers.VI-TALITY 2 also provides a chat interface that allow users to perform complex queries without learning any new programming language.This also enables users to take advantage of the knowledge captured in the LLM from its enormous training corpus.Finally, we demonstrate the applicability of VITALITY 2 through two usage scenarios.VITALITY 2 is available as open-source software at https://vitality-vis.github.io.</p>
<p>INTRODUCTION</p>
<p>In recent years, the proliferation of vast digital repositories of academic papers has posed numerous challenges for researchers and scholars alike, including (i) efficiently retrieving pertinent information from this expansive pool, (ii) comprehensively visualizing the relationships within scholarly literature, and (iii) summarizing extensive bodies of literature.Traditional information retrieval methods for literature reviews often fall short in capturing the nuanced connections between academic papers, thereby hindering the seamless extraction of relevant insights; e.g., existing literature review approaches are often ad hoc rather than systematic [35] and even systematic approaches using techniques like keyword search can lead to inadequate coverage of evidence [2,35].</p>
<p>VITALITY 1 began to address these challenges by introducing a transformer-based approach to exploring academic literature [26].This system included a web-based visualization interface that used text embeddings to identify semantically similar literature based on input papers or even unpublished paper abstracts using SPECTER [8] and GloVe [31].It also introduced a corpus of more than 59,000 papers across 38 prominent visualization venues, with summative user feedback demonstrating the utility of the tool.</p>
<p>While prior approaches to visualize academic articles have made significant strides towards finding semantically related articles [1,16,42,26] and exploring citation networks [6,14,9,43], these efforts nonetheless are limited in their ability to support intuitive interaction with a corpus of academic literature or assist individuals in summarizing large bodies of literature.We recognize the past two years have seen an explosive growth and innovation in the capability and range of applications of Large Language Models (LLMs) [51], including use within the visualization community for generating visualizations [39,32] and authoring data-driven articles [37].We observe an opportunity to build upon prior literature search visualization approaches and recent developments with LLMs to introduce literature search methods to address these gaps.</p>
<p>We present VITALITY 2, an open-source visualization system for conducting literature searches, that incorporates a novel Retrieval Augmented Generation (RAG) architecture [21].VITAL-ITY 2 enables users to search for relevant literature using (1) a paper(s) as the seed to find similar work, (2) the abstract of an existing paper or paper to-be-written, (3) traditional keyword-based searching, or (4) a natural language query to an LLM-powered chat interface.Results are queried from a vector database and shown in a rank-ordered table with a similarity score including title, abstract, authors, citation counts, and links to the original sources.Results are also visualized in a projection where papers that appear closer together spatially are more similar in the vector embedding space.Users can also interact with the LLM interface to summarize a corpus of papers and ask contextually relevant questions, e.g., "what is the grounded theory method?"</p>
<p>Over the VITALITY 1 predecessor system, VITALITY 2 introduces a number of novel features, including its RAG architecture and integration of prompt chaining (described in Section 3).The system supports natural context-aware conversations through LLMs to summarize and understand collections of papers.The system also augments the dataset of 59,000 papers from VITALITY 1 to cover publications across 38 visualization venues over the past 3 years for a total of 66,692 papers.VITALITY 2 is available as opensource software at https://vitality-vis.github.io.</p>
<p>The remainder of this paper is organized as follows: we review relevant background information in Section 2. Next, we discuss the architecture of VITALITY 2 in Section 3 followed by a description of the front-end interface and system capabilities in Section 4. We provide exemplary usage scenarios in Section 5. Finally, we discuss the implications and applications of this work in Section 6.</p>
<p>RELATED WORK</p>
<p>With the ever growing body of scientific literature, there is a constant need for support to conduct more effective and efficient literature research.This challenge has attracted increasing research attention as more publications become digitally available more widely.Early work comes from both within the visualization community (such as PaperVis [6]) and beyond (such as the work by El-Arini and Guestrin [10]).For this paper, we will focus on the results from visualization and related communities.</p>
<p>Broadly speaking, the visualization research for literature analysis spans a spectrum, from providing an overview of a field to helping find and understand papers related to a specific topic.For the arXiv:2408.13450v1[cs.HC] 24 Aug 2024 former, a well-known example is the work by Isenberg et al. [16] that aims to provide an overview of the entire visualization research landscape, using papers spanning two and a half decades.While there are other efforts that utilize a similar paper collection, i.e., all the papers from IEEE VIS, few solely focus on the large picture.</p>
<p>The other end of the spectrum targets literature for a specific topic.This is closer to the goal of VITALITY 2, so we would focus our discussion here.Common approaches employed by these visualization efforts include topic or semantic analysis, network analysis, and supporting the literature review workflow.From early on it has been recognized that keyword matching alone is not an effective way to find relevant papers [10,42,44], as a concept or technique can often be expressed in several different ways.Most of the proposed approaches use some form of text analytics or natural language processing methods.Topic modeling is a popular approach among the early work that can help identify relevant papers by grouping similar ones together [10,42,44].This includes techniques that are designed for text analysis in general and can be easily applied to literature analysis such as Serendip [1].As NLP research progresses, new techniques are being used for literature analysis, such as text embeddings generated by transformerbased models [4], including GloVe [30] and SPECTER [8] used in VITALITY 1 [26].VITALITY 2 takes advantage of text embeddings created by the latest LLMs, which represent a breakthrough for many NLP tasks, achieving performance levels close to humans.</p>
<p>Network analysis is another common approach used by literature visualization, creating and visualizing co-author and affiliation network [42], citation network [6,14,50], and similarity network [6,16,4,26], among others.VITALITY 2 focuses on paper similarity, using the latest LLM-based embeddings to create a more accurate network topology.Finally, there are a series of work targeting the literature review workflow, such as LitSense [36] and Relatedly [29].While not a focus, the interface and interactions of VITALITY 2 are designed to match and support the workflow of common literature analysis tasks.</p>
<p>Dimensionality reduction is essential for visualizing highdimensional data, such as the embeddings derived from LLMs.VI-TALITY 2 leverages state-of-the-art techniques to reduce the dimensionality of embeddings while retaining their semantic relationships.UMAP [3] has been shown to outperform traditional methods like t-SNE [40] and PCA [22] in terms of preserving both local and global structures in the data.This capability makes UMAP particularly suitable for visualizing the complex relationships within the literature embeddings generated by LLMs.Once the embeddings are reduced to a lower-dimensional space, they are visualized using interactive plots.These plots allow users to explore the relationships between different papers intuitively.</p>
<p>Using LLMs for literature visualization is still in its infancy.The closest example we found is SciDaSynth [41], which has not been peer reviewed.While it also uses LLMs to create publication embedding and provides a visual interface for exploration and analysis, it does not come with a collection of papers, and the focus of analysis is on paper comparison.Similar to SciDaSynth, VITALITY 2 introduces LLMs while retaining the original paper visualization and retrieval capabilities of VITALITY 1. VITALITY 2 offers users a novel natural language-based interaction for paper retrieval.The search results are integrated into the existing exploratory and analytical visualization panels of VITALITY 1, thereby combining LLMs technology with visualization.</p>
<p>ARCHITECTURE AND IMPLEMENTATION</p>
<p>Figure 1 illustrates the architecture, flow of data, and core links of VITALITY 2. All the papers in the corpus are first pre-processed using an LLM to create their embeddings (for similarity search), with the results stored in a vector database.This is shown as the orange part on the top.Similarity search is then performed within the vector database without further LLM access (the bottom line).User input from the chat interface (Step 1) is broken down into a series of smaller tasks using "Prompt chaining."For each sub-task, relevant information is retrieved from the vector database (Step 2 &amp; 3) and then combined with user input in the prompt before sending it to the LLM (Step 4)."Prompt history" provides context from previous conversations.Once all the sub-tasks are completed (Step 5), the final results are returned to the user (Step 6).</p>
<p>Similarity Search using LLM</p>
<p>All the papers in the corpus are pre-processed using an LLM to create their embeddings based on paper metadata, which includes the title, authors, the conference or journal in which the paper is published, publication date, keywords, and abstract.These text embeddings convert textual data into high-dimensional vectors that capture semantic relationships among paper metadata [38], ensuring thorough comprehension of each paper and enabling more effective similarity searches within the vector database.For clarity, the "text embedding" mentioned refers to embeddings created from paper metadata rather than the full paper content.This metadata-focused approach balances detail with computational efficiency and allows for robust semantic matching.</p>
<p>To find similar papers from an academic literature corpus, while VITALITY 1 uses GloVe [30] and SPECTER [8] embeddings, VI-TALITY 2 uses ADA, the embeddings generated by OpenAI's textembedding-ada-020 model [28], which is a breakthrough model that generates contextually rich embeddings by leveraging advanced attention mechanisms and transformer architectures.Thus, VITALITY 2 now includes ADA, GloVe, and SPECTER embeddings.VITALITY 2 also leverages two vector databases (Faiss [17] and ChromaDB [7]) to store and manage these embeddings and also to locate similar papers via approximate nearest neighbor search.</p>
<p>Chat with Papers using RAG and Prompt Chaining</p>
<p>While text embeddings and vector databases allow finding similar papers, there are several other kinds of analyses a user may want to perform, such as understanding a technical concept, summarizing a single paper, or writing a literature review based on multiple papers.To perform such analyses, users may need to build customized tools and/or learn a new query language, neither of which are readily accessible to nor understandable by many users.VITALITY 2 addresses this issue by leveraging the powerful natural language understanding and generation capabilities of LLMs, allowing users to directly engage with the LLM using natural language.</p>
<p>However, there are a few challenges when applying LLMs for such analyses: (1) Prompt Size -This refers to the largest prompt that an LLM can accept and depends on the LLM type and version.Earlier versions of LLMs accepted a few thousand tokens, wherein each word in a prompt accounted for a few tokens.More recent versions have a larger limit, e.g., up to 16K tokens for GPT 3.5 [11].However, this can still limit analyses involving a large corpus of papers, which often have thousands of words each.(2) Hallucination -it is well known that LLMs can create seemingly plausible information about something that does not exist [47], e.g., suggest papers that never existed, that can be detrimental.(3) Limited Comprehension Capabilities -Despite the ability to generate seemingly logical and coherent text, LLMs lack genuine comprehension capabilities.This implies that an LLM may encounter difficulties when handling complex problems or tasks, particularly those requiring a deeper understanding of context or concepts.</p>
<p>To overcome these issues, we employ two popular approaches: Retrieval Augmented Generation (RAG) and Prompt Chaining.RAG.RAG semantically processes the user's input query to only retrieve relevant information from within the data corpus, thereby reducing the subsequent prompt size and also minimizing the risk of hallucination.This retrieval process is not about finding exact answers but rather fetching documents that contain potentially useful context or information related to the query.Prompt Chaining.Prompt chaining breaks down complex tasks to a series of smaller steps and provides specific prompts that are known to be effective for each of these steps [45].VITALITY 2 borrows this idea and implements a conversation framework to manage the content and context of user queries and LLM responses.VITALITY 2 uses LangChain [20] a popular open-source library for this purpose.As LLM API calls operate independently of each other without inherent memorization or management of the context, there is no built-in functionality for retaining conversation history.Sending the entire dialogue history with each LLM API call risks exceeding the maximum token limit.Therefore, we adopt a two-step approach in VITALITY 2: first, a summary of recent conversations is generated by calling the LLM API once to obtain a "condensed conversation history."Then, in the second API call, the user's query, conversation history, and retrieved information are concatenated into a prompt and sent to the LLM API.This an example of prompt chaining, that efficiently combines the results of two or more LLM API calls.</p>
<p>VITALITY 2</p>
<p>We present VITALITY 2, an LLM-powered visual analytics tool to help users write academic literature reviews.</p>
<p>Dataset of Academic Articles</p>
<p>VITALITY 1 [26] provided a dataset of 59,232 academic papers from visualization and HCI literature, along with their metadata such as their title, abstract, author(s), keyword(s), publisher, publication year, citation counts, and n-dimensional and 2-dimensional vector embeddings (GloVe and SPECTER).VITALITY 1 also open-sourced a web-scraping framework to extract the above information from digital repositories such as IEEE Xplore and ACM Digital Library for continued development of the corpus.VITAL-ITY 2 used this scraper to extract more recent papers between 2021-2023, resulting in an augmented dataset of 66,692 papers.</p>
<p>Step 1</p>
<p>Use title search for papers recommended by supervisor</p>
<p>Step 2</p>
<p>Switch to Ada embedding</p>
<p>Step 3</p>
<p>Add to paper to "Similarity Search" Tab</p>
<p>Step 4</p>
<p>Find similar papers</p>
<p>Step 5</p>
<p>Click to highlight in UAMP Visualization Map or click to save papers</p>
<p>Step 6</p>
<p>Zoom in and click to select a nearby point</p>
<p>Step 7</p>
<p>Customize prompt to do "Summarize" or "Literature Review"</p>
<p>Step 8</p>
<p>Export saved papers to bib file</p>
<p>User Interface</p>
<p>Figure 2 shows the VITALITY 2 user interface, illustrating the new features built on top of VITALITY 1.</p>
<p>In VITALITY 1, the Paper Collection View (A) shows the entire corpus of publications, the Similarity Search View (B) shows options to look-up publications that are similar to another list of publications or by a work-in-progress title and abstract, the Visualization Canvas (C) shows an interactive 2-D UMAP projection of the embedding space of the entire paper collection, the Meta View (D) shows summaries of certain attributes with respect to the Paper Collection View (A), and (E) opens a Saved Papers View from which the saved papers can be exported in a JSON and .bibtexformat for later use.We added three new capabilities in VITALITY 2.</p>
<p>First, we expanded available embedding options (from GloVe and SPECTER) to add OpenAI's ADA [27].Users can view the 2dimensional ADA embeddings in the UMAP and search for similar papers (by title or abstract).Based on our own testing and consistent with prior benchmarks [25], we found ADA embeddings to perform better than GloVe and SPECTER on VITALITY 2 features.</p>
<p>Second, we added a new Chat with your Data View (F) to allow users to ask natural-language questions based on the entire corpus, e.g., "Help me find some papers related to geographic science visualization."For papers mentioned in the LLM's output, VITALITY 2 allows the user to map (view in the UMAP), select (look up other similar papers), or save them (include it in a literature review or export).This capability holds immense potential for revolutionizing scholarly information access, enabling users to effectively harness the capabilities of RAG and pose a wide spectrum of inquiries.</p>
<p>Third, we also added novel capabilities to summarize papers and conduct literature reviews (in the Saved Papers view), building upon the robust semantic understanding capabilities of LLM.Clicking the "Summarize" button outputs a short summary of each of the saved papers, one below the other.Clicking the "Literature Review" button goes a step further, and outputs a comprehensive literature review based on the saved papers, including descriptions, comparisons, and a bibliography.These enhancements enable users to swiftly grasp the key information contained within their saved papers or draft an early version of their related work section in their ongoing manuscript.Note that VITALITY 2 allows users to customize the base LLM prompts to control the verbosity, writing style, and format of the LLM's response.</p>
<p>USAGE SCENARIOS 5.1 LLM Summarization of Literature Review</p>
<p>Noori is an undergraduate student who has joined a visualization research lab, working under the supervision of a faculty member and Ph.D. student on an ongoing project conducting controlled experiments on multiverse analyses.Per her supervisor's suggestion, she uses VITALITY 2 to kickstart her literature review.</p>
<p>She begins using the "Saved Papers" feature in VITALITY 2 to collect papers through various search methods.She starts with a recent paper her supervisor suggested from the CHI conference by Sarma and colleagues [33] and both "Selects" it and "Saves" it.From this selection, she searches for related papers using the ADA embedding.Of the 25 output similar papers, she decides to save any that have a similarity score of &gt; 0.1, which results in an additional 5 papers (e.g., [12,19]).She highlights the papers in the UMAP visualization and hovers on nearby papers, selecting an additional relevant paper on parallel program performance [13].</p>
<p>Noori visits the set of saved papers in "Saved Papers" and selects "Literature Review."VITALITY 2 shares (1) a paper-by-paper summarization, then (2) a comparison and contrast of the set of 7 total papers.Noori adjusts the default prompt to describe a higherlevel summary to suit her experience, then reads through the result.The literature review points out the motivation of most of the papers on transparency of data analysis, and reveals a divide where some papers represented earlier relevant ideas (e.g., [19,34]), while the others were more recent (e.g., [33,24]).She accordingly uses these Step 3 Find Similar Papers by UMAP 1. Hover to view paper title 2. Click to select 3. Click to add selected papers to "Similarity Search"</p>
<p>Step 1</p>
<p>"Chat with your data"</p>
<p>Step 4</p>
<p>Find similar papers by seed papers</p>
<p>Step 5</p>
<p>Click to save papers in "saved paper" list</p>
<p>Step 6</p>
<p>Do "Summarize" or "Literature Review" on saved papers observations to help her write a first draft of the related work section on multiverse visualization to her team's Overleaf document.She uses the "Export" feature of VITALITY 2 to add the related papers to the .bibfile, and she adds a short paragraph detailing her literature review methodology -including use of VITALITY 2 and LLM summarization in the initial draft.She reads the papers in more detail, iterates on the writing accordingly, then shares it with her supervisor for feedback.She begins her next literature review task: identifying and summarizing literature on general data analysis workflows with VITALITY 2.</p>
<p>Contextual Conversations about Papers with LLM</p>
<p>Aaron is a new faculty member at a university, having just defended his Ph.D. Having primarily used quantitative methods for his dissertation research, he wants to explore qualitative methods in his next project: conducting in-depth interviews to understand the potential of visualization to address user concerns around misinformation.</p>
<p>Aaron begins using VITALITY 2 to search for a CSCW paper he knows on misinformation in times of crisis [15].He selects the "info" icon to read the abstract and additional paper metadata.The abstract mentions the use of "constructivist grounded theory to guide [their] inquiry."Aaron has heard of this method before, but is not familiar with how it works.Using the LLM interface in VI-TALITY 2, he asks "can you explain how the method of grounded theory works?"VITALITY 2 responds with an in-depth description (Figure 4).He then asks "can you find me some relevant papers on the topic of grounded theory?"The system responds with a description of some relevant papers, which use the grounded theory method.The result is shown in Figure 4.</p>
<p>Aaron continues his literature review accordingly, having a better baseline understanding of the method.Aaron notices that VITAL-ITY 2 highlights the titles of the papers cited in the answer in bold blue text.Aaron clicks on these titles, and VITALITY 2 pops up a function box, including (1) Highlight in the UMAP visualization, (2) Add the paper to the similarity search, and (3) Save the paper to the "Saved Papers" list.Aaron selects one of the papers he is interested in and uses the similarity search function to find other papers similar to it.Next, Aaron saves all of these articles in the "Saved Paper" list.Finally, Aaron uses the Summarize and Literature Review features to further review on these saved papers.</p>
<p>DISCUSSION, LIMITATIONS, &amp; FUTURE WORK</p>
<p>We observe a few limitations to our current approach.First, while using LLMs to summarize a set of papers can be a useful starting point for a literature review, the quality of the output is far from sufficient for being included in a paper as-is.The summarization is based on the meta-data that is contained in the VITALITY 2 paper corpus, which does not include full-text of the articles.A potential solution is to optimize the web crawler to also retrieve the full text of papers, segment these texts into appropriately sized chunks [49] and generate embeddings for these chunks.These embeddings, along with the associated meta-data (title, authors, keywords, abstract, etc.), should then be stored in the vector database.This will ensure more comprehensive and accurate summaries.</p>
<p>Furthermore, some studies also indicate that there is concern within the academic community regarding the development and use of LLMs [18].The accuracy of response by LLMs depends on the initial training.Potential biases, inaccuracies, or misunderstandings present in the data used for training the models can lead to erroneous outputs.To mitigate this problem, we suggest adding a user prompt in VITALITY 2 to explicitly warn users about the limitations of content generated by VITALITY 2, thereby reminding and cautioning users to use this tool with care.</p>
<p>Another concern is hallucination from LLMs [46].In one instance, when asked to describe the concept of "grounded theory" in Usage Scenario 2 (Section 5), the LLM described a reasonable high-level summary of the approach, but referenced a paper not contained in the VITALITY 2 database.Upon searching the title, it referred to a reasonably popular book from Birks and Mills in 2015 with more than 2,000 citations at the time of this writing [5].However, when asking for additional relevant papers on the topic of grounded theory (implying that they come from the VITALITY 2 corpus), the LLM responded with some paper titles that were neither contained in the VITALITY 2 database, nor obvious paper titles outside the database according to a Google Scholar search.Although completely eliminating hallucinations in LLMs remains challenging [48], various strategies exist to reduce their occurrence.As discussed in this paper, actually the RAG architecture is one such approach.RAG mitigates LLMs hallucinations by providing additional contextual information.In future work, we can further reduce hallucinations in RAG by incorporating external knowledge bases from reliable sources, such as Google Scholar search.Additionally, as research on LLMs advances, several potential methods to address the issue of hallucinations in LLMs have emerged.Recent benchmarks suggest that the incidence of AI hallucination is relatively small for GPT-4 by OpenAI [23].</p>
<p>Figure 1 : 3 ) 4 )
134
Figure 1: Architecture of VITALITY 2: (Step 1) User input to the system.(Step 2 &amp; 3) Retrieve data from the vector database.(Step 4) Combine the result with user input in the prompt.(Step 5) Recall result from LLM. (Step 6) Return the final result to the user.</p>
<p>2 EFigure 2 :
22
Figure 2: The VITALITY 2 User Interface.(A) Paper Collection View shows the entire corpus of publications, (B) Similarity Search View shows options to look-up publications that are similar to another list of publications or by a work-in-progress title and abstract, (C) Visualization Canvas shows an interactive 2-D UMAP projection of the embedding space of the entire paper collection, (D) Meta View shows summaries of certain attributes with respect to the Paper Collection View (A), (E) Opens a Saved Papers View from which the saved papers can be exported as JSON.Extending VITALITY 1, we added (F) Chat with your Data view to allow users to ask natural-language based questions based on the paper corpus.We also added ADA embeddings (in addition to GloVe and SPECTER embeddings) and enable users to Summarize or write a Literature Review on the Saved Papers using LLMs, including the ability to customize the prompts.</p>
<p>Figure 3 :
3
Figure 3: Noori's process of doing literature review using VITALITY 2. (Step 1) Noori searches the VITALITY 2 database for articles recommended by supervisor.(Step 2) Noori selects Ada Embedding as the embedding option used by VITALITY 2 in "Similarity search".(Step 3) Noori adds the paper she just searched as a seed for "Similarity Search".(Step 4) Noori uses "Similarity Search" to find some related papers.(Step 5) Noori saves papers with similarity score of &gt; 0.1 and highlights them in the UMAP Visualization MAP.(Step 6) Noori selects an additional paper that interested her in the UMAP visualization map.(Step 7) Noori tries to modify and use different prompts and does "Summarize" and "Literature Review".(Step 8) Noori exports the saved papers to a bib file.</p>
<p>F Step 2
2
Click to Plot in UMAP Visualization Map 1. Click on the blue highlighted title 2. Click to plot</p>
<p>Figure 4 :
4
Figure 4: Aaron's process for literature search using VITALITY 2. (Step 1) Aaron utilizes the "Chat with your data" feature to quickly explore the domain of "grounded theory", an area unfamiliar to him.(Step 2) Aaron plots an paper cited in the LLM feedback onto the UMAP visualization interface.(Step 3) Aaron selects a set of closely related papers from the UMAP visualization and Aaron adds these papers to the "Similarity Search".(Step 4) Aaron uses "Similarity Search" feature to find some semantically similar papers.(Step 5) Aaron saves a subset of papers of particular interest to his "saved papers" list.(Step 6) Aaron employs the "Summarize" and "Literature Review" feature to review the saved papers.</p>
<p>CONCLUSIONWe introduced a system, VITALITY 2, for conducting literature review using a RAG architecture, a corpus of more than 66,000 papers from visualization-related venues, and novel features for interacting with the paper corpus through Large Language Models (LLMs).We provide the system as an open-sourced code contribution at https://vitality-vis.github.io,alongside the paper corpus, and hope to stimulate future work in optimizing literature review methods.
Serendip: Topic model-driven visual exploration of text corpora. E Alexander, J Kohlmann, R Valenza, M Witmore, M Gleicher, 2014 IEEE Conference on Visual Analytics Science and Technology (VAST). IEEE20141</p>
<p>Writing narrative literature reviews. R F Baumeister, M R Leary, Review of general psychology. 131997</p>
<p>Dimensionality reduction for visualizing single-cell data using umap. E Becht, L Mcinnes, J Healy, C.-A Dutertre, I W Kwok, L G Ng, F Ginhoux, E W Newell, Nature biotechnology. 3712019</p>
<p>GlassViz: Visualizing Automatically-Extracted Entry Points for Exploring Scientific Corpora in Problem-Driven Visualization Research. A Benito-Santos, R Therón, 2020 IEEE Visualization Conference (VIS). 2020</p>
<p>Grounded theory: A practical guide. M Birks, J Mills, Sage. 62015</p>
<p>Papervis: Literature review made easy. J.-K Chou, C.-K Yang, Computer Graphics Forum. Wiley Online Library201130</p>
<p>The AI-native open-source embedding database. Chromadb, 2024-04-23. 2</p>
<p>Specter: Document-level representation learning using citationinformed transformers. A Cohan, S Feldman, I Beltagy, D Downey, D S Weld, ACL. 20201</p>
<p>Visualbib: narrative views for customized bibliographies. A Dattolo, M Corbatto, 2018 22nd International Conference Information Visualisation (IV). IEEE2018</p>
<p>Beyond keyword search: discovering relevant scientific literature. K El-Arini, C Guestrin, ACM SIGKDD. 20111</p>
<p>. Api Models -Openai, 2024-04-23. 3</p>
<p>Understanding and supporting debugging workflows in multiverse analysis. K Gu, E Jun, T Althoff, CHI. 2023</p>
<p>Visualizing parallel programs and performance. S T Hackstadt, A D Malony, IEEE CGA. 1541995</p>
<p>Citerivers: Visual analytics of citation patterns. F Heimerl, Q Han, S Koch, T Ertl, IEEE TVCG. 2212015</p>
<p>Pedersen. Connected through crisis: Emotional proximity and the spread of misinformation online. Y L Huang, K Starbird, M Orand, S A Stanek, H , ACM CSCW. 2015</p>
<p>Visualization as seen through its research paper keywords. P Isenberg, T Isenberg, M Sedlmair, J Chen, T Möller, IEEE TVCG. 122016</p>
<p>Billion-scale similarity search with gpus. J Johnson, M Douze, H Jégou, 2019</p>
<p>Chatgpt and large language model (llm) chatbots: The current state of acceptability and a proposal for guidelines on utilization in academic medicine. J K Kim, M Chua, M Rickard, A Lorenzo, 10.1016/j.jpurol.2023.05.0185Journal of Pediatric Urology. 1952023</p>
<p>Analyzing high-dimensional multivariate network links with integrated anomaly detection, highlighting and exploration. S Ko, S Afzal, S Walton, Y Yang, J Chae, A Malik, Y Jang, M Chen, D Ebert, 2014 IEEE conference on visual analytics science and technology (VAST). IEEE2014</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, S Riedel, D Kiela, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Principal components analysis (pca). A Maćkiewicz, W Ratajczak, Computers &amp; Geosciences. 1931993</p>
<p>Halgamuge. A Culturally Sensitive Test to Evaluate Nuanced GPT Hallucination. T R Mcintosh, T Liu, T Susnjak, P Watters, A Ng, M , 10.1109/TAI.2023.33328376IEEE Transactions on Artificial Intelligence. 2023IEEE Transactions on Artificial Intelligence</p>
<p>Multiverse: mining collective data science knowledge from code on the web to suggest alternative analysis approaches. M A Merrill, G Zhang, T Althoff, ACM SIGKDD. 2021</p>
<p>N Muennighoff, N Tazi, L Magne, N Reimers, arXiv:2210.07316Mteb: Massive text embedding benchmark. 2022arXiv preprint</p>
<p>VitaLITy: Promoting Serendipitous Discovery of Academic Literature with Transformers &amp; Visual Analytics. A Narechania, A Karduni, R Wesslen, E Wall, IEEE TVCG. 13Jan. 2022</p>
<p>Text and code embeddings by contrastive pre-training. A Neelakantan, T Xu, R Puri, A Radford, J M Han, J Tworek, Q Yuan, N Tezak, J W Kim, C Hallacy, 2022</p>
<p>cessed: 2024-04-23. 2OpenAI. New and improved embedding model. 2022</p>
<p>Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections. S Palani, A Naik, D Downey, A X Zhang, J Bragg, J C Chang, ACM CHI. 2023</p>
<p>GloVe: Global Vectors for Word Representation. J Pennington, R Socher, C Manning, ACM EMNLP. 2014</p>
<p>Glove: Global vectors for word representation. J Pennington, R Socher, C D Manning, ACM EMNLP. 2014</p>
<p>Generating Analytic Specifications for Data Visualization from Natural Language Queries using Large Language Models. S Sah, R Mitra, A Narechania, A Endert, J Stasko, W Dou, NLVIZ Workshop (IEEE VIS). 2024</p>
<p>multiverse: Multiplexing alternative data analyses in r notebooks. A Sarma, A Kale, M J Moon, N Taback, F Chevalier, J Hullman, M Kay, ACM CHI. 2023</p>
<p>Multiverse data-flow control. B Schindler, J Waser, H Ribičić, R Fuchs, R Peikert, IEEE TVCG. 1962012</p>
<p>Literature review as a research methodology: An overview and guidelines. H Snyder, Journal of Business Research. 10412019</p>
<p>Understanding and Supporting Academic Literature Review Workflows with LitSense. N Sultanum, C Murad, D Wigdor, Proceedings of the International Conference on Advanced Visual Interfaces. the International Conference on Advanced Visual Interfaces2020</p>
<p>Datatales: Investigating the use of large language models for authoring data-driven articles. N Sultanum, A Srinivasan, 2023 IEEE Visualization and Visual Analytics (VIS). 2023</p>
<p>PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks. J Tang, M Qu, Q Mei, ACM SIGKDD. 2015</p>
<p>Chartgpt: Leveraging LLMs to generate charts from abstract natural language. Y Tian, W Cui, D Deng, X Yi, Y Yang, H Zhang, Y Wu, IEEE TVCG. 12024</p>
<p>Visualizing data using t-sne. L Van Der Maaten, G Hinton, Journal of machine learning research. 9112008</p>
<p>X Wang, S L Huey, R Sheng, S Mehta, F Wang, 10.48550/arXiv.2404.13765Sci-DaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model. </p>
<p>Vispubcompas: a comparative analytical system for visualization publication data. Y Wang, M Yu, G Shan, H.-W Shen, Z Lu, 201922Journal of Visualization</p>
<p>J Wilkins, J Järvi, A Jain, G Kejriwal, A Kerne, V Gumudavelly, Evolutionworks, IFIP Conference on Human-Computer Interaction. Springer2015</p>
<p>Literature Explorer: effective retrieval of scientific documents through nonparametric thematic topic detection. The Visual Computer. S Wu, Y Zhao, F Parvinzamir, N Ersotelos, S Wei, F Dong, 10.1007/s00371-019-01721-72July 202036</p>
<p>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. T Wu, M Terry, C J Cai, ACM CHI. 2022</p>
<p>Hallucination is inevitable: An innate limitation of large language models. Z Xu, S Jain, M Kankanhalli, arXiv:2401.118172024arXiv preprint</p>
<p>J.-Y Yao, K.-P Ning, Z.-H Liu, M.-N Ning, L Yuan, 10.48550/arXiv.2310.014693LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. Oct. 2023</p>
<p>J.-Y Yao, K.-P Ning, Z.-H Liu, M.-N Ning, L Yuan, arXiv:2310.01469Llm lies: Hallucinations are not bugs, but features as adversarial examples. 2023arXiv preprint</p>
<p>A J Yepes, Y You, J Milczek, S Laverde, L Li, arXiv:2402.05131Financial report chunking for effective retrieval augmented generation. 2024arXiv preprint</p>
<p>A Conference Paper Exploring System Based on Citing Motivation and Topic. T Yoon, H Han, H Ha, J Hong, K Lee, IEEE PacificVis. 2020</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
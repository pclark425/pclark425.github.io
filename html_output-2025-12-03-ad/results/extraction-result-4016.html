<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4016 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4016</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4016</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-93.html">extraction-schema-93</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-69ecf88a0d9752db7dc32b4917ee24b4974cea18</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/69ecf88a0d9752db7dc32b4917ee24b4974cea18" target="_blank">JudgeLM: Fine-tuned Large Language Models are Scalable Judges</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges.</p>
                <p><strong>Paper Abstract:</strong> Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, multi-turn chat, etc. Code is available at https://github.com/baaivision/JudgeLM.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4016.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4016.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JudgeLM vs GPT-4 (teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison between JudgeLM (fine-tuned LLM judge) and GPT-4 teacher judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative and qualitative comparison of a fine-tuned LLM judge (JudgeLM) against its teacher judge GPT-4, reporting agreement, consistency, and differences in bias and failure modes, plus methods that close gaps or improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-judge (fine-tuned JudgeLM) compared to GPT-4 teacher on the JudgeLM validation benchmark (pairwise answer comparison; with/without references).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Open-ended instruction-following answer evaluation (pairwise comparisons across broad instruction categories).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (teacher); JudgeLM (7B/13B/33B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>JudgeLM-33B agreement with GPT-4 on JudgeLM val set: 89.03% (Table 8). Scaling experiments report up to 90.06% agreement with GPT-4 for JudgeLM-33B when fine-tuned with 100K samples (Table 4); GPT-4 self-consistency on swap: 85.82%, JudgeLM-33B consistency: 91.36% (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Fine-tuned JudgeLM tends to be more self-consistent (higher swap-consistency) than GPT-4 on the evaluated benchmark; JudgeLM can be biased by fine-tuning data and formats while GPT-4 (API) may show variability across prompt formats and API versions. The paper also notes GPT-4 judgments change when references are provided vs not, indicating sensitivity to external reference presence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLM teacher (GPT-4) and student (JudgeLM) both exhibit position, knowledge, and format biases; GPT-4-based API judges face reproducibility and data-exposure risks (API changes/data leakage). Fine-tuned judges can inherit and overfit teacher biases from training data if not mitigated.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>JudgeLM can match or approach GPT-4 agreement and, in these experiments, achieve higher consistency than GPT-4; fine-tuning with large, high-quality GPT-4-labeled data plus techniques (swap augmentation, reference support, reference drop) can yield judges that equal or exceed their teacher on specific benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Fine-tune open models on large, diverse GPT-4-labeled judge datasets; mitigate position bias with swap augmentation; mitigate knowledge gaps with reference support; avoid format overfitting with reference drop (mixing formats during training); validate with held-out human-annotated data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>JudgeLM: Fine-tuned Large Language Models are Scalable Judges — Lianghui Zhu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JudgeLM: Fine-tuned Large Language Models are Scalable Judges', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4016.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4016.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JudgeLM vs Human (PandaLM test)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison between JudgeLM judgments and human annotations on the PandaLM test set</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot evaluation of JudgeLM variants against human-annotated labels on the PandaLM test set, showing how fine-tuned LLM judges compare to human judgments across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Zero-shot LLM-as-a-judge evaluated against human-annotated PandaLM test set (pairwise comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Open-ended instruction-following tasks from PandaLM distribution (diverse categories; includes out-of-distribution items relative to JudgeLM train set).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>JudgeLM-7B / JudgeLM-13B / JudgeLM-33B; comparisons include GPT-3.5 and GPT-4 (reported from PandaLM/GPT results).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Reported agreement with human labels (PandaLM test): GPT-3.5* 62.96%, GPT-4* 66.47% (reported by PandaLM); JudgeLM-7B 65.07%, JudgeLM-13B 68.97%, JudgeLM-33B 75.18% (Table 2). The paper notes max human-human agreement in MT-bench is ~82% (footnote).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>JudgeLM performance depends strongly on model scale and fine-tuning data; on out-of-distribution categories (PandaLM contains many categories not in JudgeLM train), agreement is lower. LLM judges may reflect teacher/model priors rather than nuanced human judgment on categories unseen in training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Agreement with human labels is lower than ideal human-to-human agreement (reported human max ~82%); domain mismatch / out-of-distribution tasks reduce agreement; vanilla unfine-tuned models (e.g., Vicuna) may outright fail to produce usable pairwise scores for many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Large JudgeLM (33B) surpasses GPT-3.5 and GPT-4 (as reported on this PandaLM test) in agreement with human labels in this benchmark, demonstrating that a specialized, fine-tuned open judge can outperform zero-shot API judges on some human-annotated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>For human-aligned judgments, fine-tune with diverse, human-checked teacher annotations and validate on human-annotated held-outs; increase model size and data scale to improve human agreement; when evaluating OOD data, include reference-support or human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>JudgeLM: Fine-tuned Large Language Models are Scalable Judges — Lianghui Zhu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JudgeLM: Fine-tuned Large Language Models are Scalable Judges', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4016.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4016.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JudgeLM vs Human (MM-Vet multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison between JudgeLM and human annotators on the MM-Vet multimodal benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot evaluation of JudgeLM-33B on a human-annotated multimodal evaluation (MM-Vet), compared to GPT-4 (0-shot and 7-shot) and GPT-3.5, showing JudgeLM's multimodal judging generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Zero-shot LLM-as-a-judge for multimodal model outputs (evaluating LLaVA outputs) against human annotations on MM-Vet.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Multimodal output evaluation (image+text contexts) judged against human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>JudgeLM-33B (0-shot); GPT-4 (0-shot and 7-shot); GPT-3.5 (7-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Agreement with human labels (MM-Vet): GPT-4 (7-shot) 95.58%, GPT-4 (0-shot) 86.70%, GPT-3.5 (7-shot) 83.03%, JudgeLM-33B (0-shot) 91.74% (Table 13).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Few-shot prompting substantially improves GPT-4's agreement vs 0-shot; a fine-tuned JudgeLM can achieve high zero-shot agreement close to few-shot GPT-4, indicating fine-tuning transfers multimodal evaluation ability into an open model. However, best absolute agreement in these experiments remained with GPT-4 (7-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>JudgeLM (zero-shot) did not surpass GPT-4 with few-shot context; sensitivity to prompt/shots affects closed-source APIs; possible misses of subtle multimodal cues where few-shot expert prompting helps.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>JudgeLM-33B outperformed GPT-4 (0-shot) and GPT-3.5 (7-shot) in agreement with humans and obtained high precision (91.08%). This shows a fine-tuned open judge can be competitive with closed-source judges for multimodal evaluation while offering reproducibility and privacy.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>For multimodal evaluation, either use few-shot prompting with strong API judges or fine-tune an open judge on multimodal judge data; validate with human-annotated benchmarks to confirm alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>JudgeLM: Fine-tuned Large Language Models are Scalable Judges — Lianghui Zhu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JudgeLM: Fine-tuned Large Language Models are Scalable Judges', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4016.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4016.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM judge biases & failure modes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Position bias, Knowledge bias, Format bias and concrete failure modes when using LLMs as judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Detailed analysis of three inherent biases (position, knowledge, format) that degrade LLM-as-judge evaluations and concrete failure examples (e.g., vanilla Vicuna failing to output scores for many cases).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Analytic study of reliability metrics (agreement, consistency, bias toward 1st/2nd) and ablations to expose failure modes of LLM judges on the JudgeLM validation benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Open-ended answer evaluation (pairwise) across diverse categories; includes experiments with/without references and with swapped answer order.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Various: JudgeLM (7B/13B/33B), Vicuna, GPT-3.5, GPT-4 (as comparators).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Position bias: judges prefer the first-position answer (measured 'bias toward 1st' up to ~19.83% in a baseline 7B/3.5k setup and reduced with augmentation); Knowledge bias: judges lacking domain knowledge produce incorrect judgments on open-ended or factual tasks; Format bias: models fine-tuned with references perform poorly when evaluated without references and vice versa (mismatched formats can cause large drops in agreement/consistency and increase delta-bias up to ~26.63% in some settings). Also, vanilla instruction models (e.g., Vicuna-13B) sometimes fail to produce usable scores (77% failure rate reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Concrete failures reported: vanilla Vicuna-13B failed to output score pairs for 77% of JudgeLM val questions; mismatched format fine-tuning causes severe degradation (e.g., baseline ft without ref but validated with ref: agreement 73.09%, consistency 67.75%, delta bias 26.63% in Table 6); position-bias manifested as substantial bias-toward-first percentages unless mitigated; knowledge-limited judges commit factual errors on tasks outside pretraining knowledge (Fig. 10 examples referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Mitigations (swap augmentation, reference support, reference drop) reduced position and format biases and improved consistency and agreement; JudgeLM with these methods achieved substantially lower bias measures and higher agreement (e.g., swap aug improved consistency by 5.44% in Table 5; reference support improves matching-format performance in Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Adopt swap augmentation during fine-tuning to reduce position bias; include reference-support training and randomly drop references (reference drop) to avoid format overfitting; validate on matched and mismatched formats and on human-annotated held-outs; design evaluation pipelines to detect and correct position, knowledge, and format biases (e.g., automated swap checks, retrieval/injected references for missing knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>JudgeLM: Fine-tuned Large Language Models are Scalable Judges — Lianghui Zhu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JudgeLM: Fine-tuned Large Language Models are Scalable Judges', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization <em>(Rating: 2)</em></li>
                <li>UltraFeedback: Boosting language models with high-quality feedback <em>(Rating: 1)</em></li>
                <li>Vicuna: An open-source chatbot impressing gpt-4 with 90% * chatgpt quality <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4016",
    "paper_id": "paper-69ecf88a0d9752db7dc32b4917ee24b4974cea18",
    "extraction_schema_id": "extraction-schema-93",
    "extracted_data": [
        {
            "name_short": "JudgeLM vs GPT-4 (teacher)",
            "name_full": "Comparison between JudgeLM (fine-tuned LLM judge) and GPT-4 teacher judgments",
            "brief_description": "Quantitative and qualitative comparison of a fine-tuned LLM judge (JudgeLM) against its teacher judge GPT-4, reporting agreement, consistency, and differences in bias and failure modes, plus methods that close gaps or improve reliability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-a-judge (fine-tuned JudgeLM) compared to GPT-4 teacher on the JudgeLM validation benchmark (pairwise answer comparison; with/without references).",
            "task_or_domain": "Open-ended instruction-following answer evaluation (pairwise comparisons across broad instruction categories).",
            "llm_model_name": "GPT-4 (teacher); JudgeLM (7B/13B/33B variants)",
            "agreement_rate": "JudgeLM-33B agreement with GPT-4 on JudgeLM val set: 89.03% (Table 8). Scaling experiments report up to 90.06% agreement with GPT-4 for JudgeLM-33B when fine-tuned with 100K samples (Table 4); GPT-4 self-consistency on swap: 85.82%, JudgeLM-33B consistency: 91.36% (Table 8).",
            "qualitative_differences": "Fine-tuned JudgeLM tends to be more self-consistent (higher swap-consistency) than GPT-4 on the evaluated benchmark; JudgeLM can be biased by fine-tuning data and formats while GPT-4 (API) may show variability across prompt formats and API versions. The paper also notes GPT-4 judgments change when references are provided vs not, indicating sensitivity to external reference presence.",
            "limitations_or_failure_cases": "LLM teacher (GPT-4) and student (JudgeLM) both exhibit position, knowledge, and format biases; GPT-4-based API judges face reproducibility and data-exposure risks (API changes/data leakage). Fine-tuned judges can inherit and overfit teacher biases from training data if not mitigated.",
            "counterexamples_or_strengths": "JudgeLM can match or approach GPT-4 agreement and, in these experiments, achieve higher consistency than GPT-4; fine-tuning with large, high-quality GPT-4-labeled data plus techniques (swap augmentation, reference support, reference drop) can yield judges that equal or exceed their teacher on specific benchmarks.",
            "recommendations_or_best_practices": "Fine-tune open models on large, diverse GPT-4-labeled judge datasets; mitigate position bias with swap augmentation; mitigate knowledge gaps with reference support; avoid format overfitting with reference drop (mixing formats during training); validate with held-out human-annotated data.",
            "citation": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges — Lianghui Zhu et al.",
            "uuid": "e4016.0",
            "source_info": {
                "paper_title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "JudgeLM vs Human (PandaLM test)",
            "name_full": "Comparison between JudgeLM judgments and human annotations on the PandaLM test set",
            "brief_description": "Zero-shot evaluation of JudgeLM variants against human-annotated labels on the PandaLM test set, showing how fine-tuned LLM judges compare to human judgments across model sizes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Zero-shot LLM-as-a-judge evaluated against human-annotated PandaLM test set (pairwise comparisons).",
            "task_or_domain": "Open-ended instruction-following tasks from PandaLM distribution (diverse categories; includes out-of-distribution items relative to JudgeLM train set).",
            "llm_model_name": "JudgeLM-7B / JudgeLM-13B / JudgeLM-33B; comparisons include GPT-3.5 and GPT-4 (reported from PandaLM/GPT results).",
            "agreement_rate": "Reported agreement with human labels (PandaLM test): GPT-3.5* 62.96%, GPT-4* 66.47% (reported by PandaLM); JudgeLM-7B 65.07%, JudgeLM-13B 68.97%, JudgeLM-33B 75.18% (Table 2). The paper notes max human-human agreement in MT-bench is ~82% (footnote).",
            "qualitative_differences": "JudgeLM performance depends strongly on model scale and fine-tuning data; on out-of-distribution categories (PandaLM contains many categories not in JudgeLM train), agreement is lower. LLM judges may reflect teacher/model priors rather than nuanced human judgment on categories unseen in training.",
            "limitations_or_failure_cases": "Agreement with human labels is lower than ideal human-to-human agreement (reported human max ~82%); domain mismatch / out-of-distribution tasks reduce agreement; vanilla unfine-tuned models (e.g., Vicuna) may outright fail to produce usable pairwise scores for many cases.",
            "counterexamples_or_strengths": "Large JudgeLM (33B) surpasses GPT-3.5 and GPT-4 (as reported on this PandaLM test) in agreement with human labels in this benchmark, demonstrating that a specialized, fine-tuned open judge can outperform zero-shot API judges on some human-annotated benchmarks.",
            "recommendations_or_best_practices": "For human-aligned judgments, fine-tune with diverse, human-checked teacher annotations and validate on human-annotated held-outs; increase model size and data scale to improve human agreement; when evaluating OOD data, include reference-support or human oversight.",
            "citation": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges — Lianghui Zhu et al.",
            "uuid": "e4016.1",
            "source_info": {
                "paper_title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "JudgeLM vs Human (MM-Vet multimodal)",
            "name_full": "Comparison between JudgeLM and human annotators on the MM-Vet multimodal benchmark",
            "brief_description": "Zero-shot evaluation of JudgeLM-33B on a human-annotated multimodal evaluation (MM-Vet), compared to GPT-4 (0-shot and 7-shot) and GPT-3.5, showing JudgeLM's multimodal judging generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Zero-shot LLM-as-a-judge for multimodal model outputs (evaluating LLaVA outputs) against human annotations on MM-Vet.",
            "task_or_domain": "Multimodal output evaluation (image+text contexts) judged against human labels.",
            "llm_model_name": "JudgeLM-33B (0-shot); GPT-4 (0-shot and 7-shot); GPT-3.5 (7-shot).",
            "agreement_rate": "Agreement with human labels (MM-Vet): GPT-4 (7-shot) 95.58%, GPT-4 (0-shot) 86.70%, GPT-3.5 (7-shot) 83.03%, JudgeLM-33B (0-shot) 91.74% (Table 13).",
            "qualitative_differences": "Few-shot prompting substantially improves GPT-4's agreement vs 0-shot; a fine-tuned JudgeLM can achieve high zero-shot agreement close to few-shot GPT-4, indicating fine-tuning transfers multimodal evaluation ability into an open model. However, best absolute agreement in these experiments remained with GPT-4 (7-shot).",
            "limitations_or_failure_cases": "JudgeLM (zero-shot) did not surpass GPT-4 with few-shot context; sensitivity to prompt/shots affects closed-source APIs; possible misses of subtle multimodal cues where few-shot expert prompting helps.",
            "counterexamples_or_strengths": "JudgeLM-33B outperformed GPT-4 (0-shot) and GPT-3.5 (7-shot) in agreement with humans and obtained high precision (91.08%). This shows a fine-tuned open judge can be competitive with closed-source judges for multimodal evaluation while offering reproducibility and privacy.",
            "recommendations_or_best_practices": "For multimodal evaluation, either use few-shot prompting with strong API judges or fine-tune an open judge on multimodal judge data; validate with human-annotated benchmarks to confirm alignment.",
            "citation": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges — Lianghui Zhu et al.",
            "uuid": "e4016.2",
            "source_info": {
                "paper_title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLM judge biases & failure modes",
            "name_full": "Position bias, Knowledge bias, Format bias and concrete failure modes when using LLMs as judges",
            "brief_description": "Detailed analysis of three inherent biases (position, knowledge, format) that degrade LLM-as-judge evaluations and concrete failure examples (e.g., vanilla Vicuna failing to output scores for many cases).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Analytic study of reliability metrics (agreement, consistency, bias toward 1st/2nd) and ablations to expose failure modes of LLM judges on the JudgeLM validation benchmark.",
            "task_or_domain": "Open-ended answer evaluation (pairwise) across diverse categories; includes experiments with/without references and with swapped answer order.",
            "llm_model_name": "Various: JudgeLM (7B/13B/33B), Vicuna, GPT-3.5, GPT-4 (as comparators).",
            "agreement_rate": null,
            "qualitative_differences": "Position bias: judges prefer the first-position answer (measured 'bias toward 1st' up to ~19.83% in a baseline 7B/3.5k setup and reduced with augmentation); Knowledge bias: judges lacking domain knowledge produce incorrect judgments on open-ended or factual tasks; Format bias: models fine-tuned with references perform poorly when evaluated without references and vice versa (mismatched formats can cause large drops in agreement/consistency and increase delta-bias up to ~26.63% in some settings). Also, vanilla instruction models (e.g., Vicuna-13B) sometimes fail to produce usable scores (77% failure rate reported).",
            "limitations_or_failure_cases": "Concrete failures reported: vanilla Vicuna-13B failed to output score pairs for 77% of JudgeLM val questions; mismatched format fine-tuning causes severe degradation (e.g., baseline ft without ref but validated with ref: agreement 73.09%, consistency 67.75%, delta bias 26.63% in Table 6); position-bias manifested as substantial bias-toward-first percentages unless mitigated; knowledge-limited judges commit factual errors on tasks outside pretraining knowledge (Fig. 10 examples referenced).",
            "counterexamples_or_strengths": "Mitigations (swap augmentation, reference support, reference drop) reduced position and format biases and improved consistency and agreement; JudgeLM with these methods achieved substantially lower bias measures and higher agreement (e.g., swap aug improved consistency by 5.44% in Table 5; reference support improves matching-format performance in Table 6).",
            "recommendations_or_best_practices": "Adopt swap augmentation during fine-tuning to reduce position bias; include reference-support training and randomly drop references (reference drop) to avoid format overfitting; validate on matched and mismatched formats and on human-annotated held-outs; design evaluation pipelines to detect and correct position, knowledge, and format biases (e.g., automated swap checks, retrieval/injected references for missing knowledge).",
            "citation": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges — Lianghui Zhu et al.",
            "uuid": "e4016.3",
            "source_info": {
                "paper_title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization",
            "rating": 2
        },
        {
            "paper_title": "UltraFeedback: Boosting language models with high-quality feedback",
            "rating": 1
        },
        {
            "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90% * chatgpt quality",
            "rating": 1
        }
    ],
    "cost": 0.016930999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>JudgeLM: Fine-tuned Large Language Models are Scalable Judges</h1>
<p>Lianghui Zhu ${ }^{1,2 *}$ Xinggang Wang ${ }^{1 \dagger}$ Xinlong Wang ${ }^{2 \dagger}$<br>${ }^{1}$ School of EIC, Huazhong University of Science \&amp; Technology<br>${ }^{2}$ Beijing Academy of Artificial Intelligence<br>Code \&amp; Models: https://github.com/baaivision/JudgeLM</p>
<h4>Abstract</h4>
<p>Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5 K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding $90 \%$ that even surpasses human-to-human agreement ${ }^{1}$. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, multi-turn chat, etc.</p>
<h2>1 INTRODUCTION</h2>
<p>Recent advancements in large language models (LLMs) have fostered significant interest due to their remarkable performance in following instructions and their broad capabilities in dealing with open-ended scenarios. Based on the open-source LLMs, including OPT (Zhang et al., 2022), FlanT5 (Chung et al., 2022), LLaMA (Touvron et al., 2023a), and Pythia (Biderman et al., 2023), researchers propose numerous methods to align these models with human preferences through instruction fine-tuning. These aligned LLMs demonstrate enhanced abilities in comprehending human instructions and generating more coherent responses. Nonetheless, existing benchmarks (Hendrycks et al., 2020; Liang et al., 2022) and traditional metrics (Lin, 2004; Papineni et al., 2002; Zhang et al., 2019; Sellam et al., 2020; Yuan et al., 2021) do not adequately estimate the capabilities of LLMs in open-ended scenarios. Therefore, a new benchmark method that could evaluate LLMs comprehensively in open-ended tasks is needed.</p>
<p>Concurrent works are making efforts to explore various methods for evaluating the performance of LLM. The arena-format (Zheng et al., 2023) methods leverage crowdsourced platforms to extract anonymous LLM competition results. While evaluations by humans are trustworthy, they are also time-consuming and financially demanding. Some approaches (Chiang et al., 2023) utilize GPT-4 as</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>a judge. Nevertheless, these methods grapple with challenges of potential data exposure and volatile API model transitions, potentially compromising the judge’s reproducibility. PandaLM <em>(Wang et al., 2023)</em> attempts to fine-tune open-source LLMs for evaluating answers. However, limitations stemming from the training data quality, and inherent LLM biases, undermine the effectiveness of such fine-tuned models in the role of a judge.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>(a) Data generation pipeline of our JudgeLM. We first collect 105K seed tasks as questions. Then, we extract answers from 11 LLMs and randomly sample a pair of answers from the answer set. Last, we input the tasks, the sampled answer pairs, and optionally reference answers to GPT-4, which generates scores and detailed reasons as a judge teacher.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>(b) An illustration of the JudgeLM’s fine-tuning and various functions. We use generated judge samples to fine-tune LLMs as scalable judges. When fine-tuning LLMs as judges, we also propose swap augmentation, reference support, and reference drop to address the position bias, knowledge bias, and format bias, respectively.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>(c) An illustration of various functions of our JudgeLM.</p>
<p>Figure 1: An overview of our scalable JudgeLM including data generation, fine-tuning, and various functions.</p>
<p>In this paper, we propose to evaluate LLMs through fine-tuned open-source LLMs, which serve as scalable judges (JudgeLM) achieving satisfactory agreement with the teacher judge. Our methodology incorporates scalable judges as evaluators in open-ended tasks, coupled with a high-quality dataset conducive to both training and evaluating the judge models. Within our framework, we adapt open-source LLMs to serve as judges and analyze their scaling ability in relation to model size (ranging from 7B to 33B) and volume of training data (extending from 3.5K to 100K). Our curated dataset comprises 105K seed questions, LLM answer pairs, and judgments from the teacher judge, GPT-4, as shown in Fig. 1a. Note that we generated two judgments for each seed task with and without reference answers. This dataset is partitioned, with 100K seed questions allocated for training (2 × larger than PandaLM) and the remainder for validation (29 × larger than PandaLM).</p>
<p>Utilizing LLMs as judges inevitably introduces biases such as position bias (favoring answers in specific positions), knowledge bias (over-reliance on pre-trained knowledge), and format bias (optimal performance only under specific prompt formats) as shown in Fig. 8, 10, 12, 13. When fine-tuning is not possible, GPT-4-API-based judge <em>(Zheng et al., 2023)</em> tries to alleviate this by well-designed prompt methods, i.e., Chain-of-thought, few-shot judge, and judging multiple times with different positions. JudgeLM presents a new way that can address these biases in the fine-tuning stage, skipping the complicated prompt methods and multi-turn API calling. Moreover, our JudgeLM system presents extended capabilities as shown in Fig. 1b, including grading single answers, judging multiple answers, judging multimodal models, multi-turn chat, etc.</p>
<p>In contrast to arena-format methods, our approach is rapid and has a low cost. For instance, JudgeLM-7B requires only 8 A100 GPUs and can evaluate 5000 response pairs in just 3 minutes. In comparison to closed-source LLM judges, JudgeLM ensures reproducibility and protects user privacy. When compared to concurrent open-source LLM judges, our system explores both the scaling ability and biases in LLM fine-tuning. Furthermore, JudgeLM dataset stands as the most diverse and high-quality one, significantly benefitting subsequent research in judge model investigations.</p>
<p>Our main contributions can be summarized as follows:</p>
<ul>
<li>We introduce a high-quality, large-scale dataset for judge models, enriched with diverse seed tasks, LLMs-generated answers, and detailed judgments from GPT-4, laying the foundation for future LLMs evaluating research.</li>
<li>We propose JudgeLM, a scalable language model judge, designed for evaluating LLMs in open-ended scenarios. It achieves an agreement exceeding $90 \%$ that surpasses the human-to-human agreement. Our JudgeLM can also generalize to many extended tasks.</li>
<li>We analyze the biases inherent to LLM judge fine-tuning and introduce a series of methods to address them. Our methods significantly improve the consistency of the model in different cases, making the JudgeLM more reliable and flexible.</li>
</ul>
<h1>2 Related Work</h1>
<h3>2.1 Instruction Fine-tuning of Large Language Models</h3>
<p>With the development of large language models (LLMs), researchers find that fine-tuning pre-trained LLMs such as GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2020), OPT (Zhang et al., 2022), and PaLM (Chowdhery et al., 2022) enable LLMs to follow human instructions and help with openended tasks. The instruction fine-tuned LLMs such as InstructGPT (Ouyang et al., 2022), ChatGPT (OpenAI, 2022), FLAN-T5 (Chung et al., 2022), FLAN-PaLM (Chung et al., 2022), OPTIML (Iyer et al., 2022), and GPT-4 (OpenAI, 2023) exhibit stronger ability in zero-shot or few-shot tasks than their base models. After Meta released the powerful open-source LLM LLaMA (Touvron et al., 2023a) and LLaMA2 (Touvron et al., 2023b), lots of instruction fine-tuning works based on LLaMA or LLaMA2 were proposed in the natural language generation or multimodal generation domain, such as Alpaca, Vicuna (Chiang et al., 2023), OpenFlamingo (Awadalla et al., 2023), LLaMA-Adapter (Zhang et al., 2023), and Emu (Sun et al., 2023). Our JudgeLM also belongs to the LLaMA family and takes the Vicuna series as base models. Our JudgeLM follows the instruction fine-tuning manner to create LLM judges and proposes to model the judgment-generation task as "grading, judging, and reasoning". We further collect a high-quality, large-scale dataset for research in judging the performance of LLMs.</p>
<h3>2.2 Evaluation of Large Language Models</h3>
<p>As many open-source large language models (LLMs) and their fine-tuned variants are proposed and present remarkable performance on various tasks, evaluating the capabilities of LLMs becomes a popular and challenging task. To address this problem, Chatbot Arena (Zheng et al., 2023) aims to build a crowdsourced platform that ranks the LLMs through pairwise comparison and Elo rating. The crowdsourced way to evaluate LLMs has more reliable results but faces high costs and low efficiency. Vicuna (Chiang et al., 2023) uses GPT-4 as a judge to select the better answer. Although the GPT-4-based method can judge LLMs like a human expert, the API-based methods have potential risks of data leakage and unstable performance. Zeno Build (Alex \&amp; Graham, 2023) proposes to evaluate LLMs at a customer service dataset, but using traditional metrics such as ChrF (Popović, 2015) and BERTScore (Zhang et al., 2019) can not fully evaluate the answers of LLMs in openended tasks. Besides, PandaLM (Wang et al., 2023) and Auto-J Li et al. (2023a) developed judge models based on LLaMA (Touvron et al., 2023a) or LLaMA2 (Touvron et al., 2023b) to compare answers produced by LLMs. When serving as judges, PandaLM achieves an accuracy close to ChatGPT but ignoring the inherent LLM biases limits its performance further. Our JudgeLM contains scalable judges from 7B-parameter to 33B-parameter and achieves state-of-the-art performance in both PandaLM and our benchmarks. Furthermore, researchers can use the proposed JudgeLM locally which ensures reproducibility and data security.</p>
<h2>3 DATASET</h2>
<p>High-quality, large-scale datasets are crucial for effectively fine-tuning large language models (LLMs) to act as evaluative judges. However, the concurrent datasets, such as the one by PandaLM (Wang et al., 2023), present limitations in terms of diversity and the granularity of judgment</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question <br> If Rosslyn Mountain Boys had produced two more albums, how many albums would they have produced?Rosslyn Mountain Boys was a band that backed up a number of nationally known singers, and released two albums of their own.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer 1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ground Truth</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rosslyn Mountain Boys produced two albums on their own, so if they produced two more albums then they would have produced four albums.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer 2 <br> Rosslyn Mountain Boys produced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Traditional Metrics</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Rouge-1 $\uparrow$</td>
<td style="text-align: center;">Rouge-2 $\uparrow$</td>
<td style="text-align: center;">Rouge-L $\uparrow$</td>
<td style="text-align: center;">BLEU $\uparrow$</td>
<td style="text-align: center;">BERTScore $\uparrow$</td>
<td style="text-align: center;">BLEURT $\uparrow$</td>
<td style="text-align: center;">BARTScore $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">$-1.39$</td>
<td style="text-align: center;">$-5.33$</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">$-0.78$</td>
<td style="text-align: center;">$-3.76$</td>
</tr>
<tr>
<td style="text-align: center;">Our Judgment Output</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer 1's Score: 10 <br> Answer 2's Score: 1 <br> Assistant 2's response is incomplete and does not provide any information or answer to the question, hence the low score. On the other hand, Assistant 1's response is accurate and directly answers the question. The Rosslyn Mountain Boys produced two albums, and if they had produced two more, they would have produced a total of four albums. Therefore, Assistant 1 receives a perfect score.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 2: The input and output of our JudgeLM data sample. In open-ended scenarios, traditional metrics can not judge answers accurately by comparing the answers with ground truth. However, the LLM judges can understand the questions and answers and give accurate scores and reasons.
criteria. To address this, we introduce a novel dataset replete with a rich variety of seed tasks, comprehensive answers from modern LLMs, answers' grades from the teacher judge, and detailed reasons for judgments. Section 3.1 elucidates the data generation process, while Section 3.2 delineates the methods adopted for training and evaluation using our dataset.</p>
<h1>3.1 Data Generation</h1>
<p>The primary objective of our data generation is to create a large-scale and diversified dataset that maximizes the evaluative capabilities of judge models. We sample 105 K instruction seed tasks from a large-scale set that contains Alpaca-GPT4 (Peng et al., 2023), Dolly-15K (Conover et al., 2023), GPT4All-LAION (Anand et al., 2023), and ShareGPT. To enhance the heterogeneity of the dataset, answers are collated from 11 leading open-source LLMs including, but not limited to, LLaMA (Touvron et al., 2023a), Alpaca, and Vicuna (Chiang et al., 2023). Following this, we amalgamate LLMgenerated answers with the reference answer to create answer sets. Pairs are randomly selected from the sets, upon which, fine-grained scores and detailed reasons are assigned by the advanced teacher model, GPT-4. To ensure robust and comprehensive judgments, we utilize detailed templates as demonstrated in Fig. 3. Additionally, to allow the model to judge with reference answers, the reference-inclusive template is employed as Fig. 4. This encourages the model to integrate external knowledge during the evaluative process. Please note that all samples in the JudgeLM $v a l$ set are further checked and re-annotated by authors to ensure alignment with human preference.</p>
<h3>3.2 Training and Evaluating</h3>
<p>To better utilize our dataset to train and evaluate the judge models, we partition it into a training split and a validation split. The training set contains 100 K judge samples, while the validation set has 5 K . We then introduce the way we use this dataset to train and evaluate, respectively.</p>
<p>Training. The training process of JudgeLM adheres to the instruction fine-tuning paradigm. As illustrated in Fig. 2, the model is fed a question alongside a pair of answers, and an optional reference answer, yielding outputs comprising scores and detailed reasons. It is imperative to note the significance of a detailed crafted prompt template to harness the full potential of JudgeLM's instructionfollowing ability. Distinct input templates cater to scenarios with and without references, as depicted in Fig. 3 and Fig. 4 respectively.</p>
<p>To further analyze the scaling ability of JudgeLM, we fine-tune JudgeLM with sizes of 7B, 13B, and 33B parameters. The specific hyperparameters are enumerated in Table 11. As for the scaling analysis for dataset size, we also fine-tune JudgeLM on varying data scales from 3.5 K to 100 K samples. JudgeLM demonstrates scaling ability both in terms of model size and data volume.</p>
<p>Evaluating. For the judge's result, we model it as "grading, judging, and reasoning". The judge model first generates scores for answer pairs. Subsequently, we can get the judge result from three situations: "Answer 1 wins" if the answer 1's score is higher than the answer 2's, "Answer 2 wins" if the answer 2's score is higher, or "Tie" if the scores of two answers are the same. Last, the model generates detailed reasons if needed. The advantage of this modeling is that the judge model just needs little time to grade and judge, and generates time-consuming reasoning optionally.</p>
<p>For the metrics, we employ the objective metrics and reliability metrics to evaluate the judge models comprehensively. For the objective metrics, we compute the agreement, precision, recall, and F1score between the model's judge results and those of the teacher. This provides insights into the alignment of judge models with established benchmarks, such as GPT-4 or human experts. As for reliability metrics, we first compare the results before and after swapping LLM answers. Then we calculate the self-consistency to measure the judge model's reliability. Last, we further calculate the metrics like "bias toward 1st", "bias toward 2nd", and "delta bias" to get insights from specific position biases and their variance.</p>
<h1>4 InHERENT BIAS</h1>
<p>In this paper, we also study the inherent biases that influence the reliability of fine-tuned LLM judges through reliability metrics and visualizations.</p>
<p>Position Bias. Position bias means that the LLM judges prefer answers in a certain position and it widely exists in natural language processing tasks (Ko et al., 2020; Wang et al., 2018) and decisionmaking of humans (Blunch, 1984; Raghubir \&amp; Valenzuela, 2006). The powerful LLMs, ChatGPT and GPT-4, also face this challenge when working as judges (Wang et al., 2023; Zheng et al., 2023; Li et al., 2023b). As the qualitative and quantitative results shown in Fig. 8 and Table 5, JudgeLM also faces the position bias and prefers the first answer when swapping the positions of answers.</p>
<p>Knowledge Bias. Knowledge bias arises when the pre-trained data lacks the knowledge of some seed tasks or induces possibly undesirable knowledge (Ko et al., 2020; Zheng et al., 2023) that could degenerate the generative capabilities of LLMs. Fig. 10 provides an example that LLM judges can not give correct judgments to open-ended tasks if they lack related truth.</p>
<p>Format Bias. Researchers expect that the judge model can make judgments based on pre-trained knowledge when the reference is not available and can make judgments following the reference when it is available. However, our experiments revealed that judge models fine-tuned without reference perform poorly in judging with reference, and vice versa, as shown in Fig. 12, Fig. 13, and Table 6. We hypothesize fine-tuning with references encourages the judge model to make judgments based on external knowledge and fine-tuning without references pushes the judge model to make judgments through its pre-trained knowledge. We name the situation that a judge fine-tuned without reference but validated with reference as a mismatched format, and vice versa. Such a format bias limits the further generalization of the judge model in other domains.</p>
<h2>5 Method</h2>
<p>In evaluating LLM-generated answers for a seed question, the LLM judge aims to determine the superior answer from a pair of candidates. Motivated by recent methods (Touvron et al., 2023a; Chiang et al., 2023; Ouyang et al., 2022), we present JudgeLM, a scalable judge model, and address</p>
<p>inherent biases in such models. Our methodology is depicted in Fig. 1b. The subsequent sections provide a detailed breakdown of our approach.</p>
<h1>5.1 Swap Augmentation</h1>
<p>MT-bench (Zheng et al., 2023) and PandaLM (Wang et al., 2023) alleviate the position bias by judging twice with original and reverse order. These methods regard the result as a tie if the judgments are not the same. This kind of method ignoring the inherent position bias and casting double time to evaluate, can be regarded as a compromise and does not improve the reliability of LLM judges.</p>
<p>Intuitively, swapping the positions at the fine-tuning stage could push the judge model to pay more attention to the contents of answers rather than positions. Leveraging our structured judge data, we can easily swap the positions of answers to generate a new input sample. Correspondingly, we also swap the scores and question indexes of the judgment from the teacher (i.e., GPT4) to get the new ground truth. As shown in Fig. 15, the augmented judge sample keeps the same results but exchanges the positions of answers. Overall, it is simple but effective to augment the training data and address position bias. The JudgeLM-with-swap-augmentation can give good judgment to the same judge sample as shown in Fig. 9.</p>
<h3>5.2 REFERENCE SUPPORT</h3>
<p>Introducing external knowledge in the fine-tuning stage is an intuitive way to make up for the lack of related pre-trained knowledge. To do so, we propose the reference support method to teach the model to judge with the help of reference answers. Following Zheng et al. (2023), we collect reference answers for all judge samples and re-generate reference-guided judgments by GPT-4. Please note that GPT-4 also gives different scores and judgments for most judge samples with or without references. This proves that the differences between pre-trained knowledge and reference answers greatly impact judgments. As shown in Fig. 11, the JudgeLM with reference support can avoid factual errors and give reliable judgments. Furthermore, introducing reference support to LLM judges can simply insert judge preferences. JudgeLM with reference support training can flexibly set reference answers with different preferences for different scenarios and needs. As shown in Fig. 16, changing reference answers does not need extra training and makes JudgeLM more flexible to different preferences.</p>
<h3>5.3 REFERENCE DROP</h3>
<p>To address the format bias, we introduce a method, named reference drop, in which we randomly drop the training sample with reference and use the corresponding sample without reference. As shown in Fig. 14, judge models with reference drop can alleviate the overfitting for fine-tuning formats and make judgments based on external reference or pre-trained knowledge when given reference or not, respectively. Furthermore, the reference drop method also makes the judge model easy to use and decreases the cost of fitting into different formats.</p>
<h2>6 EXPERIMENT</h2>
<p>We study the performance of JudgeLM as follows: Section 6.1 presents the main results of JudgeLM comparing with concurrent methods, Section 6.2 analyzes the scaling ability of JudgeLM from both model sizes and data scales, and Section 6.3 shows ablation studies of proposed methods in detail. Detailed settings are shown in Section A.2.</p>
<h3>6.1 MAIN RESULTS</h3>
<p>Comparison on JudgeLM Benchmark. We first evaluate the proposed JudgeLM on our val set. Note that JudgeLM val set is further checked and re-annotated by authors to ensure alignment with human preference. As shown in Table 1, we give the quantitative results of GPT-3.5, Vicuna-13B, PandaLM-7B, Auto-J-13B (Li et al., 2023a), InstructScore-7B (Xu et al., 2023), and our JudgeLM with three model sizes. Among them, GPT-3.5 is used in the form of APIs with the help of templates in Fig. 3 and Fig. 4. PandaLM-7B and Auto-J-13B are deployed with the released checkpoints and</p>
<p>Table 1: Main results for our JudgeLM and concurrent methods on our val set, which uses GPT-4 annotation results as ground truth.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Agreement $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Precision $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Recall $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">F1 $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Consistency $\uparrow$ <br> (w/ swap.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Judge w/o reference.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">73.83</td>
<td style="text-align: center;">70.70</td>
<td style="text-align: center;">52.80</td>
<td style="text-align: center;">52.85</td>
<td style="text-align: center;">68.89</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-13B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">PandaLM-7B</td>
<td style="text-align: center;">68.61</td>
<td style="text-align: center;">40.75</td>
<td style="text-align: center;">38.82</td>
<td style="text-align: center;">39.41</td>
<td style="text-align: center;">74.78</td>
</tr>
<tr>
<td style="text-align: left;">Auto-J-13B</td>
<td style="text-align: center;">74.86</td>
<td style="text-align: center;">61.65</td>
<td style="text-align: center;">57.53</td>
<td style="text-align: center;">58.14</td>
<td style="text-align: center;">84.34</td>
</tr>
<tr>
<td style="text-align: left;">Judge w/o reference (Ours).</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-7B</td>
<td style="text-align: center;">81.11</td>
<td style="text-align: center;">69.67</td>
<td style="text-align: center;">78.39</td>
<td style="text-align: center;">72.21</td>
<td style="text-align: center;">83.57</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-13B</td>
<td style="text-align: center;">84.33</td>
<td style="text-align: center;">73.69</td>
<td style="text-align: center;">80.51</td>
<td style="text-align: center;">76.17</td>
<td style="text-align: center;">85.01</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-33B</td>
<td style="text-align: center;">89.03</td>
<td style="text-align: center;">80.97</td>
<td style="text-align: center;">84.76</td>
<td style="text-align: center;">82.64</td>
<td style="text-align: center;">91.36</td>
</tr>
<tr>
<td style="text-align: left;">Judge w/ reference.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">71.46</td>
<td style="text-align: center;">56.86</td>
<td style="text-align: center;">51.12</td>
<td style="text-align: center;">51.14</td>
<td style="text-align: center;">62.94</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-13B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">PandaLM-7B</td>
<td style="text-align: center;">63.77</td>
<td style="text-align: center;">39.79</td>
<td style="text-align: center;">34.82</td>
<td style="text-align: center;">35.18</td>
<td style="text-align: center;">55.39</td>
</tr>
<tr>
<td style="text-align: left;">Auto-J-13B</td>
<td style="text-align: center;">72.90</td>
<td style="text-align: center;">58.80</td>
<td style="text-align: center;">56.12</td>
<td style="text-align: center;">56.59</td>
<td style="text-align: center;">82.84</td>
</tr>
<tr>
<td style="text-align: left;">InstructScore-7B</td>
<td style="text-align: center;">55.80</td>
<td style="text-align: center;">58.74</td>
<td style="text-align: center;">56.84</td>
<td style="text-align: center;">53.72</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Judge w/ reference (Ours).</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-7B</td>
<td style="text-align: center;">84.08</td>
<td style="text-align: center;">75.92</td>
<td style="text-align: center;">82.55</td>
<td style="text-align: center;">78.28</td>
<td style="text-align: center;">84.46</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-13B</td>
<td style="text-align: center;">85.47</td>
<td style="text-align: center;">77.71</td>
<td style="text-align: center;">82.90</td>
<td style="text-align: center;">79.77</td>
<td style="text-align: center;">87.23</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-33B</td>
<td style="text-align: center;">89.32</td>
<td style="text-align: center;">84.00</td>
<td style="text-align: center;">86.21</td>
<td style="text-align: center;">84.98</td>
<td style="text-align: center;">92.37</td>
</tr>
</tbody>
</table>
<p>Table 2: JudgeLM zero-shot evaluation results on PandaLM test set, which uses human annotation results as ground truth. "*" means the results are reported in PandaLM (Wang et al., 2023)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Agreement $\uparrow$ <br> (w/ Human)</th>
<th style="text-align: center;">Precision $\uparrow$ <br> (w/ Human)</th>
<th style="text-align: center;">Recall $\uparrow$ <br> (w/ Human)</th>
<th style="text-align: center;">F1 $\uparrow$ <br> (w/ Human)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">zero-shot methods.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5*</td>
<td style="text-align: center;">62.96</td>
<td style="text-align: center;">61.95</td>
<td style="text-align: center;">63.59</td>
<td style="text-align: center;">58.20</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4*</td>
<td style="text-align: center;">66.47</td>
<td style="text-align: center;">66.20</td>
<td style="text-align: center;">68.15</td>
<td style="text-align: center;">61.80</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned on PandaLM train set.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">PandaLM-7B*</td>
<td style="text-align: center;">59.26</td>
<td style="text-align: center;">57.28</td>
<td style="text-align: center;">59.23</td>
<td style="text-align: center;">54.56</td>
</tr>
<tr>
<td style="text-align: left;">Ours (zero-shot).</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-7B</td>
<td style="text-align: center;">65.07</td>
<td style="text-align: center;">66.89</td>
<td style="text-align: center;">71.95</td>
<td style="text-align: center;">61.92</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-13B</td>
<td style="text-align: center;">68.97</td>
<td style="text-align: center;">68.21</td>
<td style="text-align: center;">74.15</td>
<td style="text-align: center;">65.12</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-33B</td>
<td style="text-align: center;">75.18</td>
<td style="text-align: center;">69.30</td>
<td style="text-align: center;">74.93</td>
<td style="text-align: center;">69.73</td>
</tr>
</tbody>
</table>
<p>templates. These methods could be regarded as zero-shot methods because they are not fine-tuned by the JudgeLM dataset. On JudgeLM val set, the vanilla Vicuna-13B fails $77 \%$ of questions. Specifically, the vanilla Vicuna-13B can not even output a pair of scores in judgments in the failed cases. But the finetuned version, i.e., JudgeLM, would not fail any questions in the JudgeLM val set. Our JudgeLMs are fine-tuned with proposed methods, i.e., swap augmentation, reference support, and reference drop. So, they can handle situations with or without references simultaneously. It can be observed that our JudgeLM-7B outperforms PandaLM-7B, Auto-J, and InstructScore in all metrics, and even surpasses GPT-3.5. Furthermore, the proposed JudgeLM-33B exhibits the most powerful judge ability.
Comparison on Other Human Evaluation Benchmarks. We further evaluate our JudgeLM on other Human evaluation benchmarks, i.e., PandaLM test set and human-annotated MM-Vet. PandaLM's train and val sets are annotated by GPT-3.5 and humans, respectively. Following the manner of the PandaLM val set, we present the zero-shot results of JudgeLM in Table 2. It can be observed that the JudgeLM-7B outperforms GPT-3.5 and PandaLM-7B. When compared with GPT-4, JudgeLM-7B has lower accuracy and higher Precision, Recall, and F1-score than GPT4. Furthermore, JudgeLM-33B achieves higher results than GPT-4, which demonstrates that fine-</p>
<p>Table 3: Efficiency comparison for our JudgeLM and PandaLM on our val set. We use a machine with 8 Nvidia-A100 GPUs with 40G memory to evaluate their efficiency.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">model size</th>
<th style="text-align: center;">GPUs per model</th>
<th style="text-align: center;">parallel judge?</th>
<th style="text-align: center;">generate reason?</th>
<th style="text-align: center;">total time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PandaLM</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">6 hrs 40 mins</td>
</tr>
<tr>
<td style="text-align: left;">Ours.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">6 hrs 40 mins</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">24 mins</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">50 mins</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">3 mins</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">5 mins</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM</td>
<td style="text-align: center;">33B</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">15 mins</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance analysis for the scaling JudgeLM on our val set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Judge Size</th>
<th style="text-align: left;">Data Scale</th>
<th style="text-align: center;">Agreement $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Consistency $\uparrow$ <br> (w/ swap.)</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 1st</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 2nd</th>
<th style="text-align: center;">Delta bias $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">3.5 k</td>
<td style="text-align: center;">75.87</td>
<td style="text-align: center;">73.45</td>
<td style="text-align: center;">19.83</td>
<td style="text-align: center;">6.72</td>
<td style="text-align: center;">13.11</td>
</tr>
<tr>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">10 k</td>
<td style="text-align: center;">78.89</td>
<td style="text-align: center;">78.25</td>
<td style="text-align: center;">17.30</td>
<td style="text-align: center;">4.45</td>
<td style="text-align: center;">12.85</td>
</tr>
<tr>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">30 k</td>
<td style="text-align: center;">81.43</td>
<td style="text-align: center;">80.89</td>
<td style="text-align: center;">14.54</td>
<td style="text-align: center;">4.57</td>
<td style="text-align: center;">9.97</td>
</tr>
<tr>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">100 k</td>
<td style="text-align: center;">83.71</td>
<td style="text-align: center;">82.62</td>
<td style="text-align: center;">12.31</td>
<td style="text-align: center;">5.07</td>
<td style="text-align: center;">7.24</td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: left;">3.5 k</td>
<td style="text-align: center;">80.61</td>
<td style="text-align: center;">78.91</td>
<td style="text-align: center;">14.68</td>
<td style="text-align: center;">6.41</td>
<td style="text-align: center;">8.27</td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: left;">10 k</td>
<td style="text-align: center;">83.19</td>
<td style="text-align: center;">81.90</td>
<td style="text-align: center;">13.42</td>
<td style="text-align: center;">4.68</td>
<td style="text-align: center;">8.74</td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: left;">30 k</td>
<td style="text-align: center;">84.39</td>
<td style="text-align: center;">82.99</td>
<td style="text-align: center;">11.96</td>
<td style="text-align: center;">5.05</td>
<td style="text-align: center;">6.91</td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: left;">100 k</td>
<td style="text-align: center;">85.87</td>
<td style="text-align: center;">83.01</td>
<td style="text-align: center;">11.53</td>
<td style="text-align: center;">5.46</td>
<td style="text-align: center;">6.07</td>
</tr>
<tr>
<td style="text-align: left;">33B</td>
<td style="text-align: left;">3.5 k</td>
<td style="text-align: center;">85.38</td>
<td style="text-align: center;">85.16</td>
<td style="text-align: center;">9.34</td>
<td style="text-align: center;">5.50</td>
<td style="text-align: center;">3.84</td>
</tr>
<tr>
<td style="text-align: left;">33B</td>
<td style="text-align: left;">10 k</td>
<td style="text-align: center;">87.49</td>
<td style="text-align: center;">86.40</td>
<td style="text-align: center;">8.32</td>
<td style="text-align: center;">5.28</td>
<td style="text-align: center;">3.04</td>
</tr>
<tr>
<td style="text-align: left;">33B</td>
<td style="text-align: left;">30 k</td>
<td style="text-align: center;">88.84</td>
<td style="text-align: center;">87.34</td>
<td style="text-align: center;">7.57</td>
<td style="text-align: center;">5.09</td>
<td style="text-align: center;">2.48</td>
</tr>
<tr>
<td style="text-align: left;">33B</td>
<td style="text-align: left;">100 k</td>
<td style="text-align: center;">90.06</td>
<td style="text-align: center;">87.93</td>
<td style="text-align: center;">6.85</td>
<td style="text-align: center;">5.22</td>
<td style="text-align: center;">1.63</td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation study for the swap augmentation on our val set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Agreement $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Consistency $\uparrow$ <br> (w/ swap.)</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 1st</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 2nd</th>
<th style="text-align: center;">Delta Bias $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">baseline</td>
<td style="text-align: center;">75.87</td>
<td style="text-align: center;">73.45</td>
<td style="text-align: center;">19.83</td>
<td style="text-align: center;">6.72</td>
<td style="text-align: center;">13.11</td>
</tr>
<tr>
<td style="text-align: left;">+ swap aug.</td>
<td style="text-align: center;">76.51</td>
<td style="text-align: center;">78.89</td>
<td style="text-align: center;">15.34</td>
<td style="text-align: center;">5.77</td>
<td style="text-align: center;">9.57</td>
</tr>
</tbody>
</table>
<p>tuned JudgeLM can outperform its teacher in this specific task. Besides, we also propose a humanannotated multimodal judging benchmark to evaluate our JudgeLM as shown in Table 13.</p>
<p>Efficiency comparison. To further compare the efficiency between our JudgeLM and PandaLM, we conduct experiments on our val set to display the time cost using the same machine with 8 NVIDIA-A100 (40G) GPUs. As shown in Table 3, we display the methods and model sizes in the first and second columns. The third column shows the needed GPUs for each judge model. The models with 7B or 13B parameters run on 1 A100 GPU with 40G memory while the 33B-parameter needs 2 GPUs. The fourth column shows whether the methods can judge answers in parallel. The fifth column indicates whether judge reasons are generated at runtime. The sixth column presents the total time cost. We use PandaLM-7B and JudgeLM-7B as efficiency baselines, which do not use parallel judging and generate detailed reasons for all questions. Thanks to the modeling of JudgeLM, i.e., "grading, judging, and reasoning", JudgeLM can skip the reasoning phase and only requires 24 minutes, which is $16.65 \times$ faster than baselines. When we enable the engineering optimization of parallel judging, the JudgeLM can make full use of the 8 GPUs and cost only 50 minutes, which is $8 \times$ faster than the baseline running on a single GPU. When we enable parallel judging and skip the reasoning phase for JudgeLM, the JudgeLM-7B only consumes 3 minutes to judge 5000 response pairs, which is $133.3 \times$ faster than baselines. The largest judge model, JudgeLM-33B, can also complete the validation within 15 minutes. JudgeLM's high efficiency can significantly reduce the time spent on evaluating LLMs, allowing researchers and developers to boost the pace of advancements.</p>
<p>Table 6: Ablation study for the reference support and reference drop on our val set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">$f t$ <br> w/ ref?</th>
<th style="text-align: center;">val <br> w/ ref?</th>
<th style="text-align: center;">Agreement $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Consistency $\uparrow$ <br> (w/ swap.)</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 1st</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 2nd</th>
<th style="text-align: center;">Delta <br> Bias $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">matching format.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">baseline</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">75.87</td>
<td style="text-align: center;">73.45</td>
<td style="text-align: center;">19.83</td>
<td style="text-align: center;">6.72</td>
<td style="text-align: center;">13.11</td>
</tr>
<tr>
<td style="text-align: left;">baseline</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">80.15</td>
<td style="text-align: center;">81.23</td>
<td style="text-align: center;">11.55</td>
<td style="text-align: center;">7.22</td>
<td style="text-align: center;">4.33</td>
</tr>
<tr>
<td style="text-align: left;">mismatched format.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">baseline</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">73.09</td>
<td style="text-align: center;">67.75</td>
<td style="text-align: center;">29.44</td>
<td style="text-align: center;">2.81</td>
<td style="text-align: center;">26.63</td>
</tr>
<tr>
<td style="text-align: left;">baseline</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">75.69</td>
<td style="text-align: center;">73.40</td>
<td style="text-align: center;">20.89</td>
<td style="text-align: center;">5.71</td>
<td style="text-align: center;">15.18</td>
</tr>
<tr>
<td style="text-align: left;">w/ ref. drop.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">baseline</td>
<td style="text-align: center;">ref. drop</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">76.86</td>
<td style="text-align: center;">77.13</td>
<td style="text-align: center;">17.30</td>
<td style="text-align: center;">5.57</td>
<td style="text-align: center;">11.73</td>
</tr>
<tr>
<td style="text-align: left;">baseline</td>
<td style="text-align: center;">ref. drop</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">80.35</td>
<td style="text-align: center;">81.24</td>
<td style="text-align: center;">11.48</td>
<td style="text-align: center;">7.28</td>
<td style="text-align: center;">4.20</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance of JudgeLM-7B with explanation-first (CoT) or score-first (Ours) on JudgeLM val set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Agreement $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Consistency $\uparrow$ <br> (w/ swap.)</th>
<th style="text-align: center;">$\begin{gathered} \text { Bias } \downarrow \ \text { toward } 1 \text { st } \end{gathered}$</th>
<th style="text-align: center;">$\begin{gathered} \text { Bias } \downarrow \ \text { toward } 2 \text { nd } \end{gathered}$</th>
<th style="text-align: center;">Delta Bias $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">score-first (Our)</td>
<td style="text-align: center;">75.87</td>
<td style="text-align: center;">73.45</td>
<td style="text-align: center;">19.83</td>
<td style="text-align: center;">6.72</td>
<td style="text-align: center;">13.11</td>
</tr>
<tr>
<td style="text-align: center;">explanation-first (CoT)</td>
<td style="text-align: center;">75.54</td>
<td style="text-align: center;">74.39</td>
<td style="text-align: center;">15.05</td>
<td style="text-align: center;">10.56</td>
<td style="text-align: center;">4.50</td>
</tr>
</tbody>
</table>
<h1>6.2 Scaling Analysis of JudgeLM</h1>
<p>In this section, we analyze the scaling ability of the plain JudgeLM (without the proposed methods) on our val set without reference as illustrated in Table 4. As we increase the model size and data scale, we can observe the metrics increase. It demonstrates that the proposed JudgeLM is scalable and can reach up to $90.06 \%$ agreement and $87.93 \%$ consistency with 33B-parameter and 100 K finetuning data.</p>
<h3>6.3 Ablation Study</h3>
<p>In this section, we present the ablation studies of the proposed methods. For all ablation studies, we use JudgeLM-7B as the base model and 3.5 K data for fine-tuning. Based on this baseline, we analyze the improvements brought by swap augmentation, reference support, and reference drop.
Improvements of Swap Augmentation. As shown in Table 5, swap augmentation can improve the baseline model comprehensively. It improves consistency by $5.44 \%$, which demonstrates that swap augmentation can reduce the influence of position bias and push the judge to pay more attention to the contents of answers.</p>
<p>Improvements of Reference Support. As shown in the rows with the matching format of Table 6, JudgeLM fine-tuned with reference support exhibits superior performance on every metric. It demonstrates that the introduction of reference answers induces the judge to rely on external knowledge and addresses the limitation of pre-trained knowledge.
Improvements of Reference Drop. As shown in Table 6, baselines can not reach satisfactory performance when facing mismatched formats. With the help of the reference drop, the JudgeLM can handle both the format with or without reference and achieve higher agreement and consistency. It demonstrates that reference drop can address the format bias and avoid the JudgeLM overfitting to a single format.
Ablation of Judging Form We further evaluate the performance of JudgeLM-7B with explanationfirst (Chain of Thought, CoT (Wei et al., 2022)) or score-first (Ours) in Table 7. JudgeLM with CoT performs similar agreement with our score-first baseline but with higher consistency, which means that explanation-first form, i.e., CoT, can alleviate the position bias of fine-tuned judges, but not bring significant agreement improvement. As a result, we choose the score-first method for JudgeLM, which has slightly less consistency but more flexible usage.</p>
<p>Table 8: Comparison between GPT-4 teacher and JudgeLM-33B on JudgeLM val set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Agreement $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Consistency $\uparrow$ <br> (w/ swap.)</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 1st</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 2nd</th>
<th style="text-align: center;">Delta Bias $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.82</td>
<td style="text-align: center;">6.10</td>
<td style="text-align: center;">8.10</td>
<td style="text-align: center;">2.00</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-33B</td>
<td style="text-align: center;">89.03</td>
<td style="text-align: center;">91.36</td>
<td style="text-align: center;">5.55</td>
<td style="text-align: center;">3.09</td>
<td style="text-align: center;">2.46</td>
</tr>
</tbody>
</table>
<h1>6.4 ADDITIONAL EXPERIMENT</h1>
<p>Comparison with GPT-4 Teacher As shown in Table 8, we further list the metrics except for the agreement with GPT-4 itself. JudgeLM-33B achieves higher consistency than GPT-4, which demonstrates that fine-tuning judges like JudgeLM-33B can achieve higher consistency through the proposed techniques.</p>
<p>The learning paradigm of JudgeLM is similar to knowledge distillation, as JudgeLM learns from the expert judgments provided by GPT-4. In the knowledge distillation domain (Gou et al., 2021), the student model (JudgeLM) mimics the teacher model (GPT-4) to achieve competitive or even superior performance. Additionally, our JudgeLM employs three key methods and utilizes largescale training data specifically for the judging task to enhance its agreement and consistency. As mentioned in "Comparison on Other Human Evaluation Benchmarks" under Section 6.1, our experiments demonstrate that a fine-tuned specialist judge model can surpass its generalist teacher on some judging benchmarks, i.e., JudgeLM val set and PandaLM test set.</p>
<h3>6.5 DETAILS OF DATASET</h3>
<p>For the proposed dataset and benchmark, we also provide the explanation of usage scope, details of metric calculations, dataset quality, question category \&amp; distribution (among 19 categories), and comparison with UltraFeedback (Cui et al., 2023) in Sec. A.1. We hope the dataset can help researchers build more robust evaluation tools in the future.</p>
<h3>6.6 Generalization Ability of JudgeLM</h3>
<p>Not only judging answer pairs, but our JudgeLM can also generalize to various judging tasks (including math problems and code generation), unseen judging benchmarks (human-annotated benchmark, multimodal judging benchmark, retrieval-format benchmark, multiple-format benchmark, toxic chat benchmark, reward model benchmark), and other various judging extensions (grading single answer, multi-turn chat). We leave the detailed analysis in Sec. A. 3 of the appendix.</p>
<h3>6.7 MORE DISCUSSION</h3>
<p>Due to the limitation of pages, we leave more discussion in Sec. A. 4 of the appendix.</p>
<h2>7 CONCLUSION</h2>
<p>In this paper, we first introduce a high-quality, large-scale dataset for LLM evaluation, that provides a robust foundation for future research. Next, the proposed JudgeLM as scalable judges for evaluating LLMs in open-ended tasks efficiently, achieving state-of-the-art judge performance on two benchmarks. Then, we analyze two key biases and introduce a new format bias in fine-tuning LLMs as judges, and address them with the proposed techniques. We hope our work can motivate more studies to explore the judge models for LLMs in open-ended tasks and build more powerful LLMs with guidance from judge models.</p>
<p>Limitations. Although the proposed JudgeLM achieves encouraging performance and efficiency, the cost of the judge dataset limits further scaling up in the judge dataset. Currently, we spend about 4000 dollars to provide 100 K high-quality GPT-4-generated judge data to the public. We expect to further improve the performance of judge models with the help of synthetic judge data.</p>
<h1>ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING</h1>
<p>This work was partially supported by the National Natural Science Foundation of China (NSFC) under Grant No. 62276108.</p>
<h2>REFERENCES</h2>
<p>Cabrera Alex and Neubig Graham. Zeno chatbot report, 2023. URL https: //github.com/zeno-ml/zeno-build/tree/main/examples/chatbot/ report#zeno-chatbot-report. 3</p>
<p>Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https://github.com/nomic-ai/gpt4all, 2023. 4, 15</p>
<p>Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 3</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 15, 19,20</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. PMLR, 2023. 1</p>
<p>Niels J Blunch. Position bias in multiple-choice questions. Journal of Marketing Research, 21(2): 216-220, 1984. 5</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 3</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023. 1, 3, 4, 5</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. 3</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 1, 3</p>
<p>Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world's first truly open instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/ 12/dolly-first-open-commercially-viable-instruction-tuned-llm. 4, 15</p>
<p>Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. 10, 15, 19, 22</p>
<p>Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. International Journal of Computer Vision, 129(6):1789-1819, 2021. 10</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 1</p>
<p>Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023. 16</p>
<p>Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023. 19, 20</p>
<p>Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022. 3</p>
<p>Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. Tigerscore: Towards building explainable metric for all text generation tasks. Transactions on Machine Learning Research, 2023. 20, 22</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations, 2023. 19, 22</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 17</p>
<p>Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang. Look at the first sentence: Position bias in question answering. In 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, pp. 1109-1121. Association for Computational Linguistics (ACL), 2020. 5</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. 19, 22</p>
<p>Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, and Minjoon Seo. Prometheusvision: Vision-language model as a judge for fine-grained evaluation. arXiv preprint arXiv:2401.06591, 2024. 18</p>
<p>Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment. CoRR, abs/2310.05470, 2023a. URL https://doi.org/ 10.48550/arXiv. 2310.05470. 3, 6, 22</p>
<p>Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. Split and merge: Aligning position biases in large language model based evaluators. arXiv preprint arXiv:2310.01432, 2023b. 5</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. 1</p>
<p>Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004. 1</p>
<p>Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation. arXiv preprint arXiv:2310.17389, 2023. 18</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 18</p>
<p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 17</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. 16</p>
<p>Jinjie Ni, Yifan Song, Deepanway Ghosal, Bo Li, David Junhao Zhang, Xiang Yue, Fuzhao Xue, Zian Zheng, Kaichen Zhang, Mahir Shah, et al. Mixeval-x: Any-to-any evaluations from realworld data mixtures. arXiv preprint arXiv:2410.13754, 2024a. 23</p>
<p>Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and Yang You. Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures. arXiv preprint arXiv:2406.06565, 2024b. 23</p>
<p>OpenAI. Chatgpt, 2022. URL https://openai.com/blog/chatgpt/. 3
OpenAI. Gpt-4 technical report, 2023. 3
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022.3,5$</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002. 1</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. 4, 15</p>
<p>Maja Popović. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 392-395, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https: //aclanthology.org/W15-3049. 3</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020. 3</p>
<p>Priya Raghubir and Ana Valenzuela. Center-of-inattention: Position biases in decision-making. Organizational Behavior and Human Decision Processes, 99(1):66-80, 2006. 5</p>
<p>Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021. 17</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. Bleurt: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881-7892, 2020. 1</p>
<p>Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. 3</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. 1, 3, 4, 5</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. 3, 19, 21</p>
<p>Xuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork. Position bias estimation for unbiased learning to rank in personal search. In Proceedings of the eleventh ACM international conference on web search and data mining, pp. 610-618, 2018. 5</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. CoRR, abs/2306.05087, 2023. URL https://doi.org/10.48550/arXiv.2306.05087. 2, 3, 5, 6, 7, 19, 22</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022. 9</p>
<p>Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, and Lei Li. Instructscore: Explainable text generation evaluation with finegrained feedback. arXiv preprint arXiv:2305.14282, 2023. 6, 22</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023a. 16</p>
<p>Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023b. 18</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263-27277, 2021. 1</p>
<p>Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. 3</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 1, 3</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. 1, 3</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. 1, 2, 3, 5, 6, 22, 23</p>
<h1>A APPENDIX / SUPPLEMENTAL MATERIAL</h1>
<h2>A. 1 MORE ABOUT DATASET</h2>
<p>Dataset Usage Scope We emphasize that the JudgeLM dataset is intended only for academic research and any commercial use is prohibited. Because the OpenAI's terms prohibit developing models that compete with OpenAI, the instruction-tuning datasets generated by the OpenAI's API, i.e., Alpaca, PandaLM, etc., all follow this rule.</p>
<p>Details of Metric Calculations For objective metrics, we use the judgments annotated by humans or GPT-4 as ground truth labels, and the judgments generated by judge models as predicted labels. We use $T P, F P, T N$, and $F N$ to represent the true positive, false positive, true negative, and false negative, respectively. The calculation of agreement, precision, recall, and F1-score are as follows:</p>
<p>$$
\begin{aligned}
\text { Agreement } &amp; =(T P+T N) /(T P+F P+T N+F N) \
\text { Precision } &amp; =T P /(T P+F P) \
\text { Recall } &amp; =T P /(T P+F N) \
\text { F1-score } &amp; =(2 * T P) /(2 * T P+F P+F N)
\end{aligned}
$$</p>
<p>For reliability metrics, we compare the results before and after swapping the order of the two answers (A and B), i.e., from "the first answer is A , the second answer is B " to "the first answer is B , the</p>
<p>second answer is A". When the judging results change, we mark it as a biased sample. Samples with a bias toward the first position consist of three situations, "from Answer A wins to tie", "from Answer A wins to Answer B wins", and "from tie to Answer B wins". Similarly, samples with a bias toward the second position also consist of three situations, "from Answer B wins to tie", "from Answer B wins to Answer A wins", and "from tie to Answer A wins". The calculation of metrics of "bias toward 1st", "bias toward 2nd", "delta bias" are defined as follows:</p>
<p>$$
\begin{aligned}
\text { bias toward } 1 \text { st } &amp; =\frac{\text { Number of samples with a bias toward the first position }}{\text { Numbers of total samples }} \
\text { bias toward } 2 \text { nd } &amp; =\frac{\text { Number of samples with a bias toward the second position }}{\text { Numbers of total samples }} \
\text { delta bias } &amp; =|\text { bias toward } 1 \text { st }- \text { bias toward } 2 \text { nd })| .
\end{aligned}
$$</p>
<p>Dataset Quality To ensure the high quality of the proposed dataset, we filter the low-quality data samples in each step. For data samples (including seed tasks and references) in four public datasets, i.e., Alpaca-GPT4 (Peng et al., 2023), Dolly-15K (Conover et al., 2023), GPT4All-LAION (Anand et al., 2023), and ShareGPT, we first remove data samples containing obviously incorrect, irrelevant, or harmful reference answers through automated filtering scripts. Next, we randomly sample 105K samples from the filtered set and extract answers from 11 LLMs. Then, we input the tasks, randomly sampled answer pairs, and optionally reference answers to the GPT-4 teacher for judgment. Finally, the authors of this work are involved in a multi-step validation process to ensure the quality, accuracy, and reliability of the judge samples. This process includes an initial annotation step where GPT-4 provides preliminary judgments, followed by independent human re-annotation where authors provide simple judgments ("Answer 1 wins," "Answer 2 wins", or "Tie") without exposure to GPT-4's annotations. The final step involves cross-validation and refinement, where human judgments are compared with GPT-4 annotations to thoroughly verify the judge results, scores, and reasoning quality. Please note that incorporating high-quality answers from closed-sourced models e.g., Qwen (Bai et al., 2023) and Claude, could enhance the diversity of the dataset. We leave it as a future work.</p>
<p>Question Category \&amp; Distribution of Validation Set We count the distribution of questions in the JudgeLM val set as shown in Table 9. Please note that the question categories included in the JudgeLM val set and train set are the same, but none of the data samples are identical.</p>
<p>Comparison with UltraFeedback Furthermore, we compare the JudgeLM dataset with UltraFeedback (Cui et al., 2023), which is an excellent dataset serving as a solid foundation for feedbacklearning research. JudgeLM has nearly half more seeds than UltraFeedback, and an additional validation set containing 5 K seeds. JudgeLM and UltraFeedback both provide scalar and text feedback, GPT-4 annotation, and fine-grained consideration. However, the JudgeLM dataset is further checked and re-annotated by humans, which provides double-checking on the quality of feedback. Moreover, JudgeLM supports judging with references, which can make up for the lack of pre-trained knowledge or insert specific judge preferences. Finally, JudgeLM clearly splits the seeds into 19 categories providing intuitive ability estimation for judges.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">train <br> Seeds</th>
<th style="text-align: center;">val <br> Seeds</th>
<th style="text-align: center;">Feedback <br> Format</th>
<th style="text-align: center;">Annotator</th>
<th style="text-align: center;">fine <br> grained?</th>
<th style="text-align: center;">with <br> Ref.?</th>
<th style="text-align: center;">Seeds <br> Categories</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UltraFeedback</td>
<td style="text-align: center;">64 K</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Scalar \&amp; Text</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM</td>
<td style="text-align: center;">100 K</td>
<td style="text-align: center;">5 K</td>
<td style="text-align: center;">Scalar \&amp; Text</td>
<td style="text-align: center;">GPT-4 \&amp; Human</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">19</td>
</tr>
</tbody>
</table>
<p>Comparison with PandaLM Test Set Furthermore, we compare the JudgeLM dataset with the PandaLM test set. An analysis of task distributions in Table 10 shows significant differences between the PandaLM test set and the JudgeLM benchmark. For example, business, fact-QA, summarizing, linguistics, emotion, entity-processing, explain, retrieval, document, and chat are well-represented in PandaLM but absent in JudgeLM, while writing and roleplay show a significant delta percentage (over 4\%). This confirms that the PandaLM test set includes 49\% unseen task samples that are out of distribution for JudgeLM.</p>
<p>Table 9: Distribution of question categories in JudgeLM val set</p>
<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: right;">count</th>
<th style="text-align: right;">percentage</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">count</th>
<th style="text-align: right;">percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">culture</td>
<td style="text-align: right;">233</td>
<td style="text-align: right;">$4.66 \%$</td>
<td style="text-align: right;">planning</td>
<td style="text-align: right;">309</td>
<td style="text-align: right;">$6.18 \%$</td>
</tr>
<tr>
<td style="text-align: right;">recommendation</td>
<td style="text-align: right;">482</td>
<td style="text-align: right;">$9.64 \%$</td>
<td style="text-align: right;">roleplay</td>
<td style="text-align: right;">77</td>
<td style="text-align: right;">$1.54 \%$</td>
</tr>
<tr>
<td style="text-align: right;">finance</td>
<td style="text-align: right;">142</td>
<td style="text-align: right;">$2.84 \%$</td>
<td style="text-align: right;">coding</td>
<td style="text-align: right;">201</td>
<td style="text-align: right;">$4.02 \%$</td>
</tr>
<tr>
<td style="text-align: right;">science</td>
<td style="text-align: right;">393</td>
<td style="text-align: right;">$7.86 \%$</td>
<td style="text-align: right;">health</td>
<td style="text-align: right;">278</td>
<td style="text-align: right;">$5.56 \%$</td>
</tr>
<tr>
<td style="text-align: right;">technique</td>
<td style="text-align: right;">42</td>
<td style="text-align: right;">$0.84 \%$</td>
<td style="text-align: right;">writing</td>
<td style="text-align: right;">625</td>
<td style="text-align: right;">$12.50 \%$</td>
</tr>
<tr>
<td style="text-align: right;">common-sense</td>
<td style="text-align: right;">373</td>
<td style="text-align: right;">$7.46 \%$</td>
<td style="text-align: right;">hardware</td>
<td style="text-align: right;">130</td>
<td style="text-align: right;">$2.60 \%$</td>
</tr>
<tr>
<td style="text-align: right;">art</td>
<td style="text-align: right;">335</td>
<td style="text-align: right;">$6.70 \%$</td>
<td style="text-align: right;">history</td>
<td style="text-align: right;">243</td>
<td style="text-align: right;">$4.86 \%$</td>
</tr>
<tr>
<td style="text-align: right;">math</td>
<td style="text-align: right;">250</td>
<td style="text-align: right;">$5.00 \%$</td>
<td style="text-align: right;">geography</td>
<td style="text-align: right;">199</td>
<td style="text-align: right;">$3.98 \%$</td>
</tr>
<tr>
<td style="text-align: right;">private-matter</td>
<td style="text-align: right;">421</td>
<td style="text-align: right;">$8.42 \%$</td>
<td style="text-align: right;">others</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">$1.26 \%$</td>
</tr>
<tr>
<td style="text-align: right;">law</td>
<td style="text-align: right;">204</td>
<td style="text-align: right;">$4.08 \%$</td>
<td style="text-align: right;">total</td>
<td style="text-align: right;">5000</td>
<td style="text-align: right;">$100.00 \%$</td>
</tr>
</tbody>
</table>
<p>Table 10: Distribution of question categories in PandaLM test set. The $\Delta$ represents the percentage difference compared to the JudgeLM benchmark. We bolded categories that appear in the PandaLM test set but don't exist in the JudgeLM benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: right;">count</th>
<th style="text-align: right;">percentage</th>
<th style="text-align: right;">$\Delta$</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">count</th>
<th style="text-align: right;">percentage</th>
<th style="text-align: right;">$\Delta$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">business</td>
<td style="text-align: right;">87</td>
<td style="text-align: right;">$8.71 \%$</td>
<td style="text-align: right;">$8.71 \%$</td>
<td style="text-align: right;">writing</td>
<td style="text-align: right;">81</td>
<td style="text-align: right;">$8.11 \%$</td>
<td style="text-align: right;">$-4.39 \%$</td>
</tr>
<tr>
<td style="text-align: right;">fact-QA</td>
<td style="text-align: right;">70</td>
<td style="text-align: right;">$7.01 \%$</td>
<td style="text-align: right;">$7.01 \%$</td>
<td style="text-align: right;">planning</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">$5.71 \%$</td>
<td style="text-align: right;">$-0.47 \%$</td>
</tr>
<tr>
<td style="text-align: right;">summarizing</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">$6.41 \%$</td>
<td style="text-align: right;">$6.41 \%$</td>
<td style="text-align: right;">roleplay</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">$5.71 \%$</td>
<td style="text-align: right;">$4.17 \%$</td>
</tr>
<tr>
<td style="text-align: right;">linguistics</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">$4.50 \%$</td>
<td style="text-align: right;">$4.50 \%$</td>
<td style="text-align: right;">coding</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">$5.41 \%$</td>
<td style="text-align: right;">$1.39 \%$</td>
</tr>
<tr>
<td style="text-align: right;">emotion</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">$4.50 \%$</td>
<td style="text-align: right;">$4.50 \%$</td>
<td style="text-align: right;">art</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">$4.40 \%$</td>
<td style="text-align: right;">$-2.30 \%$</td>
</tr>
<tr>
<td style="text-align: right;">entity-processing</td>
<td style="text-align: right;">42</td>
<td style="text-align: right;">$4.20 \%$</td>
<td style="text-align: right;">$4.20 \%$</td>
<td style="text-align: right;">finance</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">$4.00 \%$</td>
<td style="text-align: right;">$1.16 \%$</td>
</tr>
<tr>
<td style="text-align: right;">explain</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">$4.10 \%$</td>
<td style="text-align: right;">$4.10 \%$</td>
<td style="text-align: right;">culture</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">$3.80 \%$</td>
<td style="text-align: right;">$-0.86 \%$</td>
</tr>
<tr>
<td style="text-align: right;">retrieval</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">$4.00 \%$</td>
<td style="text-align: right;">$4.00 \%$</td>
<td style="text-align: right;">math</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">$3.70 \%$</td>
<td style="text-align: right;">$-1.30 \%$</td>
</tr>
<tr>
<td style="text-align: right;">document</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$3.00 \%$</td>
<td style="text-align: right;">$3.00 \%$</td>
<td style="text-align: right;">geography</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">$1.20 \%$</td>
<td style="text-align: right;">$-2.78 \%$</td>
</tr>
<tr>
<td style="text-align: right;">chat</td>
<td style="text-align: right;">26</td>
<td style="text-align: right;">$2.60 \%$</td>
<td style="text-align: right;">$2.60 \%$</td>
<td style="text-align: right;">others</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">$0.50 \%$</td>
<td style="text-align: right;">$-0.76 \%$</td>
</tr>
<tr>
<td style="text-align: right;">recommendation</td>
<td style="text-align: right;">84</td>
<td style="text-align: right;">$8.41 \%$</td>
<td style="text-align: right;">$-1.23 \%$</td>
<td style="text-align: right;">total</td>
<td style="text-align: right;">999</td>
<td style="text-align: right;">$100.00 \%$</td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<h1>A. 2 Fine-tuning Setting</h1>
<p>We list the hyper-parameters we used, as shown in Table 11.</p>
<h2>A. 3 Generalization Ability of JudgeLM</h2>
<p>To validate the generalization ability of JudgeLM, we test JudgeLM on various judging tasks (including math problems and code generation), unseen judging benchmarks (human-annotated benchmark, multimodal judging benchmark, retrieval-format benchmark, multiple-format benchmark), and other various judging extensions (grading single answer, multi-turn chat).
Generalize to Various Judging Tasks. To further validate the judging performance on questions with specific categories, we present the judging results of JudgeLM-33B on these questions, i.e., coding, common-sense, math, roleplay, and writing. Table 12 shows that JudgeLM can handle the judging tasks of various categories. We also find that the judging performance of math questions is slightly lower than coding and common-sense questions. However we think it is a common problem of large language models (Imani et al., 2023), and future advancement on base models (Yu et al., 2023a; Luo et al., 2023) would alleviate this problem.
Generalize to Multimodal Judging Benchmark. Traditional multimodal evaluation needs prediction to match the ground truth exactly. For some open-ended questions, a human-like evaluator is</p>
<p>Table 11: JudgeLM fine-tuning setting.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">config</th>
<th style="text-align: center;">JudgeLM / -7B / -13B / -33B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">base model</td>
<td style="text-align: center;">Vicuna / -7B / -13B / -33B</td>
</tr>
<tr>
<td style="text-align: left;">model max length</td>
<td style="text-align: center;">2048</td>
</tr>
<tr>
<td style="text-align: left;">fine-tuning data source</td>
<td style="text-align: center;">JudgeLM-100K</td>
</tr>
<tr>
<td style="text-align: left;">learning rate</td>
<td style="text-align: center;">2e-5</td>
</tr>
<tr>
<td style="text-align: left;">learning rate schedule</td>
<td style="text-align: center;">cosine decay</td>
</tr>
<tr>
<td style="text-align: left;">optimizer</td>
<td style="text-align: center;">AdamW (Kingma \&amp; Ba, 2014; Loshchilov \&amp; Hutter, 2019)</td>
</tr>
<tr>
<td style="text-align: left;">optimizer hyper-parameters</td>
<td style="text-align: center;">$\beta_{1}, \beta_{2}, \epsilon=0.9,0.999,1 \mathrm{e}-8$</td>
</tr>
<tr>
<td style="text-align: left;">weight decay</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">GPU nums</td>
<td style="text-align: center;">$8 / 8 / 16$</td>
</tr>
<tr>
<td style="text-align: left;">batch size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">training epochs</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">warmup ratio</td>
<td style="text-align: center;">0.003</td>
</tr>
<tr>
<td style="text-align: left;">numerical precision</td>
<td style="text-align: center;">bf16, tf32</td>
</tr>
<tr>
<td style="text-align: left;">ZeRO optimizer (Ramesh et al., 2021)</td>
<td style="text-align: center;">stage 3</td>
</tr>
<tr>
<td style="text-align: left;">gradient checkpointing</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 and GPT-4 version</td>
<td style="text-align: center;">2023-03-15-preview</td>
</tr>
</tbody>
</table>
<p>Table 12: Performance of JudgeLM-33B with specific categories on JudgeLM val set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Agreement $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Consistency $\uparrow$ <br> (w/ swap.)</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 1st</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 2nd</th>
<th style="text-align: center;">Delta Bias $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">coding</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">val w/o ref</td>
<td style="text-align: center;">88.08</td>
<td style="text-align: center;">88.60</td>
<td style="text-align: center;">6.22</td>
<td style="text-align: center;">5.18</td>
<td style="text-align: center;">1.04</td>
</tr>
<tr>
<td style="text-align: left;">val w/ ref</td>
<td style="text-align: center;">88.83</td>
<td style="text-align: center;">91.37</td>
<td style="text-align: center;">3.55</td>
<td style="text-align: center;">5.08</td>
<td style="text-align: center;">1.53</td>
</tr>
<tr>
<td style="text-align: left;">common-sense</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">val w/o ref</td>
<td style="text-align: center;">88.41</td>
<td style="text-align: center;">90.43</td>
<td style="text-align: center;">7.25</td>
<td style="text-align: center;">2.32</td>
<td style="text-align: center;">4.93</td>
</tr>
<tr>
<td style="text-align: left;">val w/ ref</td>
<td style="text-align: center;">90.37</td>
<td style="text-align: center;">92.35</td>
<td style="text-align: center;">4.53</td>
<td style="text-align: center;">3.12</td>
<td style="text-align: center;">1.41</td>
</tr>
<tr>
<td style="text-align: left;">math</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">val w/o ref</td>
<td style="text-align: center;">86.45</td>
<td style="text-align: center;">84.08</td>
<td style="text-align: center;">7.76</td>
<td style="text-align: center;">8.16</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: left;">val w/ ref</td>
<td style="text-align: center;">86.81</td>
<td style="text-align: center;">87.85</td>
<td style="text-align: center;">4.45</td>
<td style="text-align: center;">7.69</td>
<td style="text-align: center;">3.24</td>
</tr>
<tr>
<td style="text-align: left;">croleplay</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">val w/o ref</td>
<td style="text-align: center;">88.00</td>
<td style="text-align: center;">88.00</td>
<td style="text-align: center;">6.67</td>
<td style="text-align: center;">5.33</td>
<td style="text-align: center;">1.34</td>
</tr>
<tr>
<td style="text-align: left;">val w/ ref</td>
<td style="text-align: center;">88.72</td>
<td style="text-align: center;">89.12</td>
<td style="text-align: center;">5.01</td>
<td style="text-align: center;">5.87</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: left;">writting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">val w/o ref</td>
<td style="text-align: center;">87.33</td>
<td style="text-align: center;">92.36</td>
<td style="text-align: center;">3.25</td>
<td style="text-align: center;">4.39</td>
<td style="text-align: center;">1.14</td>
</tr>
<tr>
<td style="text-align: left;">val w/ ref</td>
<td style="text-align: center;">89.11</td>
<td style="text-align: center;">92.83</td>
<td style="text-align: center;">3.67</td>
<td style="text-align: center;">3.50</td>
<td style="text-align: center;">0.17</td>
</tr>
</tbody>
</table>
<p>needed to determine whether the prediction is close to the ground truth range. Modern multimodal, such as MM-Vet (Yu et al., 2023b) and Prometheus-Vision (Lee et al., 2024), which use GPT-4V, GPT-4 or GPT-3.5 as judges. The API-based judge takes the question text, ground-truth text, the model's prediction, and optional input image as input, and makes judgments based on them. Our JudgeLM also provides good practice for such a multimodal evaluation by a slightly modified template as shown in Fig. 7. Thanks to its capacity to judge open-ended answers, our JudgeLM can also perform well in judging multimodal models, as shown in Fig. 21.</p>
<p>We further conduct experiments to evaluate JudgeLM' ability to judge multimodal models when compared with close-sourced LLM, i.e., GPT-3.5 and GPT-4. We first use GPT-4, GPT-3.5, and JudgeLM to judge the LLaVA's output (Liu et al., 2023), respectively. Then, we collect judgments from human annotators, whose judgments include three situations: completely correct, semi-correct, and completely wrong. Last, we compute the metrics between the LLM judges' judgments and human judgments, as shown in Table 13. It can be observed that JudgeLM outperforms GPT-4 (0-shot) and GPT-3.5 (7-shot). Besides, JudgeLM achieves 2.5\% higher precision than GPT-4 (7shot). The encouraging results demonstrate the generalization ability of JudgeLM in dealing with multimodal judging. Furthermore, JudgeLM can use large multimodal models, e.g., LLaVA (Liu et al., 2023), as the backbone for better processing the multimodal judging. We leave it as a future work.</p>
<p>Table 13: JudgeLM zero-shot evaluation results on human-annotated MM-Vet benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Agreement $\uparrow$ <br> (w/ Human)</th>
<th style="text-align: center;">Precision $\uparrow$ <br> (w/ Human)</th>
<th style="text-align: center;">Recall $\uparrow$ <br> (w/ Human)</th>
<th style="text-align: center;">F1 $\uparrow$ <br> (w/ Human)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4 (7-shot)</td>
<td style="text-align: center;">95.58</td>
<td style="text-align: center;">88.63</td>
<td style="text-align: center;">87.79</td>
<td style="text-align: center;">88.04</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (0-shot)</td>
<td style="text-align: center;">86.70</td>
<td style="text-align: center;">79.75</td>
<td style="text-align: center;">86.41</td>
<td style="text-align: center;">81.81</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (7-shot)</td>
<td style="text-align: center;">83.03</td>
<td style="text-align: center;">76.14</td>
<td style="text-align: center;">74.84</td>
<td style="text-align: center;">73.62</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-33B (0-shot)</td>
<td style="text-align: center;">91.74</td>
<td style="text-align: center;">91.08</td>
<td style="text-align: center;">85.58</td>
<td style="text-align: center;">87.26</td>
</tr>
</tbody>
</table>
<p>Generalize to Out-of-distribution ToxicChat Benchmark. To further evaluate the generalization ability of the proposed JudgeLM, we selected an out-of-distribution benchmark, i.e., ToxicChat (Lin et al., 2023), for evaluation. Following the guidelines provided in the ToxicChat dataset, we conducted experiments on the latest test set (0124) of ToxicChat, comparing OpenAI Moderation, the GPT-4 teacher, and our proposed JudgeLM. OpenAI Moderation is an API trained on publicly available toxicity datasets, primarily sourced from social media. For both the GPT-4 teacher and JudgeLM, we used the same templates and thresholds. As shown in Table 14, our proposed JudgeLM achieves superior precision and comparable accuracy to the specialist model, i.e., OpenAI Moderation. These results demonstrate that JudgeLM can further generalize to out-of-distribution datasets such as ToxicChat.</p>
<p>Table 14: JudgeLM zero-shot evaluation results on toxic-chat test set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Accuracy $\uparrow$</th>
<th style="text-align: center;">Precision $\uparrow$</th>
<th style="text-align: center;">Recall $\uparrow$</th>
<th style="text-align: center;">F1 $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Specialist API-based Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">OpenAI Moderation</td>
<td style="text-align: center;">89.70</td>
<td style="text-align: center;">54.76</td>
<td style="text-align: center;">69.89</td>
<td style="text-align: center;">61.41</td>
</tr>
<tr>
<td style="text-align: left;">Generalist API-based Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">88.08</td>
<td style="text-align: center;">52.17</td>
<td style="text-align: center;">73.20</td>
<td style="text-align: center;">60.92</td>
</tr>
<tr>
<td style="text-align: left;">Open-sourced Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-33B</td>
<td style="text-align: center;">89.66</td>
<td style="text-align: center;">58.79</td>
<td style="text-align: center;">61.88</td>
<td style="text-align: center;">60.30</td>
</tr>
</tbody>
</table>
<p>Generalize to Retrieval-format Benchmark. In real-world applications, we do not always have well-organized reference answers for judging. To evaluate the capability of JudgeLM in dealing with this situation, we inject the original reference answers into a randomly selected paragraph. As shown in Table 15, we select paragraphs with different words, and evaluate JudgeLM-33B with the injected paragraphs as references in a zero-shot setting. The results show that JudgeLM can retrieve the correct answers from the paragraphs and make judgments based on them. When the words of paragraphs increase to 400 , JudgeLM faces a maximum drop of $3.73 \%$ agreement and $3.37 \%$</p>
<p>consistency. The results demonstrate that JudgeLM is promising for utilizing JudgeLM to deal with unstructured references or judge in the retrieved form.</p>
<p>Table 15: Performance of JudgeLM-33B with injected paragraphs as references on JudgeLM val set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Reference <br> Paragraph</th>
<th style="text-align: center;">Agreement $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Consistency $\uparrow$ <br> (w/ swap.)</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 1st</th>
<th style="text-align: center;">Bias $\downarrow$ <br> toward 2nd</th>
<th style="text-align: center;">Delta Bias $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No</td>
<td style="text-align: center;">89.32</td>
<td style="text-align: center;">92.37</td>
<td style="text-align: center;">3.62</td>
<td style="text-align: center;">4.01</td>
<td style="text-align: center;">0.39</td>
</tr>
<tr>
<td style="text-align: left;">50 words</td>
<td style="text-align: center;">87.78</td>
<td style="text-align: center;">92.35</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">4.65</td>
<td style="text-align: center;">1.65</td>
</tr>
<tr>
<td style="text-align: left;">100 words</td>
<td style="text-align: center;">87.69</td>
<td style="text-align: center;">91.84</td>
<td style="text-align: center;">2.62</td>
<td style="text-align: center;">5.54</td>
<td style="text-align: center;">2.92</td>
</tr>
<tr>
<td style="text-align: left;">200 words</td>
<td style="text-align: center;">86.77</td>
<td style="text-align: center;">90.48</td>
<td style="text-align: center;">2.70</td>
<td style="text-align: center;">6.82</td>
<td style="text-align: center;">4.12</td>
</tr>
<tr>
<td style="text-align: left;">300 words</td>
<td style="text-align: center;">86.25</td>
<td style="text-align: center;">89.26</td>
<td style="text-align: center;">3.13</td>
<td style="text-align: center;">7.61</td>
<td style="text-align: center;">4.48</td>
</tr>
<tr>
<td style="text-align: left;">400 words</td>
<td style="text-align: center;">85.59</td>
<td style="text-align: center;">89.00</td>
<td style="text-align: center;">2.98</td>
<td style="text-align: center;">8.02</td>
<td style="text-align: center;">5.04</td>
</tr>
</tbody>
</table>
<p>Generalize to Multiple-format benchmark. To get the optimal ranking for N answers from different LLMs, other judge models need to call the model $O\left(n^{2}\right)$ times to get the full matrix, which is a much less efficient solution. We attempt to resolve this limitation by extending our JudgeLM to process multiple answers at the same time. We first need to modify the template as shown in Fig. 5. As shown in Fig. 18, JudgeLM can judge and rank the multiple answers within the context limit of LLM.</p>
<p>We further conduct experiments to evaluate the consistency in the judging form of answer pairs and multiple answers. We first generate answers on JudgeLM val set through 3 LLMs, i.e., Vicuna-13B, LLaMA-7B, and alpaca-7B, for evaluation. Then we use pairwise judging and multiple judging to grade answers and rank them, respectively. Last, we compute the consistency between the two ranking results. Please note that 'Error Rate@2' indicates the position orderings of two answers are different between the result of paired judgment and the result of multiple judgment, and 'Error Rate@3' means the position orderings of three answers are different. Table 16 shows that consistency between judging pairwise and judging multiple can reach $93.48 \%$, and only $0.14 \%$ results are totally wrong. The results are impressive but the $6.38 \%$ of 'Error Rate@2' also shows room for improvement as well, which could be addressed by the further improvement of JudgeLM's selfconsistency.</p>
<p>Table 16: Performance of JudgeLM-33B in judging multiple answers on JudgeLM val set. We calculate the consistency between the pairwise judging results and multiple judging ones.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Consistency $\uparrow$ <br> (w/ pairwise)</th>
<th style="text-align: center;">Error Rate@2 $\downarrow$ <br> (w/ pairwise)</th>
<th style="text-align: center;">Error Rate@3 $\downarrow$ <br> (w/ pairwise)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">JudgeLM-33B multipile</td>
<td style="text-align: center;">93.48</td>
<td style="text-align: center;">6.38</td>
<td style="text-align: center;">0.14</td>
</tr>
</tbody>
</table>
<p>Generalize to Single Answer Grading. The Concurrent judge method (Wang et al., 2023) usually judges a pair of answers to decide which one is better or tie but they lack the ability to evaluate a single answer. Thanks to our judging mode of scoring first and then calculating the judging results, our JudgeLM provides an alternative practice to grade a single answer by slightly modifying the template as shown in Fig. 5. Putting the reference answer in the first position and giving it a full grade as a prior, JudgeLM can give quantitative fine-grained evaluations as shown in Fig. 17.</p>
<p>The capability of grading a single answer is an important extension, which only relies the text-form prediction and ground truth to make judgments. For example, the following extension "Judging multimodal models" is also based on this capability.
Generalize to Reward Model. We further compare the proposed JudgeLM with closed-source methods, powerful reward models, Llama-2-based reward methods (Touvron et al., 2023b; Ivison et al., 2023), and Qwen-1.5-based reward methods (Bai et al., 2023) on the requested reward model benchmark as shown in Table 17. Following the evaluation manner of GPT-3.5-turbo-0125 and Prometheus series (Kim et al., 2023) in RewardBench (Lambert et al., 2024), we evaluate the models among 4 subsets, i.e., Chat, (Chat) Hard, Safety, and Reasoning, and present averaged score in the Score column. The proposed JudgeLM-7B outperforms the GPT-3.5-turbo-0125, the Prometheus series reward models (Kim et al., 2023), UltraRM-13B (Cui et al., 2023), TIGERScore-</p>
<p>Table 17: Comparison with advanced reward models on RewardBench.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Chat</th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;">Safety</th>
<th style="text-align: center;">Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Closed-source Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo-0125</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">59.1</td>
</tr>
<tr>
<td style="text-align: left;">Powerful Reward Models.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Prometheus-8×7B-v2.0</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">77.4</td>
</tr>
<tr>
<td style="text-align: left;">Prometheus-7B-v2.0</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">76.5</td>
</tr>
<tr>
<td style="text-align: left;">UltraRM-13B</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: left;">TIGERScore-13B</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">32.7</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-based Models.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Tulu-2-dpo-70B</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">74.1</td>
</tr>
<tr>
<td style="text-align: left;">Tulu-2-dpo-13B</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">73.2</td>
</tr>
<tr>
<td style="text-align: left;">Tulu-2-dpo-7B</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr>
<td style="text-align: left;">Qwen1.5-based Models.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Qwen1.5-72B-Chat</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">85.5</td>
</tr>
<tr>
<td style="text-align: left;">Qwen1.5-14B-Chat</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">89.6</td>
</tr>
<tr>
<td style="text-align: left;">Qwen1.5-7B-Chat</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">90.4</td>
</tr>
<tr>
<td style="text-align: left;">Ours.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-7B</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">82.3</td>
</tr>
</tbody>
</table>
<p>13B (Jiang et al., 2023), Tulu-2-dpo-13B (Ivison et al., 2023), Tulu-2-dpo-7B (Ivison et al., 2023), and Qwen1.5-based reward models (Bai et al., 2023). Besides, JudgeLM-7B even achieves similar performance to Tulu-2-dpo-70B (Ivison et al., 2023). Noting the RewardBench paper mentions that involving Llama-3 as the base model can significantly improve the metrics on Hard and Reasoning, we leave it as future work.</p>
<p>Multi-turn Chat about Judgments. It is worth noting that fine-tuning with judge samples does not compromise the multi-turn chat ability extended from base models. As illustrated in Fig. 19 and Fig. 20, our JudgeLM retains the capability to engage in meaningful dialogues with users, providing them with a richer context, detailed information, additional examples, and specific details.</p>
<h1>A. 4 MORE DISCUSSION</h1>
<p>Table 18: Comparison of different base models for JudgeLM-7B on JudgeLM val set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Base Models <br> (for JudgeLM-7B)</th>
<th style="text-align: center;">Agreement $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Precision $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Recall $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">F1 $\uparrow$ <br> (w/ GPT-4)</th>
<th style="text-align: center;">Consistency $\uparrow$ <br> (w/ swap.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Judge w/o reference.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Vicuna</td>
<td style="text-align: center;">81.11</td>
<td style="text-align: center;">69.67</td>
<td style="text-align: center;">78.39</td>
<td style="text-align: center;">72.21</td>
<td style="text-align: center;">83.57</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA2-chat</td>
<td style="text-align: center;">83.87</td>
<td style="text-align: center;">73.43</td>
<td style="text-align: center;">80.06</td>
<td style="text-align: center;">75.91</td>
<td style="text-align: center;">85.17</td>
</tr>
<tr>
<td style="text-align: left;">Judge w/ reference.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Vicuna</td>
<td style="text-align: center;">84.08</td>
<td style="text-align: center;">75.92</td>
<td style="text-align: center;">82.55</td>
<td style="text-align: center;">78.28</td>
<td style="text-align: center;">84.46</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA2-chat</td>
<td style="text-align: center;">86.60</td>
<td style="text-align: center;">79.47</td>
<td style="text-align: center;">83.11</td>
<td style="text-align: center;">81.02</td>
<td style="text-align: center;">87.74</td>
</tr>
</tbody>
</table>
<p>Format Bias As shown in Table 16, it can be seen that the judging of multiple answers does not receive a significant performance drop. We hold the viewpoint that judging multiple answers is an easy extension for JudgeLM, which does not change the basis for judging. As mentioned in ' 4 Inherent Biases - Format Bias', format bias means the model judging basis changes from pre-trained knowledge to reference, or vice versa. So, judging in mismatched situations faces format bias but judging multiple answers does not.
Other Human-annotated benchmarks For a fair comparison, we also evaluate JudgeLM on the PandaLM test set in a zero-shot setting. The PandaLM train and test sets are annotated by GPT3.5</p>
<p>and humans, respectively. As shown in Table 2, the zero-shot results of JudgeLM also outperform other judging methods, i.e., PandaLM, GPT-3.5, and GPT-4. Furthermore, JudgeLM also achieves a superior 0 -shot judging performance on the multimodal benchmark with human annotation, i.e., MM-Vet, as shown in Table 13.</p>
<p>Reasoning Ability of LLM Judges Nowadays, NLP researchers are still struggling with proposing LLMs with superior reasoning abilities. JudgeLM also needs the proposed reference sup method to enhance the judging ability for out-of-domain or counterfactual tasks, as shown in Fig. 11 and Fig. 16. Notably, the proposed JudgeLM can benefit from stronger foundation LLMs, e.g., the LLaMA2-7B-Chat-based (Touvron et al., 2023b) JudgeLM outperforms the original JudgeLM-7B on all metrics, as shown in Table 18. The research of judge models is critical for the development of LLMs and can benefit from advanced LLMs, establishing a positive cycle.</p>
<p>Critiques for Judgements Beyond using LLMs to compare answer pairs and generate reasons, the critique and correction of these reasons have become increasingly important topics. This approach allows LLMs to reassess their generated reasons in multiple rounds, thereby enhancing judging accuracy. Works like UltraCM, Auto-J, and Shepherd have reliably evaluated the quality of textual reasons. Recently, CriticBench has also provided a reliable benchmark to evaluate the reasons and evaluation abilities of LLMs.</p>
<p>For JudgeLM, its ability to generalize to multi-turn chat enables us to construct multi-turn judgement critique data by combining data samples without references and those with references. Through two rounds of judge Q\&amp;A, i.e., without reference and with reference, JudgeLM can acquire the capability to critique its own judgments. We leave these experiments for future work.</p>
<p>Reference Drop as an Independent Method We think the reference drop is an independent and significant method. At first, we argue that judging with or without references are two sub-benchmarks, which require judges to make judgments with internal knowledge or by comparing LLM-generated answers with a reference answer, respectively. The reference drop is not only a simple but effective hyper-parameter, but also an important method that bridges the two sub-benchmarks, which enables the JudgeLM to make judgments in different situations.</p>
<p>Table 19: Comparison between PandaLM and JudgeLM components in terms of datasets and methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Base Model</th>
<th style="text-align: center;">Data</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">val w/ ref?</th>
<th style="text-align: center;">Agreement $\uparrow$</th>
<th style="text-align: center;">Consistency $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PandaLM baseline.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">+ 300K PandaLM data</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">68.61</td>
<td style="text-align: center;">74.78</td>
</tr>
<tr>
<td style="text-align: center;">change to JudgeLM data.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">+3.5k JudgeLM data</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">71.30</td>
<td style="text-align: center;">69.59</td>
</tr>
<tr>
<td style="text-align: center;">w/ the proposed methods.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">+3.5k JudgeLM data</td>
<td style="text-align: center;">+swap aug</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">72.41</td>
<td style="text-align: center;">73.50</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">+3.5k JudgeLM data</td>
<td style="text-align: center;">+ref sup</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">75.15</td>
<td style="text-align: center;">74.08</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">+3.5k JudgeLM data</td>
<td style="text-align: center;">+ref drop</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">75.93</td>
<td style="text-align: center;">74.77</td>
</tr>
<tr>
<td style="text-align: center;">w/ all proposed methods.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">+3.5k JudgeLM data</td>
<td style="text-align: center;">+ all</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">75.01</td>
<td style="text-align: center;">76.28</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">+3.5k JudgeLM data</td>
<td style="text-align: center;">+ all</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">78.10</td>
<td style="text-align: center;">79.50</td>
</tr>
</tbody>
</table>
<p>Differences between PandaLM and JudgeLM To fairly assess the impact of our proposed dataset and methods, we conducted experiments using the PandaLM baseline with LLaMA-7B as the base model. As shown in Table 19, our dataset and methods both provide significant improvements.</p>
<p>Compared with PandaLM, our method has these different novelties:</p>
<ul>
<li>We introduce a high-quality, large-scale dataset for judge models, enriched with diverse seed tasks, LLMs-generated answers, and detailed judgments from GPT-4, laying the foundation for future LLMs evaluating research.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*This work was done when Lianghui Zhu was an intern at Beijing Academy of Artificial Intelligence. ${ }^{\dagger}$ Corresponding authors: xgwang@hust.edu.cn and wangxinliang@baai.ac.cn.
${ }^{\dagger}$ As a reference, the max agreement among humans in MT-bench (Zheng et al., 2023) is $82 \%$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
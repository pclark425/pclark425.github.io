<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8367 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8367</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8367</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-8d8fc878bf4c7005546c866824a72d0c46ca91a3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8d8fc878bf4c7005546c866824a72d0c46ca91a3" target="_blank">Localizing Model Behavior with Path Patching</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper Abstract:</strong> Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8367.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8367.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 numeric completion (path-patching)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Numeric-completion heuristics in GPT-2 analyzed with path patching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper probes how GPT-2 small and GPT-2 XL predict numeric completions for prompts like "The organization estimates that [N]-" (N in 0..100), finding that models rely on heuristics (e.g., prefer numbers slightly larger than N and round numbers) and that specific attention heads are implicated in recognizing the token as a number.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 small (117M) and GPT-2 XL (1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained GPT-2 family transformer models; GPT-2 small (≈117M parameters, 12 layers / 144 heads total reported for small experiments) and GPT-2 XL (≈1.5B parameters). The paper does not provide additional training details beyond using the standard GPT-2 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numeric next-token completion / numeric prediction (prompts of the form "The organization estimates that [N]-" with N in 0..100); not explicit arithmetic operations like addition/subtraction.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Heuristic-based numeric priors implemented via attention: the model appears to use heuristics such as preferring numbers slightly larger than the prompt number and favoring round numbers; certain attention heads (examples identified: heads 5.6 and 6.1 in GPT-2 small) are implicated in recognizing that the token before the hyphen is a number (i.e., contributing to a 'number-detection' signal that affects completion distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Path patching (primary): testing hypotheses by replacing all paths through a target head with values taken from counterfactual inputs sampled from a dataset; compared against input corruption / causal tracing (adding Gaussian noise to inputs) and zero-ablation style interventions in discussion. Metrics: mean KL divergence over 100 (x_r, x_c) pairs, plus measured probability of numeric completions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline probability of a numeric completion on the dataset: 88.1%. Results from Table 5 (GPT-2 small):
- Head 5.6: Corrupted-input KL = 0.025; KL given #completion = 0.006; Probability of numeric completion after corruption = 82.5%. Patched-input KL = 0.011; KL given #completion = 0.008; Probability of numeric completion after patching = 88.0%.
- Head 6.1: Corrupted-input KL = 0.017; KL given #completion = 0.004; Probability after corruption = 83.9%. Patched-input KL = 0.003; KL given #completion = 0.002; Probability after patching = 88.3%.
- All heads (ablated/corrupted): Corrupted-input KL = 2.328; KL given #completion = 0.883; Probability after corruption = 26.9%. Patched-input KL = 3.108; KL given #completion = 3.393; Probability after patching = 88.1%.
(Mean KL computed over 100 (x_r,x_c) pairs.)</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Off-distribution effects when corrupting inputs (input corruption can drive downstream activations off-distribution and produce misleadingly large effects); different interventions disagree (corruption vs. patching can produce different sets of 'important' heads). The paper notes the results are speculative without ground truth, and that corruption may produce non-number tokens or otherwise unnatural activations that break heuristics. Patching with counterfactual numbers preserves on-distribution behavior but can miss heads that are sensitive to the exact numeric identity rather than the presence-of-number signal.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Path-patching experiments show that corrupting certain heads (e.g., 5.6, 6.1) reduces the probability of a numeric completion, while patching those same heads (replacing their inputs with values from a different numeric counterfactual) has little or no effect on numeric completion probability — consistent with these heads contributing a number-detection signal rather than encoding a specific numeric identity. Visualizations (Figures 9 and 10) show per-head mean-KL importance maps; Table 5 quantifies KL and numeric completion probabilities under corruption vs. patching.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Authors caution they cannot conclusively prove the mechanistic claim: corruption and patching produce different head-importance patterns and it's unclear which better reflects true causal structure. Corruption moves the model off-distribution and may disturb later computations, so heads flagged by corruption may not genuinely 'use' N in an on-distribution way. Conversely, patching uses counterfactuals sampled as numbers, which could hide heads that respond to non-numeric features; lack of ground truth precludes definitive attribution. The method also only tests sufficiency on the chosen dataset, not completeness or generalization to other distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Localizing Model Behavior with Path Patching', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8367.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8367.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path Patching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Path patching (interventional localization of behaviors via path-specific counterfactuals)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general causal-intervention method that tests hypotheses about which computation paths in a model mediate an input-output relationship by replacing inputs to unimportant paths with counterfactual values and measuring the change in outputs (AUE/ATE and proportion explained).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Method is model-agnostic; applied in this paper to a small 2-layer attention-only transformer (8 heads/layer) and to GPT-2 small (117M) and GPT-2 XL (1.5B) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model; a probing / intervention technique applied to transformer models in the paper. The paper demonstrates path patching on: (1) a 2-layer attention-only autoregressive transformer (8 heads/layer) trained on OpenWebText, and (2) pretrained GPT-2 small and GPT-2 XL.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used for analyzing numeric heuristics (numeric next-token completion) in this paper; the method itself is general and can be applied to arithmetic tasks if datasets/hypotheses are constructed for them.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Path patching reasons about mechanisms as contributions along explicit computational paths (e.g., token -> layer0 head -> later head key/query/value -> unembedding). It represents candidate causal paths and measures their sufficiency for mediating behavior on a dataset using counterfactual inputs shared across unimportant paths.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Interventional: define important paths P, create a Treeify-expanded graph where each path has its own input copy, set unimportant-path inputs to a counterfactual x_c (reused across unimportant paths), evaluate G_H(x_r, x_c), and measure average unexplained effect (AUE) or proportion explained relative to total effect (ATE). Compared empirically to input-corruption/causal tracing (Gaussian noise) and zero ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Faithfulness metrics defined include AUE (average unexplained effect), ATE (average total effect), and proportion explained = (1 - AUE/ATE)*100%. In experiments, path patching yields quantitative per-hypothesis proportions explained (e.g., induction-head hypotheses progressed from 28.2% to 73.5% proportion explained across refinements). For numeric-completion experiments, per-head mean KL divergence (over 100 pairs) was used as the primary metric.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Metrics susceptible to cancellation across examples (difference in expected loss can hide per-example failures); AUE measures sufficiency on the tested distribution but not completeness or generalization; reuse of the same counterfactual x_c can hide path-specific idiosyncrasies; finite sampling causes estimation error; interventions that push activations off-distribution (e.g., input corruption) can produce misleading signals.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper demonstrates iterative hypothesis construction and refinement on induction/copying behavior and numeric heuristics; path patching attribution visualizations identify which paths increase/decrease loss when treated as unimportant; refinements (Direct -> Positional -> Long -> All-Final) progressively increase proportion explained, showing path patching can localize mechanisms and guide discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Path patching cannot prove hypotheses for all inputs (only tests sufficiency on sampled distribution); it can fail to reject false hypotheses due to dataset narrowness or cancellation; it may produce different results than corruption-based causal tracing or zero-ablation, and deciding which intervention is more faithful may require additional evidence or ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Localizing Model Behavior with Path Patching', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Locating and editing factual associations in gpt <em>(Rating: 2)</em></li>
                <li>Interpretability in the wild: a circuit for indirect object identification in gpt-2 small <em>(Rating: 1)</em></li>
                <li>In-context learning and induction heads <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8367",
    "paper_id": "paper-8d8fc878bf4c7005546c866824a72d0c46ca91a3",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "GPT-2 numeric completion (path-patching)",
            "name_full": "Numeric-completion heuristics in GPT-2 analyzed with path patching",
            "brief_description": "The paper probes how GPT-2 small and GPT-2 XL predict numeric completions for prompts like \"The organization estimates that [N]-\" (N in 0..100), finding that models rely on heuristics (e.g., prefer numbers slightly larger than N and round numbers) and that specific attention heads are implicated in recognizing the token as a number.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 small (117M) and GPT-2 XL (1.5B)",
            "model_description": "Pretrained GPT-2 family transformer models; GPT-2 small (≈117M parameters, 12 layers / 144 heads total reported for small experiments) and GPT-2 XL (≈1.5B parameters). The paper does not provide additional training details beyond using the standard GPT-2 variants.",
            "arithmetic_task_type": "Numeric next-token completion / numeric prediction (prompts of the form \"The organization estimates that [N]-\" with N in 0..100); not explicit arithmetic operations like addition/subtraction.",
            "mechanism_or_representation": "Heuristic-based numeric priors implemented via attention: the model appears to use heuristics such as preferring numbers slightly larger than the prompt number and favoring round numbers; certain attention heads (examples identified: heads 5.6 and 6.1 in GPT-2 small) are implicated in recognizing that the token before the hyphen is a number (i.e., contributing to a 'number-detection' signal that affects completion distributions).",
            "probing_or_intervention_method": "Path patching (primary): testing hypotheses by replacing all paths through a target head with values taken from counterfactual inputs sampled from a dataset; compared against input corruption / causal tracing (adding Gaussian noise to inputs) and zero-ablation style interventions in discussion. Metrics: mean KL divergence over 100 (x_r, x_c) pairs, plus measured probability of numeric completions.",
            "performance_metrics": "Baseline probability of a numeric completion on the dataset: 88.1%. Results from Table 5 (GPT-2 small):\n- Head 5.6: Corrupted-input KL = 0.025; KL given #completion = 0.006; Probability of numeric completion after corruption = 82.5%. Patched-input KL = 0.011; KL given #completion = 0.008; Probability of numeric completion after patching = 88.0%.\n- Head 6.1: Corrupted-input KL = 0.017; KL given #completion = 0.004; Probability after corruption = 83.9%. Patched-input KL = 0.003; KL given #completion = 0.002; Probability after patching = 88.3%.\n- All heads (ablated/corrupted): Corrupted-input KL = 2.328; KL given #completion = 0.883; Probability after corruption = 26.9%. Patched-input KL = 3.108; KL given #completion = 3.393; Probability after patching = 88.1%.\n(Mean KL computed over 100 (x_r,x_c) pairs.)",
            "error_types_or_failure_modes": "Off-distribution effects when corrupting inputs (input corruption can drive downstream activations off-distribution and produce misleadingly large effects); different interventions disagree (corruption vs. patching can produce different sets of 'important' heads). The paper notes the results are speculative without ground truth, and that corruption may produce non-number tokens or otherwise unnatural activations that break heuristics. Patching with counterfactual numbers preserves on-distribution behavior but can miss heads that are sensitive to the exact numeric identity rather than the presence-of-number signal.",
            "evidence_for_mechanism": "Path-patching experiments show that corrupting certain heads (e.g., 5.6, 6.1) reduces the probability of a numeric completion, while patching those same heads (replacing their inputs with values from a different numeric counterfactual) has little or no effect on numeric completion probability — consistent with these heads contributing a number-detection signal rather than encoding a specific numeric identity. Visualizations (Figures 9 and 10) show per-head mean-KL importance maps; Table 5 quantifies KL and numeric completion probabilities under corruption vs. patching.",
            "counterexamples_or_challenges": "Authors caution they cannot conclusively prove the mechanistic claim: corruption and patching produce different head-importance patterns and it's unclear which better reflects true causal structure. Corruption moves the model off-distribution and may disturb later computations, so heads flagged by corruption may not genuinely 'use' N in an on-distribution way. Conversely, patching uses counterfactuals sampled as numbers, which could hide heads that respond to non-numeric features; lack of ground truth precludes definitive attribution. The method also only tests sufficiency on the chosen dataset, not completeness or generalization to other distributions.",
            "uuid": "e8367.0",
            "source_info": {
                "paper_title": "Localizing Model Behavior with Path Patching",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Path Patching",
            "name_full": "Path patching (interventional localization of behaviors via path-specific counterfactuals)",
            "brief_description": "A general causal-intervention method that tests hypotheses about which computation paths in a model mediate an input-output relationship by replacing inputs to unimportant paths with counterfactual values and measuring the change in outputs (AUE/ATE and proportion explained).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Method is model-agnostic; applied in this paper to a small 2-layer attention-only transformer (8 heads/layer) and to GPT-2 small (117M) and GPT-2 XL (1.5B) in experiments.",
            "model_description": "Not a model; a probing / intervention technique applied to transformer models in the paper. The paper demonstrates path patching on: (1) a 2-layer attention-only autoregressive transformer (8 heads/layer) trained on OpenWebText, and (2) pretrained GPT-2 small and GPT-2 XL.",
            "arithmetic_task_type": "Used for analyzing numeric heuristics (numeric next-token completion) in this paper; the method itself is general and can be applied to arithmetic tasks if datasets/hypotheses are constructed for them.",
            "mechanism_or_representation": "Path patching reasons about mechanisms as contributions along explicit computational paths (e.g., token -&gt; layer0 head -&gt; later head key/query/value -&gt; unembedding). It represents candidate causal paths and measures their sufficiency for mediating behavior on a dataset using counterfactual inputs shared across unimportant paths.",
            "probing_or_intervention_method": "Interventional: define important paths P, create a Treeify-expanded graph where each path has its own input copy, set unimportant-path inputs to a counterfactual x_c (reused across unimportant paths), evaluate G_H(x_r, x_c), and measure average unexplained effect (AUE) or proportion explained relative to total effect (ATE). Compared empirically to input-corruption/causal tracing (Gaussian noise) and zero ablation.",
            "performance_metrics": "Faithfulness metrics defined include AUE (average unexplained effect), ATE (average total effect), and proportion explained = (1 - AUE/ATE)*100%. In experiments, path patching yields quantitative per-hypothesis proportions explained (e.g., induction-head hypotheses progressed from 28.2% to 73.5% proportion explained across refinements). For numeric-completion experiments, per-head mean KL divergence (over 100 pairs) was used as the primary metric.",
            "error_types_or_failure_modes": "Metrics susceptible to cancellation across examples (difference in expected loss can hide per-example failures); AUE measures sufficiency on the tested distribution but not completeness or generalization; reuse of the same counterfactual x_c can hide path-specific idiosyncrasies; finite sampling causes estimation error; interventions that push activations off-distribution (e.g., input corruption) can produce misleading signals.",
            "evidence_for_mechanism": "Paper demonstrates iterative hypothesis construction and refinement on induction/copying behavior and numeric heuristics; path patching attribution visualizations identify which paths increase/decrease loss when treated as unimportant; refinements (Direct -&gt; Positional -&gt; Long -&gt; All-Final) progressively increase proportion explained, showing path patching can localize mechanisms and guide discovery.",
            "counterexamples_or_challenges": "Path patching cannot prove hypotheses for all inputs (only tests sufficiency on sampled distribution); it can fail to reject false hypotheses due to dataset narrowness or cancellation; it may produce different results than corruption-based causal tracing or zero-ablation, and deciding which intervention is more faithful may require additional evidence or ground truth.",
            "uuid": "e8367.1",
            "source_info": {
                "paper_title": "Localizing Model Behavior with Path Patching",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Locating and editing factual associations in gpt",
            "rating": 2
        },
        {
            "paper_title": "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small",
            "rating": 1
        },
        {
            "paper_title": "In-context learning and induction heads",
            "rating": 1
        }
    ],
    "cost": 0.0111655,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Localizing Model Behavior With Path PatchING</h1>
<p>Nicholas Goldowsky-Dill ${ }^{1}$, Chris MacLeod ${ }^{1}$, Lucas Sato ${ }^{1}$, \&amp; Aryaman Arora ${ }^{1}$<br>${ }^{1}$ Redwood Research<br>nix@rdwrs.com, chris@rdwrs.com,<br>satojk@stanford.edu, aa2190@georgetown.edu</p>
<h4>Abstract</h4>
<p>Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments.</p>
<h2>1 INTRODUCTION</h2>
<p>Deep neural networks can perform many complex tasks, but our ability to reason about their behavior is limited (Räuker et al., 2023). Recent work has reverse engineered toy networks (Chughtai et al., 2023) such that the function of every neuron can be understood. For state of the art networks, this remains out of reach and current works focus on approximately explaining small parts of the network. For example, some behaviors can be approximately understood with reference to only an abstracted "circuit" containing a small number of interacting components (Wang et al., 2022; Olsson et al., 2022; Olah et al., 2020).</p>
<p>Currently, we lack guarantees that deep networks will behave robustly out of distribution. To mitigate these risks, one avenue of research is generating simplified causal abstractions of the network (Geiger et al., 2021; 2023a). Such abstractions aim to be more compact and thus easier to reason about than the network, while approximating the network's behavior sufficiently well over some range of inputs. These goals inherently trade off, but we hope that even substantially incomplete abstractions can be useful in a variety of downstream tasks such as finding adversarial examples.</p>
<p>Path patching was first introduced in Wang et al. (2022), where they considered a sender attention head that interacted with the key, query, or value inputs of one or more receiver attention heads. By performing causal interventions, they could measure composition between the heads, or the influence of a head on the logits.</p>
<p>In this work, we generalize path patching to test hypotheses containing any number of paths from input to output in an arbitrary computational graph. The use of paths is strictly more expressive than considering a set of graph nodes such as neurons or attention heads. We define a precisely specified format for localization claims, enabling quantitative comparison between claims and a clear understanding of their scope.</p>
<p>Formally, a hypothesis is a claim that a subset of paths in a network (which we call "important paths") mediate (Pearl, 2013) the relationship between input and output on a given distribution; all other paths in the network are "unimportant paths". By expressing the network as a computational graph in a particular form and removing all contributions of unimportant paths, we arrive at an approximate abstraction of the network. Path patching can then show you how similar the behavior of the abstraction is to the original network. When path patching rejects a hypothesis, path patching</p>
<p>attribution shows the source of the discrepancies, allowing the researcher to iteratively refine the claim.</p>
<p>Our contributions are:</p>
<ul>
<li>We formalize the path patching methodology for precisely describing and testing localization claims.</li>
<li>We use path patching to test and iteratively refine hypotheses about induction heads in an attention-only transformer.</li>
<li>We formalize, test, and refine a hypothesis about a behavior of GPT-2.</li>
<li>We provide an open source framework for path patching experiments.</li>
</ul>
<h1>2 Methodology</h1>
<h3>2.1 LOCALIZATION</h3>
<p>Localization is the problem of finding which parts of the network matter for a chosen behavior; it does not consider what computations they perform. The granularity of localization can vary greatly: an individual neuron (Finlayson et al., 2021), an individual attention head (Vig et al., 2020), subspaces (Geiger et al., 2023b), a composition of attention heads (Olsson et al., 2022), a transformer block (Belrose et al., 2023) or ranges of MLP layers (Meng et al., 2022). Our framework handles all of these in a uniform way using a set of paths in a computational graph, which is strictly more expressive than treating a set of nodes such as neurons or attention heads as the atom of description.</p>
<h3>2.2 Choosing the DataSET</h3>
<p>We define a "behavior" or object of study in terms of input-output pairs on a specific dataset. This makes the choice of dataset an essential part of the behavior. An accurate approximation on one dataset may transfer to a different dataset, but in general this is not the case. As illustrated in Bolukbasi et al. (2021), parts of the network with one behavior on specific distribution can have very different behavior on other distributions. By clearly stating the domain under consideration, we avoid treating evidence in favor of a narrow explanation as sufficient to support a broader explanation.</p>
<h3>2.3 PATH PATCHING WITH NODES AS MEDIATORS</h3>
<p>We'll consider the simpler case where nodes are mediators first, and then build up to working with paths. Let $G$ be a function with input $x \in X$ and output $y \in Y$. Our methodology applies to arbitrary functions, but for our experiments we will focus on the forward pass function of autoregressive transformers, where $x$ would be a sequence of tokens and $y$ the next-token probabilities.
The computation of $G$ can be represented by $\mathcal{G}$, a directed acyclic graph (DAG) where nodes are functions and edges are values. We can represent the same computation at different levels of granularity by expanding or combining nodes.
As a running example, consider a neural network with two layers and skip connections. Here $G(x)=f_{1}(A(x))+A(x)$ where $A(x)=f_{0}(x)+x$. Two equivalent computational graphs $\mathcal{G}$ corresponding to the function $G$ are shown in Figure 1.
Suppose we suspect that $f_{1}$ is an unimportant node. We define a function $G_{H}: X^{2} \rightarrow Y$ where $G_{H}\left(x_{r}, x_{c}\right)$ means we evaluate $G$ on a reference input $x_{r}$ except that we replace each unimportant node in $\mathcal{G}$ with the value that node has when evaluating $G$ on a counterfactual input $x_{c}$, as shown on Figure 2. Our hypothesis predicts the output should not change.
In the language of the causal mediation literature this measures the natural indirect effect through $f_{1}$ when moving from $x_{r}$ to $x_{c}$.
Formally, a hypothesis H is a tuple $(\mathcal{G}, \delta, S, D)$ where $\delta: Y^{2} \rightarrow \mathbb{R}$ is some measure of dissimilarity, such as absolute difference between two scalars or KL divergence between two probability distributions, $S$ is a set of "important nodes", $\mathcal{G} \backslash S$ are the unimportant nodes, and $D$ is a joint distribution</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Left: a two layer residual network, where $f_{0}$ and $f_{1}$ are layer functions and A and Y are skip connections that add their inputs. Right: dividing each function into two functions that sum to the original. See Appendix E for discussion of other common rewrites.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: To compute $G_{H}\left(x_{r}, x_{c}\right)$ where $f_{1}$ is unimportant, we replace $f_{1}$ in $\mathcal{G}$ with $f_{1}\left(x_{c}\right)=$ $f_{0}\left(x_{c}\right)+x_{c}$.
over $\left(x_{r}, x_{c}\right)$ pairs. Normally the $x_{r}$ come from some reference distribution $D_{r}$ such as a training or validation set.</p>
<p>There are many options for $x_{c}$ corresponding to different experiments: $x_{c}$ can come from $D_{r}$ (resampling), be computed via a transformation of $x_{r}$ (input manipulation or input corruption), be computed as the mean of $D_{r}$ (mean ablation of the input) or be the zero tensor (zero ablation of the input).</p>
<p>The strictest version of what it means for H to hold is that $\delta\left(G\left(x_{r}\right), G_{H}\left(x_{r}, x_{c}\right)\right)$ is exactly zero everywhere on $D$. In modern neural networks, we find that this is too stringent and requires us to severely restrict the domain of $D$ and/or include nearly all nodes in $S$. Instead, as a notion of approximate abstraction we define the "average unexplained effect" of H :</p>
<p>$$
\operatorname{AUE}(H):=\mathbb{E}<em r="r">{\left(x</em>\right)\right)\right]
$$}, x_{c}\right) \sim D}\left[\delta\left(G\left(x_{r}\right), G_{H}\left(x_{r}, x_{c</p>
<p>The hypothesis claims the AUE is zero. A nonzero AUE tells us we're missing some aspects of the behavior, which may be acceptable depending on the use case. Given two hypotheses with the same dataset and same number of paths, we generally prefer the one with a lower AUE.</p>
<p>Note that this allows the unexplained effect to be very large on some inputs as long as it's small in expectation. A large unexplained effect on some inputs could indicate that a distinct mechanism is used for these inputs, in which case investigating those inputs separately would be fruitful. For applications where we care about rare failures it could be more appropriate to consider the maximum unexplained effect instead to measure worst case behavior, as in Beckers et al. (2020)</p>
<h1>2.4 PATH PATCHING WITH PATHS AS MEDIATORS</h1>
<p>Residual networks naturally decompose into a sum of paths, where a small set of relatively shallow paths often contribute most of the effect (Veit et al., 2016).</p>
<p>In our running example, suppose our exploratory analysis suggests to us that $f_{0}$ and $f_{1}$ were both important, and that computation of $f_{1}$ doesn’t use $f_{0}$’s output. For example, this can happen in a transformer if $f_{1}$ is an attention layer reading from a different subspace than the subspace written by $f_{0}$. We express this as " $x \rightarrow f_{0} \rightarrow A \rightarrow f_{1}$ is unimportant". In order to feed $x_{c}$ to this path without affecting other paths, we introduce the function $\operatorname{Treeify}(\mathcal{G})$, which identifies each subtree that has multiple consumers of the subtree's output, and then copies the subtree so each consumer has its own copy (Figure 3).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: $\operatorname{Treeify}(\mathcal{G})$ : Left: red dashed lines are nodes with multiple outputs that will be copied. Center: Copying the subtree at A so that Y and $f_{1}$ take separate copies of A as input. Right: Copying X so that each A and $f_{0}$ has an independent copy. Note the order of copying doesn't matter; we could have duplicated X first and ended up with the same result.</p>
<p>The resulting graph has a one-to-one correspondence between paths in the network and copies of the input node. Now we can set the rightmost X to $x_{c}$ without affecting any other paths.
Algebraically, $\operatorname{Treeify}(\mathcal{G})$ implements a function $G_{T}: X^{N} \rightarrow Y$, where $G_{T}(x, x, x, \ldots, x)=$ $G(x)$ and $N$ is the number of paths. We specify $G_{T}$ by fully expanding the equation for the network, then relabelling each occurrence of $x$ with an unique subscript (in arbitrary order):</p>
<p>$$
G_{T}\left(x_{0}, x_{1}, x_{2}, x_{3}\right)=f_{1}(A(x))+A(x)=f_{1}\left(f_{0}\left(x_{3}\right)+x_{2}\right)+f_{0}\left(x_{1}\right)+x_{0}
$$</p>
<p>Now we allow H to be a tuple $(\mathcal{G}, \delta, P, D)$ where instead of a set of nodes S , we have a set of paths P. We define a path version of $G_{H}$ :</p>
<p>$$
G_{H}\left(x_{r}, x_{c}\right):=G_{T}\left(x_{1}, x_{2}, \ldots, x_{N}\right) \text { with } x_{i}= \begin{cases}x_{r} &amp; p_{i} \in P \ x_{c} &amp; p_{i} \notin P\end{cases}
$$</p>
<p>where $p_{1}, p_{2}, \ldots p_{N}$ are paths through $\mathcal{G}$ in the same order as the arguments of $G_{T}$.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Left: an arbitrary numbering of $\operatorname{Treeify}(\mathcal{G})$ 's input nodes. Right: testing the hypothesis "only $x \rightarrow f_{0} \rightarrow A \rightarrow f_{1}$ is unimportant" reduces to testing "only $x_{3}$ is unimportant".</p>
<p>Above, we had only one unimportant path; when there are multiple unimportant paths we simply reuse the same $x_{c}$ for all of them. We argue in Appendix A that this is a sensible default compared to other strategies such as sampling a different $x_{c}$ for each unimportant path.</p>
<p>Pearl's path specific effect. $G_{H}\left(x_{r}, x_{c}\right)$ is a deterministic version of the path-specific effect for a probabilistic graphical model (PGM). In Pearl (2013), they modify the PGM so the input to all unselected paths is held at a reference value $x^{<em>}$. Then set the input X to the intervention value $x$ and compute the probability distribution over the output Y. If the reference $x^{</em>}$ corresponds to our $x_{r}$, the intervention $x$ to our $x_{c}$, and "unselected paths" with our important paths, then the path-specific effect is exactly equal to $G_{H}\left(x_{r}, x_{c}\right)$.</p>
<h1>2.5 METRICS</h1>
<p>Proportion explained. The average total effect (ATE) is defined in Pearl (2013) as the effect of replacing the reference input with the counterfactual input:</p>
<p>$$
\operatorname{ATE}(\mathrm{H}):=\mathbb{E}<em r="r">{\left(x</em>\right)\right)\right]
$$}, x_{c}\right) \sim D}\left[\delta\left(G\left(x_{r}\right), G\left(x_{c</p>
<p>This is identical to the AUE of a hypothesis that no paths are important. For convenience of reporting, we can divide AUE values by the ATE to compute proportion explained as:</p>
<p>$$
(1-\operatorname{AUE}(\mathrm{H}) / \operatorname{ATE}(\mathrm{H})) * 100 \%
$$</p>
<p>Note that since it's possible that $\mathrm{AUE}&gt;\mathrm{ATE}$, proportion explained can be less than $0 \%$. The hypothesis that all paths are important has a proportion recovered of $100 \%$ by definition.</p>
<p>Including the loss in the computational graph. Assuming we have access to ground truth labels $y: Y$ and a loss function $L: Y \times Y \rightarrow \mathbb{R}$, we may wish to include the labels and loss as part of $\mathcal{G}$. When we compute an intervention, the label in the graph $y_{r}$ always corresponds to $x_{r}$ (the label is always important), $\delta$ is the absolute value function, and the unexplained effect is just the absolute difference in loss after intervention.</p>
<p>We are doing this because we hope that the set of paths that are important towards getting a good loss is more sparse than the set necessary to approximate the full output distribution. In the case of cross entropy loss, only the probability assigned to the true label contributes to the loss. Any paths that only redistribute probability among the other classes are considered unimportant. For example, these paths could implement heuristics that are beneficial on a wider distribution but neutral on D.</p>
<p>When loss is included, it's possible to get a misleadingly high proportion recovered. For example, if the network is usually correct with high confidence, then the $\operatorname{ATE}(\mathrm{H})=L\left(G\left(x_{c}\right), y_{r}\right)$ will be very high and even a poor hypothesis will recover near $100 \%$. In this situation, a better denominator could be the loss when predicting a uniform distribution, or loss when predicting the class frequencies.</p>
<p>Difference in expected loss. Suppose you were studying prompts of the form "Which animal is bigger, [animal1] or [animal2]? The answer is:". If we consider the predicted logit difference between correct and incorrect answers, we might hope that mediators describe "parts of the model that store animal facts". However, we would also observe mediators that are not the subject of study such as "parts storing unigram frequencies".</p>
<p>If our dataset also contains symmetrical prompts with "bigger" swapped for "smaller", then the contribution of the unigram frequencies will cancel out on average. Swapping the order of the animals would likewise cancel out any heuristic that promotes recent tokens.</p>
<p>This implies we could try moving the dissimilarity metric in equation 1 outside the expectation so that parts implementing these heuristics will test as unimportant. When we do this, our hypothesis claims the following metric will be zero:</p>
<p>$$
\left|\mathbb{E}<em r="r">{\left(x</em>[L(G(x), y)]\right|
$$}, x_{c}\right) \sim D}\left[L\left(G_{H}\left(x_{r}, x_{c}\right), y_{r}\right)\right]-E_{x \sim D</p>
<p>We can compare to a baseline corresponding to the claim that no paths are important:</p>
<p>$$
\left|\mathbb{E}<em r="r">{\left(x</em>[L(G(x), y)]\right|
$$}, x_{c}\right) \sim D}\left[L\left(G\left(x_{c}\right), y_{r}\right)\right]-E_{x \sim D</p>
<p>If the abstraction gets lower loss on some inputs and higher loss on other inputs, cancellation will occur and AUE will be misleadingly low (Scheurer et al., 2023).</p>
<p>Path patching attribution. It's often useful to take one specific $x_{r}$ and visualize which paths are responsible for that specific output. We call this path patching attribution and we simply fix the joint distribution $D$ to only include that $x_{r}$, while sampling various $x_{c}$. In particular for every (prompt, completion) pair $\left(x_{r}, y_{r}\right)$ we compute:</p>
<p>$$
\mathbb{E}<em c="c">{x</em>\right)\right]
$$} \sim D}\left[L\left(G_{H}\left(x_{r}, x_{c}\right), y_{r}\right)-L\left(G\left(x_{r}\right), y_{r</p>
<p>Note we drop the absolute value to preserve directional information; this allows us to see whether the patched model had a net higher or net lower loss than the original model.</p>
<h1>3 ReSults ON IndUCTION</h1>
<p>Given a prompt with just the token " N ", GPT-2 uses learned bigram statistics to predict common continuations like "athan" or "ancy". However, if given a prompt like "Nathan and Mary went to the store. N", now GPT-2 is much more confident that "athan" is the next token. We can represent the prompt as "[A][B]...[A]".
Elhage et al. (2021) present a mechanistic explanation for this behavior in terms of two interacting attention heads. They note that more complex mechanisms are possible, but the minimal version is as follows. Define $i$ as the position of the second [A] and define $j$ as the position of [B]. First, a previous token head (PTH) attends from $j$ to $j-1$ and adds a vector to the residual stream representing " $[A]$ is previous". Second, an induction head (IH) in a later layer has a query of [A], a key of " $[A]$ is previous", and thus attends strongly from $i$ to $j$. Then the value operation adds a vector to the residual stream which unembeds to [B].
In this section, we apply our methodology to measure how much of the induction behavior is explained by the minimal explanation versus more complex interactions.</p>
<h3>3.1 OUR MODEL</h3>
<p>We investigated a 2-layer attention-only autoregressive transformer trained on the OpenWebText dataset. Our model has 8 attention heads per layer preceded by layer norm, and uses shortformer positional encodings (Press et al., 2020). In shortformer, learned positional embeddings are provided to the keys and queries of each attention head, but not to the values. This means positional information can be used to compute attention scores, but never enters the residual stream. The unembedding matrix is separate (in contrast to GPT-2, which has a tied unembedding).</p>
<p>Let the notation L.H represent an attention head, where L is the 0 -indexed attention layer number, and H is the 0 -indexed head number within that layer. By direct inspection of the attention patterns, we found evidence supporting head 0.0 as a PTH and heads 1.5 and 1.6 as induction heads - see Appendix B for visualizations.
In our experiments, we define the dataset $D_{r}$ as the first 300 tokens of 100 K examples from OpenWebText held-out from training. Our computational graph includes the cross-entropy loss at each individual token, which means we are explaining the probability the model places on the actual next token, without caring about the probabilities on all other tokens.</p>
<p>Baseline The baseline hypothesis is that paths through the two induction heads are unimportant, and all other paths are important. That is, $x_{c}$ is randomly sampled from $D_{r}$ and used for all paths that pass through 1.5 or 1.6. Path patching shows a mean per-token absolute loss difference of 0.702 relative to the original model.</p>
<h3>3.2 INITIAL HYPOTHESES</h3>
<p>As a running example we'll use a prompt from Carly Rae Jepsen's "Call Me Maybe":
Before you came into my life, I missed you so bad
I missed you so</p>
<p>At the first occurrence of " so", the model predicts " much"1, whereas at the second occurrence of " so" it correctly predicts that " bad" is repeated.</p>
<p>Our initial hypothesis claims that only the following paths are important to induction:</p>
<ul>
<li>The direct path to value hypothesis (Direct-V) claims the value-input to the induction heads only cares about the token at $j$ via the skip connection, and we can thus patch all paths through the layer 0 heads.</li>
<li>The direct path to query hypothesis (Direct-Q) claims the same about the query input at $i$.</li>
<li>The previous token head to key hypothesis (PTH-K) claims the key-input to the induction heads only depends on the previous token head. At this stage, we won't claim that the PTH is only a PTH, so we'll let it use the current or any prior token.</li>
</ul>
<p>The All-Initial hypothesis says the union of the paths in the three hypotheses are important (a total of $1+1+13=15$ in this example).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: PTH-K (dark blue), Direct-V (light blue, dashed), and Direct-Q (red, dot-dashed) hypotheses. In the first attention layer, only the PTH is important.</p>
<p>Note that the positional embeddings into the Q and K inputs are not shown; at a given position these don't change when we swap tokens in an intervention, so they can be considered not important. Layer normalization (LN) before an attention head is included when the head is included. For example, in Direct-Q (above) the L0 LN is excluded and the L1 LN is included.
The results of performing the corresponding path patching experiments are summarized below.</p>
<p>Table 1: Results of initial hypotheses</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hypothesis</th>
<th style="text-align: center;">Direct-V</th>
<th style="text-align: center;">Direct-Q</th>
<th style="text-align: center;">PTH-K</th>
<th style="text-align: center;">All-Initial</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Proportion explained</td>
<td style="text-align: center;">$64.1 \%$</td>
<td style="text-align: center;">$48.7 \%$</td>
<td style="text-align: center;">$53.3 \%$</td>
<td style="text-align: center;">$28.2 \%$</td>
</tr>
</tbody>
</table>
<p>Recall that on this scale, $0 \%$ means the hypothesis is as inaccurate as a hypothesis that the induction heads don't matter at all, while $100 \%$ means that for every individual example on the dataset, the hypothesis produces equal loss to the original model.</p>
<h1>3.3 First Refinement: Positional Hypotheses</h1>
<p>To the extent that a layer 0 head attends from $i$ to $i$, its output is just a linear transformation of the layer norm of the token at $i$. By visual inspection, it does appear that several attention heads have substantial attention from $i$ to $i$, implying that it's feasible for the induction heads to have adapted to reading this information in addition to reading the token embedding directly via the skip connection.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>This suggests adding 8 additional paths to the Direct-Q hypothesis of the form token $i \rightarrow$ layer 0 head $\rightarrow$ induction head to form the Positional-Query Hypothesis (Positional-Q).</p>
<p>We can test this by creating a spliced input which has the last token from the reference input and all other tokens from the counterfactual input. We then compute the value of the query on this spliced input.</p>
<p>We similarly define positional versions of the other two hypotheses:</p>
<ul>
<li>Positional-Value Hypothesis (Positional-V): we add the 8 paths: token $j \rightarrow$ layer 0 head $\rightarrow$ induction head.</li>
<li>Positional-Key Hypothesis (Positional-K): inspection of the PTH's attention patterns suggests we can try eliminating tokens earlier than $j-1$ to make our hypothesis more sparse. Then we can add the paths: token $j-1 \rightarrow$ layer 0 head $\rightarrow$ induction head.
<img alt="img-5.jpeg" src="img-5.jpeg" /></li>
</ul>
<p>Figure 6: The Positional-K (dark blue), Positional-V (light blue, dashed), and Positional-Q (red, dot-dashed) hypotheses.</p>
<p>The results (with the previous numbers for comparison) are in Table 2.</p>
<p>Table 2: Results of positional hypotheses</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Value</th>
<th style="text-align: center;">Query</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Key</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">All</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Direct-V</td>
<td style="text-align: center;">64.1\%</td>
<td style="text-align: center;">Direct-Q</td>
<td style="text-align: center;">48.7\%</td>
<td style="text-align: center;">PTH-K</td>
<td style="text-align: center;">53.5\%</td>
<td style="text-align: center;">All-Initial</td>
</tr>
<tr>
<td style="text-align: center;">Positional-V</td>
<td style="text-align: center;">86.2\%</td>
<td style="text-align: center;">Positional-Q</td>
<td style="text-align: center;">72.6\%</td>
<td style="text-align: center;">Positional-K</td>
<td style="text-align: center;">55.8\%</td>
<td style="text-align: center;">All-Positional</td>
</tr>
</tbody>
</table>
<h1>3.4 SECOND REFINEMENT: LONG INDUCTION</h1>
<p>Positional-K was not significantly improved, which suggests that information earlier than $j-1$ is in fact useful. By visual inspection, it does appear that 0.0 attends almost $100 \%$ to the previous token, but other heads (particularly 0.6 ) attend to multiple recent tokens. This suggests that a longer induction pattern like "[A1][A2][B]...[A1][A2]" $\rightarrow$ [B] might be implemented in the model by a "recent tokens head" that integrates information from both A1 and A2.</p>
<p>To test this, we add back paths to the key from previous tokens starting at $j-K$, and paths to the query starting at $i-K+1$. This gives the long positional query and key hypothesis (Long-QK). We verify that $K=3$ is the shortest window that performs well.</p>
<p>The results indicate that increasing the induction context is beneficial, but there is still room for improvement.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: the All-Long hypothesis with $K=3$</p>
<p>Table 3: Results of long hypotheses</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Query and Key</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">All</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Positional-Q + Positional-K</td>
<td style="text-align: left;">$52.5 \%$</td>
<td style="text-align: left;">All-Positional</td>
<td style="text-align: left;">$48.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Long-QK</td>
<td style="text-align: left;">$59.5 \%$</td>
<td style="text-align: left;">All-Long</td>
<td style="text-align: left;">$55.2 \%$</td>
</tr>
</tbody>
</table>
<h1>3.5 THIRD REFINEMENT: 1.5 AND REPEATING ENTITIES</h1>
<p>At this point our hypothesis contains all of the pathways that should be important for induction. One cause of the remaining unexplained effect could be that these heads are not exclusively implementing the induction behavior, as observed by Olsson et al. (2022).</p>
<p>In order to tell what these heads do that we might be missing, we'll use "path patching attribution" see where our hypothesis is unfaithful (Figure 8).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Figure 8: Path patching attribution. Tokens with red/blue highlighting are where loss was higher/lower than the original model when all paths through 1.5 not included in All-Long are considered unimportant. For "Newport Folk. . .Newport" the original model predicts a second " Folk" by induction, but the actual continuation is " Jazz". The patched model is less capable of induction, so it gets lower loss on " Jazz".</p>
<p>The patched model tends to get higher loss on tokens like " Bat" and " Newport" when they have appeared previously. We propose that head 1.5 implements both induction and a distinct "parroting" heuristic. Parroting says "for each previous token, the fact that it appeared is evidence that it is likely to repeat". Mechanistically, parroting can be implemented by having 1.5 's QK circuit attend to previous tokens in proportion to how likely they were to repeat on the training set. Attention due to induction and due to parroting are summed and softmaxed, and 1.5 's OV copies over tokens in the appropriate ratios.</p>
<p>By defining narrower datasets, we can distinguish induction and parroting. We define an "induction" subset where induction is helpful, and an "repeats" subset, where copying a token from earlier in the context is helpful (but not necessarily because of induction). For more details see Appendix C.</p>
<p>Table 4: All-Long results on subsets</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Full dataset</th>
<th style="text-align: left;">Induction subset</th>
<th style="text-align: left;">Repeats subset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AUE</td>
<td style="text-align: left;">0.244</td>
<td style="text-align: left;">0.219</td>
<td style="text-align: left;">0.777</td>
</tr>
<tr>
<td style="text-align: left;">Baseline (ATE)</td>
<td style="text-align: left;">0.432</td>
<td style="text-align: left;">0.907</td>
<td style="text-align: left;">1.444</td>
</tr>
</tbody>
</table>
<p>We do observe worse performance on the repeats subset. Inspecting the attention patterns (Appendix D) also gives evidence of attending to specific tokens in a context-independent manner. To successfully parrot, head 1.5 must be able to attend to $j$, so the paths $j \rightarrow$ (L0 attn or skip) $\rightarrow$ head 1.5's K input are important. Adding these paths gives All-Final with a proportion explained of $73.5 \%$, an increase of $18.3 \%$ over All-Long and an increase of $45.3 \%$ over All-Initial.</p>
<p>In summary, while All-Initial is very sparse, it fails to capture most of the behavior. By adding a modest number of additional paths, we can obtain a much better approximation that gives insight into where the minimal story is insufficient.</p>
<h1>4 Path Patching vs Causal Tracing and Zero Ablation on GPT-2</h1>
<p>The causal tracing method of Meng et al. (2022) can be considered a special case of path patching where the counterfactual input is sampled by adding Gaussian noise to the reference input. Another well-known technique is to zero ablate attention heads by simply skipping them in the computation (or equivalently, setting their output to zero). Either technique could be combined with our Treeify transformation, but we don't do this here.</p>
<p>In this section, we use a toy behavior to contrast the techniques and show that they can be used together to answer different questions. We give results on GPT-2 small (117M parameters) and GPT-2 XL (1.5B parameters).</p>
<p>Consider the set of prompts "The organization estimates that [N]-" where N ranges from 0 to 100. When GPT-2 models are given these prompts, by inspection ${ }^{2}$ it appears that a combination of heuristics are used such as "a number strictly larger but not hugely larger than N is likely" and "round numbers are more likely, especially if N is round".</p>
<p>Suppose we want to identify which attention heads use N to affect the output distribution. First, we run one experiment for each of the attention heads ( 144 in GPT-2 small) where all paths through that head are unimportant, and everything else (including MLPs) is important. We compute the mean KL divergence over $100\left(x_{r}, x_{c}\right)$ pairs.</p>
<p>Path patching shows fewer heads than input corruption. One reason to expect this is since the counterfactual input is as similar as possible to the reference, the network's activations stay more ondistribution. In contrast, the corrupted version of the prompt number is in general off distribution and could represent a non-number or something the network has never encountered before. Similarly, if an attention head doesn't output a zero vector when on-distribution, later heads that read from that head will also be taken off distribution. However, without a ground truth it's impossible to conclusively say which of the plots is more representative of the true mechanism.</p>
<p>We speculate that for GPT-2 small, heads 6.1 and 5.6 help recognize that the token before the hyphen is a number. If this is true, then corrupting the input to these heads would disrupt the heuristic and decrease the chance that the completion is a number, while patching a different number should not affect these heads. Table 5 shows evidence consistent with this speculation.</p>
<p>Much of the power of path patching is that it allows testing very specific claims: because we constructed a dataset where the counterfactual input is always a number, we are able to precisely exclude heads like 5.6 and 6.1 from consideration.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: GPT-2 small. For each head, darker color indicates a larger mean KL divergence from the original model when that head is ablated or patched. For casual tracing, we sample noise from $\mathcal{N}(0,0.2)$.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: As above, but for GPT-2 XL.</p>
<p>Table 5: Corrupting heads 5.6 and 6.1 decreases the probability of the completion being a number, while patching those heads has no effect. The baseline probability of a numerical completion is $88.1 \%$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Head</th>
<th style="text-align: center;">Corrupted Input</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Patched Input</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KL</td>
<td style="text-align: center;">KL given <br> #completion</td>
<td style="text-align: center;">Prob of <br> #completion</td>
<td style="text-align: center;">KL</td>
<td style="text-align: center;">KL given <br> #completion</td>
<td style="text-align: center;">Prob of <br> #completion</td>
</tr>
<tr>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">$82.5 \%$</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">$88.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">$83.9 \%$</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">$88.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">All Heads</td>
<td style="text-align: center;">2.328</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">$26.9 \%$</td>
<td style="text-align: center;">3.108</td>
<td style="text-align: center;">3.393</td>
<td style="text-align: center;">$88.1 \%$</td>
</tr>
</tbody>
</table>
<h1>5 GREEDILY BUILDING HYPOTHESES</h1>
<p>Generation of hypotheses is currently labor intensive, and in the induction result we relied on visual inspection and domain knowledge. To reduce human labor, a naive automated baseline is to greedily add heads in descending order of mean KL. A greedy algorithm is not at all optimal, and more</p>
<p>advanced techniques such as Conmy et al. (2023) should perform better, but this is relatively quick to run and hypotheses produced in this way can serve as a starting point for further refinement.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: GPT-2 small. The hypothesis "paths through all heads are unimportant except 9.1 and 8.8 " recovers $72.8 \%$ of the loss. Including 8 of the 144 heads as important recovers $98.0 \%$.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: As above, for GPT2-XL which has $48 \times 25=1200$ heads.</p>
<h1>6 RELATED WORK</h1>
<h3>6.1 CAUSAL Scrubbing</h3>
<p>Path patching is a simpler special case of causal scrubbing (Chan et al., 2022) where we make no claim about equivalence classes on nodes. Path patching experiments are more computationally efficient because only two samples $x_{c}$ and $x_{r}$ are needed, while in causal scrubbing you generally use more distinct samples.</p>
<h3>6.2 CAusal Mediation Theory</h3>
<h3>6.3 CAusal Mediation for Neural Networks</h3>
<p>Wang et al. (2022) combine mean ablation with a simple form of path patching and identify a circuit of 26 attention heads that explain GPT-2 small's ability to identify indirect objects on a synthetic dataset. Their faithfulness metric is a difference of expectations, which is susceptible to the same possibility of cancellation described in Scheurer et al. (2023).
Vig et al. (2020) also apply causal mediation analysis (Pearl, 2013) to Transformer language models. While we measure the effect of altering unimportant nodes, they measure the effect of altering important nodes. For example, in the prompt "The nurse said that", replacing "nurse" with "man" changes the completion from "she" to "he". They intervene on individual neurons or attention heads and do not consider paths.
Geiger et al. (2020) apply interchange interventions to BERT on a lexical entailment task. Interchange interventions operate on nodes instead of paths, but they are more detailed in that they specify semantic meaning for the node such as "this important node uses information about entailment". As with Vig et al. (2020), while we intervene on the unimportant paths holding the important paths constant and expect no change, interchange interventions intervene on an important node and use the semantic meaning to predict how the model output should change.
Geiger et al. (2021) apply interchange intervention to BERT on a more complex natural language inference task. They consider each example as a vertex in a graph, then adding an edge between</p>
<p>examples $\mathrm{a}, \mathrm{b}$ if the hypothesis holds both when intervening from a to b and from b to a. This identifies cliques in the graph for which the hypothesis holds fully. This avoids the complexity of measuring how approximate an abstraction is, at the cost of only applying to a very restricted set of inputs.</p>
<p>Finlayson et al. (2021) apply activation patching to study subject-verb agreement in GPT-2 and other transformers. They measure the relative probabilities of the correct and incorrect verb tenses; this means that heads only affecting other logits (or affecting correct and incorrect equally) don't have to be explained.</p>
<p>Geiger et al. (2023a) generalize causal abstraction, reframe existing methods into the causal abstraction framework, and give their own notion of approximate causal abstraction.</p>
<h1>7 DISCUSSION</h1>
<p>In our induction head investigation we found that the prefix matching and copying behaviors are relatively separate phenomena, and in particular head 1.5 in our model performs copying both with and without prefix matching. It may be that the induction heads perform additional behaviors not characterized here.</p>
<p>Compared to causal tracing and zero ablation, path patching is able to apply more precisely targeted interventions and thereby obtain a sparser abstraction.</p>
<h3>7.1 LIMITATIONS</h3>
<p>Measuring sufficiency rather than completeness. The AUE metric answers a very specific question: in expectation, how sufficient is your chosen set of paths to mediate changes in output on the given distribution? For a simplified example, suppose your behavior is implemented by majority vote of a number of identical voters. Any set of important paths containing a majority of the voters will reach 0 AUE because their agreement is sufficient to mediate the output of the vote regardless of the minority's outputs. Of course, a set of paths containing all voters will also have 0 AUE, but since this hypothesis is less parsimonious we would by default not prefer it.</p>
<p>A more realistic example would be that the voters represent correlated heuristics, whose weighted outputs feed into a saturating nonlinearity like sigmoid. Approximately the same thing happens: a set with most of the voting power will usually agree enough to saturate the sigmoid in either direction. Then this compact set achieves low AUE already and compares favorably to the set with all voters and marginally lower AUE. Therefore, it's important to interpret AUE-based metrics in terms of sufficiency and not completeness.</p>
<p>No evidence about downstream tasks. While this work is motivated by finding abstractions suitable for downstream tasks, in this work we don't establish the connection between AUE and specific applications. Tay et al. (2022) found that for language models, pretraining perplexity is related to downstream performance, but that other factors like model shape also matter. Similarly, we expect AUE to be incomplete as a predictor of abstraction quality.</p>
<p>Path patching makes no claims outside the tested distribution. We consider paths that are approximately zero on the data distribution to be unimportant, as well as sets of paths that approximately cancel. The reason these contributions are small is often contingent on properties of the distribution, so low AUE doesn't imply low AUE on a wider distribution where those properties don't hold. This is comparable to training a model: low loss on some training distribution doesn't imply low loss off-distribution. Traditional techniques like an appropriate inductive bias, increasing dataset diversity, and use of regularization could increase the chances of generalization; other possible avenues are designing architectures to be easier to interpret in the first place or incorporating incentives for interpretability into the training process (Geiger et al., 2022; Chen et al., 2019; Elhage et al., 2022)</p>
<p>Path patching cannot reject all false hypotheses. Even if our dataset is the full distribution of interest, we can still fail to reject false hypotheses for reasons different than a too-narrow dataset:</p>
<ul>
<li>Some metrics, e.g. difference in expected loss, are susceptible to cancellation across examples: if the loss is higher on some examples and lower on others, these discrepancies can cancel. This is why we recommend using metrics that don't suffer from this, like average KL divergence.</li>
<li>AUE must be estimated from a finite number of samples in practice; if the computational budget is small relative to the diversity of the distribution, then naive sampling can fail to sample inputs that are rare but have a large unexplained effect.</li>
</ul>
<p>Path patching alone cannot definitively prove hypotheses. Given a correct hypothesis, proving it is correct with path patching would require testing all possible inputs, which is infeasible on large networks. We see path patching as providing evidence complementary with other lines of evidence like labor-intensive mechanistic analysis and other interpretability techniques.</p>
<p>Narrow scope of demonstrations. Additional experiments are needed to characterize the effectiveness of the methodology on a diverse set of tasks and models. The results presented are narrow in scope compared to the full range of behaviors exhibited by language models.</p>
<h1>7.2 Future Work</h1>
<p>Scaling To State of the Art Models. We have used path patching at the 1.5B parameter scale, but state of the art models are still orders of magnitude larger. Many interpretability techniques have only been applied to small networks (Räuker et al., 2023), and it remains to be shown that path patching scales to the largest models.</p>
<p>Detecting Distribution Shift. One downstream application is the use of interpretability to detect distribution shift. Suppose that a given behavior is consistently explained by some hypothesis during training, but in deployment we observe that the same behavior is no longer explained by that hypothesis. Even if the actual outputs are indistinguishable, we could flag that the output is now produced for a different "reason" and investigate the anomaly further.</p>
<p>Producing Adversarial Examples. It should be possible to produce adversarial examples based on knowledge about model mechanisms. It would be useful to correlate the AUE metric with downstream performance on this task: how accurate do explanations need to be to produce adversarial examples in this way?</p>
<p>Automating Hypothesis Search. Given a computational graph, it's straightforward to search over combinations of paths and automatically find candidates with favorable combinations of low AUE and sparseness. Ground truth labels provided by a tool like Tracr (Lindner et al., 2023) could be used to benchmark search methods.</p>
<h3>7.3 CONCLUSION</h3>
<p>As neural networks continue to grow in capabilities, it is increasingly important to rigorously characterize their behavior. Path patching is an expressive formalism for localization claims that is both principled and sufficiently efficient to run on real models.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We would like to thank Stephen Casper, Jenny Nitishinskaya, Nate Thomas, Fabien Roger, Kshitij Sachan, Buck Shlegeris, and Ben Toner for feedback on a draft of this paper.</p>
<h2>REFERENCES</h2>
<p>Sander Beckers, Frederick Eberhardt, and Joseph Y Halpern. Approximate causal abstractions. In Uncertainty in Artificial Intelligence, pp. 606-615. PMLR, 2020.</p>
<p>Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens, 2023.</p>
<p>Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Viégas, and Martin Wattenberg. An interpretability illusion for bert. arXiv preprint arXiv:2104.07143, 2021.</p>
<p>Lawrence Chan, Adrià Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. Causal scrubbing: a method for rigorously testing interpretability hypotheses., 2022. URL https://bit.ly/3WRBhPD.</p>
<p>Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks like that: deep learning for interpretable image recognition. Advances in neural information processing systems, 32, 2019.</p>
<p>Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations, 2023.</p>
<p>Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability, 2023.</p>
<p>Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html.</p>
<p>Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christopher Olah. Softmax linear units. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/solu/index.html.</p>
<p>Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan Belinkov. Causal analysis of syntactic agreement mechanisms in neural language models. arXiv preprint arXiv:2106.06087, 2021.</p>
<p>Atticus Geiger, Kyle Richardson, and Christopher Potts. Neural natural language inference models partially embed theories of lexical entailment and negation. arXiv preprint arXiv:2004.14623, 2020.</p>
<p>Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural networks. Advances in Neural Information Processing Systems, 34:9574-9586, 2021.</p>
<p>Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Goodman, and Christopher Potts. Inducing causal structure for interpretable neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 7324-7338. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/geiger22a.html.</p>
<p>Atticus Geiger, Chris Potts, and Thomas Icard. Causal abstraction for faithful model interpretation. arXiv preprint arXiv:2301.04709, 2023a.</p>
<p>Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D Goodman. Finding alignments between interpretable causal variables and distributed neural representations. arXiv preprint arXiv:2303.02536, 2023b.</p>
<p>David Lindner, János Kramár, Matthew Rahtz, Thomas McGrath, and Vladimir Mikulik. Tracr: Compiled transformers as a laboratory for interpretability, 2023.</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372, 2022.</p>
<p>Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 5(3):e00024-001, 2020.</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, T. J. Henighan, Benjamin Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, John Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom B. Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Christopher Olah. In-context learning and induction heads. ArXiv, abs/2209.11895, 2022.</p>
<p>Judea Pearl. Direct and indirect effects. CoRR, abs/1301.2300, 2013. URL http://arxiv. org/abs/1301.2300.</p>
<p>Ofir Press, Noah A Smith, and Mike Lewis. Shortformer: Better language modeling using shorter inputs. arXiv preprint arXiv:2012.15832, 2020.</p>
<p>Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent ai: A survey on interpreting the inner structures of deep neural networks, 2023.</p>
<p>Jérémy Scheurer Scheurer, Phil3, tony, Jacques Thibodeau, and David Lindner. Practical pitfalls of causal scrubbing, 2023. URL https://www.alignmentforum.org/posts/ DFarDnQjMnjsKvW8s/practical-pitfalls-of-causal-scrubbing.</p>
<p>Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efficiently: Insights from pre-training and fine-tuning transformers, 2022.</p>
<p>Andreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles of relatively shallow networks. Advances in neural information processing systems, 29, 2016.</p>
<p>Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. Investigating gender bias in language models using causal mediation analysis. Advances in neural information processing systems, 33:12388-12401, 2020.</p>
<p>Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small, 2022.</p>
<h1>A WHY REUSE THE SAME COUNTERFACTUAL INPUT?</h1>
<p>Reuse allows rewrites to preserve unimportance. If a node is unimportant and we rewrite it into any number of nodes that together add to the original node, then intuitively the new nodes considered together should also be unimportant. This can only be guaranteed by using the same $x_{c}$ for both new nodes.</p>
<p>Reuse does the right thing for additive ensembles. Residual networks often act like ensembles, where a behavior is implemented by combining contributions from multiple distributed components (Veit et al., 2016). In particular, using dropout during training incentivizes this because then the behavior will degrade gradually instead of abruptly when a fraction of contributions are set to zero.</p>
<p>Reuse allows paths that cancel across the domain to be unimportant. Suppose that in the figure below, $f_{0}(x) \approx-x$ on the domain $D_{r}$. When we sample $x_{c} \sim D_{r}, x_{c} \sim D_{r}$, testing each path individually rejects the claim of unimportance - clearly they each individually mediate the output.
However, if we consider both paths together, they approximately cancel out - intuitively they are not important anymore. Concretely, this could happen if $f_{0}$ learns to erase information in $x$ that is irrelevant or harmful to predictions on $D_{r}$. Note that we can only exclude both paths from our explanation because we are restricting our claims to $D_{r}$ - we wouldn't expect this simplification to hold on arbitrary inputs.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: Left: suppose $f_{0}, f_{1}$, and $f_{2}$ all compute the same function $f$ and then V takes the mean of its 3 inputs. Center: reusing the same $x_{c}$ means that V will output $f\left(x_{c}\right)$, which matches our intuition and is on distribution whenever $x_{c}$ is on distribution. Right: using distinct $x_{c} i$ means V outputs $\left(f\left(x_{c} 1\right)+f\left(x_{c} 2\right)+f\left(x_{c} 3\right)\right) / 3$, which is off distribution in general and can cause unusual behavior later in the network.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: For the graph on the left, tests that check if (a) $x \rightarrow f_{0} \rightarrow Y$, (b) $x \rightarrow Y$, and (c) both together are unimportant.</p>
<p>Reuse is computationally efficient. By reusing $x_{c}$, we are able to reuse sub-expressions that use $x_{c}$. For example, suppose our hypothesis for the above network was that was that only $x \rightarrow A \rightarrow f_{1}$ is important. To test this we evaluate:</p>
<p>$$
G_{T}\left(x_{c}, x_{c}, x_{r}, x_{c}\right)=f_{1}\left(f_{0}\left(x_{c}\right)+x_{r}\right)+f_{0}\left(x_{c}\right)+x_{c}
$$</p>
<p>In this case our software would compute the repeated term $f_{0}\left(x_{c}\right)$ only once and cache it; if the two terms were $f_{0}\left(x_{c 0}\right)$ and $f_{0}\left(x_{c 1}\right)$ this would not be possible. For a residual network, the number of paths grows exponentially in network depth and it's beneficial to cache as much as possible when working with large models.</p>
<h1>B IDENTIFYING PREVIOUS TOKEN AND INDUCTION HEADS</h1>
<p>Consider the string "Mr. Dursley, Mrs. Dursley, Dudley Dursley" and let the number of tokens in the tokenized string be N. Because " Dursley" is tokenized as " D", "urs", "ley", we would expect an induction head to attend to the first occurrence of "urs" from the second and third occurrences of " D", the first occurrence of "ley" from the second and third occurrences of "urs", etc.</p>
<p>For each attention head, we plot a NxN heatmap, where the i-th row represents the attention pattern over all tokens when the i-th token is the query and the j -th column represents the attention probabilities given to the j -th token as we vary the query token. Induction heads would then exhibit short diagonal patterns where the rows and columns are two distinct occurrences of the token sequence " D", "urs", "ley".</p>
<p>To identify the previous-token head, we look at the corresponding attention pattern plots in the 0 -th layer. The pattern of interest here is simply a reliable diagonal line running immediately below the main diagonal of the plot (for each query token, the previous-token head should attend primarily to the immediately preceding token). We find that head 0.0 fits the description:</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: Layer 1 heads. Heads 1.5 and 1.6 exhibit clear induction patterns.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 16: Layer 0 heads. Head 0.0 is (primarily) a previous token head.</p>
<h1>C DEFINING THE INDUCTION AND UNCOMMON REPEAT SUBSETS</h1>
<p>The induction subset is the same as in Chan et al. (2022).
See
https://www.lesswrong.com/s/h95ayYYwMebGEYN5y/p/ j6s9H9SHrEhEfuJnq#How_we_picked_the_subset_of_tokens for a full description.</p>
<p>For the uncommon repeat subset, we include a token if it previously occurred in the context, and the token is not one of the 200 most common tokens in the validation set. We observed the results to be robust to varying the number of tokens filtered.</p>
<h1>D Repeated Attention to Proper NOUns</h1>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Path patching attribution when patching head 1.5 .
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>The attention pattern of head 1.5 on an example that illustrates repeated attention to certain proper nouns such as " Brazil" and " UFC". Attention paid to the first token "[BEGIN]" is not shown.</p>
<h2>E MODEL REWRITES</h2>
<p>Since the space of paths available depends on the exact structure of the computation graph, we often want to rewrite the graph to one that allows more fine-grained localization while preserving the behavior of $G$ on all inputs.</p>
<p>For example, we divide an attention layer into attention heads - now we can say that only some heads in a layer matter.</p>
<p>Similarly, we can divide a vector of tokens into slices so we can say that only some tokens matter.
The residual rewrite can be used to generalize subspaces and mean ablation.
Rewriting an activation as its projection onto a subspace and the remainder means you can check if each part is important individually.</p>
<p>Rewriting an activation into a constant value (specifically a mean) and the remainder. The constant is unimportant by definition (you can only resample it with the same constant), but then you can say the deviation is or isn't important.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Explore the behavior interactively at: https://modelbehavior.ngrok.io/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7766 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7766</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7766</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-277999781</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.16728v2.pdf" target="_blank">IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery</a></p>
                <p><strong>Paper Abstract:</strong> The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7766.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7766.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCTS (IRIS adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Tree Search (adapted for research ideation in IRIS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of Monte Carlo Tree Search where the LLM-based Review Agent provides reward estimates to guide exploration/exploitation of the space of research briefs; actions include generate, refine w/ retrieval, refine w/ review, and refine w/ user feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.0-Flash (used as core LLM for ideation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Gemini-2.0-Flash (version; parameter count not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-disciplinary scientific ideation (examples: AI/NLP, Chemistry, Physics, HCI)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis / research brief generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MCTS with LLM-based Review Agent (UCT-guided tree search)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Build a search tree rooted on the research goal; states encapsulate the current research brief, reward estimate, review feedback, and retrieved knowledge. SELECT uses UCT, EVALUATE obtains reward via Review Agent, EXPAND applies action set A, BACKPROPAGATE updates Q and N. Exploration constant c and max depth control tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average per-node reward (Q/N) aggregated from Review Agent scores used as MCTS reward; UCT score for node selection</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Node average reward Q/N (unit: same scoring scale produced by Review Agent, aggregated averages used to pick BESTCHILD); UCT(n) = Q(n)/N(n) + c * sqrt(ln N(n_p)/N(n)) where c is exploration constant</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Retrieval uses AI2 Scholar QA / Semantic Scholar (200M open access papers); no external benchmark used to evaluate MCTS itself</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human researchers steer or verify fine-grained feedback during runs; user study N=8 researchers (10 case studies) interacted with MCTS-guided ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Interaction with MCTS (tree depth 3) increased average absolute LLM-as-a-judge scores by +0.5 points and ELO ratings by +12 points (reported for IRIS).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM-generated briefs produced under IRIS/MCTS were compared to baseline LLM generations and human preference rankings (see reported correlations and improvements); specific numeric comparisons reported via ELO/absolute metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Scientific ideation quality is subjective; MCTS relies on LLM-based Review Agent as a proxy judge which can be gamed unless human-verified; computationally intensive (budget controls needed); assumes domain expertise of researcher to verify feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7766.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7766.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Review Taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Review Taxonomy for fine-grained feedback (aspect-based critique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical set of evaluation aspects (Originality, Assumptions, Clarity, Feasibility, Effectiveness, Impact, Ethical/Social considerations) broken into sub-aspects used by the Review Agent to produce targeted, segment-level feedback and scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.0-Flash (Review Agent uses LLM capabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Gemini-2.0-Flash (version; parameter count not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-disciplinary scientific ideation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis / research brief evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Aspect-based fine-grained review scoring (hierarchical taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The Review Agent evaluates distinct segments of a research brief with respect to taxonomy aspects (e.g., Originality: Lack of Novelty; Feasibility: Practicality; Effectiveness: Evaluation and Validation Issues), produces targeted feedback, and computes rewards by averaging scores across verified aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-aspect scores aggregated (averaged) into an overall reward used for MCTS and reported as LLM-as-a-judge absolute scores or used in head-to-head comparisons for ELO.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Per-aspect numeric scores (paper implies use of the same numeric scale as absolute scoring, e.g., 1–10) which are averaged across verified aspects to produce a composite reward; exact per-aspect scale not explicitly enumerated beyond aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>No external benchmark; taxonomy applied to generated research briefs within IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Fine-grained feedback is verified (accepted/omitted) by the researcher before being used to compute reward; user study N=8 showed users found feedback insightful (Table 1 ratings).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Users rated usefulness of fine-grained feedback 4.3 ± 0.7 (1–5 Likert). Feedback matched user's own concerns in 87.5% of users and sparked novel insights in 50% of cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on researchers to verify relevance of fine-grained feedback; taxonomy-driven scoring depends on LLM scoring fidelity and available context; does not by itself test empirical reproducibility of proposed hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7766.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7766.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge (Absolute & ELO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge evaluation using absolute scores and ELO ratings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated evaluation procedure where LLMs assign absolute plausibility/quality scores (1–10) to hypotheses and/or conduct head-to-head comparisons whose results are aggregated into ELO ratings; used in prior work and adopted here for automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with multiple LLMs (Gemini-2.0-Flash, ChatGPT, ChatGPT+Search, Claude 3.5 Haiku)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Gemini-2.0-Flash (version); ChatGPT/Claude versions as referenced (exact parameter counts not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-disciplinary scientific ideation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis / research brief evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-as-a-judge with absolute scoring and head-to-head ELO aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LLMs rate generated research briefs on an absolute numeric scale (1–10). For relative evaluation, LLMs perform pairwise/head-to-head comparisons and preferences are used to compute ELO ratings reflecting relative quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Absolute score (1–10) and ELO rating (derived from head-to-head preference aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Absolute score: integer or real value on a 1–10 scale per hypothesis; ELO: standard Elo rating system aggregated from pairwise comparison outcomes (units: Elo points).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>No external benchmark; applied to generated briefs and baseline LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human users performed blind rankings and also rated generations; correlation between human baseline rankings and LLM-based metrics was computed (Pearson r).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Human baseline rankings correlated with LLM-based ELO scores (Pearson's r = 0.60) and weakly with LLM absolute scores (r = 0.45). IRIS interaction increased average absolute scores by +0.5 and ELO by +12 points (depth 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Moderate alignment between human preferences and LLM ELO (r=0.60); weaker alignment with LLM absolute scores (r=0.45), motivating preference for ELO display.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM-as-a-judge can be gamed (reward hacking) and may not align strongly with human judgments; coarse absolute scores can be less reliable than relative ELO aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7766.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7766.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI2 ScholarQA / Semantic Scholar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI2 Scholar QA pipeline over the Semantic Scholar corpus (retrieval backend)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retrieval pipeline that uses AI2 Scholar QA over the Semantic Scholar corpus (200M open access papers) to extract and re-rank relevant passages, cluster by paper/section, and produce cited, section-wise summaries to ground ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retrieval pipeline used to augment Gemini-2.0-Flash ideation agent</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-disciplinary literature grounding</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>literature grounding / evidence retrieval for hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Query-based retrieval with two-stage retrieval, re-ranking, clustering and summarized cited reports</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate targeted queries for the research goal, use Ai2 Scholar QA snippet search to extract passages, re-rank top-k passages aggregated at paper level, extract quotes, cluster passages, then generate cited section-wise summaries for the ideation agent.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>User-rated quality of literature summaries (Likert), qualitative grounding of ideas</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>User ratings reported on 1–5 Likert scale (Table 1); example reported mean 3.7 ± 0.8 for quality of literature summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Semantic Scholar open corpus (snippet search API); optional user-uploaded PDFs parsed via Grobid doc2json</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Users evaluated retrieval-augmented summaries; domain-dependent quality noted (lower in chemistry/physics due to corpus gaps).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Quality of literature summaries rated 3.7 ± 0.8 (1–5 Likert); retrieval facilitated grounding but quality varied by domain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Naive RAG can fail on global corpus questions; semantic scholar coverage varies across domains causing lower-quality retrievals for some fields; retrieval quality affects downstream ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7766.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7766.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>User Study (blind ranking + Likert)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>User study with blind ranking, interactive ideation sessions, and post-task Likert surveys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human evaluation protocol involving 8 researchers (10 sessions), blind ranking of initial hypotheses, interactive use of IRIS, and post-task Likert-scale surveys to measure usability, steerability, feedback usefulness, and perceived hypothesis improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated generated outputs from Gemini-2.0-Flash and baseline LLMs (ChatGPT, ChatGPT+Search, Claude 3.5 Haiku)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-disciplinary</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis / research brief evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Blind ranking + interactive session + post-task Likert questionnaire</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Participants (researchers) define a research goal, blindly rank initial hypotheses, interact with IRIS to refine briefs, then complete a post-task survey including Likert ratings (1–5) and qualitative feedback; blind rankings used to compute baseline human preferences and correlations with automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Blind ranking aggregated into preference orderings; Likert-scale ratings for system features; reported Pearson correlations between human and LLM metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Likert ratings on 1–5 scale (Table 1); blind ranking converted to comparative data and correlated with LLM ELO and absolute scores (Pearson r).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>User-provided research goals and case studies (no external dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>N=8 researchers (10 case studies with two participants twice), ~60 minute sessions, steps: define goal, blind rank initial set, interact with IRIS, post-task survey. Reported feature ratings: Fine-grained feedback 4.3±0.7, MCTS tree 4.2±0.6, Lit summaries 3.7±0.8, Usability 4.5±0.7, Overall satisfaction 3.9±0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>25% users found hypothesis substantially better post-interaction, 50% marginally better, 25% similar; qualitative metrics: critiques matched user concerns in 87.5% and sparked novel insights in 50% of cases; Pearson r between human rankings and LLM ELO = 0.60.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Human rankings used as baseline to validate alignment of automated metrics; moderate correlation found with LLM ELO (r=0.60) and weaker with absolute scores (r=0.45).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small sample size (N=8), domain coverage variability, reliance on researcher expertise to verify feedback, and limited exploration of frontier LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7766.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7766.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline LLMs used for comparison: Gemini-2.0-Flash; ChatGPT; ChatGPT w/ search; Claude 3.5 Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of LLMs prompted to generate novel research briefs to compare IRIS outputs; their generations were evaluated by humans and LLM-as-a-judge (absolute scores and ELO).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.0-Flash; ChatGPT; ChatGPT + search; Claude 3.5 Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Versions referenced (exact parameter counts not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-disciplinary</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis / research brief generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Baseline generation compared via LLM-as-a-judge and human blind ranking</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt baselines to generate novel research briefs; have humans and LLM judges rate and rank outputs (absolute 1–10 and pairwise preferences for ELO).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Absolute scores (1–10) and ELO ratings derived from pairwise preferences; human blind rankings correlated against these metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Absolute score: 1–10 plausibility/quality rating; ELO: relative rating points from pairwise comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Generated briefs from each baseline given user research goals (no external corpus for evaluation other than retrieval augmentations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Same user study participants and blind ranking protocol applied to baseline outputs; correlations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Comparative plots shown (Figure 4) for absolute scores and ELO across baselines; specific numerical differences not exhaustively tabulated beyond correlations and overall IRIS improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Baseline LLM outputs were rated and compared; human-LLM metric alignment results motivated using ELO as preferred display metric (moderate correlation with human ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Paper reports limited set of baselines and did not test frontier LLMs (e.g., Claude 3.7 Sonnet, Grok-3) due to budget constraints; model parameter sizes not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>Can large language models unlock novel scientific research ideas? <em>(Rating: 2)</em></li>
                <li>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback <em>(Rating: 2)</em></li>
                <li>Bandit based monte-carlo planning <em>(Rating: 2)</em></li>
                <li>Bandit algorithms for tree search <em>(Rating: 2)</em></li>
                <li>Litsearch: A retrieval benchmark for scientific literature search <em>(Rating: 2)</em></li>
                <li>Openscholar: Synthesizing scientific literature with retrieval-augmented lms <em>(Rating: 1)</em></li>
                <li>Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7766",
    "paper_id": "paper-277999781",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "MCTS (IRIS adaptation)",
            "name_full": "Monte Carlo Tree Search (adapted for research ideation in IRIS)",
            "brief_description": "An adaptation of Monte Carlo Tree Search where the LLM-based Review Agent provides reward estimates to guide exploration/exploitation of the space of research briefs; actions include generate, refine w/ retrieval, refine w/ review, and refine w/ user feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini-2.0-Flash (used as core LLM for ideation)",
            "model_size": "Gemini-2.0-Flash (version; parameter count not specified)",
            "scientific_domain": "multi-disciplinary scientific ideation (examples: AI/NLP, Chemistry, Physics, HCI)",
            "theory_type": "hypothesis / research brief generation",
            "evaluation_method_name": "MCTS with LLM-based Review Agent (UCT-guided tree search)",
            "evaluation_method_description": "Build a search tree rooted on the research goal; states encapsulate the current research brief, reward estimate, review feedback, and retrieved knowledge. SELECT uses UCT, EVALUATE obtains reward via Review Agent, EXPAND applies action set A, BACKPROPAGATE updates Q and N. Exploration constant c and max depth control tradeoffs.",
            "evaluation_metric": "Average per-node reward (Q/N) aggregated from Review Agent scores used as MCTS reward; UCT score for node selection",
            "metric_definition": "Node average reward Q/N (unit: same scoring scale produced by Review Agent, aggregated averages used to pick BESTCHILD); UCT(n) = Q(n)/N(n) + c * sqrt(ln N(n_p)/N(n)) where c is exploration constant",
            "dataset_or_benchmark": "Retrieval uses AI2 Scholar QA / Semantic Scholar (200M open access papers); no external benchmark used to evaluate MCTS itself",
            "human_evaluation_details": "Human researchers steer or verify fine-grained feedback during runs; user study N=8 researchers (10 case studies) interacted with MCTS-guided ideation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Interaction with MCTS (tree depth 3) increased average absolute LLM-as-a-judge scores by +0.5 points and ELO ratings by +12 points (reported for IRIS).",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM-generated briefs produced under IRIS/MCTS were compared to baseline LLM generations and human preference rankings (see reported correlations and improvements); specific numeric comparisons reported via ELO/absolute metrics.",
            "limitations_noted": "Scientific ideation quality is subjective; MCTS relies on LLM-based Review Agent as a proxy judge which can be gamed unless human-verified; computationally intensive (budget controls needed); assumes domain expertise of researcher to verify feedback.",
            "uuid": "e7766.0",
            "source_info": {
                "paper_title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Review Taxonomy",
            "name_full": "Hierarchical Review Taxonomy for fine-grained feedback (aspect-based critique)",
            "brief_description": "A hierarchical set of evaluation aspects (Originality, Assumptions, Clarity, Feasibility, Effectiveness, Impact, Ethical/Social considerations) broken into sub-aspects used by the Review Agent to produce targeted, segment-level feedback and scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini-2.0-Flash (Review Agent uses LLM capabilities)",
            "model_size": "Gemini-2.0-Flash (version; parameter count not specified)",
            "scientific_domain": "multi-disciplinary scientific ideation",
            "theory_type": "hypothesis / research brief evaluation",
            "evaluation_method_name": "Aspect-based fine-grained review scoring (hierarchical taxonomy)",
            "evaluation_method_description": "The Review Agent evaluates distinct segments of a research brief with respect to taxonomy aspects (e.g., Originality: Lack of Novelty; Feasibility: Practicality; Effectiveness: Evaluation and Validation Issues), produces targeted feedback, and computes rewards by averaging scores across verified aspects.",
            "evaluation_metric": "Per-aspect scores aggregated (averaged) into an overall reward used for MCTS and reported as LLM-as-a-judge absolute scores or used in head-to-head comparisons for ELO.",
            "metric_definition": "Per-aspect numeric scores (paper implies use of the same numeric scale as absolute scoring, e.g., 1–10) which are averaged across verified aspects to produce a composite reward; exact per-aspect scale not explicitly enumerated beyond aggregation.",
            "dataset_or_benchmark": "No external benchmark; taxonomy applied to generated research briefs within IRIS",
            "human_evaluation_details": "Fine-grained feedback is verified (accepted/omitted) by the researcher before being used to compute reward; user study N=8 showed users found feedback insightful (Table 1 ratings).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Users rated usefulness of fine-grained feedback 4.3 ± 0.7 (1–5 Likert). Feedback matched user's own concerns in 87.5% of users and sparked novel insights in 50% of cases.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Relies on researchers to verify relevance of fine-grained feedback; taxonomy-driven scoring depends on LLM scoring fidelity and available context; does not by itself test empirical reproducibility of proposed hypotheses.",
            "uuid": "e7766.1",
            "source_info": {
                "paper_title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLM-as-a-judge (Absolute & ELO)",
            "name_full": "LLM-as-a-judge evaluation using absolute scores and ELO ratings",
            "brief_description": "Automated evaluation procedure where LLMs assign absolute plausibility/quality scores (1–10) to hypotheses and/or conduct head-to-head comparisons whose results are aggregated into ELO ratings; used in prior work and adopted here for automated evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Used with multiple LLMs (Gemini-2.0-Flash, ChatGPT, ChatGPT+Search, Claude 3.5 Haiku)",
            "model_size": "Gemini-2.0-Flash (version); ChatGPT/Claude versions as referenced (exact parameter counts not specified)",
            "scientific_domain": "multi-disciplinary scientific ideation",
            "theory_type": "hypothesis / research brief evaluation",
            "evaluation_method_name": "LLM-as-a-judge with absolute scoring and head-to-head ELO aggregation",
            "evaluation_method_description": "LLMs rate generated research briefs on an absolute numeric scale (1–10). For relative evaluation, LLMs perform pairwise/head-to-head comparisons and preferences are used to compute ELO ratings reflecting relative quality.",
            "evaluation_metric": "Absolute score (1–10) and ELO rating (derived from head-to-head preference aggregation)",
            "metric_definition": "Absolute score: integer or real value on a 1–10 scale per hypothesis; ELO: standard Elo rating system aggregated from pairwise comparison outcomes (units: Elo points).",
            "dataset_or_benchmark": "No external benchmark; applied to generated briefs and baseline LLM outputs",
            "human_evaluation_details": "Human users performed blind rankings and also rated generations; correlation between human baseline rankings and LLM-based metrics was computed (Pearson r).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Human baseline rankings correlated with LLM-based ELO scores (Pearson's r = 0.60) and weakly with LLM absolute scores (r = 0.45). IRIS interaction increased average absolute scores by +0.5 and ELO by +12 points (depth 3).",
            "comparison_to_human_generated": true,
            "comparison_results": "Moderate alignment between human preferences and LLM ELO (r=0.60); weaker alignment with LLM absolute scores (r=0.45), motivating preference for ELO display.",
            "limitations_noted": "LLM-as-a-judge can be gamed (reward hacking) and may not align strongly with human judgments; coarse absolute scores can be less reliable than relative ELO aggregation.",
            "uuid": "e7766.2",
            "source_info": {
                "paper_title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "AI2 ScholarQA / Semantic Scholar",
            "name_full": "AI2 Scholar QA pipeline over the Semantic Scholar corpus (retrieval backend)",
            "brief_description": "Retrieval pipeline that uses AI2 Scholar QA over the Semantic Scholar corpus (200M open access papers) to extract and re-rank relevant passages, cluster by paper/section, and produce cited, section-wise summaries to ground ideation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Retrieval pipeline used to augment Gemini-2.0-Flash ideation agent",
            "model_size": null,
            "scientific_domain": "multi-disciplinary literature grounding",
            "theory_type": "literature grounding / evidence retrieval for hypotheses",
            "evaluation_method_name": "Query-based retrieval with two-stage retrieval, re-ranking, clustering and summarized cited reports",
            "evaluation_method_description": "Generate targeted queries for the research goal, use Ai2 Scholar QA snippet search to extract passages, re-rank top-k passages aggregated at paper level, extract quotes, cluster passages, then generate cited section-wise summaries for the ideation agent.",
            "evaluation_metric": "User-rated quality of literature summaries (Likert), qualitative grounding of ideas",
            "metric_definition": "User ratings reported on 1–5 Likert scale (Table 1); example reported mean 3.7 ± 0.8 for quality of literature summaries.",
            "dataset_or_benchmark": "Semantic Scholar open corpus (snippet search API); optional user-uploaded PDFs parsed via Grobid doc2json",
            "human_evaluation_details": "Users evaluated retrieval-augmented summaries; domain-dependent quality noted (lower in chemistry/physics due to corpus gaps).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Quality of literature summaries rated 3.7 ± 0.8 (1–5 Likert); retrieval facilitated grounding but quality varied by domain.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Naive RAG can fail on global corpus questions; semantic scholar coverage varies across domains causing lower-quality retrievals for some fields; retrieval quality affects downstream ideation.",
            "uuid": "e7766.3",
            "source_info": {
                "paper_title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "User Study (blind ranking + Likert)",
            "name_full": "User study with blind ranking, interactive ideation sessions, and post-task Likert surveys",
            "brief_description": "Human evaluation protocol involving 8 researchers (10 sessions), blind ranking of initial hypotheses, interactive use of IRIS, and post-task Likert-scale surveys to measure usability, steerability, feedback usefulness, and perceived hypothesis improvement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluated generated outputs from Gemini-2.0-Flash and baseline LLMs (ChatGPT, ChatGPT+Search, Claude 3.5 Haiku)",
            "model_size": null,
            "scientific_domain": "multi-disciplinary",
            "theory_type": "hypothesis / research brief evaluation",
            "evaluation_method_name": "Blind ranking + interactive session + post-task Likert questionnaire",
            "evaluation_method_description": "Participants (researchers) define a research goal, blindly rank initial hypotheses, interact with IRIS to refine briefs, then complete a post-task survey including Likert ratings (1–5) and qualitative feedback; blind rankings used to compute baseline human preferences and correlations with automated metrics.",
            "evaluation_metric": "Blind ranking aggregated into preference orderings; Likert-scale ratings for system features; reported Pearson correlations between human and LLM metrics.",
            "metric_definition": "Likert ratings on 1–5 scale (Table 1); blind ranking converted to comparative data and correlated with LLM ELO and absolute scores (Pearson r).",
            "dataset_or_benchmark": "User-provided research goals and case studies (no external dataset)",
            "human_evaluation_details": "N=8 researchers (10 case studies with two participants twice), ~60 minute sessions, steps: define goal, blind rank initial set, interact with IRIS, post-task survey. Reported feature ratings: Fine-grained feedback 4.3±0.7, MCTS tree 4.2±0.6, Lit summaries 3.7±0.8, Usability 4.5±0.7, Overall satisfaction 3.9±0.7.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "25% users found hypothesis substantially better post-interaction, 50% marginally better, 25% similar; qualitative metrics: critiques matched user concerns in 87.5% and sparked novel insights in 50% of cases; Pearson r between human rankings and LLM ELO = 0.60.",
            "comparison_to_human_generated": true,
            "comparison_results": "Human rankings used as baseline to validate alignment of automated metrics; moderate correlation found with LLM ELO (r=0.60) and weaker with absolute scores (r=0.45).",
            "limitations_noted": "Small sample size (N=8), domain coverage variability, reliance on researcher expertise to verify feedback, and limited exploration of frontier LLMs.",
            "uuid": "e7766.4",
            "source_info": {
                "paper_title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Baselines (LLMs)",
            "name_full": "Baseline LLMs used for comparison: Gemini-2.0-Flash; ChatGPT; ChatGPT w/ search; Claude 3.5 Haiku",
            "brief_description": "Set of LLMs prompted to generate novel research briefs to compare IRIS outputs; their generations were evaluated by humans and LLM-as-a-judge (absolute scores and ELO).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.0-Flash; ChatGPT; ChatGPT + search; Claude 3.5 Haiku",
            "model_size": "Versions referenced (exact parameter counts not specified in paper)",
            "scientific_domain": "multi-disciplinary",
            "theory_type": "hypothesis / research brief generation",
            "evaluation_method_name": "Baseline generation compared via LLM-as-a-judge and human blind ranking",
            "evaluation_method_description": "Prompt baselines to generate novel research briefs; have humans and LLM judges rate and rank outputs (absolute 1–10 and pairwise preferences for ELO).",
            "evaluation_metric": "Absolute scores (1–10) and ELO ratings derived from pairwise preferences; human blind rankings correlated against these metrics.",
            "metric_definition": "Absolute score: 1–10 plausibility/quality rating; ELO: relative rating points from pairwise comparisons.",
            "dataset_or_benchmark": "Generated briefs from each baseline given user research goals (no external corpus for evaluation other than retrieval augmentations)",
            "human_evaluation_details": "Same user study participants and blind ranking protocol applied to baseline outputs; correlations reported.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Comparative plots shown (Figure 4) for absolute scores and ELO across baselines; specific numerical differences not exhaustively tabulated beyond correlations and overall IRIS improvements.",
            "comparison_to_human_generated": true,
            "comparison_results": "Baseline LLM outputs were rated and compared; human-LLM metric alignment results motivated using ELO as preferred display metric (moderate correlation with human ranking).",
            "limitations_noted": "Paper reports limited set of baselines and did not test frontier LLMs (e.g., Claude 3.7 Sonnet, Grok-3) due to budget constraints; model parameter sizes not provided.",
            "uuid": "e7766.5",
            "source_info": {
                "paper_title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "Can large language models unlock novel scientific research ideas?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_unlock_novel_scientific_research_ideas"
        },
        {
            "paper_title": "Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback",
            "rating": 2,
            "sanitized_title": "ideasynth_iterative_research_idea_development_through_evolving_and_composing_idea_facets_with_literaturegrounded_feedback"
        },
        {
            "paper_title": "Bandit based monte-carlo planning",
            "rating": 2,
            "sanitized_title": "bandit_based_montecarlo_planning"
        },
        {
            "paper_title": "Bandit algorithms for tree search",
            "rating": 2,
            "sanitized_title": "bandit_algorithms_for_tree_search"
        },
        {
            "paper_title": "Litsearch: A retrieval benchmark for scientific literature search",
            "rating": 2,
            "sanitized_title": "litsearch_a_retrieval_benchmark_for_scientific_literature_search"
        },
        {
            "paper_title": "Openscholar: Synthesizing scientific literature with retrieval-augmented lms",
            "rating": 1,
            "sanitized_title": "openscholar_synthesizing_scientific_literature_with_retrievalaugmented_lms"
        },
        {
            "paper_title": "Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks",
            "rating": 1,
            "sanitized_title": "is_llm_a_reliable_reviewer_a_comprehensive_evaluation_of_llm_on_automatic_paper_reviewing_tasks"
        }
    ],
    "cost": 0.0171585,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery
24 May 2025</p>
<p>Aniketh Garikaparthi aniketh.g@tcs.com 
TCS Research</p>
<p>Manasi Patwardhan manasi.patwardhan@tcs.com 
TCS Research</p>
<p>Lovekesh Vig lovekesh.vig@tcs.com 
TCS Research</p>
<p>Arman Cohan arman.cohan@yale.edu 
Yale University</p>
<p>IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery
24 May 2025F0B18A475921D7444EF898C6C10441D6arXiv:2504.16728v2[cs.AI]
The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery?This work tackles the crucial first stage of research, generating novel hypotheses.While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach.To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation.IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and querybased literature synthesis.Designed to empower researchers with greater control and insight throughout the ideation process.We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation.We open-source our code here.</p>
<p>Introduction</p>
<p>With the growing capabilities of large language models (LLMs), the automation of scientific discovery has captured a lot of attention (Gridach et al., 2025).Agentic LLM based systems have shown potential of outperforming PhD researchers and postdocs on short-horizon scientific tasks like question answering, summarization and contradiction detection in various domains (Skarlinski et al., 2024;Asai et al., 2024).These advancements have spurred new opportunities of LLMs accelerating scientific discovery, which is essential given the exponential growth of scientific publications (Landhuis, 2016;Fire and Guestrin, 2019).</p>
<p>Current solutions that leverage LLMs in scientific ideation primarily remain hinged on multiagent frameworks or extending test-time compute (Si et al., 2024;Hu et al., 2024;Gottweis, 2025), and aim to validate the quality of the final ideas through human validation or LLM-as-a-judge evaluations (Wang et al., 2024;Li et al., 2024;Baek et al., 2025).However, these approaches often fail to integrate human supervision during generation in a truly complementary manner, neglecting the nuanced expectations and goals of the user.Consequently, despite investing significant computational resources to develop objectively "novel" ideas, they might not align with the user's research goals, inevitably leading to dissatisfaction (Ou et al., 2022;Kim et al., 2024).</p>
<p>Moreover, the importance of meaningful human intervention in the research process cannot be overstated.Notably, AI models have been known to fabricate convincing yet fraudulent scientific information (Májovský et al., 2023).More troubling are cases of deceptive and misaligned AI behaviors (Ryan Greenblatt, 2025;Booth, 2025;Betley et al., 2025;Baker et al., 2025).Recent developments of more capable Agentic LLMs have shown difficulties in transparently delegating sub-tasks, leading to "reward hacking" behaviors (Anthropic, 2025).In the context of idea generation, we find signs of similar "reward hacking" where LLMs adopt fancy terminology e.g."Prompt Learning and Optimization Nexus" for building a library of prompts, or often proposing the use of "graphs" without any clear motivation or description behind the design choice.We observe that naive recursive feedback loops (Baek et al., 2025) forcing the LLM to be more novel inevitably lead to gamifying LLM-as-a-judge metrics without adding actual value.Gupta and Pruthi (2025) carefully study the results of AI-Researcher (Si et al., 2024) and advise careful assessment of LLM generated hypotheses due to signs of skillful plagiarism.These examples Despite the recent innovations made in LLMbased scientific ideation, several key limitations persist.These include (1) generating hypotheses in a single pass (Si et al., 2024) , which overlooks the iterative nature of the ideation process.In contrast, Pu et al. (2024) find that researchers typically seek to refine their hypotheses into concrete research briefs.(2) Optimization through feedback on coarse-grained criteria like rigorousness, originality, generalizability etc. (Baek et al., 2025), while often critiquing entire ideas rather than specific components.(3) Simplistic retrieval augmentation such as appending keywords or abstracts of previous papers in context (Wang et al., 2024;Si et al., 2024), whereas effective ideation demands a deeper, more holistic understanding of the domain literature.(4) Unstructured and sub-optimal search of the idea space through either refinement of a generated base-idea (exploitation) (Wang et al., 2024;Baek et al., 2025), or through initial search and plan (exploration) without subsequent refinement of promising ideas (Hu et al., 2024).Finally, there is a lack of open-source implementations that would encourage broader adoption.In light of these challenges, we propose IRIS, tackling each of these limitations while enabling human intervention at every stage of the ideation process.Specifically, we make the following contributions:</p>
<p>• HITL Framework: A user-centered design balancing human control with automation instead of entirely delegating the process of ideation to AI</p>
<p>• Monte Carlo Tree Search: A systematic method to iteratively explore the idea space and extend test time compute via alternating phases of exploration and exploitation ( §3.2)</p>
<p>• Fine-grained Review based Refinement: An exhaustive taxonomy (Table 2) with finegrained actionable feedback for improving hypotheses (Figure 2) ( §3.1)</p>
<p>• Query-based Retrieval: Generating targeted queries for retrieving relevant literature, with re-ranking, clustering and summarization to produce comprehensive, technical and cited responses ( §3.1)</p>
<p>• Open Source: Publicly available platform for AI-Assisted scientific ideation Finally, we conduct a user study with researchers from diverse disciplines validating the effectiveness of our designed system ( §4).The integration of (AI) into scientific research has evolved from early concept-linking tools (Swanson, 1986;Sybrandt et al., 2020;Nadkarni et al., 2021) to sophisticated systems that enhance various research stages.In recent years, LLMs have significantly transformed research life-cycles by assisting in literature searches (Zheng et al., 2024;Ajith et al., 2024;Asai et al., 2024), citation recommendations (Pillai and R, 2022;Zhang and Zhu, 2022;Press et al., 2024), review of scientific documents (Zhou et al., 2024), experimental design (Huang et al., 2024;Schmidgall et al., 2025), scientific claim verification (Lu et al., 2024), theorem proving (Song et al., 2025), manuscript writing (Weng et al., 2025), and reading assistants1 .</p>
<p>Human-AI Co-creation Systems</p>
<p>The emergence of Gen AI has introduced a new dimension to Co-creation systems, setting them apart from previous ones where machines primarily served as supportive tools for human users (Davis et al., 2015;Muller et al., 2020;Weisz et al., 2024).Recent studies, such as those by Kantosalo and Jordanous (2021); Liu et al. (2024), demonstrate the effectiveness of Gen AI tools in creative tasks, particularly through their steerability and explain-ability.This has led to growing emphasis among researchers to develop design guidelines for integrating Gen AI into existing frameworks (Amershi et al., 2019;Shneiderman, 2020).We build IRIS for researcher-in-the-loop ideation while incorporating design principles from prior work, such as minimizing opacity, adopting granular feedback, encouraging AI processing delays (Amershi et al., 2019;Liu et al., 2024), and replacing rigid post-hoc analysis with oversight across planning, generation, and retrospection stages (Shneiderman, 2020).Spangler et al. (2014) demonstrate the first proof of principle for automated hypothesis generation through text mining of scientific literature, leveraging techniques such as entity detection and graphbased diffusion of information.Rising capabilities of text completion models has driven significant advancements in this field (Wang et al., 2024;Lu et al., 2024;Li et al., 2024;Hu et al., 2024;Si et al., 2024;Kumar et al., 2024;Baek et al., 2025;Gottweis, 2025).However, current efforts focus on fully automated systems, often overlooking the critical role of human involvement.Acceleron demonstrates one of the first human-in-the-loop (HITL) framework assisting researchers in validation of motivation behind a research problem and synthesizing a method for the same (Nigam et al., 2024), followed by Pu et al. (2024) making an attempt to develop an interactive idea generation system.These approaches remain limited, allowing idea exploration only within a predefined framework, restricting flexibility and adaptability.Furthermore, their system lacks sophisticated components like automated fine-grained feedback, literature retrieval targeted to the research goal and scaling test-time compute.</p>
<p>Automated Hypothesis Generation</p>
<p>IRIS</p>
<p>Broadly, the system expects as input a research goal G consisting of a research problem and it's motivation, and outputs a research brief B consisting of a Title, Proposed Methodology and Experiment Plan, while improving it's quality; either in semi-automatic manner through directions from the researcher or autonomously exploiting Monte Carlo Tree Search (MCTS).We provide detailed overview of our system including the implementation of agents ( §3.1) and MCTS adaptation for hypothesis generation ( §3.2).</p>
<p>Agent Architecture</p>
<p>IRIS employs a three-agent architecture consisting of an ideation agent, a review agent, and a retrieval agent.The ideation agent navigates the search space of possible research ideas, while the review and retrieval agents provide feedback and relevant scientific context respectively.</p>
<p>Ideation Agent generates and iteratively improves the research brief.It can toggle between a semi-automatic mode, to receive guidance from a researcher to refine research briefs through steering reviews, retrievals or employing custom feedback, and a completely autonomous mode to explore and exploit the idea space by leveraging actions which support iterative refinement of the research briefs through MCTS.</p>
<p>Review Agent is accountable for two tasks namely providing reward and feedback.For evaluation of an idea, we have defined a hierarchical taxonomy of aspects grounded in real-world scientific critique (For example, (Ghosal et al., 2022), (Kennard et al., 2022), (Dycke et al., 2023)), detailed in Table 2. Review Agent is auto-triggered after each new generation of the research brief to provide a reward averaged over the scores assigned to distinct aspects, based on the evaluation provided for the complete research brief.</p>
<p>As opposed to the parallel works (Wang et al., 2024;Baek et al., 2025) that focus on coarse-level criteria and provide broad evaluation of the entire generated research brief, usually, a feedback with respect to an aspect is applicable to only specific parts of the research brief.For example, only some component of the brief can be infeasible or some other component requires more clarity.Addressing this need, when explicitly triggered by the researcher, the review agent switches to a finegrained evaluation, delivering targeted, actionable feedback on each aspect of the taxonomy for distinct segments of the current research-brief (Figures 1 and 2 (R) ).This fine-grained feedback is verified by the researcher and omitted if deemed irrelevant.Then the review agent computes reward based on the scores of the verified aspects of the feedback.This adept human intervention coupled with granular feedback, successfully mitigates "reward hacking" behavior of LLMs.</p>
<p>Retrieval Agent: For the input research goal, the retrieval agent synthesizes queries targeted to retrieve literature relevant to the research goal.For answering each query, it adopts Ai2 Scholar QA API2 .The pipeline consists of two-stage retrieval followed by three-stage generation.The Semantic Scholar API's (storing over 200M open access papers) snippet search endpoint (Kinney et al., 2023) extracts relevant passages, which are re-ranked to retain top-k passages and aggregated at the paper level.With the finalized set of passages, the retrieval agent (i) extracts quotes from the passages relevant to the query, (ii) generates a plan to produce an organized report with sections, and clusters the top-k passages accordingly, and (iii) generates cited sections-wise reports along with summaries (Figure 2 (L)).Our motivation for adopting Schol-arQA stems from the limitations of naive RAG failing to appropriately answer global questions targeted at a corpus as opposed to a single document (Edge et al., 2025).We also provide the ability for the researcher to upload papers in the form of PDF documents, which they think to be relevant but have been missed out as the part of the retrieval.The retrieval agent parses the PDF through Grobid based doc2json tool3 and appends the most relevant chunks to the context for the ideation agent to refine the research brief.</p>
<p>Monte Carlo Tree Search Framework</p>
<p>To systematically explore the vast space of potential research ideas, IRIS employs Monte Carlo Tree Search (MCTS) (Kocsis and Szepesvári, 2006).MCTS allows the system to effectively extend testtime compute similar to recent work in augmenting LLM reasoning (Qi et al., 2024;Guan et al., 2025).Unlike applications with objective rewards (e.g., mathematics, code generation), scientific ideation quality is subjective.We adapt MCTS by using the LLM-based Review Agent as a proxy judge to estimate the quality (reward) of generated hypotheses.</p>
<p>Formally, given a research goal G, our system constructs a search tree T rooted with G.A state s encapsulates the current {research brief b, reward estimate r, latest review feedback f (if applicable, else ϕ), and retrieved knowledge k (if applicable, else ϕ)}.Edges represent actions a taken by the Ideation Agent to transition between states.We define a comprehensive action space A = {a 1 : generate, a 2 : refine w/ retrieval, a 3 : refine w/ review, a 4 : refine w/ user feedback}.The MCTS process iteratively builds the tree over N iterations, guided by the Upper Confidence Bound for Trees (UCT) algorithm (Coquelin and Munos, 2007).UCT of a node n is defined by:
UCT(n) = Q(n) N (n) + c ln N (n p ) N (n)(1)
where Q(n) is the total reward at child node n accumulated from its children, N (n) is its visit count, N (n p ) is the visit count of the parent node of n , and c is the exploration constant.Algorithm 1 outlines the MCTS process.Each node n stores its state s n as defined above, Q(n) and N (n).</p>
<p>Algorithm 1 MCTS for Research Idea Generation</p>
<p>Require: Research goal G, iterations N , max depth d max , actions A, constant c 1: Initialize tree T with root n 0 (state
s 0 = G, Q(n 0 ) = 0, N (n 0 ) = 0). 2: for i = 1 to N do 3: n leaf ← SELECT(n 0 , c) 4: r ← EVALUATE(n leaf ) 5: if depth &lt; d max then 6: EXPAND(n leaf , A) 7:
end if 8: BACKPROPAGATE(n leaf , r) 9: end for 10: return BESTCHILD(n 0 ) Each iteration involves four phases: SELECT(n root , c): Traverse the tree from the root n 0 to select a leaf node n leaf .At each node n during traversal, if n has any unvisited children (Q(n) = 0), one such child is randomly selected.If all children of n have been visited, the next node is chosen by: arg max n ′ ∈children(n) (UCT(n ′ )).</p>
<p>EVALUATE(n leaf ): Obtain reward r for the state s leaf of n leaf via the Review Agent.</p>
<p>EXPAND(n leaf , A): If n leaf is non-terminal and below d max , create child nodes n ′ for each applicable action a ∈ A, with Q(n ′ ) = 0, N (n ′ ) = 0. BACKPROPAGATE(n leaf , r): Update Q and N values for n leaf and its ancestors with reward r.</p>
<p>BESTCHILD(n 0 ): After N iterations, select the child of n 0 with the highest average reward Q/N .</p>
<p>Memory: Agents maintain trajectory-level memory.For instance, the Ideation Agent recalls generated briefs, the Retrieval Agent remembers past queries, and the Review Agent tracks prior feedback.This helps steer the generation towards nonredundant refinements.Cost: MCTS can be computationally intensive.IRIS incorporates budget controls, allowing users to set limits.For tighter budgets, the system prioritizes exploitation by lowering the exploration constant c, ensuring delivery of few refined outputs rather than numerous low-quality ones.</p>
<p>Evaluation</p>
<p>To assess the effectiveness and usability of IRIS, we conduct automated evaluations and user studies.</p>
<p>Experiment Setup</p>
<p>System Implementation: IRIS's user interface is developed using HTML, CSS, JavaScript.The core LLM functionalities are powered by Gemini-2.0-Flash(DeepMind, 2024) accessed via LiteLLM4 , which allows users to substitute other LLMs of their choice.We utilize Gemini's built-in safety filters to mitigate harmful or inappropriate queries.</p>
<p>Metrics: We employ LLM-as-a-judge, popularly adopted in parallel literature (Baek et al., 2025;Gottweis, 2025).We use two methods guided by our pre-defined criteria (Table 2).absolute score: each generated hypothesis (1-10), and relative score: aggregating head-to-head comparisons and preferences to compute ELO ratings.</p>
<p>To contextualize the alignment of LLM-as-ajudge with human preferences in the context of scientific ideation, we prompt baselines Gemini-2.0-Flash,ChatGPT, ChatGPT w/ search and Claude 3.5 Haiku to generate novel research briefs.Then ask users and LLMs to rate the generations in the order of their preference.</p>
<p>User Study</p>
<p>We conducted a user study with 8 researchers (N=8) from diverse fields (AI/NLP, Chem, Physics, HCI) and experience levels.Two users voluntarily participated twice (10 total case studies).Each ∼60 min session involved: 1) Defining a research goal, 2) Blindly ranking initial set of hypotheses, 3) Interacting with IRIS, 4) Completing a post-task survey.</p>
<p>Results and Analysis</p>
<p>Metric Validation: Human baseline rankings correlated moderately with LLM based ELO scores (Pearson's r=0.60) but weakly with LLM based absolute scores (r=0.45).With this learning we plan to replace the LLM-as-the-judge scores, displayed to showcase the quality of the idea, with the ELO ratings.</p>
<p>Automated Evaluation: LLM-as-a-judge evaluations (Figure 3) showed that user interaction within IRIS consistently improved hypothesis quality, increasing average absolute scores by 0.5 points and ELO ratings by 12 points for a tree depth of 3.  User Study Feedback: Quantitative ratings (Table 1) show users found the fine-grained feedback highly insightful and unpromptedly mentioned better usability and control over other reading assistant interfaces mentioned in §2.</p>
<p>Feature / Aspect</p>
<p>Mean Rating (± Std Dev) Additionally, through qualitative feedback we arrived at the following insights:</p>
<p>• Steerability: All users valued the MCTS tree for control and transparency over ideation.</p>
<p>• Feedback: Critiques often reflected user's own concerns (87.5% users) and sometimes sparked novel insights (50% cases).</p>
<p>• Retrieval: Found to be facilitating grounding of ideas, but quality varied with domains such as chemistry and physics research, matching the lower rating (3.7/5).We attribute this to reduced availability of relevant literature in the semantic scholar corpus.</p>
<p>• Relevance: hypotheses often shared similarities with or extended users' ongoing work (62.5% users).</p>
<p>Overall Improvement: Post-interaction, 25% (2/8) found the hypothesis substantially better, 50% (4/8) marginally better, and 25% (2/8) similar quality.Crucially, all users reported enhanced understanding of the proposed methodology, and considered it to be promising.</p>
<p>Conclusion</p>
<p>We introduce IRIS, an Interactive Research Ideation System, to augment automated scientific hypothesis generation with human expertise.We apply MCTS to iteratively explore the idea space, refine ideas with fine-grained segment level reviews and targeted query based multi-document retrieval; offering a steerable environment for researchers during LLM-driven scientific ideation.Our user study validates the usability and effectiveness of our system, demonstrating consistent improvement in hypothesis quality increasing average absolute scores by 0.5 points and ELO ratings by 12 points for a tree depth of 3. Crucially, users frequently considered the generated hypotheses plausible and worthy of further investigation.We position that the potential of LLMs, particularly within human-AI collaborative frameworks, for developing novel scientific hypothesis remains a heavily underexplored avenue.We present IRIS as a concrete step towards realizing this untapped potential.</p>
<p>Limitations</p>
<p>Currently the system relies on the researcher as the judge to verify the quality of the emerging idea at each iteration, augmented by LLM-as-thejudge.This reliance is based on the assumption of sufficient domain expertise of the researcher.As opposed to this in future we aim for a true Human AI Co-creation System, where more foundational LLMs with scientific expertise, questions researchers for the choices he or she has made leading to a two way socratic review and refinement communication, simulating a more realistic scenario of brain-storming between colleagues or a mentor and a mentee.</p>
<p>Due to budget constraints, we have not explored frontier LLMs such as Claude 3.7 Sonnet, Grok-3 or reasoning models like Gemini-2.5-Pro,o1 etc.The quality of produced hypothesis in terms of novelty and effectiveness would likely benefit from stronger base models.</p>
<p>A Review Taxonomy</p>
<p>Aspect Sub-aspect Definition Originality Lack of Novelty The idea does not introduce a significant or meaningful advancement over existing work, lacking originality or innovation.</p>
<p>Assumptions</p>
<p>The idea relies on untested or unrealistic assumptions that may weaken its validity or applicability.</p>
<p>Clarity</p>
<p>Vagueness</p>
<p>The idea is presented in an unclear or ambiguous manner, making it difficult to understand its core components or contributions.</p>
<p>Contradictory Statements</p>
<p>The idea contains internal inconsistencies or conflicts in its assumptions, methods, or conclusions.</p>
<p>Alignment</p>
<p>The idea is not aligned with the problem statement and its objectives.</p>
<p>Feasibility</p>
<p>Feasibility and Practicality</p>
<p>The idea is not practical or achievable given current technological, theoretical, or resource constraints.</p>
<p>Justification for Methods</p>
<p>The idea does not provide sufficient reasoning or evidence to explain why specific methods, techniques, or approaches were chosen.</p>
<p>Effectiveness</p>
<p>Evaluation and Validation Issues The idea lacks rigorous evaluation methods, such as insufficient benchmarks, inadequate baselines, or poorly defined success metrics.Reproducibility and Robustness The idea does not provide sufficient detail or transparency to allow others to replicate or verify its findings, and is not resilient to variations in input data, assumptions, or environmental conditions.The degree to which the solution consistently produces accurate and dependable results is low, making it less reliable.</p>
<p>Impact</p>
<p>Overgeneralization and Overstatement The idea extends its conclusions or applicability beyond the scope of the context provided or exaggerates its claims, significance, or potential impact beyond what is supported by evidence or reasoning.</p>
<p>Impact</p>
<p>The idea is not impactful or significant.It does not solve a real problem.It does not create value by solving a significant problem or fulfilling a need for individuals, organizations, or society.Ethical and Social Considerations</p>
<p>The idea does not adhere to ethical standards and is harmful to individuals, communities, or the environment.</p>
<p>Figure 1 :
1
Figure 1: Human-in-the-loop Idea Generation with Monte-Carlo-Tree-Search. G: Research Goal, B: Research Brief</p>
<p>Figure 2 :
2
Figure 2: IRIS Platform Interface with (L) Retrieval Panel, (C) Chat Overview Panel, (R) Research Brief Panel</p>
<p>Figure 3 :
3
Figure 3: Iterative improvement in hypothesis quality within IRIS over interaction depth (up to depth 3).Interaction enhances both absolute scores and ELO ratings.</p>
<p>Figure 4 :
4
Figure 4: Top: Comparison of hypothesis quality generated by baseline methods (ChatGPT, ChatGPT+Search, Claude 3.5 Haiku, Gemini-2.0-Flash)using LLM-as-a-judge absolute scores and ELO ratings.Bottom: User Survey Feedback Form Questions.</p>
<p>Table 1 :
1
User ratings (1-5 Likert scale) for key IRIS features and overall satisfaction (N=10).
Usefulness of Fine-grained Feedback4.3 ± 0.7MCTS Tree Interface (Steerability)4.2 ± 0.6Quality of Lit. Summaries3.7 ± 0.8Usability and control4.5 ± 0.7Overall Satisfaction (Final Research Brief)3.9 ± 0.7</p>
<p>Table 2 :
2
Hierarchical Review Taxonomy</p>
<p>JenniAI, SciSpace, ScholarAI <br />
https://allenai.org/blog/ai2-scholarqa
https://github.com/allenai/s2orc-doc2json
https://docs.litellm.ai/docs/</p>
<p>Litsearch: A retrieval benchmark for scientific literature search. Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, Tianyu Gao, arXiv:2407.189402024Preprint</p>
<p>Guidelines for human-ai interaction. Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, Eric Horvitz, 10.1145/3290605.3300233Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI '19. the 2019 CHI Conference on Human Factors in Computing Systems, CHI '19New York, NY, USAAssociation for Computing Machinery2019</p>
<p>Claude 3.7 sonnet system card. Accessed. Anthropic, 2025</p>
<p>Openscholar: Synthesizing scientific literature with retrieval-augmented lms. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D , David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen Tau Yih, Pang Wei Koh, Hannaneh Hajishirzi, arXiv:2411.141992024Preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382025Preprint</p>
<p>Monitoring reasoning models for misbehavior and the risks of promoting obfuscation. Bowen Baker, Joost Huizinga, Leo Gao, Zehao Dou, Melody Y Guan, Aleksander Madry, Wojciech Zaremba, Jakub Pachocki, David Farhi, 2025</p>
<p>Emergent misalignment: Narrow finetuning can produce broadly misaligned llms. Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans, arXiv:2502.174242025Preprint</p>
<p>Accessed: 2025-02-25. Pierre-Arnaud Coquelin and Rémi Munos. Harry Booth, arXiv:cs/0703062Time. 2025. 2007PreprintBandit algorithms for tree search</p>
<p>An Enactive Model of Creativity for Computational Collaboration and Co-creation. Nicholas Davis, Chih-Pin Hsiao, Yanna Popova, Brian Magerko, 10.1007/978-1-4471-6681-8_72015SpringerLondon, London</p>
<p>Google gemini ai update. Google Deepmind, 2024. December 2024</p>
<p>. Accessed, </p>
<p>NLPeer: A unified resource for the computational study of peer review. Nils Dycke, Ilia Kuznetsov, Iryna Gurevych, 10.18653/v1/2023.acl-long.277Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>From local to global: A graph rag approach to query-focused summarization. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson, arXiv:2404.161302025Preprint</p>
<p>Overoptimization of academic publishing metrics: observing goodhart's law in action. Michael Fire, Carlos Guestrin, 10.1093/gigascience/giz0532019GigaScience853</p>
<p>Peer review analyze: A novel benchmark resource for computational analysis of peer reviews. Tirthankar Ghosal, Sandeep Kumar, Prabhat Kumar Bharti, Asif Ekbal, 10.1371/journal.pone.0259238PLOS ONE. 1712022</p>
<p>Towards an ai co-scientist. Juraj Gottweis, 2025</p>
<p>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions. Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, Christina Mack, arXiv:2503.089792025Preprint</p>
<p>rstar-math: Small llms can master math reasoning with self-evolved deep thinking. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang, arXiv:2501.045192025Preprint</p>
<p>All that glitters is not novel: Plagiarism in ai generated research. Tarun Gupta, Danish Pruthi, arXiv:2502.164872025Preprint</p>
<p>An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.142552024NovaPreprint</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, arXiv:2310.033022024Preprint</p>
<p>Rolebased perceptions of computer participants in humancomputer co-creativity. Anna Kantosalo, Anna Jordanous, 7th Computational Creativity Symposium at AISB 2021. London, UK. AISB2021</p>
<p>DISAPERE: A dataset for discourse structure in peer review discussions. Nayak Neha, Tim O Kennard, Rajarshi 'gorman, Akshay Das, Chhandak Sharma, Matthew Bagchi, Pranay Clinton, Hamed Kumar Yelugam, Andrew Zamani, Mccallum, 10.18653/v1/2022.naacl-main.89Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Understanding users' dissatisfaction with chatgpt responses: Types, resolving tactics, and the effect of knowledge level. Yoonsu Kim, Jueon Lee, Seoyoung Kim, Jaehyuk Park, Juho Kim, 10.1145/3640543.3645148Proceedings of the 29th International Conference on Intelligent User Interfaces, IUI '24. the 29th International Conference on Intelligent User Interfaces, IUI '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>The semantic scholar open data platform. Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David Graham, Fangzhou Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey Macmillan, Tyler Murray, Chris Newell, Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, Amber Tanaka, Alex D Wade, Linda Wagner, Lucy Lu Wang, Chris Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, arXiv:2301.101402023Madeleine Van Zuylen, and Daniel S. WeldPreprint</p>
<p>Bandit based monte-carlo planning. Levente Kocsis, Csaba Szepesvári, Machine Learning: ECML 2006. Berlin, Heidelberg; Berlin HeidelbergSpringer2006</p>
<p>Can large language models unlock novel scientific research ideas?. Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal, arXiv:2409.061852024Preprint</p>
<p>Scientific literature: Information overload. Esther Landhuis, Nature. 5352016</p>
<p>Chain of ideas: Revolutionizing research via novel idea development with llm agents. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, Deli Zhao, Yu Rong, Tian Feng, Lidong Bing, arXiv:2410.131852024Preprint</p>
<p>How ai processing delays foster creativity: Exploring research question co-creation with an llm-based agent. Yiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, Yun Huang, 10.1145/3613904.3642698Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24. the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024Preprint</p>
<p>Artificial intelligence can generate fraudulent but authentic-looking scientific medical articles: Pandora's box has been opened. Martin Májovský, Martin Černý, Matěj Kasal, Martin Komarc, David Netuka, 10.2196/46924J Med Internet Res. 25e469242023</p>
<p>Mixed initiative generative ai interfaces: An analytic framework for generative ai applications. Michael Muller, Justin D Weisz, Werner Geyer, Proceedings of the Workshop The Future of Co-Creative Systems-A Workshop on Human-Computer Co-Creativity of the 11th International Conference on Computational Creativity. the Workshop The Future of Co-Creative Systems-A Workshop on Human-Computer Co-Creativity of the 11th International Conference on Computational Creativity2020ICCC 2020</p>
<p>Scientific language models for biomedical knowledge base completion: An empirical study. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, arXiv:2106.097002021Preprint</p>
<p>An interactive co-pilot for accelerated research ideation. Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff, 10.18653/v1/2024.hcinlp-1.6Proceedings of the Third Workshop on Bridging Human-Computer Interaction and Natural Language Processing. the Third Workshop on Bridging Human-Computer Interaction and Natural Language ProcessingMexico City, MexicoAssociation for Computational Linguistics2024</p>
<p>The human in the infinite loop: A case study on revealing and explaining human-ai interaction loop failures. Changkun Ou, Daniel Buschek, Sven Mayer, Andreas Butz, 10.1145/3543758.3543761Proceedings of Mensch Und Computer 2022, MuC '22. Mensch Und Computer 2022, MuC '22New York, NY, USA2022Association for Computing Machinery</p>
<p>A survey on citation recommendation system. S Reshma, Pillai, L R Deepthi, 10.1109/ICICICT54557.2022.99178872022 Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT). 2022</p>
<p>Citeme: Can language models accurately cite scientific claims?. Ori Press, Andreas Hochlehnert, Ameya Prabhu, arXiv:2407.128612024PreprintVishaal Udandarao, Ofir Press, and Matthias Bethge</p>
<p>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. Kevin Pu, K J Kevin Feng, Tovi Grossman, Tom Hope, Bhavana Dalvi Mishra, Matt Latzke, Jonathan Bragg, Joseph Chee Chang, Pao Siangliulue, arXiv:2410.040252024Preprint</p>
<p>Mutual reasoning makes smaller llms stronger problem-solvers. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, Mao Yang, arXiv:2408.061952024Preprint</p>
<p>Alignment faking in large language models. et.al Ryan Greenblatt2025</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025Preprint</p>
<p>Bridging the gap between ethics and practice: Guidelines for reliable, safe, and trustworthy human-centered ai systems. Ben Shneiderman, 10.1145/3419764ACM Trans. Interact. Intell. Syst. 1042020</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024Preprint</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. D Michael, Sam Skarlinski, Jon M Cox, James D Laurent, Michaela Braza, Michael J Hinks, Manvitha Hammerling, Ponnapati, G Samuel, Andrew D Rodriques, White, arXiv:2409.137402024Preprint</p>
<p>Lean copilot: Large language models as copilots for theorem proving in lean. Peiyang Song, Kaiyu Yang, Anima Anandkumar, arXiv:2404.125342025Preprint</p>
<p>Automated hypothesis generation based on mining scientific literature. Scott Spangler, Angela D Wilkins, Benjamin J Bachman, Meena Nagarajan, Tajhal Dayaram, Peter Haas, Sam Regenbogen, Curtis R Pickering, Austin Comer, Jeffrey N Myers, Ioana Stanoi, Linda Kato, Ana Lelescu, Jacques J Labrie, Neha Parikh, Andreas Martin Lisewski, Lawrence Donehower, Ying Chen, Olivier Lichtarge, 10.1145/2623330.2623667Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14. the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14New York, NY, USA2014Association for Computing Machinery</p>
<p>Undiscovered public knowledge. Don R Swanson, The Library Quarterly: Information, Community. 198656</p>
<p>Agatha: Automatic graph mining and transformer based hypothesis generation approach. Justin Sybrandt, Ilya Tyagin, Michael Shtutman, Ilya Safro, 10.1145/3340531.3412684Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, CIKM '20. the 29th ACM International Conference on Information &amp; Knowledge Management, CIKM '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/v1/2024.acl-long.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Design principles for generative ai applications. Justin D Weisz, Jessica He, Michael Muller, Gabriela Hoefer, Rachel Miles, Werner Geyer, 10.1145/3613904.3642466Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24. the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, arXiv:2411.008162025Preprint</p>
<p>Citation recommendation using semantic representation of cited papers' relations and content. Jinzhu Zhang, Lipeng Zhu, 10.1016/j.eswa.2021.115826Expert Systems with Applications. 1871158262022</p>
<p>Openresearcher: Unleashing ai for accelerated scientific research. Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, Yang Xu, Qingkai Min, Zizhao Zhang, Yiwen Wang, Wenjie Li, Pengfei Liu, arXiv:2408.069412024Preprint</p>
<p>Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks. Ruiyang Zhou, Lu Chen, Kai Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>            </div>
        </div>

    </div>
</body>
</html>
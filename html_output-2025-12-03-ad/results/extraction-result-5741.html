<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5741 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5741</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5741</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-663dc434fabc1cf1d3e85fff3f7ddcd313035d18</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/663dc434fabc1cf1d3e85fff3f7ddcd313035d18" target="_blank">LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters</a></p>
                <p><strong>Paper Venue:</strong> ACM Transactions on Intelligent Systems and Technology</p>
                <p><strong>Paper TL;DR:</strong> LLM4TS is superior to existing state-of-the-art methods compared with trained-from-scratch models in full-shot scenarios and also achieves the highest rank in few-shot scenarios and evaluations compared with different unsupervised representation learning approaches highlight LLM4TS’s effectiveness with representation learning in forecasting tasks.</p>
                <p><strong>Paper Abstract:</strong> Multivariate time-series forecasting is vital in various domains, e.g., economic planning and weather prediction. Deep train-from-scratch models have exhibited effective performance yet require large amounts of data, which limits real-world applicability. Recently, researchers have leveraged the representation learning transferability of pre-trained Large Language Models (LLMs) to handle limited non-linguistic datasets effectively. However, incorporating LLMs with time-series data presents challenges of limited adaptation due to different compositions between time-series and linguistic data, and the inability to process multi-scale temporal information. To tackle these challenges, we propose LLM4TS, a framework for time-series forecasting with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the time-series alignment stage to align LLMs with the nuances of time-series data and the forecasting fine-tuning stage for downstream time-series forecasting tasks. Furthermore, our framework features a novel two-level aggregation method that integrates multi-scale temporal data within pre-trained LLMs, enhancing their ability to interpret time-specific information. In experiments across seven time-series forecasting datasets, LLM4TS is superior to existing state-of-the-art methods compared with trained-from-scratch models in full-shot scenarios and also achieves the highest rank in few-shot scenarios. In addition, evaluations compared with different unsupervised representation learning approaches highlight LLM4TS’s effectiveness with representation learning in forecasting tasks. Ablation studies further validate each component’s contribution to LLM4TS and underscore the essential role of utilizing LLM’s pre-trained weights for optimal performance. The code is available at https://github.com/blacksnail789521/LLM4TS.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5741.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5741.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4TS (anomaly mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper (LLM4TS) adapts a pre-trained GPT-2 for time-series forecasting via a two-stage fine-tuning and a two-level temporal aggregation; it explicitly notes as future work the extension of the framework to other tasks including anomaly detection but does not perform anomaly detection experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (backbone used in LLM4TS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 backbone (causal Transformer), 12-layer base model; in experiments the paper uses the first 6 layers of GPT-2 and freezes most Transformer parameters while applying PEFT (LayerNorm tuning and LoRA).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>85M (total params of the variant used in experiments reported in Table 7); 3.4M trainable (~4%)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Not implemented in this work — anomaly detection is listed as prospective future work (no method, prompting, or fine-tuning protocol for anomaly detection is described).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Time-series sequences / multivariate time-series (paper focuses on forecasting, not anomaly detection).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No anomaly detection experiments are reported; the paper only mentions anomaly detection as a future extension, so no empirical evidence, datasets, or comparisons are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5741.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5741.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TabLLM (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TabLLM: Few-shot classification of tabular data with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited related work that adapts LLMs for tasks on tabular/structured data (few-shot classification), referenced as evidence that LLMs can transfer to structured data modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tabllm: Few-shot classification of tabular data with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not detailed in this paper; referenced as an example of applying LLMs to tabular data (few-shot classification).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tabular / structured table data (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Only cited in related work context; LLM4TS does not report using or evaluating TabLLM for anomaly detection and provides no details or comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5741.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5741.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Table-meets-LLM (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Table meets LLM: Can large language models understand structured table data? a benchmark and empirical study</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited benchmark study evaluating LLMs on structured/tabular data tasks, included as related work indicating LLM applicability to structured data modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Table meets llm: Can large language models understand structured table data? a benchmark and empirical study.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified in this paper; referenced as a study benchmarking LLM performance on table/structured data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Structured / tabular data (benchmark study referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited only as prior work; no details about anomaly detection tasks or performance are given in LLM4TS.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5741.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5741.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4TS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4TS (pre-existing method for time-series forecasting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An existing method that uses pre-trained GPT-2 for time-series forecasting (patching and channel-independence) and is cited and compared against in experiments; it is not used for anomaly detection in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (pre-trained, used as backbone in GPT4TS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained GPT-2 (causal Transformer) used with patching and channel-independence for time-series forecasting; details in the referenced GPT4TS work; in this paper GPT4TS is a baseline for forecasting comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Time-series sequences (forecasting), not anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Weather, Traffic, Electricity, ETTh1/ETTh2, ETTm1/ETTm2 (used as forecasting benchmarks in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MSE and MAE for forecasting (reported in forecasting comparisons), but no anomaly detection metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>GPT4TS is a main baseline for forecasting; in few-shot settings LLM4TS outperforms GPT4TS, but no anomaly detection comparison exists.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No use or evaluation for anomaly detection reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5741.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5741.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Time-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Time-LLM: Time series forecasting by reprogramming large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work that reprograms LLMs for time-series forecasting by converting time series into text prototypes and using prompts; it is referenced as evidence LLMs can be used on sequence data but not for anomaly detection within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Time-LLM: Time series forecasting by reprogramming large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as a method that reprograms LLMs (details in original Time-LLM paper); LLM4TS cites it as a related approach for time-series tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Time-series sequences (forecasting), converted to text prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Cited and compared in forecasting experiments (LLM4TS often outperforms or is comparable) but no anomaly detection comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No anomaly-detection experiments or failure cases discussed in LLM4TS regarding Time-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5741.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5741.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEMPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TEMPO: Prompt-based generative pre-trained transformer for time series forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced LLM-based method that adapts GPT-like models for time-series forecasting via decomposition and prompts; cited as related work and baseline (zero-shot evaluation) in forecasting comparisons, not used for anomaly detection here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TEMPO: Prompt-based generative pre-trained transformer for time series forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-based GPT-like generative model for forecasting (zero-shot prompt evaluation in cited work); LLM4TS includes TEMPO as a comparator (not for anomaly detection).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Time-series sequences (forecasting) via prompt-based generation.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>TEMPO is evaluated in zero-shot forecasting scenarios in the paper; no anomaly detection comparison is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No anomaly-detection use reported in this paper; evaluated only for forecasting (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5741.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5741.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TEST: Text prototype aligned embedding to activate LLM's ability for time series</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited method that aligns time series with LLM embeddings (tokenization + contrastive learning) enabling forecasting with pre-trained LLMs without fine-tuning; mentioned as related work for sequences, not for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TEST: Text prototype aligned embedding to activate LLM's ability for time series.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as a method that maps time series into LLM embedding space using tokenization and contrastive learning; LLM4TS compares to TEST for forecasting tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Time-series sequences (forecasting via embedding alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Included among baselines for forecasting performance; no anomaly detection experiments or comparisons reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No anomaly-detection experiments discussed in this paper related to TEST.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tabllm: Few-shot classification of tabular data with large language models. <em>(Rating: 2)</em></li>
                <li>Table meets llm: Can large language models understand structured table data? a benchmark and empirical study. <em>(Rating: 2)</em></li>
                <li>Time-LLM: Time series forecasting by reprogramming large language models. <em>(Rating: 2)</em></li>
                <li>TEMPO: Prompt-based generative pre-trained transformer for time series forecasting. <em>(Rating: 1)</em></li>
                <li>GPT4TS <em>(Rating: 1)</em></li>
                <li>TEST: Text prototype aligned embedding to activate LLM's ability for time series. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5741",
    "paper_id": "paper-663dc434fabc1cf1d3e85fff3f7ddcd313035d18",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "LLM4TS (anomaly mention)",
            "name_full": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters",
            "brief_description": "This paper (LLM4TS) adapts a pre-trained GPT-2 for time-series forecasting via a two-stage fine-tuning and a two-level temporal aggregation; it explicitly notes as future work the extension of the framework to other tasks including anomaly detection but does not perform anomaly detection experiments.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-2 (backbone used in LLM4TS)",
            "model_description": "GPT-2 backbone (causal Transformer), 12-layer base model; in experiments the paper uses the first 6 layers of GPT-2 and freezes most Transformer parameters while applying PEFT (LayerNorm tuning and LoRA).",
            "model_size": "85M (total params of the variant used in experiments reported in Table 7); 3.4M trainable (~4%)",
            "anomaly_detection_method": "Not implemented in this work — anomaly detection is listed as prospective future work (no method, prompting, or fine-tuning protocol for anomaly detection is described).",
            "data_type": "Time-series sequences / multivariate time-series (paper focuses on forecasting, not anomaly detection).",
            "anomaly_type": null,
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "limitations_or_failure_cases": "No anomaly detection experiments are reported; the paper only mentions anomaly detection as a future extension, so no empirical evidence, datasets, or comparisons are provided.",
            "uuid": "e5741.0",
            "source_info": {
                "paper_title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "TabLLM (related work)",
            "name_full": "TabLLM: Few-shot classification of tabular data with large language models",
            "brief_description": "Cited related work that adapts LLMs for tasks on tabular/structured data (few-shot classification), referenced as evidence that LLMs can transfer to structured data modalities.",
            "citation_title": "Tabllm: Few-shot classification of tabular data with large language models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Not detailed in this paper; referenced as an example of applying LLMs to tabular data (few-shot classification).",
            "model_size": null,
            "anomaly_detection_method": null,
            "data_type": "Tabular / structured table data (mentioned in related work).",
            "anomaly_type": null,
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "limitations_or_failure_cases": "Only cited in related work context; LLM4TS does not report using or evaluating TabLLM for anomaly detection and provides no details or comparisons.",
            "uuid": "e5741.1",
            "source_info": {
                "paper_title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Table-meets-LLM (related work)",
            "name_full": "Table meets LLM: Can large language models understand structured table data? a benchmark and empirical study",
            "brief_description": "Cited benchmark study evaluating LLMs on structured/tabular data tasks, included as related work indicating LLM applicability to structured data modalities.",
            "citation_title": "Table meets llm: Can large language models understand structured table data? a benchmark and empirical study.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Not specified in this paper; referenced as a study benchmarking LLM performance on table/structured data.",
            "model_size": null,
            "anomaly_detection_method": null,
            "data_type": "Structured / tabular data (benchmark study referenced).",
            "anomaly_type": null,
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "limitations_or_failure_cases": "Cited only as prior work; no details about anomaly detection tasks or performance are given in LLM4TS.",
            "uuid": "e5741.2",
            "source_info": {
                "paper_title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT4TS",
            "name_full": "GPT4TS (pre-existing method for time-series forecasting)",
            "brief_description": "An existing method that uses pre-trained GPT-2 for time-series forecasting (patching and channel-independence) and is cited and compared against in experiments; it is not used for anomaly detection in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (pre-trained, used as backbone in GPT4TS)",
            "model_description": "Pre-trained GPT-2 (causal Transformer) used with patching and channel-independence for time-series forecasting; details in the referenced GPT4TS work; in this paper GPT4TS is a baseline for forecasting comparisons.",
            "model_size": null,
            "anomaly_detection_method": null,
            "data_type": "Time-series sequences (forecasting), not anomaly detection.",
            "anomaly_type": null,
            "dataset_name": "Weather, Traffic, Electricity, ETTh1/ETTh2, ETTm1/ETTm2 (used as forecasting benchmarks in this paper)",
            "performance_metrics": "MSE and MAE for forecasting (reported in forecasting comparisons), but no anomaly detection metrics.",
            "baseline_comparison": "GPT4TS is a main baseline for forecasting; in few-shot settings LLM4TS outperforms GPT4TS, but no anomaly detection comparison exists.",
            "limitations_or_failure_cases": "No use or evaluation for anomaly detection reported in this paper.",
            "uuid": "e5741.3",
            "source_info": {
                "paper_title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Time-LLM",
            "name_full": "Time-LLM: Time series forecasting by reprogramming large language models",
            "brief_description": "Cited work that reprograms LLMs for time-series forecasting by converting time series into text prototypes and using prompts; it is referenced as evidence LLMs can be used on sequence data but not for anomaly detection within this paper.",
            "citation_title": "Time-LLM: Time series forecasting by reprogramming large language models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Referenced as a method that reprograms LLMs (details in original Time-LLM paper); LLM4TS cites it as a related approach for time-series tasks.",
            "model_size": null,
            "anomaly_detection_method": null,
            "data_type": "Time-series sequences (forecasting), converted to text prototypes.",
            "anomaly_type": null,
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": "Cited and compared in forecasting experiments (LLM4TS often outperforms or is comparable) but no anomaly detection comparison.",
            "limitations_or_failure_cases": "No anomaly-detection experiments or failure cases discussed in LLM4TS regarding Time-LLM.",
            "uuid": "e5741.4",
            "source_info": {
                "paper_title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "TEMPO",
            "name_full": "TEMPO: Prompt-based generative pre-trained transformer for time series forecasting",
            "brief_description": "Referenced LLM-based method that adapts GPT-like models for time-series forecasting via decomposition and prompts; cited as related work and baseline (zero-shot evaluation) in forecasting comparisons, not used for anomaly detection here.",
            "citation_title": "TEMPO: Prompt-based generative pre-trained transformer for time series forecasting.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Prompt-based GPT-like generative model for forecasting (zero-shot prompt evaluation in cited work); LLM4TS includes TEMPO as a comparator (not for anomaly detection).",
            "model_size": null,
            "anomaly_detection_method": null,
            "data_type": "Time-series sequences (forecasting) via prompt-based generation.",
            "anomaly_type": null,
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": "TEMPO is evaluated in zero-shot forecasting scenarios in the paper; no anomaly detection comparison is provided.",
            "limitations_or_failure_cases": "No anomaly-detection use reported in this paper; evaluated only for forecasting (zero-shot).",
            "uuid": "e5741.5",
            "source_info": {
                "paper_title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "TEST",
            "name_full": "TEST: Text prototype aligned embedding to activate LLM's ability for time series",
            "brief_description": "Cited method that aligns time series with LLM embeddings (tokenization + contrastive learning) enabling forecasting with pre-trained LLMs without fine-tuning; mentioned as related work for sequences, not for anomaly detection.",
            "citation_title": "TEST: Text prototype aligned embedding to activate LLM's ability for time series.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Referenced as a method that maps time series into LLM embedding space using tokenization and contrastive learning; LLM4TS compares to TEST for forecasting tasks.",
            "model_size": null,
            "anomaly_detection_method": null,
            "data_type": "Time-series sequences (forecasting via embedding alignment).",
            "anomaly_type": null,
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": "Included among baselines for forecasting performance; no anomaly detection experiments or comparisons reported.",
            "limitations_or_failure_cases": "No anomaly-detection experiments discussed in this paper related to TEST.",
            "uuid": "e5741.6",
            "source_info": {
                "paper_title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tabllm: Few-shot classification of tabular data with large language models.",
            "rating": 2
        },
        {
            "paper_title": "Table meets llm: Can large language models understand structured table data? a benchmark and empirical study.",
            "rating": 2
        },
        {
            "paper_title": "Time-LLM: Time series forecasting by reprogramming large language models.",
            "rating": 2
        },
        {
            "paper_title": "TEMPO: Prompt-based generative pre-trained transformer for time series forecasting.",
            "rating": 1
        },
        {
            "paper_title": "GPT4TS",
            "rating": 1
        },
        {
            "paper_title": "TEST: Text prototype aligned embedding to activate LLM's ability for time series.",
            "rating": 1
        }
    ],
    "cost": 0.013506999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Preprint Notice</h1>
<p>This paper has been accepted for publication in ACM Transactions on Intelligent Systems and Technology (TIST) 2025. The final version will be available at https://doi.org/ $10.1145 / 3719207$</p>
<h1>LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters</h1>
<p>Ching Chang ${ }^{1}$, Wei-Yao Wang ${ }^{1}$, Wen-Chih Peng ${ }^{1}$, Tien-Fu Chen ${ }^{1}$<br>${ }^{1}$ National Yang Ming Chiao Tung University, Hsinchu, Taiwan<br>blacksnail789521.cs10@nycu.edu.tw, sf1638.cs05@nctu.edu.tw, wcpeng@cs.nycu.edu.tw, tfchen@cs.nycu.edu.tw</p>
<h4>Abstract</h4>
<p>Multivariate time-series forecasting is vital in various domains, e.g., economic planning and weather prediction. Deep train-from-scratch models have exhibited effective performance yet require large amounts of data, which limits real-world applicability. Recently, researchers have leveraged the representation learning transferability of pre-trained Large Language Models (LLMs) to handle limited non-linguistic datasets effectively. However, incorporating LLMs with time-series data presents challenges of limited adaptation due to different compositions between time-series and linguistic data, and the inability to process multi-scale temporal information. To tackle these challenges, we propose LLM4TS, a framework for time-series forecasting with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the time-series alignment stage to align LLMs with the nuances of time-series data, and the forecasting fine-tuning stage for downstream time-series forecasting tasks. Furthermore, our framework features a novel two-level aggregation method that integrates multi-scale temporal data within pre-trained LLMs, enhancing their ability to interpret time-specific information. In experiments across 7 time-series forecasting datasets, LLM4TS is superior to existing state-of-the-art methods compared with trained-from-scratch models in full-shot scenarios, and also achieves the highest rank in few-shot scenarios. In addition, evaluations compared with different unsupervised representation learning approaches highlight LLM4TS's effectiveness with representation learning in forecasting tasks. Ablation studies further validate each component's contribution to LLM4TS and underscore the essential role of utilizing LLM's pre-trained weights for optimal performance. The code is available at https://github.com/blacksnail789521/LLM4TS.</p>
<h2>1 Introduction</h2>
<p>Forecasting is a vital task in multivariate time-series analysis, not only for its ability to operate without manual labeling but also for its importance in practical applications such as economic planning $[21,33,6]$ and weather prediction $[51,23,14]$. Recently, numerous deep train-fromscratch models have been developed for time-series forecasting [46, 28, 40, 52, 51, 44], although some lean towards unsupervised representation learning [42, 45, 34, 8, 50, 5, 27] and transfer learning [47, 53, 11, 36, 12]. Generally, these approaches aim to employ adept representation learners: first extracting rich representations from the time-series data and then using these representations for forecasting.</p>
<p>Achieving an adept representation learner requires sufficient training data [15, 48, 37], yet in real-world scenarios, there is often a lack of large-scale time-series datasets. For instance, in industrial manufacturing, the sensor data for different products cannot be combined for further analysis, leading to limited data for each product type [43, 7, 1]. Recent research has pivoted towards pre-trained LLMs in Natural Language Processing (NLP) [29, 3, 35], exploiting their robust representation learning and few-shot learning capabilities. Moreover, these LLMs can adapt to non-linguistic datasets (e.g., images [26, 24], audio [9, 30], tabular data [13, 31], and</p>
<p>time-series data [53, 17]) by fine-tuning with only a few parameters and limited data. While LLMs are renowned for their exceptional transfer learning capabilities across various fields, the domain-specific nuances of time-series data introduce two challenges in leveraging these models for time-series forecasting.</p>
<p>The first challenge of employing LLMs for time-series forecasting is their limited adaptation to the unique characteristics of time-series data due to LLMs' initial pre-training focus on the linguistic corpus. While LLMs have been both practically and theoretically proven [53] to be effective in transfer learning across various modalities thanks to their data-independent selfattention mechanism, their primary focus on general text during pre-training causes a shortfall in recognizing key time-series patterns and nuances crucial for accurate forecasting. This limitation is evident in areas such as meteorology and electricity forecasting [51], where failing to account for weather patterns and energy consumption trends leads to inaccurate predictions.</p>
<p>The second challenge lies in the limited capacity to process multi-scale temporal information. While LLMs are adept at understanding the sequence and context of words, they struggle to understand temporal information due to the lack of utilizing multi-scale time-related data such as time units (e.g., seconds, minutes, hours, etc.) and specific dates (e.g., holidays, significant events). This temporal information is vital in time-series analysis for identifying and predicting patterns [40, 39]; for instance, in energy management, it is used to address consumption spikes during daytime and in summer/winter, in contrast to the lower demand during the night and in milder seasons [51]. This underscores the importance of models adept at interpreting multi-scale temporal patterns (hourly to seasonal) for precise energy demand forecasting. However, most LLMs (e.g., [29, 35]) built on top of the Transformer architecture do not naturally incorporate multi-scale temporal information, leading to models that fail to capture crucial variations across different time scales.</p>
<p>To address the above issues, we propose LLM4TS, a framework for time-series forecasting with pre-trained LLMs. Regarding the first challenge, our framework introduces a two-stage fine-tuning approach: the time-series alignment stage and the forecasting fine-tuning stage. The first stage focuses on aligning the LLMs with the characteristics of time-series data by utilizing the autoregressive objective, enabling the fine-tuned LLMs to adapt to time-series representations. The second stage is incorporated to learn corresponding time-series forecasting tasks. In this manner, our model supports effective performance in full- and few-shot scenarios. Notably, throughout both stages, most parameters in the pre-trained LLMs are frozen, thus preserving the model's inherent representation learning capability. To overcome the limitation of LLMs in integrating multi-scale temporal information, we introduce a novel two-level aggregation strategy. This approach embeds multi-scale temporal information into the patched time-series data, ensuring that each patch not only represents the series values but also encapsulates the critical time-specific context. Consequently, LLM4TS emerges as a data-efficient time-series forecaster, demonstrating robust few-shot performance across various datasets (Fig. 1).</p>
<p>In summary, the paper's main contributions are as follows:</p>
<ul>
<li>Aligning LLMs Toward Time-Series Data: To the best of our knowledge, LLM4TS is the first method that aligns pre-trained Large Language Models with time-series characteristics, effectively utilizing existing representation learning and few-shot learning capabilities.</li>
<li>Multi-Scale Temporal Information in LLMs: To adapt to time-specific information, a two-level aggregation method is proposed to integrate multi-scale temporal data within pre-trained LLMs.</li>
<li>Robust Performance in Forecasting: LLM4TS excels in 7 real-world time-series forecasting benchmarks, outperforming state-of-the-art methods, including those trained from scratch. It also demonstrates strong few-shot capabilities, particularly with only $5 \%$ of</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Model performance comparison on few-shot forecasting.
data, where it surpasses the best baseline that uses $10 \%$ of data. This efficiency makes LLM4TS highly relevant for practical, real-world forecasting applications.</p>
<h1>2 Related Work</h1>
<h3>2.1 Transfer Learning Across Various Modalities with LLMs</h3>
<p>LLMs have demonstrated their effectiveness in transfer learning across a variety of modalities, such as images [26, 24], audio [9, 30], tabular data [13, 31], and time-series data [53, 17]. A key motivation for employing LLMs in various modalities is their ability to achieve notable performance with limited data [53]. To preserve their data-independent representation learning capability, most parameters in these LLMs are kept fixed. Empirical evidence [26, 53] indicates that LLMs keeping most parameters unchanged often outperform those trained from scratch, underscoring the value of maintaining these models' pre-existing representation learning strengths (more experiments can be found in Section 5.4.3). Theoretically, it is shown that the self-attention modules in these pre-trained transformers develop the capacity for dataindependent operations (akin to principal component analysis [53]), enabling them to function effectively as universal compute engines [26] or general computation calculators [10]. In the time-series domain, GPT4TS [53] utilizes the pre-trained GPT-2 and demonstrates strong performance in time-series forecasting under few-shot conditions without modifying most parameters. Time-LLM [18] reprograms LLMs for time series forecasting by converting time series into text prototypes and using prompts to guide prediction. It outperforms specialized models, particularly in few-shot and zero-shot settings. TEMPO [4] adapts GPT-like models for time series forecasting by decomposing trends and using prompts for better distribution adaptation, excelling in zero-shot scenarios. TEST [32] aligns time series with LLM embeddings through tokenization and contrastive learning, enabling effective time series forecasting using pre-trained LLMs without fine-tuning. With our LLM4TS, we address the challenges of limited adaptation to time-series characteristics and the difficulty in processing multi-scale temporal information, thereby enhancing performance in time-series forecasting.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Problem formulation for multivariate time-series forecasting.</p>
<h3>2.2 Long-term Time-Series Forecasting</h3>
<p>Numerous efforts have been dedicated to employing Transformer models for long-term timeseries forecasting [51, 40, 52, 28, 49, 22]. While Transformer-based models have gained traction, DLinear [46] reveals that a single-layer linear model can surpass many of these sophisticated Transformer-based approaches. These deep train-from-scratch models exhibit outstanding performance when trained on sufficient datasets, but their efficacy decreases in limited-data scenarios. In contrast, LLM4TS sets new benchmarks alongside these state-of-the-art approaches in both full- and few-shot scenarios.</p>
<h3>2.3 Time-Series Representation Learning</h3>
<p>In the time-series domain, self-supervised learning emerges as a prominent approach to representation learning. While Transformers are widely recognized as prime candidates for end-to-end time-series analysis [41, 25, 28, 38, 2], CNN-based [45] or RNN-based [34] backbones consistently stand out as the preferred architecture in time-series self-supervised learning. However, the inherent capability of Transformers to model long-range dependencies and capture patterns aligns perfectly with time-series data, which involve complex sequential relationships. Since the time-series alignment stage in LLM4TS can be seen as a self-supervised learning approach, we evaluate LLM4TS's representation learning capability and demonstrate the full potential of Transformers in unsupervised representation learning, surpassing the performance of conventional CNN and RNN-based models.</p>
<h2>3 Problem Formulation</h2>
<p>Given a complete and evenly-sampled multivariate time-series, we use a sliding data window to extract sequential samples, as illustrated in Fig. 2. This window moves with a stride of 1 and has a total length of $T_{in}+T_{out}$ — comprising past data $\mathbf{x}<em T__in="T_{in">{in}=(d_1, \ldots, d</em>}})$ with a look-back window length $T_{in}$ and future data $\mathbf{x<em T__in="T_{in">{out}=(d</em>}+1}, \ldots, d_{T_{in}+T_{out}})$ with a prediction length $T_{out}$. For each time step $t$, $d_t$ represents a $C$-dimensional vector, where $C$ denotes the number of features. Our objective is to use the past data $\mathbf{x<em in="in">{in} \in \mathbb{R}^{T</em>} \times C}$ to predict the future data $\mathbf{x<em out="out">{out} \in \mathbb{R}^{T</em>$.} \times C</p>
<h2>4 The Proposed LLM4TS</h2>
<p>Fig. 3 illustrates our LLM4TS framework, leveraging the pre-trained GPT-2 [29] as the backbone model. We first introduce the time-series alignment stage, which focuses on aligning the LLMs with the characteristics of time-series data using an autoregressive objective (Section</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: LLM4TS framework. The numbers in the patched time-series (e.g., 1, 2, ..., 16 in the first patch) indicate the sequential order of the timestamps. The framework consists of two stages: (a) Time-series alignment, which uses the autoregressive approach to align the pre-trained LLM with patched time-series data. (b) Forecasting fine-tuning, which starts with linear probing (i.e., only the output layer is unfrozen), followed by full fine-tuning (all the layers and PEFT components in the LLM are unfrozen).</p>
<p>4.1). Subsequently, the forecasting fine-tuning stage is designed to further enhance the model's ability to handle time-series forecasting tasks (Section 4.2).</p>
<h3>4.1 Time-Series Alignment</h3>
<p>Existing LLMs are pre-trained on a general language corpus, which means they fail to learn contextualized information outside linguistic domains; therefore, the <em>time-series alignment</em> stage is proposed to align LLMs with the characteristics of time-series data. Given our selection of GPT-2 [29] as the backbone model, which is a causal language model, we ensure that this stage adopts the same <em>autoregressive</em> training methodology used during its pre-training phase. Fig. 3(a) illustrates the autoregressive objective in the time-series alignment stage: given an input sequence of patched time-series data (e.g., 1<sup>st</sup> patch, 2<sup>nd</sup> patch, 3<sup>rd</sup> patch, etc.), the backbone model generates an output sequence shifted one patch to the right (e.g., 2<sup>nd</sup> patch, 3<sup>rd</sup> patch, 4<sup>th</sup> patch, etc.).</p>
<p><strong>Instance Normalization</strong> Data normalization is essential for stable performance when adapting pre-trained models across various modalities. Alongside the layer normalization used in the pre-trained LLM, we incorporate instance normalization to improve consistency and reliability in handling diverse time-series datasets. In our model, instance normalization is employed without incorporating a trainable affine transformation. This is crucial because when a batch of data is gathered and instance normalization is applied with a trainable affine transformation, the resulting transformed data becomes unsuitable to be the ground truth for the output. Given that an autoregressive objective is used at this stage, applying a trainable affine transformation is not feasible.</p>
<p>Given an input time-series sample <strong>x</strong><sub>in</sub> ∈ <strong>R</strong><sup>T<sub>in</sub>×<sup>C</sup></sup>, we apply instance normalization (IN) to produce a normalized time-series sample <strong>x</strong><sub>normed</sub> ∈ <strong>R</strong><sup>T<sub>in</sub>×<sup>C</sup></sup> with zero mean and unit standard deviation:</p>
<p>$$\mathbf{x}<em in="in">{normed} = \text{IN}(\mathbf{x}</em>$$}). \tag{1</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Multi-scale temporal encoding for <em>patched</em> time-series data. This process involves a two-level aggregation. Here, only the first patch is shown for simplicity; in practice, all patches in a batch are processed simultaneously. Level 1 aggregation calculates the temporal embedding for each time unit and sums them together. Next, Level 2 aggregation applies a pooling method to extract the final temporal embedding.</p>
<p>Time-Series Tokenization The context window sizes in pre-trained LLMs (e.g., 1024 in GPT-2) are sufficient for NLP tasks but are inadequate for long-term time-series forecasting. In our experiments, a prediction length of 720 combined with a look-back window size of 512 easily exceeds these limits. To address this, we adopt <em>channel-independence</em> along with <em>patching</em> [28] for time-series tokenization, effectively resolving the context window size constraint and simultaneously reducing the time and space complexity of the Transformer quadratically. Channel-independence converts multivariate time-series data into multiple univariate time-series data, thus transforming the data's dimension to $\mathbb{R}^{T_{in} \times 1}$, with the channel dimension $C$ merged into the batch size dimension. The subsequent patching step groups adjacent time steps into a singular patch-based token, reducing the input sample's time dimension from $T_{i n}$ to $T_{p}$, where $T_{p}$ denotes the number of patches, and concurrently expanding the feature dimension from 1 to $P$, with $P$ representing the patch length.</p>
<p>Given a normalized time-series sample $\mathbf{x}<em i="i" n="n">{\text{normed}} \in \mathbb{R}^{T</em>$ :} \times C}$, we first apply channel-independence (CI), and then patching to produce a series of patches $\mathbf{p} \in \mathbb{R}^{T_{p} \times P</p>
<p>$$
\mathbf{p} = \text{patching}(\text{CI}(\mathbf{x}_{\text{normed}}))
$$</p>
<p>Three Encodings for Patched Time-Series Data Given our goal to adapt a pre-trained LLM for time-series data, the original token encoding layer (designed for text) becomes unsuitable due to the mismatched modalities. Additionally, we design a new multi-scale temporal encoding layer to address the inability to process multi-scale temporal information.</p>
<p>Given a series of tokens, applying token encoding is necessary to align their dimensions with the latent embedding dimension of the pre-trained LLM. In standard NLP practices, this encoding uses a trainable lookup table to map tokens into a high-dimensional space. However, this method only suits <em>scalar</em> tokens, whereas our patched time-series data are <em>vectors</em>. Therefore, we drop the original token encoding layer in the LLM, and employ a one-dimensional convolutional layer $\text{Conv}<em _text_token="\text{token">{\text{token}}$ as our new token encoding layer. As opposed to employing a linear layer [53], we choose a convolutional layer due to its superior ability to retain local semantic information within the time-series data. This results in the generation of the token embedding $\mathbf{e}</em>$, where $D$ denotes the dimension of the embeddings:}} \in \mathbb{R}^{T_p \times D</p>
<p>$$
\mathbf{e}<em _text_token="\text{token">{\text{token}} = \text{Conv}</em>)
$$}}(\mathbf{p</p>
<p>For the positional encoding layer, we employ a trainable lookup table $E_{\text {pos }}$ to map patch locations. This results in the generation of the positional embedding $\mathbf{e}<em p="p">{\text {pos }} \in \mathbb{R}^{T</em>$ :} \times D</p>
<p>$$
\mathbf{e}<em o="o" p="p" s="s">{p o s}=E</em>)
$$}(\mathbf{i</p>
<p>where $\mathbf{i} \in \mathbb{R}^{T_{p}}$ represents the indices of the patch locations.
To address the challenge LLMs face in processing multi-scale temporal information, we introduce a multi-scale temporal encoding layer. When processing time-related data, we face two challenges due to the need to aggregate multiple pieces of information into one unified representation (Fig. 4):</p>
<ol>
<li>Each timestamp includes a range of multi-scale temporal attributes (e.g., seconds, minutes, hours, holidays, etc.).</li>
<li>Each patch encompasses multiple timestamps.</li>
</ol>
<p>To address the first challenge associated with diverse temporal attributes within a timestamp, we employ Level 1 aggregation: a trainable lookup table for each temporal attribute (e.g., $E_{\text {sec }}, E_{\text {min }}, \ldots$ ), mapping it into a high-dimensional space, and then summing them to produce a singular temporal embedding. In response to the second challenge of multiple timestamps within a patch, we use Level 2 aggregation: a pooling method to extract the final temporal embedding. For the pooling method, we opt for the "select first" method, where the initial timestamp is designated as representative of the entire patch. This is because the first timestamp often carries the most significant and representative information for the entire duration covered by the patch, especially in time-series data where earlier events can have a substantial influence on the subsequent sequence. This process generates the final temporal embedding $\mathbf{e}<em p="p">{\text {temp }} \in \mathbb{R}^{T</em>$ :} \times D</p>
<p>$$
\mathbf{e}<em _="{" _in_123_text="\in{\text" a="a" sec_min_hour_="sec,min,hour,">{\text {temp }}=\text { Pooling }\left(\sum</em>\right)\right)
$$} \ldots}} E_{a}\left(\mathbf{t}_{a</p>
<p>where $a$ represents different temporal attributes (seconds, minutes, hours, holidays, etc.), $E_{a}$ denotes the trainable lookup table for each temporal attribute, $\mathbf{t}<em p="p">{a} \in \mathbb{R}^{T</em>$ are the series of patches containing temporal information for that temporal attribute, and Pooling applies the pooling method to the aggregated embeddings.} \times P</p>
<p>Finally, the token, positional, and temporal embeddings are summed to yield the final embedding $\mathbf{e} \in \mathbb{R}^{T_{p} \times D}$, which is then fed into the pre-trained Transformer blocks:</p>
<p>$$
\mathbf{e}=\mathbf{e}<em o="o" p="p" s="s">{\text {token }}+\mathbf{e}</em>
$$}+\mathbf{e}_{t e m p</p>
<p>Pre-Trained LLM To preserve LLMs' data-independent representation learning capability, most parameters in these LLMs are kept fixed. Empirical evidence [26, 53] shows that training these LLMs from scratch often hurts performance, highlighting the importance of fixing most parameters to retain the LLM's representation learning capability. To that end, we opt for freezing most parameters, particularly those associated with the multi-head attention and feedforward layers in the Transformer block, as they are the most responsible for representation learning [53].</p>
<p>For the remaining trainable parameters in the pre-trained LLM, we employ two ParameterEfficient Fine-Tuning (PEFT) methods to selectively adjust or introduce a limited set of trainable parameters. Specifically, we utilize Layer Normalization Tuning [26] to adjust pre-existing parameters in Transformer blocks, making the affine transformation in layer normalization trainable. Concurrently, we employ Low-Rank Adaptation (LoRA) [16], which introduces trainable low-rank matrices that are applied to the query $(\mathrm{Q})$ and key $(\mathrm{K})$ matrices in the self-attention</p>
<p>mechanism. With these two PEFT techniques, only $1.5 \%$ of the pre-trained LLM's total parameters are used to be trained.</p>
<p>Given the embedding $\mathbf{e}$ (which is adjusted to the required embedding dimension $D$ by three encoding layers), we pass it into the pre-trained LLM, which comprises a series of pre-trained Transformer blocks (TBs) (with $L$ blocks in total). This process yields the final embeddings $\mathbf{z} \in \mathbb{R}^{T_{p} \times D}$ :</p>
<p>$$
\mathbf{z}=\operatorname{TBs}(\mathbf{e})
$$</p>
<p>After being processed by the pre-trained LLM, we employ a linear output layer $W_{t s a} \in \mathbb{R}^{P \times D}$ to transform the output embedding back to patched time-series data:</p>
<p>$$
\hat{\mathbf{p}}<em a="a" s="s" t="t">{\text {shifted }}=\mathbf{z} W</em>
$$}^{\top</p>
<p>where $\hat{\mathbf{p}}<em p="p">{\text {shifted }} \in \mathbb{R}^{T</em>} \times P}$ represents our prediction target, corresponding to the original timeseries patches ( $\mathbf{p}$ ) shifted one patch to the right, in line with the autoregressive objective of this stage. To ensure the prediction precisely reconstructs the actual shifted patched data $\mathbf{p<em p="p">{\text {shifted }} \in \mathbb{R}^{T</em>$, we use the Mean Squared Error (MSE) as the loss function:} \times P</p>
<p>$$
\mathcal{L}<em _shifted="{shifted" _text="\text">{t s a}=\operatorname{MSE}\left(\mathbf{p}</em>\right)
$$}}, \hat{\mathbf{p}}_{\text {shifted }</p>
<h1>4.2 Forecasting Fine-tuning</h1>
<p>After aligning the pre-trained LLM with patched time-series data in the time-series alignment stage, we transfer the trained weights of the backbone model, including those from the encoding layers, to the forecasting fine-tuning stage. When fine-tuning the backbone model for the forecasting task, two primary training strategies are available: full fine-tuning (where all model parameters are updated) and linear probing (where only the final linear output layer is updated). Studies have shown that a sequential approach - initial linear probing followed by full fine-tuning (LP-FT, as illustrated in Fig. 3(b))-consistently surpasses strategies exclusively employing either method [20]. The superiority of LP-FT is due to its dual-phase approach: first finding an optimized output layer to minimize later adjustments in fine-tuning (preserving feature extractor efficacy for out-of-distribution (OOD) scenarios), and then employing full fine-tuning to adapt the model to the specific task (enhancing in-distribution (ID) accuracy) [20].</p>
<p>For the model architecture in the forecasting fine-tuning stage, we preserve most of the structure as in the time-series alignment stage, including the three encoding layers and the pre-trained LLM. However, there are two architectural differences in this stage: instance normalization and the output layer.</p>
<p>The first architectural difference is in the instance normalization, where we adopt Reversible Instance Normalization (RevIN) [19] to enhance forecasting accuracy. RevIN involves batchspecific instance normalization and subsequent denormalization, both sharing the same trainable affine transformation. The additional denormalization step addresses distribution shifts between training and testing data, which is a common challenge in the time-series domain (e.g., seasonal changes). Therefore, during the time-series tokenization step, we apply RevIN's normalization, succeeded by channel-independence and patching:</p>
<p>$$
\mathbf{p}=\text { patching }\left(\mathrm{CI}\left(\operatorname{RevIN}<em i="i" n="n">{\text {norm }}\left(\mathbf{x}</em>\right)\right)\right)
$$</p>
<p>Notably, the denormalization step is applicable only to unpatched time-series data; hence, in the time-series alignment stage, standard instance normalization is employed.</p>
<p>The second architectural difference lies in the output layer, whose function is to transform the final embedding $\mathbf{z}$ into the predicted future data, presented in the general (unpatched) timeseries format. This involves flattening the data and passing it through the linear output layer</p>
<p>Table 1: Statistical overview of the 7 datasets for long-term time-series forecasting.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">Features</th>
<th style="text-align: center;">Timesteps</th>
<th style="text-align: center;">Granularity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Weather</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">52,696</td>
<td style="text-align: center;">10 min</td>
</tr>
<tr>
<td style="text-align: center;">Traffic</td>
<td style="text-align: center;">862</td>
<td style="text-align: center;">17,544</td>
<td style="text-align: center;">1 hour</td>
</tr>
<tr>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">321</td>
<td style="text-align: center;">26,304</td>
<td style="text-align: center;">1 hour</td>
</tr>
<tr>
<td style="text-align: center;">ETTh1 \&amp; ETTh2</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">17,420</td>
<td style="text-align: center;">1 hour</td>
</tr>
<tr>
<td style="text-align: center;">ETTm1 \&amp; ETTm2</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">69,680</td>
<td style="text-align: center;">5 min</td>
</tr>
</tbody>
</table>
<p>$W_{f f t} \in \mathbb{R}^{T_{\text {out }} \times T_{p} \cdot D}$, followed by rearrangement, and then applying RevIN's denormalization to obtain the final prediction $\hat{\mathbf{x}}<em _out="{out" _text="\text">{\text {out }} \in \mathbb{R}^{T</em>$ :}} \times C</p>
<p>$$
\hat{\mathbf{x}}<em _denorm="{denorm" _text="\text">{\text {out }}=\operatorname{RevIN}</em>\right)\right.
$$}}\left(\operatorname{Rearrange}\left((\operatorname{Flatten}(\mathbf{z})) W_{f f t}^{T</p>
<p>To ensure that this prediction accurately reconstructs the future data $\mathbf{x}<em _out="{out" _text="\text">{\text {out }} \in \mathbb{R}^{T</em>$, we use MSE as the loss function:}} \times C</p>
<p>$$
\mathcal{L}<em _out="{out" _text="\text">{f f t}=\operatorname{MSE}\left(\mathbf{x}</em>\right)
$$}}, \hat{\mathbf{x}}_{\text {out }</p>
<h1>5 Experiments</h1>
<p>Datasets In our long-term forecasting analysis, we experiment on 7 real-world, publicly accessible benchmark datasets. We present detailed statistics for these datasets in Table 1, including the number of features, the total length of the datasets, and their sampling frequency.</p>
<p>Weather ${ }^{1}$ comprises local climatological data spanning 4 years for approximately 1,600 locations in the U.S. It includes 11 weather variables in each record, in addition to the target variable 'wet bulb.' Traffic ${ }^{2}$ consists of hourly observations from the California Department of Transportation, detailing road occupancy rates captured by various sensors located on freeways in the San Francisco Bay area. Electricity ${ }^{3}$ comprises hourly power usage data for 321 customers, spanning from 2012 to 2014, with 'MT_320' set as the target variable. ETT [51] focuses on long-duration electric power deployment data. The collection includes two datasets sampled hourly (ETTh1, ETTh2) and two datasets sampled every 15 minutes (ETTm1, ETTm2), covering a period of over two years from various provinces in China. Each dataset in the ETT series features one oil temperature variable alongside six power load variables.</p>
<p>Evaluation Metrics In time-series forecasting, Mean Squared Error (MSE) and Mean Absolute Error (MAE) are commonly utilized metrics for evaluating performance. The Mean Squared Error (MSE) can be expressed as:</p>
<p>$$
\mathrm{MSE}=\frac{1}{N} \sum_{n=1}^{N}\left(x_{\text {out }}-\hat{x}_{\text {out }}\right)^{2}
$$</p>
<p>Here, $x_{\text {out }}$ indicates the actual future data that corresponds to the past data $x_{\text {in }}$, while $\hat{x}_{\text {out }}$ represents the predicted future data based on the input past data. $N$ is the total number of samples. The Mean Absolute Error (MAE) is given by:</p>
<p>$$
\mathrm{MAE}=\frac{1}{N} \sum_{n=1}^{N}\left|x_{\text {out }}-\hat{x}_{\text {out }}\right|
$$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Baselines For long-term time-series forecasting, we focus on a range of state-of-the-art models. The same set of models is used for few-shot learning and ablation studies. GPT4TS [53] employs patching and channel independence to initially transform time-series data into tokens. It then utilizes a pre-trained Large Language Model (GPT-2), maintaining the pre-trained weights in the self-attention and feedforward layers of the residual blocks from the pre-trained LLM. DLinear [46] challenges the prevailing use of Transformer-based models for long-term time-series forecasting. It introduces a simple one-layer linear model that surprisingly outperforms complex Transformer-based models on several real datasets. PatchTST [28] introduces a Transformer-based approach for time-series forecasting, focusing on efficiency by employing patching and channel-independence to transform time-series data into patches. FEDformer [52] combines Transformers with seasonal-trend decomposition and frequency enhancement for efficient and effective long-term series forecasting, overcoming traditional Transformer limitations by capturing both global trends and detailed structures with linear complexity. TimeLLM [18] reprograms LLMs for time series forecasting by converting time series into text prototypes and using prompts to guide prediction. It outperforms specialized models, particularly in few-shot and zero-shot settings. TEMPO [4] adapts GPT-like models for time series forecasting by decomposing trends and using prompts for better distribution adaptation, excelling in zero-shot scenarios. TEST [32] aligns time series with LLM embeddings through tokenization and contrastive learning, enabling effective time series forecasting using pre-trained LLMs without fine-tuning.</p>
<p>For unsupervised representation learning in time-series analysis, we explore advanced models that excel in extracting meaningful representations without relying on labeled data. PatchTST [28] in this context is a variant distinct from the one used in forecasting experiments, with an emphasis on representation learning. It adopts an MLM (Masked Language Model) strategy, similar to BERT, to learn representations. BTSF [42] introduces a Bilinear Temporal-Spectral Fusion framework to improve time-series representation learning by integrating temporal and spectral information. This approach minimizes sampling bias and optimizes feature representation through instance-level augmentation and fusion techniques. TS2Vec [45] is the first universal framework dedicated to learning representations of time-series data. It emphasizes distinguishing multi-scale contextual information on both the instance and timestamp levels, demonstrating effectiveness in various time-series tasks. TNC [34] utilizes the Augmented Dickey-Fuller test to detect temporal neighborhoods and implements Positive-Unlabeled learning to address sampling bias. TS-TCC [8] produces two different views via strong and weak augmentations and enhances representations through contrastive learning, focusing on the temporal and contextual differences between these views.</p>
<p>Implementation Details For our experiments in long-term time-series forecasting, few-shot learning, and ablation studies, we utilize the settings from PatchTST [28] for a consistent comparison. We first explore the model's performance in few-shot scenarios, followed by a comprehensive evaluation under full-shot settings to ensure a thorough and balanced analysis. We set our look-back window length $T_{i n}$ to either 336 or 512 (reporting the best results), and configure the patch length $P$ as 16 with a stride $S$ of 8 . For unsupervised representation learning, the settings are slightly adjusted to $T_{i n}=512, P=12$, and $S=12$. Aligned with the GPT4TS configuration [53], we utilize only the first 6 layers of the 12-layer GPT-2 base [29].</p>
<h1>5.1 Few-Shot Learning in Long-Term Time-Series Forecasting</h1>
<p>Table 2 shows the results of long-term time-series forecasting using only $5 \%$ of the training data, while Table 3 presents similar results but with merely $10 \%$ of the training data utilized. In our experiments, consistent splits for training, validation, and test sets are maintained across both full and few-shot learning scenarios. We deliberately limited the training data percentage to $5 \%$ and $10 \%$ to evaluate model performance in few-shot scenarios. For each dataset, we train a single</p>
<p>Table 2: Few-shot long-term forecasting using $5 \%$ of the training data. For most datasets, results are reported over prediction lengths $T_{\text {out }} \in{96,192,336,720}$. However, for datasets marked with * (ETTh1, ETTh2, and Traffic), only $T_{\text {out }} \in{96,192,336}$ are used because there are insufficient data to constitute a training set when $T_{\text {out }}=720$. The best results are in bold, while the second-best results are in underlined. Note that TEMPO* is evaluated under a zero-shot setting as it is a prompt-based method.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLM4TS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT4TS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PatchTST</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Time-LLM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TEMPO*</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TEST</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE MAE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE MAE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE MAE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE MAE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE MAE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE MAE</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Weather</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.172</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">0.276</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.283</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.325</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.388</td>
</tr>
<tr>
<td style="text-align: center;">ETTh1</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.447</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.717</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.570</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.533</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.728</td>
<td style="text-align: center;">0.589</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.619</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.636</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ETTh2</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.456</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.336</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.457</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.486</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">1.424</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.428</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ETTm1</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.381</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.451</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.515</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.464</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.499</td>
</tr>
<tr>
<td style="text-align: center;">ETTm2</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.275</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.286</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.373</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.674</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.439</td>
</tr>
<tr>
<td style="text-align: center;">ECL</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.246</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.248</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.304</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.205</td>
<td style="text-align: center;">0.277</td>
</tr>
<tr>
<td style="text-align: center;">Traffic</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.317</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.320</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.323</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Avg. Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.120</td>
<td style="text-align: center;">2.200</td>
<td style="text-align: center;">4.200</td>
<td style="text-align: center;">4.000</td>
<td style="text-align: center;">4.680</td>
<td style="text-align: center;">5.080</td>
<td style="text-align: center;">4.760</td>
<td style="text-align: center;">4.640</td>
<td style="text-align: center;">2.720</td>
<td style="text-align: center;">2.920</td>
<td style="text-align: center;">4.400</td>
<td style="text-align: center;">4.280</td>
<td style="text-align: center;">5.000</td>
<td style="text-align: center;">4.640</td>
</tr>
</tbody>
</table>
<p>model in the time-series alignment stage, which is then applied consistently across all prediction lengths. In contrast, in the forecasting fine-tuning stage, we fine-tune a distinct model for each prediction length, while ensuring that all these models share the same hyperparameters.</p>
<p>Both LLM4TS and GPT4TS [53] consistently surpass most train-from-scratch models in limited-data scenarios across various datasets, thanks to the pre-existing representation learning capability encapsulated in GPT-2. With the additional time-series alignment and multi-scale temporal information integration, LLM4TS emerges as a better data-efficient time-series forecaster against GPT4TS, achieving better performance across all datasets. Notably, LLM4TS with only $5 \%$ of data outperforms the best baseline that uses $10 \%$ of data. For the largest dataset (Traffic), PatchTST emerges as the leading model in the full-shot scenario, though this trend does not extend to few-shot scenarios. With only $10 \%$ training data, LLM4TS outperforms PatchTST in 5 out of 8 evaluations, and with just $5 \%$ training data, it leads in 5 out of 6 evaluations. This suggests that in few-shot scenarios, traditional deep train-from-scratch models generally still underperform compared to those leveraging pre-trained LLMs.</p>
<p>Table 3: Few-shot long-term forecasting using $10 \%$ of the training data. We use prediction lengths $T \in{96,192,336,720}$ for all datasets. The best results are in bold, while the second-best results are underlined. Note that TEMPO* is evaluated under a zero-shot setting as it is a prompt-based method.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th></th>
<th>LLM4TS</th>
<th></th>
<th>GPT4TS</th>
<th></th>
<th>DLinear</th>
<th></th>
<th>PatchTST</th>
<th></th>
<th>Time-LLM</th>
<th></th>
<th>TEMPO*</th>
<th></th>
<th>TEST</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Metric</td>
<td></td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
</tr>
<tr>
<td>Weather</td>
<td>96</td>
<td>0.158</td>
<td>0.207</td>
<td>0.163</td>
<td>0.215</td>
<td>0.171</td>
<td>0.224</td>
<td>0.165</td>
<td>0.215</td>
<td>0.161</td>
<td>0.210</td>
<td>0.211</td>
<td>0.254</td>
<td>0.163</td>
<td>0.213</td>
</tr>
<tr>
<td></td>
<td>192</td>
<td>0.204</td>
<td>0.249</td>
<td>0.210</td>
<td>0.254</td>
<td>0.215</td>
<td>0.263</td>
<td>0.210</td>
<td>0.257</td>
<td>0.204</td>
<td>0.248</td>
<td>0.254</td>
<td>0.298</td>
<td>0.230</td>
<td>0.263</td>
</tr>
<tr>
<td></td>
<td>336</td>
<td>0.254</td>
<td>0.288</td>
<td>0.256</td>
<td>0.292</td>
<td>0.258</td>
<td>0.299</td>
<td>0.259</td>
<td>0.297</td>
<td>0.261</td>
<td>0.302</td>
<td>0.292</td>
<td>0.332</td>
<td>0.258</td>
<td>0.282</td>
</tr>
<tr>
<td></td>
<td>720</td>
<td>0.322</td>
<td>0.336</td>
<td>0.321</td>
<td>0.339</td>
<td>0.320</td>
<td>0.346</td>
<td>0.332</td>
<td>0.346</td>
<td>0.309</td>
<td>0.332</td>
<td>0.370</td>
<td>0.379</td>
<td>0.301</td>
<td>0.328</td>
</tr>
<tr>
<td>ETTh1</td>
<td>96</td>
<td>0.417</td>
<td>0.432</td>
<td>0.458</td>
<td>0.456</td>
<td>0.492</td>
<td>0.495</td>
<td>0.516</td>
<td>0.485</td>
<td>0.448</td>
<td>0.460</td>
<td>0.400</td>
<td>0.406</td>
<td>0.455</td>
<td>0.457</td>
</tr>
<tr>
<td></td>
<td>192</td>
<td>0.469</td>
<td>0.468</td>
<td>0.570</td>
<td>0.516</td>
<td>0.565</td>
<td>0.538</td>
<td>0.598</td>
<td>0.524</td>
<td>0.484</td>
<td>0.483</td>
<td>0.426</td>
<td>0.421</td>
<td>0.572</td>
<td>0.519</td>
</tr>
<tr>
<td></td>
<td>336</td>
<td>0.505</td>
<td>0.499</td>
<td>0.608</td>
<td>0.535</td>
<td>0.721</td>
<td>0.622</td>
<td>0.657</td>
<td>0.550</td>
<td>0.589</td>
<td>0.540</td>
<td>0.441</td>
<td>0.430</td>
<td>0.578</td>
<td>0.531</td>
</tr>
<tr>
<td></td>
<td>720</td>
<td>0.708</td>
<td>0.572</td>
<td>0.725</td>
<td>0.591</td>
<td>0.986</td>
<td>0.743</td>
<td>0.762</td>
<td>0.610</td>
<td>0.700</td>
<td>0.604</td>
<td>0.443</td>
<td>0.451</td>
<td>0.723</td>
<td>0.594</td>
</tr>
<tr>
<td>ETTh2</td>
<td>96</td>
<td>0.282</td>
<td>0.351</td>
<td>0.331</td>
<td>0.374</td>
<td>0.357</td>
<td>0.411</td>
<td>0.353</td>
<td>0.389</td>
<td>0.275</td>
<td>0.326</td>
<td>0.301</td>
<td>0.353</td>
<td>0.332</td>
<td>0.374</td>
</tr>
<tr>
<td></td>
<td>192</td>
<td>0.364</td>
<td>0.400</td>
<td>0.402</td>
<td>0.411</td>
<td>0.569</td>
<td>0.519</td>
<td>0.403</td>
<td>0.414</td>
<td>0.374</td>
<td>0.373</td>
<td>0.355</td>
<td>0.389</td>
<td>0.401</td>
<td>0.433</td>
</tr>
<tr>
<td></td>
<td>336</td>
<td>0.374</td>
<td>0.416</td>
<td>0.406</td>
<td>0.433</td>
<td>0.671</td>
<td>0.572</td>
<td>0.426</td>
<td>0.441</td>
<td>0.406</td>
<td>0.429</td>
<td>0.379</td>
<td>0.408</td>
<td>0.408</td>
<td>0.440</td>
</tr>
<tr>
<td></td>
<td>720</td>
<td>0.445</td>
<td>0.461</td>
<td>0.449</td>
<td>0.464</td>
<td>0.824</td>
<td>0.648</td>
<td>0.477</td>
<td>0.480</td>
<td>0.427</td>
<td>0.449</td>
<td>0.409</td>
<td>0.440</td>
<td>0.459</td>
<td>0.480</td>
</tr>
<tr>
<td>ETTm1</td>
<td>96</td>
<td>0.360</td>
<td>0.388</td>
<td>0.390</td>
<td>0.404</td>
<td>0.352</td>
<td>0.392</td>
<td>0.410</td>
<td>0.419</td>
<td>0.346</td>
<td>0.388</td>
<td>0.438</td>
<td>0.424</td>
<td>0.392</td>
<td>0.401</td>
</tr>
<tr>
<td></td>
<td>192</td>
<td>0.386</td>
<td>0.401</td>
<td>0.429</td>
<td>0.423</td>
<td>0.382</td>
<td>0.412</td>
<td>0.437</td>
<td>0.434</td>
<td>0.373</td>
<td>0.416</td>
<td>0.461</td>
<td>0.432</td>
<td>0.423</td>
<td>0.426</td>
</tr>
<tr>
<td></td>
<td>336</td>
<td>0.415</td>
<td>0.417</td>
<td>0.469</td>
<td>0.439</td>
<td>0.419</td>
<td>0.434</td>
<td>0.476</td>
<td>0.454</td>
<td>0.413</td>
<td>0.426</td>
<td>0.515</td>
<td>0.467</td>
<td>0.471</td>
<td>0.444</td>
</tr>
<tr>
<td></td>
<td>720</td>
<td>0.470</td>
<td>0.445</td>
<td>0.569</td>
<td>0.498</td>
<td>0.490</td>
<td>0.477</td>
<td>0.681</td>
<td>0.556</td>
<td>0.485</td>
<td>0.476</td>
<td>0.591</td>
<td>0.509</td>
<td>0.552</td>
<td>0.501</td>
</tr>
<tr>
<td>ETTm2</td>
<td>96</td>
<td>0.184</td>
<td>0.265</td>
<td>0.188</td>
<td>0.269</td>
<td>0.213</td>
<td>0.303</td>
<td>0.191</td>
<td>0.274</td>
<td>0.177</td>
<td>0.261</td>
<td>0.185</td>
<td>0.267</td>
<td>0.233</td>
<td>0.262</td>
</tr>
<tr>
<td></td>
<td>192</td>
<td>0.240</td>
<td>0.301</td>
<td>0.251</td>
<td>0.309</td>
<td>0.278</td>
<td>0.345</td>
<td>0.252</td>
<td>0.317</td>
<td>0.241</td>
<td>0.314</td>
<td>0.243</td>
<td>0.304</td>
<td>0.303</td>
<td>0.302</td>
</tr>
<tr>
<td></td>
<td>336</td>
<td>0.294</td>
<td>0.337</td>
<td>0.307</td>
<td>0.346</td>
<td>0.338</td>
<td>0.385</td>
<td>0.306</td>
<td>0.353</td>
<td>0.274</td>
<td>0.327</td>
<td>0.309</td>
<td>0.345</td>
<td>0.359</td>
<td>0.341</td>
</tr>
<tr>
<td></td>
<td>720</td>
<td>0.386</td>
<td>0.393</td>
<td>0.426</td>
<td>0.417</td>
<td>0.436</td>
<td>0.440</td>
<td>0.433</td>
<td>0.427</td>
<td>0.417</td>
<td>0.390</td>
<td>0.386</td>
<td>0.395</td>
<td>0.452</td>
<td>0.419</td>
</tr>
<tr>
<td>ECL</td>
<td>96</td>
<td>0.135</td>
<td>0.231</td>
<td>0.139</td>
<td>0.237</td>
<td>0.150</td>
<td>0.253</td>
<td>0.140</td>
<td>0.238</td>
<td>0.139</td>
<td>0.241</td>
<td>0.178</td>
<td>0.276</td>
<td>0.138</td>
<td>0.235</td>
</tr>
<tr>
<td></td>
<td>192</td>
<td>0.152</td>
<td>0.246</td>
<td>0.156</td>
<td>0.252</td>
<td>0.164</td>
<td>0.264</td>
<td>0.160</td>
<td>0.255</td>
<td>0.151</td>
<td>0.248</td>
<td>0.198</td>
<td>0.293</td>
<td>0.158</td>
<td>0.255</td>
</tr>
<tr>
<td></td>
<td>336</td>
<td>0.173</td>
<td>0.267</td>
<td>0.175</td>
<td>0.270</td>
<td>0.181</td>
<td>0.282</td>
<td>0.180</td>
<td>0.276</td>
<td>0.169</td>
<td>0.270</td>
<td>0.209</td>
<td>0.309</td>
<td>0.176</td>
<td>0.275</td>
</tr>
<tr>
<td></td>
<td>720</td>
<td>0.229</td>
<td>0.312</td>
<td>0.233</td>
<td>0.317</td>
<td>0.223</td>
<td>0.321</td>
<td>0.241</td>
<td>0.323</td>
<td>0.240</td>
<td>0.322</td>
<td>0.279</td>
<td>0.355</td>
<td>0.230</td>
<td>0.311</td>
</tr>
<tr>
<td>Traffic</td>
<td>96</td>
<td>0.402</td>
<td>0.288</td>
<td>0.414</td>
<td>0.297</td>
<td>0.419</td>
<td>0.298</td>
<td>0.403</td>
<td>0.289</td>
<td>0.418</td>
<td>0.291</td>
<td>0.476</td>
<td>0.343</td>
<td>0.415</td>
<td>0.317</td>
</tr>
<tr>
<td></td>
<td>192</td>
<td>0.416</td>
<td>0.294</td>
<td>0.426</td>
<td>0.301</td>
<td>0.434</td>
<td>0.305</td>
<td>0.415</td>
<td>0.296</td>
<td>0.414</td>
<td>0.296</td>
<td>0.496</td>
<td>0.355</td>
<td>0.425</td>
<td>0.300</td>
</tr>
<tr>
<td></td>
<td>336</td>
<td>0.429</td>
<td>0.302</td>
<td>0.434</td>
<td>0.303</td>
<td>0.449</td>
<td>0.313</td>
<td>0.426</td>
<td>0.304</td>
<td>0.421</td>
<td>0.311</td>
<td>0.503</td>
<td>0.356</td>
<td>0.436</td>
<td>0.310</td>
</tr>
<tr>
<td></td>
<td>720</td>
<td>0.480</td>
<td>0.326</td>
<td>0.487</td>
<td>0.337</td>
<td>0.484</td>
<td>0.336</td>
<td>0.474</td>
<td>0.331</td>
<td>0.462</td>
<td>0.327</td>
<td>0.538</td>
<td>0.376</td>
<td>0.489</td>
<td>0.338</td>
</tr>
<tr>
<td>Avg. Rank</td>
<td></td>
<td>2.036</td>
<td>1.679</td>
<td>4.000</td>
<td>3.786</td>
<td>5.143</td>
<td>5.679</td>
<td>5.036</td>
<td>5.071</td>
<td>2.214</td>
<td>2.786</td>
<td>4.786</td>
<td>4.821</td>
<td>4.536</td>
<td>3.857</td>
</tr>
</tbody>
</table>
<p>We also compare the latest LLM-based methods in the domain of time-series forecasting: Time-LLM, TEMPO, and TEST. It's important to note that TEMPO is evaluated under a zero-shot setting, as it is a prompt-based method. Consequently, in both few-shot and full-shot scenarios, TEMPO consistently reports zero-shot results. For larger datasets (like Weather, Electricity, and Traffic), LLM4TS consistently achieves the best performance, even with only $5 \%$ or $10 \%$ of the training data. TEMPO performs best on the ETTh1 and ETTh2 datasets, while Time-LLM leads on the ETTm1 and ETTm2 datasets. Overall, LLM4TS demonstrates superior performance across all these LLM-based methods in both the $5 \%$ and $10 \%$ training data scenarios.</p>
<h1>5.2 Full-Shot Learning in Long-Term Time-Series Forecasting</h1>
<p>Table 4 presents the results of long-term time-series forecasting averaged over a consistent prediction length set $T_{\text {out }} \in{96,192,336,720}$ for all datasets. While the primary focus of using pre-trained LLMs is on few-shot learning, LLM4TS not only excels in this area but also outper-</p>
<p>Table 4: Long-term forecasting for multivariate time-series data. We use prediction lengths $T \in{96,192,336,720}$ for all datasets. The best results are in bold, while the secondbest results are underlined. Note that TEMPO* is evaluated under a zero-shot setting as it is a prompt-based method.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLM4TS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT4TS</th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;">PatchTST</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Time-LLM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TEMPO*</th>
<th style="text-align: center;">TEST</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
</tr>
<tr>
<td style="text-align: center;">Weather</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.211</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">0.254</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.292</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.370</td>
</tr>
<tr>
<td style="text-align: center;">ETTh1</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.400</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.426</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.441</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.456</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.443</td>
</tr>
<tr>
<td style="text-align: center;">ETTh2</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.336</td>
<td style="text-align: center;">0.268</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.301</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.355</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.396</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.379</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.409</td>
</tr>
<tr>
<td style="text-align: center;">ETTm1</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.438</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.461</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.515</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.591</td>
</tr>
<tr>
<td style="text-align: center;">ETTm2</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.185</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.243</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.268</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.309</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.386</td>
</tr>
<tr>
<td style="text-align: center;">ECL</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.178</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.198</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.209</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.279</td>
</tr>
<tr>
<td style="text-align: center;">Traffic</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.476</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.496</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.503</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.538</td>
</tr>
<tr>
<td style="text-align: center;">Avg. Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.036</td>
<td style="text-align: center;">2.214</td>
<td style="text-align: center;">4.786</td>
<td style="text-align: center;">4.750</td>
<td style="text-align: center;">5.536</td>
<td style="text-align: center;">5.357</td>
<td style="text-align: center;">2.571</td>
<td style="text-align: center;">2.750</td>
<td style="text-align: center;">1.643</td>
<td style="text-align: center;">2.143</td>
<td style="text-align: center;">6.607</td>
</tr>
</tbody>
</table>
<p>forms all deep train-from-scratch methods, even with full dataset access, thanks to its two-stage fine-tuning and integration of multi-scale temporal information. In contrast, GPT4TS, despite utilizing the pre-trained GPT-2's representation learning capabilities, does not achieve superior performance over traditional train-from-scratch baselines in full-shot scenarios. This is particularly evident when handling extensive volumes of training data. This limitation mainly stems from its absence of time-series alignment and the inclusion of multi-scale temporal information, which is essential for enhancing performance in time-series forecasting. Interestingly, for the largest dataset (Traffic), PatchTST can outcompete both LLM4TS and GPT4TS. This suggests that with complete dataset access and sufficient data volume, traditional deep train-from-scratch models may sometimes outshine those leveraging pre-trained LLMs.</p>
<p>We also compare the latest LLM-based methods in the domain of time-series forecasting: Time-LLM, TEMPO, and TEST. It's important to note that TEMPO is evaluated under a zeroshot setting, as it is a prompt-based method, and therefore consistently reports zero-shot results in both few-shot and full-shot scenarios. Time-LLM emerges as the leading model, thanks to its innovative approach of reprogramming LLMs for time series forecasting by converting time</p>
<p>Table 5: Unsupervised representation learning evaluation in forecasting with linear probing. We use prediction lengths $T_{\text {out }} \in{24,48,168,336,720}$ for the ETTh1 dataset. The best average results are in bold, while the second-best results are in underlined.</p>
<p>| Methods | | LLM4TS | | PatchTST | | BTSF | | TS2Vec | | TNC | | TS-TCC | |
| Metric | | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ETTh1 | 24 | 0.315 | 0.365 | 0.322 | 0.369 | 0.541 | 0.519 | 0.599 | 0.534 | 0.632 | 0.596 | 0.653 | 0.610 |
| | 48 | 0.342 | 0.384 | 0.354 | 0.385 | 0.613 | 0.524 | 0.629 | 0.555 | 0.705 | 0.688 | 0.720 | 0.693 |
| | 168 | 0.401 | 0.415 | 0.419 | 0.424 | 0.640 | 0.532 | 0.755 | 0.636 | 1.097 | 0.993 | 1.129 | 1.044 |
| | 336 | 0.421 | 0.427 | 0.445 | 0.446 | 0.864 | 0.689 | 0.907 | 0.717 | 1.454 | 0.919 | 1.492 | 1.076 |
| | 720 | 0.426 | 0.447 | 0.487 | 0.478 | 0.993 | 0.712 | 1.048 | 0.790 | 1.604 | 1.118 | 1.603 | 1.206 |
| | Avg. | 0.381 | 0.408 | 0.405 | 0.420 | 0.730 | 0.595 | 0.788 | 0.646 | 1.098 | 0.863 | 1.119 | 0.926 |</p>
<p>series into text prototypes and using prompts to guide predictions. However, LLM4TS remains very close in performance to Time-LLM. Notably, LLM4TS still outperforms Time-LLM in few-shot scenarios, demonstrating its exceptional data efficiency and robustness in limited-data settings.</p>
<h1>5.3 Unsupervised Representation Learning</h1>
<p>Given that the autoregressive objective used in the time-series alignment stage can be seen as a pretext task in unsupervised representation learning, we aim to assess LLM4TS's representation learning capability. To evaluate the effectiveness of unsupervised representation learning, we conduct a linear evaluation on time-series forecasting. This involves pre-training the backbone model using the pretext task, freezing its weights, and then training an attached linear layer on the downstream forecasting task. With the backbone model's parameters fixed, strong performance in forecasting depends on the expressiveness of the learned representations. Table 5 shows LLM4TS's superior performance over competitors on the ETTh1 dataset, highlighting the effectiveness of adapting the LLM to time-series characteristics in the time-series alignment stage. This comparison exclusively includes self-supervised learning methods, thereby excluding deep train-from-scratch models designed explicitly for time-series forecasting. Similarly, GPT4TS is not part of this experiment as it lacks a distinct stage of representation learning. The variant of PatchTST used here differs from that in the forecasting experiments; this variant focuses on representation learning. PatchTST incorporates an MLM (Masked Language Model) approach, akin to BERT, for learning representations. Despite this, LLM4TS still emerges as the top method in representation learning capability among all evaluated methods, achieving an average improvement of $6.02 \%$ in MSE. Within the domain of unsupervised representation learning for time-series data, models based on CNN (such as BTSF, TS2Vec, TS-TCC) and RNN (like TNC) are typically favored over Transformers. However, our experiments reveal that Transformer-based models can surpass these traditional choices in performance with welldesigned pretext tasks.</p>
<h3>5.4 Ablation Study</h3>
<h3>5.4.1 Key Components in LLM4TS</h3>
<p>Fig. 5 explores the effects of time-series alignment, multi-scale temporal encoding, and PEFT in LLM4TS, assessing both full- and few-shot scenarios on the ETTh1 dataset. A comparative analysis - with and without these components - highlights their individual importance in enhancing forecasting accuracy in both scenarios. Notably, LLM4TS delivers exceptional per-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Ablation study on key components in LLM4TS. Each ablation is conducted under both full- and few-shot learning with 10% training data. We report results averaged over prediction lengths <em>Tout</em> ∈ {96, 192, 336, 720} for the ETTh1 dataset. The best results are in bold.</p>
<p>formance in few-shot learning, averaging a 6.2% reduction in MSE with each incorporation of these components.</p>
<p>In the experimental results, we observe three key insights. First, there is a notable trend where the MSE improvement increases as the prediction length extends. This indicates that the core elements of LLM4TS become increasingly beneficial in situations where a higher level of predictive capability is required, particularly evident with longer prediction lengths. Second, few-shot scenarios exhibit more substantial gains than full-shot scenarios upon integrating these main components into LLM4TS. It emphasizes LLM4TS's strength as a data-efficient time-series forecaster, a quality primarily attributed to its intrinsic components. Third, among the two PEFT methods, LoRA proves to be more beneficial than Layer Normalization. This advantage is consistently observed in both full-shot and few-shot scenarios, highlighting LoRA's effectiveness in enhancing the model's performance.</p>
<h3>5.4.2 Training Strategies in Forecasting Fine-Tuning</h3>
<p>As discussed in Section 4.2, while linear probing (LP) shows superior performance in out-of-distribution (OOD) scenarios and full fine-tuning (FT) excels in in-distribution (ID) scenarios, LP-FT can surpass FT and LP in both OOD and ID scenarios. Fig. 6 shows that LP-FT enhances performance in both full- and few-shot learning on the ETTh1 dataset, achieving an average improvement of 0.7% in MSE for full-shot learning and 2.51% for few-shot learning. The subtle improvements in both scenarios can be attributed to the limited number of trainable parameters in the LLM4TS's backbone model even when using FT, which narrows the distinction between LP and FT. The results further reveal that few-shot learning derives a greater advantage from LP-FT, primarily due to its higher vulnerability to severe OOD issues. Additionally, consistent with observations in the ablation study of LLM4TS's main components, we note a similar trend where longer prediction lengths yield more significant benefits in few-shot scenarios.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Ablation study on training strategies in forecasting fine-tuning. Each ab- lation is conducted under both full- and few-shot learning with 10% training data. We report results averaged over prediction lengths <em>Tout</em> ∈ {96, 192, 336, 720} for the ETTh1 dataset. The best results are in bold.</p>
<p>Table 6: Ablation study on the effectiveness of LLM's pre-trained weights. Each ablation is conducted under few-shot learning with 10% and 5% training data. 'No Freeze' refers to the model utilizing LLM's pre-trained weights without freezing any layers during training, whereas 'No Pretrain' denotes the model not utilizing LLM's pre-trained weights, implying the model is trained from scratch. We report results averaged over prediction lengths <em>Tout</em> ∈ {96, 192, 336, 720} for the Weather, ETTm1, and ETTm2 datasets. The best average results are in bold.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th></th>
<th>LLM4TS</th>
<th></th>
<th>GPT4TS</th>
<th></th>
<th>No Freeze</th>
<th></th>
<th>No Pretrain</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Metric</td>
<td></td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
</tr>
<tr>
<td>10%</td>
<td>Weather</td>
<td>0.235</td>
<td>0.270</td>
<td>0.238</td>
<td>0.275</td>
<td>0.273</td>
<td>0.302</td>
<td>0.278</td>
<td>0.305</td>
</tr>
<tr>
<td></td>
<td>ETTm1</td>
<td>0.408</td>
<td>0.413</td>
<td>0.464</td>
<td>0.441</td>
<td>0.546</td>
<td>0.484</td>
<td>0.473</td>
<td>0.446</td>
</tr>
<tr>
<td></td>
<td>ETTm2</td>
<td>0.276</td>
<td>0.324</td>
<td>0.293</td>
<td>0.335</td>
<td>0.340</td>
<td>0.367</td>
<td>0.361</td>
<td>0.385</td>
</tr>
<tr>
<td>5%</td>
<td>Weather</td>
<td>0.256</td>
<td>0.292</td>
<td>0.264</td>
<td>0.302</td>
<td>0.284</td>
<td>0.312</td>
<td>0.298</td>
<td>0.324</td>
</tr>
<tr>
<td></td>
<td>ETTm1</td>
<td>0.413</td>
<td>0.417</td>
<td>0.467</td>
<td>0.450</td>
<td>0.562</td>
<td>0.496</td>
<td>0.470</td>
<td>0.452</td>
</tr>
<tr>
<td></td>
<td>ETTm2</td>
<td>0.286</td>
<td>0.332</td>
<td>0.308</td>
<td>0.347</td>
<td>0.327</td>
<td>0.362</td>
<td>0.413</td>
<td>0.411</td>
</tr>
</tbody>
</table>
<p>5.4.3 Effectiveness of LLM's Pre-trained Weights</p>
<p>As discussed in Section 2.1, most parameters in these LLMs are kept fixed to preserve their data-independent representation learning capability. Table 6 shows that LLM4TS performs the best when most parameters remain unchanged on the Weather, ETTm1, and ETTm2 datasets. Specifically, LLM4TS demonstrates a notable average improvement of 17.78% in MSE com- pared to the 'No Freeze' approach, where pre-trained weights are utilized without freezing any layers during training. Furthermore, when compared to the 'No Pretrain' approach, where the model is trained from scratch without leveraging pre-trained weights, LLM4TS showcases an even more significant average improvement of 18.28% in MSE. This emphasizes the importance of retaining the pre-existing strengths in representation learning inherent to these models, at- tributable primarily to the self-attention mechanisms within pre-trained transformers, which facilitate the development of data-independent operations.</p>
<p>5.5 Training and Inference Cost</p>
<p>Evaluating the computational costs of LLM-based models is essential for determining their practicality in real-world scenarios. In this context, we compare LLM4TS with two other</p>
<p>Table 7: Training parameters.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Trainable Parameters</th>
<th>Total Parameters</th>
<th>Trainable Parameters Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLM4TS</td>
<td>3.4 M</td>
<td>85 M</td>
<td>$4 \%$</td>
</tr>
<tr>
<td>PatchTST</td>
<td>20 M</td>
<td>20 M</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>FEDformer</td>
<td>33 M</td>
<td>33 M</td>
<td>$100 \%$</td>
</tr>
</tbody>
</table>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparison of training and inference time (in seconds) for one batch. We use the prediction length $T_{\text {out }}=96$ for the ETTh2 dataset. The best results are in bold.
leading Transformer-based baselines, PatchTST and FEDformer. Details regarding the number of trainable parameters and the total parameters can be found in Table 7. LLM4TS distinguishes itself by keeping most pre-trained parameters fixed and employing two Parameter-Efficient FineTuning (PEFT) methods: Layer Normalization Tuning and LoRA. Consequently, only $4 \%$ of its parameters are trainable, rendering the number of trainable parameters in LLM4TS significantly lower than those in its train-from-scratch counterparts.</p>
<p>The execution time for both training and inference of LLM4TS, in comparison with PatchTST and FEDformer, is evaluated using an NVIDIA Tesla V100 GPU, and the results are illustrated in Fig. 7. To ensure a fair comparison across all methods, we standardized the batch size at 128 and set the hidden dimensions to 768 , aligning with the specifications of GPT-2. The evaluation was conducted on a single batch, and for LLM4TS, we provided the training time for both stages, as it involved a two-stage training process. LLM4TS outperformed the baselines in execution time during both the training and inference stages. This enhanced efficiency is credited to its architecture, which leverages a majority of non-trainable parameters, thus markedly diminishing the computational burden throughout both the training and inference phases.</p>
<h1>6 Conclusion</h1>
<p>In this paper, we present LLM4TS, a framework for time-series forecasting utilizing pre-trained LLMs. LLM4TS employs a two-stage fine-tuning strategy, beginning with the time-series alignment stage to adapt LLMs to the characteristics of time-series data, followed by the forecasting fine-tuning stage designed for time-series forecasting tasks. Our framework also introduces a novel two-level aggregation method, integrating multi-scale temporal data within pre-trained</p>
<p>LLMs to improve their interpretation of time-related information. Through experiments on 7 time-series forecasting datasets, LLM4TS demonstrates superior performance over existing state-of-the-art methods, including those trained from scratch, in both full and few-shot scenarios.</p>
<p>In future work, we plan to extend our research in two directions. First, while we chose GPT2 as our primary LLM in this paper for a fair comparison over GPT4TS, we plan to evaluate more recent LLMs like GPT-3.5 and LLaMA-2 to assess their advancements. Second, we aim to explore other tasks, such as classification and anomaly detection. Although forecasting is highly relevant to real-world applications without the need for manual labeling, extending it to other tasks enables the broader applicability of our LLM4TS framework.</p>
<h1>References</h1>
<p>[1] Akbar Abbaspour Ghadim Bonab. A comparative study of demand forecasting based on machine learning methods with time series approach. Journal of applied research on industrial engineering, 9(3):331-353, 2022.
[2] Sabeen Ahmed, Ian E. Nielsen, Aakash Tripathi, Shamoon Siddiqui, Ravi Prakash Ramachandran, and Ghulam Rasool. Transformers in time-series analysis: A tutorial. Circuits Syst. Signal Process., 42(12):7433-7466, 2023.
[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.
[4] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. TEMPO: Prompt-based generative pre-trained transformer for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024.
[5] Ching Chang, Chiao-Tung Chan, Wei-Yao Wang, Wen-Chih Peng, and Tien-Fu Chen. Timedrl: Disentangled representation learning for multivariate time-series. CoRR, abs/2312.04142, 2023.
[6] Ananda Chatterjee, Hrisav Bhowmick, and Jaydip Sen. Stock price prediction using time series, econometric, machine learning, and deep learning models. CoRR, abs/2111.01137, 2021 .
[7] Hao-Yi Chih, Yao-Chung Fan, Wen-Chih Peng, and Hai-Yuan Kuo. Product quality prediction with convolutional encoder-decoder architecture and transfer learning. In CIKM, pages 195-204. ACM, 2020.
[8] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In IJCAI, pages 2352-2359. ijcai.org, 2021.
[9] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-toaudio generation using instruction guided latent diffusion model. In ACM Multimedia, pages $3590-3598$. ACM, 2023.</p>
<p>[10] Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In ICML, volume 202 of Proceedings of Machine Learning Research, pages 11398-11442. PMLR, 2023 .
[11] Qi-Qiao He, Patrick Cheong-Iao Pang, and Yain-Whar Si. Transfer learning for financial time series forecasting. In PRICAI (2), volume 11671 of Lecture Notes in Computer Science, pages 24-36. Springer, 2019.
[12] Qi-Qiao He, Patrick Cheong-Iao Pang, and Yain-Whar Si. Multi-source transfer learning with ensemble for financial time series forecasting. In WI/IAT, pages 227-233. IEEE, 2020.
[13] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David A. Sontag. Tabllm: Few-shot classification of tabular data with large language models. In AISTATS, volume 206 of Proceedings of Machine Learning Research, pages $5549-5581$. PMLR, 2023.
[14] Pradeep Hewage, Ardhendu Behera, Marcello Trovati, Ella Pereira, Morteza Ghahremani, Francesco Palmieri, and Yonghuai Liu. Temporal convolutional neural (TCN) network for an effective weather forecasting using time-series data from the local weather station. Soft Comput., 24(21):16453-16482, 2020.
[15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. CoRR, abs/2203.15556, 2022.
[16] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In $I C L R$. OpenReview.net, 2022.
[17] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series forecasting by reprogramming large language models. CoRR, abs/2310.01728, 2023.
[18] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time series forecasting by reprogramming large language models. In The Twelfth International Conference on Learning Representations, 2024.
[19] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In ICLR. OpenReview.net, 2022.
[20] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In ICLR. OpenReview.net, 2022.
[21] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long- and short-term temporal patterns with deep neural networks. In SIGIR, pages 95-104. ACM, 2018 .
[22] Sangwon Lee, Junho Hong, Ling Liu, and Wonik Choi. Ts-fastformer: Fast transformer for time-series forecasting. ACM Trans. Intell. Syst. Technol., 15(2), feb 2024.</p>
<p>[23] Bryan Lim and Stefan Zohren. Time-series forecasting with deep learning: a survey. Philosophical Transactions of the Royal Society A, 379(2194):20200209, 2021.
[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.
[25] Minghao Liu, Shengqi Ren, Siyuan Ma, Jiahui Jiao, Yizhou Chen, Zhiguang Wang, and Wei Song. Gated transformer networks for multivariate time series classification. CoRR, abs/2103.14438, 2021.
[26] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Frozen pretrained transformers as universal computation engines. In AAAI, pages 7628-7636. AAAI Press, 2022.
[27] Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. Unsupervised representation learning for time series: A review. CoRR, abs/2308.01578, 2023.
[28] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In ICLR. OpenReview.net, 2023 .
[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1:9, 2019.
[30] Fangxun Shu, Lei Zhang, Hao Jiang, and Cihang Xie. Audio-visual LLM for video understanding. CoRR, abs/2312.06720, 2023.
[31] Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Table meets llm: Can large language models understand structured table data? a benchmark and empirical study. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, WSDM '24, page 645-654, New York, NY, USA, 2024. Association for Computing Machinery.
[32] Chenxi Sun, Hongyan Li, Yaliang Li, and Shenda Hong. TEST: Text prototype aligned embedding to activate LLM's ability for time series. In The Twelfth International Conference on Learning Representations, 2024.
[33] Hiteshi Tandon, Prabhat Ranjan, Tanmoy Chakraborty, and Vandana Suhag. Coronavirus (covid-19): Arima-based time-series analysis to forecast near future and the effect of school reopening in india. Journal of Health Management, 24(3):373-388, 2022.
[34] Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learning for time series with temporal neighborhood coding. In ICLR. OpenReview.net, 2021.
[35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023.
[36] Alexandros-Menelaos Tzortzis, Sotiris Pelekis, Evangelos Spiliotis, Spiros Mouzakitis, John E. Psarras, and Dimitris Askounis. Transfer learning for day-ahead load forecasting: a case study on european national electricity demand time series. CoRR, abs/2310.15555, 2023 .
[37] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://www.ncei.noaa.gov/data/local-climatological-data/
${ }^{2}$ http://pems.dot.ca.gov/
${ }^{3}$ https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
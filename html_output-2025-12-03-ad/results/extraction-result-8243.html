<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8243 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8243</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8243</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-274581897</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.05023v3.pdf" target="_blank">Steps are all you need: Rethinking STEM Education with Prompt Engineering</a></p>
                <p><strong>Paper Abstract:</strong> Few shot and Chain-of-Thought prompting have shown promise when applied to Physics Question Answering Tasks, but are limited by the lack of mathematical ability inherent to LLMs, and are prone to hallucination. By utilizing a Mixture of Experts (MoE) Model, along with analogical prompting, we are able to show improved model performance when compared to the baseline on standard LLMs. We also survey the limits of these prompting techniques and the effects they have on model performance. Additionally, we propose Analogical CoT prompting, a prompting technique designed to allow smaller, open source models to leverage Analogical prompting, something they have struggled with, possibly due to a lack of specialist training data.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8243.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8243.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source decoder-only transformer language model (~7 billion parameters) used in this paper as a baseline small model; quantized and LoRA-adapted for inference in experiments on STEM question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source ~7B parameter decoder-only transformer; in this work it was quantized and adapted with LoRA for inference and evaluated on physics & math QA.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['0-shot (baseline)', 'Few-shot Chain-of-Thought (K-shot CoT)', 'Analogical Prompting', 'Combined CoT + Analogical Prompting (Analogical CoT)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>0-shot: direct prompting without exemplars; Few-shot CoT: K example step-wise Chain-of-Thought prompts constructed using dataset steps to elicit intermediate reasoning; Analogical Prompting: asking the model to self-supply relevant exemplars and recall them to solve the target; Combined (Analogical CoT): providing CoT few-shot examples plus instructing the model to generate/recall analogical exemplars to guide solving.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (multiple methods tested; the model relied effectively on CoT but failed to leverage analogical prompting robustly)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared baseline 0-shot vs K-shot CoT with varying K (K varied and performance tracked), tested pure Analogical prompting, and tested combined CoT+Analogical prompts; used dataset steps to construct CoT examples and asked the model to self-generate exemplars for analogical prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>StemStep (custom high-school-level physics & mathematics dataset; ~928-1000 questions with stepwise solutions) and the Anand et al. dataset referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported human-evaluated accuracy (Table 1): Baseline 0-shot accuracy = 31.5%; K-Shot CoT accuracy = 53.0% (peak at K≈3); Analogical CoT accuracy = 32.0%. Additional metrics (BERTScore, METEOR, ROUGE) were computed but numeric breakdowns by method are not fully reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Few-shot CoT substantially improved results over baseline (peak at K=3); Mistral-7B often could not properly recall or self-generate relevant analogical exemplars so pure Analogical prompting produced poor/irrelevant answers; CoT prompts reduced hallucination when the model was given explicit step guidance; performance deteriorated when K grew too large (context window limits) and responses were quantized/limited by model size.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>For small open-source models like Mistral-7B, Few-shot Chain-of-Thought reliably improves performance relative to 0-shot, but pure Analogical prompting is not effective because the model cannot recall or generate useful exemplars; combining CoT with analogical guidance can help but Mistral's gains are modest due to parameter count, training data coverage, quantization, and context window limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Steps are all you need: Rethinking STEM Education with Prompt Engineering', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8243.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8243.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral 8x7B (Mixture-of-Experts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Mixture-of-Experts (MoE) style open-source model composed of eight 7B experts with a routing layer (reported as Mixtral 8x7B) evaluated for its ability to integrate different prompting/reasoning styles on STEM QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-Experts architecture (eight 7B experts with router); larger effective capacity and context handling than single 7B models; evaluated with LoRA/quantization in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['0-shot (baseline)', 'Few-shot Chain-of-Thought (K-shot CoT)', 'Analogical Prompting', 'Combined CoT + Analogical Prompting (Analogical CoT)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same prompting families as for Mistral-7B: baseline 0-shot; K-shot CoT where K varied and examples included stepwise solutions; Analogical prompting where the model was asked to recall/self-generate exemplars; Combined CoT+Analogical where CoT exemplars were provided and the model was asked to generate/recall further exemplars to solve the target.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (the model was evaluated under multiple distinct reasoning/prompting methods and showed differing abilities across them)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparisons between baseline, K-shot CoT (K varied, observed trends), pure Analogical prompting, and the combined CoT+Analogical setup; ablations effectively performed by turning on/off analogical generation and varying K to probe context-window and exemplar effects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>StemStep (custom high-school physics & mathematics dataset) and the Anand et al. dataset; evaluation used human accuracy and text-similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported human-evaluated accuracy (Table 1): Baseline 0-shot accuracy = 42.0%; K-Shot CoT accuracy = 64.5%; Analogical CoT accuracy = 66.2%. Paper also reports that K up to 6 improved performance (best consistency at K=6) and that K=8 led to sharp declines due to context limits. Other metrics (BERTScore, METEOR, ROUGE) were computed but not fully tabulated by condition.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Mixtral-8x7B outperformed Mistral-7B across prompting types; it was better able to leverage analogical prompts and the combined CoT+Analogical approach, suggesting MoE architecture and larger context window help integrate self-supplied exemplars; combined CoT+Analogical prompting most strongly improved performance on complex questions; pure Analogical prompting alone sometimes failed to help unless the model could recall relevant examples (which Mixtral did better than Mistral).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Mixture-of-Experts architecture (Mixtral-8x7B) better supports analogical and combined reasoning prompts than smaller single-expert models; combining Chain-of-Thought with Analogical prompting yields the best gains (especially on complex physics problems), while pure analogical prompting is unreliable on smaller open-source models unless the model has specialist training/finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Steps are all you need: Rethinking STEM Education with Prompt Engineering', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are analogical reasoners <em>(Rating: 2)</em></li>
                <li>Towards understanding mixture of experts in deep learning <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>The impact of reasoning step length on large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8243",
    "paper_id": "paper-274581897",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral 7B",
            "brief_description": "An open-source decoder-only transformer language model (~7 billion parameters) used in this paper as a baseline small model; quantized and LoRA-adapted for inference in experiments on STEM question answering.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_description": "Open-source ~7B parameter decoder-only transformer; in this work it was quantized and adapted with LoRA for inference and evaluated on physics & math QA.",
            "reasoning_methods": [
                "0-shot (baseline)",
                "Few-shot Chain-of-Thought (K-shot CoT)",
                "Analogical Prompting",
                "Combined CoT + Analogical Prompting (Analogical CoT)"
            ],
            "reasoning_methods_description": "0-shot: direct prompting without exemplars; Few-shot CoT: K example step-wise Chain-of-Thought prompts constructed using dataset steps to elicit intermediate reasoning; Analogical Prompting: asking the model to self-supply relevant exemplars and recall them to solve the target; Combined (Analogical CoT): providing CoT few-shot examples plus instructing the model to generate/recall analogical exemplars to guide solving.",
            "reasoning_diversity": "both (multiple methods tested; the model relied effectively on CoT but failed to leverage analogical prompting robustly)",
            "reasoning_diversity_experimental_setup": "Compared baseline 0-shot vs K-shot CoT with varying K (K varied and performance tracked), tested pure Analogical prompting, and tested combined CoT+Analogical prompts; used dataset steps to construct CoT examples and asked the model to self-generate exemplars for analogical prompting.",
            "task_or_benchmark": "StemStep (custom high-school-level physics & mathematics dataset; ~928-1000 questions with stepwise solutions) and the Anand et al. dataset referenced in the paper.",
            "performance_results": "Reported human-evaluated accuracy (Table 1): Baseline 0-shot accuracy = 31.5%; K-Shot CoT accuracy = 53.0% (peak at K≈3); Analogical CoT accuracy = 32.0%. Additional metrics (BERTScore, METEOR, ROUGE) were computed but numeric breakdowns by method are not fully reported in the paper.",
            "qualitative_findings": "Few-shot CoT substantially improved results over baseline (peak at K=3); Mistral-7B often could not properly recall or self-generate relevant analogical exemplars so pure Analogical prompting produced poor/irrelevant answers; CoT prompts reduced hallucination when the model was given explicit step guidance; performance deteriorated when K grew too large (context window limits) and responses were quantized/limited by model size.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "For small open-source models like Mistral-7B, Few-shot Chain-of-Thought reliably improves performance relative to 0-shot, but pure Analogical prompting is not effective because the model cannot recall or generate useful exemplars; combining CoT with analogical guidance can help but Mistral's gains are modest due to parameter count, training data coverage, quantization, and context window limits.",
            "uuid": "e8243.0",
            "source_info": {
                "paper_title": "Steps are all you need: Rethinking STEM Education with Prompt Engineering",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Mixtral-8x7B",
            "name_full": "Mixtral 8x7B (Mixture-of-Experts)",
            "brief_description": "A Mixture-of-Experts (MoE) style open-source model composed of eight 7B experts with a routing layer (reported as Mixtral 8x7B) evaluated for its ability to integrate different prompting/reasoning styles on STEM QA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7B",
            "model_description": "Mixture-of-Experts architecture (eight 7B experts with router); larger effective capacity and context handling than single 7B models; evaluated with LoRA/quantization in the paper's experiments.",
            "reasoning_methods": [
                "0-shot (baseline)",
                "Few-shot Chain-of-Thought (K-shot CoT)",
                "Analogical Prompting",
                "Combined CoT + Analogical Prompting (Analogical CoT)"
            ],
            "reasoning_methods_description": "Same prompting families as for Mistral-7B: baseline 0-shot; K-shot CoT where K varied and examples included stepwise solutions; Analogical prompting where the model was asked to recall/self-generate exemplars; Combined CoT+Analogical where CoT exemplars were provided and the model was asked to generate/recall further exemplars to solve the target.",
            "reasoning_diversity": "both (the model was evaluated under multiple distinct reasoning/prompting methods and showed differing abilities across them)",
            "reasoning_diversity_experimental_setup": "Direct comparisons between baseline, K-shot CoT (K varied, observed trends), pure Analogical prompting, and the combined CoT+Analogical setup; ablations effectively performed by turning on/off analogical generation and varying K to probe context-window and exemplar effects.",
            "task_or_benchmark": "StemStep (custom high-school physics & mathematics dataset) and the Anand et al. dataset; evaluation used human accuracy and text-similarity metrics.",
            "performance_results": "Reported human-evaluated accuracy (Table 1): Baseline 0-shot accuracy = 42.0%; K-Shot CoT accuracy = 64.5%; Analogical CoT accuracy = 66.2%. Paper also reports that K up to 6 improved performance (best consistency at K=6) and that K=8 led to sharp declines due to context limits. Other metrics (BERTScore, METEOR, ROUGE) were computed but not fully tabulated by condition.",
            "qualitative_findings": "Mixtral-8x7B outperformed Mistral-7B across prompting types; it was better able to leverage analogical prompts and the combined CoT+Analogical approach, suggesting MoE architecture and larger context window help integrate self-supplied exemplars; combined CoT+Analogical prompting most strongly improved performance on complex questions; pure Analogical prompting alone sometimes failed to help unless the model could recall relevant examples (which Mixtral did better than Mistral).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Mixture-of-Experts architecture (Mixtral-8x7B) better supports analogical and combined reasoning prompts than smaller single-expert models; combining Chain-of-Thought with Analogical prompting yields the best gains (especially on complex physics problems), while pure analogical prompting is unreliable on smaller open-source models unless the model has specialist training/finetuning.",
            "uuid": "e8243.1",
            "source_info": {
                "paper_title": "Steps are all you need: Rethinking STEM Education with Prompt Engineering",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are analogical reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_analogical_reasoners"
        },
        {
            "paper_title": "Towards understanding mixture of experts in deep learning",
            "rating": 2,
            "sanitized_title": "towards_understanding_mixture_of_experts_in_deep_learning"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "The impact of reasoning step length on large language models",
            "rating": 2,
            "sanitized_title": "the_impact_of_reasoning_step_length_on_large_language_models"
        }
    ],
    "cost": 0.00937475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Steps are all you need: Rethinking STEM Education with Prompt Engineering
11 Jun 2025</p>
<p>Krishnasai Addala krishnasai20442@iiitd.ac.in 
IIIT Delhi Delhi
India</p>
<p>Kabir Dev 
IIIT Delhi Delhi
India</p>
<p>Paul Baghel 
IIIT Delhi Delhi
India</p>
<p>Navya Gupta guptanavya1808@gmail.com 
MIDAS Lab Delhi
India</p>
<p>Rishitej Reddy Vyalla rishitej23439@iiitd.ac.in 
IIIT Delhi Delhi
India</p>
<p>Chhavi Kirtani 
IIIT Delhi Delhi
India</p>
<p>Avinash Anand avinasha@iiitd.ac.in 
IIIT Delhi Delhi
India</p>
<p>Rajiv Ratn rajivratn@iiitd.ac.in 
IIIT Delhi Delhi
India</p>
<p>Steps are all you need: Rethinking STEM Education with Prompt Engineering
11 Jun 2025B2C048C2887D960677F05546E262F974arXiv:2412.05023v3[cs.CL]
Few shot and Chain-of-Thought prompting have shown promise when applied to Physics Question Answering Tasks, but are limited by the lack of mathematical ability inherent to LLMs, and are prone to hallucination.By utilizing a Mixture of Experts (MoE) Model, along with analogical prompting, we are able to show improved model performance when compared to the baseline on standard LLMs.We also survey the limits of these prompting techniques and the effects they have on model performance.Additionally, we propose Analogical CoT prompting, a prompting technique designed to allow smaller, open source models to leverage Analogical prompting, something they have struggled with, possibly due to a lack of specialist training data.</p>
<p>Introduction</p>
<p>The emergence of Large Language Models has transformed various industries.Significant advancements have been made in enhancing LLMs' capabilities to transform the education sector (Anand et al., 2023b(Anand et al., ,a, 2024a,b),b).The ability of large language models to tailor learning experiences, meet individual student needs, and offer scalable educational solutions positions them as an ideal fit for academic purposes.This paper delves into the role of prompt engineering in enhancing high school STEM education.The educational landscape is increasingly seeking innovative and effective teaching approaches.Conventional teaching materials often fail to address the varied learning preferences and requirements of students.Different students learn in different ways and such personalized attention is not always available.This points to the necessity of a system which can coherently answer and explain questions to students.Recent advancements in natural language processing have significantly impacted various domains, including education.Large language models have shown potential in enhancing educational tools and resources, particularly in STEM education.Previous research has demonstrated the effectiveness of LLMs in physics question answering (Anand et al., 2023b) and citation generation (Anand et al., 2023d).Building on these findings, this paper explores the use of prompt engineering to further improve the performance of LLMs in high school STEM education.Physics and Mathematics, core subjects within the high school curriculum, pose unique challenges.Its abstractions can be hard for both students and LLMs to follow, necessitating customized explanations for students to understand intricate theories.This is where the capabilities of large language models shine, offering assistance in digesting concepts and addressing queries, thereby enriching the educational process.In this study, we evaluated our prompts by using Mistral 7B and Mixtral 8x7B.The models were tested using the dataset published by (Anand et al., 2023a), along with StemStep, allowing us to assess a model's ability to solve STEM problems.LoRA (Hu et al., 2021) was employed to mitigate the computational challenges associated with inference.This work is structured as follows: Section 2 addresses the related work.In Section 3, we discuss StemStep, our dataset contribution.In Section 4, we outline our experimentation, and Section 5 shows the evaluation results.Section 6 discusses the analysis and study of this research, and the paper's limitations, future scope and conclusion are summarized in Section 7, 8 and 9, respectively.</p>
<p>2 Related Work</p>
<p>Chain of Thought</p>
<p>Chain of Thought is a reasoning method that involves breaking down complex problems into simpler, more manageable parts, allowing an AI to "think" in a step-by-step manner similar to human problem-solving.Key studies in this area, such as by (Wei et al., 2023), have demonstrated how CoT can significantly improve the performance of AI models on complex tasks that require multi-step reasoning.Applications of CoT are widespread, including in natural language processing, where it enhances comprehension and problem-solving abilities of AI systems.Chain of Thought is a cognitive-inspired approach that mimics human reasoning by breaking down complex problems into a series of intermediate steps or thoughts.This method enables AI models, particularly in the field of Natural Language Processing, to tackle tasks that require multi-step reasoning, such as mathematics problems, multi-hop question answering, and commonsense reasoning tasks.</p>
<p>Key Studies and Findings</p>
<p>Wei et al. (Wei et al., 2023) demonstrated the effectiveness of CoT in improving language models' performance on complex reasoning tasks.The study found that prompting models to produce intermediate reasoning steps before reaching a final answer significantly improved problem-solving accuracy.Chain of Thought Prompting in LLMs: Recent studies have investigated the application of Chain of Thought (CoT) prompting in large-scale language models like GPT-3.These studies have shown that even when not explicitly trained for CoT, large models can exhibit remarkable reasoning abilities when prompted correctly.</p>
<p>Applications in AI</p>
<p>Educational Technologies: CoT has been applied in educational software to provide step-by-step explanations to students, aiding in learning and understanding complex concepts.Advanced Problem Solving: In domains requiring complex decision-making, such as programming and engineering, CoT helps in breaking down tasks into more manageable sub-tasks, leading to more efficient and effective solutions.Enhanced Comprehension in NLP: By incorporating CoT, LLMs achieve a deeper understanding of text, improving performance in summarization, question answering, and text generation tasks by providing contextually rich, reasoned responses.</p>
<p>Examples of CoT in Practice</p>
<p>Math Word Problems: AI models use CoT to dissect math word problems into smaller arithmetic operations, sequentially solving each part to arrive at the final answer.This approach has the potential to overcome the difficulty most LLMs have with mathematical reasoning.Scientific Research: LLMs have been shown to be helpful (Bryant, 2023) in assisting researchers in Physics related research, with the caveat that they sometimes hallucinate in a way that disrupts their ability to respond properly.CoT assists in parsing complex scientific texts, enabling models to outline step-by-step methodologies, results interpretation, and conclusions, thus aiding in literature review and hypothesis generation, all while reducing the hallucination rate (Gupta et al., 2023) in responses.</p>
<p>Few Shot Prompting</p>
<p>Few Shot Prompting (Brown et al., 2020) refers to the technique of training AI models with a very limited amount of data, typically just a few examples or "shots".This approach contrasts with Zero-Shot Learning, where the model receives no examples, and Many-Shot Learning, where the model is trained on large datasets.Recent advances in this area, particularly in the context of LLMs like GPT4(OpenAI, 2023), have shown remarkable abilities to generalize from minimal data, making it a valuable technique for tasks where data is scarce or costly to obtain.</p>
<p>Analogical Prompting</p>
<p>Analogical Prompting, as framed in (Yasunaga et al., 2023), represents an innovative paradigm wherein large language models are endowed with the capacity for analogical reasoning.This methodology asks the model to generate contextually relevant exemplars, thereby augmenting the models' efficacy in navigating a spectrum of reasoning tasks.Distinct from conventional methodologies that necessitate manual exemplar curation, this approach heralds a more dynamic and efficient mechanism, demonstrably surpassing traditional 0-shot and fewshot prompting modalities in its applicability and outcomes.However, analogical prompting has been mainly tested on proprietary models such as GPT4, we investigate its efficacy on non-finetuned open source models such as Mistral 7B (Jiang et al., 2023) and Mixtral 8x7B (Jiang et al., 2024).</p>
<p>Mixture of Experts</p>
<p>The Mixture of Experts (Chen et al., 2022) (MoE) model is a machine learning paradigm where multiple specialist models (experts) are trained on different parts of a problem and a gating mechanism decides which expert to use for a given input.This approach allows for more scalable and efficient learning, as each expert can become highly specialized.MoE has been successfully integrated with other AI techniques, such as deep learning and reinforcement learning, leading to significant improvements in tasks like language translation and image recognition.</p>
<p>Physics Question Answering using RAG</p>
<p>SciPhyRAG, developed by (Anand et al., 2023b), is a retrieval-augmented model designed to improve the performance of LLMs in answering physics questions.Traditional LLMs often struggle with the complex and nuanced nature of physics problems, which require not only an understanding of fundamental concepts but also the ability to apply these concepts to solve specific problems.Sci-PhyRAG addresses these challenges by integrating external knowledge sources, such as scientific literature and databases, to provide more contextually relevant and accurate answers.This approach significantly enhances the model's ability to comprehend and apply intricate physics concepts, thereby improving the overall reliability and accuracy of its responses.</p>
<p>The SciPhyRAG model operates by first retrieving relevant information from external sources based on the input question.This retrieved information is then used to augment the LLM's internal knowledge, allowing it to generate more informed and precise answers.By combining retrieval-augmented generation with advanced natural language processing techniques, SciPhyRAG sets a new standard for AI-driven physics education and research, demonstrating the potential for AI to assist in fields that require deep, domain-specific knowledge.</p>
<p>Advancements in Controllable Text Generation</p>
<p>Goel (Goel et al., 2023) explored significant advancements in the methods for controllable text generation, particularly focusing on the fine-tuning of LLMs for specific scientific domains.The study highlighted the importance of controlling various aspects of text generation, such as tone, style, and content relevance, to produce high-quality, domainspecific content.This capability is particularly crucial in educational applications, where the precision and clarity of generated content can directly impact learning outcomes.Goel's work involved developing techniques to enable LLMs to generate text that adheres to specific constraints and guidelines provided by the user.By fine-tuning models with data from specialized scientific domains, the research demonstrated that LLMs could produce more accurate and contextually appropriate content.This advancement not only improves the usability of LLMs in academic and professional settings but also enhances their potential as tools for personalized education and automated content creation in various fields.</p>
<p>Citation Generation</p>
<p>The work on context-enhanced language models by (Anand et al., 2023d) and the KG-CTG model (Anand et al., 2023c) demonstrates the effectiveness of using knowledge graphs to guide LLMs in generating precise citations.These approaches improve the accuracy and relevance of citations in scholarly papers, showcasing the potential of LLMs in academic writing.</p>
<p>Multimodal Physics Question Answering</p>
<p>MM-PhyQA (Anand et al., 2024b), a multimodal question-answering system that uses multiple images for chain-of-thought prompting.This approach enhances the accuracy and depth of answers in physics education by leveraging visual aids alongside textual explanations.</p>
<p>Mathematical Problem Solving</p>
<p>Mathify (Anand et al.) assessed the performance of LLMs on mathematical problem-solving tasks.The results reveal significant challenges that LLMs encounter with complex mathematical queries, emphasizing the necessity for specialized training data to enhance their problem-solving abilities.</p>
<p>2.10 Similar Datasets 2.10.1 SciQ (Welbl et al., 2017) SciQ comprises 13.7K science-based questions, but these are multiple-choice format.In contrast, the dataset we present mainly consists of questions that demand several intermediate steps to solve, making them better suited for a CoT paradigm.Furthermore, we include mathematics questions to ensure a more diverse and balanced dataset.</p>
<p>2.10.2GSM8K (Cobbe et al., 2021) GSM8K contains questions that are of middle school difficulty, which is below the standard required to evaluate prompts aimed at solving high school level problems.(Arora et al., 2023) JEEBench consists of approximately 450 questions in the domain of pre-engineering studies.These questions are generally more challenging than typical high school STEM problems and are primarily in multiple-choice format, which is often less suitable for prompting or training applications.</p>
<p>JEEBench</p>
<p>Dataset</p>
<p>We introduce StemStep as a dataset that can be used to evaluate models in answering both physics and mathematics questions at a high school level.The dataset contains 928 high-quality questions on all topics relevant to high school physics and mathematics.Each question in the dataset is accompanied by a set of steps required to reach the final answer.Below is a sample question from the dataset.</p>
<p>Question: "100g of water is supercooled to -10 degrees .At this point, due to some disturbance , mechanised or otherwise.Some of it suddenly freezes to ice.What will be the temperature of the resultant mixture and how much mass would freeze?</p>
<p>Steps:" "Mass of water =100g ", "At -10 degrees the mixture has water and ice", The data was gathered by scraping online resources aimed at high school students studying mathematics and physics.The data was scraped as LaTex text in order to maintain semantic integrity of the answers and their explanations.</p>
<p>Figure 1 shows the distribution of the number of steps for a given question in the dataset.</p>
<p>Figure 2 shows the step length per question distribution throughout the dataset.</p>
<p>It is clear to see that the dataset is biased towards questions with shorter explanations, which should be easier for models to comprehend.</p>
<p>Student survey</p>
<p>We verified the quality of the dataset by asking 5 individuals familiar with high-school level Physics and Mathematics to rate a sample of data points out of 10.These evaluators cumulatively gave the dataset an average score of 9 10 after reviewing 50 questions each, indicating a strong alignment between the data and the human student's expectation of what would be helpful in learning the given subjects.</p>
<p>Experimentation</p>
<p>We frame the objective of answering Physics questions as a text-generation task in order to leverage the strengths of smaller models which may struggle with heavier reasoning tasks.We use the models Mistral 7B and Mixtral 8x7B from mistralai, in order to investigate to what extent an MoE model affects the performance on these tasks.We establish a baseline for each model by asking the model to solve a given problem as a 0-Shot (Kojima et al., 2023) prompt.The presence of the baseline should serve to contextualize the expected performance gains from prompt engineering.</p>
<p>K Variable K Shot Prompting</p>
<p>We begin by investigating the effect of the number of examples in a few shot prompt on model performance, i.e, the effect of varying K in a K-shot prompt.We construct the K-Shot CoT prompts by following the pipeline proposed by (Anand et al., 2023a) , however, we utilize the steps from our dataset to guide the model into generating better structured answers.</p>
<p>We ran inference on base Mistral 7B, for its effectiveness in Text Generation and Question Answering tasks.We penalized the model for repetitive answers and quantized Mistral using LoRA to reduce hardware requirements.
E = kQ r 2 E = 8.99 × 10 9 × 2 × 10 −6 (0.5) 2 E = 8.99 × 10 9 × 2 × 10 −6 0.25 E = 17.98 × 10 3 0.25 E = 7.19 × 10 4 N/C
The results of this study can be seen in Figure 3, 4. The accuracy difference when using Mixtral 8x7B is shown in Table 1.We did not observe a large improvement in performance, although we did observe a moderate increase in accuracy.</p>
<p>Analogical Prompting</p>
<p>We adapt Analogical prompting from (Yasunaga et al., 2023) in order to investigate its utility in this domain.We asked the models to self-supply varying numbers of examples and measured the performance of the model in each scenario.In practice, we observe that the base models are unable to leverage analogical prompting, as they do not properly recall examples.This may be due to the relatively small datasets these models are trained on compared to models such as GPT3.5 or GPT4 which are able to recall relevant examples and solve the physics problems.Both Mistral 7B and Mixtral 8x7B give qualitatively poor answers, and more often than not do not address the given question.</p>
<p>Example 3: Analogical Prompting for</p>
<p>Kinematics-: Question: "A car accelerates from rest at a constant rate of 2 m/s 2 .Calculate the time it takes to reach a velocity of 20 m/s." examples, and ask it to generate 3 further examples on its own.These values were chosen on analysis of prior work (Yasunaga et al., 2023) done in this domain.</p>
<p>We tested the prompt on GPT4 and Gemini Pro (Team, 2023), in addition to the Mistral models to gauge prompt viability.</p>
<p>Example 4: Combined CoT and</p>
<p>Analogical Prompting for Energy-:</p>
<p>Question: "A 5 kg block slides down a frictionless inclined plane with an angle of 30 degrees.Calculate the speed of the block after sliding 2 meters."</p>
<p>3-Shot CoT + Analogical Prompt: 1.A 3 kg block slides down a frictionless inclined plane with an angle of 45 degrees.Calculate the speed of the block after sliding 1 meter.2. A 4 kg block slides down a frictionless inclined plane with an angle of 60 degrees.Calculate the speed of the block after sliding 1.5 meters.3. A 6 kg block slides down a frictionless inclined plane with an angle of 30 degrees.Calculate the speed of the block after sliding 2.5 meters.Recall relevant exemplars and solve the question: "A 5 kg block slides down a frictionless inclined plane with an angle of 30 degrees.Calculate the speed of the block after sliding 2 meters."Model Response: Given:
• Mass, m = 5 kg • Angle, θ = 30 • • Distance, d = 2 m • Acceleration due to gravity, g = 9.8 m/s 2
The component of gravitational force along the incline:
F = mg sin(θ) F = 5 × 9.8 × sin(30 • ) F = 5 × 9.8 × 0.5 F = 24.5 N
Using the work-energy principle:
W = F × d W = 24.5 × 2 W = 49 J Since W = ∆K: ∆K = 1 2 mv 2 49 = 1 2 × 5 × v 2 49 = 2.5v 2 v 2 = 49 2.5 v = √ 19.6 v = 4.43 m/s
We observe that GPT4 and Gemini Pro are able to give qualitatively better answers, and are able to answer more accurately than a simple CoT prompt.Mistral 7B struggled to generate relevant examples on its own, however it was able to answer the questions to a reasonable degree of accuracy, keeping in mind the lower parameter count and quantized nature.Of the open-source models, only Mixtral 8x7B was able to succesfully answer questions using this prompting style at a better than chance accuracy, Mistral 7B struggled to generate relevant text.The results of this study can be seen in Figure 5.</p>
<p>Results</p>
<p>Mistral 7B: We observe that the model performance peaks around K=3, while higher levels of K show that the performance slightly decreased and eventually plateaued.The metrics recorded signify that the model is able to generate text that is relatively semantically similar to the human-produced ground truth.The model also produced qualitatively good responses relative to the baseline and did not hallucinate when given some direction.However, this model was unable to leverage Analogical prompting at any real level, perhaps due to the small parameter count or due to the lack of specialized domain specific data in the training stage.</p>
<p>Mixtral 8x7B: The model did not display a discernible trend as Mistral 7B did, although K=6 seems to show the most consistent performance.This may be due to the larger context window being more accommodating of the longer prompts.The variability of the recorded metrics show that the model had trouble consistently generating text that was very similar to the ground truth, however, the addition of CoT+Analogical prompting did lead to a more aligned answer relative to the baseline.Model accuracy did increase as more examples were added, upto K=6.The addition of pure Analogical prompting did not show a positive effect on accuracy, as the model had trouble answering the original question.However: This model was unable to leverage Analogical prompting in that it was unable to recall relevant examples, this may be addressed in the future by adding specialized domain specific data in the training stage.</p>
<p>K-Shot CoT Performance</p>
<p>The sharp decline in performance at K=8 highlights the limitations of the models' context window.Mixtral 8x7B consistently outperforms Mistral 7B, likely due to its mixture of experts architecture, which enhances its ability to leverage context effectively.</p>
<p>Analogical Prompting Performance</p>
<p>Mixtral 8x7B's superior performance in analogical prompting suggests that its architecture better supports the integration of analogical examples into its reasoning process.This could be due to its higher parameter count and more sophisticated attention mechanisms.</p>
<p>Combined CoT and Analogical Prompting</p>
<p>The combined approach significantly improves performance, particularly in more complex questions.This suggests that guiding the model in generating its own examples helps it better understand and solve the problems.The effectiveness of this approach is most evident in the physics domain, where combining conceptual understanding with procedural steps is crucial.</p>
<p>Metrics</p>
<p>A detailed evaluation of model outputs was performed to determine the quality of their output.We measured widely recognized metrics in the field of text generation and summarization, including BERTScore (Zhang et al., 2019), ME-TEOR (Banerjee and Lavie, 2005), ROUGE-N, and ROUGE-L (Lin, 2004).The BERTScores evaluates how closely the model's generated text aligns with the reference text in terms of content and meaning.METEOR assesses the congruence between the generated texts and the original answers by integrating precision, recall, and a measure based on the alignment of similar content.In contrast, ROUGE-L focuses on pinpointing the longest sequence of words present in both the generated and the reference solutions, thus offering an indication of the text's smoothness and logical consistency.The ROUGE-N metric expands this evaluation to n-gram co-occurrence analysis, providing a layered dissection of the text's alignment with the ground truth.We evaluated Accuracy by asking the the evaluator to assess whether they felt the model had generated text inline with the ground truth.We avoided exact matching, as the models are likely to generate text in a different manner than the ground truth.The results for the experiments can be seen in Tables 1 and Figures 3,4,5,6</p>
<p>Discussion</p>
<p>We observe improvements over the baseline upon the application of Analogical CoT prompting in the Mixtral 8x7B model and improvement over the baseline for Mistral 7B on the application of Few Shot CoT.</p>
<p>Of note:</p>
<p>• The models became better at giving more structured outputs relative to the baseline • The increase in the recorded metrics points to a better ability to match the meaning and spirit of the ground truth, in this case, the reference answers.</p>
<p>• The models were likely held back by a lack of training data specifically in this domain.</p>
<p>It is likely that upon finetuning the models on a dataset such as SciQ, JeeBench, or the dataset contributed by (Anand et al., 2023a) , the models would perform better on the given tasks.</p>
<p>Additionally, we observed a trend in line with (Jin et al., 2024) with respect to the length of the reasoning chain, i.e, longer (or more complex) questions benefited more from longer steps and a higher number of steps.</p>
<p>We also observed that prompting the model with the steps explicitly delineated led to a better quality answer, along with a slight boost in accuracy relative to the baseline.</p>
<p>Limitations</p>
<p>Analogical Prompting is a relatively taxing technique, as it requires the model to be trained on a large amount of domain-specific data in order to be able to recall relevant examples.However, as noted in the original work, this method of prompting does hold merit for larger, better trained models such as GPT4.Theoretically, one could expect to see similar performance on open source models, should they be finetuned on specialist data.</p>
<p>Mixture of Expert Models are somewhat difficult to work with.While they do boast faster inference times due to their router layers, the same attribute makes it difficult to finetune these models without overfitting.4-Bit Quantizing (Dettmers et al., 2023) these models also leads to a degradation in performance, as the Mixtral model is essentially eight 7B models stacked together with a router layer, and smaller models tend to experience degradation from quantization.</p>
<p>The Data Comprehensiveness of StemStep may be lower than is optimal for finetuning LLMs, as it contains only 928 questions.By expanding this dataset, we may see further utility with respect to prompt construction and evaluation.</p>
<p>8 Future Scope</p>
<p>The Mixture of Experts approach could hypothetically be applied to enhance question-answering capabilities in STEM subjects by leveraging specialized models for different subdomains of physics for example, such as classical mechanics, quantum physics, and thermodynamics.Each expert, knowledgeable about its specific area, can provide precise and nuanced answers.When integrated, this collective expertise allows for a comprehensive and accurate response to diverse and complex physics questions, making MoE a promising framework for tackling interdisciplinary challenges in question answering.It is feasible that prompt engineering designed to exploit such a model could lead to a high accuracy, high quality Question Answering system in a given domain.</p>
<p>Supervised Finetuning of Mixtral 8x7B may allow the model to better leverage Analogical prompting due to the increased specialist knowledge in the given domains enabling the model to better recall relevant examples to bolster its own 'understanding' of the topic at hand.Further expansion of the StemStep dataset would improve its utility for prompt evaluation and model training, allowing models to learn the ideal structure for such domain-specific queries through methods such as RLHF (Ouyang et al., 2022).</p>
<p>Conclusion</p>
<p>A Appendix: Additional Experimentation Details Experimentation</p>
<p>To evaluate model performance on each prompting method, we prompt the model using questions from StemStep and compare the resulting output with the ground truth present in the dataset.For example, given a question on the topic of kinematics:</p>
<p>Instruction: "A car accelerates from rest at a constant rate of 2 m/s 2 .Calculate the time it takes to reach a velocity of 20 m/s."</p>
<p>Analogical Prompt:</p>
<p>Recall relevant exemplars and solve the question: "A car accelerates from rest at a constant rate of 2 m/s 2 .Calculate the time it takes to reach a velocity of 20 m/s."Such a prompt would be applied to evaluate Analogical prompting, the generated output of the model would then be compared to the ground truth, using the metrics described in 5.4, along with fuzzy matching, which was used to regenerate responses which had a similarity of less than 0.3, in order to ensure responses which greatly deviated from the relevant topic were discarded.To compare texts using cosine similarity, we first convert the texts into TF-IDF vectors.Let T A and T B be the TF-IDF vectors for texts A and B, respectively.The cosine similarity between these vectors is given by: cosine_similarity(T A , T B ) =
T A • T B ∥T A ∥∥T B ∥
where:</p>
<p>• T A •T B = n i=1 T A,i T B,i is the dot product of the TF-IDF vectors.</p>
<p>• ∥T A ∥ = n i=1 (T A,i ) 2 is the norm (magnitude) of vector T A .</p>
<p>• ∥T B ∥ = n i=1 (T B,i ) 2 is the norm (magnitude) of vector T B .</p>
<p>• n is the number of unique terms in the texts.</p>
<p>To regenerate responses that have a similarity of less than 0.3, we use the following condition:</p>
<p>If cosine_similarity(T A , T B ) &lt; 0.3 then regenerate response</p>
<p>Figure 1 :
1
Figure 1: The Average number of Steps per Question is 3.24</p>
<p>4.1.1Example 1: Projectile Motion-: Question: "A ball is thrown vertically upwards with an initial velocity of 20 m/s.Calculate the maximum height reached by the ball.Assume g = 9.8 m/s 2 ."3-Shot CoT Prompt: 1. "A stone is thrown vertically upwards with an initial velocity of 15 m/s.Calculate the maximum height reached by the stone."2. "An arrow is shot vertically upwards with an initial velocity of 25 m/s.Calculate the maximum height reached by the arrow."3. "A rocket is launched vertically upwards with an initial velocity of 30 m/s.Calculate the maximum height reached by the rocket."Model Response: Given: • Initial velocity, u = 20 m/s • Acceleration due to gravity, g = 9.8 m/s 2 Using the equation of motion: v 2 = u 2 − 2gh At maximum height, the final velocity v = 0electric field at a point 0.5 m away from a charge of 2 × 10 −6 C. Use Coulomb's constant k = 8.99 × 10 9 N m 2 /C 2 ."3-Shot CoT Prompt: 1. Calculate the electric field at a point 1 m away from a charge of 1 × 10 −6 C. 2. Calculate the electric field at a point 0.2 m away from a charge of 3 × 10 −6 C. 3. Calculate the electric field at a point 0.8 m away from a charge of 4 × 10 −6 C. Model Response: Given: • Charge, Q = 2 × 10 −6 C • Distance, r = 0.5 m • Coulomb's constant, k = 8.99 × 10 9 N m 2 /C 2 Using Coulomb's law:</p>
<p>Recall relevant exemplars and solve the question: "A car accelerates from rest at a constant rate of 2 m/s 2 .Calculate the time it takes to reach a velocity of 20 m/s." Model Response: Given: • Initial velocity, u = 0 • Acceleration, a = 2 m/s 2 • Final velocity, v = 20 m/s Using the equation of motion: In order to allow open-source models to leverage the strengths of both Few Shot CoT and Analogical prompting, we propose combining the two to guide the model in the kind of self-supplied examples it should generate.We supply the model with 3</p>
<p>Figure 3 :
3
Figure 3: K Variable K Shot Prompting on Mistral 7B</p>
<p>Figure 4 :
4
Figure 4: K Variable K Shot Prompting on Mixtral 8x7B</p>
<p>Table 1 :
1
Response Match Accuracy using Human Evaluation
ModelBaseline Accuracy (%)Mistral-7B31.5Mixtral-8x7B42ModelK-Shot CoT Accuracy (%)Mistral-7B53Mixtral-8x7B64.5ModelAnalogical CoT Accuracy (%)Mistral-7B32Mixtral-8x7B66.2</p>
<p>Through this work, we present both StemStep, as well as a potential avenue for leveraging analogical prompting on open source models for educational purposes.StemStep is a new dataset containing 1000 Physics and Mathematics questions, as well as the steps required to solve them.We believe this contribution will help others who wish to evaluate and finetune LLMs in the STEM Education domain.Analogical prompting was floated as an alternative to the more resource intensive CoT prompting methods, however, most open source models are unable to leverage it to any real degree of usefulness.The Analogical Chain of Thought Prompting method we have proposed could serve to bridge the gap between closed and open source models, although further evaluation on finetuned models is neccesary to draw stronger conclusions.We hope to move STEM education towards a new paradigm of Student Centric topic explanations, as opposed to the current paradigm centered on the ability and willingness of the teacher to impart knowledge, and in doing so, improve knowledge in this domain significantly.</p>
<p>Dataset PreprocessingThe StemStep dataset was preprocessed to ensure consistency and compatibility with the models used in this study.The following steps were performed:1.All LaTeX equations were converted to plain text using a custom script.2. Any special characters or symbols were replaced with their corresponding ASCII representations.3. Duplicate or near-duplicate questions were removed from the dataset.4. The dataset was split into training, validation, and test sets using an 60/20/20 ratio.Computing InfrastructureAll experiments were conducted on the following GPU hardware• GPU: 2 x NVIDIA A100 (40GB)
Revolutionizing high school physics education: A novel dataset. Avinash Anand, Krishnasai Addala, Kabir Baghel, Arnav Goel, Medha Hira, Rushali Gupta, Rajiv Ratn Shah, International Conference on Big Data Analytics. Springer2023a</p>
<p>Sciphyrag-retrieval augmentation to improve llms on physics q &amp;a. Avinash Anand, Arnav Goel, Medha Hira, Snehal Buldeo, Jatin Kumar, Astha Verma, Rushali Gupta, Rajiv Ratn Shah, International Conference on Big Data Analytics. Springer2023b</p>
<p>Kg-ctg: Citation generation through knowledge graph-guided large language models. Avinash Anand, Mohit Gupta, Kritarth Prasad, Ujjwal Goel, Naman Lal, Astha Verma, Rajiv Ratn Shah, International Conference on Big Data Analytics. Springer2023c</p>
<p>Avinash Anand, Mohit Gupta, Kritarth Prasad, Navya Singla, Sanjana Sanjeev, Jatin Kumar, Adarsh Raj Shivam, and Rajiv Ratn Shah. Mathify: Evaluating large language models on mathematical problem solving tasks. </p>
<p>Avinash Anand, Mohit Gupta, Kritarth Prasad, Navya Singla, Sanjana Sanjeev, Jatin Kumar, arXiv:2404.13099arXiv:2404.08704Avinash Anand, Janak Kapuriya, Apoorv Singh, Jay Saraf, Naman Lal, Astha Verma, Rushali Gupta, and Rajiv Shah. 2024b. Mm-phyqa: Multimodal physics question-answering with multi-image cot prompting. arXiv preprintAdarsh Raj Shivam, and Rajiv Ratn Shah. 2024a. Mathify: Evaluating large language models on mathematical problem solving tasks</p>
<p>Context-enhanced language models for generating multi-paper citations. Avinash Anand, Kritarth Prasad, Ujjwal Goel, Mohit Gupta, Naman Lal, Astha Verma, Rajiv Ratn Shah, International Conference on Big Data Analytics. Springer2023d</p>
<p>Have llms advanced enough? a challenging problem solving benchmark for large language models. Daman Arora, Himanshu Gaurav Singh, arXiv:2305.150742023arXiv preprint</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, arXiv:2005.14165Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford2020Preprint</p>
<p>Assessing gpt-4's role as a cocollaborator in scientific research: A case study analyzing einstein's special theory of relativity. Steven Bryant, 2023</p>
<p>Towards understanding mixture of experts in deep learning. Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, Yuanzhi Li, arXiv:2208.028132022Preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, arXiv:2305.14314Qlora: Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>Arnav Goel, Medha Hira, Avinash Anand, Siddhesh Bangar, Dr Rajiv, Ratn Shah, arXiv:2307.05538Advancements in scientific controllable text generation methods. 2023arXiv preprint</p>
<p>Santosh Mashetty, and Chitta Baral. 2023. Instruction tuned models are quick learners. Himanshu Gupta, Arjun Saurabh, Swaroop Sawant, Mutsumi Mishra, Arindam Nakamura, Mitra, ArXiv, abs/2306.05539</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, arXiv:2401.04088Mixtral of experts. Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2024Preprint</p>
<p>The impact of reasoning step length on large language models. Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, arXiv:2401.049252024Preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162023Preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAIPreprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, arXiv:2203.02155Jan Leike, and Ryan Lowe. 2022Preprint</p>
<p>Gemini: A family of highly capable multimodal models. Gemini Team, arXiv:2312.118052023Preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>
<p>Crowdsourcing multiple choice science questions. Johannes Welbl, Nelson F Liu, Matt Gardner, arXiv:1707.062092017arXiv preprint</p>
<p>Large language models as analogical reasoners. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H Chi, Denny Zhou, arXiv:2310.017142023Preprint</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
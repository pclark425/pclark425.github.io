<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7833 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7833</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7833</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-273346397</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.09084v1.pdf" target="_blank">Diagnosing Robotics Systems Issues with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Quickly resolving issues reported in industrial applications is crucial to minimize economic impact. However, the required data analysis makes diagnosing the underlying root causes a challenging and time-consuming task, even for experts. In contrast, large language models (LLMs) excel at analyzing large amounts of data. Indeed, prior work in AI-Ops demonstrates their effectiveness in analyzing IT systems. Here, we extend this work to the challenging and largely unexplored domain of robotics systems. To this end, we create SYSDIAGBENCH, a proprietary system diagnostics benchmark for robotics, containing over 2500 reported issues. We leverage SYSDIAGBENCH to investigate the performance of LLMs for root cause analysis, considering a range of model sizes and adaptation techniques. Our results show that QLoRA finetuning can be sufficient to let a 7B-parameter model outperform GPT-4 in terms of diagnostic accuracy while being significantly more cost-effective. We validate our LLM-as-a-judge results with a human expert study and find that our best model achieves similar approval ratings as our reference labels.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7833.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7833.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge vs Experts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-judge evaluation and human expert evaluation for robotics root-cause analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper-internal comparison between an LLM-based judge (asked to score similarity between predicted and reference root causes) and human domain experts; reports per-sample and aggregate agreement metrics, qualitative failure modes, and limitations of using LLMs as judges for system-diagnostics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Diagnosing Robotics Systems Issues with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Root cause analysis / system diagnostics (judge rates similarity between predicted and reference root causes)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SYSDIAGBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>LLM-as-a-judge (unspecified LLM instance; judging similarity 1–10 as per Appendix B.3)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Paper uses an LLM-as-a-judge protocol (prompt in Appendix B.3). The specific model used for the judge is not explicitly named in the evaluation description; reference-label extraction used GPT-4 with a chain-of-thought prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>10 internal domain experts from the robotics product team (n = 53 answer sets); experts rated helpfulness (yes/no/maybe), correctness on a 1–10 scale, and compared reference/predicted RCs.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>correlation (reported as ρ; paper does not specify Pearson vs Spearman; both per-sample and aggregate correlations reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.2</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>moderate per-sample agreement; lower reliability at sample-level; judge less reliable when reference labels are unreliable; cannot substitute expert human judgment for sample-level comparisons; disagreement and variability among experts themselves; imperfect reference labels (only ~49% considered correct by experts)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Overall rankings induced by mean-similarity-score (MSS) from the LLM-judge and mean expert ratings are consistent, but model ranking can change on the subset evaluated by experts (e.g., fully-finetuned MISTRAL-LITE-7B ranks higher under human review). Per-sample correlation between LLM-judge scores and individual expert ratings is moderate (ρ = 0.20 overall), improves on samples where reference labels are judged correct (reported ρ = 0.34 in Q2; paper also reports an aggregate MSS-to-mean-expert correlation of ρ = 0.57 when restricting to samples with correct reference labels). Inter-expert correlations reported as ρ = 0.32 (per-sample) and ρ = 0.77 (mean scores), indicating substantial human disagreement and emphasizing difficulty of sample-level evaluation. The LLM-judge is a good proxy for human judgment when reference labels are reliable and for comparing models at aggregate level, but is not a replacement for panel expert judgments for per-sample decisions. The study also notes cases where model-predicted RCs were highly rated by experts even when reference labels were judged incorrect, indicating models can sometimes uncover insights not reflected in hindsight reference labels.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Scalability and cost-effectiveness for large-scale model comparisons; provides a consistent quantitative metric (MSS) for model ranking; correlates well with mean human ratings on samples with reliable reference labels, enabling reliable aggregate comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>LLM-as-a-judge prompt (Appendix B.3) asked a model to rate similarity between predicted RC and reference RC on a 1–10 scale; primary metric is mean similarity score (MSS). Reference RCs were produced by GPT-4 using chain-of-thought on historic ticket discussions (not available at inference). Human study: 10 internal experts provided ratings on a subset (n=53 answer sets); experts rated reference labels for correctness (yes/maybe/no) and scored RCs (1–10). Reported comparisons include per-sample correlations (ρ) between judge scores and expert ratings, correlations restricted to samples with 'correct' reference labels, and inter-expert correlations for context.</td>
                        </tr>
                        <tr>
                            <td><strong>notes_on_failure_modes_and_limitations</strong></td>
                            <td>Paper highlights that (1) reference labels are imperfect (experts considered them correct or maybe-correct only 49% of the time), (2) LLM-as-a-judge shows only moderate per-sample agreement with experts (ρ ≈ 0.20) and thus is unreliable for single-sample decisions, (3) judge reliability improves on subsets with good reference labels (higher correlations reported), and (4) human experts themselves show limited agreement, complicating ground-truth assessment. Authors conclude LLM-judge is valuable for comparing models at scale but cannot substitute expert judgment for sample-level validation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_quantitative_values_summary</strong></td>
                            <td>Per-sample correlation between LLM-judge and individual expert ratings: ρ = 0.20 (overall); per-sample correlation on samples with correct reference labels: ρ = 0.34 (Q2) / aggregate MSS-to-mean-expert correlation on samples with correct labels: ρ = 0.57; inter-expert per-sample correlation: ρ = 0.32; inter-expert mean-score correlation: ρ = 0.77. Reference-label self-consistency (same strong model sampled multiple times) yields MSS = 7.5 as calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_context_snippet</strong></td>
                            <td>The paper reports that LLM-as-a-judge scores correlate with human expert ratings; per-sample correlation ρ = 0.20 (rising on reliable-label subsets) and MSS correlates highly with mean expert ratings when reference labels are judged correct, but the LLM-as-a-judge cannot substitute expert human judgment for sample-level decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diagnosing Robotics Systems Issues with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Swe-bench: Can language models resolve real-world github issues? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7833",
    "paper_id": "paper-273346397",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-judge vs Experts",
            "name_full": "Comparison of LLM-as-a-judge evaluation and human expert evaluation for robotics root-cause analysis",
            "brief_description": "Paper-internal comparison between an LLM-based judge (asked to score similarity between predicted and reference root causes) and human domain experts; reports per-sample and aggregate agreement metrics, qualitative failure modes, and limitations of using LLMs as judges for system-diagnostics tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Diagnosing Robotics Systems Issues with Large Language Models",
            "evaluation_task": "Root cause analysis / system diagnostics (judge rates similarity between predicted and reference root causes)",
            "dataset_name": "SYSDIAGBENCH",
            "judge_model_name": "LLM-as-a-judge (unspecified LLM instance; judging similarity 1–10 as per Appendix B.3)",
            "judge_model_details": "Paper uses an LLM-as-a-judge protocol (prompt in Appendix B.3). The specific model used for the judge is not explicitly named in the evaluation description; reference-label extraction used GPT-4 with a chain-of-thought prompt.",
            "human_evaluator_type": "10 internal domain experts from the robotics product team (n = 53 answer sets); experts rated helpfulness (yes/no/maybe), correctness on a 1–10 scale, and compared reference/predicted RCs.",
            "agreement_metric": "correlation (reported as ρ; paper does not specify Pearson vs Spearman; both per-sample and aggregate correlations reported)",
            "agreement_score": 0.2,
            "reported_loss_aspects": "moderate per-sample agreement; lower reliability at sample-level; judge less reliable when reference labels are unreliable; cannot substitute expert human judgment for sample-level comparisons; disagreement and variability among experts themselves; imperfect reference labels (only ~49% considered correct by experts)",
            "qualitative_findings": "Overall rankings induced by mean-similarity-score (MSS) from the LLM-judge and mean expert ratings are consistent, but model ranking can change on the subset evaluated by experts (e.g., fully-finetuned MISTRAL-LITE-7B ranks higher under human review). Per-sample correlation between LLM-judge scores and individual expert ratings is moderate (ρ = 0.20 overall), improves on samples where reference labels are judged correct (reported ρ = 0.34 in Q2; paper also reports an aggregate MSS-to-mean-expert correlation of ρ = 0.57 when restricting to samples with correct reference labels). Inter-expert correlations reported as ρ = 0.32 (per-sample) and ρ = 0.77 (mean scores), indicating substantial human disagreement and emphasizing difficulty of sample-level evaluation. The LLM-judge is a good proxy for human judgment when reference labels are reliable and for comparing models at aggregate level, but is not a replacement for panel expert judgments for per-sample decisions. The study also notes cases where model-predicted RCs were highly rated by experts even when reference labels were judged incorrect, indicating models can sometimes uncover insights not reflected in hindsight reference labels.",
            "advantages_of_llm_judge": "Scalability and cost-effectiveness for large-scale model comparisons; provides a consistent quantitative metric (MSS) for model ranking; correlates well with mean human ratings on samples with reliable reference labels, enabling reliable aggregate comparisons.",
            "experimental_setting": "LLM-as-a-judge prompt (Appendix B.3) asked a model to rate similarity between predicted RC and reference RC on a 1–10 scale; primary metric is mean similarity score (MSS). Reference RCs were produced by GPT-4 using chain-of-thought on historic ticket discussions (not available at inference). Human study: 10 internal experts provided ratings on a subset (n=53 answer sets); experts rated reference labels for correctness (yes/maybe/no) and scored RCs (1–10). Reported comparisons include per-sample correlations (ρ) between judge scores and expert ratings, correlations restricted to samples with 'correct' reference labels, and inter-expert correlations for context.",
            "notes_on_failure_modes_and_limitations": "Paper highlights that (1) reference labels are imperfect (experts considered them correct or maybe-correct only 49% of the time), (2) LLM-as-a-judge shows only moderate per-sample agreement with experts (ρ ≈ 0.20) and thus is unreliable for single-sample decisions, (3) judge reliability improves on subsets with good reference labels (higher correlations reported), and (4) human experts themselves show limited agreement, complicating ground-truth assessment. Authors conclude LLM-judge is valuable for comparing models at scale but cannot substitute expert judgment for sample-level validation.",
            "reported_quantitative_values_summary": "Per-sample correlation between LLM-judge and individual expert ratings: ρ = 0.20 (overall); per-sample correlation on samples with correct reference labels: ρ = 0.34 (Q2) / aggregate MSS-to-mean-expert correlation on samples with correct labels: ρ = 0.57; inter-expert per-sample correlation: ρ = 0.32; inter-expert mean-score correlation: ρ = 0.77. Reference-label self-consistency (same strong model sampled multiple times) yields MSS = 7.5 as calibration.",
            "citation_context_snippet": "The paper reports that LLM-as-a-judge scores correlate with human expert ratings; per-sample correlation ρ = 0.20 (rising on reliable-label subsets) and MSS correlates highly with mean expert ratings when reference labels are judged correct, but the LLM-as-a-judge cannot substitute expert human judgment for sample-level decisions.",
            "uuid": "e7833.0",
            "source_info": {
                "paper_title": "Diagnosing Robotics Systems Issues with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Swe-bench: Can language models resolve real-world github issues?",
            "rating": 1,
            "sanitized_title": "swebench_can_language_models_resolve_realworld_github_issues"
        }
    ],
    "cost": 0.010201749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Diagnosing Robotics Systems Issues with Large Language Models
6 Oct 2024</p>
<p>Jordis Emilia 
Linköping University
3 LogicStar.ai 4 ETH Zurich</p>
<p>Herrmann †1 
Linköping University
3 LogicStar.ai 4 ETH Zurich</p>
<p>Aswath Mandakath Gopinath 
Linköping University
3 LogicStar.ai 4 ETH Zurich</p>
<p>Mikael Norrlof 
Linköping University
3 LogicStar.ai 4 ETH Zurich</p>
<p>Mark Niklas Müller 
Linköping University
3 LogicStar.ai 4 ETH Zurich</p>
<p>Abb Robotics 
Linköping University
3 LogicStar.ai 4 ETH Zurich</p>
<p>Diagnosing Robotics Systems Issues with Large Language Models
6 Oct 202485B97F8FB392E98C9995D93CCA8DBBECarXiv:2410.09084v1[cs.CL]
Quickly resolving issues reported in industrial applications is crucial to minimize economic impact.However, the required data analysis makes diagnosing the underlying root causes a challenging and time-consuming task, even for experts.In contrast, large language models (LLMs) excel at analyzing large amounts of data.Indeed, prior work in AI-Ops demonstrates their effectiveness in analyzing IT systems.Here, we extend this work to the challenging and largely unexplored domain of robotics systems.To this end, we create SYS-DIAGBENCH, a proprietary system diagnostics benchmark for robotics, containing over 2 500 reported issues.We leverage SYSDIAGBENCH to investigate the performance of LLMs for root cause analysis, considering a range of model sizes and adaptation techniques.Our results show that QLORA finetuning can be sufficient to let a 7B-parameter model outperform GPT-4 in terms of diagnostic accuracy while being significantly more cost-effective.We validate our LLM-as-a-judge results with a human expert study and find that our best model achieves similar approval ratings as our reference labels.</p>
<p>Introduction</p>
<p>Identifying the root cause for issues with complex industrial systems is a time-critical task but challenging and time-consuming for human experts as they struggle with the required analysis of large amounts of log data which large language models (LLMs) excel at.Indeed, there is substantial work in AI-Ops exploring automated diagnostics for IT systems (Díaz-de-Arcaya et al., 2024;Zhaoxue et al., 2021).However, the challenging domain of robotics systems remains largely unexplored.</p>
<p>This Work: Automated Diagnostics for Robotics Systems To address this challenge, we create SYSDIAGBENCH, a proprietary benchmark for diagnosing root causes of complex robotics systems † Correspondence author: jordis.herrmann@se.abb.comfailures, containing over 2 500 real-world issues.In particular, each instance corresponds to a support ticket, containing an issue description, a set of log files, communications with the support engineers, a reference root cause extracted from expert discussions, and the ultimate issue resolution.The goal in SYSDIAGBENCH is to predict the root cause underlying the reported issue, given only the information available at the creation of the ticket.</p>
<p>LLM-Based Diagnostics</p>
<p>We leverage SYSDI-AGBENCH to investigate multiple LLM-based diagnostic approaches in the robotics setting, using both LLM-as-a-judge (Zheng et al., 2023) and human experts for evaluation.In particular, we consider a range of model sizes and adaptation techniques from zero-shot prompting to full finetuning, to assess their cost-performance trade-off.Interestingly, we observe that even QLORA (Dettmers et al., 2023) can be sufficient to let a 7B-parameter model outperform GPT-4 in terms of diagnostic accuracy while being significantly more cost-effective.Validating our results in an expert study, we find that LLM-as-a-judge scores correlate well with human expert ratings, with our reference labels matching the experts' analysis in over half the cases and our best model achieving similar approval ratings as these reference labels.</p>
<p>Key Contributions</p>
<p>Our key contributions are:</p>
<p>• We create SYSDIAGBENCH, a proprietary benchmark for automated root cause analysis of robotics systems, based on thousands of real-world issues (Section 3).</p>
<p>• We propose a range of LLM-based diagnostic tools (Section 4).</p>
<p>• We leverage SYSDIAGBENCH to analyze these techniques and identify the most effective and efficient strategies (Section 5).</p>
<p>• We validate the effectiveness of our approach using a human expert study (Section 6).</p>
<p>AI-Ops leverages machine learning (ML) in IT operations (Díaz-de-Arcaya et al., 2024) to analyze large amounts of semi-structured data such as logs and traces (Zhaoxue et al., 2021) with the goal of discovering anomalies and their root causes.As many traditional ML methods require structured data, AI-Ops long focused on developing methods enhancing (Yuan et al., 2012;Zhao et al., 2017) and parsing (He et al., 2017;Messaoudi et al., 2018) log files, using well-established methods such as SVMs (Zhang and Sivasubramaniam, 2008;Zuo et al., 2020), simple clustering techniques (Zhao et al., 2019;Lou et al., 2010), and decision tree (ensembles) (Chen et al., 2004) for the actual analysis.</p>
<p>LLM-based Approaches</p>
<p>As LLMs can directly process the semi-structured log data, they have recently gained popularity in the field (Shao et al., 2022;Chen and Liao, 2022;Lee et al., 2023;Ott et al., 2021).As a representative example, Gupta et al. (2023) use an encoder architecture, pre-trained on a large amount of log data, to compute embeddings for further analysis.In contrast to these methods, we propose to directly predict root causes from log data.</p>
<p>SYSDIAGBENCH: A Benchmark for Robotics System Diagnostics</p>
<p>SYSDIAGBENCH is a proprietary system diagnostics benchmark focusing on root causes (RC) prediction for real-world robotics issues, constructed from a decade of industry data.Concretely, each SYSDIAGBENCH instance corresponds to a support ticket and contains a detailed problem description, a set of log files from the affected system, and a reference root cause description.Below, we first describe the information contained in a ticket and then the process of constructing SYSDIAGBENCH.</p>
<p>Unfortunately, the underlying data cannot be published at this point due to privacy concerns.</p>
<p>Support Tickets A ticket is created when a reported issue cannot be resolved by the service support engineers and as a result needs to be escalated to the product development team.Every ticket contains metadata on the affected system (e.g. the robot and application type), a detailed problem description, and a system diagnostic file capturing the system state after the issue occurred.For SYSDIAGBENCH, we consider three log files contained in the system diagnostic that are commonly analyzed by experts when investigating a ticket.The elog logs all error, warning, and information events that occur during the operation of the system.The print-spool logs all outputs that are written to the console during the operation of the system.The startup logs all events that occur during the startup of the system.All three log types contain a significant amount of data at average lengths of 37k, 23k, and 8.1k tokens, respectively.Historic Tickets which were already resolved successfully additionally contain the discussion among the experts working on the issue, communication with the support engineers, and final resolution of the issue.However, as even these historic tickets generally do not contain a description of the root cause (RC), we need to extract it from the available information to obtain a reference label.</p>
<p>Benchmark Construction</p>
<p>To create SYSDIAGBENCH, we collected over 12 000 historic tickets and filtered out those that do not contain a system diagnostic with an elog, pspool, and startup file, leaving us with 2 585, split into a training, validation, and test set corresponding to 75%, 5%, and 20%, respectively.</p>
<p>Root Cause Extraction To extract a concise root cause description from a historic ticket, we leverage a strong LLM (GPT-4) (illustrated in Figure 1).Concretely, we query the LLM with the problem description, expert discussion, support engineer communication, and final resolution using a chainof-thought (CoT) prompt (Wei et al., 2022), instructing the model to carefully analyze all provided information before describing the root cause (see Appendix B.1 for more details).We highlight that the information used to create these labels is not available when a ticket is created and can thus not be used to predict the RC at inference time.Finally, we validate the quality of the extracted root causes in a human study in Section 6.</p>
<p>Evaluation Metrics</p>
<p>Evaluating root cause correctness is inherently challenging, as descriptions of the same, correct root cause can be highly diverse, making similarity measures such as the ROUGE score (Lin, 2004) unsuitable.Further, there is frequently a trade-off between specificity and correctness, i.e., generic descriptions can be correct yet unhelpful, while very precise ones may get minor details wrong while still being overall very helpful.We thus adopt an LLM-as-a-judge evaluation (Zheng et al., 2023) asking a model to judge the similarity between the predicted and the reference RC on a scale of 1 to 10 (see Appendix B.3 for details).We report the mean similarity score (M SS) with respect to the reference labels as our primary evaluation metric and validate it against human experts in Section 6.</p>
<p>LLM-Based Systems Diagnostic</p>
<p>In this section, we describe the system diagnostic approaches we evaluated on SYSDIAGBENCH.</p>
<p>Input Preprocessing and Prompting</p>
<p>For both training and inference, we preprocess all log files by removing timestamps, dates, and sequence numbers.We further remove all consecutive duplicate lines and filter the elog to only include error and warning but not information events, as these are most likely to be relevant for diagnosing the root cause.As both the print-spool and startup frequently contain tens of thousands of lines, we only retain the 10 lines before and after each error or warning event in the elog file.Finally, while the original elog file contains only integer error IDs, we map these to human-readable error descriptions using a lookup table.This preprocessing reduces the mean total token count of the log files per ticket from 68k to 16k tokens, with the distribution change illustrated in Figure 2.</p>
<p>The LLM input is now constructed by combining a detailed CoT instruction (Wei et al., 2022) with a context of the preprocessed log files and the issue description (see Appendix B.2 for more details).Despite our preprocessing, the resulting inputs frequently exceed 8k (68%) and even 32k (9%) tokens (see Figure 2), requiring models with large context capabilities for processing.</p>
<p>Training for System Diagnostics</p>
<p>While modern LLMs have impressive zero-shot capabilities (Kojima et al., 2022), adapting them to specific tasks (Zhao et al., 2024) can improve their performance significantly.However, the long input lengths make in-context learning, e.g., via fewshot prompting, unpractical for system diagnostics.We, thus, consider three adaptation techniques, full finetuning (FFT), LORA (Hu et al., 2022), and QLORA (Dettmers et al., 2023), with different performance-cost trade-offs (see Appendix A).</p>
<p>Experimental Evaluation</p>
<p>Below, we discuss the experimental setup and key results.For more details on setup and extended results, please see Appendices C and D, respectively.Experimental Setup We consider MISTRAL-LITE-7B (Yin Song and Chen Wu and Eden Duthie, 2023), MIXTRAL-8X7B (Jiang et al., 2024), and GPT-4 (OpenAI, 2023), using Axolotl (2024) for finetuning on 2 to 8 NVIDIA A100s.We train for 3 epochs and unless indicated otherwise, use rank r = 32 for (Q)LORA and NFloat4 + DQ (double quantization) for QLORA.performance for LORA, there is only a minimal effect for QLORA.As both a reduced rank and quantization act as regularizers, we conclude that QLORA is already at optimal regularization, while LORA still benefits from further regularization.</p>
<p>Human Study</p>
<p>We conduct a study with human experts to answer the following three questions: Q1: Are the automatically extracted reference labels accurate?Q2: Does our LLM-as-a-judge evaluation correlate well with human expert ratings?And Q3: Are our best models helpful in root cause analysis?</p>
<p>Study Setup We ask 10 experts to solve a subset of tickets, leading to n = 53 answer sets.We provide the experts with all the tools they typically use to resolve issues and the full historic ticket data.</p>
<p>We then ask them to describe the issue's RC and judge their confidence.Next, we let them assess our reference label, in terms of helpfulness (yes, no, maybe), correctness (on a scale from 1 to 10), and preference compared to their own answer.Where available, we let them assess their colleague's RC, in the same way.Finally, we let them rate the RCs generated by our four best models on the same scale.See Appendix E for more details.</p>
<p>Q1: Reference RC Quality Asked directly, whether our reference RC was correct, experts agreed (yes or maybe) in 49% of cases, saying it was as good as their own assessment in 55% of cases, but only as good as their colleague's in 29%.</p>
<p>Interestingly, they still assigned a higher or equal score to our reference RC (6.0 on average) than to their colleagues' RC (7.9 average) in 43% of cases.</p>
<p>Combined with experts only being highly (moderately) confident in their assessment 32% (58%) of the time, this suggests that diagnosing root causes is a particularly hard task, with our reference labels having high but not perfect quality.</p>
<p>Q2: LLM-as-a-Judge Evaluation While overall M SS (LLM-as-a-judge) and mean expert ratings induce the same ranking, the M SS evaluated on the samples considered by the experts ranks the fully finetuned MISTRAL-LITE-7B first instead of third.Comparing per-sample ratings, we find a correlation of ρ = 0.20, increased to 0.34 on samples where our reference labels are considered correct by the experts.This matches the per-sample correlation of human expert scores of ρ = 0.32.These results suggest that our LLM-as-a-judge evaluation is a good proxy for a human expert judgment where we have reliable reference labels and allows for reliably comparing models.</p>
<p>Q3: Model Helpfulness We compare the expert ratings of our reference and predicted RCs in Figure 4 and observe that the RCs predicted by our best two models match the reference labels extracted by GPT-4 with hindsight knowledge in half the cases.</p>
<p>We further find that even when the reference labels are considered incorrect, there are instances where the predicted RCs are rated highly (see Figure 9 in Appendix D).For these instances, we found by manual inspection, that our models were able to leverage a deeper understanding of the log files to identify the underlying RC.These results suggest that our models can provide valuable insights and help experts in diagnosing the root causes of complex robotics systems issues.</p>
<p>Conclusion</p>
<p>We created SYSDIAGBENCH, a benchmark for robotics systems diagnostics, containing over 2 500 real-world issues.We leveraged SYSDIAGBENCH to investigate the performance of large language models (LLMs) for automated root cause analysis, considering a range of model sizes and adaptation techniques.Our results show that QLORA finetuning allows a 7B-parameter model to outperform GPT-4 in terms of diagnostic accuracy while being significantly more cost-effective.We validated our results with an expert study and found that while both our reference label extraction and LLM-as-ajudge evaluation cannot replace human experts, our best models can provide valuable insights.</p>
<p>Limitations</p>
<p>As no ground truth root cause annotations exist for (historic) tickets, we generated reference labels for SYSDIAGBENCH using a strong LLM to extract them from the rich data available for historic tickets.</p>
<p>While the human expert study shows the generated labels to be as good as an expert's analysis in over half the cases, they are not perfect.In particular, they are only considered correct (yes or maybe) 49% of the time by the experts.Using multiple experts to annotate the same tickets could have improved the quality of the reference labels and thus both the performance of our finetuned models and the evaluation quality, but this was not feasible due to the significant effort required to annotate tickets and time constraints of available experts.Further, even for correct reference labels, the LLM-as-a-judge evaluation is not perfect.While we achieve a high correlation of ρ = 0.57 between the M SS and mean expert ratings when only considering samples with correct (yes or maybe) reference labels, the per-sample correlation remains at a moderate ρ = 0.20.However, even the interexpert correlation of ρ = 0.77 for mean scores and ρ = 0.32 for per-sample correlation, remains far from perfect, highlighting again the difficulty of accurately assessing root causes.We conclude that while the LLM-as-a-judge evaluation is a valuable tool for comparing different models, it cannot substitute expert human judgment, especially for sample-level comparison.We note that for reliable sample-level comparison, a panel of experts would be needed.</p>
<p>Finally, we only consider two base models and a limited number of hyperparameters for our experiments due to both budget and time constraints.While we find for MISTRAL-LITE-7B that LORA and QLORA finetuning perform exceptionally well, even outperforming full finetuning, this was not the case for MIXTRAL-8X7B.While we hypothesize that this is due to MISTRAL-LITE-7B's training specifically for long context retrieval tasks, a detailed study of this effect is out of scope here and left for future work.</p>
<p>Ethical Considerations and Broader Impact</p>
<p>Ethical Considerations The dataset underlying SYSDIAGBENCH contains real-world support tickets from a robotics company, which may contain sensitive information about the company's products and customers, thus precluding its public release.</p>
<p>To mitigate the risk of privacy breaches during internal use, we have anonymized all tickets by removing all personally identifying information fields.</p>
<p>For our human expert study, we have recruited internal experts from the robotics company's product team, who regularly handle the support tickets constituting SYSDIAGBENCH.We have obtained informed consent from all participants and have anonymized the expert's analysis before sharing it with other experts for the inter-expert assessment.All experts were paid their regular wages during their participation in the study.</p>
<p>Broader Impact While we demonstrate the effectiveness of LLM-based systems for the automated diagnostics of robotics systems, we also highlight their limitations.In particular, we show that while LLMs can achieve moderately high performance and even match experts in some cases, they are unable to fully replace the expert's analysis.We thus expect that LLM-based tools will soon become useful aids for human experts, but not fully replace them in the foreseeable future, similar to many other domains.</p>
<p>A Backgorund</p>
<p>Prompting Once the remarkable zero-shot capabilities of LLMs had been demonstrated (Kojima et al., 2022), a wide range of prompting schemes was proposed that aim to elicit higher quality answers from the same model by evoking a (more thorough) reasoning process (Wang et al., 2023;Yao et al., 2023;Xu et al., 2023;Zhou et al., 2023).In particular, Chain-of-Thought (CoT) prompting (Wei et al., 2022) instructs the model to "think step-by-step" when answering, which has been shown to improve performance on a wide range of tasks, with multiple follow-up works trading-off increased inference cost and better performance (Wang et al., 2023;Yao et al., 2023).</p>
<p>(Full) Finetuning If zero-shot performance is unsatisfactory and labeled training data is available, one can continue training the model on the specific task at hand, a process known as finetuning.In particular, full finetuning refers to training the entire model on the new task.</p>
<p>LORA However, the huge size of modern LLMs makes GPU memory a bottleneck for training, with common optimizers like Adam (Kingma and Ba, 2015) and AdamW (Loshchilov and Hutter, 2019) requiring three full precision values (the gradient, and its first and second moment) to be tracked for every parameter.To alleviate this issue, LORA (Hu et al., 2022) proposes that instead of updating all parameters in a weight matrix W ∈ R n×n one only computes a low-rank update AB where A ∈ R n×k and B ∈ R k×n with k ≪ n.We thus obtain the updated weight matrix as W ′ = W + AB and reduce the memory footprint of the optimizer from O(n 2 ) to O(nk).Finally, recent work has shown that LORA can also be seen as a form of regularization that reduces forgetting and can thereby actually improve performance (Jimenez et al., 2023).</p>
<p>Model Quantization and QLORA To reduce a model's memory footprint not only during training but also during inference, model quantization techniques have been proposed that reduce the precision of the model's weights and sometimes activations.In particular, representing the model's weight matrices using 4-, 3-, or even 2-bit precision rather than the standard (for LLMs) 16-bit half-precision representation can lead to significant memory savings (Park et al., 2018;Frantar et al., 2022;Lin et al., 2023).However, quantization can also lead to a significant drop in performance, especially if applied after training.To mitigate this issue, QLORA (Dettmers et al., 2023) proposes to quantize the weight matrices already during training, allowing the half-precision LORA adapters to learn to correct for the quantization errors, while still significantly reducing the memory footprint compared to standard LORA.</p>
<p>B Detailed Prompt descriptions B.1 Root Cause Extraction</p>
<p>As described in Section 3.1, we extract the root cause (RC) from the historic tickets using a strong LLM (GPT-4) by concatenating the problem description, expert discussion, customer communication, and the final resolution with a chain-of-thought (CoT) prompt (Wei et al., 2022) instructing the model to carefully analyze all provided information before generating a root cause description.We show the full prompt used to this end in Figure 5.</p>
<p>B.2 Root Cuase Prediction</p>
<p>As described in Section 4, we use a range of LLMs to predict the root causes of the tickets in SYSDIAG-BENCH.To this end, we use zero-shot prompting with the problem description and our preprocessed logs (see Section 4).We show the full prompt used for this task in Figure 6.</p>
<p>B.3 LLM-as-a-Judge: Similarity Score Prediction</p>
<p>To assess the quality of generated root cause descriptions, we use a LLM-as-a-judge evaluation procedure (Zheng et al., 2023) where we ask a model to judge the similarity between the predicted and the reference root cause descriptions on a scale of 1 to 10.We show the full prompt used for this task in Figure 7.</p>
<p>An issue with an industrial grade robot is reported in the context below.What is the root cause of the reported issue?</p>
<p>Let's think step by step to answer this question.First, analyze each section in the context and systematically identify root causes and their relative probability.Remember that a section can have multiple root causes or no root causes at all.Finally, pick the root cause with the highest relative probability and respond with the root cause in JSON format with key as "Root Cause".If the root cause is unknown, respond "Unknown root cause" in JSON format.</p>
<p>C Experimental Setup</p>
<p>Model Selection We select models based on three criteria: i) sufficient (≥ 32k tokens) context length, ii) good general reasoning capabilities, and iii) a permissive license.Based on these criteria, we choose MIXTRAL-8X7B * (Mixtral-8x7B-Instruct-v0.1 under Apache-2.0License Jiang et al. 2024) and the smaller MISTRAL-LITE-7B † (MistralLite under Apache-2.0License Yin Song and Chen Wu and Eden Duthie 2023), which was specifically finetuned for long context tasks.As a reference frontier model, we consider GPT-4 (gpt-4-32k-0613 OpenAI 2023).</p>
<p>Experimental Setup We use Axolotl (Axolotl, 2024) with DeepSpeed (Rajbhandari et al., 2020;Rasley et al., 2020) for finetuning with AdamW (β 1 = 0.9 and β 2 = 0.95) (Loshchilov and Hutter, 2019) on 2 to 8 NVIDIA A100s.We train for 3 epochs at an effective batch size of 64 for full finetuning and 16 for LORA and QLORA using an initial learning rate of 10 −5 and a cosine decay with a warm-up ratio of 10%.Unless indicated otherwise, we use rank r = 32 for LORA and QLORA and NFloat4 + DQ (double Please act as an impartial judge and evaluate the similarity of the two analyses provided below by two different AI assistants.Both were given the same data related to an issue in a robotics system and asked to identify the root cause.Your job is to evaluate and quantify the similarity between the two answers.Begin your evaluation by comparing the two answers and identifying key differences.Do not allow the length of the responses to influence your evaluation.Do not favor certain names of the assistant.Be as objective as possible.After providing your explanation, please rate the similarity on a scale of 1 to 10 by strictly following this format: "[[rating]]", for example: "Rating: [[5]]".quantization) for QLORA.</p>
<p>Computational Requirements</p>
<p>We provide an overview of the runtimes required for our different training setups in Table 2.</p>
<p>D Extended Experimental Evaluation</p>
<p>Similarity Score Callibration We repeat the label extraction process described in Section 3.1 twice more for each ticket, sampling at a temperature of t = 0.5, and compute similarity scores to the reference label, obtained with greedy decoding.We thus obtain an M SS = 7.5, for root causes extracted by the same strong model with access to the same information, yielding a reference for an excellent M SS.Feature Importance Analysis To assess feature importance, we train MISTRAL-LITE-7B on different feature subsets using LORA and report results in Table 3.We observe that the model's performance drops significantly when excluding the elog and Problem Description, indicating their importance.In contrast, removing the startup and printspool improves the model's performance, suggesting that while these features may be helpful to debug compilation issues, they are less important for reported issues and can even distract the model (Jimenez et al., 2023).</p>
<p>Distribution of Similarity Scores Illustrating performance distributions in Figure 8, we observe that distributions are similar across all models with better models having significantly fewer very low (≤ 2) scores and a uniform increase in the frequency of all higher scores (≥ 3).</p>
<p>Model Helpfulness Beyond the analysis in Section 5, we visualize the expert rating of their highestrated model depending on whether the experts consider our reference label to be correct in Figure 9.We find that while average scores are higher when the reference label is considered correct, there is a significant number of examples, where the predicted RC is highly rated even when the reference label is  considered incorrect.By manual inspection, we find that in these cases, identifying the root cause required understanding of the log files and was not possible from the expert discussion, communication with the customer, and issue resolution alone, which was used for our reference label extraction.Together, these results suggest that our models can provide valuable insights and help experts in diagnosing root causes of complex robotics systems issues.</p>
<p>E Human Expert Study</p>
<p>We illustrate the survey interface including all questions in Figure 10.Unfortunately, we cannot provide examples of the analyzed data and identified RCs due to the proprietary nature of the data.Figure 10: Human expert study questions and interface.</p>
<p>Figure 1 :
1
Figure 1: Visualization of the label extraction process for historic tickets based on querying a strong LLM.Note that during inference time only the grey, but not the blue, boxes are available.</p>
<p>Figure 2 :
2
Figure 2: Token count distribution of processed finetuning inputs (blue) and corresponding raw logs (red).</p>
<p>Figure 3 :
3
Figure 3: Mean similarity score of MISTRAL-LITE-7B for LORA and QLORA training depending on rank r.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: CoT prompt used for root cause extraction, where <PLACEHOLDERS> for data from the ticket are marked red.instruction: An issue with an industrial grade robot is reported in the input.Determine the root cause for the reported issue.input: Elog log message: <ELOG> Error description: <PROBLEM DESCRIPTION> Startup log message: <STARTUP> Print spool log message: <PRINT SPOOL> output: <REFERENCE ROOT CAUSE></p>
<p>[</p>
<p>Figure 7 :
7
Figure 7: Prompt used for LLM-as-a-judge evaluation, where <PLACEHOLDERS> are replaced with the issue specific data.</p>
<p>Figure 8 :
8
Figure 8: Similarity score distribution of GPT-4 and variants of MISTRAL-LITE-7B (ML).</p>
<p>Figure 9 :
9
Figure 9: Human expert rating of fully finetuned MISTRAL-LITE-7B depending on whether they considered the reference label to be correct (yes, maybe, or no).</p>
<p>Table 1 :
1
Mean similarity score M SS for different models and adaptation methods on SYSDIAGBENCH.
ModelBaseFFT LORA QLORAMISTRAL-LITE-7B2.392.943.073.27MIXTRAL-8X7B2.252.692.242.30GPT-42.52---System Diagnostic Performance We comparethe performance of different models and train-ing methods in Table 1. Interestingly, we findthat the smaller MISTRAL-LITE-7B outperformsMIXTRAL-8X7B across all adaptation settings.We hypothesize this is because it was specificallytrained for long context capabilities, crucial foranalyzing long log files. While GPT-4 is the best-performing base model, we find that our finetunedmodels outperform it by a significant margin, withQLORA training yielding the best performance.Rank Deficiency as Regularization Observing
that LORA and QLORA outperform full finetuning for MISTRAL-LITE-7B (see Table1), we investigate the impact of the LORA rank r on the model's performance, illustrating results in Figure3.We find that while a smaller rank improves</p>
<p>Table 2 :
2
GPU (NVIDIA A100) hours required for different training setups.
ModelTraining Mode Training Time [h]FFT23MISTRAL-LITE-7BLORA20QLORA20FFT64MIXTRAL-8X7BLORA40QLORA40</p>
<p>Table 3 :
3
Mean similarity score of MISTRAL-LITE-7B with LORA training depending on features used.
ModelM SSAll Features3.07without startup3.17without printspool3.11without elog2.85without Problem Description2.58without printspool and startup3.2250Frequency [%]GPT-4ML Base25ML FFT ML LoRAML QLoRA0123456789 10Similarity Score</p>
<p>. Axolotl, 2024</p>
<p>Failure diagnosis using decision trees. Mike Y Chen, Alice X Zheng, Jim Lloyd, Michael I Jordan, Eric A Brewer, Proc. of ICAC. of ICAC2004</p>
<p>Bert-log: Anomaly detection for system logs based on pre-trained language model. Song Chen, Hai Liao, Appl. Artif. Intell. 12022</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Proc. of NeurIPS. of NeurIPS2023</p>
<p>A joint study of the challenges, opportunities, and roadmap of mlops and aiops: A systematic survey. Josu Díaz-De-Arcaya, Ana I Torre-Bastida, Gorka Zárate, Raúl Miñón, Aitor Almeida, ACM Comput. Surv. 42024</p>
<p>GPTQ: accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, CoRR, abs/2210.173232022</p>
<p>Learning representations on logs for aiops. Pranjal Gupta, Harshit Kumar, Debanjana Kar, Karan Bhukar, Pooja Aggarwal, Prateeti Mohapatra, Proc. of CLOUD. of CLOUD2023</p>
<p>Drain: An online log parsing approach with fixed depth tree. Pinjia He, Jieming Zhu, Zibin Zheng, Michael R Lyu, Proc. of ICWS. of ICWS2017</p>
<p>Lora: Low-rank adaptation of large language models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, Proc. of ICLR. of ICLR2022</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Emma Bou Casas, Florian Hanna, Gianna Bressand, Guillaume Lengyel, Guillaume Bour, Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, CoRR, abs/2401.04088Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2024</p>
<p>Swe-bench: Can language models resolve real-world github issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, CoRR, abs/2310.067702023Ofir Press, and Karthik Narasimhan</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, Proc. of ICLR. of ICLR2015</p>
<p>Lanobert: System log anomaly detection based on BERT masked language model. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Proc. of NeurIPS. Yukyung Lee, Jina Kim, and Pilsung Kang. of NeurIPS. Yukyung Lee, Jina Kim, and Pilsung Kang2022. 2023Large language models are zero-shot reasoners</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. 2004</p>
<p>AWQ: activationaware weight quantization for LLM compression and acceleration. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han, CoRR, abs/2306.009782023</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, Proc. of ICLR. of ICLR2019</p>
<p>Mining invariants from console logs for system problem detection. Jian-Guang Lou, Qiang Fu, Shengqi Yang, Ye Xu, Jiang Li, Proc. of USENIX. of USENIX2010</p>
<p>A search-based approach for accurate identification of log message formats. Salma Messaoudi, Annibale Panichella, Domenico Bianculli, Lionel C Briand, Raimondas Sasnauskas, Proc. of ICPC. of ICPC2018</p>
<p>CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Robust and transferable anomaly detection in log data using pretrained language models. Harold Ott, Jasmin Bogatinovski, Alexander Acker, Sasho Nedelkoski, Odej Kao, CoRR, abs/2102.115702021</p>
<p>Value-aware quantization for training and inference of neural networks. Eunhyeok Park, Sungjoo Yoo, Peter Vajda, Proc. of ECCV. of ECCV2018</p>
<p>Zero: memory optimizations toward training trillion parameter models. Samyam Rajbhandari, Jeff Rasley, Proc. of SC. of SC2020Olatunji Ruwase, and Yuxiong He</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Proc. of KDD. of KDD2020Olatunji Ruwase, and Yuxiong He</p>
<p>Log anomaly detection method based on bert model optimization. Yangyi Shao, Wenbin Zhang, Peishun Liu, Ren Huyue, Ruichun Tang, Qilin Yin, Qi Li, Proc. of CLOUD. of CLOUDIEEE2022</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, Proc. of ICLR. of ICLR2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proc. of NeurIPS. of NeurIPS2022</p>
<p>Expertprompting: Instructing large language models to be distinguished experts. Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao, CoRR, abs/2305.146882023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Proc. of NeurIPS. of NeurIPS2023</p>
<p>. Yin Song, Chen Wu, Eden Duthie, 2023</p>
<p>Improving software diagnosability via log enhancement. Ding Yuan, Jing Zheng, Soyeon Park, Yuanyuan Zhou, Stefan Savage, ACM Trans. Comput. Syst. 12012</p>
<p>Failure prediction in IBM bluegene/l event logs. Yanyong Zhang, Anand Sivasubramaniam, Proc. of IPDPS. of IPDPS2008</p>
<p>Alex Sherstinsky, Piero Molino, Travis Addair, and Devvret Rishi. Justin Zhao, Timothy Wang, Wael Abid, Geoffrey Angus, Arnav Garg, Jeffery Kinnison, preprint, abs/2405.00732Lora land: 310 fine-tuned llms that rival gpt-4, technical report. 2024</p>
<p>Log20: Fully automated optimal placement of log printing statements under specified overhead threshold. Xu Zhao, Kirk Rodrigues, Yu Luo, Michael Stumm, Ding Yuan, Yuanyuan Zhou, Proc. of SOSP. of SOSP2017</p>
<p>Robust anomaly detection on unreliable data. Zilong Zhao, Sophie Cerf, Robert Birke, Bogdan Robu, Sara Bouchenak, Sonia Ben Mokhtar, Lydia Y Chen, Proc. of DSN. of DSN2019</p>
<p>Jiang Zhaoxue, Tong Li, Zhenguo Zhang, Jingguo Ge, Junling You, Liangxiong Li, A survey on log research of aiops: Methods and trends. 2021</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Proc. of NeurIPS. of NeurIPS2023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, Ed H Chi, Proc. of ICLR. of ICLR2023</p>
<p>An intelligent anomaly detection scheme for micro-services architectures with temporal and spatial data analysis. Yuan Zuo, Yulei Wu, Geyong Min, Chengqiang Huang, Ke Pei, IEEE Trans. Cogn. Commun. Netw. 22020</p>            </div>
        </div>

    </div>
</body>
</html>
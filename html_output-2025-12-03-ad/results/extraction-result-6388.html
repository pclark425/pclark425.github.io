<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6388 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6388</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6388</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-a0b07f40de7b307dfc40e5c569af0d14d4160e8e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a0b07f40de7b307dfc40e5c569af0d14d4160e8e" target="_blank">Common 7B Language Models Already Possess Strong Math Capabilities</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy when selecting the best response from 256 random generations, and finds that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers.</p>
                <p><strong>Paper Abstract:</strong> Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples. This straightforward approach achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B models, surpassing previous models by 14.2% and 20.8%, respectively. We also provide insights into scaling behaviors across different reasoning complexities and error types.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6388.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6388.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-7B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 7B (pretrained base model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter decoder-only transformer family model (LLaMA-2) used as the paper's principal base; demonstrates strong latent math capability but unstable generation of correct answers without large SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Common pretraining on web-scale corpora used by LLaMA-2 (no special math-focused pretraining reported in this work); models evaluated in their SFT/instruction-tuned variants.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems / symbolic reasoning (grade-school to contest-level)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems with chain-of-thought (CoT) style solution generation</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school; MATH: contest-level (hard)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-Thought (CoT) style inference; evaluation uses random-sampling generation (temperature 0.7) and greedy decodes for comparisons; SFT uses Vicuna system prompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@N (Pass@256 used to show capability) and PassRatio@N / Pass@1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Pass@256: GSM8K 97.7% ; MATH 72.0%. PassRatio@256: GSM8K 48.2% ; MATH 7.9%. Pass@1 (single-generation): GSM8K ~49.5% ; MATH ~7.9% (reported for first answer)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper finds strong latent capability: correct answers are often present among many stochastic generations (high Pass@256) but not reliably produced (low PassRatio and Pass@1); uses a single-step accuracy model Acc_final = Acc_step^s to infer per-step reliability and employs GPT-4 Turbo to annotate first-error positions; no mechanistic neuron- or attention-level probing reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Instability (correct answers present only at low frequency), numerical/calculation mistakes (wrong arithmetic), reasoning errors (loss of problem conditions, concept confusion); stochastic sampling yields both correct and incorrect chains-of-thought for same problem.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Scaling up supervised fine-tuning (SFT) data stabilizes outputs (PassRatio and Pass@1 increase) while Pass@256 (upper capability) changes little; suggests upper capability existed pre-fine-tuning but stability improves nearly linearly/super-linearly with SFT size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Common 7B Language Models Already Possess Strong Math Capabilities', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6388.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6388.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Xwin-Math-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Xwin-Math (LLaMA-2-7B fine-tuned with synthetic SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-2-7B model fine-tuned with up to 960K synthetic GSM8K-style and 480K MATH-style supervised examples generated via GPT-4 Turbo; dramatically improves single-shot math accuracy and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Xwin-Math-7B (LLaMA-2-7B fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (LLaMA-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Supervised fine-tuning on a mixture of real GSM8K/MATH train sets and large-scale synthetic data (up to 960K GSM8K-like and 480K MATH-like examples) generated and verified by GPT-4 Turbo; CoT-style answers included as targets; Vicuna-v1.1 system prompt during training.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH (also SVAMP, ASDiv, Hungarian HS Exam for OOD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and mathematical word problems (addition/subtraction/multiplication/division combinations, multi-step reasoning, algebraic manipulations)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems with chain-of-thought answer target (teacher-style step-by-step solutions provided in SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school; MATH: contest-level (varied, includes hard problems after resampling)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Supervised fine-tuning (SFT) on synthetic CoT answers; inference uses CoT prompting; evaluation sampling temperature 0.7; also uses greedy decoding for baseline measures</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (Pass@1 / exact final-answer match) reported in standard benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 82.6% accuracy; MATH: 40.6% accuracy (these are final single-response metrics for Xwin-Math-7B after large-scale synthetic SFT, from Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors estimate single-step reasoning accuracy via Acc_final = Acc_step^s and report estimated Acc_step increases with SFT scale (7.5K -> 78.9%; 120K -> 89.7%; 960K -> 94.2%); they used GPT-4 Turbo to label normalized first-error positions (improves with SFT). Analysis shows most improvement is in stepwise reliability rather than expansion of latent capability.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Remaining errors are dominated more by logical/reasoning errors than numerical calculation with larger SFT; calculation errors decrease faster than reasoning errors with more data. Instability largely mitigated but not fully eliminated.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Large gains from scaling SFT (real or synthetic) up to ~1M examples; Pass@256 saturates early but Pass@1 and PassRatio increase substantially with SFT size; resampling to upweight longer CoT examples improves hard-problem accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Common 7B Language Models Already Possess Strong Math Capabilities', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6388.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6388.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Xwin-Math-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Xwin-Math (LLaMA-2-70B fine-tuned with synthetic SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger 70B LLaMA-2 variant fine-tuned with the same synthetic SFT pipeline, achieving state-of-the-art open-source results on GSM8K and MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Xwin-Math-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (LLaMA-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on large-scale synthetic SFT datasets (e.g., 960K GSM8K-style or 480K MATH-style examples generated by GPT-4 Turbo) with CoT answers; training hyperparameters described (Adam, cosine LR, up to 3 epochs).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, Hungarian National High School Exam (OOD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math reasoning and contest-level problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems with CoT targets</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to contest-level (MATH highly challenging)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>SFT with CoT answers; inference uses CoT and sampling (temperature 0.7) / greedy decode comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (Pass@1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 90.6% ; MATH: 52.8% (reported in Table 2 for Xwin-Math-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Same higher-level analyses as for 7B: large SFT scale increases single-step accuracy; larger base size improves both upper capability and stability relative to smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Residual reasoning errors on hard MATH problems; fewer calculation errors than smaller models; instability further reduced but present for the hardest problems.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Larger base model plus large synthetic SFT yields higher final accuracy and gains across both GSM8K and MATH; improvements relative to 7B are notable on both benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Common 7B Language Models Already Possess Strong Math Capabilities', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6388.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6388.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Turbo (used for synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Turbo (used as synthetic data generator and verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source, high-capability model (GPT-4 Turbo) used by the authors to generate new math questions, verify solvability, and produce chain-of-thought answers at scale for SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo (used via API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (OpenAI GPT-4 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>closed-source (not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not detailed in this paper; used as a high-quality generator/verifier to create synthetic math Q/A and CoT answers</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>used to produce synthetic GSM8K and MATH-style training examples (not directly benchmarked here)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>generation of synthetic math word problems, verification via attempted solutions, and generation of CoT answers</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems (generated) and CoT solution strings</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>controls for dataset-specific difficulty (GSM8K grade-school, MATH contest-level) via seeding prompts</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Three-step synthetic pipeline: (1) generate brand-new question given a reference question with constraints (common-sense, solvable, no answers), (2) verify question solvability by integrating solving+verification prompts, (3) generate CoT answers; temperature set to 1.0 for synthesis in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used qualitatively/indirectly to judge synthetic-data quality; also reported closed-source baseline performance elsewhere (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported baseline performances (for reference): GPT-4 Turbo (1106) GSM8K 94.8% ; MATH 64.5% (from Table 2) — these values are from cited closed-source runs, not direct SFT experiments by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper uses GPT-4 Turbo both to generate and to verify/repair synthetic questions; also used as an oracle to estimate error positions in student models (e.g., finding first incorrect CoT step). No internal mechanistic probing of GPT-4 Turbo itself is performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Synthetic generation can produce questions that accidentally leak test copies if not verified; authors measure LM loss statistics to argue against leakage. Other synthetic methods (change numbers, add constraints) vary in scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Synthetic data produced by GPT-4 Turbo scales nearly linearly: synthetic SFT up to ~1M examples continues to improve fine-tuned model stability and accuracy with no clear saturation within examined range.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Common 7B Language Models Already Possess Strong Math Capabilities', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6388.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6388.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pass@N / PassRatio metric analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pass@N and PassRatio@N metrics and Acc_step analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational metrics and an analytical model used in the paper to quantify math capability and stability: Pass@N measures whether any of N attempts include a correct answer; PassRatio@N measures fraction correct among N samples; Acc_final ≈ Acc_step^s links final-answer accuracy to per-step accuracy across s CoT steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pass@N / PassRatio@N / Acc_step model (analysis applied to evaluated models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>analytic / evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>applied to GSM8K and MATH evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>evaluation of multi-step math problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language problems with CoT outputs; evaluated over N sampled generations (temperature 0.7)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>used across GSM8K (easier) and MATH (harder) to compare behaviors</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>CoT inference with sampling (N up to 256); sampling temperature 0.7 for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@N, PassRatio@N, Pass@1 reported; also estimated single-step accuracy (Acc_step) using the relation Acc_final = Acc_step^s</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Demonstrated that many correct answers exist among 256 samples (high Pass@256) even for small models, but PassRatio@256 and Pass@1 are much lower; estimated Acc_step for Xwin-Math-7B: 7.5K data -> 78.9% ; 120K -> 89.7% ; 960K -> 94.2% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Acc_final = Acc_step^s model used to infer that improvements from SFT are due mainly to increases in per-step reliability; authors used GPT-4 Turbo to label step counts and first-error positions to validate the single-step model; observed that later data points deviate when complex-problem proportion is low, motivating resampling.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Pass@256 vs PassRatio gap reveals instability: models produce correct solutions rarely but do possess them in latent distribution; single-step model breaks down if CoT step counts are mis-estimated or if step independence assumption fails (authors note deviations for complex problems).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Pass@256 saturates quickly, while PassRatio@N and Pass@1 improve substantially with SFT scale; single-step accuracy rises smoothly with data and can be amplified for hard problems by resampling to increase longer-CoT examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Common 7B Language Models Already Possess Strong Math Capabilities', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6388.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6388.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) use</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting and CoT targets in SFT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CoT is used both as an inference prompting strategy and as training targets in SFT (synthetic CoT answers produced by GPT-4 Turbo), enabling multi-step intermediate reasoning to be learned and evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting / CoT SFT</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>prompting / fine-tuning technique</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>SFT targets include step-by-step CoT solutions generated/verified by GPT-4 Turbo; longer CoT examples are upweighted in resampling experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH (CoT annotations used for step counts and training targets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning tasks (arithmetic, algebra, contest math)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language with explicit step-by-step solution text</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>applies across grade-school to contest-level; longer CoT corresponds to harder problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>CoT prompting at inference; SFT uses CoT solutions as targets (supervised), synthetic generation pipeline explicitly requests CoT answers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (final-answer) and inferred per-step accuracy; number of CoT steps used as analytic variable</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Using CoT targets in SFT and inference, large synthetic datasets lift final single-answer accuracy substantially: e.g., Xwin-Math-7B GSM8K 82.6% (with CoT SFT). Resampling to increase long CoT examples raised PassRatio@256 from 71.1 to 72.8 in one experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors treat CoT as a sequence of s reasoning steps and use Acc_final ≈ Acc_step^s to relate final accuracy to per-step reliability; resampling to increase fraction of long-CoT examples improves accuracy on complex questions; GPT-4 used to count steps and find first-step errors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When CoT steps are long or correlated, independence assumption in Acc_final model weakens; models still make reasoning errors (losing conditions or concept confusion) even when arithmetic steps are corrected.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Longer CoT examples are rarer in base SFT; augmenting/resampling to increase their share materially improves performance on hard questions; per-step accuracy improves with more SFT, yielding super-linear final improvements for multi-step problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Common 7B Language Models Already Possess Strong Math Capabilities', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>Metamath: Bootstrap your own mathematical questions for large language models <em>(Rating: 2)</em></li>
                <li>MuggleMath: Augmenting GSM8K with GPT-4 operations <em>(Rating: 2)</em></li>
                <li>Llemma: An open language model for mathematics <em>(Rating: 2)</em></li>
                <li>Let's verify step by step <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6388",
    "paper_id": "paper-a0b07f40de7b307dfc40e5c569af0d14d4160e8e",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "LLaMA-2-7B (base)",
            "name_full": "LLaMA-2 7B (pretrained base model)",
            "brief_description": "A 7-billion-parameter decoder-only transformer family model (LLaMA-2) used as the paper's principal base; demonstrates strong latent math capability but unstable generation of correct answers without large SFT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Common pretraining on web-scale corpora used by LLaMA-2 (no special math-focused pretraining reported in this work); models evaluated in their SFT/instruction-tuned variants.",
            "benchmark_name": "GSM8K, MATH",
            "task_type": "multi-step math word problems / symbolic reasoning (grade-school to contest-level)",
            "problem_format": "natural-language word problems with chain-of-thought (CoT) style solution generation",
            "difficulty_level": "GSM8K: grade-school; MATH: contest-level (hard)",
            "prompting_method": "Chain-of-Thought (CoT) style inference; evaluation uses random-sampling generation (temperature 0.7) and greedy decodes for comparisons; SFT uses Vicuna system prompt",
            "performance_metric": "Pass@N (Pass@256 used to show capability) and PassRatio@N / Pass@1",
            "performance_value": "Pass@256: GSM8K 97.7% ; MATH 72.0%. PassRatio@256: GSM8K 48.2% ; MATH 7.9%. Pass@1 (single-generation): GSM8K ~49.5% ; MATH ~7.9% (reported for first answer)",
            "internal_analysis": "Paper finds strong latent capability: correct answers are often present among many stochastic generations (high Pass@256) but not reliably produced (low PassRatio and Pass@1); uses a single-step accuracy model Acc_final = Acc_step^s to infer per-step reliability and employs GPT-4 Turbo to annotate first-error positions; no mechanistic neuron- or attention-level probing reported.",
            "failure_modes": "Instability (correct answers present only at low frequency), numerical/calculation mistakes (wrong arithmetic), reasoning errors (loss of problem conditions, concept confusion); stochastic sampling yields both correct and incorrect chains-of-thought for same problem.",
            "scaling_trend": "Scaling up supervised fine-tuning (SFT) data stabilizes outputs (PassRatio and Pass@1 increase) while Pass@256 (upper capability) changes little; suggests upper capability existed pre-fine-tuning but stability improves nearly linearly/super-linearly with SFT size.",
            "uuid": "e6388.0",
            "source_info": {
                "paper_title": "Common 7B Language Models Already Possess Strong Math Capabilities",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Xwin-Math-7B",
            "name_full": "Xwin-Math (LLaMA-2-7B fine-tuned with synthetic SFT)",
            "brief_description": "A LLaMA-2-7B model fine-tuned with up to 960K synthetic GSM8K-style and 480K MATH-style supervised examples generated via GPT-4 Turbo; dramatically improves single-shot math accuracy and stability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Xwin-Math-7B (LLaMA-2-7B fine-tuned)",
            "model_family": "decoder-only transformer (LLaMA-2)",
            "model_size": "7B (fine-tuned)",
            "training_data_description": "Supervised fine-tuning on a mixture of real GSM8K/MATH train sets and large-scale synthetic data (up to 960K GSM8K-like and 480K MATH-like examples) generated and verified by GPT-4 Turbo; CoT-style answers included as targets; Vicuna-v1.1 system prompt during training.",
            "benchmark_name": "GSM8K, MATH (also SVAMP, ASDiv, Hungarian HS Exam for OOD)",
            "task_type": "multi-step arithmetic and mathematical word problems (addition/subtraction/multiplication/division combinations, multi-step reasoning, algebraic manipulations)",
            "problem_format": "natural-language word problems with chain-of-thought answer target (teacher-style step-by-step solutions provided in SFT)",
            "difficulty_level": "GSM8K: grade-school; MATH: contest-level (varied, includes hard problems after resampling)",
            "prompting_method": "Supervised fine-tuning (SFT) on synthetic CoT answers; inference uses CoT prompting; evaluation sampling temperature 0.7; also uses greedy decoding for baseline measures",
            "performance_metric": "Accuracy (Pass@1 / exact final-answer match) reported in standard benchmarks",
            "performance_value": "GSM8K: 82.6% accuracy; MATH: 40.6% accuracy (these are final single-response metrics for Xwin-Math-7B after large-scale synthetic SFT, from Table 2)",
            "internal_analysis": "Authors estimate single-step reasoning accuracy via Acc_final = Acc_step^s and report estimated Acc_step increases with SFT scale (7.5K -&gt; 78.9%; 120K -&gt; 89.7%; 960K -&gt; 94.2%); they used GPT-4 Turbo to label normalized first-error positions (improves with SFT). Analysis shows most improvement is in stepwise reliability rather than expansion of latent capability.",
            "failure_modes": "Remaining errors are dominated more by logical/reasoning errors than numerical calculation with larger SFT; calculation errors decrease faster than reasoning errors with more data. Instability largely mitigated but not fully eliminated.",
            "scaling_trend": "Large gains from scaling SFT (real or synthetic) up to ~1M examples; Pass@256 saturates early but Pass@1 and PassRatio increase substantially with SFT size; resampling to upweight longer CoT examples improves hard-problem accuracy.",
            "uuid": "e6388.1",
            "source_info": {
                "paper_title": "Common 7B Language Models Already Possess Strong Math Capabilities",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Xwin-Math-70B",
            "name_full": "Xwin-Math (LLaMA-2-70B fine-tuned with synthetic SFT)",
            "brief_description": "A larger 70B LLaMA-2 variant fine-tuned with the same synthetic SFT pipeline, achieving state-of-the-art open-source results on GSM8K and MATH.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Xwin-Math-70B",
            "model_family": "decoder-only transformer (LLaMA-2)",
            "model_size": "70B",
            "training_data_description": "Fine-tuned on large-scale synthetic SFT datasets (e.g., 960K GSM8K-style or 480K MATH-style examples generated by GPT-4 Turbo) with CoT answers; training hyperparameters described (Adam, cosine LR, up to 3 epochs).",
            "benchmark_name": "GSM8K, MATH, Hungarian National High School Exam (OOD)",
            "task_type": "multi-step math reasoning and contest-level problems",
            "problem_format": "natural-language word problems with CoT targets",
            "difficulty_level": "grade-school to contest-level (MATH highly challenging)",
            "prompting_method": "SFT with CoT answers; inference uses CoT and sampling (temperature 0.7) / greedy decode comparisons",
            "performance_metric": "Accuracy (Pass@1)",
            "performance_value": "GSM8K: 90.6% ; MATH: 52.8% (reported in Table 2 for Xwin-Math-70B)",
            "internal_analysis": "Same higher-level analyses as for 7B: large SFT scale increases single-step accuracy; larger base size improves both upper capability and stability relative to smaller models.",
            "failure_modes": "Residual reasoning errors on hard MATH problems; fewer calculation errors than smaller models; instability further reduced but present for the hardest problems.",
            "scaling_trend": "Larger base model plus large synthetic SFT yields higher final accuracy and gains across both GSM8K and MATH; improvements relative to 7B are notable on both benchmarks.",
            "uuid": "e6388.2",
            "source_info": {
                "paper_title": "Common 7B Language Models Already Possess Strong Math Capabilities",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-4 Turbo (used for synthesis)",
            "name_full": "GPT-4 Turbo (used as synthetic data generator and verifier)",
            "brief_description": "A closed-source, high-capability model (GPT-4 Turbo) used by the authors to generate new math questions, verify solvability, and produce chain-of-thought answers at scale for SFT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo (used via API)",
            "model_family": "decoder-only transformer (OpenAI GPT-4 family)",
            "model_size": "closed-source (not specified)",
            "training_data_description": "Not detailed in this paper; used as a high-quality generator/verifier to create synthetic math Q/A and CoT answers",
            "benchmark_name": "used to produce synthetic GSM8K and MATH-style training examples (not directly benchmarked here)",
            "task_type": "generation of synthetic math word problems, verification via attempted solutions, and generation of CoT answers",
            "problem_format": "natural-language word problems (generated) and CoT solution strings",
            "difficulty_level": "controls for dataset-specific difficulty (GSM8K grade-school, MATH contest-level) via seeding prompts",
            "prompting_method": "Three-step synthetic pipeline: (1) generate brand-new question given a reference question with constraints (common-sense, solvable, no answers), (2) verify question solvability by integrating solving+verification prompts, (3) generate CoT answers; temperature set to 1.0 for synthesis in experiments",
            "performance_metric": "Used qualitatively/indirectly to judge synthetic-data quality; also reported closed-source baseline performance elsewhere (Table 2)",
            "performance_value": "Reported baseline performances (for reference): GPT-4 Turbo (1106) GSM8K 94.8% ; MATH 64.5% (from Table 2) — these values are from cited closed-source runs, not direct SFT experiments by the authors.",
            "internal_analysis": "Paper uses GPT-4 Turbo both to generate and to verify/repair synthetic questions; also used as an oracle to estimate error positions in student models (e.g., finding first incorrect CoT step). No internal mechanistic probing of GPT-4 Turbo itself is performed.",
            "failure_modes": "Synthetic generation can produce questions that accidentally leak test copies if not verified; authors measure LM loss statistics to argue against leakage. Other synthetic methods (change numbers, add constraints) vary in scalability.",
            "scaling_trend": "Synthetic data produced by GPT-4 Turbo scales nearly linearly: synthetic SFT up to ~1M examples continues to improve fine-tuned model stability and accuracy with no clear saturation within examined range.",
            "uuid": "e6388.3",
            "source_info": {
                "paper_title": "Common 7B Language Models Already Possess Strong Math Capabilities",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Pass@N / PassRatio metric analysis",
            "name_full": "Pass@N and PassRatio@N metrics and Acc_step analysis",
            "brief_description": "Operational metrics and an analytical model used in the paper to quantify math capability and stability: Pass@N measures whether any of N attempts include a correct answer; PassRatio@N measures fraction correct among N samples; Acc_final ≈ Acc_step^s links final-answer accuracy to per-step accuracy across s CoT steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pass@N / PassRatio@N / Acc_step model (analysis applied to evaluated models)",
            "model_family": "analytic / evaluation metrics",
            "model_size": "n/a",
            "training_data_description": "n/a",
            "benchmark_name": "applied to GSM8K and MATH evaluations",
            "task_type": "evaluation of multi-step math problem solving",
            "problem_format": "natural-language problems with CoT outputs; evaluated over N sampled generations (temperature 0.7)",
            "difficulty_level": "used across GSM8K (easier) and MATH (harder) to compare behaviors",
            "prompting_method": "CoT inference with sampling (N up to 256); sampling temperature 0.7 for evaluation",
            "performance_metric": "Pass@N, PassRatio@N, Pass@1 reported; also estimated single-step accuracy (Acc_step) using the relation Acc_final = Acc_step^s",
            "performance_value": "Demonstrated that many correct answers exist among 256 samples (high Pass@256) even for small models, but PassRatio@256 and Pass@1 are much lower; estimated Acc_step for Xwin-Math-7B: 7.5K data -&gt; 78.9% ; 120K -&gt; 89.7% ; 960K -&gt; 94.2% (Table 4).",
            "internal_analysis": "Acc_final = Acc_step^s model used to infer that improvements from SFT are due mainly to increases in per-step reliability; authors used GPT-4 Turbo to label step counts and first-error positions to validate the single-step model; observed that later data points deviate when complex-problem proportion is low, motivating resampling.",
            "failure_modes": "Pass@256 vs PassRatio gap reveals instability: models produce correct solutions rarely but do possess them in latent distribution; single-step model breaks down if CoT step counts are mis-estimated or if step independence assumption fails (authors note deviations for complex problems).",
            "scaling_trend": "Pass@256 saturates quickly, while PassRatio@N and Pass@1 improve substantially with SFT scale; single-step accuracy rises smoothly with data and can be amplified for hard problems by resampling to increase longer-CoT examples.",
            "uuid": "e6388.4",
            "source_info": {
                "paper_title": "Common 7B Language Models Already Possess Strong Math Capabilities",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) use",
            "name_full": "Chain-of-Thought prompting and CoT targets in SFT",
            "brief_description": "CoT is used both as an inference prompting strategy and as training targets in SFT (synthetic CoT answers produced by GPT-4 Turbo), enabling multi-step intermediate reasoning to be learned and evaluated.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought prompting / CoT SFT",
            "model_family": "prompting / fine-tuning technique",
            "model_size": "n/a",
            "training_data_description": "SFT targets include step-by-step CoT solutions generated/verified by GPT-4 Turbo; longer CoT examples are upweighted in resampling experiments.",
            "benchmark_name": "GSM8K, MATH (CoT annotations used for step counts and training targets)",
            "task_type": "multi-step reasoning tasks (arithmetic, algebra, contest math)",
            "problem_format": "natural-language with explicit step-by-step solution text",
            "difficulty_level": "applies across grade-school to contest-level; longer CoT corresponds to harder problems",
            "prompting_method": "CoT prompting at inference; SFT uses CoT solutions as targets (supervised), synthetic generation pipeline explicitly requests CoT answers",
            "performance_metric": "Accuracy (final-answer) and inferred per-step accuracy; number of CoT steps used as analytic variable",
            "performance_value": "Using CoT targets in SFT and inference, large synthetic datasets lift final single-answer accuracy substantially: e.g., Xwin-Math-7B GSM8K 82.6% (with CoT SFT). Resampling to increase long CoT examples raised PassRatio@256 from 71.1 to 72.8 in one experiment.",
            "internal_analysis": "Authors treat CoT as a sequence of s reasoning steps and use Acc_final ≈ Acc_step^s to relate final accuracy to per-step reliability; resampling to increase fraction of long-CoT examples improves accuracy on complex questions; GPT-4 used to count steps and find first-step errors.",
            "failure_modes": "When CoT steps are long or correlated, independence assumption in Acc_final model weakens; models still make reasoning errors (losing conditions or concept confusion) even when arithmetic steps are corrected.",
            "scaling_trend": "Longer CoT examples are rarer in base SFT; augmenting/resampling to increase their share materially improves performance on hard questions; per-step accuracy improves with more SFT, yielding super-linear final improvements for multi-step problems.",
            "uuid": "e6388.5",
            "source_info": {
                "paper_title": "Common 7B Language Models Already Possess Strong Math Capabilities",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Metamath: Bootstrap your own mathematical questions for large language models",
            "rating": 2,
            "sanitized_title": "metamath_bootstrap_your_own_mathematical_questions_for_large_language_models"
        },
        {
            "paper_title": "MuggleMath: Augmenting GSM8K with GPT-4 operations",
            "rating": 2,
            "sanitized_title": "mugglemath_augmenting_gsm8k_with_gpt4_operations"
        },
        {
            "paper_title": "Llemma: An open language model for mathematics",
            "rating": 2,
            "sanitized_title": "llemma_an_open_language_model_for_mathematics"
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 1,
            "sanitized_title": "lets_verify_step_by_step"
        }
    ],
    "cost": 0.01537675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Common 7B Language Models Already Possess Strong Math Capabilities</h1>
<p>Chen Li ${ }^{1,4}$, Weiqi Wang ${ }^{2,4}$, Jingcheng $\mathbf{H u}^{3,4}$, Yixuan Wei ${ }^{3,4}$, Nanning Zheng ${ }^{1}$, Han $\mathbf{H u}^{4}$, Zheng Zhang ${ }^{4 <em>}$, Houwen Peng ${ }^{4 </em>}$<br>${ }^{1}$ IAIR, Xi'an Jiaotong University ${ }^{2}$ University of Science and Technology of China<br>${ }^{3}$ Tsinghua University ${ }^{4}$ Microsoft Research Asia<br>edward82@stu.xjtu.edu.cn {v-weiqiwang, t-jingchu, t-yixuanwei, zhez, houwen.peng}@microsoft.com<br>nnzheng@xjtu.edu.cn ancientmooner@gmail.com</p>
<h4>Abstract</h4>
<p>Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pretraining already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of $97.7 \%$ and $72.0 \%$ on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to $49.5 \%$ and $7.9 \%$ on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples. This straightforward approach achieves an accuracy of $82.6 \%$ on GSM8K and $40.6 \%$ on MATH using LLaMA-2 7B models, surpassing previous models by $14.2 \%$ and $20.8 \%$, respectively. We also provide insights into scaling behaviors across different reasoning complexities and error types.</p>
<h2>1 Introduction</h2>
<p>Mathematical capabilities have long been considered so challenging that they are thought to emerge in common language models only at a very large scale. For instance, studies by (Wei et al., 2022a,b) suggest that only models with size exceeding 50</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The orange star markers represent the accuracy achieved by selecting the best response from 256 random generations of the LLaMA-2 7B model. The high accuracy on the MATH (left) and GSM8K (right) benchmarks ( $72.0 \%$ and $97.7 \%$, respectively) suggest that the LLaMA-2 7B already possesses strong mathematical capabilities, although the stability in generating correct answers could be enhanced. This paper demonstrates that by scaling synthetic SFT data, the stability can be significantly improved as evidenced by the curves. Through this straightforward scaling of SFT data, the top-performing model has exceeded an early GPT-4 model by $10.3 \%$ on the MATH benchmark.
billion parameters can attain meaningful accuracy or benefit from chain-of-thought processing on math problems. A strategy to equip smaller language models with mathematical abilities involves creating math-specific base models trained on hundreds of billions of math-related pre-training data (Lewkowycz et al., 2022; Azerbayev et al., 2023). However, the accuracy of such models remains modest; for example, Llemma-7B (Azerbayev et al., 2023) only achieves $36.4 \%$ on the GSM8K dataset (Cobbe et al., 2021) and $18.0 \%$ on the MATH dataset (Hendrycks et al., 2021).</p>
<p>In this paper, we demonstrate that common language models of small size, such as the LLaMA-2 7B model (Touvron et al., 2023b), already possess strong mathematical capabilities without specific pre-training on math-related data. Surprisingly, we find that with supervised fine-tuning on just thousands of math questions (noting that the SFT stage does not enhance capabilities as stated in</p>
<p>Table 1: Comparison of SFT data scaling with real versus synthetic math questions. It reveals that synthetic math questions are nearly as effective as real ones.</p>
<table>
<thead>
<tr>
<th>Data size</th>
<th>GSM8K-real</th>
<th>GSM8K-syn</th>
<th>MATH-real</th>
<th>MATH-syn</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.94K</td>
<td>26.7</td>
<td>25.9</td>
<td>4.2</td>
<td>3.9</td>
</tr>
<tr>
<td>1.88K</td>
<td>32.8</td>
<td>31.9</td>
<td>5.6</td>
<td>4.9</td>
</tr>
<tr>
<td>3.75K</td>
<td>43.3</td>
<td>42.2</td>
<td>6.6</td>
<td>6.0</td>
</tr>
<tr>
<td>7.50K</td>
<td>50.2</td>
<td>49.5</td>
<td>8.4</td>
<td>7.9</td>
</tr>
</tbody>
</table>
<p><em>Bai et al. (2022); Ouyang et al. (2022)</em>), the model can correctly solve 97.7% of GSM8K questions and 72.0% of MATH questions, when selecting the best answer from 256 random generations, as indicated by the orange star marks in Figure 1. It is noteworthy that the accuracy has even outperformed those reported for the GPT-4 model, which achieved 92.0% on GSM8K and 42.5% on MATH . Therefore, we conclude that the LLaMA-2 7B model has indeed developed strong mathematical capabilities. The primary issue is the lack of guarantee that the correct answer will be digged out, as most generations are incorrect. In fact, the accuracy drops to 49.5% on GSM8K and 7.9% on MATH if we consider only one random generation per question. We refer to this as the instability issue.</p>
<p>To address the instability issue, we first observe that the accuracy improves almost in linear or even super-linear with exponentially increased supervised fine-tuning (SFT) data. Moreover, we note that the accuracy is far from reaching a plateau when utilizing all available GSM8K and MATH training data (as shown in Table 1). This observation encourages us to further scale up the SFT data. However, we face a challenge as there is a lack of publicly accessible real data to support this continuous scaling.</p>
<p>To overcome this limitation, we turn to synthetic data, employing a prestigious language model, namely GPT-4 Turbo, to produce synthetic math questions. We find that a straightforward "brand-new" generation strategy, which prompts the GPT-4 Turbo to create a completely new question based on preference ones and then applies a simple verifier (also GPT-4 Turbo based), has been highly effective. Specifically, as indicated in Table 1, the use of synthetically generated math questions can achieve accuracy nearly on par with that of real questions, highlighting the potential of synthetic SFT math questions for the scaling purpose.</p>
<p>Leveraging synthetic data has allowed us to scale our SFT data significantly, for instance, from 7.5K to 960K on GSM8K and from 7.5K to 480K on MATH. This data scaling shows nearly perfect scaling behavior, as drawn in Figure 1. Specifically, by simply scaling the SFT data, our model has become the first to exceed 80% and 40% accuracy on GSM8K and MATH, respectively, using a standard LLaMA-2 7B base model (achieving 82.6% and 40.6% respectively).</p>
<p>The straightforward synthetic SFT data proves effective from stronger base models as well, such as LLaMA-2 70B, which achieves 90.6% on GSM8K and 52.8% on MATH. To the best of our knowledge, this is the first open-source model to exceed 90% accuracy on GSM8K. It is also the first open-source model to outperform GPT-4 (i.e., GPT-4-0314) on the MATH benchmark, demonstrating the efficacy of our simple synthetic scaling method.</p>
<p>In addition to the strong results, we have also gleaned insights into the effectiveness of our approach: 1) As the scale of SFT data increases, the model's accuracy tends to plateau when utilizing 256 attempts; however, there is a marked increase using 1 response. This indicates that while the model's upper capability limit remains fairly constant, the performance gains are primarily due to enhanced stability in generating correct answers. 2) The accuracy of solving math problems follows a power law with respect to the number of chain-of-thought (CoT) steps with different SFT data quantities. An expanded SFT dataset improves the reliability of each reasoning step. Further increasing the proportion of training samples with longer CoT steps through resampling can significantly improve the accuracy of the model for difficult questions. 3) An analysis of error types during the scaling process reveals that calculation errors are more readily mitigated compared to reasoning errors.</p>
<h2>2 Examine Math Capability of Language Models</h2>
<p>Metrics We employ two metrics to examine the math capabilities of language models.</p>
<p><sup>2</sup>Concurrently, DeepSeek-MATH-7B <em>Shao et al. (2024)</em> also surpasses 80% accuracy. However, their approach relies on a much stronger base model extensively pre-trained on math-related corpora and a sophisticated RL algorithm. Our results are complementary to theirs.</p>
<p>The first is a Pass@N metric</p>
<p>$\operatorname{Pass}@\mathrm{N}=\operatorname*{\mathbb{E}}_{\text {Problems }}[\min (c, 1)],$ (1)</p>
<p>where $c$ represents the number of correct answers out of $N$ responses. This metric considers a question to be solved if at least one correct answer is produced from $N$ random generations. We employ this metric to reflect the potential or capability of a model in solving a math question. To enhance the diversity of the $N$ generations, we set the temperature of the generation process to $0.7^{3}$.</p>
<p>The second is a PassRatio@N metric</p>
<p>$\operatorname{PassRatio}@\mathrm{N}=\operatorname*{\mathbb{E}}_{\text {Problems }}\left[\frac{c}{N}\right],$ (2)</p>
<p>which measures the percentage of correct answers within the $N$ generated answers. This metric is somewhat equivalent to Pass@1, but with reduced variance.</p>
<p>Observations Based on these two metrics, we examine the performance of the LLaMA-2 models on the GSM8K and the MATH benchmarks as shown in Figure 1. To adapt models for these two benchmarks in instruction-following settings, we use their SFT versions, which are trained with a limited amount of SFT data (i.e., 7.5K). As demonstrated in (Bai et al., 2022; Ouyang et al., 2022), the SFT stage does not enhance capabilities (and may even lead to a reduction, as mentioned in the context of "alignment taxes"). Therefore, employing the SFT version provides a fair assessment of the models' mathematical capabilities.</p>
<p>We first observe that the Pass@256 metrics for the LLaMA-2 7B model on both benchmarks are remarkably high: $97.7 \%$ on GSM8K and $72.0 \%$ on MATH. This suggests that the LLaMA-2 7B model possesses a strong capability for solving mathematical problems.</p>
<p>We then notice that the PassRatio@256 is significantly lower than that of Pass@256, being 48.2\% on GSM8K and 7.9\% on MATH. This suggests that while the correct answers to most math questions are present within 256 random generations, there is no assurance that the correct answers will consistently be extracted, a phenomenon we refer to as an "instability issue".</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>In the following, we will present a simple approach to significantly reduce the instability issue.</p>
<h2>3 Scaling SFT Data using Synthetic Math Questions</h2>
<p>In this section, we first demonstrate that scaling up the limited real SFT data can significantly alleviate the instability issue. We also observe that the accuracy has not yet plateaued when using the full available GSM8K and MATH training data. We consider further scaling up SFT data using synthetic math questions. To this aim, we introduce a straight-forward method for synthetic data generation utilizing the GPT-4 Turbo API. The synthetic data proves to be as effective as real math questions. Consequently, we boldly scale the synthetic SFT data to 960 K on GSM8K and 480 K on MATH, respectively, resulting in nearly perfect scaling behavior, and reach state-of-the-art accuracy.</p>
<p>Scaling using Real Math Questions We begin by examining the scaling behavior of real math questions across the entire GSM8K and MATH training sets. As indicated in Table 1, we observe a consistent accuracy improvement, increasing from $26.7 \%$ to $50.2 \%$ on GSM8K, and from $4.2 \%$ to $8.4 \%$ on MATH, with no signs of saturation.</p>
<p>Synthetic SFT Data Generation Since the real data has been exhausted, we contemplate further scaling up SFT data using synthetically generated math questions.</p>
<p>We introduce a straightforward three-step approach with the assistance of the GPT-4 Turbo API:</p>
<ul>
<li>Step 1. Generate a new math question. We request the GPT-4 Turbo API to generate a brand-new question using a reference math question as a starting point. To improve the validity of the new questions, we incorporate three rules into the prompt: Firstly, the new question must obey common knowledge; secondly, it should be solvable independently of the original question; and thirdly, it must not include any answer responses. Besides, we have set specific formatting requirements for questions and answers tailored to various target datasets.</li>
<li>Step 2. Verify the question. We further enhance the quality of the generated questions by validating and refining them through attempted solutions. By integrating solving and</li>
</ul>
<p>verification steps into a single prompt, we have found that this approach consistently elevates the validity of questions across different benchmarks.</p>
<ul>
<li>Step 3. Generate chain-of-thought (CoT) answers. We request GPT-4 Turbo to produce a chain-of-thought (CoT) answer response for each newly generated question.</li>
</ul>
<p>The detailed prompt designs are shown in Appendix A.</p>
<p>Comparison of Synthetic SFT Data versus Real Data To assess the quality of the synthetically generated math questions, we evaluate their effectiveness against real questions from the GSM8K and MATH training sets, utilizing a LLaMA-2 7B model, as detailed in Table 1. The results indicate that the synthetic math questions are nearly as effective as the real ones.</p>
<p>We also explored various other synthetic methods as proposed in previous works (Xu et al., 2023; Yu et al., 2023; An et al., 2023). These methods also prove to be effective, though marginally less so than the our approach, as illustrated in Figure 6.</p>
<p>Scaling to about a Million SFT Math Data Considering the effectiveness of the synthetic approach, we substantially increase the scale of the SFT data for both GSM8K and MATH problems, to 960 K and 480 K , respectively. Figure 1 presents the main reasults utilizing various sizes of the LLaMA2 series. The straightforward scaling strategy yields state-of-the-art accuracy.</p>
<p>It is also worth noting that the accuracy has not yet reached its peak. Exploring the effects of additional scaling will be left as our future research.</p>
<h2>4 Experiments</h2>
<h3>4.1 Datasets and Evaluations</h3>
<p>We conduct experiments on 5 benchmarks to evaluate the efficacy of the proposed method.
GSM8K (Cobbe et al., 2021). This is a highquality, linguistically diverse math dataset, whose math knowledge mainly covers grade school level. It includes 7,473 training examples and 1,319 test cases. In this work, we use its training set as the given questions to generate new synthetic data.
MATH (Hendrycks et al., 2021). This dataset focuses on competitive-level math problems that requires high levels of reasoning ability and mathematical knowledge. It consists of 7,500 training
examples and 5,000 test cases. We use the training examples to generate synthetic data.
SVAMP (Patel et al., 2021). This dataset comprises elementary-level math problems. We utilize all 1,000 of its test cases to assess the cross-dataset performance of our models.
ASDiv (Miao et al., 2021). This dataset contains a set of math problems with diverse language patterns and types of questions. We adopt the test set of 2,305 problems as evaluation benchmark.
Hungarian National High School Exam This evaluation benchmark is first introduced by Grok1 (xAI, 2023), which is designed for evaluating the out-of-domain capability of math models. It consists of 33 challenging problems.</p>
<p>It is worth noting that the final answers of Hungarian National High School Exam dataset is annotated by human, while other benchmarks are labelled using automatic scripts, similar to previous works (Luo et al., 2023; Gou et al., 2023).</p>
<h3>4.2 Implementation Details</h3>
<p>In data synthesis, we utilize the GPT-4 Turbo API, setting the temperature to 1.0 for both question and answer generation.</p>
<p>For supervised fine-tuning, we employ the Adam optimizer with a cosine learning rate schedule spanning a total of 3 epochs of training. The maximum learning rate is set $2 \mathrm{e}-5$ (except that $2 \mathrm{e}-6$ for the Mistral-7b model) and there is a $4 \%$ linear warmup. The maximum token length is set 2048, and the Vicuna-v1.1 (Zheng et al., 2023) system prompt is used. All experiments are conducted on $8 \times$ Nvidia H100 GPUs. Our most resource-intensive experiment, involving a 70B model and 960K data points, takes 1900 H100 GPU hours.</p>
<p>For evaluation, we use the same prompt as used in SFT and set the maximum sequence length to 2048. The vLLM (Kwon et al., 2023) is used in answer generation.</p>
<h3>4.3 Main Results and Comparison with State-of-the-art Models</h3>
<p>In this comparison, we examine both in-domain benchmarks, GSM8K/MATH, and out-of-domain benchmarks, such as the Hungarian National High School Exam. For in-domain evaluation of each benchmark, we utilize data synthesized from its respective training samples. For GSM8K, 960K synthetic data is employed, while for MATH, 480K synthetic data is used. For out-domain evaluation,</p>
<p>Table 2: Math reasoning performances of various LLMs.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>GSM8K</th>
<th>MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td>Closed-source models</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4 Turbo (1106)</td>
<td>94.8</td>
<td>64.5</td>
</tr>
<tr>
<td>GPT-4-0314</td>
<td>94.7</td>
<td>52.6</td>
</tr>
<tr>
<td>GPT-4 (Achiam et al., 2023)</td>
<td>92.0</td>
<td>42.5</td>
</tr>
<tr>
<td>Claude-2 (Anthropic, 2023)</td>
<td>88.0</td>
<td>-</td>
</tr>
<tr>
<td>GPT-3.5-Turbo (OpenAI, 2023a)</td>
<td>80.8</td>
<td>34.1</td>
</tr>
<tr>
<td>Open-source models LLaMA-2-7B</td>
<td></td>
<td></td>
</tr>
<tr>
<td>WizardMath-7B (Luo et al., 2023)</td>
<td>54.9</td>
<td>10.7</td>
</tr>
<tr>
<td>MuggleMath-7B (Li et al., 2023)</td>
<td>68.4</td>
<td>-</td>
</tr>
<tr>
<td>MetaMath-7B (Yu et al., 2023)</td>
<td>66.5</td>
<td>19.8</td>
</tr>
<tr>
<td>LEMA-LLaMA-2-7B (An et al., 2023)</td>
<td>54.1</td>
<td>9.4</td>
</tr>
<tr>
<td>Xwin-Math-7B (ours)</td>
<td>82.6</td>
<td>40.6</td>
</tr>
<tr>
<td>Open-source models Mistral-7B</td>
<td></td>
<td></td>
</tr>
<tr>
<td>WizardMath-7B-v1.1 (Luo et al., 2023)</td>
<td>83.2</td>
<td>33.0</td>
</tr>
<tr>
<td>MetaMath-Mistral-7B (Yu et al., 2023)</td>
<td>77.4</td>
<td>28.2</td>
</tr>
<tr>
<td>Xwin-Math-Mistral-7B (ours)</td>
<td>89.2</td>
<td>43.7</td>
</tr>
<tr>
<td>Open-source models Llemma-7B</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MetaMath-Llemma-7B (Yu et al., 2023)</td>
<td>69.2</td>
<td>30.0</td>
</tr>
<tr>
<td>Xwin-Math-Llemma-7B (ours)</td>
<td>84.2</td>
<td>47.2</td>
</tr>
<tr>
<td>Open-source models LLaMA-2-13B</td>
<td></td>
<td></td>
</tr>
<tr>
<td>WizardMath-13B (Luo et al., 2023)</td>
<td>63.9</td>
<td>14.0</td>
</tr>
<tr>
<td>MuggleMath-13B (Li et al., 2023)</td>
<td>74.0</td>
<td>-</td>
</tr>
<tr>
<td>MetaMath-13B (Yu et al., 2023)</td>
<td>72.3</td>
<td>22.4</td>
</tr>
<tr>
<td>LEMA-LLaMA-2-13B (An et al., 2023)</td>
<td>65.7</td>
<td>12.6</td>
</tr>
<tr>
<td>Xwin-Math-13B (ours)</td>
<td>88.1</td>
<td>44.9</td>
</tr>
<tr>
<td>Open-source models LLaMA-2-70B</td>
<td></td>
<td></td>
</tr>
<tr>
<td>WizardMath-70B (Luo et al., 2023)</td>
<td>81.6</td>
<td>22.7</td>
</tr>
<tr>
<td>MuggleMath-70B (Li et al., 2023)</td>
<td>82.3</td>
<td>-</td>
</tr>
<tr>
<td>MetaMath-70B (Yu et al., 2023)</td>
<td>82.3</td>
<td>26.6</td>
</tr>
<tr>
<td>LEMA-LLaMA-2-70B (An et al., 2023)</td>
<td>83.5</td>
<td>25.0</td>
</tr>
<tr>
<td>Xwin-Math-70B (ours)</td>
<td>90.6</td>
<td>52.8</td>
</tr>
</tbody>
</table>
<p>we test models trained using GSM8K, MATH, or a mixed of two synthetic sets.</p>
<p>For base models, we consider both common language models, i.e., LLaMA-2 7B/13B/70B/Mistral7B, and math-specific models, such as Llemma-7B, to assess the generality of the proposed approach.</p>
<p>In-Domain Results Table 2 presents a comparison of the proposed approach with the state-of-the-art open and closed-source models. Across all base models, our method significantly outperforms the previous best approaches that use the same pretrained base model.</p>
<p>On LLaMA-2-7B, our approach exceeds the prior best by absolutely +14.2 on GSM8K (compared to MuggleMath-7B (Li et al., 2023)), and by +20.8 on MATH (compared to MetaMath-7B (Yu et al., 2023)), respectively. It even surpasses several latest 70B models dedicated for math capabilities, such as WizardMath-70B (Luo et al., 2023) (82.6 versus 81.6 on GSM8K). On LLaMA-2-13B, the Table 3: Hungarian national high school exam test result of various LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Test Score (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4 (Achiam et al., 2023)</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: left;">Grok-1 (xAI, 2023)</td>
<td style="text-align: center;">59</td>
</tr>
<tr>
<td style="text-align: left;">Claude-2 (Anthropic, 2023)</td>
<td style="text-align: center;">55</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 Turbo (OpenAI, 2023a)</td>
<td style="text-align: center;">41</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-LLM-67B-Chat (Bi et al., 2024)</td>
<td style="text-align: center;">58</td>
</tr>
<tr>
<td style="text-align: left;">Xwin-Math-70B (480K GSM8K)</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: left;">Xwin-Math-70B (120K MATH)</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: left;">Xwin-Math-70B (480K MATH)</td>
<td style="text-align: center;">59</td>
</tr>
<tr>
<td style="text-align: left;">Xwin-Math-70B (480K Mix)</td>
<td style="text-align: center;">65</td>
</tr>
</tbody>
</table>
<p>improvements are +14.1 on GSM8K (compared to MuggleMath-13B (Li et al., 2023)) and +22.5 on MATH (compared to MetaMath-13B (Yu et al., 2023)), respectively. On LLaMA-2-70B, the gains are +7.1 on GSM8K (compared to LEMA-LLaMA-2-70B (An et al., 2023)) and +26.2 on MATH (compared to MetaMath-70B (Yu et al., 2023)), respectively.</p>
<p>On a stronger common language model, i.e., Mistral-7B, the improvements are +6.0 on GSM8K and +10.7 on MATH (compared to WizardMath-7B-v1.1 (Luo et al., 2023)), respectively.</p>
<p>On a math-specific base model, such as Llemma7B, the gains are +15.0 on GSM8K and +17.2 on MATH (compared to MetaMath-Llemma-7B (Luo et al., 2023)), respectively.</p>
<p>It is also noteworthy that our LLaMA-2-70B model achieves competitive accuracy with early versions of GPT-4 on GSM8K and MATH. To our knowledge, this is the first LLaMA-based model to outperform GPT-4-0314 on MATH.</p>
<p>These results demonstrate the significant effectiveness and broad applicability of scaling synthetic math SFT data.</p>
<p>Out-of-Domain Results We test the models trained using GSM8K, MATH, or a mixed of two synthetic sets on an out-of-domain benchmark, Hungarian National High-School Exam Test, following the practice in (xAI, 2023).</p>
<p>Table 3 shows the results. Our model trained on the mixing data (240K MATH synthetic data + 240K GSM8K synthetic data) ranked as the second, just behind the GPT-4 and much better than other models. Additionally, we plot the correlation between GSM8K and Hungarian national high-school exam scores in Appendix B. The results show that there is no significant benchmark overfitting in our model.</p>
<p>Figure. 2 (Left) presents the results of the model</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparing the increase in SFT data scale using either a single dataset or mixed datasets.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The Pass@256 and PassRatio@256 curve with increasing data size on GSM8K and MATH benchmark.</p>
<p>The mass and mass ratio (MAR) are the main parameters. The MATH is trained on GSM8K synthetic data, while Figure 2 presents the results of the model trained on MATH. We find that the accuracy of other benchmarks also improves as the amount of data increases for models trained with either GSM8K or MATH synthetic data. We also note that the generalization behaviors differ for GSM8K and MATH models: 1) SVAMP and ASDiv benefit more from GSM8K models than from MATH models. 2) While MATH models perform relatively well on the GSM8K benchmark, GSM8K models perform considerably worse on MATH benchmarks.</p>
<p>Figure 2 (Right) shows the results of models using a mixture of GSM8K and MATH in a 1:1 ratio. These models exhibit balanced scaling behaviors in both in-domain and out-of-domain benchmarks.</p>
<h3>4.4 What Happens behind Performance Improvements?</h3>
<p><strong>Pass@256 v.s. PassRatio@256</strong> To deepen the understanding behind the performance improvements, we tracked Pass@N metric and PassRatio@N metric under different data sizes. The results are shown in Figure 3. With very limited synthetic data (<em>e.g.</em>, 7.5K samples), the Xwin-Math-70B model already has very high Pass@256, indicating the strong ability to generate correct answers through multiple attempts. Meanwhile, the Pass@256 metric only changed slightly with increasing the amount of used data. In contrast, PassRatio@256, which reflects the stability to generate correct answers, increases significantly with the amount of synthetic data, and its growth trend is similar to that of Pass@1. This result confirms our hypothesis that the performance improvements are mainly caused by better stability in the answer generation rather than stronger ability to answer the question.</p>
<p><strong>Estimated Single-step Reasoning Accuracy</strong> Because of the Chain-of-Thought (CoT) are adopted in inference, the process of answer mathematical problems is completed by a multi-step reasoning process. Therefore, we hypothesize that the increase in final answer accuracy can be interpreted by the improvement in single-step reasoning accuracy. Based on this assumption, if one question can be theoretically answered by <em>s</em> reasoning steps in CoT, the final answer accuracy can be approximately as high as the number of times the single-step reasoning accuracy:</p>
<p>$$Acc_{final} = Acc_{step}^s \tag{3}$$</p>
<p>With this equation, step accuracy can be estimated from the final answer accuracy. We experimented on GSM8K. For each question in the test</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Left: The relationship between the mean accuracy on the GSM8K and the number of annotated CoT steps with data increasing. The solid line is fitted using all seven points, while the dashed line is fitted using the first four points. Right: Changes in mean accuracy when resampling is used to increase the CoT lengeh of training data.</p>
<p>set, we generated 256 responses and used the number of steps in the GSM8k test set's CoT annotations as the theoretical CoT steps. We draw the curve to show the relationship between the number of CoT reasoning steps and mean final answer accuracy and show the fitted curve based on Equation. 3. We test Xwin-Math-7B models with different synthetic data, and the results are shown in Figure 4. The solid line is fitted using all seven points and Table 4 shows the estimated single-step accuracy when using different amounts of data using all data points, and it can be seen that the single-step accuracy improve significantly with more data.</p>
<p>However, when we fit based on Equation. 3 to the first four points, as shown in dashed lines, we found that the latter three points were significantly below the curve. We believe this phenomenon may be related to the smaller proportion of more complex problems in the training data. Therefore, we resampled the 960K synthetic data according to the number of sentences in CoT solution. As can be seen from Figure 4 (right), when the proportion of complex problems is increased, the accuracy for simpler problems remains virtually unchanged, but the accuracy for more complex problems can be significantly improved. Moreover, the utilization of data resampling can increase the model's Pass-Ratio@256 from 71.1 to 72.8. This experimental result provides new insights into data selection for mathematical reasoning tasks.</p>
<p>In addition, we further used the GPT-4 Turbo to find the position where the first step in our answer was wrong and normalized that position by the total number of steps in each answer. As the estimated single-step accuracy gets higher, the first</p>
<p>Table 4: The estimated single-step reasoning accuracy and the average normalized first error position by GPT-4 Turbo in Xwin-Math-7B on GSM8K benchmark.</p>
<table>
<thead>
<tr>
<th>Data size</th>
<th>Estimated Accstep</th>
<th>Normalized first error position</th>
</tr>
</thead>
<tbody>
<tr>
<td>7.5K</td>
<td>78.9</td>
<td>67.1</td>
</tr>
<tr>
<td>120K</td>
<td>89.7</td>
<td>83.9</td>
</tr>
<tr>
<td>960K</td>
<td>94.2</td>
<td>90.9</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Changes in the proportion of calculation and reasoning mistake during data increased.</p>
<p>error position of the normalization is postponed.</p>
<p><strong>The Improvement in the Accuracy of Numerical Calculations is More Significant than Logical Reasoning</strong></p>
<p>The performance of the model gradually improves as the synthetic data increases. For a deeper understanding, we analyze the error proportion for different types of errors on GSM8K. We categorized errors into two types: reasoning errors and calculation errors. Reasoning errors primarily encompass issues such as loss of conditions and concept confusion, while calculation errors include incorrect analysis of quantitative relationships and numerical computation mistakes. Based on the experimental results illustrated in Figure 5, we observe a gradual decrease in the percentage of calculation errors, suggesting that GSM8K is correcting calculation errors at a faster rate than reasoning errors.</p>
<h3>4.5 Ablations on the Data Synthetic Schema</h3>
<p><strong>Comparison with Other Data Synthetic Methods</strong></p>
<p>We compared our approach with the following common used data synthetic methods:</p>
<p><em>Add Constraint.</em> Adding one more constraint to the original question while keeping others unchanged, which is used in WizardMath and MuggleMath.</p>
<p><em>Change Numbers.</em> Changing the numbers that appear in the problem while keeping the context intact, which is used in MuggleMath.</p>
<p><em>Change Background.</em> Changing the background in</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: GSM8K and MATH performance of different synthetic methods.</p>
<p>Table 5: Ablation of question verification on MATH.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Pass@1 (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Xwin-Math-70B (7.5K data)</td>
<td>28.9</td>
</tr>
<tr>
<td>Xwin-Math-70B (7.5K data) w/o verification</td>
<td>28.1 (-0.8)</td>
</tr>
<tr>
<td>Xwin-Math-70B (30K data)</td>
<td>37.6</td>
</tr>
<tr>
<td>Xwin-Math-70B (30K data) w/o verification</td>
<td>36.6 (-1.0)</td>
</tr>
</tbody>
</table>
<p>The question while keeping others the same.</p>
<p><em>The Combination of Changing Numbers and Background.</em> A hybrid approach that combines changing both numbers and background.</p>
<p><em>MetaMath Approach.</em> The synthetic methods proposed in MetaMath, including answer augmentation, rephrasing question, self-verification question, and FOBAR question. In experiments, we follow the implementation of MetaMath but use GPT-4 Turbo instead of GPT-3.5 Turbo to generate response data using their released questions.</p>
<p>The experimental results in Figure 6 show that when the data size is relatively small, <em>e.g.</em>, 7.5k and 30k samples, the performance gap between the different methods is negligible. However, as the data size increases, our method and the method with added constraints show stronger performance. This suggests that the choice of data synthetic strategy becomes more critical as the data size increases, and that some methods can scale the data more efficiently, thus improving the performance.</p>
<p><strong>Effects of Question Verification.</strong> The question verification is used to further improve the generation quality. In our experiments, we found it can improve the performance on MATH benchmark, the results are shown in Table 5, while we do not see significantly impact on GSM8K dataset.</p>
<h2>5 Related Works</h2>
<p><strong>Large Language Models</strong> Large language models (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023a,b) have made significant achievements, with impressive performance on a wide range of tasks. Currently, closed-source large language models, represented by GPT (Brown et al., 2020; Achiam et al., 2023), Gemini (Team et al., 2023), Grok (xAI, 2023), and Claude-2 (Anthropic, 2023), are the most advanced models in terms of performance. However, open-source models, represented by LLaMA (Touvron et al., 2023a), LLaMA-2 (Touvron et al., 2023b) and Mixtral (Jiang et al., 2024), have also progressed rapidly, and have even shown competitive performance with the closed-source models on some tasks. Our work, which aims to improve the performance of open-source LLMs on mathematical tasks by fine-tuning them on synthetic data.</p>
<p><strong>Reasoning Framework for Improving Mathematical Capability</strong> Chain-of-thoughts (Wei et al., 2022b) encourages the LLMs perform multistep reasoning by specific designed prompts and can improve reasoning performance. Based on this work, many subsequent works suggesting further improvements (Fu et al., 2022; Zhang et al., 2022; Kojima et al., 2022). The above works focus primarily on how to improve performance through better prompt design or inference strategies without fine-tuning the model, whereas our work focuses on how to improve the model itself, and thus these approaches are complementary to ours.</p>
<p><strong>Fine-tuned LLM for Math</strong> Another sort of works (Lightman et al., 2023; Luo et al., 2023; Azerbayev et al., 2023; Yue et al., 2023; Yu et al., 2023; An et al., 2023; Li et al., 2023; Gou et al., 2023) try to improve performance directly by training the model on mathematical data. A direct way is to use fine-tuning to improve models. One widely used method is to use synthetic data, which is very close to our approach: MetaMath (Yu et al., 2023) presents to bootstrap questions to augment data. LeMA (An et al., 2023) collects mistake-correction data pairs by using GPT-4 as a corrector. And MuggleMath (Li et al., 2023) augments the GSM8K dataset by incorporating GPT-4 with a series of predefined operations. Compared to these synthetic data-based efforts, our data synthetic method is much simpler and more scalable due to introduce less prior and constraint.</p>
<p><strong>SFT Data Scaling</strong> Recently, some research efforts have focused on the data scale for supervised fine-tuning. For instance, LIMA (Zhou et al., 2023) mentions that fine-tuning with 1,000 high-quality</p>
<p>instructions can yield impressive results in various general tasks. Other studies have indicated that performance scales with data size in mathematical and coding tasks (Dong et al., 2023). Recent work (Bi et al., 2024) even uses 1.5 million data for instruct fine-tuning to obtain top performance. However, the intrinsic reasons behind this scaling effect have not been thoroughly investigated.</p>
<h2>6 Conclusion</h2>
<p>This study reveals that common 7B language models, such as LLaMA-2 7B, already exhibit strong mathematical capabilities, challenging the previous belief that advanced mathematical reasoning is exclusive to larger, more extensively pre-trained models. By significantly scaling up SFT data, we have markedly improved the stability of the model's mathematical problem-solving skills. Our methodology has enabled the Xwin-Math models to reach performance levels comparable to, and in some instances surpassing, those of their larger counterparts. Our analysis also indicates that the enhancements are primarily attributable to heightened accuracy in single-step reasoning and a extra resampling of training data can improve the accuracy of harder questions. Additionally, we see more substantial reduction of calculation errors as opposed to logical reasoning errors. Our research contributes valuable insights into the mathematical capabilities of large language models.</p>
<h2>Acknowledgments</h2>
<p>Chen Li and Nanning Zheng were supported in part by NSFC under grant No. 62088102. Thank Shengnan An at IAIR, Xi'an Jiaotong University for his valuable advice on this work.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. 2023. Learning from mistakes makes llm better reasoner. arXiv preprint arXiv:2310.20689.</p>
<p>Anthropic. 2023. Model card and evaluations for claude models.</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback.</p>
<p>Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023. How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexitybased prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. 2023. Tora: A tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213.</p>
<p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models.</p>
<p>Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. 2023. Query and response augmentation cannot help out-of-domain math reasoning generalization. arXiv preprint arXiv:2310.05506.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. arXiv preprint arXiv:2305.20050.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583.</p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2021. A diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772.</p>
<p>OpenAI. 2023a. Gpt-3.5 turbo fine-tuning and api updates.</p>
<p>OpenAI. 2023b. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191.</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-ofthought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.
xAI. 2023. Grok-1.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244.</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mtbench and chatbot arena.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206.</p>
<h1>A Synthetic Prompt on GSM8K</h1>
<h2>Prompt 1: Question Generation</h2>
<p>Please act as a professional math teacher.
Your goal is to create high quality math word problems to help students learn math.
You will be given a math question. Please create a new question based on the Given Question and following instructions.
To achieve the goal, you have three jobs.
# Please generate a similar but new question according to the Given Question.
# Check the question by solving it step-by-step to find out if it adheres to all principles.
# Modify the created question according to your checking comment to ensure it is of high quality.
You have five principles to do this.
# Ensure the new question only asks for one thing, be reasonable, be based on the Given Question, and can be answered with only a number (float or integer). For example, DO NOT ask, 'what is the amount of A, B and C?'.
# Ensure the new question is in line with common sense of life. For example, the amount someone has or pays must be a positive number, and the number of people must be an integer.
# Ensure your student can answer the new question without the given question. If you want to use some numbers, conditions or background in the given question, please restate them to ensure no information is omitted in your new question.
# Please DO NOT include solution in your question.
# If the created question already follows these principles upon your verification. Just keep it without any modification.
Given Question: given question
Your output should be in the following format:
CREATED QUESTION: <your created question>
VERIFICATION AND MODIFICATION: <solve the question step-by-step and modify it to follow all principles>
FINAL CREATED QUESTION: <your final created question></p>
<h2>Prompt 2: Answer Generation</h2>
<p>Please act as a professional math teacher.
Your goal is to accurately solve a math word problem.
To achieve the goal, you have two jobs.
# Write detailed solution to a Given Question.
# Write the final answer to this question.
You have two principles to do this.
# Ensure the solution is step-by-step.
# Ensure the final answer is just a number (float or integer).
Given Question: given question
Your output should be in the following format:
SOLUTION: <your detailed solution to the given question>
FINAL ANSWER: <your final answer to the question with only an integer or float number></p>
<h2>Prompt 3: Question Generation w/o verification</h2>
<p>Please act as a professional math teacher.
Your goal is to create high quality math word problems to help students learn math.
You will be given a math question. Please create a new question based on the Given Question and following instructions.
To achieve the goal, you have one job.
# Please generate a similar but new question according to the Given Question.
You have four principles to do this.
# Ensure the new question only asks for one thing, be reasonable, be based on the Given Question, and can be answered with only a number(float or integer). For example, DO NOT ask, 'what is the amount of A, B and C?'.
# Ensure the new question is in line with common sense of life. For example, the amount someone has or pays must be a positive number, and the number of people must be an integer.
# Ensure your student can answer the new question without the given question. If you want to use some numbers, conditions or background in the given question, please restate them to ensure no information is omitted in your new question.
# You only need to create the new question. Please DO NOT solve it.
Given Question: given question
Your output should be in the following format:
CREATED QUESTION: <your created question></p>
<h1>B Additional Results</h1>
<p>Figure 7: Xwin-Math's aggregate performance on these two benchmarks is second only to GPT-4, demonstrating our model's robust generalization capabilities.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Table 6: To validate benchmark data leakage during data generation, we compare LM loss on: 1) a training subset, which is a small subset with 256 samples from all synthetic data; 2) a regenerated training subset, where we maintain the original questions from the training subset and use GPT-4 Turbo to rewrite answers; 3) a regenerated test set, where we keep the questions from the evaluation metrics unchanged and use GPT-4 Turbo to rewrite answers; 4) a reference test set, where we use the test set as seed to generate new questions and answers via GPT-4 Turbo. Referring to Skywork, we also report two key metrics: $\Delta_{1}=L_{\text {test-regen }}-L_{\text {test-ref }}, \Delta_{2}=L_{\text {test-regen }}-L_{\text {train-regen }}$, As $\Delta_{1}$ is close to 0 and $\Delta_{2}$ is significantly greater than 0 in two benchmarks, we believe that there is no leakage during the process of data synthesis.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">$L_{\text {test-regen }}$</th>
<th style="text-align: center;">$L_{\text {test-ref }}$</th>
<th style="text-align: center;">$L_{\text {train }}$</th>
<th style="text-align: center;">$L_{\text {train-regen }}$</th>
<th style="text-align: center;">$\Delta_{1}$</th>
<th style="text-align: center;">$\Delta_{2}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.19</td>
</tr>
<tr>
<td style="text-align: left;">MATH</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.20</td>
</tr>
</tbody>
</table>
<h1>C Case Study of Synthetic Data</h1>
<h2>C. 1 GSM8K</h2>
<h2>Original Question</h2>
<p>Randy has 60 mango trees on his farm. He also has 5 less than half as many coconut trees as mango trees. How many trees does Randy have in all on his farm?</p>
<h2>Synthetic Question w/o Verification</h2>
<p>Maria has 80 apple trees in her orchard. She also has 4 less than a third as many cherry trees as apple trees. How many trees does Maria have in all in her orchard?</p>
<h2>Verification Comment</h2>
<p>Let's check the created question:</p>
<ul>
<li>Maria has 80 apple trees.</li>
<li>She has 4 less than a third as many cherry trees as apple trees. A third of the apple trees is 80/3, which is approximately 26.67 .</li>
<li>Four less than that would be around 22.67, but since we cannot have a fraction of a tree, we round it to the nearest whole number, which is 27 .</li>
<li>The modified question should ask for the number of cherry trees without the decimal. So we adjust the question to have a whole number as a result.</li>
<li>Since 80 is not divisible by 3 , to get a whole number of cherry trees, we can change the number so that it is divisible by 3 . Let's use 81 apple trees instead of 80 .</li>
</ul>
<h2>Synthetic Question with Verification</h2>
<p>Maria has 81 apple trees in her orchard. She also has 4 less than a third as many cherry trees as apple trees. How many trees does Maria have in all in her orchard?</p>
<h2>Synthetic Solution and Final Answer</h2>
<p>First, let's determine the number of cherry trees Maria has. We are told she has 4 less than a third as many cherry trees as apple trees.
Since Maria has 81 apple trees, let's find one third of that number:
$1 / 3$ of 81 apple trees $=81$ apple trees $/ 3=27$ apple trees.
Now, since she has 4 less than this amount in cherry trees, we subtract 4 from 27 to find the number of cherry trees:
27 cherry trees $-4=23$ cherry trees.
To find the total number of trees Maria has in her orchard, we add the number of apple trees to the number of cherry trees:
81 apple trees +23 cherry trees $=104$ trees.
The answer is 104 .</p>
<h1>C. 2 MATH</h1>
<h2>Original Question</h2>
<p>The value of $\log _{10} 28471$ is between the consecutive integers $a$ and $b$. Find $a+b$.</p>
<h2>Synthetic Question w/o Verification</h2>
<p>The value of $\log _{2} 16384$ is between the consecutive integers $x$ and $y$. Find $x+y$.</p>
<h2>Verification Comment</h2>
<p>To ensure the question can be solved, we need to check if the logarithm given can be evaluated and falls between two consecutive integers.</p>
<ol>
<li>Identify the value of $2^{n}$ that equals 16384 .</li>
<li>If 16384 is a power of 2 , then $\log _{2} 16384$ will be an integer.</li>
<li>Check to ensure the result is an integer and find the consecutive integers around it.</li>
</ol>
<p>Let's perform the check:
$2^{0}=1,2^{1}=2,2^{2}=4,2^{3}=8,2^{4}=16$
...
$2^{14}=16384$
So, $\log <em 2="2">{2} 16384=14$.
The consecutive integers around 14 are (13 and 14) or (14 and 15).
Since the logarithm gives an exact integer, to make the question valid, we will modify it to use a number that is not an exact power of 2 .
MODIFIED CREATED QUESTION:
The value of $\log </em> 10000$ is between the consecutive integers $m$ and $n$. Find $m+n$.
Now, let's check the modified question:</p>
<ol>
<li>Since 10000 is not a power of $2, \log _{2} 10000$ will not be an integer.</li>
<li>The goal is to find two consecutive integers between which this logarithm value lies.</li>
<li>Knowing that $2^{13}=8192$ and $2^{14}=16384, \log _{2} 10000$ is between 13 and 14.</li>
</ol>
<p>The consecutive integers around $\log _{2} 10000$ are 13 and 14.</p>
<h2>Synthetic Question with Verification</h2>
<p>The value of $\log _{2} 10000$ is between the consecutive integers $m$ and $n$. Find $m+n$.</p>
<h2>Synthetic Solution and Final Answer</h2>
<p>To find the consecutive integers between which $\log <em 2="2">{2} 10000$ lies, we need to find the powers of 2 that are immediately less than and greater than 10000 .
First, let's find the largest power of 2 that is less than 10000 . Starting with $2^{1}, 2^{2}, \ldots$ and so on, we find:
$2^{10}=1024,2^{11}=2048,2^{12}=4096,2^{13}=8192,2^{14}=16384$
Here, we see that $2^{13}&lt;10000&lt;2^{14}$, which means that $\log </em> 10000$ is greater than 13 but less than 14 .
Therefore, the consecutive integers $m$ and $n$ between which $\log _{2} 10000$ lies are $m=13$ and $n=14$. Now we will add these two integers to find the required sum:
$m+n=13+14=27$.
The answer is 27 .</p>
<h1>D Case Study of Model Response</h1>
<h2>D. 1 GSM8K</h2>
<h2>Question and Ground Truth Solution</h2>
<p>Question: A treasure hunter found a buried treasure chest filled with gems. There were 175 diamonds, 35 fewer rubies than diamonds, and twice the number of emeralds than the rubies. How many of the gems were there in the chest?
Solution: There were $175-35=140$ rubies in the treasure chest. There were $140 \times 2=280$ emeralds in the treasure chest. So, the total number of gems in the treasure chest is $175+140+280$ $=595$.</p>
<h2>Response - Xwin-Math-7B (7.5k data) Greedy Decode</h2>
<p>First, let's find out the number of rubies. We are told there are 35 fewer rubies than diamonds. Since there are 175 diamonds, we can calculate the number of rubies as follows:
Number of rubies $=$ Number of diamonds $-35=175-35=140$
Next, we need to find out the number of emeralds. We are told there are twice the number of emeralds than the rubies. So we can calculate the number of emeralds as follows:
Number of emeralds $=2 \times$ Number of rubies $=2 \times 140=280$
Now, we can find the total number of gems by adding the number of diamonds, rubies, and emeralds together:
Total number of gems $=$ Number of diamonds + Number of rubies + Number of emeralds $=$ $175+140+280=605$
The answer is: 605 . Incorrect $\longrightarrow$ Wrong Calculation</p>
<h2>Response - Xwin-Math-7B (7.5k data) One Sample from Temperature $=0.7$</h2>
<p>Let's find the number of each type of gem in the chest step by step.
First, we are given the number of diamonds:
Number of diamonds $=175$
Next, we are told there are 35 fewer rubies than diamonds. To find the number of rubies, we subtract 35 from the number of diamonds:
Number of rubies $=$ Number of diamonds $-35=175-35=140$
Then, we are told there are twice the number of emeralds than the rubies. To find the number of emeralds, we multiply the number of rubies by 2 :
Number of emeralds $=2 *$ Number of rubies $=2 * 140=280$
Now, we add the number of diamonds, rubies, and emeralds together to find the total number of gems in the chest:
Total number of gems $=$ Number of diamonds + Number of rubies + Number of emeralds $=175+$ $140+280=595$
The answer is: 595 . Correct</p>
<h2>Response - Xwin-Math-7B (960k data) Greedy Decode</h2>
<p>Let's break down the information given in the problem step by step.</p>
<ol>
<li>We know there are 175 diamonds.</li>
<li>There are 35 fewer rubies than diamonds. Since there are 175 diamonds, we can calculate the number of rubies by subtracting 35 from 175 .
Number of rubies $=$ Number of diamonds $-35=175-35=140$</li>
<li>There are twice the number of emeralds than the rubies. We can find the number of emeralds by multiplying the number of rubies by 2 .
Number of emeralds $=2 \times$ Number of rubies $=2 \times 140=280$
Now, to find the total number of gems in the chest, we add the number of diamonds, rubies, and emeralds together.
Total number of gems $=$ Number of diamonds + Number of rubies + Number of emeralds $=175+$ $140+280=595$
The answer is: 595 . Correct</li>
</ol>
<h1>Question and Ground Truth Solution</h1>
<p>Question: If $f(x)=\frac{3 x-2}{x-2}$, what is the value of $f(-2)+f(-1)+f(0)$ ? Express your answer as a common fraction.
Solution: $f(-2)+f(-1)+f(0)=\frac{3(-2)-2}{-2-2}+\frac{3(-1)-2}{-1-2}+\frac{3(0)-2}{0-2}=\frac{-8}{-4}+\frac{-5}{-3}+\frac{-2}{-2}=2+\frac{5}{3}+1=\frac{14}{3}$</p>
<h2>Response - Xwin-Math-7B (7.5k data) Greedy Decode</h2>
<p>To find the value of $f(-2)+f(-1)+f(0)$, we need to evaluate the function $f(x)$ at these three values of $x$.
First, let's find $f(-2): f(x)=\frac{3 x-2}{x-2}$ Plug in $x=-2: f(-2)=\frac{3(-2)-2}{-2-2}=\frac{-6-2}{-4}=\frac{-8}{-4}$
Now, let's find $f(-1)$ : Plug in $x=-1: f(-1)=\frac{3(-1)-2}{-1-2}=\frac{-3-2}{-3}=\frac{-5}{-3}$
Finally, let's find $f(0)$ : Plug in $x=0: f(0)=\frac{3(0)-2}{0-2}=\frac{-2}{-2}=1$
Now, we can find the sum: $f(-2)+f(-1)+f(0)=\frac{-8}{-4}+\frac{-5}{-3}+1$
To add these fractions, we need to find a common denominator, which is the least common multiple (LCM) of the three denominators $(-4,-2$, and -3$)$ The LCM of these denominators is -12 .
Now, we can rewrite each fraction with the common denominator:
$\frac{-8}{-4}=\frac{4}{3} ; \frac{-5}{-3}=\frac{5}{3}, 1=\frac{12}{12}$
Now, we can add the fractions: $\frac{4}{3}+\frac{5}{3}+\frac{12}{12}=\frac{4+5+12}{3}=\frac{19}{3}$
The answer is: $\frac{19}{3}$. Incorrect Wrong calculation</p>
<h2>Response - Xwin-Math-7B (7.5k data) One Sample from Temperature=0.7</h2>
<p>To find the value of $f(-2)+f(-1)+f(0)$, we first need to evaluate each function at these specific values of x .
For $f(-2): f(x)=\frac{3 x-2}{x-2}$ Substitute $x=-2$ into the equation: $f(-2)=\frac{3(-2)-2}{-2-2}=\frac{-6-2}{-4}=\frac{-8}{-4}=$ 2
For $f(-1): f(x)=\frac{3 x-2}{x-2}$ Substitute $x=-1$ into the equation: $f(-1)=\frac{3(-1)-2}{-1-2}=\frac{-3-2}{-3}=\frac{-5}{-3}=$ $\frac{5}{3}$
For $f(0): f(x)=\frac{3 x-2}{x-2}$ Substitute $x=0$ into the equation: $f(0)=\frac{3(0)-2}{0-2}=\frac{-2}{-2}=1$
Now we can add the values of $f(-2), f(-1)$, and $f(0): f(-2)+f(-1)+f(0)=2+\frac{5}{3}+1=\frac{14}{3}$
The answer is: $\frac{14}{3}$. Correct</p>
<h2>Response - Xwin-Math-7B (480k data) Greedy Decode</h2>
<p>To find the value of $f(-2)+f(-1)+f(0)$, we need to evaluate the function $f(x)$ at each of the given points and then sum the results.
First, let's find $f(-2): f(-2)=\frac{3(-2)-2}{(-2)-2}=\frac{-6-2}{-2-2}=\frac{-8}{-4}=2$
Next, let's find $f(-1): f(-1)=\frac{3(-1)-2}{(-1)-2}=\frac{-3-2}{-1-2}=\frac{-5}{-3}=\frac{5}{3}$
Finally, let's find $f(0): f(0)=\frac{3(0)-2}{0-2}=\frac{0-2}{-2}=\frac{-2}{-2}=1$
Now, we can sum these values: $f(-2)+f(-1)+f(0)=2+\frac{5}{3}+1$ To add these, we need a common denominator, which is $3: 2+\frac{5}{3}+1=\frac{2.3}{3}+\frac{5}{3}+\frac{1.3}{3}=\frac{6}{3}+\frac{5}{3}+\frac{3}{3}$
Now, we add the numerators: $\frac{6}{3}+\frac{5}{3}+\frac{3}{3}=\frac{6+5+3}{3}=\frac{14}{3}$
The answer is: $\frac{14}{3}$. Correct</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ It is worth noting that most math models utilize a greedy generation strategy with the temperature set to 0 . However, the impact of this difference is minimal.
${ }^{4}$ Following (Lightman et al., 2023), we utilize a subset of 500 test samples from the MATH benchmark for experimental efficiency.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
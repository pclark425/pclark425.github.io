<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4422 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4422</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4422</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-275324069</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.01540v2.pdf" target="_blank">BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. Despite the significant promise of LLM-based scientific agents, no benchmarks systematically test LLM's ability to propose scientific models, collect experimental data, and revise them in light of new data. We introduce BoxingGym, a benchmark with 10 environments for systematically evaluating both experimental design (e.g. collecting data to test a scientific theory) and model discovery (e.g. proposing and revising scientific theories). To enable tractable and quantitative evaluation, we implement each environment as a generative probabilistic model with which a scientific agent can run interactive experiments. These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology. To quantitatively evaluate a scientific agent's ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of a generative model. A good scientific theory is a concise and predictive explanation. Therefore, to quantitatively evaluate model discovery, we ask a scientific agent to explain their model and then assess whether this explanation enables another scientific agent to make reliable predictions about this environment. In addition to this explanation-based evaluation, we compute standard model evaluation metrics such as prediction errors. We find that current LLMs, such as GPT-4o, struggle with both experimental design and model discovery. We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4422.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4422.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BOED / EIG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimal Experimental Design — Expected Information Gain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-theoretic BOED metric (Expected Information Gain, EIG) used to quantify how much running an experiment reduces posterior uncertainty about generative-model parameters; used as the primary metric of experimental informativeness in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Expected Information Gain (EIG) within Bayesian Optimal Experimental Design (BOED)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>EIG(d) = E_{p(y|d)}[H[p(θ)] - H[p(θ|y,d)]], i.e., the expected reduction in Shannon entropy of the parameter posterior after observing experiment outcome y under design d. In BoxingGym each environment is implemented as a generative model p(y|θ,d) so EIG can be estimated by sampling. EIG is used to score candidate experiments (designs) for informativeness and to compare agent-chosen experiments against baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Informativeness measured as posterior entropy reduction (EIG). Lower posterior uncertainty and higher EIG indicate more informative experiments; EIG is complemented by prediction performance metrics for downstream goals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, Claude-3.7-sonnet, Qwen-2.5, OpenThinker variants, Box's Apprentice</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 32B (variants used: Qwen-2.5-7B / 32B, OpenThinker-7B / 32B, GPT-4o (closed-source), Claude-3.7-sonnet (closed-source))</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (psychology, ecology, epidemiology, education, population dynamics, moral psychology, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Predictive generative/statistical models (parametric and mechanistic), including behavioral decision models, hierarchical regression, Lotka-Volterra dynamics, IRT</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>EIG used to compute informativeness of experiments and to derive EIG regret. Empirically, models that chose experiments with higher EIG tended to achieve better downstream prediction in several environments; GPT-4o achieved the lowest EIG regret and strong predictive performance. The paper reports that larger LLMs (32B) generally select more informative experiments (higher EIG) than 7B variants. Numerical EIG values are environment-specific and estimated by nested Monte Carlo (see another entry).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: EIG is computed analytically/estimated from the benchmark's generative models (no human raters).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Justified by BOED literature; consistency of the chosen nested Monte Carlo estimator for EIG is cited (reference to nested Monte Carlo work). Empirical validation: correlation shown between lower EIG regret and improved downstream prediction for some agents (e.g., GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>EIG assumes the simulator p(y|θ,d) is a correct model of the true data-generating process; in real-world settings model misspecification can invalidate EIG. EIG is typically intractable in closed form, requiring costly approximations. The estimator's variance and computational cost limit practical search over designs. The paper notes that methods approximating EIG often assume a fixed generative model and struggle when agents revise models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>BoxingGym: ten pymc-based generative environments implementing domain-specific generative models (Location finding, Hyperbolic discounting, Death process, IRT, Dugongs, Peregrines, Mastectomy survival, Predator-Prey, Emotions from outcomes, Moral Machines).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4422.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4422.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nested Monte Carlo EIG Estimator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nested Monte Carlo estimator for Expected Information Gain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A consistent nested Monte Carlo estimator used to approximate EIG by drawing outer samples of θ and inner likelihood averages; chosen for simplicity and consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Nested Monte Carlo (NMC) estimator of EIG</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Estimator (µ_NMC) computes EIG by drawing N outer samples (θ_n^0) and simulating y_n ∼ p(y|θ_n^0,d), then approximating log p(y_n|θ_n^0,d) - log( (1/M) Σ_{m=1}^M p(y_n|θ_{n,m},d) ) with inner samples θ_{n,m} ∼ p(θ). This nested sampling yields a consistent estimate of EIG but can be computationally expensive and high variance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Consistency of estimator to true EIG in the limit; practical trade-offs between N and M determine bias/variance and compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Used across all BoxingGym domains (cross-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Applies to parameter inference for generative statistical/mechanistic models</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The NMC estimator is used to compute ground-truth-like EIGs and to compare agent-chosen designs to random baselines (used inside EI Regret). Paper cites that this estimator is consistent (reference to Rainforth et al. / nested MC literature) and straightforward to implement; no absolute numeric convergence diagnostics reported beyond empirical application.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated numeric estimation using Monte Carlo sampling from the benchmark generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>The paper cites established theory that the NMC estimator is a consistent estimator of EIG and references prior work on nesting Monte Carlo; empirical use in the benchmark acts as practical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>High computational cost and estimator variance; requires many inner/outer samples for low-variance EIG estimates, which constrains search over large design spaces. Nested MC assumes ability to sample from p(θ) and evaluate p(y|θ,d) exactly as implemented in the pymc simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to BoxingGym's 10 generative environments implemented in pymc.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4422.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4422.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EIG Regret</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Information Gain Regret</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A regret metric comparing the EIG of the agent-chosen experiment to the maximum EIG achievable by random search (100 random experiments), used to quantify how suboptimal an agent's experimental design choices are.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Expected Information (EIG) Regret</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>EI Regret = (EIG_max_from_random_candidates - EIG_agent_choice). The baseline maximum is computed by sampling 100 random designs and taking the best EIG among them. Lower EI Regret means the agent chose a design close to the best observed EIG; used to evaluate experimental-design quality across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relative informativeness of chosen designs compared to a random-candidate baseline; used as a trajectory-level measure over multiple experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o and other LLM agents evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 32B variants (see experiments: Qwen 2.5, OpenThinker, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (applied per BoxingGym environment)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation of experiments for parameter-identifying predictive models</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4o achieved the lowest EI Regret overall, indicating more informative experiment choice; other models varied — some models (Qwen-32B) had good prediction but higher EI Regret (i.e., good prediction does not always imply optimal design). Scale improved EI Regret (32B < 7B). Numerical EI Regret values are environment- and model-specific and plotted in paper's figures.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (computed from EIG estimates produced by nested MC on the benchmark simulators).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical correlation shown between lower EI Regret and improved downstream standardized prediction error in several environments (evidence that EI Regret is meaningful for evaluating experimental design).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Using a max-over-100-random baseline is only an approximate upper bound—true global optimum may be missed; EIG estimation noise propagates into regret; random-candidate baseline may be weak in structured high-dimensional design spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4422.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4422.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Communication-based Discovery Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Communication-based evaluation of model discovery via explanation-to-novice</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel evaluation framework that measures model-discovery quality by how well a scientist agent's natural-language explanation enables a novice agent (without experiment access) to make accurate predictions; combines communicative and predictive criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Communication-based model-discovery evaluation (explanation → novice prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>After performing experiments, the scientist agent distills findings into a token-limited natural-language explanation. A separate novice agent (always GPT-4o in experiments) receives only the explanation (no experiment data) and must make predictions about held-out queries. The novice's standardized prediction error (relative to prior predictive mean) measures how much the explanation transfers actionable, predictive knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Predictive accuracy of novice using explanation (standardized error), conciseness/parsimony (token limit on explanation), and whether explanation is predictive and parsimonious. Also compare novice error to scientist's own error (scientist as positive control).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Scientist agents: GPT-4o, Claude-3.7-sonnet, Qwen variants, OpenThinker variants, Box's Apprentice; Novice agent: GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 32B variants for scientist agents; novice GPT-4o (closed-source) size unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain across BoxingGym environments (behavioral models, ecological, epidemiological, IRT, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Explanatory verbalizations of statistical/mechanistic models and distilled statistical models (natural-language explanations of predictive models)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Explanations produced by larger scientist models (32B) enabled better novice performance than explanations from 7B models; however, novices generally performed worse than the original scientist (who had direct experiments). Explanations were more helpful in simpler domains (e.g., animal growth) and less helpful in complex domains (moral judgments). Box's Apprentice's fitted pymc models did not reliably improve explanations for the novice.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated/hybrid: evaluation is automated because the novice is an LLM (GPT-4o) that produces quantitative predictions which are scored; the explanation itself is natural language but scored via automated prediction loss. No human raters were used.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Positive control: scientist's own prediction error after experiments is used as a weak benchmark; improvement in novice prediction relative to prior-predictive baseline indicates explanatory power. The authors also examine scaling trends (model size) and domain differences to validate sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on an LLM novice (GPT-4o) rather than human novices, so success measures transfer to model-to-model communication rather than human interpretability. Token-limit introduces a parsimony proxy but is arbitrary. Natural-language explanations may omit technical model details; explanations that help LLM novices may not align with human-understandable explanations. Variation across domains suggests explanations may capture some but not all inferential content gained through experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4422.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4422.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standardized Prediction Error</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standardized prediction error relative to prior predictive</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-agnostic prediction-score that standardizes agent prediction error by subtracting the error of the prior-predictive mean and dividing by the prior-predictive standard deviation; used to evaluate goal achievement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Standardized prediction error (relative to prior predictive mean/variance)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For ground-truth outputs y_i and agent predictions ŷ_i, compute per-example discrepancy ϵ_i = f(ŷ_i,y_i) (e.g., MSE) and baseline discrepancy ϵ_{µ0} = f(µ0,y_i) where µ0 and σ0 are prior-predictive mean and standard deviation estimated by sampling designs uniformly and θ from prior. The standardized error is (ϵ_i - ϵ_{µ0}) / σ0 (can be negative). This measures improvement relative to an uninformed prior predictive baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Prediction accuracy normalized by prior predictive uncertainty; captures how much agent learning (and explanation) improves over prior expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated for scientist and novice agents; models include GPT-4o, Qwen, OpenThinker, Claude, Box's Apprentice</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 32B variants where applicable</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>All BoxingGym domains (prediction targets differ by environment: e.g., predator population, student responses, survival probabilities, choices)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Predictive models (statistical/mechanistic) being evaluated by predicted numeric or categorical outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as the principal numeric outcome reported in tables/figures; larger models and more experiments generally reduce standardized error. Some environments show negative standardized error (better than prior predictive baseline), demonstrating successful information gain; others remain challenging (hyperbolic discounting). The paper reports averaged standardized errors across runs and steps (see Table 1 and figures).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated numeric metric computed from model predictions and simulator-generated ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Baseline defined by prior predictive mean/variance (obtained by sampling design uniformly and θ from prior); the metric's ability to be negative allows showing improvements over prior baseline. Evaluated across multiple seeds/runs with reported standard errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depends on prior predictive baseline selection (uniform over design space); prior choice and sampling can influence normalization. Metric conflates errors from multiple sources (experiment selection, model inference, and explanation quality) and may hide different error modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bayesian Experimental Design: A Review <em>(Rating: 2)</em></li>
                <li>Variational bayesian optimal experimental design <em>(Rating: 2)</em></li>
                <li>On Nesting Monte Carlo Estimators <em>(Rating: 2)</em></li>
                <li>Automated Statistical Model Discovery with Language Models <em>(Rating: 2)</em></li>
                <li>A Useful Method for Model-Building <em>(Rating: 1)</em></li>
                <li>Deep adaptive design: Amortizing sequential bayesian experimental design <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4422",
    "paper_id": "paper-275324069",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "BOED / EIG",
            "name_full": "Bayesian Optimal Experimental Design — Expected Information Gain",
            "brief_description": "An information-theoretic BOED metric (Expected Information Gain, EIG) used to quantify how much running an experiment reduces posterior uncertainty about generative-model parameters; used as the primary metric of experimental informativeness in the benchmark.",
            "citation_title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
            "mention_or_use": "use",
            "evaluation_method_name": "Expected Information Gain (EIG) within Bayesian Optimal Experimental Design (BOED)",
            "evaluation_method_description": "EIG(d) = E_{p(y|d)}[H[p(θ)] - H[p(θ|y,d)]], i.e., the expected reduction in Shannon entropy of the parameter posterior after observing experiment outcome y under design d. In BoxingGym each environment is implemented as a generative model p(y|θ,d) so EIG can be estimated by sampling. EIG is used to score candidate experiments (designs) for informativeness and to compare agent-chosen experiments against baselines.",
            "evaluation_criteria": "Informativeness measured as posterior entropy reduction (EIG). Lower posterior uncertainty and higher EIG indicate more informative experiments; EIG is complemented by prediction performance metrics for downstream goals.",
            "model_name": "GPT-4o, Claude-3.7-sonnet, Qwen-2.5, OpenThinker variants, Box's Apprentice",
            "model_size": "7B, 32B (variants used: Qwen-2.5-7B / 32B, OpenThinker-7B / 32B, GPT-4o (closed-source), Claude-3.7-sonnet (closed-source))",
            "scientific_domain": "Cross-domain (psychology, ecology, epidemiology, education, population dynamics, moral psychology, etc.)",
            "theory_type": "Predictive generative/statistical models (parametric and mechanistic), including behavioral decision models, hierarchical regression, Lotka-Volterra dynamics, IRT",
            "human_comparison": false,
            "evaluation_results": "EIG used to compute informativeness of experiments and to derive EIG regret. Empirically, models that chose experiments with higher EIG tended to achieve better downstream prediction in several environments; GPT-4o achieved the lowest EIG regret and strong predictive performance. The paper reports that larger LLMs (32B) generally select more informative experiments (higher EIG) than 7B variants. Numerical EIG values are environment-specific and estimated by nested Monte Carlo (see another entry).",
            "automated_vs_human_evaluation": "Automated: EIG is computed analytically/estimated from the benchmark's generative models (no human raters).",
            "validation_method": "Justified by BOED literature; consistency of the chosen nested Monte Carlo estimator for EIG is cited (reference to nested Monte Carlo work). Empirical validation: correlation shown between lower EIG regret and improved downstream prediction for some agents (e.g., GPT-4o).",
            "limitations_challenges": "EIG assumes the simulator p(y|θ,d) is a correct model of the true data-generating process; in real-world settings model misspecification can invalidate EIG. EIG is typically intractable in closed form, requiring costly approximations. The estimator's variance and computational cost limit practical search over designs. The paper notes that methods approximating EIG often assume a fixed generative model and struggle when agents revise models.",
            "benchmark_dataset": "BoxingGym: ten pymc-based generative environments implementing domain-specific generative models (Location finding, Hyperbolic discounting, Death process, IRT, Dugongs, Peregrines, Mastectomy survival, Predator-Prey, Emotions from outcomes, Moral Machines).",
            "uuid": "e4422.0",
            "source_info": {
                "paper_title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Nested Monte Carlo EIG Estimator",
            "name_full": "Nested Monte Carlo estimator for Expected Information Gain",
            "brief_description": "A consistent nested Monte Carlo estimator used to approximate EIG by drawing outer samples of θ and inner likelihood averages; chosen for simplicity and consistency.",
            "citation_title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
            "mention_or_use": "use",
            "evaluation_method_name": "Nested Monte Carlo (NMC) estimator of EIG",
            "evaluation_method_description": "Estimator (µ_NMC) computes EIG by drawing N outer samples (θ_n^0) and simulating y_n ∼ p(y|θ_n^0,d), then approximating log p(y_n|θ_n^0,d) - log( (1/M) Σ_{m=1}^M p(y_n|θ_{n,m},d) ) with inner samples θ_{n,m} ∼ p(θ). This nested sampling yields a consistent estimate of EIG but can be computationally expensive and high variance.",
            "evaluation_criteria": "Consistency of estimator to true EIG in the limit; practical trade-offs between N and M determine bias/variance and compute cost.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Used across all BoxingGym domains (cross-domain)",
            "theory_type": "Applies to parameter inference for generative statistical/mechanistic models",
            "human_comparison": null,
            "evaluation_results": "The NMC estimator is used to compute ground-truth-like EIGs and to compare agent-chosen designs to random baselines (used inside EI Regret). Paper cites that this estimator is consistent (reference to Rainforth et al. / nested MC literature) and straightforward to implement; no absolute numeric convergence diagnostics reported beyond empirical application.",
            "automated_vs_human_evaluation": "Automated numeric estimation using Monte Carlo sampling from the benchmark generative models.",
            "validation_method": "The paper cites established theory that the NMC estimator is a consistent estimator of EIG and references prior work on nesting Monte Carlo; empirical use in the benchmark acts as practical validation.",
            "limitations_challenges": "High computational cost and estimator variance; requires many inner/outer samples for low-variance EIG estimates, which constrains search over large design spaces. Nested MC assumes ability to sample from p(θ) and evaluate p(y|θ,d) exactly as implemented in the pymc simulators.",
            "benchmark_dataset": "Applied to BoxingGym's 10 generative environments implemented in pymc.",
            "uuid": "e4422.1",
            "source_info": {
                "paper_title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "EIG Regret",
            "name_full": "Expected Information Gain Regret",
            "brief_description": "A regret metric comparing the EIG of the agent-chosen experiment to the maximum EIG achievable by random search (100 random experiments), used to quantify how suboptimal an agent's experimental design choices are.",
            "citation_title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
            "mention_or_use": "use",
            "evaluation_method_name": "Expected Information (EIG) Regret",
            "evaluation_method_description": "EI Regret = (EIG_max_from_random_candidates - EIG_agent_choice). The baseline maximum is computed by sampling 100 random designs and taking the best EIG among them. Lower EI Regret means the agent chose a design close to the best observed EIG; used to evaluate experimental-design quality across steps.",
            "evaluation_criteria": "Relative informativeness of chosen designs compared to a random-candidate baseline; used as a trajectory-level measure over multiple experiments.",
            "model_name": "GPT-4o and other LLM agents evaluated",
            "model_size": "7B and 32B variants (see experiments: Qwen 2.5, OpenThinker, etc.)",
            "scientific_domain": "Cross-domain (applied per BoxingGym environment)",
            "theory_type": "Evaluation of experiments for parameter-identifying predictive models",
            "human_comparison": false,
            "evaluation_results": "GPT-4o achieved the lowest EI Regret overall, indicating more informative experiment choice; other models varied — some models (Qwen-32B) had good prediction but higher EI Regret (i.e., good prediction does not always imply optimal design). Scale improved EI Regret (32B &lt; 7B). Numerical EI Regret values are environment- and model-specific and plotted in paper's figures.",
            "automated_vs_human_evaluation": "Automated (computed from EIG estimates produced by nested MC on the benchmark simulators).",
            "validation_method": "Empirical correlation shown between lower EI Regret and improved downstream standardized prediction error in several environments (evidence that EI Regret is meaningful for evaluating experimental design).",
            "limitations_challenges": "Using a max-over-100-random baseline is only an approximate upper bound—true global optimum may be missed; EIG estimation noise propagates into regret; random-candidate baseline may be weak in structured high-dimensional design spaces.",
            "uuid": "e4422.2",
            "source_info": {
                "paper_title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Communication-based Discovery Eval",
            "name_full": "Communication-based evaluation of model discovery via explanation-to-novice",
            "brief_description": "A novel evaluation framework that measures model-discovery quality by how well a scientist agent's natural-language explanation enables a novice agent (without experiment access) to make accurate predictions; combines communicative and predictive criteria.",
            "citation_title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
            "mention_or_use": "use",
            "evaluation_method_name": "Communication-based model-discovery evaluation (explanation → novice prediction)",
            "evaluation_method_description": "After performing experiments, the scientist agent distills findings into a token-limited natural-language explanation. A separate novice agent (always GPT-4o in experiments) receives only the explanation (no experiment data) and must make predictions about held-out queries. The novice's standardized prediction error (relative to prior predictive mean) measures how much the explanation transfers actionable, predictive knowledge.",
            "evaluation_criteria": "Predictive accuracy of novice using explanation (standardized error), conciseness/parsimony (token limit on explanation), and whether explanation is predictive and parsimonious. Also compare novice error to scientist's own error (scientist as positive control).",
            "model_name": "Scientist agents: GPT-4o, Claude-3.7-sonnet, Qwen variants, OpenThinker variants, Box's Apprentice; Novice agent: GPT-4o",
            "model_size": "7B and 32B variants for scientist agents; novice GPT-4o (closed-source) size unspecified",
            "scientific_domain": "Cross-domain across BoxingGym environments (behavioral models, ecological, epidemiological, IRT, etc.)",
            "theory_type": "Explanatory verbalizations of statistical/mechanistic models and distilled statistical models (natural-language explanations of predictive models)",
            "human_comparison": false,
            "evaluation_results": "Explanations produced by larger scientist models (32B) enabled better novice performance than explanations from 7B models; however, novices generally performed worse than the original scientist (who had direct experiments). Explanations were more helpful in simpler domains (e.g., animal growth) and less helpful in complex domains (moral judgments). Box's Apprentice's fitted pymc models did not reliably improve explanations for the novice.",
            "automated_vs_human_evaluation": "Automated/hybrid: evaluation is automated because the novice is an LLM (GPT-4o) that produces quantitative predictions which are scored; the explanation itself is natural language but scored via automated prediction loss. No human raters were used.",
            "validation_method": "Positive control: scientist's own prediction error after experiments is used as a weak benchmark; improvement in novice prediction relative to prior-predictive baseline indicates explanatory power. The authors also examine scaling trends (model size) and domain differences to validate sensitivity.",
            "limitations_challenges": "Relies on an LLM novice (GPT-4o) rather than human novices, so success measures transfer to model-to-model communication rather than human interpretability. Token-limit introduces a parsimony proxy but is arbitrary. Natural-language explanations may omit technical model details; explanations that help LLM novices may not align with human-understandable explanations. Variation across domains suggests explanations may capture some but not all inferential content gained through experimentation.",
            "uuid": "e4422.3",
            "source_info": {
                "paper_title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Standardized Prediction Error",
            "name_full": "Standardized prediction error relative to prior predictive",
            "brief_description": "A domain-agnostic prediction-score that standardizes agent prediction error by subtracting the error of the prior-predictive mean and dividing by the prior-predictive standard deviation; used to evaluate goal achievement.",
            "citation_title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
            "mention_or_use": "use",
            "evaluation_method_name": "Standardized prediction error (relative to prior predictive mean/variance)",
            "evaluation_method_description": "For ground-truth outputs y_i and agent predictions ŷ_i, compute per-example discrepancy ϵ_i = f(ŷ_i,y_i) (e.g., MSE) and baseline discrepancy ϵ_{µ0} = f(µ0,y_i) where µ0 and σ0 are prior-predictive mean and standard deviation estimated by sampling designs uniformly and θ from prior. The standardized error is (ϵ_i - ϵ_{µ0}) / σ0 (can be negative). This measures improvement relative to an uninformed prior predictive baseline.",
            "evaluation_criteria": "Prediction accuracy normalized by prior predictive uncertainty; captures how much agent learning (and explanation) improves over prior expectations.",
            "model_name": "Evaluated for scientist and novice agents; models include GPT-4o, Qwen, OpenThinker, Claude, Box's Apprentice",
            "model_size": "7B and 32B variants where applicable",
            "scientific_domain": "All BoxingGym domains (prediction targets differ by environment: e.g., predator population, student responses, survival probabilities, choices)",
            "theory_type": "Predictive models (statistical/mechanistic) being evaluated by predicted numeric or categorical outcomes",
            "human_comparison": false,
            "evaluation_results": "Used as the principal numeric outcome reported in tables/figures; larger models and more experiments generally reduce standardized error. Some environments show negative standardized error (better than prior predictive baseline), demonstrating successful information gain; others remain challenging (hyperbolic discounting). The paper reports averaged standardized errors across runs and steps (see Table 1 and figures).",
            "automated_vs_human_evaluation": "Automated numeric metric computed from model predictions and simulator-generated ground truth.",
            "validation_method": "Baseline defined by prior predictive mean/variance (obtained by sampling design uniformly and θ from prior); the metric's ability to be negative allows showing improvements over prior baseline. Evaluated across multiple seeds/runs with reported standard errors.",
            "limitations_challenges": "Depends on prior predictive baseline selection (uniform over design space); prior choice and sampling can influence normalization. Metric conflates errors from multiple sources (experiment selection, model inference, and explanation quality) and may hide different error modes.",
            "uuid": "e4422.4",
            "source_info": {
                "paper_title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bayesian Experimental Design: A Review",
            "rating": 2,
            "sanitized_title": "bayesian_experimental_design_a_review"
        },
        {
            "paper_title": "Variational bayesian optimal experimental design",
            "rating": 2,
            "sanitized_title": "variational_bayesian_optimal_experimental_design"
        },
        {
            "paper_title": "On Nesting Monte Carlo Estimators",
            "rating": 2,
            "sanitized_title": "on_nesting_monte_carlo_estimators"
        },
        {
            "paper_title": "Automated Statistical Model Discovery with Language Models",
            "rating": 2,
            "sanitized_title": "automated_statistical_model_discovery_with_language_models"
        },
        {
            "paper_title": "A Useful Method for Model-Building",
            "rating": 1,
            "sanitized_title": "a_useful_method_for_modelbuilding"
        },
        {
            "paper_title": "Deep adaptive design: Amortizing sequential bayesian experimental design",
            "rating": 1,
            "sanitized_title": "deep_adaptive_design_amortizing_sequential_bayesian_experimental_design"
        }
    ],
    "cost": 0.015020499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery
14 Oct 2025</p>
<p>Kanishk Gandhi kanishk.gandhi@stanford.edu 
Equal Contribution</p>
<p>Michael Y Li 
Equal Contribution</p>
<p>Lyle Goodyear 
Agam Bhatia 
Louise Li 
Aditi Bhaskar 
Mohammed Zaman 
Noah D Goodman </p>
<p>Stanford University</p>
<p>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery
14 Oct 2025052F6DD5B075B454BB6566C50BE9CDEDarXiv:2501.01540v2[cs.LG]
Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research.Proposing theories, designing experiments to test them, and then revising them based on data are key to scientific discovery.Despite the promise of LLM-based scientific agents, no benchmarks systematically test their ability to propose scientific models, collect experimental data, and revise them in light of new data.We introduce BoxingGym , a benchmark with 10 environments for evaluating experimental design (e.g., collecting data to test a scientific theory) and model discovery (e.g., proposing and revising scientific theories).To enable quantitative and principled evaluation, we implement each environment as a generative probabilistic model with which a scientific agent can run interactive experiments.These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology.To evaluate a scientific agent's ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of a generative model.A good scientific theory is a concise and predictive explanation.To quantitatively evaluate model discovery, we ask a scientific agent to explain their model and evaluate whether this explanation helps another scientific agent make more accurate predictions.We evaluate several open and closed-source language models of varying sizes.We find that larger models (32B) consistently outperform smaller variants (7B), and that closed-source models generally achieve better results than open-source alternatives.However, all current approaches struggle with both experimental design and model discovery, highlighting these as promising directions for future research. 2"To understand a system, you must perturb it."-George Box (ad sensum)</p>
<p>Introduction</p>
<p>Helping humans understand the world (and themselves) by discovering scientific theories is a foundational goal of artificial intelligence research [31].Proposing theories about the world, conducting experiments to test them, and revising them based on data is central to this process [9].Recent advances in large language models (LLMs), have shown promising potential for accelerating scientific discovery.LLMs have extensive scientific knowledge [2], strong inductive reasoning capabilities [53,43], and the ability to propose models of data [27,28,11].These promising results suggest that LLMs, functioning as autonomous agents, could be well-suited for experimental design (i.e., col- The BoxingGym Framework is designed to holistically evaluate experimental design and model discovery capabilities in the spirit of George Box [9]. 1) The process starts with a user defining a goal for the scientist agent.</p>
<p>2) The scientist formulates a theory.3) This theory guides the experimental design, where the scientist interacts with a simulated world to gather new data.4) The scientist then analyzes the new and old data to propose and refine theories.This iterative process continues for several iterations.5) The scientist is then asked to explain the findings to a novice.6) We evaluate the novice and the scientist by casting the goal as a prediction problem.</p>
<p>lecting informative experiments to test scientific theories) and model discovery (i.e., developing interpretable models based on experimental data).</p>
<p>Previous work has evaluated automated experimental design and model discovery in isolation [16,17,15,27].However, they are fundamentally coupled in real-world settings: scientists collect experimental data to build better models and better models inform better experiments.While scientific agents are promising, there is currently no systematic way to evaluate an agent's ability to propose scientific models, collect experimental data, and revise them in light of new data.This motivates the need for a benchmark that evaluates an agent's capabilities holistically in an integrated scientific discovery pipeline.</p>
<p>We outline the key desiderata for a framework that evaluates experimental design and model discovery:</p>
<p>(1) The framework should enable the agent to actively experiment with the environment without requiring the agent to perform time-consuming and resource-intensive real-world lab experiments.</p>
<p>(2) Since scientific theories come in different forms, the framework should flexibly accommodate different representations of scientific theories.</p>
<p>(3) The framework should evaluate experimental design and model discovery in an integrated way.(4) Science is often goal-directed or driven by an inquiry.For example, a biologist might perform experiments with the goal of identifying cellular mechanisms underlying circadian rhythm in mammals.Our framework should allow users to specify high-level goals to guide the agent's discovery process.Our desiderata are inspired by the framework for scientific modeling introduced by George Box [7,8], which emphasizes an iterative process of building models, designing experiments to test them, and revising them accordingly.</p>
<p>To achieve these desiderata, we introduce BoxingGym (Fig. 1) a flexible framework for evaluating experimental design and model discovery with autonomous agents.Our benchmark consists of 10 environments grounded in real-world scientific models.To enable agents to actively experiment, we implement each environment as a generative model.This key design choice makes simulating active experimentation tractable because it corresponds to sampling from the underlying generative model, conditioned on the experimental interventions.To accommodate various representations of scientific theories, all environments are designed with a flexible language based interface (Fig. 2).Finally, our environments can be instantiated with different goals, or intents for inquiry, that encourage the agent to adapt their experimentation towards accomplishing the goal (e.g., understand the parameters underlying participant behavior in a psychology study) by specifying the goal in language.(center) Pseudocode illustrating the workflow of setting goals, performing experiments, predicting outcomes, and providing explanations.(right) An example, hyperbolic temporal discounting, where the agent predicts a participant's choice between immediate and delayed rewards and explains the concept to a novice.We introduce principled evaluation metrics that measure the quality of experiments and discovered models.To evaluate experimental design, we draw from Bayesian optimal experimental (BOED) design [44] and use expected information gain (EIG) to measure the informativeness of an experiment.EIG captures how much an experiment reduces uncertainty in the parameters of a generative model and, importantly, this measure complements our decision to implement environments as generative models.To evaluate model discovery, we take inspiration from the fact that science is a communicative endeavor.We propose a communication-based evaluation strategy: we ask a scientist agent to distill their experiments into a natural language explanation and evaluate how much that explanation empowers a novice agent, who does not have access to the experiments conducted by the scientist, to make accurate predictions about the environment.</p>
<p>We evaluate several open and closed-source language models ranging from 7B to 32B parameters.We find that larger models consistently outperform smaller variants, and closed-source models generally achieve better results than open-source alternatives.We also evaluate Box's Apprentice [27], which augments language models with statistical modeling capabilities, but find that this augmentation does not reliably improve performance.Notably, we observe substantial variation in difficulty across environments, which remaining challenging even for the strongest models.Promisingly, some environments show clear performance improvements with model scale.These results highlight significant opportunities for improving automated scientific reasoning.</p>
<p>Related Works</p>
<p>Optimal Experimental Design.Bayesian optimal experimental design (BOED) is a principled framework for designing maximally informative experiments across various disciplines [49,12,35].While theoretically appealing, BOED's practical implementation is challenging due to the intractability of information gain metrics like expected information gain (EIG).Although several methods [44,16,17] exist to approximate EIG, they assume the data follows a fixed generative model-limiting their utility when model revision is needed as new data is collected.Automated Model Discovery.Automated model discovery from data has been a long-standing goal in AI, aiming to build interpretable models that capture underlying patterns in data-from physical laws [6,32] to nonparametric regression [15].Recent work [27,28] has integrated language models into this process, leveraging their ability to both propose and critique candidate models, demonstrating their potential as tools for automated model discovery.This work highlights the potential of using language models as a powerful tool for model discovery.</p>
<p>Reasoning and Exploration with LLMs.Language models have shown promising capabilities in both deductive reasoning (deriving consequences from hypotheses) [47,46,42] and inductive reasoning (inferring hypotheses from observations) [53,43].While reinforcement learning has improved LLMs' reasoning abilities [24,21,20,22], these advances have primarily focused on deterministic, verfiable systems rather than the stochastic data typical in scientific discovery.Efficient exploration and information-seeking are crucial for experimental design and model building.Recent work [37,33,19,18,48,26] has investigated in-context exploration strategies and shown how language models can learn how to search and explore directly through sequence modeling, developing effective search strategies in language.</p>
<p>Interactive Environments.Drawing inspiration from established reinforcement learning principles [10,34], BoxingGym adopts the modularity and simplicity of classic environments like OpenAI Gym while shifting focus to evaluation rather than agent training.While recent work has expanded interactive benchmarks to language agents -spanning tasks from software debugging [25] to automated scientific research [36,29], our work advances this direction by introducing a principled framework for evaluating language agents' capabilities in iterative experimental design and model discovery.</p>
<p>3 Boxing Gym 3.1 Problem Formulation.</p>
<p>We formalize experimental design and model discovery using probabilistic modeling and Bayesian optimal experimental design (BOED).In BoxingGym , each environment is implemented as a generative model defining a joint distribution over the experimental outcome y, experimental design d, and unobserved parameters θ.This joint distribution is defined in terms of a prior distribution over θ, p(θ) and a simulator p(y|θ, d) which is a model of the experimental outcome y given parameters θ and design d.For example, in a psychology experiment, θ could be the parameters of a behavioral model of participants, d could be the questions posed to participants, and y could be the participant's response to d.Running an experiment corresponds to choosing a design d and observing a sample y from the marginal predictive distribution conditioned on that design, i.e., y ∼ p(y|d
) = E p(θ) [p(y|θ, d)]) 3 .</p>
<p>Evaluation</p>
<p>Evaluating experimental design via Expected Information Gain</p>
<p>To evaluate experimental design, we take inspiration from the Bayesian OED literature [16,17].Crucially, our choice to implement environments as generative models enables us to leverage this literature.For each domain, we have an underlying predictive model p(y|θ, d).We quantify the informativeness of a design d through the expected information gain (EIG), that measures the reduction in posterior uncertainty about the model parameters θ after running an experiment d.Below, H is the Shannon entropy.
EIG(d) = E p(y|d) [H[p(θ)] − H[p(θ|y, d)]]
Since the EIG is typically not available in closed-form, we use a Nested Monte Carlo estimator
μNMC (d) = 1 N N n=1 log p(y n |θ n,0 , d) 1 M M m=1 p(y n |θ n,m , d)
where θ n,m i.i.d.</p>
<p>∼ p(θ), y n ∼ p(y|θ = θ n,0 , d)</p>
<p>We chose this estimator because it is a consistent estimator of the true EIG [44] and is straightforward to implement.EIG measures the value of an experiment under the assumption that the true distribution of experimental outcomes is modeled by p(y|d).In general, this assumption is not true, but EIG is still a useful measure since we generate data from an underlying model in our benchmarks.</p>
<p>Evaluating model discovery via communication</p>
<p>To evaluate the quality of a model, we use standard model evaluation metrics (e.g., prediction MSE) and a communication-based metric that takes advantage of the natural language interface.In particular, a scientist agent interacts with an environment through experiments.After these experiments, we ask the scientist agent to synthesize their findings through an explanation.We then evaluate how much that explanation enables a novice agent to make more accurate predictions about the environment without any additional experiments.Since a good explanation is both predictive and parsimonious, we set a token limit on the explanation.Crucially, this evaluation method can accommodate different forms of scientific theories.In our experiments, we ask the scientist agent to produce a statistical model and then distill the model into a natural language explanation to guide the novice agent.</p>
<p>Evaluating goals via prediction</p>
<p>To evaluate success at achieving a specific goal (e.g., how do the populations of predator and prey change with time) we employ a prediction target (e.g., predict the population of predators at a particular time) and calculate a standardized prediction error.First, we compute the error between the predicted and true values.Then, we standardize this error with respect to the prior predictive mean, which is obtained by assuming a uniform prior over the design space.Specifically, for each domain, we sample a design d uniformly from the design space and a parameter θ from the prior distribution p(θ).We then generate samples from the predictive model p(y|θ, d) and average over multiple d and θ to obtain the prior predictive mean µ 0 and variance σ 0 .Let {y i } n i=1 be the ground truth outputs for inputs {x i } n i=1 .and let { ŷi } n i=1 be the predictions of the agent.The standardized prediction error is then calculated using these quantities, providing a measure of the agent's performance relative to the prior predictive mean.We use a domain-specific function f computing the discrepancy between a prediction ŷi and ground truth value y i (e.g., MSE).We compute the errors ϵ i = f ( ŷi , y i ) and ϵ µ0 = f (µ 0 , y i ).Finally, we compute the standardized error as ϵi−ϵµ 0 σ0 .Crucially, since this metric is computed with respect to the prior predictive, this metric can be negative.</p>
<p>Design Decisions in Constructing BoxingGym</p>
<p>We outline the key design decisions of BoxingGym that allow it to capture key aspects of scientific discovery within a flexible, simulated, and extensible environment.</p>
<p>Discovery via active experimentation.The agent actively interacts with the environment by conducting experiments, reflecting the real-world coupling of experimentation and model discovery.This approach assesses the agent's ability to gather relevant data and refine its models based on experimental results.</p>
<p>Real-world scientific models.Our environments are grounded in real-world scientific models from several domains, ensuring the benchmark tests the agent's ability to handle realistic scenarios.We implement these environment as pymc generative models to make active experimentation an automatic and tractable process.</p>
<p>Goal-driven discovery.Each environment has a specific goal, mirroring the inquiry-driven nature of scientific research.This encourages the agent to engage in targeted experimentation.</p>
<p>Language-based interface for experiments.We use a language-based interface for our experiments because it's flexible (i.e., scientific domains can generally be described in language), easily integrates with LLMs, and interpretable to humans.</p>
<p>Emphasis on Measuring Discovery with Explanations.BoxingGym places a strong emphasis on measuring the quality of the agent's discoveries through the explanations it can provide after experimentation ( §3.2.2).This design decision is motivated by two considerations.From a theoretical perspective, science is fundamentally about developing better theories, and scientific theories are explanations of observed phenomena.From a practical perspective, communicating findings to the broader scientific community is an essential aspect of scientific research.By using language, we do not have to commit to a particular representation of a scientific theory.We illustrate this flexibility, by showing how different representations can be easily integrated within our method for measuring natural language explanations.Extensible/modular environments for benchmarking agents.BoxingGym is easily extensible and modular, enabling researchers to integrate new environments and test different agents with minimal effort.We illustrate this in Fig. 2 which provides a pseudo-code example of how to implement a new environment and goal in BoxingGym .</p>
<p>Domains</p>
<p>BoxingGym consists of 10 environments (see App. D for full details) that cover a range of scientific domains and test different aspects of experimental design and model discovery.Some environments are designed to test optimal experiment design, while others focus on model discovery or involve simulated neuro-symbolic human participants.</p>
<p>Location finding.[17] In an n-dimensional space with k signal-emitting sources, the scientist measure signals at any grid location.Goals include predicting the signal at any point or locating the sources.</p>
<p>Hyperbolic temporal discounting.[17] The scientist observes a participant's choices for different immediate rewards (ir), delayed rewards (dr), and delay periods (D days) Fig. 2 (right).Goals include predicting choices of a participant or discount factors.</p>
<p>Death process.[17] A disease spreads at an infection rate.The scientist can measure the number of infected individuals at different points of time to predict future infections or the infection rate.</p>
<p>Item Response Theory (IRT).[45] In this environment, there is a set of students and a set of questions.The experimenter can observe the correctness of a student's response to a particular question.The goal is to discover the underlying model that relates student ability and question difficulty to the probability of a correct response.</p>
<p>Animal growth curves.[30] An experimenter can observe the length of a dugong at a particular age.The goal is to discover the underlying growth model of dugongs.</p>
<p>Population growth dynamics.[30] An experimenter can observe the population of peregrines at a particular point in time.The goal is to discover the underlying population dynamics model.This is tested by asking the experimenter to predict population dynamics at a particular point in time.</p>
<p>Mastectomy Survival analysis.[13] The experimenter can observe if a patient is alive after a mastectomy, including metastasis status and time since surgery.The goal is to predict survival probabilities for new patients.</p>
<p>Predator-Prey dynamics.[52] This simulates predator-prey populations over time.The goal is to discover models like the Lotka-Volterra equations to predict future populations.</p>
<p>Emotion from outcome.[38] Participants guess a player's emotions after a gambling game's outcome.The experimenter designs games with varied probabilities and prizes to model how participants judge the emotions of a player from outcomes.Human participants are simulated using a probabilistic model translated into natural language by a language model.Moral Machines.[5] Participants face moral dilemmas, choosing which group an autonomous car should save.Experimenters manipulate group compositions and required actions to model moral decision-making.Human participants are simulated with a probabilistic model, and their actions are translated into natural language by a language model.</p>
<p>Experiments</p>
<p>We conduct experiments to evaluate the performance of two baseline agents on BoxingGym .Our goal is to assess their ability to perform experimental design and theory building across a diverse set of environments.We benchmark two types of agents: a standard language model (GPT-4o, OpenAI [39]) and a language model augmented with symbolic reasoning capabilities (Box's Apprentice).</p>
<p>LLM Agent.We consider 6 LLMs, GPT-4o [39], Claude-3.7-sonnet[3], Qwen-2.5-32b-instruct,Qwen-2.5-7b-instruct[55], and reasoning variants OpenThinker-32b, and OpenThinker-7b [51]; the reasoning variants are finetuned on math and coding task.We prompt these models to interact with our environment, purely through natural language, without additional tools (see Fig. 2, see App.B for details).</p>
<p>Box's Apprentice.The apprentice agent augments language models by enabling them to implement generative models of observed data.For model discovery, the agent writes a pymc program [27] after 10 experiments, which is then fit and provided to the scientist explaining findings to the novice.For experimental design, the agent creates and uses these models to guide subsequent experiments.</p>
<p>Experiment Setup.For each environment, we run the agents for 5 independent trials.At each step, the agent chooses to perform an experiment, by specifying a design, and observes the outcome.After a fixed number of steps (0, 1, 3, 5, 7, 10), we evaluate the agent's performance using the metrics described earlier §3.2.The performance of models is averaged across 5 runs and over 10 evaluation points.We also explore a prior vs no prior condition to investigate whether domain knowledge helps or hinders scientific discovery.In the prior condition, we give the LM full context about the problem domain (e.g., "you are observing how participants balance delayed vs immediate rewards"), simulating scientists with background knowledge.In the no prior condition, we remove this context and describe the setting in a domain-agnostic way (e.g., "you receive a tuple of three values"), resembling reasoning from raw observations without preconceptions.This tests whether prior knowledge scaffolds discovery or creates biases that constrain exploration.</p>
<p>Experimental Design Evaluation</p>
<p>Setup.To evaluate the agents' performance, we first assess their ability to gather valuable information through their experiment selection and then measure how effectively they use this information to predict the environment.The Expected Information Regret (EI Regret) compares the Expected Information Gain (EIG) ( §3.2.1) of the agent's chosen experiments to the maximum EIG achievable from 100 random experiments.Lower EI Regret indicates more informative experiment selection.</p>
<p>Prior information does not improve performance.We find that models often perform better when given no prior information after 10 experiments (Fig. 3a).In some cases, this is because the LLM makes an overly strong assumption about the environment (e.g., the signal decay is symmetric around the origin) and does not revise the assumption after more experiments; this is consistent with findings reported by Li et al. [27].In other cases, such as the hyperbolic discounting environment (Fig. 4, right), the model overfits to limited observations.More experiments generally lead to better predictions.We plot the learning trajectories for three environments in (Fig. 4).The agent's average prediction error decreases as it performs more experiments.The Hyperbolic Temporal Discounting environments shows an unexpected trends where more experiments actually increases error.This may again be related to how prior knowledge interferes with effective learning from data.</p>
<p>Models Improve with Scale.Larger models consistently outperform their smaller counterparts within the same model family.Both OpenThinker-32B and Qwen2.5-32Bdemonstrate significantly better performance than their respective 7B variants across environments (Fig. 3a), highlighting the benefits of scale for experimental design tasks.</p>
<p>Instruction-Tuned Models outperform Reasoning Models.Surprisingly, the instruction-tuned Qwen2.5 models outperform the reasoning-focused OpenThinker models (Fig. 3b).This may be because OpenThinker models are finetuned to perform well on a relatively narrow set of verifiable problems in math and code, while instruction-tuned models retain broader capabilities that could be useful for experimental design.</p>
<p>Models performance varies substantially across environments.Models show varying performance across different environments (Fig. 3b).Performance is strongest on environments like population growth dynamics and death process, where the LM agent achieves negative standardized error, indicating that the LM successfully leveraged information gained through experimentation.However, in environments like hyperbolic discounting, performance is low even after experimentation, suggesting that some domains are inherently more challenging for current models.</p>
<p>EIG Regret reveals relationship between experimental design and prediction.Our EIG regret analysis (Fig. 5b) provides insight into the relationship between two key components of scientific reasoning: designing informative experiments and making accurate predictions from collected data.GPT-4o achieves both the lowest EIG regret and strong predictive performance across several environments, suggesting these capabilities can be aligned.However, the varying performance of other models is informative -for instance, Qwen-32B shows higher EIG regret despite good predictive performance in some domains, indicating that while these abilities may be related, excellence in prediction doesn't automatically translate to optimal experimental design.</p>
<p>LLMs cannot always optimally leverage statistical models.While Box's Apprentice can propose and fit explicit statistical models to observed data, it does not consistently improve over the nonaugmented LLM (GPT-4o) (Fig. 5a) From qualitative analysis of the models, we find that Box's Apprentice tends to favor overly simple functional forms due to limited data, such as using linear approximations for inherently nonlinear phenomena.</p>
<p>Evaluating Model Discovery via Communication</p>
<p>Setup.Next, we evaluate the agents' ability to build and communicate models that capture the underlying phenomena in each environment.To test this, we have the agents interact with the environment for 10 steps (scientist phase) and then generate a natural language explanation of their findings.We then provide this explanation to a novice agent, which must make predictions about the environment without any direct interaction (novice phase by using the explanation from the scientist; §3.2.2).The novice agent is always gpt-4o.The scientist's prediction after 10 observations (Error After Experiments) acts as a weak positive control.Ideally, if the scientist's explanation is effective, the novice's error should approach the positive control.</p>
<p>Explanations improve with scale.Larger models generally produce more effective explanations, as evidenced by better novice performance when using explanations from 32B variants compared to 7B models (Fig. 6a).This suggests that increased model scale improves not just experimentation but also the ability to distill and communicate findings.</p>
<p>Explanations are not as good as experiments.As expected, novice agents perform worse than scientists who directly interacted with the environment (Fig. 6b).The gap suggests that current explanation methods do not fully capture the knowledge gained through experimentation.</p>
<p>Explanations are more helpful for some environments.However, the effectiveness of explanations varies substantially across domains (Fig. 6b).For instance, explanations are helpful for animal growth, but struggle with complex domains like moral judgments.This variation likely reflects the complexity of different domains and the current limitations of language models in capturing and communicating certain types of patterns.</p>
<p>Discussion</p>
<p>We introduced BoxingGym , a benchmark measuring language-based agents' capabilities in experimental design and model discovery across 10 real-world-based environments.We evaluated experimental design using information gain metrics and developed a novel model discovery metric based on an agent's ability to explain its model to a novice agent.Our evaluation across multiple model scales (7B-32B parameters) shows that while larger and closed-source models generally perform better, fundamental challenges persist.Neither domain-specific prior knowledge nor statistical modeling capabilities consistently improved performance.Some environments yielded strong results with larger models, while others remained challenging for all approaches.BoxingGym has limitations: it uses pre-defined experimental paradigms rather than requiring design from scratch [14], ignores resource constraints, and covers limited scientific domains.Future work should address these limitations by incorporating experiment design from scratch, resource constraints, and more diverse fields [23].We could also expand the human behavior environments (Moral Machines, Emotions) with more sophisticated participant simulations [4,1,50,40,41].While our experiments demonstrated potential for interfaces that augment language models' scientific reasoning capabilities, future research should explore data visualization, model validation [28], and web-based research strategies to enhance experimental guidance and discovery.</p>
<p>Justification: No proofs or new theoretical result.Guidelines:</p>
<p>• The answer NA means that the paper does not include theoretical results.</p>
<p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.</p>
<p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.</p>
<p>Experimental result reproducibility</p>
<p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
<p>Answer: [Yes] Justification: Yes, further, all our code, results and scripts are available on github.Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.</p>
<p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p>
<p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>
<p>Open access to data and code</p>
<p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
<p>Answer: [Yes]</p>
<p>Justification: All the code is accessible on the github.</p>
<p>Guidelines:</p>
<p>• The answer NA means that paper does not include experiments requiring code.• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p>
<p>Experimental setting/details</p>
<p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
<p>Answer: [Yes] Justification: We describe this in detail in experimental setup and have the full specification in the appendix.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.• The full details can be provided either with the code, in appendix, or as supplemental material.</p>
<p>Experiment statistical significance</p>
<p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
<p>Answer: [Yes]</p>
<p>Justification: We report statistical significance in all our results...</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.• The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).</p>
<p>• It should be clear whether the error bar is the standard deviation or the standard error of the mean.• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.</p>
<p>Experiments compute resources</p>
<p>Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>
<p>Answer: [Yes] Justification: See appendix section B.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).</p>
<p>Code of ethics</p>
<p>Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</p>
<p>Answer: [Yes]</p>
<p>Justification: Single blind submission and we follow the code.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p>
<p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p>
<p>Broader impacts</p>
<p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
<p>Answer: [No] Justification: We don't discuss these as there are no direct negative societal impacts.</p>
<p>Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.</p>
<p>• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
<p>Answer: [NA]</p>
<p>Justification: Not relevant for the paper.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>
<p>Licenses for existing assets</p>
<p>Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>
<p>Answer: [Yes]</p>
<p>Justification: All models have been cited appropriately.The papers that inspired the environments have been credited too.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.</p>
<p>• If this information is not available online, the authors are encouraged to reach out to the asset's creators.• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p>
<p>Declaration of LLM usage</p>
<p>Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research?Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.Answer: [NA] Justification: None of the core methods used LLMs.Guidelines:</p>
<p>• The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.</p>
<p>A Full Results</p>
<p>See Tab. 1, and Tab. 2 for4 prediction errors across all environments for GPT-4o and the Box's apprentice with GPT-4o.Full results are available in the Github Repository.</p>
<p>B LLM Agent</p>
<p>The LLM agent provides an easy way for a large language model (LLM) to interact with BoxingGym .By tailoring the system message to the specific environment, we can clearly define goals for the LLM, elicit experimental designs from it, make accurate predictions for queries, and generate explanations for a novice.This agent class also incorporates a simple retry mechanism that allows the LLM to correct its designs if they are initially invalid.</p>
<p>Models were configured with a temperature parameter of 0.0 to ensure deterministic outputs.Maximum token limits were set to 512 tokens for instruct models and 1024 tokens for thinking variants, providing sufficient thinking tokens for generating an answer without multiple retries.</p>
<p>Env Goal</p>
<p>Error@0 Error@10 Discovery@10 Hyperbolic Discounting Choice 0.66 ± 0.25 0.66 ± 0.25 GPT-4o and Claude-3.7-Sonnetwere accessed via their APIs, while all other models were deployed using vLLM.For the vLLM-served models, we utilized a dual A40 GPU configuration: one GPU dedicated to model serving and the other for inference execution through the vLLM endpoint.This architecture ensured optimal resource allocation and performance stability throughout the experimental process.</p>
<p>Each OED experimental run consisted of 10 predictions conducted after 0, 1, 3, 5, 7, and 10 observations, respectively.Comprehensive log files were generated for each set of predictions to facilitate subsequent analysis.Execution time varied across model architectures, with most configurations requiring approximately 2-3 minutes per run (defined as a single seed, configuration, and environment combination).Models accessed through external APIs typically required longer execution times due to network latency and rate limiting considerations.Discovery experiments reduced execution times compared to OED experiments due to the decreased number of required API calls.</p>
<p>C Box's Apprentice</p>
<p>We closely follow Li et al. [27].In particular, to generate a candidate, we sample a single probabilistic program z from the proposal LM, q LM (•).For the model discovery experiments, we perform this once after 10 experiments.For the OED experiments, we perform this three times over the course of 10 experiments.In all experiments, we use GPT-4o (gpt-4o-2024-05-13).The proposal LM q LM "conditions" on h t ∈ Σ * , a natural language instruction synthesizing previous modeling approaches and suggesting new approaches, the previous program z t−1 , and a textual representation of the dataset D.
z t ∼ q LM (•|z t−1 , h t−1 , D).
We run this at a temperature of 0.0.Chain-of-thought reasoning, or generating intermediate reasoning steps, improves the performance of LMs [54].Motivated by this, we instruct q LM to reflect on the properties of the dataset, sketch a high-level modeling approach, state the hypotheses that it will address before writing a program, and add comments to code.See the system prompt in Figure 7.</p>
<p>D Domains D.1 Location Finding</p>
<p>The location finding environment has hidden signal sources that emit a signal.The scientist can makeg measurements of the superimposed signal at various points.The experiment is directly taken from Foster et al. [16].In table 3, we describe the inputs and outputs of the experiment.</p>
<p>Parameter Description</p>
<p>Model Superposition of K signal sources in d-dim space Setup Parameters Num signal sources K, dim of space d, base signal b, max signal m, noise σ Observations Total noisy signal at point of measurement Goals</p>
<p>Predicting signal intensity at new points and source locations Table 3: Location Finding</p>
<p>We define k = 3 signal sources in R d = R 2 space with locations at θ k .The number of sources is predefined and is known to the agent.Each source emits a signal strength α k .In our implementation, we choose α k to be fixed for all sources.The signal strength decays according to the inverse square law-if an agent measures at point ξ, then the noisy superimposed signal observed will be distributed according to N (µ(θ, ξ), σ) where σ is the signal noise, µ(θ, ξ) is the total intensity at point ξ,
µ(θ, ξ) = b + K k=1 α k m+ || θ k − ξ || 2(1)
and b, m &gt; 0 are constants governing background and maximum signal.Note that unlike Foster et al. [17], we observe the total intensity, not the log total intensity.</p>
<p>D.2 Hyperbolic Discounting</p>
<p>The hyperbolic discounting domain has two hidden variables (k, α) to describe a participant's behavior, where each participant is asked to choose between an immediate reward $iR or a delayed reward $dR in D days.The experiment is outlined in table 4 below.</p>
<p>Parameter Description</p>
<p>Model</p>
<p>Human decision-making in temporal discounting of rewards Setup Parameters Params of the discount function (ϵ, mean and std for log k, scale for α)</p>
<p>Observations</p>
<p>Choice between immediate iR and delayed reward dR at delay D Goals Predicting choices and the value of the discount factor Table 4: Hyperbolic Discounting</p>
<p>In each measurement, we require iR is strictly smaller than dR and all three values have to be positive, because we assume a rational participant would always choose a higher immediate reward over a lower delayed reward.We follow the prior distribution of the latent variables given by Foster et al. [16]:
log k ∼ N (−4.25, 1.5), α ∼ Half N ormal(0, 2)(2)
where the HalfNormal distribution is a normal distribution truncated at 0. For each test, there are three variables in design: iR, dR, and D. We give values to each choice: receiving the immediate reward $iR has value V i = iR, while receiving the delayed reward $dR in D days has value V d = dR 1+kD .Then, whether each participant's chooses the delayed reward in each scenario is characterized as a Bernoulli random variable X ∼ Bernoulli(p) where the probability of choosing the delayed reward is given by You will go through multiple rounds of revision .
p(X = 1|k, α, iR, dR, D) = ϵ + (1 − 2ϵ)Φ( V d − V i α )(3)
where Φ is the cumulative distribution function of the standard normal distribution.In our implementation, we set ϵ = 0.01 for all scenarios.</p>
<p>D.3 Death Process</p>
<p>The death process environment models an infection spreading among a healthy population of N individuals.The infection rate θ determines how the probability of infection increases over time.The environment is outlined in table 5 below.</p>
<p>Parameter Description</p>
<p>Model</p>
<p>The spread of an infection over time Setup Parameters Pop size N , params of the infetion rate (µ, σ, upper and lower bounds) Observations Number of infected individuals at observation time Goals</p>
<p>Predicting the number of infected individuals at a time and the infection rate Table 5: Death Process</p>
<p>In our model, θ is given by the prior distribution outlined in Foster et al. [17].
θ ∼ TruncatedNormal(µ = 1, σ = 1, min = 0, max = ∞)(4)
The number of infected individuals Y at time t is distributed as a binomial random variable:
Y |θ, t ∼ Binomial(N, η)(5)
where η = 1 − e −θt , and N is the population size.We ask the agent to make observations sequentially by giving a time t &gt; 0 at each step.</p>
<p>D.4 IRT</p>
<p>1PL IRT Model The one parameter IRT (or Rasch) domain models the performance of multiple students on multi-question exams.The binary outcome (whether the student is correct) of a studentquestion pair is determined by latent variables governing the student's proficiency and the question's difficulty (Figure 2).The agent's goal is to predict the outcome of a particular student-question pair.The agent may observe other student-question pairs to view their outcome.We define the ability α j of student j and the difficulty β k of question k.In our implementation, α and β are standard normals.The outcome O jk of a student j on question k is determined by a Bernoulli trial where the probability of success p jk is determined by the logit function of z jk = α j − β k .
p jk = 1 1 + e −z jk(6)
In summary, for a given student-question pair, we compute the probability of the student getting the question correct and return the result of the corresponding Bernoulli trial.</p>
<p>2PL IRT Model</p>
<p>The two parameter IRT model is identical to the 1PL variant with an additional variable governing the discriminability γ k of question k.The discriminability models how sensitive the question is to incorrect answers.For higher values of γ, the probability of a student's answer being correct is higher.Thus the outcome O jk of a student j on question k is determined by a Bernoulli trial where the probability of success p jk is determined by the logit function of z jk = γ k (α j − β k ).</p>
<p>3PL IRT Model</p>
<p>The three parameter IRT model is identical to the 2PL variant with an additional variable modeling how susceptible a question is to guessing.For question k, c k determines the probability that a student gets the question right by guessing.Thus the outcome O jk of a student j on question k is determined by a Bernoulli trial where the probability of success p jk is determined by
p jk = c k + (1 − c k ) 1 1 + e −z jk(7)
where
z jk = γ k (α j − β k ) as in 2PL.
We use the 2PL model in BoxingGym .</p>
<p>D.5 Dugongs</p>
<p>The dugongs environment has the ages and lengths of dugongs (sea cows) [30].The goal is to model the length of a dugong based on its age.The following table describes the inputs and outputs of the experiment:</p>
<p>Parameter Description</p>
<p>Model</p>
<p>Bayesian hierarchical model Setup Parameters alpha, beta, lambda, lower limit, upper limit Observations</p>
<p>Length of dugong at a given age Goals Predicting the length of dugongs at different ages
m = α − β • |λ| x(8)
The observed lengths are then drawn from a normal distribution:
Y ∼ N (m, σ)(9)
where σ is the noise in the observed lengths, set to a fixed value (e.g., 0.25).</p>
<p>D.6 Peregrines</p>
<p>The peregrine environment models the population count of peregrine falcons at different times [30].</p>
<p>The goal is to understand how the population changes over time.The following table describes the inputs and outputs of the experiment:</p>
<p>In this environment, the population count of peregrine falcons at time t is modeled using a Poisson regression model with parameters α, β 1 , β 2 , and β 3 .The time values range between 0 and 5.The population count C at a given time t is generated from a Poisson distribution with a mean that is a death_outcome ∼ Bernoulli(p(death))</p>
<p>D.8 Predator-Prey Dynamics</p>
<p>The predator-prey environment models the interaction between populations of predators and prey over time using the Lotka-Volterra equations [52].The following In this environment, the populations of prey and predators at time t are modeled using the Lotka-Volterra equations.The initial populations of prey and predators are given by the parameters 'prey_init' and 'predator_init', respectively.The interaction between the populations is governed by the parameters α, β, γ, and δ.The time values range between 0 and 50.The Lotka-Volterra system of differential equations is defined as follows:
dprey dt = α • prey − β • prey • predator(15)dpredator dt = δ • prey • predator − γ • predator(16)
The populations of prey and predators at any given time t are obtained by solving these differential equations.The observed data consists of tuples indicating the time and the populations of prey and predators at that time.</p>
<p>For example, for a given time t, the populations of prey and predators are computed by solving the Lotka-Volterra equations with the specified parameters and initial populations.The resulting populations are nonnegative integers representing realistic population counts.</p>
<p>D.9 Emotions from Outcomes</p>
<p>The Emotions from Outcomes environment models a participant's predictions of a players emotions after spinning a wheel with three possible monetary outcomes [38].The model considers the actual outcome, the expected outcome, and the absolute difference between the actual and expected outcomes.The following table describes the inputs and outputs of the experiment:</p>
<p>Parameter Description</p>
<p>Model Forward regression model with priors for emotional response Setup Parameters Prize values, probabilities, outcome, LLM Observations Prediction in natural language of how a player feels and why</p>
<p>Goals</p>
<p>Predicting what a participant thinks a player feels on a likert scale of 8 emotions.</p>
<p>Table 11: Emotions From Outcomes Environment</p>
<p>In this environment, the participant's predictions of a player's emotions are modelled after observing the outcome of the player spinning a wheel with three possible prizes.Each outcome has a known probability and monetary value.The emotion predictions are influenced by the actual outcome, the difference between the actual outcome and the expected outcome, and the absolute difference between the actual outcome and the expected outcome.</p>
<p>• β species : Preference for sparing humans over pets.</p>
<p>The logistic regression model generates a choice for which group to save based on the computed attributes and intervention.These predictions are translated into free-form natural language observations by a language model with the prompt shown in Fig. 9.For example, in a scenario where group 1 consists of a boy and a girl, and group 2 consists of an elderly man and an elderly woman, with the intervention being 'swerve', the simulated participant responds with "I choose to save group 1 because they are younger and have more potential life ahead of them."</p>
<p>LLM prompt to translate predictions from the logistic regression model to observations</p>
<p>1 have to make choices in a moral dilemma involving autonomous vehicles .In each dilemma , the vehicle must decide between two outcomes , each involving the death of a different group of characters . 2 There are two groups of characters , and the vehicle must decide which group to save .You can query the participant for their decision by providing the descriptions of the characters in each group and the intervention the car would take ( ' swerve ' or ' stay ' to save group 1) .3 The characters can be any of the following : { characters } 4 Translate the decision that is shown to you in natural language and provide reasons for it .5 The importance that you place on the different attributes of the characters affects your decision .Values range from -1 to 1 , with 0 being neutral .6 Your preferences are as follows : 7 -Preference for human count : { beta_human_count } 8 -Preference for group 1 ( passengers ) : { beta_group } 9 -Preference for males : { beta_gender } 10 -Preference for young : { beta_age } 11 -Social status : { beta_ social_status } 12 -Preference for Fitness : { beta_fitness } 13 -Preference for humans over pets : { beta_species } 14 -Preference for having to take an action : { beta_intervention } 15 DO NOT talk about the values of preferences directly in your response .Only talk about the most salient reasons for the decision .Be concise , write one sentence only .16 Translate the decision that is shown to you in natural language and provide reasons for it .17 The scenario presented to you was as follows :</p>
<p>E Qualitative Examples</p>
<p>Example of an explanation produced by the LLM Agent for the IRT Environment 1 Based on the observations made , we can infer some patterns about the students and questions :  correctly .This indicates that question 0 might also be relatively easy for students , though we lack data for students 0 , 1 , and 5.   11 From these observations , it appears that some questions ( like question 1 and question 0) might be easier for a majority of students , but individual performance can vary .Without more data , it ' s challenging to make definitive predictions for all student -question pairs , but these patterns provide a starting point for understanding the environment .</p>
<p>Figure 1 :
1
Figure1: Overview of BoxingGym.The BoxingGym Framework is designed to holistically evaluate experimental design and model discovery capabilities in the spirit of George Box[9].1) The process starts with a user defining a goal for the scientist agent.2) The scientist formulates a theory.3) This theory guides the experimental design, where the scientist interacts with a simulated world to gather new data.4) The scientist then analyzes the new and old data to propose and refine theories.This iterative process continues for several iterations.5) The scientist is then asked to explain the findings to a novice.6) We evaluate the novice and the scientist by casting the goal as a prediction problem.</p>
<p>Figure 2 :
2
Figure 2: Python pseudocode examples.(left)BoxingGym is instantiated as modular classes and methods for the environment (WorldEnv), goals (Goal), and agents (Agent).(center) Pseudocode illustrating the workflow of setting goals, performing experiments, predicting outcomes, and providing explanations.(right) An example, hyperbolic temporal discounting, where the agent predicts a participant's choice between immediate and delayed rewards and explains the concept to a novice.We introduce principled evaluation metrics that measure the quality of experiments and discovered models.To evaluate experimental design, we draw from Bayesian optimal experimental (BOED) design[44] and use expected information gain (EIG) to measure the informativeness of an experiment.EIG captures how much an experiment reduces uncertainty in the parameters of a generative model and, importantly, this measure complements our decision to implement environments as generative models.To evaluate model discovery, we take inspiration from the fact that science is a communicative endeavor.We propose a communication-based evaluation strategy: we ask a scientist agent to distill their experiments into a natural language explanation and evaluate how much that explanation empowers a novice agent, who does not have access to the experiments conducted by the scientist, to make accurate predictions about the environment.</p>
<p>Figure 3 :
3
Figure 3: Normalized Error Compared across Models.(a) Comparison of the normalized errors for different LLMs with or without prior information included in the prompt.(b) Comparison of reasoning models (OpenThinker) and instruct models (Qwen) across environments.Error bars are the standard error across 5 runs.</p>
<p>Figure 4 :
4
Figure 4: Normalized Errors Over Number of Observations.Normalized errors for the LLM agent with gpt-4o with prior information (solid blue) and without prior information (dotted yellow) across three domains: Population Growth Dynamics (left), IRT (center) and Hyperbolic Discounting (right).Error bars are the standard error across 5 runs.</p>
<p>Figure 5 :
5
Figure 5: (a) Comparison of the Box's Apprentice with an LLM agent.(b) EIG Regret scores for six large language models, with lower values indicating better performance.</p>
<p>Figure 6 :
6
Figure 6: Evaluation of Model Discovery via Communication.(a) Comparison of the standardized error of the Novice (gpt-4o) with different Scientist models.(b) Comparison of errors made by the Novice and the Scientist (both models are gpt-4o).Error bars are standard error.</p>
<p>For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g.</p>
<p>18
18
Group 1: { group1 } 19 Group 2: { group2 } 20 Intervention : { intervention } 21 Choice made : { choice } 22 Translate the decision that is shown to you in natural language and provide reasons for it .23 Only talk about the most salient reasons for the decision .24 The sentence should be concise and should not refer to the importance factors / values directly .25 Start with " I choose to save group 1/ group 2 because ...".Be concise and faithful to the importance weights .</p>
<p>Figure 9 :
9
Figure 9: LLM prompt for simulated participant.LLM prompt to translate predictions from the logistic regression model to observations in free-form natural language.</p>
<p>2 3 1
21
. ** Question 1**: Students 0 , 1 , 2 , and 3 answered question 1 correctly , while student 4 answered it incorrectly .This suggests that question 1 is generally easier for most students , but not universally so .</p>
<p>4 5 2 .
42
** Question 0**: Students 2 , 3 , and 4 answered question 0</p>
<p>6 7 3 .
63
** Student 1**: Answered question 1 correctly but answered question 2 incorrectly .This suggests that student 1 ' s performance may vary depending on the question .</p>
<p>8 9 4 .
84
** Student 4**: Answered question 0 correctly but answered question 1 incorrectly .This indicates that student 4 ' s performance also varies by question .</p>
<p>10
10</p>
<p>Figure 11 :
11
Figure 11: Example Explanation.Example of an explanation produced by the LLM Agent for the IRT Environment.</p>
<p>• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).• The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)for more details.• The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</p>
<ol>
<li>New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [Yes] Justification: We add documentation to the BoxingGym code.Guidelines:• The answer NA means that the paper does not release new assets.• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.• At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.14.Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?
Answer: [NA]Justification: No human participants were recruited.Guidelines:• The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects.• Including this information in the supplemental material is fine, but if the main contribu-tion of the paper involves human subjects, then as much detail as possible should beincluded in the main paper.• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector.15. Institutional review board (IRB) approvals or equivalent for research with humansubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: Paper does not use human participants.Guidelines:• The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects.• Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper.</li>
</ol>
<p>Table 1 :
1
• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)for what should or should not be described.Performance of GPT-4o Across Different Tasks.Numbers shown are normalized-0 errors.Errors with prior (top line) and without prior (bottom line) appear on different lines.Errors are averaged across 5 runs.
EnvGoalError@0Error@10 Discovery@10Hyperbolic DiscountingChoice0.32±0.04 0.96±0.150.87±0.08 1.04±0.040.79±0.37 0.96±0.07Hyperbolic DiscountingDiscount-0.06±0.00 --0.06±0.00 ---Location FindingSignal0.30±0.25 0.63±0.390.59±0.55 0.86±0.474.75±4.51 1.52±1.28Location FindingSource Location1.29±1.3 --0.15 ±0.4 ---Death ProcessNum Infected0.54±0.52 -0.31±0.30-1.06±0.03 -1.04±0.01-1.08±0.01 -1.00±0.11Death ProcessInfection Rate0.13±0.37 -1.64±1.12 ---IRTCorrectness0.12±0.07 0.08±0.15-0.24±0.10 0.00±0.130.12±0.18 0.12±0.14DugongsLength-0.04±0.02 -0.04±0.02-0.08±0.00 -0.08±0.00-0.06±0.04 -0.07±0.02PeregrinesPopulation1.95±0.22 1.30±0.11-0.57±0.09 -0.65±0.01-0.65±0.02 -0.66±0.03MastectomySurvival0.04±0.14 0.32±0.080.36±0.10 0.27±0.121.00±0.41 0.45±0.18Predator-PreyPopulation0.38±0.04 0.75±0.02-0.31±0.05 -0.42±0.01-0.01±0.12 -0.07±0.40EmotionsPrediction1.04±0.21 N/A1.22±0.29 N/A0.90±0.58 N/AMoral MachinesJudgement0.40±0.07 N/A0.36±0.04 N/A0.68±0.13 N/A</p>
<p>Table 2 :
2
Performance of Box's Apprentice Across Different Tasks.Standardized errors shown here.Errors with prior (top line) and without prior (bottom line) appear on different lines.Errors are averaged across 5 runs.
1.17 ± 0.140.66 ± 0.300.91 ± 0.090.74 ± 0.42</p>
<p>19</p>
<p>If there ' s a previous program in your context window and a list of hypotheses , revise based on this information !
20Explicitly cite the hypotheses ( if there are any ) that youaddress in your sketch .21 2. After coming up with a plan , write your program and addcomments to lines of code that address certain hypotheses .22 ```python23import pymc as pm24import numpy as np25def gen_model ( observed_data ) :26# convert observed_data columns to numpy arrays27# index the appropriate column names2829....30rng1 = np . random . default_rng (42)31rng2 = np . random . default_rng (314)32with pm . Model as model () :33# create a pm . MutableData object for each non -observation column34... Your code here ...35# Copy the rest of this code verbatim but remember tohave this indented in scope of model () !36trace = pm . sample (1000 , tune =500 , target_accept =0.90 ,chains =3 , cores =1 , random_seed = rng1 )</p>
<p>Table 6
6
below details the inputs, outputs, and target for every variation of the IRT model.
ParamDescriptionModelStudent performance on multi-question examsSetup Parameters Number of students N , number of questions Q, student-question pair to predictObservationsOutcomes of various student-question pairsGoalsPredicting the correctness of student responses to questions</p>
<p>Table 6 :
6
IRT Model</p>
<p>Table 7 :
7
Dugongs EnvironmentIn this environment, the length of a dugong at age x is modeled using a hierarchical Bayesian model with parameters α, β, and λ.The age values range between 0 and 5.The observed length Y at a given age x is generated from a normal distribution with a mean that is a function of x and the parameters α, β, and λ, and a fixed standard deviation.The function representing the mean length m is defined as:</p>
<p>Table 10 :
10
table describes the inputs and outputs of the experiment: Predator-Prey Environment
ParameterDescriptionModelLotka-Volterra equationsSetup Parameters Initial prey population, initial predator population, α, β, γ, and δObservationsPopulations of prey and predators at a given timeGoalsPredicting populations
In the sequential setting, we replace the prior p(θ) with the posterior p(θ|y, d).
We omit the predatory-prey and Emotions domains for Box's Apprentice, since GPT-4o could not reliably produce pymc programs
po st er io r_ pre dictive = pm . s a m p l e _ p o s t e r i o r _ p r e d i c t i v e ( trace , random_seed = rng2 , return_inferencedata = False )
return model , posterior_predictive , trace
``F igure 7: BoxLM system prompt The system prompt for the proposal p LM . We also include some additional instructions on pymc syntax such as wrapping features in a MutableData container.
NeurIPS Paper ChecklistClaimsQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?Answer: [Yes] Justification: We describe the design of our benchmark accurately, summarize results with different models.Guidelines:• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes] Justification: See discussion.Guidelines:• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory assumptions and proofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] sample !! 12 IMPORTANT : Use the variable name " y_obs " for the observations when you define it !13 IMPORTANT : Use the variable name " y_obs " for the observations when you define it !14 IMPORTANT : Index the appropriate column names when grabbing data from observed_data .These column names are indicated in the column description .1516Your answer should follow the template in the following order .The observed population counts are then drawn from a Poisson distribution:This model allows for capturing the non-linear trends in the population data over time.D.7 Survival Analysis: MastectomyThe survival analysis environment models the outcomes of breast cancer patients based on the time since surgery and the metastasized status.The following table describes the inputs and outputs of the experiment:Parameter Description Model Survival analysis using a Bayesian approach Setup Parameters num_patients, time_upper_bound, lambda, beta Observations Whether a selected patient is alive or dead Goals Predict survival based on time since surgery and if the cancer had metastasized Table9: Survival Analysis EnvironmentIn this environment, the outcome (alive or dead) of a patient is modeled based on the time since surgery and whether the cancer metastasized[13].The outcomes are generated using a Bayesian model with parameters λ 0 and β.The number of patients and the upper bound of the time since surgery are configurable.At the start of an episode, we sample a set of patients that have undergone mastectomy, with varying times since they had surgery and if their cancer had metastasized or not.The experimenter can then choose to observe specific patients to see if they are alive or dead.The probability of death is calculated using the following model:The probability of death for a patient is given by the logistic function:Each patient's outcome is simulated from a Bernoulli distribution with the calculated death probability.The observed data consists of tuples indicating whether the patient died, the time since surgery, and the metastasized status.For example, for a patient with a given time since surgery and metastasized status, the death outcome is sampled as follows:The model uses the following parameters:1. Prize values: The monetary values of the three possible outcomes.2. Probabilities: The probabilities of each outcome occurring.Outcome:The actual outcome of the wheel spin.The emotions are measured on a Likert scale from 1 to 9 for the following eight emotions: Happiness, Sadness, Anger, Surprise, Fear, Disgust, Contentment, DisappointmentThe emotional response is generated based on the following model:where:• α are the intercepts for each emotion.• β win are the coefficients for the actual outcome.• β PE are the coefficients for the prediction error (PE).• β absPE are the coefficients for the absolute prediction error (absPE).For each emotion, the value is sampled from a normal distribution with the computed mean and a predefined standard deviation.The generative model produces Likert scale ratings for the 8 emotions for the participant's predictions of what a player would feel.These predictions are translated into free-form natural language observations by a language model with the prompt shown in Fig.8.For example, an observation when the prizes are $50, $20, $10 with probabilities 0.1, 0.4, 0.5, and the player wins $50, the simulated participant responds with "The player might be feeling quite happy and content because they landed on the highest possible outcome, which was unexpected given its low probability."D.10 Moral MachinesThe Moral Machine environment Awad et al.[5]The model uses the following parameters:1. Character attributes: gender, age, social status, fitness, species (human or pet).2. Intervention type: 'swerve' or 'stay'.The decision to save a group is influenced by the difference in attributes between the two groups and the intervention required.The logistic regression model considers the following coefficients:LLM prompt to translate predictions from the generative model to observations1 You are observing a user play a game where they spin a wheel .2The wheel has three possible outcomes ( monetary values ) , and the probabilities of landing on each are known to you and the player .3 You are observing the player play the game and the outcomes .4You are asked to predict how the player feels after each spin of the wheel .5 Translate the values for emotions to a sentence that describes the player .6The decisions are based on the following model and features : 7 -Your predition of the player ' s happiness , sadness , anger , surprise , fear , disgust , contentment , and disappointment are influenced by a few factors .8 -The player ' s emotions are influenced by the actual outcome of the spin .9 -The player ' s emotions are influenced by the difference between the actual outcome and the expected outcome .10 -The player ' s emotions are influenced by the absolute difference between the actual outcome and the expected outcome .11 The wheel has three possible outcomes with the following probabilities :The player has spun the wheel and landed on { outcome }.16This is how you think the player feels :  • β intervention : Preference for inaction.• β group : Preference for group 1 (passengers).• β gender : Preference for sparing females.• β fitness : Preference for sparing the fit.• β social_status : Preference for sparing higher status individuals.• β age : Preference for sparing the young.• β human_count : Preference for sparing more characters.Box's Apprentice proposed programs 1 with pm .Model () as model :2# Priors for student abilities and question difficulties 3 student_ability = pm .Normal (" student_ability " , mu =0 , sigma =1 , shape =6) 4 q ues tio n_d if fic ulty = pm .Normal (" question_difficulty " , mu =0 , sigma =1 , shape =6)      12 ** General Patterns :** 13 -Students with higher abilities are more likely to answer correctly across various questions .14 -Easier questions are more likely to be answered correctly by most students .1516To predict if a student will answer a question correctly , consider both the student ' s ability and the question ' s difficulty .Higher student ability and lower question difficulty increase the likelihood of a correct answer .
Using large language models to simulate multiple humans and replicate human subject studies. Rosa I Gati V Aher, Adam Arriaga, Kalai Tauman, International Conference on Machine Learning. PMLR2023</p>
<p>Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. 2023</p>
<p>. Anthropic, Claude, 2024</p>
<p>Out of one, many: Using language models to simulate human samples. Ethan C Lisa P Argyle, Nancy Busby, Joshua R Fulda, Christopher Gubler, David Rytting, Wingate, Political Analysis. 3132023</p>
<p>The moral machine experiment. Edmond Awad, Sohan Dsouza, Richard Kim, Jonathan Schulz, Joseph Henrich, Azim Shariff, Jean-François Bonnefon, Iyad Rahwan, Nature. 56377292018</p>
<p>Automated reverse engineering of nonlinear dynamical systems. Josh C Bongard, Hod Lipson, Proceedings of the National Academy of Sciences. 1042007</p>
<p>A Useful Method for Model-Building. G E P Box, William G Hunter, Technometrics. 41962</p>
<p>Sampling and Bayes' Inference in Scientific Modelling and Robustness. E P George, Box, Journal of the Royal Statistical Society. Series A (General). 0035923814341980</p>
<p>Science and statistics. E P George, Box, Journal of the American Statistical Association. 713561976</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.015402016Openai gym. arXiv preprint</p>
<p>Discovering symbolic cognitive models from human and animal behavior. Pablo Samuel Castro, Nenad Tomasev, Ankit Anand, Navodita Sharma, Rishika Mohanta, Aparna Dev, Kuba Perlin, Siddhant Jain, Kyle Levin, Noémi Éltető, Will Dabney, Alexander Novikov, Glenn C Turner, Maria K Eckstein, Nathaniel D Daw, Kevin J Miller, Kimberly L Stachenfeld, 10.1101/2025.02.05.636732bioRxiv. 2025</p>
<p>Bayesian Experimental Design: A Review. Kathryn Chaloner, Isabella Verdinelli, 10.1214/ss/1177009939Statistical Science. 1031995</p>
<p>Analysis of survival data. David Roxbee, Cox , 2018Chapman and Hall/CRC</p>
<p>Emergent complexity and zero-shot transfer via unsupervised environment design. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, Sergey Levine, Advances in neural information processing systems. 332020</p>
<p>Structure discovery in nonparametric regression through compositional kernel search. David Duvenaud, James Lloyd, Roger Grosse, Joshua Tenenbaum, Ghahramani Zoubin, Proceedings of the 30th International Conference on Machine Learning. Sanjoy Dasgupta, David Mcallester, the 30th International Conference on Machine LearningAtlanta, Georgia, USAPMLRJun 201328</p>
<p>Variational bayesian optimal experimental design. Adam Foster, Martin Jankowiak, Elias Bingham, Paul Horsfall, Yee Whye Teh, Thomas Rainforth, Noah Goodman, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Deep adaptive design: Amortizing sequential bayesian experimental design. Adam Foster, Ilyas Desi R Ivanova, Tom Malik, Rainforth, Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research. the 38th International Conference on Machine Learning, Machine Learning ResearchPMLRJul 2021</p>
<p>Kanishk Gandhi, Dorsa Sadigh, Noah D Goodman, arXiv:2305.19165Strategic reasoning with language models. 2023arXiv preprint</p>
<p>Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, Noah D Goodman, arXiv:2404.03683Stream of search (sos): Learning to search in language. 2024arXiv preprint</p>
<p>Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D Goodman, arXiv:2503.013072025arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Teaching large language models to reason with reinforcement learning. Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, Roberta Raileanu, arXiv:2403.046422024arXiv preprint</p>
<p>Automated hypothesis validation with agentic sequential falsifications. Kexin Huang, Ying Jin, Ryan Li, Michael Y Li, Emmanuel Candès, Jure Leskovec, 2025</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>SWE-bench: Can language models resolve real-world github issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, ; Karthik R Narasimhan, The Twelfth International Conference on Learning Representations. Ofir Press2024</p>
<p>Beyond a*: Better planning with transformers via search dynamics bootstrapping. Lucas Lehnert, Sainbayar Sukhbaatar, Dijia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, Yuandong Tian, arXiv:2402.140832024arXiv preprint</p>
<p>Automated Statistical Model Discovery with Language Models. Emily B Michael Y Li, Noah D Fox, Goodman, International Conference on Machine Learning (ICML). 2024</p>
<p>Critical: Critic automation with language models. Y Michael, Vivek Li, Noah D Vajipey, Emily B Goodman, Fox, 2024</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024arXiv preprint</p>
<p>posteriordb: a set of posteriors for Bayesian inference and probabilistic programming. Måns Magnusson, Paul Bürkner, Aki Vehtari, October 2023</p>
<p>A proposal for the dartmouth summer research project on artificial intelligence. John Mccarthy, Marvin L Minsky, Nathaniel Rochester, Claude E Shannon, AI magazine. 274august 31. 1955. 1955</p>
<p>Hybrid grammar-based approach to nonlinear dynamical system identification from biological time series. B A Mckinney, J E Crowe, H U Voss, P S Crooke, N Barney, J H Moore, 10.1103/PhysRevE.73.021912Phys. Rev. E. 7321912Feb 2006</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi, Metaicl, arXiv:2110.15943Learning to learn in context. 2021arXiv preprint</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, arXiv:1312.56022013arXiv preprint</p>
<p>A tutorial on adaptive design optimization. Jay I Myung, Daniel R Cavagnaro, Mark A Pitt, 10.1016/j.jmp.2013.05.005.URLhttps://www.sciencedirect.com/science/article/pii/S0022249613000503Journal of Mathematical Psychology. 0022-24965732013</p>
<p>Vincent Moens. Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon ; Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, arXiv:2502.14499A new framework and benchmark for advancing ai research agents. 2025arXiv preprint</p>
<p>Evolve: Evaluating and optimizing llms for exploration. Allen Nie, Yi Su, Bo Chang, Jonathan N Lee, Ed H Chi, Quoc V Le, Minmin Chen, arXiv:2410.062382024arXiv preprint</p>
<p>Affective cognition: Exploring lay theories of emotion. Jamil Desmond C Ong, Noah D Zaki, Goodman, Cognition. 1432015</p>
<p>Openai, Hello, GPT-4. 2024</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. Sung Joon, Lindsay Park, Carrie Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, Noah D Goodman, arXiv:2306.04031Certified deductive reasoning with language models. 2023arXiv preprint</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Tom Rainforth, Robert Cornish, Hongseok Yang, Andrew Warrington, Frank Wood, On Nesting Monte Carlo Estimators. International Conference on Machine Learning (ICML). 2018</p>
<p>Probabilistic models for some intelligence and attainment tests. Georg Rasch, 1993ERIC</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, arXiv:2210.012402022arXiv preprint</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Mehran Kazemi, Najoung Kim, He He, Advances in Neural Information Processing Systems. 202436</p>
<p>Mastering board games by external and internal planning with language models. John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel Hennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, arXiv:2412.121192024arXiv preprint</p>
<p>Bayesian inference and online experimental design for mapping neural microcircuits. Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski, Advances in Neural Information Processing Systems. C J Burges, L Bottou, M Welling, Z Ghahramani, K Q Weinberger, Curran Associates, Inc201326</p>
<p>Rehearsal: Simulating conflict to teach conflict resolution. Omar Shaikh, Valentino Chai, Michele J Gelfand, Diyi Yang, Michael S Bernstein, arXiv:2309.123092023arXiv preprint</p>
<p>Variations and fluctuations of the number of individuals in animal species living together. Vito Volterra, ICES Journal of Marine Science. 311928</p>
<p>Hypothesis search: Inductive reasoning with language models. Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D Goodman, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, arXiv:2412.15115Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. Xingzhang Ren, Xuancheng Ren,2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
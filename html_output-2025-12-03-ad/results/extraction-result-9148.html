<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9148 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9148</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9148</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-278327728</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.01482v1.pdf" target="_blank">Understanding LLM Scientific Reasoning through Promptings and Model’s Explanation on the Answers</a></p>
                <p><strong>Paper Abstract:</strong> —Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and problem-solving across various domains. However, their ability to perform complex, multi-step reasoning tasks—essential for applications in science, medicine</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9148.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9148.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-2024-08-06 (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI large language model instance used in this study to perform text-based scientific reasoning on the GPQA dataset across multiple scientific subdomains via prompt engineering; configured with reduced temperature and restricted knowledge cutoff for controlled evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-2024-08-06</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4o instance used in experiments with knowledge cutoff restricted to December 2023, model checkpoint dated April 1, 2024; temperature set to 0.5 to reduce randomness; completion token limit 4,096. (No parameter count or training-data corpus details provided in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Multiple: organic chemistry, analytical chemistry, inorganic chemistry, physical chemistry; molecular biology, genetics; physics subdomains including quantum mechanics, astrophysics, condensed matter, electromagnetism/photonics, optics/acoustics, relativistic mechanics, statistical mechanics, and high-energy particle physics.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of scientific problem solving / reasoning: answer graduate-level multiple-choice scientific questions (GPQA-main, 448 questions) and produce explanations (rationales) for chosen answers under different prompt-engineering strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Primary: accuracy (% correct) on GPQA-main questions; additional evaluation of explanation quality via cosine similarity between model-produced explanation and GPQA ground-truth explanation (MPNet sentence-embedding cosine similarity). Supplementary: Jaccard-based distance on final answers, linear-regression slope and R^2 for cumulative-correctness trends, counts per subdomain, and analysis of 'not answered' cases.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported accuracies across prompt-engineering strategies on 448 GPQA questions: self-consistency 52.99% (237/448), direct answer (zero-shot) 52.23% (234/448), zero-shot CoT 50.00% (224/448), multipath 48.44% (217/448), decomposition (option 2) 47.77% (214/448), chain-of-thought (CoT) 43.75% (196/448). Additionally: 98/448 (21.86%) questions answered correctly by all prompts; 104/448 (23.21%) answered incorrectly by all prompts; 246/448 (54.91%) answered correctly by some but not all prompts. Per-subdomain correct-counts are provided in the paper's Table IV (counts rather than percent in each subdomain).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Identified factors include: (1) prompt-engineering method (direct, CoT, zero-shot CoT, self-ask, self-consistency, decomposition, multipath) — different prompts change accuracy and explanation quality; (2) temperature setting (0.5 used; number of self-consistency paths affects diversity and final answer); (3) presentation of subproblems (concatenating subproblems at once reduced hallucination relative to sequential subproblem prompting); (4) dataset domain complexity and subdomain training coverage (some subdomains produced dense regions of unanimous correctness or incorrectness); (5) whether the model's internally computed answer matches one of the provided multiple-choice options (model sometimes refused to pick when none matched); (6) token limit and whether all reasoning paths and explanations are captured; (7) reliance on pattern recognition vs. true logical inference leading to inconsistent multi-step reasoning; (8) number of sampled reasoning paths for aggregation (self-consistency used 3 samples in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human expert performance cited for GPQA: domain experts ~65% accuracy (background). The paper also reports a separate Table III (GPQA Diamond) showing other model accuracies for context (OpenAI-o1-0912 77.3%, DeepSeekR1-Zero 73.3%, Claude3.5 59.4%, Sonnet 51.1%, Llama3 405B 45.9%, Qwen2.5-72B 39.3%) — these are comparative benchmarks referenced but not re-run as part of the paper's GPT-4o experiments. GPT-4o's reported per-prompt accuracies are lower than the best reported model on GPQA Diamond (OpenAI-o1-0912 at 77.3% on that different split).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations and failure modes include: (1) hallucination increased when decomposition subproblems were sent sequentially (Option 1) — caused Option 1 to be removed from analysis; (2) model refusal to select an answer when none of the multiple-choice options matched its internal calculation (except direct-answer prompt which forced a choice); (3) self-consistency produced highest accuracy but the worst explanations (low cosine similarity to ground truth); (4) multipath prompts produced diverse justifications that could be internally consistent yet far from ground-truth explanations; (5) reliance on pattern-matching rather than true logical inference, causing inconsistencies on complex multi-step reasoning; (6) study grading of explanations performed by non-domain-experts (limitation acknowledged); (7) token-limit prevented capturing explanations for all internal paths; (8) single temperature used for both path generation and final decision may limit performance.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors' suggestions include: (1) concatenate subproblems and send them all at once to reduce hallucination rather than sending sequentially; (2) consider using two temperature levels — one higher for path exploration and one lower for final decision — to decouple exploration from exploitation in multipath/self-consistency; (3) integrate structured reasoning frameworks, hybrid AI approaches, and human-in-the-loop methodologies to improve both accuracy and interpretability; (4) collect explanations for all reasoning paths (requires larger token budgets) for deeper analysis; (5) use domain-expert grading of explanations beyond embedding-similarity metrics; (6) use prompt engineering strategies tailored to both accuracy and explanation quality rather than optimizing only one.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Explanations were evaluated by MPNet sentence embeddings' cosine similarity to GPQA ground-truth explanations; direct answer, CoT and zero-shot CoT produced explanations most similar to ground truth (median similarity ranks), while self-consistency scored highest in accuracy but lowest in explanation similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding LLM Scientific Reasoning through Promptings and Model’s Explanation on the Answers', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9148.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9148.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Frontier GPQA models (benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmarked frontier LLMs evaluated on GPQA (OpenAI-o1-0912, DeepSeekR1-Zero, Claude3.5, Sonnet, Llama3 405B, Qwen2.5-72B, ChatGLM-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Other LLMs reported in the paper's Table III as comparative GPQA Diamond performance baselines; these are referenced for context and not directly used in the paper's GPT-4o experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI-o1-0912, DeepSeekR1-Zero, Claude3.5, Sonnet, Llama3 405B, Qwen2.5-72B, ChatGLM-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>These models are referenced as frontier LLMs evaluated on GPQA Diamond in the paper's Appendix/Table III; the paper does not provide internal details (parameters or training corpora) for each model, only reported accuracies on GPQA Diamond.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>GPQA benchmark tasks spanning graduate-level questions in biology, chemistry and physics subdomains (same GPQA domains as used for GPT-4o evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based scientific question answering / reasoning on GPQA (Diamond split) used as a benchmark of reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%) on GPQA Diamond (as reported in Table III of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported GPQA Diamond accuracies (from paper Table III): OpenAI-o1-0912 77.3%, DeepSeekR1-Zero 73.3%, Claude3.5 59.4%, Sonnet 51.1%, Llama3 405B 45.9%, Qwen2.5-72B 39.3% (ChatGLM-4 presence is listed but exact percent not explicitly given in the truncated table in the provided text).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Not detailed per-model in this paper; implicitly the same factors (prompt engineering, dataset difficulty, reasoning technique, model training and architecture differences) likely affect these results. The paper references other work that uses reasoning-specific techniques (e.g., DeepSeekR1 via RL incentives, self-consistency, CoT) which influence performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>These models are presented as comparative baselines to contextualize GPT-family performance on GPQA Diamond; human expert performance (≈65% on GPQA-main cited elsewhere in the paper) provides another reference point.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper does not report detailed per-model failure analyses for these benchmark models; Table III entries are cited for context only and not further analyzed in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>The paper uses these benchmark numbers to motivate the need to study not just accuracy but explanation quality and the effect of prompt engineering; authors recommend future work that compares reasoning quality (explanations) in addition to accuracy across models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding LLM Scientific Reasoning through Promptings and Model’s Explanation on the Answers', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpqa: A graduate-level google-proof q&a benchmark. <em>(Rating: 2)</em></li>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9148",
    "paper_id": "paper-278327728",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o-2024-08-06 (GPT-4o)",
            "brief_description": "An OpenAI large language model instance used in this study to perform text-based scientific reasoning on the GPQA dataset across multiple scientific subdomains via prompt engineering; configured with reduced temperature and restricted knowledge cutoff for controlled evaluation.",
            "citation_title": "Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers",
            "mention_or_use": "use",
            "model_name": "GPT-4o-2024-08-06",
            "model_description": "GPT-4o instance used in experiments with knowledge cutoff restricted to December 2023, model checkpoint dated April 1, 2024; temperature set to 0.5 to reduce randomness; completion token limit 4,096. (No parameter count or training-data corpus details provided in the paper.)",
            "scientific_subdomain": "Multiple: organic chemistry, analytical chemistry, inorganic chemistry, physical chemistry; molecular biology, genetics; physics subdomains including quantum mechanics, astrophysics, condensed matter, electromagnetism/photonics, optics/acoustics, relativistic mechanics, statistical mechanics, and high-energy particle physics.",
            "simulation_task": "Text-based simulation of scientific problem solving / reasoning: answer graduate-level multiple-choice scientific questions (GPQA-main, 448 questions) and produce explanations (rationales) for chosen answers under different prompt-engineering strategies.",
            "evaluation_metric": "Primary: accuracy (% correct) on GPQA-main questions; additional evaluation of explanation quality via cosine similarity between model-produced explanation and GPQA ground-truth explanation (MPNet sentence-embedding cosine similarity). Supplementary: Jaccard-based distance on final answers, linear-regression slope and R^2 for cumulative-correctness trends, counts per subdomain, and analysis of 'not answered' cases.",
            "simulation_accuracy": "Reported accuracies across prompt-engineering strategies on 448 GPQA questions: self-consistency 52.99% (237/448), direct answer (zero-shot) 52.23% (234/448), zero-shot CoT 50.00% (224/448), multipath 48.44% (217/448), decomposition (option 2) 47.77% (214/448), chain-of-thought (CoT) 43.75% (196/448). Additionally: 98/448 (21.86%) questions answered correctly by all prompts; 104/448 (23.21%) answered incorrectly by all prompts; 246/448 (54.91%) answered correctly by some but not all prompts. Per-subdomain correct-counts are provided in the paper's Table IV (counts rather than percent in each subdomain).",
            "factors_affecting_accuracy": "Identified factors include: (1) prompt-engineering method (direct, CoT, zero-shot CoT, self-ask, self-consistency, decomposition, multipath) — different prompts change accuracy and explanation quality; (2) temperature setting (0.5 used; number of self-consistency paths affects diversity and final answer); (3) presentation of subproblems (concatenating subproblems at once reduced hallucination relative to sequential subproblem prompting); (4) dataset domain complexity and subdomain training coverage (some subdomains produced dense regions of unanimous correctness or incorrectness); (5) whether the model's internally computed answer matches one of the provided multiple-choice options (model sometimes refused to pick when none matched); (6) token limit and whether all reasoning paths and explanations are captured; (7) reliance on pattern recognition vs. true logical inference leading to inconsistent multi-step reasoning; (8) number of sampled reasoning paths for aggregation (self-consistency used 3 samples in this study).",
            "comparison_baseline": "Human expert performance cited for GPQA: domain experts ~65% accuracy (background). The paper also reports a separate Table III (GPQA Diamond) showing other model accuracies for context (OpenAI-o1-0912 77.3%, DeepSeekR1-Zero 73.3%, Claude3.5 59.4%, Sonnet 51.1%, Llama3 405B 45.9%, Qwen2.5-72B 39.3%) — these are comparative benchmarks referenced but not re-run as part of the paper's GPT-4o experiments. GPT-4o's reported per-prompt accuracies are lower than the best reported model on GPQA Diamond (OpenAI-o1-0912 at 77.3% on that different split).",
            "limitations_or_failure_cases": "Reported limitations and failure modes include: (1) hallucination increased when decomposition subproblems were sent sequentially (Option 1) — caused Option 1 to be removed from analysis; (2) model refusal to select an answer when none of the multiple-choice options matched its internal calculation (except direct-answer prompt which forced a choice); (3) self-consistency produced highest accuracy but the worst explanations (low cosine similarity to ground truth); (4) multipath prompts produced diverse justifications that could be internally consistent yet far from ground-truth explanations; (5) reliance on pattern-matching rather than true logical inference, causing inconsistencies on complex multi-step reasoning; (6) study grading of explanations performed by non-domain-experts (limitation acknowledged); (7) token-limit prevented capturing explanations for all internal paths; (8) single temperature used for both path generation and final decision may limit performance.",
            "author_recommendations_or_insights": "Authors' suggestions include: (1) concatenate subproblems and send them all at once to reduce hallucination rather than sending sequentially; (2) consider using two temperature levels — one higher for path exploration and one lower for final decision — to decouple exploration from exploitation in multipath/self-consistency; (3) integrate structured reasoning frameworks, hybrid AI approaches, and human-in-the-loop methodologies to improve both accuracy and interpretability; (4) collect explanations for all reasoning paths (requires larger token budgets) for deeper analysis; (5) use domain-expert grading of explanations beyond embedding-similarity metrics; (6) use prompt engineering strategies tailored to both accuracy and explanation quality rather than optimizing only one.",
            "additional_notes": "Explanations were evaluated by MPNet sentence embeddings' cosine similarity to GPQA ground-truth explanations; direct answer, CoT and zero-shot CoT produced explanations most similar to ground truth (median similarity ranks), while self-consistency scored highest in accuracy but lowest in explanation similarity.",
            "uuid": "e9148.0",
            "source_info": {
                "paper_title": "Understanding LLM Scientific Reasoning through Promptings and Model’s Explanation on the Answers",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Frontier GPQA models (benchmarks)",
            "name_full": "Benchmarked frontier LLMs evaluated on GPQA (OpenAI-o1-0912, DeepSeekR1-Zero, Claude3.5, Sonnet, Llama3 405B, Qwen2.5-72B, ChatGLM-4)",
            "brief_description": "Other LLMs reported in the paper's Table III as comparative GPQA Diamond performance baselines; these are referenced for context and not directly used in the paper's GPT-4o experiments.",
            "citation_title": "Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers",
            "mention_or_use": "mention",
            "model_name": "OpenAI-o1-0912, DeepSeekR1-Zero, Claude3.5, Sonnet, Llama3 405B, Qwen2.5-72B, ChatGLM-4",
            "model_description": "These models are referenced as frontier LLMs evaluated on GPQA Diamond in the paper's Appendix/Table III; the paper does not provide internal details (parameters or training corpora) for each model, only reported accuracies on GPQA Diamond.",
            "scientific_subdomain": "GPQA benchmark tasks spanning graduate-level questions in biology, chemistry and physics subdomains (same GPQA domains as used for GPT-4o evaluation).",
            "simulation_task": "Text-based scientific question answering / reasoning on GPQA (Diamond split) used as a benchmark of reasoning capability.",
            "evaluation_metric": "Accuracy (%) on GPQA Diamond (as reported in Table III of the paper).",
            "simulation_accuracy": "Reported GPQA Diamond accuracies (from paper Table III): OpenAI-o1-0912 77.3%, DeepSeekR1-Zero 73.3%, Claude3.5 59.4%, Sonnet 51.1%, Llama3 405B 45.9%, Qwen2.5-72B 39.3% (ChatGLM-4 presence is listed but exact percent not explicitly given in the truncated table in the provided text).",
            "factors_affecting_accuracy": "Not detailed per-model in this paper; implicitly the same factors (prompt engineering, dataset difficulty, reasoning technique, model training and architecture differences) likely affect these results. The paper references other work that uses reasoning-specific techniques (e.g., DeepSeekR1 via RL incentives, self-consistency, CoT) which influence performance.",
            "comparison_baseline": "These models are presented as comparative baselines to contextualize GPT-family performance on GPQA Diamond; human expert performance (≈65% on GPQA-main cited elsewhere in the paper) provides another reference point.",
            "limitations_or_failure_cases": "The paper does not report detailed per-model failure analyses for these benchmark models; Table III entries are cited for context only and not further analyzed in this work.",
            "author_recommendations_or_insights": "The paper uses these benchmark numbers to motivate the need to study not just accuracy but explanation quality and the effect of prompt engineering; authors recommend future work that compares reasoning quality (explanations) in addition to accuracy across models.",
            "uuid": "e9148.1",
            "source_info": {
                "paper_title": "Understanding LLM Scientific Reasoning through Promptings and Model’s Explanation on the Answers",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpqa: A graduate-level google-proof q&a benchmark.",
            "rating": 2,
            "sanitized_title": "gpqa_a_graduatelevel_googleproof_qa_benchmark"
        },
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.0105805,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers</p>
<p>Alice Rueda ice.rueda@unityhealth.to 
Member, IEEEMohammed S Hassan mohammed.siadhassan@torontomu.ca 
Argyrios Perivolaris gyrios.perivolaris@unityhealth.to 
Bazen G Teferra bazen-gashaw.teferra@unityhealth.to 
Reza Samavi samavi@torontomu.ca 
Sirisha Rambhatla sirisha.rambhatla@uwaterloo.ca 
Yuqi Wu yuqi14@ualberta.ca 
Yanbo Zhang yanbo9@ualberta.ca 
Bo Cao 
Divya Sharma divya03@yorku.ca 
Sridhar Krishnan krish-nan@torontomu.ca 
Member, IEEEVenkat Bhat venkat.bhat@utoronto.ca 
M S Hassan </p>
<p>Department of Psychiatry and has received research funding
University of Toronto
Canadian Institutes of Health Research, Brain &amp; Behavior Foundation</p>
<p>Department of National Defence (Government of Canada)
American Foundation for Suicide Prevention
Ontario Ministry of Health Innovation Funds
Royal College of Physicians and Surgeons of Canada
New Frontiers in Research Fund
Associated Medical Services Inc. Healthcare
Roche CanadaNovartisand Eisai</p>
<p>Interventional Psychiatry Program
St. Michael's Hospital
Unity Health Toronto
TorontoOntarioCanada</p>
<p>Department of Electrical, Computer, and Biomedical Engineering
Toronto Metropolitan University
Toronto</p>
<p>Interventional Psychiatry Program
St. Michael's Hospital
Unity Health Toronto
TorontoOntarioCanada</p>
<p>Interventional Psychiatry Program
St. Michael's Hospital
Unity Health Toronto
TorontoOntarioCanada</p>
<p>Department of Electrical, Computer, and Biomedical Engineering
Toronto Metropolitan University
Toronto</p>
<p>Department of Management Science and Engineering
University of Waterloo
OntarioCanada</p>
<p>Department of Electrical and Computer Engineering
University of Alberta
Canada</p>
<p>Department of Psychiatry
Faculty of Medicine and Dentistry
University of Alberta
EdmontonAlbertaCanada</p>
<p>Department of Psychiatry
Faculty of Medicine and Dentistry
University of Alberta
EdmontonAlbertaCanada</p>
<p>Department of Mathematics and Statistics
York University
OntarioCanada</p>
<p>Department of Electrical, Computer
and Biomed-ical Engineering
Toronto Metropolitan University
Toronto</p>
<p>Interventional Psychiatry Program
St. Michael's Hospital
Unity Health Toronto
TorontoOntarioCanada</p>
<p>Department of Psychiatry
University of Toronto
TorontoOntarioCanada</p>
<p>Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers
E7410979067BFB34EFA8122412AC9A05
Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and problem-solving across various domains.However, their ability to perform complex, multi-step reasoning tasks-essential for applications in science, medicine, and law-remains an area of active investigation.This paper examines the reasoning capabilities of contemporary LLMs, analyzing their strengths, limitations, and potential for improvement.The study uses prompt engineering techniques on the Graduate-Level GoogleProof Q&amp;A (GPQA) dataset to assess the scientific reasoning of GPT-4o.Five popular prompt engineering techniques and two tailored promptings were tested: baseline direct answer</p>
<p>(zero-shot), chain-of-thought (CoT), zero-shot CoT, self-ask, selfconsistency, decomposition, and multipath promptings.Our findings indicate that while LLMs exhibit emergent reasoning abilities, they often rely on pattern recognition rather than true logical inference, leading to inconsistencies in complex problem-solving.The results indicated that self-consistency outperformed the other prompt engineering technique with an accuracy of 52.99%, followed by direct answer (52.23%).Zero-shot CoT (50%) outperformed multipath (48.44%), decomposition (47.77%), selfask (46.88%), and CoT (43.75%).Self-consistency performed the second worst in explaining the answers.Simple techniques such as direct answer, CoT, and zero-shot CoT have the best scientific reasoning.We propose a research agenda aimed at bridging these gaps by integrating structured reasoning frameworks, hybrid AI approaches, and human-in-the-loop methodologies.By critically evaluating the reasoning mechanisms of LLMs, this paper contributes to the ongoing discourse on the future of artificial general intelligence and the development of more robust, trustworthy AI systems Impact Statement-The development of large language models (LLMs) has streamlined tasks such as language translation, summarization, and writing.LLMs outperform humans in general knowledge tasks even with zero-shot promptings.This study uses prompt engineering techniques on the Graduate-Level GoogleProof Q&amp;A (GPQA) dataset to assess the scientific reasoning of GPT-4o.Five popular prompt engineering techniques and two tailored promptings were tested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot CoT, selfask, self-consistency, decomposition, and multipath promptings.The results indicated that self-consistency outperformed the other prompt engineering technique with an accuracy of 52.99%, followed by direct answer (52.23%).Zero-shot CoT (50%) outperformed multipath (48.44%), decomposition (47.77%), selfask (46.88%), and CoT (43.75%).Self-consistency performed the second worst in explaining the answers.Simple techniques such as direct answer, CoT, and zero-shot CoT have the best scientific reasoning.This study establishes a baseline for evaluating GPT-4o's scientific reasoning and optimizing prompt engineering strategies.</p>
<p>Index Terms-Artificial Intelligence (AI), Interpretability, Large Language Models (LLMs), Logical Inference, Reasoning Capabilities, Reliability</p>
<p>I. INTRODUCTION</p>
<p>L ARGE Language Models (LLMs) are largely trained with general knowledge and can be used directly for knowledge retrieval purposes, such as in zero-shot prompting or with one or more examples in few-shot prompting.Despite LLMs being demonstrated to mimic cognitive processes of arXiv:2505.01482v2[cs.AI] 25 Jul 2025 humans [1], particularly in language processes [1], [2], their reasoning capability is still limited.The current architecture and training paradigm require models to be equipped with specific content knowledge and optimized prompting to reduce hallucination and improve performance.Often, research will focus on how to improve task performance through prompt engineering and less on tracking the reasoning process of an LLM.This paper uses GPT-4o as an example to investigate the scientific reasoning capability of LLMs with no access to additional information or external tools.This paper aims to illustrate the scientific reasoning capability of LLMs using GPT-4o on the Graduate-Level Google-Proof Q&amp;A (GPQA) dataset as an example by analyzing the responses and explanations to various prompt engineering techniques.The GPQA is a challenging dataset of 448 multiple-choice questions written by content experts in biology, physics, and chemistry.II.BACKGROUND Despite the growing body of research on LLMs and their performance across various tasks, most studies have primarily focused on improving accuracy through prompt engineering rather than investigating how these models reason.Prior work has largely evaluated LLMs on task completion, such as mathematical problem-solving [3] or common-sense reasoning [4].However, these studies typically measure performance improvements without systematically analyzing the underlying reasoning process that leads to a model's answers.The GPQA dataset [5], while previously used to test the accuracy of LLMs on scientific questions, has not yet been applied to investigate the reasoning strategies employed by these models.Prior to DeepSeekR1 [6], accuracy, rather than explanation, has been the de-facto performance measure for scientific reasoning capability (see Appendix A for frontier LLM performance).A few methods have been proposed to improve accuracy on GPQA dataset, including more sophisticated prompt engineering techniques such as Iteration of Thought [7], Story of Thought [8], Multi-thinking Modes Tree [9], make use of analogies [10], or using multiple LLMs [11].Assessing both accuracy and reasoning quality across different prompting techniques can provide new insights into the multifaceted interplay of an LLM's processes and justification for complex scientific problems, bridging a critical gap in the literature.</p>
<p>Prompt engineering is a technique used to improve instructions to obtain desirable and more structured responses from LLMs that are pre-trained in general tasks.Using wellcrafted prompts, LLMs excel in context-aware retrieval tasks, understanding user intent, and can even use tools to perform more complex tasks through API calls.However, LLMs can sometimes present false or misleading information as facts, often referred to as hallucinations.Some well-known techniques reduce hallucination, such as Retrieval Augmented Generation (RAG) [12] and ReACT [13] using content to provide grounding.Others, like Chain-of-Thought (CoT) [14] and self-consistency [15], are designed for reasoning and logic.CoT-like prompts constitute the majority of the reasoning and logic prompts [16], including Auto-CoT [17], LogiCoT [18], Chain-of-Symbol [19], Tree of Thought [20], Thread of Thought [21], and Chain of Table prompting [22].</p>
<p>Prompt engineering has been used to perform reasoning tasks [23], especially in mathematical reasoning where CoT, self-consistency, and Program-of-Thought (separating reasoning steps from final computation) were used on various models to solve mathematical problems [3].The CoT's superiority was demonstrated on 23 of BIG-Bench's reasoning tasks.[24] Chen et al. [25] proposed Multi-CoT Consistent Knowledge Distillation (MCC-KD) to enforce consistency with the questions.This teacher-student-based CoT technique enforces consistency by using multiple teacher LLMs to extract rationales on the problem, N-gram, and Jaccard similarity to filter similar rationales to maintain diversity.The smaller language models (students) learn to answer the question correctly by measuring similarity through bi-directional KLD on their rationales with the teachers' rationales.The most common approach to engineering a prompt is to tailor the prompt to tasks and datasets.</p>
<p>Besides mathematical reasoning using GSM8K, there are other cognitive problems LLMs are tasked with that require other reasoning processes such as Commonsense QA datasets and AI2's Reasoning Challenge (ARC), general reasoning BIG-Bench [26], understanding and generating code Hu-manEval [27], and scientific reasoning dataset GPQA.Benchmark datasets for natural language processing are widely available and well defined, such as CoLA, GLUE, and Benchmark of Linguistic Minimal Pairs (BLiMP).[28] Cognitive benchmarking datasets are limited to a few well-known ones, such as CogBench [29], and MulCogBench2.CogBench consists of seven cognitive psychology experiments (tasks) to measure 10 behavioral metrics.These tasks include probabilistic reasoning, two-armed bandit tasks (horizon and restless bandit tasks), instrumental learning, two-step tasks, temporal discounting, and balloon analog risk tasks.The 10 metrics are priori weighting, likelihood weighting, directed exploration, random exploration, meta-cognition, learning rate, optimism bias, model-basedness, temporal discounting, and risk-taking.MulCogBench is a multimodal dataset that enables researchers to relate LLM cognitive processes to human brain activity.The cognitive dataset contains subjective semantic ratings, eyetracking, functional magnetic resonance imaging (fMRI), and magnetoencephalography (MEG).However, a more domainspecific dataset, such as GPQA has a lower data leakage risk, which provides a measure of the reasoning process.GPQA is designed for scalable oversight problems.</p>
<p>The scalable oversight problem is when LLMs are superior in domain knowledge yet allow humans to supervise, evaluate, and guide to solve the problems.[30] GPQA consists of extremely difficult scientific multiple-choice questions with four choices, with domain experts achieving 65% accuracy.At the same time, the solutions provided in the dataset also come with an explanation.The explanation provides another measure to judge and guide the answers.GPQA is designed to evaluate a model's reasoning capability on top of content retrieval.As titled in the paper, GPQA is a graduate-level Googleproof Q&amp;A benchmark dataset across physics, chemistry, and biology subdomains in quantum mechanics, astrophysics, organic chemistry, genetics, and molecular biology.These multiple-choice questions are high-quality, extremely difficult, and their content is not easily accessible.Non-domain experts achieved 34% accuracy with access to the internet, and 60-80% by domain experts who have completed or are currently pursuing their doctoral degree.The GPQA has been used to develop better prompts for benchmark model performance.To the best of our knowledge, GPQA has not been used to explain the scientific reasoning of LLMs.</p>
<p>By prompting the model to answer the GPQA questions and to provide an explanation of the answer, we can measure not just the accuracy of the model's problem-solving ability but also understand the reasons behind such an answer.GPQA provides a set of explanations that can be used as ground truth to compare a model's explanation.This paper uses basic promptings on GPT-4o to study LLM's scientific reasoning.The model's knowledge has been restricted to Dec 2023 to ensure no data leakage and model checkpoint on April 1, 2024, to establish reproducibility.The contributions of this study are 1) providing better insight into the scientific reasoning behind LLMs, 2) proposing an objective approach to measure reasoning using GPQA, and 3) providing an example to demonstrate automated scalable oversight problems.</p>
<p>III. METHODS</p>
<p>The experiments were conducted using GPT-4o-2024-08-06 as the candidate model and lowered the temperature setting to 0.5 to reduce randomness.Lower temperatures, closer to 0.0, tend to produce more deterministic responses, whereas higher temperatures promote exploration and variability.To understand the model's reasoning ability, we limited the knowledge cutoff set to December 2023 to ensure that no data leakage accidentally occurred through HFRL or continuous content releases and set the model date to April 1, 2024.The multiplechoice questions in GPQA were presented one after another with four possible choices shuffled.In order to capture all explanations, the token limit was set to 4,096 for prompt completion.</p>
<p>A. GPQA Dataset</p>
<p>This GPQA dataset is designed to benchmark the advancement in the reasoning capability of LLMs as the models evolve, achieving this through the concept of scalable oversight methods.Scalable oversight allows skilled non-experts (i.e., completed or pursuing doctoral degrees in other domains) to verify LLM's reasoning steps.One of the key components of scalable oversight is interpretability, where LLMs provide logical explanations throughout the process.With an accuracy of 78%, GPT-o1 has demonstrated its performance in part with human experts [5].This study uses all 448 questions provided in the GPQA-main dataset to ensure all questions are difficult and objective.In the supplementary material, Table D1 in Appendix D provides a breakdown of each subdomain in the GPQA-main dataset with their accuracy answered by experts and non-experts.</p>
<p>B. Prompt Engineering Techniques</p>
<p>Prompt engineering techniques are generally versatile and sometimes can benefit from engineering a combined method tailored to the tasks.This versatility allowed us to observe LLM's response to scientific reasoning.We implemented prompts that were classified as reasoning and logic [16] in some that were not mentioned but demonstrated the ability to elicit an LLM's reasoning capability, and two of our interpretations and creations.We explored the behaviour of the model using seven prompt engineering techniques: direct answer (zero-shot), chain-of-thought (CoT), zero-shot CoT, decomposition -an adapted implementation of decomposed prompting with its decomposed prompting's reasoning chains, modular problem-solving and compositionality, self-ask, selfconsistency, and our proposed multipath prompting.Each of these techniques is summarized below:
1) Direct Answer (Zero-Shot)
This was a baseline reference for other techniques.The model was asked to solve a multiple-choice problem.</p>
<p>2) Chain-of-thought (Cot)</p>
<p>This explicit reasoning approach was guided by three examples to show the model's step-by-step reasoning to solve multiple-choice reasoning problems.</p>
<p>3) Zero-Shot CoT</p>
<p>This prompting asked the model to solve the multiplechoice problem by "Let's think step by step" before each answer without providing any examples.</p>
<p>4) Self-Ask</p>
<p>For complex questions, intermediate steps or information are required to reduce the gaps.Self-ask allows the models to decompose questions effectively and generate correct intermediate answers.This technique was designed to handle multi-hop problems by asking the LLMs to check if there are intermediate steps by asking relevant follow-up questions and answering them before reaching the final answer.</p>
<p>5) Self-Consistency</p>
<p>Self-consistency replaces the "greedy decode" with a diverse set of reasoning paths and marginalizes the reasoning paths to aggregate the final answers that are the most consistent.As the temperature setting was reduced to 0.5 to reduce randomness, the number of paths defined for self-consistency allowed a small number.We chose the minimum setting of a small number of three to enable selection.The use of three samples allows increased control and decreased randomness associated with the technique.</p>
<p>6) Decomposition</p>
<p>This technique asked the LLM to break down complex problems recursively into simple manageable subproblems.All subproblems were sent to the model to solve in a step-by-step manner.The final results integrated solutions to all subproblems.We tried two different ways of implementing subproblem prompting.Option 1 was to send subproblems sequentially, one at a time, until all subproblems receive completion before composition.Option 2 concatenated all subproblems and sent everything at once.</p>
<p>7) Multipath Prompting</p>
<p>This prompt was created to observe the model's be-haviour using a mixture of logical and illogical steps.</p>
<p>This prompt asked the model to create a justification or verification for each of the four choices.Then, based on their justification to answer whether the choice is correct or not.A final prompt was sent with all the justifications and answers to ask the LLM to decide which one was correct.</p>
<p>C. Performance Measures</p>
<p>Besides measuring accuracy, we have captured which questions were answered correctly or incorrectly by all prompts and recorded all details to examine the reasoning capability of the model.After answering each question, GPT-4o was also asked to provide an explanation for its answer.GPQA provided an explanation feature in the dataset, providing the explanation to reach the solution.Cosine similarity was used to measure the similarity between the ground truth explanations from GPQA and the explanations elicited from the prompts.Cosine similarity (S c ) has known problems for underestimating frequent words with other same words across different contexts (within words) or different words in similar context (across words).[31] The underestimating effect has no impact on the explanations for GPQA dataset as the contents are highly specialized.A pre-trained MPNet [32] sentence transformer model was used to detect sentence semantic similarity in the explanation of prompts and ground truth.Then, we look at the effect distance of the prompts on explanations elicited using stand mean distance on the explanation supported by Jaccard distance on the final answers, i.e. 1-Jaccard similarity.</p>
<p>IV. RESULTS</p>
<p>A. Accuracy of Answers</p>
<p>After testing all prompt engineering techniques, Option 1 generated unreliable answers due to hallucination and had to be removed from the analysis.Except for self-consistency and decomposition-option 2, the number of correct answers was summarized for different prompt engineering techniques.</p>
<p>The accuracy of each prompt, represented as a percentage out of 100%, is provided in Table 1.Table 1 provides a summary of the accuracy of the questions, correct answers to the three domains, corresponding accuracy over the 448 questions, and the number of questions with missing answers.The techniques with the most correct answers ranked in descending order are self-consistency (237/448, 52.99%), direct answer (234/448, 52.23%), zero-shot CoT (224/448, 50%), multipath (217/448, 48.44%), decomposition (214/448, 47.77%), and CoT (196/448, 43.75%).A breakdown of correctness for each subdomain can be found in Supplementary Tables B1 and B2 in Appendix B. Some prompts failed to provide answers at all.GPT-4o did not provide an answer to questions when its perceived answer was not listed as one of the multiple choices.This reasoning process is captured in the returned explanation.Except for direct answer, all other prompt engineering techniques disagreed with all the choices available in one or more questions.The number and questions with no answers are listed under Not Answered in Table 1.Six of the seven prompts did not provide answers to question 257.Only a direct answer has provided both the answer and explanation, despite the answer being incorrect.All other prompts elicited disagreement.</p>
<p>Fig. 1 illustrates the cumulative correctness throughout the GPQA-main dataset in (a) and modelled with linear regression in (b).The slope represents the mean, and the distance from the slope represents the variation.A higher R-squared value demonstrates that the mean can be an accurate representation of accuracy.All prompts have a linear characteristic on the number of correct answers, as supported by the linear models presented in the Supplementary Table C1 in Appendix C. The coefficients (slopes) of the linear models are very close to the accuracy values, as all R-squared scored over 0.99.The direct answer has the steepest slope of 0.539, followed by self-consistency's 0.528.As expected, CoT has the shallowest slope of 0.438.</p>
<p>As the GPQA-main dataset has the easiest questions removed, this reduces the chance of all prompts answering the questions correctly.The majority of the questions were answered correctly by some prompts and incorrectly by others.There is a significant percentage of questions that were answered unanimously correct or incorrect.To better understand the occurrence of correctness, Fig. 2 indicates the locations of the questions that were answered correctly by all prompts and incorrectly by all prompts.There are 98/448 (21.86%) questions that were answered correctly by all, 104/448 (23.21%) were answered incorrectly by all, and the rest 246 (54.91%) were answered correctly by some.Fig. 2a presents the cumulated correct answers after sorting by subdomain and plotting on top of various coloured subdomains.To locate which questions were unanimously answered correctly and incorrectly, Fig. 2b added a vertical line to indicate the questions that were correctly answered (in green) and incorrectly answered (in red) by all prompts.Fig. 3a and 3b are also matched in regions with correctness.The dense red or green vertical lined regions are reflected by the shallowness and steepness of the slopes.</p>
<p>B. Scientific Reasoning</p>
<p>Explanation quality varied significantly across techniques.With each answer, GPT-4o was asked to provide justification (explanation) for its choice of answer.This explanation provides insights into its scientific reasoning process.Appendix C presents samples of the explanation and answers for each prompt.The prompt-induced explanation was used to compare with GPQA's ground truth explanation.Table 2 summarizes the cosine similarity measure on the explanations elicited from the prompts.The distribution of the similarity measures is not Gaussian distributed as indicated by the Shapiro-Wilk's test where all p-values were less than 0.05.Ranking the similarity by a median, the explanation elicited by the prompts in descending order is direct answer, CoT, zero-shot CoT, self-ask, decomposition, multipath, and self-consistency.Using psychology's definition of correlation [33], Table 2 also groups the cosine similarity into weak (both positive and negative), moderate, and strong similarities between the prompt-induced explanation and ground truth.The overarching explanations  Fig. 3. Distribution of the cosine similarity that GPT-4o's explanation for each prompt to the ground truth explanation provided in GPQA-main dataset.</p>
<p>Fig. 4. Cosine similarity for the first 30 questions that GPT-4o's explanation for each prompt to the ground truth explanation provided in GPQA-main dataset.</p>
<p>provided by simple prompts, such as direct answer, zero-shot CoT, CoT, and self-ask have strong similarities to the ground truth.Multipath prompting (self-consistency and multipath) and subproblem-based decomposition achieved moderate similarity.Fig. 3 illustrates the histogram of the similarities using a bin size of 0.02 overlaid with kernel density estimation.The accuracy rankings reflect how well each prompt can reason through scientific questions, with some techniques outperforming others.The direct answer, CoT, and zero-shot CoT provided the closest explanation to the ground truth with very similar explanation measures.While self-consistency scored the highest in accuracy, its explanation measure was ranked last.Self-ask, multipath, and self-consistency all have the median inside the moderate similarity category.CoT and direct answer are very similar in their explanations compared to the ground truth.The heatmap in Fig. 4 shows where the model's explanations were very different from the ground truth.The dissimilarity of self-consistency and multipath visibly stands out with the close to 0 and negative values.</p>
<p>V. DISCUSSION</p>
<p>This study evaluated the scientific reasoning capabilities of GPT-4o using the GPQA-main dataset, emphasizing the interplay between accuracy and explanation quality across various prompt engineering techniques.The findings illuminate critical insights into the model's reasoning abilities and the impact of prompt strategies on performance.GPT-4o with its knowledge set to December 2023 and model checkpoint on April 1, 2024, was used to study scientific reasoning ability using single path (direct answer, zero-shot CoT, CoT, decomposition, and self-ask) and multipath prompt engineering techniques.The model temperature setting was lowered to 0.5 to minimize hallucinations while still permitting variability in responses, particularly for self-consistency (3 paths) and the in-house designed multipath.The maximum number of completion token was set to 4,096 as required to explain GPQA questions [34] and to avoid mistakes caused by overthinking.[35] The multipath technique was intentionally designed to try to induce hallucination by justifying for all the provided choices before answering the questions.</p>
<p>Self-consistency and direct answer led to the performance in accuracy with self-consistency slightly higher in the number of correct answers and direct answer having a slightly steeper linear regression slope.CoT performed the worst in accuracy.Much like DeepSeekR1 that "may be hurt" by a few-shot CoT, GPT-4o performed better with zero-shot CoT to CoT.</p>
<p>With high R-squared scores (R2 &gt; 0.99), the slope of the cumulative correctness reflected the accuracy of all prompt techniques.The correctness for sorted subdomains matched the steepness of the slope to the vertical green (all prompts answered correctly) and red (all prompts answered incorrectly) lines.GPT-4o refuses to answer some of the questions, especially question 257.The explanation provided by GPT-4o provided a glance at the model's reasoning process to reach or not reach the answer.Compared to the ground truth explanation provided by the dataset, CoT achieved the highest cosine similarity, in descending order, followed by the direct answer, zero-shot CoT, self-ask, decomposition, multipath, and self-consistency.Although self-consistency demonstrated its superiority in answering the questions correctly, it performed poorly in providing or not providing explanations, as illustrated in Fig. 4. On the other hand, the explanations provided by multipath were as expected, as GPT-4o was forced to provide justifications for the wrong answers.</p>
<p>When subproblems were presented sequentially, hallucinations increased.To avoid this, we suggest concatenating subproblems and presenting them all at once, which helps reduce hallucination.By concatenating subproblems and sending them all at once, hallucination was managed.Surprisingly, the multipath prompting did not elicit the expected hallucination as GPT-4o was asked to provide justification for incorrect answers in the multiple-choice questions.</p>
<p>On the other hand, GPT-4o could refuse to answer if the provided choices did not match its explanation for all prompts, except for the direct answer prompt.In question 257, the model noted that none of the choices were correct but selected the closest one due to the constraints of the prompt.The instruction for direct answer was simply asking GPT-4o to answer the question, thus GPT-4o was forced to answer the question even though its answer was not listed in any of the choices as provided in the explanation provided for questions 257, "None of these options match 9.6 × 10 6 GeV."This indicates GPT-4o is able to prioritize tasks as demonstrated by the explanation, "However, since I am required to choose an The areas with dense vertical green lines (indicating correct answers from all prompts) correspond to subdomains where GPT-4o performed well.In contrast, regions with dense red lines (incorrect answers from all prompts) suggest subdomains where the model struggled.This allowed us to infer that GPT-4o is well-trained in certain subdomains, and vice versa in the shallow slope subdomains where the region is populated with dense vertical red lines.</p>
<p>GPT-4o is less confused in its reasoning when given direct or simple prompts as demonstrated by CoT, direct answer, and zero-shot CoT.Decomposition elicited the best explanations when the problem was first broken down into smaller manageable subproblems.The strong similarity in GPT-4o's explanation to the ground truth supports its scientific reasoning ability to answer non-multiple-choice questions except for prompt engineering techniques that cannot elicit reasonable explanations.For some questions, self-consistency only provided the answers with no explanation.Single-path prompts are more straightforward for the model to reason about than multipath prompts.With the multipath technique, the explanations for each path can be too diverse and yet consistent for the model to choose the explanation.As there is only one temperature setting for path creation and answer selection, probabilistic reasoning is common for both the path creation and final decision-making.If the model accepts two levels of temperature setting, one for exploration to create paths and another for final decision making, the result might be improved from exploiting the final answer.</p>
<p>This study has a few limitations to consider.Our studies are conducted by non-experts with no doctoral degrees in chemistry, physics, or biology.The study can benefit from domain experts' grading of the explanations elicited by the prompts beyond the cosine similarity measures.The study can also benefit from capturing the explanation for all the paths in addition to the current final answer's explanation.This will require increasing the token size limit and will drive the cost up.However, the current recorded responses provide large explanatory data that can be used for deeper analysis in the future.These findings can serve as a baseline for future studies on GPT-4o's reasoning capabilities and act as a baseline to compare with other models.</p>
<p>VI. CONCLUSIONS</p>
<p>In conclusion, this study provides an evaluation of GPT-4o's scientific reasoning capabilities using the GPQA dataset, highlighting the interplay between accuracy and explanation quality across different prompt engineering techniques.While self-consistency achieved the highest accuracy, direct answer, CoT, and zero-shot CoT generated explanations more similar to the ground truth, suggesting that reasoning quality does not always correlate with accuracy.Our findings underscore the need for future research to refine prompt strategies that optimize both correctness and interpretability.This work serves as a foundation for further studies on LLM reasoning, particularly in domains requiring high levels of interpretability and trust.</p>
<p>VII. APPENDICES</p>
<p>A. GPQA Diamond Performance Comparison on Frontier Models (Table III</p>
<p>Fig. 1 .Fig. 2 .
12
Fig. 1.Cumulated correctness.(a) Each correct answer for the prompts is stored as a cumulative sum.(b) Model the accumulated correctness with linear regression.</p>
<p>TABLE II COSINE
II
SIMILARITY (Sc) OF GPT-4O'S EXPLANATION TO THE GROUND TRUTH ON GPQA-MAIN in the provided data, we will select the closest option, though it is not correct based on the calculation."All other prompts reached the same calculation but refused to choose an answer.Despite this result, the accuracy of GPT-4o's responses is consistent, as shown by the high Rsquared scores, which indicate that the model's accuracy does not fluctuate wildly.The slope of the accuracy curve serves as a reliable indicator of performance in answering complex scientific questions.
Direct AnswerZS CoTCoTDecompositionSelf-AskSelf-ConsistencyMultiPathShapiro-Wilk (W)0.9150.9540.9340.9630.8220.8440.814StatisticsMedian (IQR)0.733 (0.136)0.730 (0.140) 0.736 (0.137)0.694 (0.144)0.727 (0.133)0.645 (0.200)0.684 (0.406)Mean (SD)0.718 (0.114)0.730 (0.140) 0.726 (0.111)0.678 (0.110)0.705 (0.144)0.587 (0.215)0.559 (0.288)Minimum0.0310.1680.1640.299-0.043-0.065-0.076Maximum0.9160.9360.9150.8890.9080.8830.900Similarity StrengthWeak: Sc &lt; 0.3 (n)21311155110Moderate: 0.3 ≤ Sc &lt; 0.7 (n)164171158234170242136Strong: Sc ≥ 0.7 (n)282276287213267151202Overall Similarity StrengthStrongStrongStrongModerateStrongModerateModerateoption, and acknowledging the possibility of a typographicalerror or miscommunication</p>
<p>) B. Correctness Counted by Subdomain (Table IV) C. Linear Regression Table (Table V) D. GPQA Questions and Human Performance (Table VI)</p>
<p>TABLE III BEST-PERFORMED SETTING FOR EACH OF THE WELL-KNOWN REASONING MODEL FAMILY ON GPQA DIAMOND DATASET OpenAI-o1-0912 DeepSeekR1-Zero Claude3.5 Sonnet Llama3 405B Qwen2.5-72BChatGLM-4
Accuracy77.3%73.3%59.4%51.1%45.9%39.3%</p>
<p>TABLE IV COUNT
IV
OF CORRECTLY ANSWERED QUESTIONS IN EACH SUBDOMAIN Direct Answer ZS CoT CoT Decomposition Self-Ask Self-Consistency MultiPath
Analytical Chemistry (1)0 101011Astrophysics (37)22192421212123Chemistry (general) (51)28222124222220Condensed Matter Physics (3)2222232Electromagnetism and Photonics (11)4427266Genetics (16)8545667High-energy particle physics (33)18201615171817Inorganic Chemistry (3)2211221Molecular Biology (62)30302632303224Optics and Acoustics (1)1110111Organic Chemistry (127)76716465658068Physical Chemistry (1)0000000Physics (general) (36)16151215161718Quantum Mechanics (53)22252023212224Relativistic Mechanics (9)4522344Statistical Mechanics (4)1211221Total234224196214210237217TABLE VLINEAR REGRESSION TO MODEL THE CORRECTNESS OF THE SEVEN PROMPTSDirect Answer ZS CoTCoTDecomposition Self-Ask Self-Consistency MultiPathCoefficient (Slope)0.53920.50770.43830.47170.47450.52810.4722Intercept-1.7721-3.1078-3.36153.4674-1.11401.88733.9226R 2 Score0.99920.99840.99650.99670.99880.99820.9969</p>
<p>Large language models and cognitive science: A comprehensive review of similarities, differences, and challenges. Q Niu, J Liu, Z Bi, 2024published online Sept 3</p>
<p>Mulcogbench: A multi-modal cognitive benchmark dataset for evaluating chinese and english computational language models. Y Zhang, X Zhang, C Li, S Wang, C Zong, 2024. March 2</p>
<p>Large language models for mathematical reasoning: Progresses and challenges. J Ahn, R Verma, R Lou, D Liu, R Zhang, W Yin, 2024. Jan 31</p>
<p>Explaining and improving contrastive decoding by extrapolating the probabilities of a huge and hypothetical lm. H.-S Chang, N Peng, M Bansal, A Ramakrishna, T Chung, 2024. Nov 3</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. D Rein, B L Hou, A C Stickland, 2023. Nov 20</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , D Guo, D Yang, 2025. Jan 22</p>
<p>Iteration of thought: Leveraging inner dialogue for autonomous large language model reasoning. S K Radha, Y N Jelyani, A Ghukasyan, O Goktas, 2024published online Sept 19</p>
<p>Can stories help llms reason? curating information space through narrative. V S Javadi, J R Trippas, Y K Lal, L Flek, 2024. Oct 24</p>
<p>Mtmt: Consolidating multiple thinking modes to form a thought tree for strengthening llm. C Li, X Wang, Q Chen, X Zhou, H Chen, 2024. Dec 5</p>
<p>Boosting scientific concepts understanding: Can analogy from teacher models empower student models?. S Yuan, C Jiayang, L Qiu, D Yang, 2024. June 17</p>
<p>Refining llms outputs with iterative consensus ensemble (ice). M Omar, B S Glicksberg, G N Nadkarni, E Klang, 2024. Dec 28</p>
<p>Reducing hallucination in structured outputs via retrieval-augmented generation. P Béchard, O M Ayala, 2024. April 11</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, 2022. Oct 5</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, 2022. Jan 27</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, 2022. March 21</p>
<p>A systematic survey of prompt engineering in large language models: Techniques and applications. P Sahoo, A K Singh, S Saha, V Jain, S Mondal, A Chadha, 2024. Feb 5</p>
<p>Automatic chain of thought prompting in large language models. Z Zhang, A Zhang, M Li, A Smola, 2022. Oct 7</p>
<p>Logicot: Logical chain-of-thought instruction-tuning. H Liu, Z Teng, L Cui, C Zhang, Q Zhou, Y Zhang, 2023. May 20</p>
<p>Chain-of-symbol prompting elicits planning in large language models. H Hu, H Lu, H Zhang, Y.-Z Song, W Lam, Y Zhang, 2023. May 17</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, 2023. May 17</p>
<p>Thread of thought: Unraveling chaotic contexts. Y Zhou, X Geng, T Shen, 2023. Nov 15</p>
<p>Chain-of-table: Evolving tables in the reasoning chain for table understanding. Z Wang, H Zhang, C.-L Li, 2024. Jan 9</p>
<p>Reasoning with language model prompting: A survey. S Qiao, Y Ou, N Zhang, 2022. Dec 19</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, 2022. Oct 17</p>
<p>Mcc-kd: Multi-cot consistent knowledge distillation. H Chen, S Wu, X Quan, R Wang, M Yan, J Zhang, 2023. Oct 23</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, 2022. June 9</p>
<p>Evaluating large language models trained on code. M Chen, J Tworek, H Jun, 2021. July 7</p>
<p>Blimp: The benchmark of linguistic minimal pairs for english. A Warstadt, A Parrish, H Liu, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Cogbench: A large language model walks into a psychology lab. J Coda-Forno, M Binz, J X Wang, E Schulz, 2024. Feb 28</p>
<p>Measuring progress on scalable oversight for large language models. S R Bowman, J Hyun, E Perez, 2022. Nov 4</p>
<p>Problems with cosine as a measure of embedding similarity for high frequency words. K Zhou, K Ethayarajh, D Card, D Jurafsky, 2022. May 10</p>
<p>Mpnet: Masked and permuted pre-training for language understanding. K Song, X Tan, T Qin, J Lu, T.-Y Liu, 2020. April 20</p>
<p>User's guide to correlation coefficients. H Akoglu, Turkish Journal of Emergency Medicine. 182018</p>
<p>s1: Simple testtime scaling. N Muennighoff, Z Yang, W Shi, 2025. Jan 31</p>
<p>Do not think that much for 2+3=? on the overthinking of o1-like llms. X Chen, J Xu, T Liang, 2024. Dec 30</p>
<p>. None, </p>            </div>
        </div>

    </div>
</body>
</html>
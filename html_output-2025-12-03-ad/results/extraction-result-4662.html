<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4662 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4662</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4662</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-c36b4aec0c26f1ff5b3cf7e86c0b90f51575ebea</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c36b4aec0c26f1ff5b3cf7e86c0b90f51575ebea" target="_blank">Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work introduces the first dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment, and develops novel methods measuring their mastery of case information and reasoning skills.</p>
                <p><strong>Paper Abstract:</strong> In this study, we explore the application of Large Language Models (LLMs) in \textit{Jubensha}, a Chinese detective role-playing game and a novel area in Artificial Intelligence (AI) driven gaming. We introduce the first dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment. Our work also presents a unique multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in this game. To evaluate the gaming performance of these AI agents, we developed novel methods measuring their mastery of case information and reasoning skills. Furthermore, we incorporated the latest advancements in in-context learning to improve the agents' performance in information gathering, murderer identification, and logical reasoning. The experimental results validate the effectiveness of our proposed methods. This work aims to offer a novel perspective on understanding LLM capabilities and establish a new benchmark for evaluating large language model-based agents.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4662.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4662.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ThinkThrice Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ThinkThrice LLM-based Multi-Agent Jubensha Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven multi-agent framework (ThinkThrice) for playing Jubensha (a multi-player detective text game) that augments base LLMs with an external per-agent memory retriever (MR), a Self-Refinement (SR) module to add relevant scripted facts to answers, and a Self-Verification (SV) module to check for hallucinations and enforce authenticity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ThinkThrice LLM-based agent (with MR, SR, SV variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents are LLM instances (GPT-3.5 by default in gameplay; GPT-4 evaluated for reasoning) that are given a private character script and play a role in a multi-agent Jubensha game. Agents produce dialog actions using a prompt that incorporates character summary, game rules, current dialogue, and retrieved memory fragments; variants differ by whether they (a) use an external memory retriever (MR), (b) apply Self-Refinement (SR) to augment initial answers with script facts, and (c) run Self-Verification (SV) to verify facts against scripts and re-generate if needed.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k-0613) for gameplay; GPT-4 (gpt-4-1106preview / gpt-4-0613) used for some evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jubensha (scripted Chinese multi-player detective role-playing game)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents were evaluated on: (1) Factual Question Answering (how much factual case information the agent gathered about itself and other players), (2) Inferential Question Answering (reasoning using collected information), (3) murderer identification voting and civilian win rate, and (4) qualitative/human evaluation of response naturalness/authenticity/informativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Retrieval-augmented external memory (per-agent vector database) combined with in-context/script access; MR is an explicit external episodic/dialogue memory store used for retrieval-augmented generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Text observations (dialogue turns and observed events) are stored as dense vector embeddings (OpenAI embeddings APIs: text-embedding-ada-002 for gameplay, text-embedding-3-large for evaluation) in a per-agent vector DB.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>All observations of an agent (dialogue turns, events) are recorded into the agent's exclusive vector database as they occur (i.e., updated continuously/after each observation).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>At decision time, Faiss similarity search retrieves the top-k (top 5) highest-similarity memory fragments (embeddings) from the agent's vector DB; these retrieved memory fragments are concatenated into the current prompt given to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>With MR (Memory Retriever) alone, factual-other accuracy (average on others' questions) rose from 0.305 (No MR avg) to 0.402 (MR avg). Similarity between chat history and all players' scripts (OpenAI embedding cosine) increased from 0.062 (No MR) to 0.798 (MR). Full architecture MR+SR+SV (N=3 attempts) achieved best factual QA Avg = 0.498 (Table 5) and the highest script-chat similarity 0.831 (OpenAI embedding) (Table 6). MR+SR+SV(N=3) also achieved the highest civilian win rate and murderer identification accuracy among compared architectures (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No MR (memoryless) baseline: factual QA Own Q avg = 0.770, Other's Q avg = 0.305 (CQ 0.300, MQ 0.321) (Table 5). Similarity scores (script vs chat) were very low: OpenAI embedding 0.062, TF-IDF 0.000, Trigrams 0.000 (Table 6). Win rate and identification accuracy for No MR were substantially lower than configurations with MR+SR+SV (reported qualitatively and shown in Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The paper conducts ablations across module combinations: No MR (memoryless) vs MR vs MR+SR vs MR+SR+SV with different SV attempt budgets (N=1,3). Key quantitative findings: (a) MR substantially improves agents' ability to answer questions about other players (Other's Q avg from 0.305 to 0.402). (b) Adding SR increases Other's Q avg to 0.471. (c) Adding SV (N=3) yields the best factual QA Avg = 0.498 and highest script-chat similarity (OpenAI embedding 0.831). Similarity metrics (embedding/TF-IDF/trigrams) steadily increase across MR→MR+SR→MR+SR+SV. Qualitative ablation (Table 7) shows progressive improvement in relevance and correctness of reasoning premises as modules are added. The paper also compares 'Full Script Access' (oracle) vs MR variants: Full Script Access yields highest inferential accuracy, MR+SR+SV is second.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reported issues related to memory usage and general system behavior include: (1) Limited LLM context window motivates external memory — without it agents cannot retain multi-round game history. (2) LLM hallucination is common; hence SV was introduced. (3) Stochasticity of LLM outputs causes variability in results (voting variability and divergent game traces). (4) Cost and reproducibility: embeddings, retrieval, and repeated SV attempts add compute and monetary cost; model versioning may affect replicability. (5) MR depends on retrieval quality and embedding choice; simple top-5 retrieval may still lack precise temporal or structured grounding (e.g., missing exact time details can weaken inference).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>The authors' empirical recommendations and design choices are: (1) Use an external per-agent vector memory (embeddings + Faiss) to overcome context-window limits and record episodic dialogue/events. (2) Store raw observations (dialogue turns/events) as embeddings and retrieve top-k (they used k=5) similar fragments for prompt augmentation. (3) Combine retrieval with reasoning-time modules: Self-Refinement (use script facts to decompose questions and add missing facts) to make answers more informative, and Self-Verification (compare extracted facts to scripts and enforce authenticity thresholds) to reduce hallucinations. (4) Use explicit authenticity thresholds (the paper gives example thresholds) and score candidate answers, keeping the highest-scoring output across multiple SV attempts (they tested N=1 and N=3). (5) For complex inferential tasks, stronger LLMs (GPT-4) markedly improve use of retrieved information; so pair retrieval+SR+SV with more capable models where possible. (6) Log and release chat histories and voting records to allow downstream analysis due to stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations_short</strong></td>
                            <td>Memory must be external because of limited LLM context; retrieval quality/time alignment matters; hallucination risk persists and increases cost via verification loops; experimental variability and cost limit scale.</td>
                        </tr>
                        <tr>
                            <td><strong>notes_on_implementation_details</strong></td>
                            <td>Memory embeddings: text-embedding-ada-002 (gameplay) and text-embedding-3-large (evaluation). Retrieval implementation: Faiss similarity search. Retrieved top-5 memory items included in prompts. Self-Verification scoring uses accuracy of retrieved facts, count of corrected facts, and response length; example authenticity thresholds and scoring formula are provided in Appendix G.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Can large language models play text games well? current state-of-the-art and open questions <em>(Rating: 2)</em></li>
                <li>Exploring large language models for communication games: An empirical study on werewolf <em>(Rating: 2)</em></li>
                <li>Human-level play in the game of diplomacy by combining language models with strategic reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4662",
    "paper_id": "paper-c36b4aec0c26f1ff5b3cf7e86c0b90f51575ebea",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "ThinkThrice Agent",
            "name_full": "ThinkThrice LLM-based Multi-Agent Jubensha Agent",
            "brief_description": "An LLM-driven multi-agent framework (ThinkThrice) for playing Jubensha (a multi-player detective text game) that augments base LLMs with an external per-agent memory retriever (MR), a Self-Refinement (SR) module to add relevant scripted facts to answers, and a Self-Verification (SV) module to check for hallucinations and enforce authenticity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ThinkThrice LLM-based agent (with MR, SR, SV variants)",
            "agent_description": "Agents are LLM instances (GPT-3.5 by default in gameplay; GPT-4 evaluated for reasoning) that are given a private character script and play a role in a multi-agent Jubensha game. Agents produce dialog actions using a prompt that incorporates character summary, game rules, current dialogue, and retrieved memory fragments; variants differ by whether they (a) use an external memory retriever (MR), (b) apply Self-Refinement (SR) to augment initial answers with script facts, and (c) run Self-Verification (SV) to verify facts against scripts and re-generate if needed.",
            "llm_model_name": "GPT-3.5 (gpt-3.5-turbo-16k-0613) for gameplay; GPT-4 (gpt-4-1106preview / gpt-4-0613) used for some evaluations",
            "game_or_benchmark_name": "Jubensha (scripted Chinese multi-player detective role-playing game)",
            "task_description": "Agents were evaluated on: (1) Factual Question Answering (how much factual case information the agent gathered about itself and other players), (2) Inferential Question Answering (reasoning using collected information), (3) murderer identification voting and civilian win rate, and (4) qualitative/human evaluation of response naturalness/authenticity/informativeness.",
            "memory_used": true,
            "memory_type": "Retrieval-augmented external memory (per-agent vector database) combined with in-context/script access; MR is an explicit external episodic/dialogue memory store used for retrieval-augmented generation.",
            "memory_representation": "Text observations (dialogue turns and observed events) are stored as dense vector embeddings (OpenAI embeddings APIs: text-embedding-ada-002 for gameplay, text-embedding-3-large for evaluation) in a per-agent vector DB.",
            "memory_update_mechanism": "All observations of an agent (dialogue turns, events) are recorded into the agent's exclusive vector database as they occur (i.e., updated continuously/after each observation).",
            "memory_retrieval_mechanism": "At decision time, Faiss similarity search retrieves the top-k (top 5) highest-similarity memory fragments (embeddings) from the agent's vector DB; these retrieved memory fragments are concatenated into the current prompt given to the LLM.",
            "performance_with_memory": "With MR (Memory Retriever) alone, factual-other accuracy (average on others' questions) rose from 0.305 (No MR avg) to 0.402 (MR avg). Similarity between chat history and all players' scripts (OpenAI embedding cosine) increased from 0.062 (No MR) to 0.798 (MR). Full architecture MR+SR+SV (N=3 attempts) achieved best factual QA Avg = 0.498 (Table 5) and the highest script-chat similarity 0.831 (OpenAI embedding) (Table 6). MR+SR+SV(N=3) also achieved the highest civilian win rate and murderer identification accuracy among compared architectures (Figure 3).",
            "performance_without_memory": "No MR (memoryless) baseline: factual QA Own Q avg = 0.770, Other's Q avg = 0.305 (CQ 0.300, MQ 0.321) (Table 5). Similarity scores (script vs chat) were very low: OpenAI embedding 0.062, TF-IDF 0.000, Trigrams 0.000 (Table 6). Win rate and identification accuracy for No MR were substantially lower than configurations with MR+SR+SV (reported qualitatively and shown in Figure 3).",
            "has_performance_comparison": true,
            "ablation_or_analysis": "The paper conducts ablations across module combinations: No MR (memoryless) vs MR vs MR+SR vs MR+SR+SV with different SV attempt budgets (N=1,3). Key quantitative findings: (a) MR substantially improves agents' ability to answer questions about other players (Other's Q avg from 0.305 to 0.402). (b) Adding SR increases Other's Q avg to 0.471. (c) Adding SV (N=3) yields the best factual QA Avg = 0.498 and highest script-chat similarity (OpenAI embedding 0.831). Similarity metrics (embedding/TF-IDF/trigrams) steadily increase across MR→MR+SR→MR+SR+SV. Qualitative ablation (Table 7) shows progressive improvement in relevance and correctness of reasoning premises as modules are added. The paper also compares 'Full Script Access' (oracle) vs MR variants: Full Script Access yields highest inferential accuracy, MR+SR+SV is second.",
            "challenges_or_limitations": "Reported issues related to memory usage and general system behavior include: (1) Limited LLM context window motivates external memory — without it agents cannot retain multi-round game history. (2) LLM hallucination is common; hence SV was introduced. (3) Stochasticity of LLM outputs causes variability in results (voting variability and divergent game traces). (4) Cost and reproducibility: embeddings, retrieval, and repeated SV attempts add compute and monetary cost; model versioning may affect replicability. (5) MR depends on retrieval quality and embedding choice; simple top-5 retrieval may still lack precise temporal or structured grounding (e.g., missing exact time details can weaken inference).",
            "best_practices_or_recommendations": "The authors' empirical recommendations and design choices are: (1) Use an external per-agent vector memory (embeddings + Faiss) to overcome context-window limits and record episodic dialogue/events. (2) Store raw observations (dialogue turns/events) as embeddings and retrieve top-k (they used k=5) similar fragments for prompt augmentation. (3) Combine retrieval with reasoning-time modules: Self-Refinement (use script facts to decompose questions and add missing facts) to make answers more informative, and Self-Verification (compare extracted facts to scripts and enforce authenticity thresholds) to reduce hallucinations. (4) Use explicit authenticity thresholds (the paper gives example thresholds) and score candidate answers, keeping the highest-scoring output across multiple SV attempts (they tested N=1 and N=3). (5) For complex inferential tasks, stronger LLMs (GPT-4) markedly improve use of retrieved information; so pair retrieval+SR+SV with more capable models where possible. (6) Log and release chat histories and voting records to allow downstream analysis due to stochasticity.",
            "challenges_or_limitations_short": "Memory must be external because of limited LLM context; retrieval quality/time alignment matters; hallucination risk persists and increases cost via verification loops; experimental variability and cost limit scale.",
            "notes_on_implementation_details": "Memory embeddings: text-embedding-ada-002 (gameplay) and text-embedding-3-large (evaluation). Retrieval implementation: Faiss similarity search. Retrieved top-5 memory items included in prompts. Self-Verification scoring uses accuracy of retrieved facts, count of corrected facts, and response length; example authenticity thresholds and scoring formula are provided in Appendix G.",
            "uuid": "e4662.0",
            "source_info": {
                "paper_title": "Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Can large language models play text games well? current state-of-the-art and open questions",
            "rating": 2,
            "sanitized_title": "can_large_language_models_play_text_games_well_current_stateoftheart_and_open_questions"
        },
        {
            "paper_title": "Exploring large language models for communication games: An empirical study on werewolf",
            "rating": 2,
            "sanitized_title": "exploring_large_language_models_for_communication_games_an_empirical_study_on_werewolf"
        },
        {
            "paper_title": "Human-level play in the game of diplomacy by combining language models with strategic reasoning",
            "rating": 1,
            "sanitized_title": "humanlevel_play_in_the_game_of_diplomacy_by_combining_language_models_with_strategic_reasoning"
        }
    ],
    "cost": 0.01218775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games</h1>
<p>Dekun Wu ${ }^{\dagger}$, Haochen Shi ${ }^{\dagger}$, Zhiyuan Sun and Bang Liu ${ }^{\dagger}$<br>Université de Montréal \&amp; Mila - Quebec AI Institute<br>{dekun.wu, haochen.shi, zhiyuan.sun, bang.liu}@umontreal.ca</p>
<h4>Abstract</h4>
<p>In this study, we explore the application of Large Language Models (LLMs) in Jubensha, a Chinese detective role-playing game and a novel area in Artificial Intelligence (AI) driven gaming. We introduce the first dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment. Our work also presents a unique multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in this game. To evaluate the gaming performance of these AI agents, we developed novel methods measuring their mastery of case information and reasoning skills. Furthermore, we incorporated the latest advancements in in-context learning to improve the agents' performance in information gathering, murderer identification, and logical reasoning. The experimental results validate the effectiveness of our proposed methods. This work aims to offer a novel perspective on understanding LLM capabilities and establish a new benchmark for evaluating large language model-based agents.</p>
<h2>1 Introduction</h2>
<p>Interactive role-playing games, where players unravel mysteries through strategic interactions and clue-based puzzles, have seen a significant rise in global popularity. Tracing their origins to 19thcentury murder mystery novels and early 20thcentury party puzzles, these games have particularly flourished in China in recent years. In this context, they are referred to as "Jubensha", "Scripted Murders" or 刷本杀 in Chinese, and have experienced a remarkable surge in popularity since the late 2010s. Influenced by Western murder mystery games, Jubensha games center around players who gather to identify the murderer by analyzing and interpreting the provided story scripts. The thrill</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the Jubensha game. It requires players to interact with each other and reason about who is the murderer in a story.
for players to collect and interpret clues through intense social interaction and reasoning has made playing Jubensha stand out as a notable cultural phenomenon.</p>
<p>Meanwhile, the field of Artificial Intelligence (AI) is reshaping the gaming landscape. From classic games like chess (Campbell et al., 2002), Go (Silver et al., 2016), and poker (Brown and Sandholm, 2019; Brown et al., 2020) to video games like StarCraft (Vinyals et al., 2017, 2019), League of Legends (Lohokare et al., 2020), and Honor of Kings (Ye et al., 2020), AI contenders or collaborators have been integrated into these games. The recent surge in Large Language Models (LLMs) has redirected research interest from conventional and video games to text-based games. Significant examples include an LLM deployed as agents in the strategic game CICERO ( $F A I R$ et al., 2022), in the communicative game Werewolf (Xu et al., 2023), and in the text-based adventure game Zork (Tsai et al., 2023).</p>
<p>However, the "Jubensha game" remains an undeveloped field in terms of AI agents specifically</p>
<p>tailored for its gameplay and evaluation. We believe there are several reasons. First, before the emergence of LLMs, understanding the character plots, role tasks, and game rules in the script of Jubensha games was very challenging. Not to mention the need for AI agents to engage in multi-round linguistic interactions, information gathering, and logical reasoning. Second, currently there is no publicly available dataset specifically for Jubensha games that researchers can use to develop and evaluate their agents. Third, quantitatively and qualitatively automating the evaluation of AI agents’ performance in Jubensha games is also very difficult. The most commonly used win-rate metric in other game AIs is also of limited use in assessing the performance of Jubensha game agents, i.e., win rate does not show the extent of the Jubensha AI’s mastery of case information or the level of its reasoning ability.</p>
<p>In response to the above-mentioned challenges, we attempt to provide a solution in this work. This work focuses on constructing a multi-agent interaction framework in a “Jubensha” game environment using large language models, and we have designed a set of methods to quantitatively and qualitatively assess the performance of Large Language Model based (LLM-based) agents. Our contributions can be summarized as follows:</p>
<ol>
<li>
<p>We have created a Chinese dataset providing character scripts and preset game rules to initiate a Jubensha game. To the best of our knowledge, this is the first dataset specifically tailored for AI agents playing in a Jubensha game setting.</p>
</li>
<li>
<p>We designed a framework for multi-agent interaction in a Jubensha game environment using large language models, allowing multiple LLM-based agents to autonomously interact with each other in a Jubensha game setting, without the need for human intervention.</p>
</li>
<li>
<p>To quantitatively and qualitatively assess the performance of large language model based agents in Jubensha games, we designed two novel tasks: one to evaluate their mastery of case information and another to assess their reasoning abilities with the information collected during the game.</p>
</li>
<li>
<p>Utilizing the latest advancements in the field of in-context learning, we devised modules to enhance the performance of LLM-based agents. Our evaluations show that this design significantly enhances the information mastery, murderer identification, and reasoning capabilities of LLM-based agents in the Jubensha game context.</p>
</li>
</ol>
<h2>2 Related Work</h2>
<p>Interactive Role-Playing Games IRPGs provide immersive experiences in fictional settings, serving as a multidisciplinary research testbed in various fields (Zagal and Deterding, 2018; Barreteau et al., 2003; Nagata et al., 2021). These games are categorized into SRPGs and MRPGs, each with unique research opportunities. <em>SRPGs</em> focus on character advancement through quests in various themes (Infocom, 1980; Côté et al., 2019; Szot et al., 2021; Fan et al., 2022; Shridhar et al., 2020; Wang et al., 2022a). While <em>MRPGs</em> highlight collaborative storytelling (Lai et al., 2023; Park et al., 2023; Fu et al., 2023; Kramár et al., 2022; <em>FAIR</em> et al., 2022). This work presents a dataset for MRPG Jubensha, aimed at supporting studies on the development and evaluation of communicative and reasoning AI in an adversarial game setting.</p>
<p>LLM-based Autonomous Agents. Thanks to their strong capabilities in language comprehension and reasoning, LLMs have demonstrated significant potential for achieving impressive performance in tasks that focus on advancing characters. (Wang et al., 2023a; Wei et al., 2022; Wang et al., 2022b; Yao et al., 2023, 2022; Shinn et al., 2023). However, only a few works explore the multi-agent interaction under complex settings (Xu et al., 2023; Junprung, 2023; Lai et al., 2023). To bridge the gap, we propose the first LLM-based multi-agent interaction framework on Jubensha, allowing autonomous agent engaging and facilitating further study on advanced study topics.</p>
<p>Evaluating LLM-based Agents. Despite the quick development of LLM-based agents, assessing their abilities in MRPGs can still be essential and challenging. Recent initiatives (Wang et al., 2023b; Shao et al., 2023; Liang et al., 2023; Xu et al., 2023) propose several simple subjective and objective evaluation metrics for LLM-based agents under multi-role settings. However, it can still be challenging to assess the capabilities of LLM-based agents for MRPGs using simple evaluation metrics, such as win rate. To address this issue, we propose a set of systematic and objective evaluation</p>
<p>methods designed to measure the information gathering and reasoning abilities of LLM-based agents in Jubensha.</p>
<h2>3 Jubensha Dataset</h2>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Text</th>
<th style="text-align: center;">Image</th>
<th style="text-align: center;">Audio</th>
<th style="text-align: center;">Video</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">#</td>
<td style="text-align: center;">1115</td>
<td style="text-align: center;">643</td>
<td style="text-align: center;">392</td>
<td style="text-align: center;">172</td>
<td style="text-align: center;">1115</td>
</tr>
</tbody>
</table>
<p>Table 1: Number of Jubensha game scripts by modality in our dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Players</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Tokens</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min</td>
<td style="text-align: center;">$\max$</td>
<td style="text-align: center;">avg</td>
<td style="text-align: center;">min</td>
<td style="text-align: center;">$\max$</td>
<td style="text-align: center;">avg</td>
</tr>
<tr>
<td style="text-align: center;">#</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">6.52</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">518 k</td>
<td style="text-align: center;">129 k</td>
</tr>
</tbody>
</table>
<p>Table 2: Statics on number of players and tokens for the Jubensha game scripts in our dataset.</p>
<p>To enhance AI deployment in Jubensha games, we have compiled a comprehensive dataset from over 1,100 Jubensha game instances in Chinese. ${ }^{1}$ This dataset is a valuable addition to MRPG research, presenting unique challenges and opportunities for advancement. ${ }^{2}$</p>
<p>Background of Jubensha Game Jubensha is a detective role-playing game with multiple players, where each player is assigned a unique role tied to a central murder mystery. As shown in Figure 1, the game process typically consists of six stages: 1) Each player selects a script for distinct characters in a Jubensha game. 2) Players are assigned with a role (murderer or civilian) associated with their selected scripts. 3) The players read their scripts to develop a basic understanding of the whole story from their views. 4) Each player is given a pool of clues to help them reveal or hide critical details for finding the murderer. 5) Several rounds of group discussion are held among the players to share information and find out the murderer. 6). Finally, each player anonymously votes to decide the murderer. The civilians win the game if the true murderer gets the most votes, otherwise, the murderer wins.</p>
<p>Dataset Construction To establish an environment capable of evaluating Jubensha agents and to facilitate future scaled-up works, we collect 1,115 instances of Jubensha games in Chinese online.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Each game consists of a host manual describing how to control the game process and a God's-eye view of case replays, along with individual scripts for each character in the game. As demonstrated in Table 2, the number of players can vary from 1 to 20 , and the number of tokens for the game can be as large as 518 k , facilitating further research on socially intelligent AI and introducing extra-long text comprehension and reasoning challenges. Besides, as shown in Table 1, some of these scripts also contain multimodal clues, including audio and video. To create a unified experimental environment, this work concentrates exclusively on textmodality Jubensha games.</p>
<h2>4 The ThinkThrice Framework for Jubensha Games</h2>
<p>We have designed an interaction framework for LLM-based agents specifically crafted for Jubensha games. In this framework, each LLM-based agent is responsible for playing the role of a player in the Jubensha game narrative. Typically, each Jubensha game involves 4-5 players portrayed by LLM-based agents. Besides, we have a non-player character serving as the host, who is responsible for guiding the game. Unlike other players driven by large language models, the host's questions and commands are pre-set to ensure the game follows a designated process. As for the LLM-based agent players, they are embedded with character information specific to their roles. Based on this information, they interact with other characters including other players and the host. These interactions are synchronous, meaning only one agent acts at a specific moment. All actions of the LLM-based agent players, such as whom to question or how to respond to questions from other players or the host, are generated by an LLM based on their character scripts and historical chat records, as shown in Figure 2. Moreover, we have also set a finite number of rounds for each game to mimic the time-limited rules of real Jubensha games. Although currently focusing on Jubensha, our framework design is flexible, potentially requiring minimal adjustments for application in other contexts.</p>
<p>We name our framework for multi-agent mystery games as ThinkThrice, or 三思 in Chinese. The name comes from a Chinese proverb: "thinking thrice before acting (三思而后行)". Figure 2 illustrates the overall design of our ThinkThrice framework, which outlines the process by which a player</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of our proposed ThinkThrice framework for enhancing agent's performance in multi-agent detective games (i.e., Jubensha). The three different colors of the arrows indicate the data flows of three stages: 1) Initial answer generation with Memory Retrieval; 2) Enhance answer with Self-Refinement; 3) Verify answer with Self-Verification. The brown texts in the refined answer are new information added to the initial answer.</p>
<p>Generates a response to a question through three main stages: Memory Retrieval, Self-Refinement, and Self-Verification.</p>
<p><strong>Memory Retrieval</strong> Due to the limited context window of LLMs, a special memory retrieval module is often needed to store and appropriately retrieve all historical records observed by LLM-based agents (Park et al., 2023; Lai et al., 2023). Our framework adopts this widely used method to help agents remember dialogues and events in the Jubensha games and retrieve specific memory fragments when needed. Specifically, we record all observations of the agent into a vector database exclusive to the agent using the OpenAI API (OpenAI, 2023). When the agent observes a new event requiring action, we use the Faiss library to quickly search the agent's exclusive vector database for multiple historical records with the top 5 highest similarities and include them in the current action prompt. We have observed that this simple module is also very helpful in improving the performance of LLM-based agents in our Jubensha games.</p>
<p><strong>Self-Refinement</strong> The goal of the Self-Refinement module is to ensure that LLM-based agents provide as much information and details as possible when responding to questions from others. As shown in Figure 2, when an agent receives a question from others, it first attempts to generate an initial answer internally (see the green lines). During the Self-Refinement stage, the agent decomposes the asked question <em>Q</em> into several sub-questions: <em>q</em><sup>1</sup>, <em>q</em><sup>2</sup>, . . . , <em>q</em><sub><em>n</em></sub>. It then uses these sub-questions to retrieve relevant facts from the character scripts <em>S</em> that can be used to answer these sub-questions. For each relevant fact, the agent checks whether they are included in the initial answer. If not, they are added to form part of the refined answer (see the orange lines in Figure 2). We can see that the refined answer, after the Self-Refinement module, contains more details and information than the initial answer, thereby enhancing the agent's communication efficiency within a limited number of rounds.</p>
<p><strong>Self-Verification</strong> LLM hallucinations are a very common and thorny problem (Dhuliawala et al., 2023). To ensure that the content of LLM-based agents' answers is authentic and not hallucinated, we designed a Self-Verification module. This module breaks down the agent's answer <em>A</em> into multiple facts <em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, . . . , <em>a</em><sub><em>n</em></sub> and then compares these facts one by one with the agent's character script <em>S</em> for authenticity. Only when the agent's answer meets our preset authenticity threshold (measured by the absolute number and accuracy of real facts) will the agent output the answer (see the blue lines in Figure 2). If the answer does not meet the preset</p>
<p><sup>3</sup>https://github.com/facebookresearch/faiss</p>
<p>threshold, the agent will need to regenerate the answer until the authenticity of the agent's answer meets the requirements or exceeds the maximum number of attempts. Since the agent's best output may not necessarily occur in the last attempt, we score each answer based on the degree of authenticity and the number of words, keeping a copy of the highest-scoring answer each time. By doing so, we can ensure that even if all of the agent's attempts fail to meet the authenticity threshold, it will still output the highest quality answer from all its previous attempts.</p>
<h2>5 Evaluating LLM-based Agents in Jubensha Games</h2>
<p>Previous work has primarily employed metrics such as human-likeness and win rate to assess the performance of LLM-based agents in games (Wang et al., 2023a; Lai et al., 2023). These metrics either require substantial human involvement or likely leading to less reliable experimental conclusions due to the challenges in controlling variables (Lai et al., 2023). ${ }^{4}$ Considering the unique characteristics of Jubensha games, we have designed two tasks to quantitatively and qualitatively evaluate the performance of LLM-based agents in Jubensha games: Factual Question Answering and Inferential Question Answering.</p>
<p>Factual Question Answering The process of gathering information in Jubensha games is crucial for LLM-based agents to understand the implied relationships and conflicts in the story, reconstruct the entire case, and deduce the truth. To evaluate how much information LLM-based agents gathered in Jubensha games, we designed a factual questionanswering task and use GPTs to generate questions. We have generated a total of 720 factual questions for 4 selected Jubensha games containing 4 or 5 players, of which two examples are presented in Table 3. Due to limited space, we have included the prompts of our generation method in the appendix H .</p>
<p>Inferential Question Answering Once LLMbased agents have gathered the necessary information, another important step is using it for inference. This step is particularly crucial in Jubensha games, where the truth of the case is often obscured by numerous clues. To evaluate the reasoning abili-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Example 1
Question: Who does Mate Zhang's brother work for?
Answer: Boss Yang
Source: He works for Boss Yang, owner of a famous bar in City A. (from Mate Zhang's script)</p>
<p>Example 2
Question: When did Dr. Ling meet Mate Zhang?
Answer: 18:20
Source: 18:20, you went to meet Boss Yang, and met Mate Zhang just as you were about to arrive. (from Dr. Ling's script)</p>
<p>Table 3: Two examples of Factual Questions.</p>
<p>Question: Who is most likely to have made the thumping sound that Dr. Ling heard on her way to kill Boss Yang? A. Secretary Wen, B. Boss Yang, C. Boatman Wang, D. Mate Zhang, E. Others.
Answer: A. Secretary Wen
Rationale (GT): {Secretary Wen left Boss Yang's room through the ventilation duct at 23:00} PREMISE 1, and {Dr. Ling prepared to kill Boss Yang through the ventilation duct also at 23:00} PREMISE 2. Therefore, it can be inferred that ${$ the 'thumping' sound Dr. Ling heard on her way to kill Boss Yang was most likely made by Secretary Wen $}$ CONCLUSION.</p>
<p>Table 4: Example of Inferential Questions.
ties of LLM-based agents, we manually designed a total of 56 inferential questions for 4 selected Jubensha games, of which an example is shown in Table 4. Note that the PREMISE 1 is from Secretary Wen's script and the PREMISE 2 is from Dr. Ling's script. To answer these questions successfully, LLM-based agents must integrate information collected from different characters and make inferences based on this information.</p>
<h2>6 Experiment</h2>
<p>In this section, we will briefly discuss the various LLMs used in this work. Following this, we will present our experimental results, encompassing the evaluation of the information gathering and reasoning performance of LLM-based agents.</p>
<p>|  | Own Q | Other's Q |  |  |
|  | Avg | CQ | MQ | Avg |
| --- | --- | --- | --- | --- |
| No MR | 0.770 | 0.300 | 0.321 | 0.305 |
| MR | 0.759 | 0.409 | 0.380 | 0.402 |
| MR+SR | 0.757 | 0.467 | 0.487 | 0.471 |
| MR+SR+SV(N=1) | 0.768 | 0.485 | $\mathbf{0 . 5 1 8}$ | 0.492 |
| MR+SR+SV(N=3) | $\mathbf{0 . 7 7 2}$ | $\mathbf{0 . 4 9 5}$ | 0.514 | $\mathbf{0 . 4 9 8}$ |</p>
<p>Table 5: Performance of agents on different kinds of factual questions. "Own Q" refers to questions generated from the agent's own script, while "Other's Q" refers to questions from scripts of other agents, inaccessible to the answering agent. "Other's Q" includes "CQ" (questions from other civilians' scripts) and "MQ" (questions from the murderer agent's script). The effectiveness of different module combinations in the agents is assessed based on agents' ability to provide factual responses about themselves and others. The number follows "N=" denotes the maximum number attempts an agent can try in the Self-Verification stage.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">OpenAI</th>
<th style="text-align: center;">TF-IDF</th>
<th style="text-align: center;">Trigrams</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No MR</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
</tr>
<tr>
<td style="text-align: left;">MR</td>
<td style="text-align: center;">0.798</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.027</td>
</tr>
<tr>
<td style="text-align: left;">MR+SR</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.645</td>
<td style="text-align: center;">0.064</td>
</tr>
<tr>
<td style="text-align: left;">MR+SR+SV(N=1)</td>
<td style="text-align: center;">0.825</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.075</td>
</tr>
<tr>
<td style="text-align: left;">MR+SR+SV(N=3)</td>
<td style="text-align: center;">0.831</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 7 7}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Similarity scores between all players' scripts and chat histories of agents.</p>
<h3>6.1 Utilization of LLMs in Jubensha Game Stages</h3>
<p>Throughout the gameplay and other stages of the Jubensha game experiment, including the generation of factual questions, agent Q\&amp;A sessions, and the evaluation of agent responses, we predominantly employed OpenAI's GPT-3.5 and GPT-4 models. Unless specifically indicated, the GPT3.5 and the GPT-4 model mentioned in this paper refer to gpt-3.5-turbo-16k-0613 and gpt-4-1106preview respectively. For text embedding we used text-embedding-ada-002 for gameplay and text-embedding-3-large for evaluation. More specific usages of GPT models are listed in the appendix B.</p>
<h3>6.2 Evaluation on Agents' Responses to Factual Questions</h3>
<p>The Table 5 presents the evaluation results on agents' responses to factual questions. We can see that agents with different module combinations performed quite well when answering questions from their own script. This is understandable because agents have full access to their own scripts throughout the game, thereby possessing all the
information needed to correctly answer these questions. Since the Memory Retriever (MR) module mainly records the communication history among agents after the game starts, we can consider agents without the Memory Retriever as being in a memoryless state. The experimental results in the "No MR" row show that agents' accuracy in answering questions about themselves is significantly higher than in answering questions about others. This gap highlights the significant difference between the information agents possess about others and about themselves. After introducing the MR module, we observe a significant improvement in agents' accuracy in answering questions about others, reflecting the increase in information gained from interactions among agents, which helps them better understand the roles and stories of other characters in the game.</p>
<p>The Self-Refinement (SR) and Self-Verification (SV) modules mainly ensure the authenticity and comprehensiveness of information during agent communication. We can observe that the accuracy of agents in answering questions about others achieves the best results among all under the combined effect of the MR module, the SR module, and the SV module (with a maximum of 3 attempts). This demonstrates that our designed modules effectively enhance the efficiency of communication among agents in the Jubensha game, allowing them to acquire more case information under the same game round conditions.</p>
<h3>6.3 Similarities between Agent Chat Histories and All Players' Scripts</h3>
<p>To measure the overall information about all characters acquired by LLM-based agents in the game from the textual perspective, we employed three methods to assess the similarities between the chat histories among agents and the scripts of all players. Firstly, we concatenate all players' scripts into a single document, then we treat the chat histories among agents as another document. Then we used OpenAI API (OpenAI, 2023) to encode these two documents respectively. For documents beyond OpenAI API's max input length, we split them into non-overlapping chunks, average their text embeddings to represent the document. Given embeddings of two documents, their cosine similarity can be calculated. Additionally, we utilized TF-IDF and trigrams to represent the two documents, and then calculated their cosine and Jaccard</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Average win rate of civilian players and the average murderer identification accuracy across different architectures in Jubensha games.</p>
<p>Similarities, respectively. The results from Table 6 show that agents embedded with MR, SR, and SV modules have chat histories in the game that are closer to the scripts of all players, which demonstrates that they have acquired more information about all players in a Jubensha game.</p>
<h3>6.4 Civilian Player Win Rate and Murderer Identification Accuracy</h3>
<p>Figure 3 shows the average win rates of civilian players and the murderer identification accuracy by agents with various module combinations in Jubensha games. Civilian win rate in Figure 3 is the ratio of the number of games won by civilian players to the total number of games played. Murderer identification accuracy is the proportion of votes received by the murderer to the total number of votes cast by all players. We observed that MR+SR+SV(N=3) which performs best in factual question answering tasks, also achieves the highest average civilian players win rate and murderer identification accuracy. This might be due to agents acquiring sufficient information to reconstruct the true narrative of the case, thereby making it easier to accurately identify the murderer.</p>
<h3>6.5 Evaluation of Agent's Responses to Inferential Questions</h3>
<p>To further evaluate the reasoning capabilities of LLM-based agents based on the rationale derived from collected information, we utilized a set of inferential questions. The experimental results are displayed in Figure 4, where the Overall Accuracy represents the rate at which agents correctly answer questions, without considering the rationale behind their responses. In contrast, Informed Accuracy refers to the accuracy achieved when taking into account the rationale provided by agents for their answers. An answer is counted towards Informed Accuracy only if the agent's response is correct and</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: GPT-3.5 and GPT-4's performance with different methods, where overall accuracy measure the raw correct percentage and informed accuracy take LLM's reasoning ability into consideration. FSA stands for 'Full Script Access', indicating that agents have access to the complete scripts of all players.</p>
<p>They provide the correct reasoning rationale. This metric demonstrates that agents not only know the correct answer but also understand why it is correct.</p>
<p>From the results in Figure 4, we can draw two important observations: 1) The more information agents acquire during the game, the more capable they become at solving complex problems through reasoning. For instance, when GPT-4 is employed, the agent with full access to all players' scripts achieves the highest overall accuracy and informed accuracy. Following this, agents equipped with MR+SR+SV(N=3) modules rank second. 2) Given the same amount of information, the LLMs' inherent ability to utilize this information and reason effectively determines the agents' final performance in reasoning tasks. Indeed, simply upgrading from GPT-3.5 to GPT-4 can lead to agents achieving double or even nearly triple the overall accuracy and informed accuracy.</p>
<p><strong>Qualitative Analysis.</strong> In Table 7, we provide a qualitative analysis: agents are presented with an inferential question and the correct answer which the agents do not know to be correct, and are asked to provide the rationale for their reasoning. Each rationale provided by the agents is accompanied by a GPT Eval Score, which measures the similarity between the agents' rationale and the ground truth rationale. The inferential question, answer, and the ground truth rationale can be found in Table 4. From Table 7, we observe that with Full Script Access (complete information), agents can easily identify relevant premises and deduce solid conclusions. In cases where agents have incomplete information, MR+SR+SV (N=3), due to lacking specific details about the exact time Secretary Wen was in the ventilation duct, the conclusion drawn is</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Predicted Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { No } \ &amp; \text { MR } \end{aligned}$</td>
<td style="text-align: center;">{Dr. Ling couldn't find the medical cabinet key when she returned to her room at 18:50, suggesting someone had already taken it} IRRELEVANT PREMISE 1: {Secretary Wen asked Dr. Ling for intelligence at 18:10} IRRELEVANT PREMISE 2. {giving her the opportunity to know Boss Yang's location and possibly plan an action}TENUOUS INTERIM CONCLUSION 3: plus, when Dr. Ling heard the thumping sound, she was on her way to Boss Yang's room, and {there's no clear evidence that other suspects were near Boss Yang's room at that time }FLAWED PREMISE 4: moreover, ${$ the ventilation duct's baffle had fallen, indicating someone had entered Boss Yang's room before her} RELEVANT INTERIM CONCLUSION 5: {considering Secretary Wen's role under Boss Yang, she had the chance to approach him}TANGENTIALLY RELEVANT PREMISE 6. {hence the thumping sound was most likely made by Secretary Wen}TENUOUS CONCLUSION. (GPT Eval Score: 3)</td>
</tr>
<tr>
<td style="text-align: center;">MR</td>
<td style="text-align: center;">{According to the detailed timeline of the day of the incident for Dr. Ling, she heard someone crawling in the ventilation duct at 23:00 when she was preparing to go to Boss Yang's, and she reached his room at 23:20, only to find him already dead}RELEVANT PREMISE 1. {Secretary Wen was the only one aware of Dr. Ling's mission and plan}FLAWED PREMISE 2. {and clues suggest Secretary Wen might have tampered with the intelligence letter, leaving information about the transaction between Dr. Ling and Mate Zhang }IRRELEVANT PREMISE 3. {possibly to make Boss Yang suspicious of Dr. Ling.}IRRELEVANT INTERIM CONCLUSION 4 {This indicates Secretary Wen might have had a motive to be involved in Boss Yang's death.}TANGENTIALLY RELEVANT PREMISE 5 Combining all this information, {Secretary Wen is the most likely person to have been in the ventilation duct and possibly involved in Boss Yang's death $}$ TENUOUS CONCLUSION. (GPT Eval Score: 4)</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { MR } \ &amp; + \ &amp; \text { SR } \ &amp; + \ &amp; \text { SV } \ &amp; (\mathrm{N}=3) \end{aligned}$</td>
<td style="text-align: center;">The sound Dr. Ling heard in the ventilation duct came from Secretary Wen. {According to Dr. Ling's timeline, she entered the ventilation duct at 23:00 and reached Boss Yang's cabin at 23:20, hearing someone crawling inside }RELEVANT PREMISE 1. {Secretary Wen mentioned waiting in the ventilation duct in her timeline $}$ LIMITED PREMISE 2. {so it is likely she made the noise}PLAUSIBLE CONCLUSION. {Mate Zhang and Boatman Wang's actions and timelines do not mention being active in the ventilation duct} PARTIALLY CORRECT PREMISE 3. {Boss Yang was already dead at that time, so the sound couldn't have come from him.}RELEVANT INTERIM CONCLUSION 4 (GPT Eval Score: 5)</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Full } \ &amp; \text { Script } \ &amp; \text { Access } \end{aligned}$</td>
<td style="text-align: center;">{When Dr. Ling was preparing to enter Boss Yang's cabin through the ventilation duct at 23:00, she heard a 'thumping' sound coming from the duct}RELEVANT PREMISE 1. {According to Secretary Wen's timeline of the day of the incident, she reached the ventilation duct at 22:40 and re-entered it at 23:00 to return}RELEVANT PREMISE 2. {This timing matches the moment Dr. Ling heard the thumping sound}RELEVANT AND SOLID INTERIM CONCLUSION 3, thus it can be inferred that ${$ the sound was most likely Secretary Wen crawling in the ventilation duct $}$ SOLID CONCLUSION. (GPT Eval Score: 5)</td>
</tr>
</tbody>
</table>
<p>Table 7: Qualitative analysis of agent reasoning performance with different levels of information access.
merely plausible. For MR and No MR, owing to the absence of many key details, the reasoning process involves many irrelevant or flawed premises, leading to only tenuous conclusions. This qualitative analysis demonstrates the significant impact that the collection of key information during the game has on the agents' final reasoning performance.</p>
<h2>7 Conclusion</h2>
<p>This work has explored the application of large language models in complex interactive environments, exemplified by the Chinese detective role-playing game "Jubensha". Our research has yielded four main contributions: the creation of a specialized dataset tailored for the Jubensha games, the de-
sign of a multi-agent interaction framework for the automatic conduct of Jubensha games, the development of a set of quantitative and qualitative assessment methods to measure the information gathering and reasoning abilities of LLM-based agents within the game, and the utilization of the latest in-context learning techniques to devise modules that enhance the performance of LLM-based agents. We have empirically demonstrated that our designed multi-agent interaction framework and the in-context learning modules significantly improve upon the baseline in terms of information gathering, murderer identification, and reasoning capabilities. We believe this research will advance the community's knowledge of LLM-based agents</p>
<p>and offers a new perspective on evaluating the performance of LLMs in a complex, plot-driven, and adversarial reasoning game environment constrained by narrative contexts.</p>
<h2>Ethical Considerations</h2>
<p>Our research delves into the communicative and reasoning abilities of large language models (LLMs) within the context of "Jubensha" (剧本杀), a type of Chinese detective role-playing game. It is important to clarify that any portrayals of violence in these game scenarios are purely fictional, and our work is solely for the purposes of academic analysis. It does not represent or endorse real-world violence in any way. The data employed in our study is gathered from online platforms hosting Jubensha content. When sharing this dataset, we will implement measures to ensure that its usage remains strictly for academic, non-commercial purposes and complies with fair use policies. ${ }^{5}$</p>
<h2>Limitations</h2>
<p>We outline the main limitations of our work as follows:</p>
<ul>
<li>Language Specificity of the Dataset: Our Jubensha dataset is in Chinese, which means our experimental results are specifically reflective of the communicative and reasoning capabilities of Large Language Model (LLM) based agents in Chinese contexts. Considering that the majority of LLM evaluation benchmarks are in English, and existing LLMbased agent frameworks are tailored for English applications, our Chinese-centric benchmark and framework could provide a valuable addition to the field.</li>
<li>Variability in Experimental Outcomes: The inherent stochastic nature of LLM outputs may lead to significant variability in singleexperiment results. For instance, in the murderer identification voting phase, LLM-driven players may choose differently among suspects if given another chance. Similarly, game processes can diverge significantly, even with identical starting conditions. To mitigate this, we conducted three runs of each Jubensha</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>game script with LLM-based agents for each proposed architectures and averaged the outcomes. For highly variable tasks like murderer identification, agents performed 10 memoryless votes, from which we calculated identification accuracy and civilian win rates. However, due to time and budget constraints, further experiments to solidify these results were not feasible. We will make our code, dataset, and all intermediate results available postacceptance, including chat histories, voting records, agents' responses to factual and inferential questions, and GPT models' evaluations on these responses, to help interested readers replicate our experiments and better understand our findings.</p>
<ul>
<li>Model Updates and Replication Costs: Reproducing our findings might be challenging due to OpenAI's periodic model updates, which can lead to different results with different model versions. To address this, we specify the exact version of GPT used in each experiment in the appendix B and will publish our code post-acceptance. Moreover, the cost of replicating our experiments can be costly. We aim to alleviate this by providing an illustrative cost breakdown for each experimental step in the appendix D, helping readers gauge potential expenses before attempting replication.</li>
</ul>
<h2>References</h2>
<p>Olivier Barreteau, Christophe Le Page, and Patrick D'aquino. 2003. Role-playing games, models and negotiation processes. Journal of Artificial Societies and Social Simulation.</p>
<p>Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. 2020. Combining deep reinforcement learning and search for imperfect-information games.</p>
<p>Noam Brown and Tuomas Sandholm. 2019. Superhuman ai for multiplayer poker. Science, 365(6456):885-890.</p>
<p>Murray Campbell, A.Joseph Hoane, and Feng hsiung Hsu. 2002. Deep blue. Artificial Intelligence, 134(1):57-83.</p>
<p>Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud</p>
<p>Adada, et al. 2019. Textworld: A learning environment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7. Springer.</p>
<p>Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-verification reduces hallucination in large language models.</p>
<p>Meta Fundamental AI Research Diplomacy Team FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. 2022. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science.</p>
<p>Lixxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems.</p>
<p>Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023. Improving language model negotiation with self-play and in-context learning from ai feedback. arXiv preprint arXiv:2305.10142.</p>
<p>Infocom. 1980. Zork i. https://ifdb.org/ viewgame?id=0dbnusxunq7fw5ro.</p>
<p>Edward Junprung. 2023. Exploring the intersection of large language models and agent-based modeling via prompt engineering. arXiv preprint arXiv:2308.07411.</p>
<p>János Kramár, Tom Eccles, Ian Gemp, Andrea Tacchetti, Kevin R McKee, Mateusz Malinowski, Thore Graepel, and Yoram Bachrach. 2022. Negotiation and honesty in artificial intelligence methods for the board game of diplomacy. Nature Communications.</p>
<p>Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia, Shirley Anugrah Hayati, James Rehg, and Diyi Yang. 2023. Werewolf among us: Multimodal resources for modeling persuasion behaviors in social deduction games. In Findings of the Association for Computational Linguistics: ACL 2023, pages 6570-6588.</p>
<p>Yuanzhi Liang, Linchao Zhu, and Yi Yang. 2023. Tachikuma: Understading complex interactions with multi-character and novel objects by large language models. arXiv preprint arXiv:2307.12573.</p>
<p>Aishwarya Lohokare, Aayush Shah, and Michael Zyda. 2020. Deep learning bot for league of legends. In Proceedings of the Sixteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE'20. AAAI Press.</p>
<p>Ryo Nagata, Masato Hagiwara, Kazuaki Hanawa, Masato Mita, Artem Chernodub, and Olena Nahorna. 2021. Shared task on feedback comment generation for language learners. In Proceedings of the 14th International Conference on Natural Language Generation, pages 320-324, Aberdeen, Scotland, UK. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. Embeddings API Reference. https://platform.openai.com/docs/ api-reference/embeddings.</p>
<p>Joon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.</p>
<p>Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-llm: A trainable agent for roleplaying. arXiv preprint arXiv:2310.10158.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768.</p>
<p>David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 2016. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484489 .</p>
<p>Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. 2021. Habitat 2.0: Training home assistants to rearrange their habitat. Advances in Neural Information Processing Systems.</p>
<p>Chen Feng Tsai, Xiaochen Zhou, Sierra S. Liu, Jing Li, Mo Yu, and Hongyuan Mei. 2023. Can large language models play text games well? current state-of-the-art and open questions.</p>
<p>Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. 2019. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354.</p>
<p>Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. 2017. Starcraft ii: A new challenge for reinforcement learning.</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023a. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432.</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. 2022a. Scienceworld: Is your agent smarter than a 5th grader? arXiv preprint arXiv:2203.07540.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. 2023b. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems.</p>
<p>Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. 2023. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.</p>
<p>Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang, and Wei Liu. 2020. Towards playing full moba games with deep reinforcement learning. In Proceedings of
the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA. Curran Associates Inc.</p>
<p>Jose Zagal and Sebastian Deterding. 2018. Definitions of "role-playing games". Role-playing game studies: Transmedia foundations.</p>
<h2>Appendix</h2>
<h2>A Human Evaluation on the Quality of Agents' Responses</h2>
<p>To study the impact of different architectures on the quality of agents' responses, we selected responses from agents at the Self-Introduction Stage for comparison. More specifically, for agents with each of five architectures, we selected 20 responses, resulting in a total of 100 responses. These were compared in groups of five, with each group consisting of agents playing the same character in a scripted murder mystery game. During the evaluation, we also provided the character scripts of the agents' roles as a ground truth reference. A Chinese native speaker human annotator was asked to score the agents' responses based on their naturalness, authenticity, and informativeness on a scale of 1 to 5 . The human annotator was unaware of the specific architecture used by each agent. The specific scoring guidelines are as follows:</p>
<ul>
<li>Naturalness: Naturalness primarily examines the naturalness and fluency of the agent response. Since the Jubensha game is a roleplaying game, human responses are primarily in the first person. The more closely a response resembles a human answer, the higher its naturalness score. Responses mixed with different languages or not in the first-person tone receive lower naturalness scores. Note that the naturalness score is unrelated to the character script of the agents' portrayed character.</li>
<li>Authenticity: Authenticity mainly examines whether the agent response contains content that is true to the character script of the agents' portrayed character. The Human annotator gives scores based on the proportion of the response's truthfulness, with higher scores indicating greater authenticity and lower scores indicating less. Note that, according to the rules of the Jubensha games, the murderer is allowed to lie in the game to cover up its identity. For responses from agents playing the role of the murderer, the human annotator bases the authenticity score on the following principle: if the content in the agent response that differs from the character scripts does not involve other players in the game (meaning that other players cannot judge the truthfulness of the agent response), this is considered
strategic lying by the agent playing the murderer and does not affect its authenticity score. Otherwise, points will be deducted.</li>
<li>Informativeness: Informativeness primarily examines whether the agent response provides enough information from its own character script. Human annotators score based on how much of their character script the agent response covers, with higher scores indicating more information and lower scores indicating less. Since murderers in the Jubensha game are allowed to conceal information, the human annotator bases the informativeness score for responses from agents playing the murderer role on the following principle: if the information in the character script not mentioned in the agent response could lead to suspicion of the agent, the coverage or lack thereof of this information is not considered in the informativeness score.</li>
</ul>
<p>From Figure 5, we can see that MR+SR+SV (where $\mathrm{N}=3$ ) scored the highest in terms of naturalness, authenticity, and informativeness among the evaluation metrics. This demonstrates that our proposed self-refinement and self-verification models can help agents improve the authenticity and informativeness of their responses without sacrificing naturalness.</p>
<h2>B Utilization of LLMs in Jubensha Game Stages</h2>
<p>Throughout the gameplay and other stages of the Jubensha game experiment, including the generation of factual questions, agent Q\&amp;A sessions, document embeddings, and the evaluation of agent responses, we predominantly employed OpenAI's GPT-3.5 and GPT-4 models. Unless specifically indicated, the GPT-3.5 model mentioned in this paper refers to gpt-3.5-turbo-16k-0613, and GPT-4 refers to gpt-4-1106-preview. Specific applications of each model are as follows:</p>
<ul>
<li>Gameplay: GPT-3.5 was used due to its larger context window and budget considerations. ${ }^{6}$ Future experiments may include GPT4 as costs become more feasible. For the text embeddings used for the memory retriever module, we used text-embedding-ada-002.</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Human Evaluation on the Quality of Agents’ Responses.</p>
<ul>
<li>Factual Question Generation: Both GPT3.5 and GPT-4 (gpt-4-0613) were used, with no notable difference in output quality observed.</li>
<li>Agent Q\&amp;A Sessions: GPT-3.5 was used for factual question responses, while both GPT3.5 and GPT-4 were tested for inferential question responses.</li>
<li>Response Evaluation: GPT-3.5 assessed agents' responses to factual questions, whereas GPT-4 evaluated agents' inferential question responses.</li>
<li>Document Embedding: We used text-embedding-3-large to obtain the document embeddings of agents' chat histories and all players' scripts.</li>
</ul>
<h2>C Detailed Jubensha Game Rules and Procedure</h2>
<p>The detailed Jubensha game rules we used for our experiments are as follows:</p>
<p>Rule 1: The total number of players participating in the game may be four or five, depending on the script. Only one of these players is the real murderer, known as the murderer player. Players who are not the murderer are collectively referred to as civilian players.</p>
<p>Rule 2: Civilian players need to cooperate to face a carefully planned
murder case and find the real murderer among the suspects by collecting evidence and reasoning.</p>
<p>Rule 3: Throughout the game, only the murderer player can lie. To hide their identity, the murderer may choose to frame others and exonerate themselves.</p>
<p>Rule 4: Players who are not the murderer (civilian players) must answer questions honestly from other players and the host, and provide as much detailed information about the case as they know, to help restore the truth of the case.</p>
<p>Rule 5: The host of the game is responsible only for ensuring that the game proceeds according to a specific process. They are not a player in the game and do not participate in the game's storyline.</p>
<p>Rule 6: Each player receives their personal character script from the host at the beginning of the game, thereby learning about their character's information and identity.</p>
<p>Rule 7: The content of each player's personal character script is invisible to other players, so players must and can only gather information about other players by interacting with them after the game starts.</p>
<p>Rule 8: Since there is only one murderer in the game, only the murderer player knows the identities of the other players after receiving their character script (since everyone else is a civilian). Civilian players cannot determine the true identities of other players and can only infer through interactions during the game.</p>
<p>Rule 9: During the voting phase, each player has exactly one vote, which they can cast for the player they think is the murderer (including themselves, although this is discouraged). If the player with the most votes is the murderer, the civilian players win. Otherwise, the murderer player wins.</p>
<p>The overall procedure of the game is as follows:</p>
<h2>Stage 1: Distribution of Character Scripts</h2>
<p>The host distributes character scripts to each player individually. The script includes the player's name, identity (murderer or civilian), character story, and timeline for the day of the incident.</p>
<h2>Stage 2: Self-Introduction Session</h2>
<p>Players introduce themselves under the host's guidance, explaining their relationship with the victim and their timeline on the day of the incident.</p>
<h2>Stage 3: Initial Questioning</h2>
<p>After a player responds to the host's inquiries, other players are given the opportunity to ask questions and receive answers from that player.</p>
<h2>Stage 4: Two Rounds of Open Questioning</h2>
<p>Players enter two rounds of open questioning, with each taking turns to question another player and obtain answers.</p>
<h2>Stage 5: Distribution of Clue Cards</h2>
<p>Players receive clue cards containing additional information about the
victim and the players, aiding them in deducing the case's storyline.</p>
<h2>Stage 6: Three Rounds of Open Questioning</h2>
<p>Players enter three rounds of open questioning, with each taking turns to question another player and obtain answers.</p>
<h2>Stage 7: Voting</h2>
<p>Under the host's guidance, players anonymously vote to determine who they believe is the murderer. Each player has one vote.</p>
<h2>Stage 8: Outcome Reveal</h2>
<p>The game concludes with the revelation of the voting results.</p>
<h2>D Experimental Cost Breakdown</h2>
<p>To assist readers in understanding the potential costs involved in our experiments prior to attempting a replication, we provide an illustrative cost breakdown for each experimental step in Table 8. In this example, we utilized agents with $\mathrm{MR}+\mathrm{SR}+\mathrm{SV}(\mathrm{N}=3)$ modules, with the Jubensha game script being "The Doomed Sunshine," consisting of about 16 k tokens. We observed that the most expensive stage of the experiment was using GPT-4 to evaluate agents' responses to inferential questions, while the cheapest stage involved using text-embedding-3-large to calculate document similarity between all players' scripts and chat histories of agents. Summing up all the costs, we can see that the total expense for a single complete experiment is approximately 11.9 USD.</p>
<h2>E Correlations between Automatic Evaluations and Human Evaluations</h2>
<p>In this work, we extensively utilized automatic evaluation methods to quantitatively assess the performance of LLM-based agents in Jubensha games. To ensure the reliability of our automatic evaluation methods, two Chinese native speaker human annotators were responsible for manually evaluating samples of the agents' responses. Below, we describe the human evaluation process across different stages:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Stage</th>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Cost (\$)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Gameplay</td>
<td style="text-align: left;">GPT-3.5, text-embedding-ada-002</td>
<td style="text-align: right;">2.16</td>
</tr>
<tr>
<td style="text-align: left;">Eval inferential question</td>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: right;">1.37</td>
</tr>
<tr>
<td style="text-align: left;">Eval factual question</td>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: right;">4.6</td>
</tr>
<tr>
<td style="text-align: left;">Murderer identification voting</td>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: right;">1.9</td>
</tr>
<tr>
<td style="text-align: left;">Doc similarity</td>
<td style="text-align: left;">text-embedding-3-large</td>
<td style="text-align: right;">1.86</td>
</tr>
</tbody>
</table>
<p>Table 8: Experimental cost breakdown in different stages.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Score</th>
<th style="text-align: left;">Rating</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Very Poor</td>
<td style="text-align: left;">The agent's rationale is vastly different from the ground truth</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">rationale, with reasoning steps completely illogical and impossible</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Poor</td>
<td style="text-align: left;">to justify the answer based on the reasoning provided.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">The agent's rationale is only somewhat similar to the ground truth</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">rationale in certain aspects, with most reasoning steps illogical,</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">having only a few that are correct or partially logical.</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Average</td>
<td style="text-align: left;">The agent's rationale is somewhat close to the ground truth ratio-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">nale, with some reasoning steps correct, but there are significant</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">errors or omissions.</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">The agent's rationale is very close to the ground truth rationale,</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">with the vast majority of reasoning steps correct and logical, having</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">only minor errors or deficiencies.</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: left;">Excellent</td>
<td style="text-align: left;">The agent's rationale is almost or entirely consistent with the</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">ground truth rationale, with all reasoning steps correct and logical,</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">demonstrating a high level of reasoning ability and a deep under-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">standing of the problem.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 9: Evaluation Criteria for Agent Rationales Against Ground Truth.</p>
<ul>
<li>Evaluation on Factual Question Answering: 200 factual questions and the agents' responses to these questions are randomly selected. The two human annotators then manually assessed the agents' responses as correct or incorrect based on the factual questions, their reference answers, and the agents' responses. During the evaluation process, the annotators were unaware of the agents' architectures, i.e., whether the agents were using MR architecture or $\mathrm{MR}+\mathrm{SR}+\mathrm{SV}(\mathrm{N}=3)$, etc., nor were they aware of the GPT model's evaluation of agent responses. After human scoring, we compared the GPT model's scores as predictions with the human scores as ground truths, and then calculated Spearman correlation, Pearson correlation, accuracy, and F1 score, with results shown in Figure 6(a).</li>
<li>Evaluation on Inferential Question Answering (Overall Accuracy): 100 inferential questions and the agents' responses to these questions are randomly selected. The two human annotators manually assessed the agents' answers as correct or incorrect based on the inferential questions, their reference answers, and the agents' responses. Similar to the first evaluation, the annotators were unaware of the agents' architectures and the GPT model's evaluation. After human scoring, we compared the GPT model's scores with human scores, calculating Spearman correlation, Pearson correlation, accuracy, and F1 score, with results shown in Figure 6(b).</li>
<li>Evaluation on Inferential Question Answering (Informed Accuracy): 100 inferential questions and the agents' predicted rationales for the correct answers are randomly selected. Based on these inferential questions, their reference rationales, and the agents' predicted rationales, two human annotators manually scored the agents' predicted rationales on a scale of 1-5. The evaluation criteria are shown in Table 9. Similarly, annotators were unaware of the agents' architectures and the GPT model's evaluation. After human scoring, we converted both the GPT model's scores and human scores into good or bad, with scores of 4 or above considered good. We then compared the converted GPT scores with converted human scores to calculate Spearman
correlation, Pearson correlation, accuracy, and F1 score, with results shown in Figure 6(c).</li>
<li>Evaluation on Murderer Identification (Overall Accuracy): 100 instances of agents' voting records in murderer identification stage are randomly selected. Based on these records, the names of all player characters in the game, and the actual murderer's character name, two human annotators manually identified whether the voting records pointed to the actual murderer, marking it as correct if so, and incorrect otherwise. The annotators were unaware of the agents' architectures and the results of our text matching algorithm that compared the voting records with the game's actual murderer's character name. After human scoring, we compared the text matching algorithm's scores with human scores, calculating Spearman correlation, Pearson correlation, accuracy, and F1 score, with results shown in Figure 6(d).</li>
</ul>
<p>As seen in Figures 6(a-d), all our automatic evaluations demonstrated strong correlation with human evaluations, further validating the effectiveness of our automatic evaluation methods and the reliability of our experimental results.</p>
<h2>F AI-Assisted Writing and Coding</h2>
<p>In this work, we extensively utilized GPT-4 to assist in refining the language of the paper. This included tasks such as paraphrasing, spell-checking, or translating the original content provided by the authors. Additionally, we employed GPT-4's coding capabilities to help write simple utility functions. These functions were designed for operations such as reading our stored experimental result files, aggregating information, and generating statistical tables. When releasing the code, we will clearly indicate the parts that were aided by AI-assisted coding.</p>
<h2>G Authenticity Threshold and Answer Scoring in Self-Verification Stage</h2>
<p>During the self-verification stage, the authenticity threshold is determined by three key components:</p>
<ul>
<li>The accuracy of retrieved facts,</li>
</ul>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(a) LLM Evaluation Accuracy \&amp; Correlation with Human Judgment in Factual Question Answering.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b) LLM Evaluation Accuracy \&amp; Correlation with Human Judgment in Inferential Question Answering (Overall Accuracy).</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" />
(c) LLM Evaluation Accuracy \&amp; Correlation with Human Judgment in Inferential Question Answering (Informed Accuracy).
<img alt="img-8.jpeg" src="img-8.jpeg" />
(d) Text Matching Evaluation Accuracy \&amp; Correlation with Human Judgment in Murderer Identification.</p>
<p>Figure 6: Correlation between human evaluations and automatic evaluations used in this work.</p>
<ul>
<li>The quantity of corrected facts,</li>
<li>The length of the agent's response.</li>
</ul>
<p>An agent's answer is only output if it surpasses these thresholds, provided the maximum number of attempts has not been exceeded. In our experiment, we employed two sets of parameters to define these thresholds:</p>
<ul>
<li>For questions originating from the host:</li>
<li>Accuracy threshold: 0.7,</li>
<li>Minimum number of corrected facts: 4,</li>
<li>Minimum response length: 350 words.</li>
<li>For questions from other players:</li>
<li>Accuracy threshold: 0.6,</li>
<li>Minimum number of corrected facts: 1,</li>
<li>Minimum response length: 30 words.</li>
</ul>
<p>Regarding the scoring of answers, we utilize the following formula:</p>
<p>$$
\begin{aligned}
\text { Score }= &amp; \text { Accuracy } \
&amp; +(\text { Number of Corrected Facts } \
&amp; + \text { Time-Matched Corrections } \
&amp; +\frac{\text { Length of Response }}{200})
\end{aligned}
$$</p>
<p>In this formula:</p>
<ul>
<li>"Accuracy" reflects the accuracy of retrieved facts.</li>
<li>"Number of Corrected Facts" indicates the total corrected facts.</li>
<li>"Length of Response" is the word count of the agent's response.</li>
<li>"Time-Matched Corrections" refers to the count of corrected facts that include specific time references, identified through regular expression matching.</li>
</ul>
<h2>H Prompts</h2>
<p>In this section, we introduce the various LLM prompts used in this work. We have categorized these prompts by their different functions and compiled them into Tables 19 through 27, totaling 9 tables. For each prompt, we also provide examples
of LLM input and output to aid readers in understanding the specific operation of the prompt. For the convenience of our readers, we have translated the original Chinese prompt table content into English, with the English prompts tables located from Table 10 to Table 18.</p>
<h1>LLM Prompt for Answering Questions</h1>
<div class="codehilite"><pre><span></span><code>&quot;{agent_summary}&quot;
    + &quot;\ngame_rule&quot;
    + &quot;\n{background_story}&quot;
    + &quot;\nDialogue currently happening in the game: {current_dialogue}&quot;
    + &quot;\nContent related to the ongoing dialogue in the game from previous conversations by {agent_name}:
{relevant_memories}&quot;
    + &quot;\nRelationship with the person you&#39;re talking to: {relationship_with_interlocutor}&quot;
    + &quot;\nWhat will {agent_name} say? Please answer the question using the following format: #Answer#: what
you want to say to answer the question.&quot;
    + &quot;{agent_name} responds to {inquirer}&#39;s question saying #Answer#:\n&quot;
</code></pre></div>

<h2>Example</h2>
<p>agent_name: Boatman Wang
inquirer: Host
relationship_with_interlocutor: The relationship with the speaker is: the speaker is the host, who is responsible for guiding the players to complete the game.
current_dialogue: The host said to Boatman Wang: "Please start by introducing your character, and then talk about the victim of the case you know - Boss Yang. What kind of person was Boss Yang, and what was your relationship with him? Finally, provide a detailed timeline of what you were doing on the day of the incident. It needs to be specific about who you saw and what you did at what time on the day of the incident.
relevant_memories: The host said to Dr. Ling: "Please start by introducing your character, and then talk about the victim of the case you know - Boss Yang. What kind of person was Boss Yang, and what was your relationship with him......" Dr. Ling said to the host: "Hello everyone, I am Dr. Ling. I met Boss Yang on the Sunshine Cruise; he was the owner of a well-known bar here. His bar was known for various illegal activities, including gambling, drugs, and prostitution. I approached him out of greed, hoping to gain financial security from him. On the cruise, I noticed some anomalies, often seeing Boss Yang in contact with Mate Zhang, and he hoped I could get closer to Mate Zhang......" Boatman Wang said to Dr. Ling: "Dr. Ling, you mentioned that Boss Yang had contact with Mate Zhang. Do you know what specific transactions they had?" Dr. Ling replied to Boatman Wang: "Boatman Wang, I am not clear about the specifics of their transactions because the dealings between Mate Zhang and Boss Yang were their own business. I was only responsible for observing Mate Zhang's actions and reporting any anomalies to Boss Yang......
background_story: The Sunshine Cruise has been softly gliding on the East Sea, bringing tourists to enjoy the sun and sea breeze for twenty-two years. Over these two decades, the Sunshine Cruise never suffered significant turbulence, almost as if blessed by the Sea God. However, on June 20, 2222, on the lonely sea, the Sunshine Cruise encountered a bloody calamity, with passenger Boss Yang mysteriously dying on board in a gruesome manner. What fate lies ahead for the Sunshine Cruise? Can it overcome this shadow of death and continue its voyages? Suspects identified: 1. Mate Zhang, male, 42, has been working on the Sunshine Cruise since its inception, an excellent veteran sailor. 2. Boatman Wang, male, 49, responsible for the ship's engine work, a capable crew member. 3. Dr. Ling, female, 23, the ship's doctor, young and promising, elegant in demeanor. 4. Secretary Wen, female, 26, the deceased Boss Yang's secretary.</p>
<h2>agent_summary:</h2>
<h2>【Name】: Boatman Wang</h2>
<p>【Age】: 49
【Role in the game】: Civilian
【Mission in the game】: Boatman Wang is not the murderer! To win the game, Boatman Wang needs to cooperate with other civilian players to find the real murderer.
【Character Script】: You've been working on the Sunshine Cruise for over a decade, besides the captain and Mate Zhang, you're practically the most senior person on board. You have a considerable income and a filial son, Wang Xiaogong, who also works on the cruise. However, your 'considerable income' doesn't come from your meager salary, but from collaborating with Mate Zhang in buying drugs in Southeast Asia, ensuring his safety, and sharing profits with you. Initially, you were reluctant to get involved, but the offer was too tempting to refuse. By the time you wanted out, it was already too late. After the incident, someone named Lu Renjia tried reaching out to you, asking if you were willing to break free from Mate Zhang's control and take over the entire supply chain, even suggesting that his backers could help you supplant Mate Zhang. However, you repeatedly refused, explaining you got involved with these activities unintentionally and did not wish to make further mistakes. He left you a card, telling you to contact him anytime. Two years ago, your son married Dr. Ling, the ship's doctor; intelligent, charming, and elegant...... On June 19, 2222, Boss Yang boarded the ship, and you planned to talk with him. At 19:30 that evening, just as you were about to find Boss Yang, your daughter-in-law, Dr. Ling, suddenly knocked on your door frantically. When you opened the door, she stood there, tears streaming down her face. You quickly let her in, and she told you......</p>
<p>game_rule: [Game Rule] : Scripted murder is a role-playing game where players follow a given script and character settings to deduce and interact, accomplishing their respective missions or identifying the real murderer. Here are the rules for this murder mystery game: 1. There are four players in the game, namely Mate Zhang, Dr. Ling, Boatman Wang, and Secretary Wen. Among them, there is only one true murderer. The player whose identity is the murderer is known as the murderer player. Players who are not the murderer are collectively referred to as civilian players. 2. Civilian players need to collaboratively face a meticulously planned murder case and use evidence gathering and reasoning to identify the real murderer among the suspects. 3. Only the murderer player is allowed to lie throughout the game, and to hide their identity, the murderer may choose to blame others.</p>
<h1>LLM Output</h1>
<p>#Answer#: Hello everyone, I'm Wang, the ship engineer. I'm responsible for the engine work on the Sunshine and I am also the most senior person on board. I have known Boss Yang for a long time; he is a graceful, smart, and capable person. On the ship, he often interacts with Mate Zhang, and I am aware that they have some unspeakable transactions. Personally, I do not favor their dealings, but the Mate Zhang offered was too tempting, hence I got involved in their scheme. On the day of the incident, I went to see Boss Yang at 19:48, hoping to have a chat with him. However, the bodyguard at the door told me Boss Yang was not in his room, so I didn't get to see him. Later, I returned to my room and happened to see my daughter-in-law, Ling, the ship's doctor, knocking on my door frantically. She told me Boss Yang wanted to kill her and begged me for help. I comforted her and went to look for Boss Yang, but found out that he had no intention of killing her. This is my timeline for the day of the incident.</p>
<p>Table 10: LLM prompt and output for answering questions. ( English Translation)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ Before the public release of gpt-4-1106-preview, the publicly accessible GPT-4 model had a maximum context size of 8 k , which is insufficient for the Jubensha game.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
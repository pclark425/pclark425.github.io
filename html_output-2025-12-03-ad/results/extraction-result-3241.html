<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3241 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3241</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3241</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-75.html">extraction-schema-75</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <p><strong>Paper ID:</strong> paper-53831a304fb568aea4548efcef910cc62f2d2dcb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/53831a304fb568aea4548efcef910cc62f2d2dcb" target="_blank">Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces the first multimodal dataset for modeling persuasion behaviors and explores the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes.</p>
                <p><strong>Paper Abstract:</strong> Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpora. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26 , 647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. Our dataset can be found at https://persuasion-deductiongame. socialai-data.org . The codes and models are available at https://github.com/ SALT-NLP/PersuationGames .</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3241.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3241.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (OpenAI text-davinci-002 engine, referred to as GPT-3-175B in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Off-the-shelf large language model (GPT-3 family) probed via zero-/one-/five-shot prompting (text-davinci-002) to perform utterance-level persuasion-strategy classification on transcripts from social deduction games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Used as an off-the-shelf LLM (paper refers to it as GPT-3-175B via the text-davinci-002 engine) with temperature=0; evaluated in zero-shot, one-shot and five-shot prompting setups to classify utterance-level persuasion strategies from game transcripts.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>One Night Ultimate Werewolf / The Resistance: Avalon (persuasion strategy prediction dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict utterance-level persuasion strategies (multi-label binary classification per strategy) for dialogue utterances from social deduction game videos/transcripts.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Reported performance for prompting (no memory mechanism discussed): Ego4D zero-shot Avg F1=35.4, Joint-A=58.5; one-shot Avg F1=40.7, Joint-A=56.3; five-shot Avg F1=47.0, Joint-A=59.7. YouTube zero-shot Avg F1=40.3, Joint-A=52.0; one-shot Avg F1=47.2, Joint-A=53.2; five-shot Avg F1=49.6, Joint-A=53.7.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>The paper reports that adding more in-context examples (few-shot) improved GPT-3 performance on the persuasion classification task, but GPT-3 remained inferior to fine-tuned models; no recommendations about explicit memory mechanisms are provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3241.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3241.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CICERO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CICERO (FAIR Diplomacy team agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior game-playing agent that combines large language models with planning/strategic reasoning to achieve human-level performance in the game Diplomacy; mentioned in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-level play in the game of diplomacy by combining language models with strategic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CICERO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned as a prior game agent that leverages language models together with planning and reinforcement learning approaches to play Diplomacy at human-level performance; the paper cites it in related work but does not evaluate or analyze it.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Diplomacy</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Negotiation and strategic play in the multiplayer board game Diplomacy (as reported in the cited CICERO work).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Human-level play in the game of diplomacy by combining language models with strategic reasoning. <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners. <em>(Rating: 2)</em></li>
                <li>Training an assassin ai for the resistance: Avalon. <em>(Rating: 1)</em></li>
                <li>Constructing a human-like agent for the werewolf game using a psychological model based multiple perspectives. <em>(Rating: 1)</em></li>
                <li>Finding friend and foe in multi-agent games. <em>(Rating: 1)</em></li>
                <li>Are you awerewolf? detecting deceptive roles and outcomes in a conversational role-playing game. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3241",
    "paper_id": "paper-53831a304fb568aea4548efcef910cc62f2d2dcb",
    "extraction_schema_id": "extraction-schema-75",
    "extracted_data": [
        {
            "name_short": "GPT-3 (text-davinci-002)",
            "name_full": "GPT-3 (OpenAI text-davinci-002 engine, referred to as GPT-3-175B in this paper)",
            "brief_description": "Off-the-shelf large language model (GPT-3 family) probed via zero-/one-/five-shot prompting (text-davinci-002) to perform utterance-level persuasion-strategy classification on transcripts from social deduction games.",
            "citation_title": "Language models are few-shot learners.",
            "mention_or_use": "use",
            "agent_name": "GPT-3 (text-davinci-002)",
            "agent_description": "Used as an off-the-shelf LLM (paper refers to it as GPT-3-175B via the text-davinci-002 engine) with temperature=0; evaluated in zero-shot, one-shot and five-shot prompting setups to classify utterance-level persuasion strategies from game transcripts.",
            "game_or_benchmark_name": "One Night Ultimate Werewolf / The Resistance: Avalon (persuasion strategy prediction dataset)",
            "task_description": "Predict utterance-level persuasion strategies (multi-label binary classification per strategy) for dialogue utterances from social deduction game videos/transcripts.",
            "uses_memory": false,
            "memory_type": null,
            "memory_implementation_details": null,
            "performance_with_memory": null,
            "performance_without_memory": "Reported performance for prompting (no memory mechanism discussed): Ego4D zero-shot Avg F1=35.4, Joint-A=58.5; one-shot Avg F1=40.7, Joint-A=56.3; five-shot Avg F1=47.0, Joint-A=59.7. YouTube zero-shot Avg F1=40.3, Joint-A=52.0; one-shot Avg F1=47.2, Joint-A=53.2; five-shot Avg F1=49.6, Joint-A=53.7.",
            "has_performance_comparison": false,
            "memory_benefits": null,
            "memory_limitations_or_failures": null,
            "best_practices_or_recommendations": "The paper reports that adding more in-context examples (few-shot) improved GPT-3 performance on the persuasion classification task, but GPT-3 remained inferior to fine-tuned models; no recommendations about explicit memory mechanisms are provided.",
            "uuid": "e3241.0"
        },
        {
            "name_short": "CICERO",
            "name_full": "CICERO (FAIR Diplomacy team agent)",
            "brief_description": "A referenced prior game-playing agent that combines large language models with planning/strategic reasoning to achieve human-level performance in the game Diplomacy; mentioned in related work.",
            "citation_title": "Human-level play in the game of diplomacy by combining language models with strategic reasoning.",
            "mention_or_use": "mention",
            "agent_name": "CICERO",
            "agent_description": "Mentioned as a prior game agent that leverages language models together with planning and reinforcement learning approaches to play Diplomacy at human-level performance; the paper cites it in related work but does not evaluate or analyze it.",
            "game_or_benchmark_name": "Diplomacy",
            "task_description": "Negotiation and strategic play in the multiplayer board game Diplomacy (as reported in the cited CICERO work).",
            "uses_memory": null,
            "memory_type": null,
            "memory_implementation_details": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "memory_benefits": null,
            "memory_limitations_or_failures": null,
            "best_practices_or_recommendations": null,
            "uuid": "e3241.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Human-level play in the game of diplomacy by combining language models with strategic reasoning.",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners.",
            "rating": 2
        },
        {
            "paper_title": "Training an assassin ai for the resistance: Avalon.",
            "rating": 1
        },
        {
            "paper_title": "Constructing a human-like agent for the werewolf game using a psychological model based multiple perspectives.",
            "rating": 1
        },
        {
            "paper_title": "Finding friend and foe in multi-agent games.",
            "rating": 1
        },
        {
            "paper_title": "Are you awerewolf? detecting deceptive roles and outcomes in a conversational role-playing game.",
            "rating": 1
        }
    ],
    "cost": 0.01272075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games</h1>
<p>Bolin Lai ${ }^{1 <em>}$ Hongxin Zhang ${ }^{2 </em>}$ Miao Liu ${ }^{3 <em>}$ Aryan Pariani ${ }^{1 </em>}$ Fiona Ryan ${ }^{1}$ Wenqi Jia ${ }^{1}$ Shirley Anugrah Hayati ${ }^{4}$ James M. Rehg ${ }^{1}$ Diyi Yang ${ }^{5}$<br>${ }^{1}$ Georgia Institute of Technology ${ }^{2}$ Shanghai Jiao Tong University<br>${ }^{3}$ Meta AI ${ }^{4}$ University of Minnesota ${ }^{5}$ Stanford University<br>{bolin.lai, apariani3, fkryan, wenqi.jia, rehg}@gatech.edu<br>icefox@sjtu.edu.cn, miaoliu@meta.com<br>hayat023@umn.edu, diyiy@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpora. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26, 647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. Our dataset can be found at https://persuasion-deductiongame. socialai-data.org. The codes and models are available at https://github.com/ SALT-NLP/PersuationGames.</p>
<h2>1 Introduction</h2>
<p>As humans, from childhood, we develop the ability to attribute mental belief states to ourselves and others (Premack and Woodruff, 1978). Moreover, we constantly exhibit persuasive behaviors to influence and even reshape the belief states of others during our daily social interactions (Lonigro et al., 2017). An automatic system with the ability to understand human persuasion strategies and deduce human belief states may enable more proactive humancomputer interaction, and facilitate collaborative decision-making processes.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Prior works targeted at understanding the persuasion strategies utilized on online forums like Reddit, crowd-funding platforms (Yang et al., 2019; Chen and Yang, 2021; Atkinson et al., 2019), and in 1-on1 dialogues under simulated scenarios through the Amazon Mechanical Turk platform (Wang et al., 2019; Chawla et al., 2021).</p>
<p>However, the persuasive behaviors during naturalistic group discussions with face-to-face conversation remain unexplored. More importantly, daily human social interaction is multimodal by nature. Both verbal communication (e.g. language and audio) and non-verbal communication (e.g. gesture and gaze behavior) are essential for analyzing persuasive behavior.</p>
<p>Moreover, resources for understanding how persuasion strategies affect decision and deduction outcomes during social interactions are missing from the language technologies community.</p>
<p>To bridge these gaps, we introduce the first multimodal benchmark dataset for modeling persuasive behaviors during multi-player social deduction games. As shown in Fig. 1, our dataset is captured in a naturalistic setting where groups of participants play social deduction games ${ }^{1}$. Our dataset contains both video recordings and the corresponding dialogue transcriptions. The video data is sourced from both the Ego4D Social dataset (Grauman et al., 2022) and YouTube videos. Our dataset also has annotations for persuasion strategy at the utterance level and the voting outcome of each participant during the social deduction game.</p>
<p>We benchmark our dataset by providing comprehensive experimental results and analyzing the role</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Demonstration of the six persuasion strategies included in our dataset and the corresponding video. The players are numbered as $1,2,3,4$ from left to right. Players' roles might be changed during the game. In this example, player1's and player2's cards were swapped by the troublemaker, and player3's and player4's cards were swapped by the robber. Player 2 voted for player 3 at the end while the others voted for player 2.
of the video modality and contextual cues in designing computational models for persuasion behavior prediction. We also provide results to show how different computational models generalize across different data sources and different games. Our contributions are summarized as follows:</p>
<ul>
<li>We present the first multimodal dataset for persuasion modeling. Our dataset is collected in naturalistic social game scenarios with intensive face-to-face group conversations.</li>
<li>We conduct comprehensive experiments to show the importance of context and visual signals for persuasion strategy prediction.</li>
<li>We provide additional experimental results to investigate model generalization on the persuasion modeling task and discuss how persuasion strategy influences the game voting outcome.</li>
</ul>
<h2>2 Related Work</h2>
<p>Persuasive Behaviors Understanding. A few previous works introduce datasets for the computational modeling of persuasion (Yang et al., 2019; Chen and Yang, 2021; Chawla et al., 2021; Wang et al., 2019; Luu et al., 2019; Atkinson et al., 2019). As summarized in Table 1, existing works collectpersuasion language data from online platforms</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prior Works</th>
<th style="text-align: center;">Interaction Modalities</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Setting</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(Yang et al., 2019)</td>
<td style="text-align: center;">Online</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Loan</td>
</tr>
<tr>
<td style="text-align: center;">(Chen and Yang, 2021)</td>
<td style="text-align: center;">Online</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Request</td>
</tr>
<tr>
<td style="text-align: center;">(Chawla et al., 2021)</td>
<td style="text-align: center;">1 on 1</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Negotiation</td>
</tr>
<tr>
<td style="text-align: center;">(Wang et al., 2019)</td>
<td style="text-align: center;">1 on 1</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Charity</td>
</tr>
<tr>
<td style="text-align: center;">(Luu et al., 2019)</td>
<td style="text-align: center;">1 on 1</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Debate</td>
</tr>
<tr>
<td style="text-align: center;">(Atkinson et al., 2019)</td>
<td style="text-align: center;">Online</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Reddit</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Group</td>
<td style="text-align: center;">Text+Video</td>
<td style="text-align: center;">Deduction</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Discussion</td>
<td style="text-align: center;">+Audio</td>
<td style="text-align: center;">Game</td>
</tr>
</tbody>
</table>
<p>Table 1: Previous datasets for computational persuasion.
where real-time communication is not available. Moreover, these datasets mainly contain 1-on-1 conversations and lack conversations among multiple speakers. In contrast to these prior efforts, our dataset targets capturing persuasive behaviors in a social group setting of 4 to 6 people.
Multimodal Social Interaction. A rich set of literature has addressed the problem of multimodal sentiment analysis (Xu et al., 2021b; Li et al., 2020; Lu et al., 2019; Xu et al., 2021a). We refer to a recent survey (Kaur and Kautish, 2022) for a more detailed discussion on this topic. The Ego4D Social benchmark (Grauman et al., 2022) includes the tasks of identifying who is looking at and talking to the camera wearer using video</p>
<p>and audio. Bara et al. (2021) adopts a multimodal approach to understanding dialogue behavior in a simulated setting. The most relevant work is Bai et al. (2021), which adopts a multimodal method to predict the debate outcome from a TV show. In contrast, we address the challenging tasks of predicting the utterance-level persuasion strategy and the deduction outcome from a naturalistic conversation, which requires a richer understanding of high-level social behaviors.
Computational Modeling of Deduction Games. Prior works have investigated computational models for social deduction games. One stream of work seeks to analyze strategies and develop AI agents that play deduction games using a game theory approach (Nakamura et al., 2016; Serrino et al., 2019; Chuchro, 2022; Braverman et al., 2008; Bi and Tanaka, 2016). These works focus on models of the state of the game alone and do not address understanding the dialogue and persuasive behaviors that often occur while playing. More relevantly, Chittaranjan and Hung (Chittaranjan and Hung, 2010) developed a model for predicting Werewolf game outcomes from player speaking and interrupting behaviors. Recently, (FAIR) introduced a game agent-CICERO, that achieves human-level performance in the Diplomacy game by leveraging a language model with planning and reinforcement learning algorithms. In contrast to these prior works, we present the first work for understanding persuasive behaviors in a group setting from a multimodal perspective.</p>
<h2>3 Dataset</h2>
<h3>3.1 Data collection</h3>
<p>To benchmark the generalization ability of the computational models, we collect our data from different sources, as detailed in this section. This work was approved by an Institutional Review Board.</p>
<p>Ego4D Dataset We first leverage a subset of Ego4D Social dataset (Grauman et al., 2022) for our study. This subset captures videos of groups of participants playing social deduction games. This subset contains 7.3 hours of videos with 40 games of One Night Ultimate Werewolf and 8 games of The Resistance: Avalon. Note that the Avalon data has a relatively small scale, and therefore is only used to evaluate the cross-domain game generalization ability of our models. To ensure all participants are visible in the frame, we use third-person videos
instead of the first-person videos from Ego4D for visual representation learning and transcription.</p>
<p>YouTube Video We retrieve the top search results for YouTube videos using the keywords of "one night ultimate werewolf" and "ultimate werewolf". We manually select from the searched videos to make sure the they adopt a similar game setup as the Ego4D data. Specifically, we filter all results with more than 5 players or fewer than 4 players, and those using game roles from the expansion package. We finally collect a final set of 14.8 hours of videos with 151 clips of completed games that adopt the same game setup as the Ego4D dataset and have fully visible game outcomes. We will release the YouTube URLs for the selected videos.</p>
<h3>3.2 Data Annotation</h3>
<p>Video Annotation Most Ego4D and YouTube videos contain multiple games. Therefore, we first annotate the starting time (when the game narration voice begins) and the ending time (right before the voting stage) of each game. We then ask the annotators to look through each game clip and annotate the starting role, ending role, and the voting outcome of each player.</p>
<p>Transcription We use an automatic transcription service rev.com to generate the transcript of each game clip. We further ask annotators to carefully examine the alignment of the videos and transcripts, and manually correct any errors in the transcripts. Please refer to Appendix B for more details.</p>
<p>Persuasion Strategy Annotation Inspired by prior psychology studies and other works on predicting persuasion strategies (Chawla et al., 2021; Carlile et al., 2018; Yang et al., 2019; Chen and Yang, 2021), we propose six persuasion tactics that are frequently adopted in social deduction games.</p>
<ul>
<li>Identity Declaration: State one's own role or identity in the game. This is a game-specific persuasion tactic.</li>
<li>Accusation: Claim someone has a specific identity or strategic behavior. Accusation, similar to Undervalue-Partner (Chawla et al., 2021), is a generic proself behavior.</li>
<li>Interrogation: Questions about someone's identity or behavior. Interrogation, is a proself strategy related to individual preferences.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Label</th>
<th style="text-align: center;">Example</th>
<th style="text-align: center;">Ego4D</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">YouTube</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">AUL</td>
<td style="text-align: center;">$\boldsymbol{\alpha}$</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">AUL</td>
<td style="text-align: center;">$\boldsymbol{\alpha}$</td>
</tr>
<tr>
<td style="text-align: center;">Identity <br> Declaration</td>
<td style="text-align: center;">"I'll just come out and say I was a villager, so I have no idea what's going on."</td>
<td style="text-align: center;">293</td>
<td style="text-align: center;">9.87</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1066</td>
<td style="text-align: center;">10.43</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;">Accusation</td>
<td style="text-align: center;">"So James might be the werewolf."</td>
<td style="text-align: center;">669</td>
<td style="text-align: center;">11.28</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">2830</td>
<td style="text-align: center;">11.06</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">Interrogation</td>
<td style="text-align: center;">"Who did you rob?"</td>
<td style="text-align: center;">695</td>
<td style="text-align: center;">7.56</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">3407</td>
<td style="text-align: center;">7.66</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;">Call for Action</td>
<td style="text-align: center;">"We shouldn't vote to not kill anyone. And then there could also be no werewolf."</td>
<td style="text-align: center;">236</td>
<td style="text-align: center;">9.99</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">1163</td>
<td style="text-align: center;">9.53</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: center;">Defense</td>
<td style="text-align: center;">"I think that you accused me of being a Werewolf very quickly."</td>
<td style="text-align: center;">570</td>
<td style="text-align: center;">10.04</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">2696</td>
<td style="text-align: center;">9.75</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;">Evidence</td>
<td style="text-align: center;">"If you swapped these two, he is not the werewolf."</td>
<td style="text-align: center;">489</td>
<td style="text-align: center;">11.45</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">1740</td>
<td style="text-align: center;">9.80</td>
<td style="text-align: center;">0.60</td>
</tr>
</tbody>
</table>
<p>Table 2: Utterance-level persuasion strategy annotations. AUL refers to the average utterance length in terms of the number of words in an utterance and $\alpha$ refers to Krippendorff's alpha.</p>
<ul>
<li>Call for Action: Encourage people to take an action during the game. Call for Action relates to the coordination for persuasion (Chawla et al., 2021), which is a generic prosocial behavior.</li>
<li>Defense: Defend oneself or someone else against an accusation or defend a game-related argument. An utterance demonstrates Defense when the persuader tries to use credentials to earn others' trust or justify their earlier decisions.</li>
<li>Evidence: Provide a body of game-related fact or information. Evidentiality is a general persuasion tactic that has been widely studied in previous works (Carlile et al., 2018).</li>
</ul>
<p>Following previous work (Chawla et al., 2021), we annotate the persuasion strategy at the utterance level. We provide a website annotation tool adopted from Hayati et al. (2020) for our annotation task (see Appendix C for details) to the annotators. To properly train the annotators, we first ask all three annotators to annotate the same subset of dialogues and compute inter-annotator agreement using the nominal form of Krippendorff's alpha (Krippendorff, 2018). We then discuss with the annotators on their disagreements and come up with a general rule to address the disagreements during the annotation process. We repeat the above process until the annotators reached a Krippendorff's alpha greater than 0.6 for each category. Despite the subjectivity of persuasion strategies, the previous work (Chawla et al., 2021) suggests that annotations from 3 people are reasonable enough to most humans if they have a Krippendorff's alpha greater than 0.6. In Table 2, we report the per-class Krippendorff's alpha value for the final round of
inter-annotator agreement calculation. After the annotator training phase is completed, we ask the three annotators to independently annotate the rest of the Ego4D and YouTube data.</p>
<p>Annotation Statistics Our dataset has 5,815 utterances from the Ego4D data and 20,832 utterances from the YouTube data. More than $49.2 \%$ of Ego4D utterances are labeled as no strategy because of the naturalistic social setting, while only $37.9 \%$ of YouTube utterances are labeled as no strategy since players from the YouTube videos are more proficient at the game and focused more on gameplay. Furthermore, as shown in Table 2, the adopted persuasion strategies have an imbalanced distribution, where "Accusation", "Interrogation", and "Defense" are the most frequent strategies for both the Ego4D and YouTube videos. The annotators are recruited from a Startup Data Platform dedicated to research projects. All annotators are paid hourly at a rate above the federal minimum.</p>
<h2>4 Strategy Prediction</h2>
<p>Given an utterance and its corresponding video segment, we seek to predict the persuasion strategies adopted in the utterance. We first leverage a pretrained language model (Devlin et al., 2019; Liu et al., 2019) as the text encoder to obtain the utterance embedding, and a vision transformer (Fan et al., 2021) to obtain the visual embedding. We then concatenate the textual and visual features to predict the persuasion strategy. Additionally, we study the impact of textual context by including prior utterances as input.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Architecture of the independent model for each strategy. We fix the parameters in the video encoder and train the other modules end-to-end. $\oplus$ denotes the concatenation of two feature representations</p>
<h3>4.1 Methodology</h3>
<p>In our dataset, an utterance may be labeled with multiple persuasion strategies. For instance, "I'm a villager and she is the werewolf." is labeled as both identity declaration and accusation. Therefore, we formulate this task as a binary classification problem for each strategy and consider an utterance as non-strategic if it gets negative labels in all strategies. The most straightforward approach to solving this task is fine-tuning a pre-trained language model, which is referred to as Base model. In addition, we consider the following approaches:
Modeling with Context Embedding. Since some persuasion strategies cannot be easily recognized from one single utterance, we further consider a model with additional context (prior utterances) for each utterance. This is denoted as Base $+C$.
Modeling with Video Representation. We further leverage the non-verbal signals by combining video features with the text representation for persuasion modeling. We directly use a pre-trained Vision Transformer to extract video representations, and fuse the video and text representations before feeding them into the classification layer as shown in Fig. 2. We refer to this model as Base $+V$.
Late fusion of Video and Context. Finally, we adopt a late fusion model (Base $+C+V$ ) that incorporates both video features and context cues for persuasion strategy prediction.</p>
<h3>4.2 Model Details</h3>
<p>We perform our experiments using both BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) as backbones for the text encoder. We use the bert-base-uncased and roberta-base models from Huggingface (Wolf et al., 2020) in our implementation. We adopt MViT-B-24 (Fan et al., 2021) pretrained on Kinetics-400 (Kay et al., 2017) as the video encoder. Moreover, we also implement a multi-task model (Chawla et al., 2021) as an additional baseline, referred to as MT-BERT. Context and video features are incorporated into MT-BERT in the same way as BERT and RoBERTa.
Base Model. For base models, we obtain the textual input $\mathcal{T}$ from the current utterance only. Then we input $\mathcal{T}$ into a text encoder $\phi$ followed by a classifier to get the strategy prediction.
Base + C. We first concatenate the $k$ previous utterances $C_{1}, C_{2}, \cdots, C_{k}$ with an [EOS] token to get context $C$, and then concatenate this with the current utterance $U$ using a [SEP] token to get the final input $\mathcal{T}$. Formally, we have</p>
<p>$$
\begin{aligned}
&amp; C=C_{1}[\mathrm{EOS}] C_{2}[\mathrm{EOS}] \cdots C_{k} \
&amp; \mathcal{T}=C[\mathrm{SEP}] U
\end{aligned}
$$</p>
<p>Base + V. We use video encoder $\psi$ to extract visual representation $\psi(\mathcal{V})$ of the corresponding video clip $\mathcal{V}$. During training, video features are concatenated with the text representation $\phi(\mathcal{T})$ and fed into a fusion layer, which uses a linear mapping function $W_{P}^{\mathcal{T}}$ and an activation function $\operatorname{Tanh}(\cdot)$. Finally, we apply a linear classifier $W_{P}^{\mathcal{T}}$ to obtain the prediction logits, which can be formulated as</p>
<p>$$
\text { logits }=W_{P}^{\mathcal{T}} \cdot \operatorname{Tanh}\left(W_{P}^{\mathcal{T}} \cdot(\phi(\mathcal{T}) \oplus \psi(\mathcal{V}))\right)
$$</p>
<p>where $\oplus$ denotes the concatenation of two vectors. Note that we fix the parameters of the video encoder during training. Please refer to Appendix D for more details on visual representation extraction.
Base $+\mathbf{C}+\mathbf{V}$. We further late fuse $+\mathbf{C}$ with $+\mathbf{V}$. Formally, we denote the probability predictions of the two models after softmax as $P_{C}$ and $P_{V}$. Then the output after linear combination is formulated as</p>
<p>$$
P_{C, V}=(1-\lambda) P_{C}+\lambda P_{V}
$$</p>
<p>where $\lambda$ is a scalar that balances $P_{C}$ and $P_{V}$.</p>
<h3>4.3 Training Details</h3>
<p>All models are trained using cross-entropy loss. For training hyper-parameters, we do a grid search</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Ablation study of adopting different context lengths for persuasion strategy prediction.</p>
<p>of learning rates in {1e − 5, 3e − 5, 5e − 5} and batch sizes in {16, 8} for Base models. We then fix the optimal hyper-parameters for subsequent models incorporating context or videos. We train all models with the optimal learning rates and batch sizes for 10 epochs using AdamW (Loshchilov and Hutter, 2017) as the optimizer. We run all the experiments with three random seeds and report the average score and the standard deviation.</p>
<h3>4.4 Experiment Results</h3>
<p><strong>Evaluation Metrics</strong>. Following (Chawla et al., 2021), we report the F1 score for each persuasion strategy category, the average F1 score of all categories, and Joint Accuracy. Note that the prediction is considered as correct when all the categories are predicted correctly in Joint Accuracy.</p>
<p><strong>Ablations on Additional Context</strong>. We first present a systematic ablation study of how incorporating textual context may improve the performance of persuasion strategy prediction. Specifically, we feed a fixed length of previous utterances together with the current utterance into the backbone language encoders for classification. As shown in Fig. 3, the additional context can boost the performance of all baseline models. However, setting the context length too long may confuse the model, especially for categories that can be reliably predicted with the current utterance (<em>e.g.</em> Identity Declaration, and Interrogation). We present the per-class performance in Appendix E. Our empirical finding is that a context length of 5 can consistently improve the performance of all three baseline models. Therefore, we adopt a context length of 5 as a default setting for the rest of our experiments.</p>
<p><strong>Modeling with Video Representation</strong>. We further study how incorporating video representation improves the performance of persuasion modeling. The results are summarized in Table 3. Importantly, video features can improve the BERT model by 0.8% on both the Ego4D dataset and YouTube dataset. However, RoBERTa+V only beats the RoBERTa model by 0.2% on the YouTube dataset. This may be because the YouTube dataset has more training data which enables the RoBERTa model to learn a robust representation without video feature embedding. Interestingly, including video features has a larger performance boost on predicting "Accusation", "Interrogation", and "Call for Action", which is likely due to the more frequent non-verbal communication (<em>e.g.</em> pointing to someone, raising hands, turning the head) during these persuasive behaviors.</p>
<p><strong>Off-the-shelf GPT-3 Inference</strong>. Prompting Large Language Models off-the-shelf to solve NLP tasks has received increasing attention (Brown et al., 2020). Here, we experiment with GPT-3-175B on our benchmark under three settings: zero-shot, one-shot and five-shot. Specifically, we use the text-davinci-002 engine from OpenAI's API<sup>2</sup> with temperature 0 to produce a deterministic answer. The detailed templates for different settings are shown in Appendix G. The result is shown in Table 4. Using GPT-3 off-the-shelf achieves a non-trivial performance (Joint-A of 52.0 vs. 38.8 for majority on YouTube data). Adding more examples further boosts performance, though it is still inferior to the fine-tuned models.</p>
<p><strong>Data Domain Generalization</strong>. We conduct additional experiments to show the generalization ability of language models on persuasion prediction. To begin with, we use the model trained on the</p>
<p><sup>2</sup>https://beta.openai.com/</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Identity</th>
<th style="text-align: center;">Accusation</th>
<th style="text-align: center;">Interrogation</th>
<th style="text-align: center;">Call for Action</th>
<th style="text-align: center;">Defense</th>
<th style="text-align: center;">Evidence</th>
<th style="text-align: center;">Avg F1</th>
<th style="text-align: center;">Joint-A</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Ego4D</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">$82.6 \pm 1.1$</td>
<td style="text-align: center;">$48.8 \pm 4.8$</td>
<td style="text-align: center;">$82.8 \pm 0.2$</td>
<td style="text-align: center;">$39.4 \pm 9.6$</td>
<td style="text-align: center;">$29.3 \pm 5.5$</td>
<td style="text-align: center;">$54.2 \pm 2.5$</td>
<td style="text-align: center;">$56.2 \pm 2.5$</td>
<td style="text-align: center;">$65.1 \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + C</td>
<td style="text-align: center;">$79.9 \pm 1.6$</td>
<td style="text-align: center;">$52.0 \pm 3.3$</td>
<td style="text-align: center;">$81.0 \pm 1.1$</td>
<td style="text-align: center;">$49.5 \pm 3.2$</td>
<td style="text-align: center;">$33.8 \pm 0.5$</td>
<td style="text-align: center;">$57.1 \pm 1.6$</td>
<td style="text-align: center;">$58.9 \pm 0.6$</td>
<td style="text-align: center;">$65.0 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + V</td>
<td style="text-align: center;">$81.5 \pm 3.5$</td>
<td style="text-align: center;">$52.1 \pm 1.9$</td>
<td style="text-align: center;">$83.3 \pm 1.6$</td>
<td style="text-align: center;">$42.4 \pm 3.8$</td>
<td style="text-align: center;">$28.4 \pm 5.1$</td>
<td style="text-align: center;">$52.8 \pm 1.0$</td>
<td style="text-align: center;">$56.7 \pm 1.2$</td>
<td style="text-align: center;">$64.5 \pm 1.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + C + V</td>
<td style="text-align: center;">$\mathbf{8 4 . 5} \pm 4.6$</td>
<td style="text-align: center;">$52.8 \pm 2.0$</td>
<td style="text-align: center;">$82.7 \pm 0.4$</td>
<td style="text-align: center;">$47.3 \pm 3.4$</td>
<td style="text-align: center;">$34.5 \pm 1.7$</td>
<td style="text-align: center;">$54.9 \pm 1.1$</td>
<td style="text-align: center;">$59.4 \pm 1.6$</td>
<td style="text-align: center;">$\mathbf{6 6 . 5} \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">$81.7 \pm 2.6$</td>
<td style="text-align: center;">$51.7 \pm 0.9$</td>
<td style="text-align: center;">$83.4 \pm 0.9$</td>
<td style="text-align: center;">$43.3 \pm 8.7$</td>
<td style="text-align: center;">$33.1 \pm 2.2$</td>
<td style="text-align: center;">$51.7 \pm 2.1$</td>
<td style="text-align: center;">$57.5 \pm 1.4$</td>
<td style="text-align: center;">$63.4 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + C</td>
<td style="text-align: center;">$81.5 \pm 0.7$</td>
<td style="text-align: center;">$\mathbf{5 9 . 4} \pm 2.4$</td>
<td style="text-align: center;">$83.5 \pm 1.1$</td>
<td style="text-align: center;">$43.7 \pm 3.7$</td>
<td style="text-align: center;">$33.0 \pm 3.1$</td>
<td style="text-align: center;">$52.4 \pm 2.9$</td>
<td style="text-align: center;">$58.9 \pm 1.2$</td>
<td style="text-align: center;">$64.6 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + V</td>
<td style="text-align: center;">$79.8 \pm 0.6$</td>
<td style="text-align: center;">$51.4 \pm 1.0$</td>
<td style="text-align: center;">$82.8 \pm 2.1$</td>
<td style="text-align: center;">$50.1 \pm 5.3$</td>
<td style="text-align: center;">$31.3 \pm 3.1$</td>
<td style="text-align: center;">$54.6 \pm 3.2$</td>
<td style="text-align: center;">$58.3 \pm 0.7$</td>
<td style="text-align: center;">$64.0 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + C + V</td>
<td style="text-align: center;">$82.7 \pm 0.2$</td>
<td style="text-align: center;">$58.5 \pm 2.3$</td>
<td style="text-align: center;">$83.8 \pm 1.2$</td>
<td style="text-align: center;">$46.1 \pm 4.5$</td>
<td style="text-align: center;">$35.4 \pm 3.4$</td>
<td style="text-align: center;">$53.4 \pm 3.3$</td>
<td style="text-align: center;">$60.0 \pm 0.8$</td>
<td style="text-align: center;">$66.1 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT</td>
<td style="text-align: center;">$80.9 \pm 1.3$</td>
<td style="text-align: center;">$51.5 \pm 3.3$</td>
<td style="text-align: center;">$83.0 \pm 1.3$</td>
<td style="text-align: center;">$\mathbf{5 6 . 6} \pm 2.3$</td>
<td style="text-align: center;">$25.9 \pm 2.0$</td>
<td style="text-align: center;">$53.6 \pm 1.3$</td>
<td style="text-align: center;">$58.6 \pm 0.3$</td>
<td style="text-align: center;">$65.5 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + C</td>
<td style="text-align: center;">$79.8 \pm 2.2$</td>
<td style="text-align: center;">$54.4 \pm 0.8$</td>
<td style="text-align: center;">$83.2 \pm 0.7$</td>
<td style="text-align: center;">$50.8 \pm 7.2$</td>
<td style="text-align: center;">$\mathbf{3 6 . 5} \pm 2.8$</td>
<td style="text-align: center;">$\mathbf{6 1 . 5} \pm 2.2$</td>
<td style="text-align: center;">$\mathbf{6 1 . 0} \pm 1.1$</td>
<td style="text-align: center;">$66.3 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + V</td>
<td style="text-align: center;">$79.9 \pm 1.6$</td>
<td style="text-align: center;">$51.9 \pm 0.8$</td>
<td style="text-align: center;">$\mathbf{8 4 . 8} \pm 2.4$</td>
<td style="text-align: center;">$53.9 \pm 4.5$</td>
<td style="text-align: center;">$35.4 \pm 2.2$</td>
<td style="text-align: center;">$53.3 \pm 1.0$</td>
<td style="text-align: center;">$59.8 \pm 0.7$</td>
<td style="text-align: center;">$62.1 \pm 3.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + C + V</td>
<td style="text-align: center;">$80.7 \pm 1.9$</td>
<td style="text-align: center;">$55.2 \pm 0.9$</td>
<td style="text-align: center;">$83.6 \pm 0.6$</td>
<td style="text-align: center;">$50.0 \pm 0.8$</td>
<td style="text-align: center;">$36.1 \pm 2.7$</td>
<td style="text-align: center;">$60.5 \pm 1.0$</td>
<td style="text-align: center;">$\mathbf{6 1 . 0} \pm 0.3$</td>
<td style="text-align: center;">$66.3 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">$80.2 \pm 1.6$</td>
<td style="text-align: center;">$64.7 \pm 1.1$</td>
<td style="text-align: center;">$89.6 \pm 0.4$</td>
<td style="text-align: center;">$77.2 \pm 2.5$</td>
<td style="text-align: center;">$43.5 \pm 1.0$</td>
<td style="text-align: center;">$58.3 \pm 0.7$</td>
<td style="text-align: center;">$68.9 \pm 0.0$</td>
<td style="text-align: center;">$64.6 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + C</td>
<td style="text-align: center;">$82.6 \pm 0.7$</td>
<td style="text-align: center;">$66.7 \pm 1.0$</td>
<td style="text-align: center;">$89.6 \pm 1.5$</td>
<td style="text-align: center;">$78.1 \pm 2.4$</td>
<td style="text-align: center;">$45.7 \pm 1.1$</td>
<td style="text-align: center;">$59.7 \pm 1.1$</td>
<td style="text-align: center;">$70.4 \pm 0.3$</td>
<td style="text-align: center;">$64.4 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + V</td>
<td style="text-align: center;">$82.4 \pm 0.5$</td>
<td style="text-align: center;">$65.4 \pm 1.4$</td>
<td style="text-align: center;">$89.7 \pm 0.1$</td>
<td style="text-align: center;">$78.0 \pm 0.8$</td>
<td style="text-align: center;">$45.3 \pm 2.8$</td>
<td style="text-align: center;">$58.4 \pm 1.3$</td>
<td style="text-align: center;">$69.9 \pm 0.4$</td>
<td style="text-align: center;">$66.2 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + C + V</td>
<td style="text-align: center;">$83.6 \pm 0.1$</td>
<td style="text-align: center;">$67.2 \pm 1.2$</td>
<td style="text-align: center;">$\mathbf{9 0 . 2} \pm 1.0$</td>
<td style="text-align: center;">$78.5 \pm 1.6$</td>
<td style="text-align: center;">$46.6 \pm 1.1$</td>
<td style="text-align: center;">$59.9 \pm 1.0$</td>
<td style="text-align: center;">$71.0 \pm 0.2$</td>
<td style="text-align: center;">$66.7 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">$84.3 \pm 0.1$</td>
<td style="text-align: center;">$67.2 \pm 0.6$</td>
<td style="text-align: center;">$89.4 \pm 0.1$</td>
<td style="text-align: center;">$78.2 \pm 0.8$</td>
<td style="text-align: center;">$44.3 \pm 0.4$</td>
<td style="text-align: center;">$59.0 \pm 1.7$</td>
<td style="text-align: center;">$70.4 \pm 0.2$</td>
<td style="text-align: center;">$64.8 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + C</td>
<td style="text-align: center;">$82.4 \pm 0.3$</td>
<td style="text-align: center;">$67.0 \pm 1.1$</td>
<td style="text-align: center;">$\mathbf{9 0 . 2} \pm 0.0$</td>
<td style="text-align: center;">$77.1 \pm 1.0$</td>
<td style="text-align: center;">$46.1 \pm 0.7$</td>
<td style="text-align: center;">$59.9 \pm 0.7$</td>
<td style="text-align: center;">$70.5 \pm 0.3$</td>
<td style="text-align: center;">$64.7 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + V</td>
<td style="text-align: center;">$83.4 \pm 0.4$</td>
<td style="text-align: center;">$66.4 \pm 0.3$</td>
<td style="text-align: center;">$89.5 \pm 0.1$</td>
<td style="text-align: center;">$\mathbf{7 8 . 7} \pm 2.0$</td>
<td style="text-align: center;">$46.6 \pm 0.6$</td>
<td style="text-align: center;">$59.0 \pm 1.0$</td>
<td style="text-align: center;">$70.6 \pm 0.1$</td>
<td style="text-align: center;">$65.3 \pm 1.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + C + V</td>
<td style="text-align: center;">$83.7 \pm 0.6$</td>
<td style="text-align: center;">$67.4 \pm 0.4$</td>
<td style="text-align: center;">$89.8 \pm 0.3$</td>
<td style="text-align: center;">$78.5 \pm 1.2$</td>
<td style="text-align: center;">$\mathbf{4 8 . 2} \pm 0.7$</td>
<td style="text-align: center;">$60.4 \pm 0.8$</td>
<td style="text-align: center;">$\mathbf{7 1 . 3} \pm 0.2$</td>
<td style="text-align: center;">$66.4 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT</td>
<td style="text-align: center;">$80.7 \pm 0.4$</td>
<td style="text-align: center;">$65.1 \pm 1.5$</td>
<td style="text-align: center;">$88.5 \pm 0.8$</td>
<td style="text-align: center;">$76.2 \pm 2.2$</td>
<td style="text-align: center;">$42.3 \pm 1.5$</td>
<td style="text-align: center;">$57.4 \pm 1.3$</td>
<td style="text-align: center;">$68.4 \pm 0.3$</td>
<td style="text-align: center;">$65.6 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + C</td>
<td style="text-align: center;">$83.1 \pm 1.1$</td>
<td style="text-align: center;">$65.0 \pm 1.5$</td>
<td style="text-align: center;">$90.1 \pm 0.3$</td>
<td style="text-align: center;">$74.6 \pm 2.4$</td>
<td style="text-align: center;">$46.5 \pm 0.8$</td>
<td style="text-align: center;">$59.2 \pm 0.3$</td>
<td style="text-align: center;">$69.7 \pm 0.6$</td>
<td style="text-align: center;">$66.7 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + V</td>
<td style="text-align: center;">$82.8 \pm 0.6$</td>
<td style="text-align: center;">$\mathbf{6 8 . 5} \pm 1.0$</td>
<td style="text-align: center;">$89.3 \pm 0.7$</td>
<td style="text-align: center;">$75.6 \pm 2.8$</td>
<td style="text-align: center;">$47.8 \pm 0.3$</td>
<td style="text-align: center;">$59.6 \pm 0.8$</td>
<td style="text-align: center;">$70.6 \pm 0.8$</td>
<td style="text-align: center;">$66.9 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + C + V</td>
<td style="text-align: center;">$\mathbf{8 4 . 4} \pm 0.6$</td>
<td style="text-align: center;">$68.4 \pm 1.0$</td>
<td style="text-align: center;">$89.5 \pm 0.6$</td>
<td style="text-align: center;">$76.5 \pm 2.1$</td>
<td style="text-align: center;">$47.3 \pm 0.5$</td>
<td style="text-align: center;">$\mathbf{6 0 . 6} \pm 0.2$</td>
<td style="text-align: center;">$71.1 \pm 0.5$</td>
<td style="text-align: center;">$\mathbf{6 8 . 1} \pm 0.2$</td>
</tr>
</tbody>
</table>
<p>Table 3: Experimental Results on incorporating visual features for persuasion strategy prediction. We train an independent model for each category using BERT and RoBERTa backbones. Additionally, we also use the off-theshelf Multi-Task BERT model (MT-BERT) (Chawla et al., 2021) to jointly predict all categories.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: center;">Ego4D</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">YouTube</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Avg F1</td>
<td style="text-align: center;">Joint-A</td>
<td style="text-align: center;">Avg F1</td>
<td style="text-align: center;">Joint-A</td>
</tr>
<tr>
<td style="text-align: left;">Majority</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">38.8</td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: left;">One-Shot</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">53.2</td>
</tr>
<tr>
<td style="text-align: left;">Five-Shot</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">53.7</td>
</tr>
</tbody>
</table>
<p>Table 4: GPT-3 results on Ego4D and YouTube data.</p>
<p>YouTube data to make predictions on the Ego4D Werewolf testing data without any fine-tuning. As shown in Fig. 4, the resulting model achieves better performance than models trained only on Ego4D in most cases, due to the larger amount of available training data from the YouTube dataset. This also suggests that, for the text modality, the domain gap between the Ego4D and the YouTube data is small. We further fine-tune the model trained on the YouTube data with the Ego4D training data, and the resulting model performs even better. These results suggest promise in leveraging the large body of videos available online as a pre-training source for persuasion modeling in naturalistic social interactions. Another finding from our experiments is
that the multi-task setting (MT-BERT) may compromise the model's generalization ability. We also find that including video representation cannot improve the model generalization ability (see more details in Appendix D), suggesting that the video modality domain gap between the two data sources is much larger than the text modality.</p>
<p>Game Domain Generalization. We also study the model generalization ability on another social deduction game - Avalon. Werewolf and Avalon are vastly different in the game rules and winning conditions, especially because Werewolf has only one voting round per game, while Avalon has multiple rounds per game. Therefore, the persuasion strategies adopted in Avalon have a different distribution from Werewolf (see Appendix H). We run inference on the Avalon data using models trained only on the Ego4D Werewolf data without fine-tuning. Results are shown in Figure 5. Despite the large domain gap between the two games, our models achieve decent performance on the Avalon data. However, we find incorporating additional context has marginal performance improvements, and may even compromise the performance of the RoBERTa</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Data domain generalization experiments. We report the testing performance on the Ego4D dataset using models trained only on YouTube data (w.o. Fine-tuning), and trained on YouTube data and further fine-tuned with Ego4D data (w. Fine-tuning). We also report the performance (refer to Table 3) of the models trained only on Ego4D dataset (Ego4D Only) as comparison.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Game domain generalization experiments. We report the testing performance on Ego4D Avalon data using models trained only on Ego4D Werewolf data.</p>
<p>model. The detailed results of data and game domain generalization are shown in Appendix F.</p>
<h2>5 Game Outcome Deduction</h2>
<p>In addition to predicting persuasion strategies, we further model the human deduction process by predicting the voting outcomes of each pair of players, <em>i.e.</em>, whether player A (<em>voter</em>) votes for player B (<em>candidate</em>). Therefore, in a game of <em>n</em> players, there are <em>C<sup>2</sup><sub>n</sub></em> (Combinations) of player pairs, corresponding to <em>P<sup>2</sup><sub>n</sub></em> (Permutations) data points. We merge all data points from the Ego4D Werewolf data and YouTube data to enlarge the dataset size, and split the resulting data into 2741/427/827 samples for train/val/test sets. Since each player is only allowed to vote for one player, the resulting data has an imbalanced distribution, with 20.4% of the samples being positive (positive indicates the voter votes for the candidate).</p>
<h3>5.1 Method</h3>
<p>For deduction modeling, we encode the input feature with three embeddings: a 7 × 1 vector representing the persuasion strategy distribution (including non-strategy) adopted by the voter; a 7 × 1 vector representing the persuasion strategy distribution adopted by the candidate; and a 12 × 1 one-hot vector representing starting role of the voter (One Night Werewolf has 12 roles in total). Therefore, the input for deduction is a 26 × 1 vector. We use a simple logistic regression model for deduction modeling. To address the class imbalance of positive and negative samples, we train the model with weighted binary classification loss.</p>
<h3>5.2 Experiment Results</h3>
<p>Our model achieves an F1 of 32.7% and an AUC of 54.7%, outperforming random prediction, which obtains F1 and AUC of 28.6% and 50.0%, respectively. These results show the effectiveness of persuasion strategy usage and role as predictors for game-level outcomes. To analyze the contribution of persuasion strategy embedding and role embedding, we consider another model that only takes the persuasion strategy embeddings as inputs. This model achieves an F1 of 32.2% and an AUC of 54.6%. Overall we find that the persuasion strategy embedding is more informative for the predicting game outcomes than the role embedding.</p>
<p>We visualize the weights of logistic regression in Fig. 6. Interestingly, for positive prediction (the voter votes for the candidate), the weights of the candidate are higher than the voter. It indicates that a player's voting choice depends more on the candidate's behaviors. This confirms our intuition that players make their decisions based on candidates' arguments.</p>
<p>As for the negative prediction, we see that evidence is the most important strategy for the candi-</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Weights visualization of persuasion strategies in logistic regression. The connection between a strategy and 0 means this strategy contributes to the prediction of 0 (i.e. the voter doesn't vote for the candidate). Likewise, the connection between a strategy and 1 denotes this strategy contributes to the prediction of 1 (i.e. the voter votes for the candidate). The transparency of lines corresponds to the weights of logistic regression. A less transparent line suggests a greater weight and more impact on the output.
date to negate suspicion. It confirms that players are inclined to trust those who provide more information and evidence to find the werewolf.</p>
<h2>6 Conclusion and Future Work</h2>
<p>In this work, we introduce the first persuasion modeling dataset with multiple modalities and rich utterance-level persuasion strategy annotations. We design a computational model that leverages both textual and visual representations for understanding persuasion behaviors in social deduction games. Our experiments show that visual cues benefit model performance on persuasion strategy prediction. We encourage future work to explore the role of the audio modality in persuasion modeling and to investigate joint learning of multimodal representations for the social persuasion setting.</p>
<h2>Limitations</h2>
<p>We only use pre-trained video transformers off-theshelf to encode the videos, while more nuanced and specific utilization of other models can be explored to further improve the performance. There are also valuable egocentric videos and demographic statistics along with the Ego4D dataset that we have not yet incorporated in our approach. Due to the difficulty and cost of collecting videos with transcriptions and voting outcome annotations, the total number of games is insufficient to train a deep neural network for voting outcome deduction, though data augmentation techniques can be explored to mitigate this limitation.</p>
<h2>Ethics Statement</h2>
<p>How humans use persuasion strategies in their communication has been long studied in psychology, communication, and NLP (Hovland et al., 1953; Crano and Prislin, 2006; Petty and Cacioppo, 1986; Yang et al., 2019; Wang et al., 2019; Chen and Yang, 2021). We recognize that persuasion skills could be used for good and bad purposes. In this study, our goal is to study persuasive behaviors by multiple speakers through social deduction games. While we recognize that in both games of One Night Ultimate Werewolf and Avalon games players could use persuasion strategies for behaviors that perhaps are considered morally wrong, such as deception, bias, and emotional manipulation, our study does not encourage such behaviors. Instead, we aim to understand people's behavior in a group setting when persuasion happens. Having these persuasion skills could benefit people to perform well in their workplace, such as pitching their ideas, or advocating for peace-making (Simons, 1976).</p>
<p>For our data collection and annotation process, this study has been reviewed and approved by our institution's internal review board. We obtain consent from the players who are recorded and deidentify personally identifiable information (PII), as part of the Ego4D efforts. Moreover, to mitigate potential risks of harmful usage of this dataset in the future, we ask any users to sign an online agreement before using our resources for their research as follows: "I will not use this dataset for malicious purposes (but not limited to): deception, impersonation, mockery, discrimination, manipulation, targeted harassment, and hate speech."</p>
<h2>Acknowledgements</h2>
<p>We are thankful to the members of SALT Lab for their helpful feedback on the draft. This research was supported, in part, by NSF CCRI Research Infrastructure CNS-2308994.</p>
<h2>References</h2>
<p>David Atkinson, Kumar Bhargav Srinivasan, and Chenhao Tan. 2019. What gets echoed? understanding the "pointers" in explanations of persuasive arguments. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2911-2921, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Chongyang Bai, Haipeng Chen, Srijan Kumar, Jure Leskovec, and VS Subrahmanian. 2021. M2p2: Multimodal persuasion prediction using adaptive fusion. IEEE Transactions on Multimedia.</p>
<p>Cristian-Paul Bara, Sky CH-Wang, and Joyce Chai. 2021. MindCraft: Theory of mind modeling for situated dialogue in collaborative tasks. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1112-1125, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Xiaoheng Bi and Tetsuro Tanaka. 2016. Human-side strategies in the werewolf game against the stealth werewolf strategy. In International Conference on Computers and Games, pages 93-102. Springer.</p>
<p>Mark Braverman, Omid Etesami, and Elchanan Mossel. 2008. Mafia: A theoretical study of players and coalitions in a partial information environment. The Annals of Applied Probability, 18(3):825-846.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Winston Carlile, Nishant Gurrapadi, Zixuan Ke, and Vincent Ng. 2018. Give me more feedback: Annotating argument persuasiveness and related attributes in student essays. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 621-631, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale Lucas, Jonathan May, and Jonathan Gratch. 2021. CaSiNo: A corpus of campsite negotiation dialogues for automatic negotiation systems. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3167-3185, Online. Association for Computational Linguistics.</p>
<p>Jiaao Chen and Diyi Yang. 2021. Weakly-supervised hierarchical models for predicting persuasive strategies in good-faith textual requests. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12648-12656.</p>
<p>Gokul Chittaranjan and Hayley Hung. 2010. Are you awerewolf? detecting deceptive roles and outcomes
in a conversational role-playing game. In 2010 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 5334-5337. IEEE.</p>
<p>Robert Chuchro. 2022. Training an assassin ai for the resistance: Avalon. arXiv preprint arXiv:2209.09331.</p>
<p>William D Crano and Radmila Prislin. 2006. Attitudes and persuasion. Annual review of psychology, 57:345.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Meta Fundamental AI Research Diplomacy Team (FAIR) $\dagger$, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. 2022. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067-1074.</p>
<p>Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. 2021. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6824-6835.</p>
<p>Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. 2022. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995-19012.</p>
<p>Shirley Anugrah Hayati, Dongyeop Kang, Qingxiaoyang Zhu, Weiyan Shi, and Zhou Yu. 2020. INSPIRED: Toward sociable recommendation dialog systems. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8142-8152, Online. Association for Computational Linguistics.</p>
<p>Carl Iver Hovland, Irving Lester Janis, and Harold H Kelley. 1953. Communication and persuasion. Yale University Press.</p>
<p>Ramandeep Kaur and Sandeep Kautish. 2022. Multimodal sentiment analysis: A survey and comparison. Research Anthology on Implementing Sentiment Analysis Across Multiple Disciplines, pages 1846-1870.</p>
<p>Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. 2017. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950.</p>
<p>Klaus Krippendorff. 2018. Content analysis: An introduction to its methodology. Sage publications.</p>
<p>Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. 2020. Hero: Hierarchical encoder for video+language omni-representation pretraining. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.</p>
<p>Antonia Lonigro, Roberto Baiocco, Emma Baumgartner, and Fiorenzo Laghi. 2017. Theory of mind, affective empathy, and persuasive strategies in schoolaged children. Infant and Child Development, 26(6):e2022.</p>
<p>Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.</p>
<p>Chujie Lu, Long Chen, Chilie Tan, Xiaolin Li, and Jun Xiao. 2019. Debug: A dense bottom-up grounding approach for natural language video localization. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Kelvin Luu, Chenhao Tan, and Noah A. Smith. 2019. Measuring online debaters' persuasive skill from text over time. Transactions of the Association for Computational Linguistics, 7:537-550.</p>
<p>Noritsugu Nakamura, Michimasa Inaba, Kenichi Takahashi, Fujio Toriumi, Hirotaka Osawa, Daisuke Katagami, and Kousuke Shinoda. 2016. Constructing a human-like agent for the werewolf game using a psychological model based multiple perspectives. In 2016 IEEE Symposium Series on Computational Intelligence (SSCI), pages 1-8. IEEE.</p>
<p>Richard E Petty and John T Cacioppo. 1986. The elaboration likelihood model of persuasion. In Communication and persuasion, pages 1-24. Springer.</p>
<p>David Premack and Guy Woodruff. 1978. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515-526.</p>
<p>Jack Serrino, Max Kleiman-Weiner, David C Parkes, and Josh Tenenbaum. 2019. Finding friend and foe in multi-agent games. Advances in Neural Information Processing Systems, 32.</p>
<p>Herbert W Simons. 1976. Persuasion. Reading: Addison-Wesley, 21.</p>
<p>Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, and Zhou Yu. 2019. Persuasion for good: Towards a personalized persuasive dialogue system for social good. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5635-5649, Florence, Italy. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, and Luke Zettlemoyer. 2021a. Vlm: Task-agnostic video-language model pretraining for video understanding. arXiv preprint arXiv:2105.09996.</p>
<p>Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, and Florian Metze Luke Zettlemoyer Christoph Feichtenhofer. 2021b. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Diyi Yang, Jiaao Chen, Zichao Yang, Dan Jurafsky, and Eduard Hovy. 2019. Let's make your request more persuasive: Modeling persuasive strategies via semisupervised neural nets on crowdfunding platforms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3620-3630, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<h2>A Game Rules</h2>
<p>One Night Werewolf. In this game, players are divided into two teams - the team of villagers and the team of werewolves. In each game, players close their eyes in the night phase and take some actions (e.g. swapping cards) depending on their roles. Players' roles might be changed during the night, but they don't know their new roles except for a few special roles. Then all players open their eyes. The villager team needs to find the werewolf through communication and negotiation. The werewolf team must mislead the others and try to hide their identities. At the end of the game, everyone has to point out the most suspicious player. If at</p>
<p>least one werewolf is voted out, the villager team wins the game. Otherwise, the werewolf team wins. We refer to https://en.wikipedia.org/wiki/ Ultimate_Werewolf#One_Night_roles for detailed explanations of game rules and roles.
The Resistance: Avalon. In this game, players are divided into two teams - the team of Minions and the team of Loyal Servants of Arthur. After shuffling and distributing cards to players, they secretly check their role cards and place them face down on the table. Each player will take turns serving as the Leader. In each round, the Leader proposes a Team to do a Quest, and all players are involved in discussing if the Team assignment is passed or rejected. After the Team Building phase, the approved Team will decide if the Quest is successful or not. In the Quest phase, the Good Team can only use the Quest Success card, and the Evil Team can use either Success or Fail card. The Good Team wins when three successful Quests are made, while the Evil Team wins when three failed Quests are made or the Evil players identify Merlin in the Good Team. We refer to https://en.wikipedia.org/wiki/The_ Resistance_(game)#Avalon_variant for detailed explanations of game rules and roles.</p>
<h2>B Transcription Interface</h2>
<p>We provide a screenshot of the transcription tool (rev.com) in Fig 7. We upload video clips to this online platform for transcription. We also provide player names and roles involved in each game to make the transcription more accurate. As illustrated in Fig 7, they return the transcription of each utterance, the name of the speaker and the corresponding timestamp. Then we ask annotators to watch videos again and examine the alignment of videos and transcripts. Annotators also correct errors in speakers' names and texts.</p>
<h2>C Annotation Interface</h2>
<p>We provide a screenshot in Fig. 8 of our interface used by annotators to annotate utterance-level persuasion strategies.</p>
<h2>D Details of Video Representation</h2>
<p>We now introduce video representation extraction. Given an utterance $U_{i}$, We first localize the corresponding video segment using the utterance timestamp $t_{i}$, and then approximate the duration of the
utterance by $d_{i}=t_{i+1}-t_{i}$. We have an average duration of 2 seconds on both the Ego4D and Youtube datasets. To tolerate some misalignment of videos and transcripts, we set a 2 -second time window for utterances shorter than 2 seconds, and hence the final duration of an utterance $U_{i}$ is $d_{i}^{\prime}=\max \left(d_{i}, 2\right)$. Then we sample $N$ frames out of the corresponding video segment with equal spacing, i.e., $\mathcal{V}=\left{\mathcal{V}<em 2="2">{1}, \mathcal{V}</em>}, \ldots, \mathcal{V<em i="i">{N}\right}$. All videos in our dataset have an aspect ratio of 16:9. Hence we make three square crops on the left, center, and right of each frame to cover the entire view. Correspondingly, The visual embedding from the vision encoder is composed of three parts, i.e., $\mathcal{V}</em>}=\left{\mathcal{V<em i="i">{i}^{\text {left }}, \mathcal{V}</em>\right)\right}$. The three video representations are flattened as a single vector when we concatenate them with the text representation. In our experiments, we adopt the 24-layer multiscale vision transformer (MViT) (Fan et al., 2021) pretrained on Kinetics-400 as the video encoder. The number of sampled frames $N$ is set as 32 in our experiments. Note that we don't finetune the video encoder on our datasets because the performance of some models drops after finetuning due to overfitting.}^{\text {center }}, \mathcal{V}_{i}^{\text {right }}\right}$. We input the left crops, center crops, and right crops into the video encoder separately and obtain the corresponding representations, i.e., $\psi(\mathcal{V})=$ $\left{\psi\left(\mathcal{V}^{\text {left }}\right), \psi\left(\mathcal{V}^{\text {center }}\right), \psi\left(\mathcal{V}^{\text {right }</p>
<p>In the experiments on Ego4D Werewolf data and Youtube data, the video features improve the performance prominently. However, the video feature does not necessarily help with model generalization. When we apply the model Base $+V$ trained on Youtube data to Ego4D data, the average F1 of BERT and RoBERTa drop by $0.8 \%$ and $3.0 \%$, respectively, while the F1 of MT-BERT increases by $1.1 \%$ after involving video features. This suggests a domain gap exists in the videos of the two datasets which is caused by the differences in the camera locations, angles of views, brightness in the room and etc. Video models are sensitive to these visual differences, resulting in limited performance in generalization on different datasets. In contrast, players communicate in a similar way in different conditions, so the pure text model generalizes better to other data.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Screenshot of the transcription tool.</p>
<h2>E Per-class Results for Experiments with Different Context Lengths</h2>
<p>We showcase our experimental results including per-strategy scores on incorporating additional conversational context, in full detail in Table 5.</p>
<h2>F Experiments of Domain Generalization</h2>
<p>We demonstrate the detailed experiment results of data domain generalization (training models on Youtube data and testing on Ego4D Werewolf test set), as well as game domain generalization (training models on Ego4D Werewolf data and testing on Avalon data). Results are reported in Table 6 and Table 7, respectively.</p>
<h2>G Detailed prompt templates used for GPT-3</h2>
<p>For the prompt templates, we use the guideline and the persuasion strategy definitions provided to the annotators under the zero-shot setting, and append one/five more examples under one/five-shot setting. The detailed prompt template we used for GPT-3 inference is shown in Table 8.</p>
<h2>H Persuasion Strategy Annotation on Avalon Games</h2>
<p>Adjacent pie-charts comparing the distributions of annotated utterance-level persuasion strategies for</p>
<p>One Night Ultimate Werewolf games and Avalon games in Ego4D are shown in Fig. 9. We can observe a different distribution of adopted persuasion strategies in the two games, suggesting a large game domain gap.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Identity</th>
<th style="text-align: center;">Accusation</th>
<th style="text-align: center;">Interrogation</th>
<th style="text-align: center;">Call for Action</th>
<th style="text-align: center;">Defense</th>
<th style="text-align: center;">Evidence</th>
<th style="text-align: center;">Avg F1</th>
<th style="text-align: center;">Joint-A</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">82.6 $\pm 1.1$</td>
<td style="text-align: center;">$48.8 \pm 4.8$</td>
<td style="text-align: center;">$82.8 \pm 0.2$</td>
<td style="text-align: center;">$39.4 \pm 9.6$</td>
<td style="text-align: center;">$29.3 \pm 5.5$</td>
<td style="text-align: center;">$54.2 \pm 2.5$</td>
<td style="text-align: center;">$56.2 \pm 2.5$</td>
<td style="text-align: center;">$65.1 \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + context1</td>
<td style="text-align: center;">$80.4 \pm 0.8$</td>
<td style="text-align: center;">$49.0 \pm 3.6$</td>
<td style="text-align: center;">$82.6 \pm 0.4$</td>
<td style="text-align: center;">$46.4 \pm 8.3$</td>
<td style="text-align: center;">$29.1 \pm 2.8$</td>
<td style="text-align: center;">$53.8 \pm 3.6$</td>
<td style="text-align: center;">$56.9 \pm 0.7$</td>
<td style="text-align: center;">$64.0 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + context3</td>
<td style="text-align: center;">$81.7 \pm 1.3$</td>
<td style="text-align: center;">$51.8 \pm 4.7$</td>
<td style="text-align: center;">$81.1 \pm 2.6$</td>
<td style="text-align: center;">$45.7 \pm 4.6$</td>
<td style="text-align: center;">$32.5 \pm 2.4$</td>
<td style="text-align: center;">$52.2 \pm 0.9$</td>
<td style="text-align: center;">$57.5 \pm 1.6$</td>
<td style="text-align: center;">$64.8 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + context5</td>
<td style="text-align: center;">$79.9 \pm 1.6$</td>
<td style="text-align: center;">$52.0 \pm 3.3$</td>
<td style="text-align: center;">$81.0 \pm 1.1$</td>
<td style="text-align: center;">$49.5 \pm 3.2$</td>
<td style="text-align: center;">$33.8 \pm 0.5$</td>
<td style="text-align: center;">$57.1 \pm 1.6$</td>
<td style="text-align: center;">$58.9 \pm 0.6$</td>
<td style="text-align: center;">$65.0 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + context7</td>
<td style="text-align: center;">$80.7 \pm 3.1$</td>
<td style="text-align: center;">$47.4 \pm 5.4$</td>
<td style="text-align: center;">$80.7 \pm 1.7$</td>
<td style="text-align: center;">$38.6 \pm 12.0$</td>
<td style="text-align: center;">$34.7 \pm 2.2$</td>
<td style="text-align: center;">$55.3 \pm 0.7$</td>
<td style="text-align: center;">$56.3 \pm 2.4$</td>
<td style="text-align: center;">$63.5 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + context9</td>
<td style="text-align: center;">$77.9 \pm 0.1$</td>
<td style="text-align: center;">$47.5 \pm 2.7$</td>
<td style="text-align: center;">$78.5 \pm 1.9$</td>
<td style="text-align: center;">$43.0 \pm 5.3$</td>
<td style="text-align: center;">$31.8 \pm 0.9$</td>
<td style="text-align: center;">$54.1 \pm 3.5$</td>
<td style="text-align: center;">$55.5 \pm 0.7$</td>
<td style="text-align: center;">$63.7 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">$81.7 \pm 2.6$</td>
<td style="text-align: center;">$51.7 \pm 0.9$</td>
<td style="text-align: center;">$83.4 \pm 0.9$</td>
<td style="text-align: center;">$43.3 \pm 8.7$</td>
<td style="text-align: center;">$33.1 \pm 2.2$</td>
<td style="text-align: center;">$51.7 \pm 2.1$</td>
<td style="text-align: center;">$57.5 \pm 1.4$</td>
<td style="text-align: center;">$63.4 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + context1</td>
<td style="text-align: center;">$79.9 \pm 2.8$</td>
<td style="text-align: center;">$53.1 \pm 0.6$</td>
<td style="text-align: center;">$82.1 \pm 0.9$</td>
<td style="text-align: center;">$41.8 \pm 7.9$</td>
<td style="text-align: center;">$34.1 \pm 1.4$</td>
<td style="text-align: center;">$55.2 \pm 2.9$</td>
<td style="text-align: center;">$57.7 \pm 1.4$</td>
<td style="text-align: center;">$64.1 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + context3</td>
<td style="text-align: center;">$81.7 \pm 1.0$</td>
<td style="text-align: center;">$53.9 \pm 3.9$</td>
<td style="text-align: center;">$82.3 \pm 1.6$</td>
<td style="text-align: center;">$39.6 \pm 9.0$</td>
<td style="text-align: center;">$35.4 \pm 2.9$</td>
<td style="text-align: center;">$54.0 \pm 3.8$</td>
<td style="text-align: center;">$57.8 \pm 2.1$</td>
<td style="text-align: center;">$64.1 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + context5</td>
<td style="text-align: center;">$81.5 \pm 0.7$</td>
<td style="text-align: center;">$59.4 \pm 2.4$</td>
<td style="text-align: center;">$83.5 \pm 1.1$</td>
<td style="text-align: center;">$43.7 \pm 3.7$</td>
<td style="text-align: center;">$33.0 \pm 3.1$</td>
<td style="text-align: center;">$52.4 \pm 2.9$</td>
<td style="text-align: center;">$58.9 \pm 1.2$</td>
<td style="text-align: center;">$64.6 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + context7</td>
<td style="text-align: center;">$78.6 \pm 1.7$</td>
<td style="text-align: center;">$55.5 \pm 0.5$</td>
<td style="text-align: center;">$80.6 \pm 0.4$</td>
<td style="text-align: center;">$38.2 \pm 4.0$</td>
<td style="text-align: center;">$30.1 \pm 4.9$</td>
<td style="text-align: center;">$51.9 \pm 3.2$</td>
<td style="text-align: center;">$55.8 \pm 1.2$</td>
<td style="text-align: center;">$62.4 \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + context9</td>
<td style="text-align: center;">$80.2 \pm 1.8$</td>
<td style="text-align: center;">$56.0 \pm 2.2$</td>
<td style="text-align: center;">$83.0 \pm 1.4$</td>
<td style="text-align: center;">$42.5 \pm 10.4$</td>
<td style="text-align: center;">$32.0 \pm 2.4$</td>
<td style="text-align: center;">$53.5 \pm 1.7$</td>
<td style="text-align: center;">$57.9 \pm 1.8$</td>
<td style="text-align: center;">$63.0 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT</td>
<td style="text-align: center;">$80.9 \pm 1.3$</td>
<td style="text-align: center;">$51.5 \pm 3.3$</td>
<td style="text-align: center;">$83.0 \pm 1.3$</td>
<td style="text-align: center;">$56.6 \pm 2.3$</td>
<td style="text-align: center;">$25.9 \pm 2.0$</td>
<td style="text-align: center;">$53.6 \pm 1.3$</td>
<td style="text-align: center;">$58.6 \pm 0.3$</td>
<td style="text-align: center;">$65.5 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + context1</td>
<td style="text-align: center;">$79.2 \pm 2.2$</td>
<td style="text-align: center;">$53.3 \pm 2.3$</td>
<td style="text-align: center;">$84.3 \pm 0.6$</td>
<td style="text-align: center;">$52.9 \pm 2.9$</td>
<td style="text-align: center;">$31.1 \pm 5.8$</td>
<td style="text-align: center;">$55.0 \pm 3.2$</td>
<td style="text-align: center;">$59.3 \pm 0.4$</td>
<td style="text-align: center;">$66.0 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + context3</td>
<td style="text-align: center;">$77.4 \pm 2.2$</td>
<td style="text-align: center;">$52.6 \pm 3.8$</td>
<td style="text-align: center;">$83.2 \pm 2.1$</td>
<td style="text-align: center;">$46.2 \pm 2.4$</td>
<td style="text-align: center;">$35.1 \pm 2.3$</td>
<td style="text-align: center;">$56.1 \pm 2.7$</td>
<td style="text-align: center;">$58.4 \pm 0.1$</td>
<td style="text-align: center;">$65.1 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + context5</td>
<td style="text-align: center;">$79.8 \pm 2.2$</td>
<td style="text-align: center;">$54.4 \pm 0.8$</td>
<td style="text-align: center;">$83.2 \pm 0.7$</td>
<td style="text-align: center;">$50.8 \pm 7.2$</td>
<td style="text-align: center;">$36.5 \pm 2.8$</td>
<td style="text-align: center;">$61.5 \pm 2.2$</td>
<td style="text-align: center;">$61.0 \pm 1.1$</td>
<td style="text-align: center;">$66.3 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + context7</td>
<td style="text-align: center;">$78.5 \pm 2.5$</td>
<td style="text-align: center;">$54.7 \pm 3.3$</td>
<td style="text-align: center;">$82.6 \pm 1.2$</td>
<td style="text-align: center;">$47.9 \pm 2.5$</td>
<td style="text-align: center;">$33.5 \pm 2.2$</td>
<td style="text-align: center;">$53.4 \pm 1.4$</td>
<td style="text-align: center;">$58.4 \pm 1.1$</td>
<td style="text-align: center;">$65.0 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + context9</td>
<td style="text-align: center;">$78.2 \pm 2.2$</td>
<td style="text-align: center;">$54.7 \pm 1.6$</td>
<td style="text-align: center;">$82.1 \pm 0.4$</td>
<td style="text-align: center;">$47.8 \pm 3.8$</td>
<td style="text-align: center;">$30.5 \pm 5.8$</td>
<td style="text-align: center;">$56.3 \pm 1.0$</td>
<td style="text-align: center;">$58.3 \pm 0.5$</td>
<td style="text-align: center;">$64.8 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">$80.2 \pm 1.6$</td>
<td style="text-align: center;">$64.7 \pm 1.1$</td>
<td style="text-align: center;">$89.6 \pm 0.4$</td>
<td style="text-align: center;">$77.2 \pm 2.5$</td>
<td style="text-align: center;">$43.5 \pm 1.0$</td>
<td style="text-align: center;">$58.3 \pm 0.7$</td>
<td style="text-align: center;">$68.9 \pm 0.1$</td>
<td style="text-align: center;">$64.6 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + context1</td>
<td style="text-align: center;">$81.2 \pm 1.1$</td>
<td style="text-align: center;">$66.5 \pm 0.5$</td>
<td style="text-align: center;">$90.2 \pm 0.3$</td>
<td style="text-align: center;">$77.7 \pm 0.3$</td>
<td style="text-align: center;">$43.6 \pm 2.7$</td>
<td style="text-align: center;">$59.5 \pm 0.6$</td>
<td style="text-align: center;">$69.8 \pm 0.3$</td>
<td style="text-align: center;">$65.1 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + context3</td>
<td style="text-align: center;">$82.6 \pm 0.7$</td>
<td style="text-align: center;">$65.9 \pm 0.5$</td>
<td style="text-align: center;">$90.1 \pm 0.7$</td>
<td style="text-align: center;">$77.4 \pm 1.2$</td>
<td style="text-align: center;">$43.0 \pm 1.5$</td>
<td style="text-align: center;">$60.4 \pm 0.9$</td>
<td style="text-align: center;">$69.9 \pm 0.4$</td>
<td style="text-align: center;">$64.4 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + context5</td>
<td style="text-align: center;">$82.6 \pm 0.7$</td>
<td style="text-align: center;">$66.7 \pm 1.0$</td>
<td style="text-align: center;">$89.6 \pm 1.5$</td>
<td style="text-align: center;">$78.1 \pm 2.4$</td>
<td style="text-align: center;">$45.7 \pm 1.1$</td>
<td style="text-align: center;">$59.7 \pm 1.1$</td>
<td style="text-align: center;">$70.4 \pm 0.3$</td>
<td style="text-align: center;">$64.4 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + context7</td>
<td style="text-align: center;">$81.8 \pm 0.6$</td>
<td style="text-align: center;">$67.2 \pm 1.2$</td>
<td style="text-align: center;">$90.5 \pm 0.2$</td>
<td style="text-align: center;">$77.7 \pm 0.5$</td>
<td style="text-align: center;">$45.0 \pm 0.7$</td>
<td style="text-align: center;">$60.2 \pm 1.2$</td>
<td style="text-align: center;">$70.4 \pm 0.5$</td>
<td style="text-align: center;">$64.8 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + context9</td>
<td style="text-align: center;">$80.6 \pm 1.1$</td>
<td style="text-align: center;">$66.7 \pm 0.4$</td>
<td style="text-align: center;">$90.3 \pm 0.2$</td>
<td style="text-align: center;">$77.0 \pm 1.2$</td>
<td style="text-align: center;">$42.2 \pm 2.0$</td>
<td style="text-align: center;">$59.6 \pm 0.2$</td>
<td style="text-align: center;">$69.4 \pm 0.4$</td>
<td style="text-align: center;">$64.0 \pm 1.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">$84.3 \pm 0.1$</td>
<td style="text-align: center;">$67.2 \pm 0.6$</td>
<td style="text-align: center;">$89.4 \pm 0.1$</td>
<td style="text-align: center;">$78.2 \pm 0.8$</td>
<td style="text-align: center;">$44.3 \pm 0.4$</td>
<td style="text-align: center;">$59.0 \pm 1.7$</td>
<td style="text-align: center;">$70.4 \pm 0.2$</td>
<td style="text-align: center;">$64.8 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + context1</td>
<td style="text-align: center;">$83.3 \pm 0.2$</td>
<td style="text-align: center;">$67.0 \pm 0.3$</td>
<td style="text-align: center;">$89.9 \pm 0.2$</td>
<td style="text-align: center;">$78.4 \pm 0.9$</td>
<td style="text-align: center;">$43.4 \pm 2.7$</td>
<td style="text-align: center;">$59.7 \pm 0.5$</td>
<td style="text-align: center;">$70.3 \pm 0.5$</td>
<td style="text-align: center;">$65.7 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + context3</td>
<td style="text-align: center;">$82.7 \pm 1.5$</td>
<td style="text-align: center;">$67.8 \pm 0.1$</td>
<td style="text-align: center;">$90.3 \pm 0.4$</td>
<td style="text-align: center;">$77.4 \pm 0.4$</td>
<td style="text-align: center;">$43.1 \pm 1.5$</td>
<td style="text-align: center;">$61.0 \pm 1.9$</td>
<td style="text-align: center;">$70.4 \pm 0.2$</td>
<td style="text-align: center;">$65.5 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + context5</td>
<td style="text-align: center;">$82.4 \pm 0.3$</td>
<td style="text-align: center;">$67.0 \pm 1.1$</td>
<td style="text-align: center;">$90.2 \pm 0.0$</td>
<td style="text-align: center;">$77.1 \pm 1.0$</td>
<td style="text-align: center;">$46.1 \pm 0.7$</td>
<td style="text-align: center;">$59.9 \pm 0.7$</td>
<td style="text-align: center;">$70.5 \pm 0.3$</td>
<td style="text-align: center;">$64.7 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + context7</td>
<td style="text-align: center;">$83.5 \pm 0.8$</td>
<td style="text-align: center;">$66.0 \pm 0.6$</td>
<td style="text-align: center;">$90.2 \pm 0.4$</td>
<td style="text-align: center;">$77.8 \pm 0.3$</td>
<td style="text-align: center;">$46.6 \pm 1.5$</td>
<td style="text-align: center;">$58.4 \pm 1.1$</td>
<td style="text-align: center;">$70.4 \pm 0.3$</td>
<td style="text-align: center;">$65.1 \pm 1.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + context9</td>
<td style="text-align: center;">$82.9 \pm 2.0$</td>
<td style="text-align: center;">$66.6 \pm 0.7$</td>
<td style="text-align: center;">$90.6 \pm 0.2$</td>
<td style="text-align: center;">$75.5 \pm 0.5$</td>
<td style="text-align: center;">$46.8 \pm 0.9$</td>
<td style="text-align: center;">$58.9 \pm 1.1$</td>
<td style="text-align: center;">$70.2 \pm 0.3$</td>
<td style="text-align: center;">$64.9 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT</td>
<td style="text-align: center;">$80.7 \pm 0.4$</td>
<td style="text-align: center;">$65.1 \pm 1.5$</td>
<td style="text-align: center;">$88.5 \pm 0.8$</td>
<td style="text-align: center;">$76.2 \pm 2.2$</td>
<td style="text-align: center;">$42.3 \pm 1.5$</td>
<td style="text-align: center;">$57.4 \pm 1.3$</td>
<td style="text-align: center;">$68.4 \pm 0.3$</td>
<td style="text-align: center;">$65.6 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + context1</td>
<td style="text-align: center;">$82.9 \pm 0.9$</td>
<td style="text-align: center;">$67.2 \pm 1.2$</td>
<td style="text-align: center;">$88.7 \pm 1.5$</td>
<td style="text-align: center;">$77.8 \pm 1.6$</td>
<td style="text-align: center;">$43.4 \pm 0.6$</td>
<td style="text-align: center;">$59.0 \pm 0.8$</td>
<td style="text-align: center;">$69.8 \pm 0.3$</td>
<td style="text-align: center;">$67.3 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + context3</td>
<td style="text-align: center;">$80.5 \pm 2.5$</td>
<td style="text-align: center;">$65.9 \pm 1.5$</td>
<td style="text-align: center;">$89.9 \pm 0.2$</td>
<td style="text-align: center;">$75.2 \pm 1.4$</td>
<td style="text-align: center;">$44.9 \pm 1.9$</td>
<td style="text-align: center;">$58.3 \pm 0.4$</td>
<td style="text-align: center;">$69.1 \pm 0.6$</td>
<td style="text-align: center;">$65.8 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + context5</td>
<td style="text-align: center;">$83.1 \pm 1.1$</td>
<td style="text-align: center;">$65.0 \pm 1.5$</td>
<td style="text-align: center;">$90.1 \pm 0.3$</td>
<td style="text-align: center;">$74.6 \pm 2.4$</td>
<td style="text-align: center;">$46.5 \pm 0.8$</td>
<td style="text-align: center;">$59.2 \pm 0.3$</td>
<td style="text-align: center;">$69.7 \pm 0.6$</td>
<td style="text-align: center;">$66.7 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + context7</td>
<td style="text-align: center;">$82.1 \pm 0.8$</td>
<td style="text-align: center;">$67.3 \pm 0.1$</td>
<td style="text-align: center;">$89.4 \pm 1.1$</td>
<td style="text-align: center;">$76.0 \pm 0.9$</td>
<td style="text-align: center;">$43.2 \pm 1.3$</td>
<td style="text-align: center;">$57.8 \pm 0.5$</td>
<td style="text-align: center;">$69.3 \pm 0.3$</td>
<td style="text-align: center;">$67.6 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + context9</td>
<td style="text-align: center;">$81.0 \pm 2.0$</td>
<td style="text-align: center;">$67.8 \pm 0.2$</td>
<td style="text-align: center;">$89.6 \pm 0.6$</td>
<td style="text-align: center;">$72.8 \pm 3.8$</td>
<td style="text-align: center;">$44.3 \pm 2.1$</td>
<td style="text-align: center;">$58.8 \pm 1.0$</td>
<td style="text-align: center;">$69.1 \pm 0.8$</td>
<td style="text-align: center;">$66.5 \pm 0.5$</td>
</tr>
</tbody>
</table>
<p>Table 5: Experimental Results on incorporating the conversational context of different lengths for persuasion strategy prediction</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Identity</th>
<th style="text-align: center;">Accusation</th>
<th style="text-align: center;">Interrogation</th>
<th style="text-align: center;">Call for Action</th>
<th style="text-align: center;">Defense</th>
<th style="text-align: center;">Evidence</th>
<th style="text-align: center;">Avg F1</th>
<th style="text-align: center;">Joint-A</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">$82.0 \pm 1.2$</td>
<td style="text-align: center;">$53.9 \pm 1.6$</td>
<td style="text-align: center;">$84.1 \pm 0.9$</td>
<td style="text-align: center;">$53.0 \pm 4.1$</td>
<td style="text-align: center;">$33.9 \pm 0.5$</td>
<td style="text-align: center;">$53.5 \pm 3.9$</td>
<td style="text-align: center;">$60.1 \pm 0.7$</td>
<td style="text-align: center;">$65.6 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + C</td>
<td style="text-align: center;">$83.6 \pm 0.8$</td>
<td style="text-align: center;">$55.7 \pm 1.1$</td>
<td style="text-align: center;">$85.6 \pm 1.0$</td>
<td style="text-align: center;">$46.5 \pm 2.7$</td>
<td style="text-align: center;">$34.5 \pm 2.5$</td>
<td style="text-align: center;">$60.9 \pm 3.2$</td>
<td style="text-align: center;">$61.1 \pm 0.9$</td>
<td style="text-align: center;">$65.6 \pm 1.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">$86.9 \pm 0.9$</td>
<td style="text-align: center;">$57.0 \pm 1.4$</td>
<td style="text-align: center;">$85.0 \pm 2.0$</td>
<td style="text-align: center;">$53.5 \pm 3.6$</td>
<td style="text-align: center;">$31.5 \pm 1.3$</td>
<td style="text-align: center;">$55.0 \pm 1.3$</td>
<td style="text-align: center;">$61.5 \pm 0.8$</td>
<td style="text-align: center;">$66.0 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + C</td>
<td style="text-align: center;">$82.5 \pm 2.4$</td>
<td style="text-align: center;">$56.3 \pm 2.7$</td>
<td style="text-align: center;">$86.2 \pm 0.6$</td>
<td style="text-align: center;">$50.7 \pm 4.1$</td>
<td style="text-align: center;">$37.6 \pm 1.8$</td>
<td style="text-align: center;">$59.6 \pm 1.8$</td>
<td style="text-align: center;">$62.2 \pm 1.3$</td>
<td style="text-align: center;">$67.6 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT</td>
<td style="text-align: center;">$80.6 \pm 2.9$</td>
<td style="text-align: center;">$50.4 \pm 3.6$</td>
<td style="text-align: center;">$83.5 \pm 1.7$</td>
<td style="text-align: center;">$45.3 \pm 10.6$</td>
<td style="text-align: center;">$34.7 \pm 0.7$</td>
<td style="text-align: center;">$55.2 \pm 3.2$</td>
<td style="text-align: center;">$58.3 \pm 2.7$</td>
<td style="text-align: center;">$64.8 \pm 2.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + C</td>
<td style="text-align: center;">$81.8 \pm 2.1$</td>
<td style="text-align: center;">$53.8 \pm 2.7$</td>
<td style="text-align: center;">$83.4 \pm 1.9$</td>
<td style="text-align: center;">$44.1 \pm 8.2$</td>
<td style="text-align: center;">$35.9 \pm 1.7$</td>
<td style="text-align: center;">$53.5 \pm 2.1$</td>
<td style="text-align: center;">$58.7 \pm 2.3$</td>
<td style="text-align: center;">$66.3 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">$82.0 \pm 1.4$</td>
<td style="text-align: center;">$54.9 \pm 0.9$</td>
<td style="text-align: center;">$82.8 \pm 0.6$</td>
<td style="text-align: center;">$53.0 \pm 1.9$</td>
<td style="text-align: center;">$29.9 \pm 1.0$</td>
<td style="text-align: center;">$61.7 \pm 0.7$</td>
<td style="text-align: center;">$60.7 \pm 0.2$</td>
<td style="text-align: center;">$68.1 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT + C</td>
<td style="text-align: center;">$84.1 \pm 0.1$</td>
<td style="text-align: center;">$55.6 \pm 3.5$</td>
<td style="text-align: center;">$86.1 \pm 0.4$</td>
<td style="text-align: center;">$49.9 \pm 2.2$</td>
<td style="text-align: center;">$32.6 \pm 1.5$</td>
<td style="text-align: center;">$61.5 \pm 2.4$</td>
<td style="text-align: center;">$61.6 \pm 1.1$</td>
<td style="text-align: center;">$69.0 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">$86.7 \pm 1.3$</td>
<td style="text-align: center;">$56.6 \pm 1.4$</td>
<td style="text-align: center;">$85.3 \pm 1.6$</td>
<td style="text-align: center;">$54.8 \pm 3.9$</td>
<td style="text-align: center;">$29.4 \pm 2.5$</td>
<td style="text-align: center;">$57.3 \pm 2.0$</td>
<td style="text-align: center;">$61.7 \pm 1.4$</td>
<td style="text-align: center;">$67.4 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa + C</td>
<td style="text-align: center;">$84.0 \pm 1.5$</td>
<td style="text-align: center;">$58.9 \pm 1.6$</td>
<td style="text-align: center;">$84.9 \pm 0.2$</td>
<td style="text-align: center;">$52.4 \pm 4.4$</td>
<td style="text-align: center;">$38.0 \pm 2.3$</td>
<td style="text-align: center;">$62.5 \pm 2.4$</td>
<td style="text-align: center;">$63.4 \pm 1.7$</td>
<td style="text-align: center;">$69.1 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT</td>
<td style="text-align: center;">$81.9 \pm 1.4$</td>
<td style="text-align: center;">$54.7 \pm 1.6$</td>
<td style="text-align: center;">$83.0 \pm 0.6$</td>
<td style="text-align: center;">$60.2 \pm 5.3$</td>
<td style="text-align: center;">$25.6 \pm 1.6$</td>
<td style="text-align: center;">$59.2 \pm 2.3$</td>
<td style="text-align: center;">$60.8 \pm 1.0$</td>
<td style="text-align: center;">$68.5 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-BERT + C</td>
<td style="text-align: center;">$83.7 \pm 0.9$</td>
<td style="text-align: center;">$54.3 \pm 2.4$</td>
<td style="text-align: center;">$84.5 \pm 1.1$</td>
<td style="text-align: center;">$53.8 \pm 3.2$</td>
<td style="text-align: center;">$33.8 \pm 3.4$</td>
<td style="text-align: center;">$58.1 \pm 2.8$</td>
<td style="text-align: center;">$61.4 \pm 1.0$</td>
<td style="text-align: center;">$70.0 \pm 1.5$</td>
</tr>
</tbody>
</table>
<p>Table 6: Data domain generalization experiments. We train models on Youtube data and test on Ego4D testing set. Then we fine-tune the models on Ego4D training set and test again.</p>
<h1>Label the Persuation Strategy for the Dialogues during Social Deduction Game</h1>
<p>Read the instruction given below carefully:
Your goal is to read the given dialog during the social deduction game. And then label the strategy someone used to persuade others by clicking the corresponding check box. A sentence is labeled as No Strategy if it is not a strategy. We have 9 labels (including No Strategy).</p>
<h2>Strategy Definition and Examples:</h2>
<h2>1. Specific Definition</h2>
<p>State one's own role or identity in the game
Example:
"I was the seer, and I know you were the robber."
"I'll just come out and say I was a villager, so I have no idea what's going on. "
"I was the robber, but I have no ideaed after the swap. "</p>
<h2>2. Description</h2>
<p>Claim someone has a specific identity or strategic behavior Example:
"I think you are lying."
"So James might be the werewolf."
"I think Sukeshi is the werewolf she is just too quiet."</p>
<h2>3. Description</h2>
<p>Questions about someone's identity or behavior.
Example:
"Who did you rob?"
"Was Elliot actually a villager?"
"So, who does everyone hate the most?"</p>
<h2>4. Action</h2>
<p>Encourage people to take an action during the game.
Example:
"Let's kill Sukeshi."
"We shouldn't vote to not kill anyone. And then there could also be no werewolf."
"But you never... You gotta listen to me."
Please carefully read the input text first. Then, click the appropriate strategy of button for each sentence.
Once you have finished the annotation, click Yes to confirm you have labeled every statement. Then you can click the button at the bottom to end the task. If the annotation file could not be downlowed, this means at least one of the entry has not been annotated. You can click on the last Save Progress button to find the entries that have not been annotated.</p>
<p>Step 1: Please enter your first name (required) and last name (optional) in the text boxes below:
First Name:
Aryan
Step 2: Upload the appropriately formatted text file (.txt) of your desired transcript here:</p>
<p>Choose File player2_Gamell_1.txt</p>
<p>Step 3: (Optional) If you saved your annotation progress specific to the uploaded transcript file above in your last session and wish to continue from where you left off, please upload the annotations csv (.csv) file that you had downloaded and saved in your last session here:</p>
<p>Choose File No file chosen</p>
<p>Step 4: Choose appropriate persuasion strategy(ies) or "No Strategy" for the utterances in the dialogs below.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The screenshot of persuasion strategy annotation interface</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Identity</th>
<th style="text-align: center;">Accusation</th>
<th style="text-align: center;">Interrogation</th>
<th style="text-align: center;">Call for Action</th>
<th style="text-align: center;">Defense</th>
<th style="text-align: center;">Evidence</th>
<th style="text-align: center;">Avg F1</th>
<th style="text-align: center;">Joint-A</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">$66.1 \pm 3.7$</td>
<td style="text-align: center;">$37.2 \pm 4.9$</td>
<td style="text-align: center;">$73.3 \pm 3.7$</td>
<td style="text-align: center;">$20.9 \pm 7.3$</td>
<td style="text-align: center;">$23.5 \pm 6.3$</td>
<td style="text-align: center;">$26.6 \pm 5.3$</td>
<td style="text-align: center;">$41.3 \pm 3.3$</td>
<td style="text-align: center;">$62.9 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;">BERT + C</td>
<td style="text-align: center;">$60.4 \pm 4.5$</td>
<td style="text-align: center;">$41.7 \pm 2.2$</td>
<td style="text-align: center;">$73.2 \pm 0.3$</td>
<td style="text-align: center;">$34.4 \pm 4.8$</td>
<td style="text-align: center;">$25.0 \pm 4.3$</td>
<td style="text-align: center;">$18.2 \pm 2.9$</td>
<td style="text-align: center;">$42.1 \pm 1.4$</td>
<td style="text-align: center;">$63.9 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">$63.0 \pm 3.2$</td>
<td style="text-align: center;">$45.9 \pm 3.0$</td>
<td style="text-align: center;">$73.1 \pm 2.8$</td>
<td style="text-align: center;">$33.0 \pm 12.1$</td>
<td style="text-align: center;">$33.6 \pm 5.7$</td>
<td style="text-align: center;">$27.5 \pm 1.2$</td>
<td style="text-align: center;">$46.0 \pm 1.8$</td>
<td style="text-align: center;">$64.3 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa + C</td>
<td style="text-align: center;">$46.1 \pm 9.3$</td>
<td style="text-align: center;">$40.3 \pm 2.3$</td>
<td style="text-align: center;">$71.8 \pm 2.9$</td>
<td style="text-align: center;">$35.8 \pm 2.8$</td>
<td style="text-align: center;">$28.1 \pm 3.0$</td>
<td style="text-align: center;">$22.5 \pm 8.5$</td>
<td style="text-align: center;">$40.8 \pm 1.7$</td>
<td style="text-align: center;">$63.5 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;">MT-BERT</td>
<td style="text-align: center;">$56.3 \pm 3.6$</td>
<td style="text-align: center;">$38.2 \pm 4.6$</td>
<td style="text-align: center;">$73.1 \pm 1.9$</td>
<td style="text-align: center;">$39.3 \pm 5.7$</td>
<td style="text-align: center;">$23.5 \pm 8.0$</td>
<td style="text-align: center;">$25.3 \pm 1.0$</td>
<td style="text-align: center;">$42.6 \pm 1.7$</td>
<td style="text-align: center;">$63.4 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">MT-BERT + C</td>
<td style="text-align: center;">$61.9 \pm 2.9$</td>
<td style="text-align: center;">$42.2 \pm 2.5$</td>
<td style="text-align: center;">$71.2 \pm 2.7$</td>
<td style="text-align: center;">$34.2 \pm 1.0$</td>
<td style="text-align: center;">$29.3 \pm 5.2$</td>
<td style="text-align: center;">$23.2 \pm 3.7$</td>
<td style="text-align: center;">$43.7 \pm 1.2$</td>
<td style="text-align: center;">$64.2 \pm 0.3$</td>
</tr>
</tbody>
</table>
<p>Table 7: Game domain generalization experiments. We train models on Ego4D Werewolf training set and test them on Avalon data.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Persuasion strategy distributions for Ego4D Werewolf games and Ego4D Avalon games: Pie charts representing the percentage of annotated utterances for every persuasion strategy and "No Strategy" in One Night Ultimate Werewolf games and The Resistance: Avalon games in Ego4D.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">zero-shot</th>
<th style="text-align: center;">Label the Persuasion Strategy for the Utterances in Dialogues during Social Deduction Game. Do not hesitate to select multiple strategies if one category can not summarize the given utterance. <br> Strategy Definition: <br> 1. Identity Declaration: State one's own role or identity in the game <br> 2. Accusation: Claim someone has a specific identity or strategic behavior <br> 3. Interrogation: Questions about someone's identity or behavior <br> 4. Call for Action: Encourage people to take an action during the game <br> 5. Defense: Defending yourself or someone else against an accusation or defending a game-related argument <br> 6. Evidence: Provide a body of game-related facts or information <br> 7. No Strategy: Any sentences that do not fall into other categories are here. Clarification or discussion of game rules should also be considered "No-Strategy" <br> Utterance: "\$utterance\$" <br> Strategy:</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">one-shot</td>
<td style="text-align: center;">Label the Persuasion Strategy for the Utterances in Dialogues during Social Deduction Game. Do not hesitate to select multiple strategies if one category can not summarize the given utterance. <br> Strategy Definition: <br> {same as above} <br> Utterance: "No, but in order to find it, I had to really tap around to find it." <br> Strategy: Defense, Evidence <br> Utterance: "\$utterance\$" <br> Strategy:</td>
</tr>
<tr>
<td style="text-align: center;">five-shot</td>
<td style="text-align: center;">Label the Persuasion Strategy for the Utterances in Dialogues during Social Deduction Game. Do not hesitate to select multiple strategies if one category can not summarize the given utterance. <br> Strategy Definition: <br> {same as above} <br> Utterance: "I'll just come out and say I was a villager, so I have no idea what's going on." <br> Strategy: Identity Declaration <br> Utterance: "So James might be the werewolf." <br> Strategy: Accusation <br> Utterance: "Did anybody do any swapping? Anybody willing to fess up to anything about swapping?" <br> Strategy: Interrogation, Call for Action <br> Utterance: "No, but in order to find it, I had to really tap around to find it." <br> Strategy: Defense, Evidence <br> Utterance: "Okay. Good point." <br> Strategy: No Strategy <br> Utterance: "\$utterance\$" <br> Strategy:</td>
</tr>
</tbody>
</table>
<p>Table 8: Prompt templates used for GPT-3, the variable within dollars is to be replaced with the corresponding value.</p>
<h1>A A1. Did you describe the limitations of your work?</h1>
<h2>Limitations</h2>
<p>A2. Did you discuss any potential risks of your work?
Ethics Statement
A3. Do the abstract and introduction summarize the paper's main claims?
1
A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B $\square$ Did you use or create scientific artifacts?</h2>
<p>3; 4; Appendix C
B1. Did you cite the creators of artifacts you used?
3.1; 4.2</p>
<p>B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
3.1; 4.2</p>
<p>B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
3.1; 4.2</p>
<p>B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
3
B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
3
B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
$3 ; 5$</p>
<h2>C $\quad$ Did you run computational experiments?</h2>
<p>$4 ; 5$
C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?
4.2; 4.3</p>
<p>The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
4.4; 5.2; Appendix D E F</p>
<p>C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
4.4</p>
<p>C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
$4.2 ; 4.3$
D Did you use human annotators (e.g., crowdworkers) or research with human participants? 3.2</p>
<p>D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
Appendix B C
D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
3.2</p>
<p>D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
3
D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? 3.1</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
3.2</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>denotes equal contribution.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ We consider two games in our dataset: One Night Ultimate Werewolf and The Resistance: Avalon. Appendix A describes the rules of these two games.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
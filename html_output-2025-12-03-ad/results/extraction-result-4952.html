<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4952 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4952</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4952</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-258588316</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.06349v2.pdf" target="_blank">RECKONING: Reasoning through Dynamic Knowledge Encoding</a></p>
                <p><strong>Paper Abstract:</strong> Recent studies on transformer-based language models show that they can answer questions by reasoning over knowledge provided as part of the context (i.e., in-context reasoning). However, since the available knowledge is often not filtered for a particular question, in-context reasoning can be sensitive to distractor facts, additional content that is irrelevant to a question but that may be relevant for a different question (i.e., not necessarily random noise). In these situations, the model fails to distinguish the knowledge that is necessary to answer the question, leading to spurious reasoning and degraded performance. This reasoning failure contrasts with the model's apparent ability to distinguish its contextual knowledge from all the knowledge it has memorized during pre-training. Following this observation, we propose teaching the model to reason more robustly by folding the provided contextual knowledge into the model's parameters before presenting it with a question. Our method, RECKONING, is a bi-level learning algorithm that teaches language models to reason by updating their parametric knowledge through back-propagation, allowing them to then answer questions using the updated parameters. During training, the inner loop rapidly adapts a copy of the model weights to encode contextual knowledge into its parameters. In the outer loop, the model learns to use the updated weights to reproduce and answer reasoning questions about the memorized knowledge. Our experiments on two multi-hop reasoning datasets show that RECKONING's performance improves over the in-context reasoning baseline (by up to 4.5%). We also find that compared to in-context reasoning, RECKONING generalizes better to longer reasoning chains unseen during training, is more robust to distractors in the context, and is more computationally efficient when multiple questions are asked about the same knowledge.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4952.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4952.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RECKONING</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RECKONING: Reasoning through Dynamic Knowledge Encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bi-level learning algorithm that teaches transformer language models to reason by encoding contextual knowledge into model parameters via a small number of gradient updates (inner loop) and optimizing meta-parameters (outer loop) so the updated model can answer downstream logical questions without re-reading the context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (base) with RECKONING</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The authors instantiate RECKONING on GPT-2-base (causal transformer) by performing N inner-loop gradient steps on a CLM loss over contextual facts to produce updated weights, then applying the updated model to answer the question; outer loop optimizes meta-parameters and a per-step-per-layer adaptive inner-loop learning rate. Multi-task outer objective (QA + knowledge recovery) is used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ProofWriter; CLUTRR-Systematic-Generalization (CLUTRR-SG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ProofWriter: synthetic multi-hop deductive reasoning over natural-language facts and rules (hypothesis classification and proof generation) with proof depths (hops) up to 5; CLUTRR-SG: inductive / family-relationship multi-hop reasoning (k-hop relations) evaluating systematic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Bi-level optimization: inner loop performs a small number of gradient updates (minimizing CLM loss) to encode facts into parameters; outer loop trains meta-parameters θ so that the updated model answers questions correctly. Multi-task outer loss adds a CLM-based knowledge-recovery objective. Per-step-per-layer adaptive inner-loop learning rates are learned.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>RECKONING with multi-task outer objective (RECKONING MT) on GPT-2-base: ProofWriter label accuracy by hop: 2-hop 99.5%, 3-hop 99.7%, 5-hop 99.8%. CLUTRR-SG label accuracy by hop: 2-hop 98.3%, 4-hop 97.6%, 6-hop 94.8%. With distractors (ProofWriter) RECKONING MT average label accuracy across hops = 82.5% (when all distractors included) and exact-match fact-reproduction: without distractors avg 99.3%, with distractors avg 73.6%. Run-time: single-question inference slower than FT-ICR (e.g., FT-ICR 0.1887s vs RECKONING 1-step 0.2532s and 4-step 0.9664s), but multi-question setting faster (FT-ICR 18 questions 2.0436s vs RECKONING 1-step 0.6228s and 4-step 1.4839s).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Slower per-question inference due to inner-loop gradient updates (single-question slower than forward-pass ICR); experiments are on synthetic multi-hop datasets (ProofWriter and CLUTRR-SG) using GPT-2-base so scaling/generalization to large real-world LLMs and natural corpora is not demonstrated; performance still degrades with many distractors (though less than ICR); memory and compute constraints (training batch=2) noted; caching of hidden states for ICR not handled (may affect run-time comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to a fine-tuned in-context reasoning (FT-ICR) baseline on the same GPT-2-base backbone, RECKONING MT yields consistent gains (average ~1% across ProofWriter and CLUTRR-SG in the base no-distractor setting) and substantially better robustness to distractors (ProofWriter with all distractors: RECKONING MT avg label accuracy 82.5% vs FT-ICR MT 70.9%). RECKONING generalizes better to longer/extrapolated reasoning chains and is more efficient than ICR when answering many questions about the same knowledge because knowledge is encoded once in parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Inner-loop steps: increasing inner-loop gradient steps improves multi-hop performance (e.g., performance gap between 6 vs 1 inner steps is large for 4-hop (≈+42.3%) and 6-hop (≈+34.7%) problems, but small for 2-hop (≈+5.9%)). Multi-task outer objective (QA + knowledge recovery) is critical: RECKONING MT outperforms RECKONING ST (single-task) by average 2.8% across datasets and hops, and ST fails to learn effective inner-loop memorization. Adaptive per-step-per-layer inner-loop learning rates are essential: replacing dynamic rates with a fixed shared rate causes large drop in performance (average drop ≈34.2%; for 4-hop ≈45.5% and 6-hop ≈39.5%). Exact-match reproduction ablation: RECKONING MT achieves avg exact match 99.3% without distractors vs 73.6% with distractors, showing memorization trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RECKONING: Reasoning through Dynamic Knowledge Encoding', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4952.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4952.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FT-ICR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-Tuned In-Context Reasoning (FT-ICR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach where a transformer (GPT-2-base) is fine-tuned to perform reasoning by concatenating the contextual facts and the question into a single input and predicting the answer via a forward pass (standard in-context reasoning fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (base) with FT-ICR</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2-base model fine-tuned in a supervised manner to predict answers from concatenated context [K; x] (standard in-context reasoning). Evaluated with both single-task (QA only) and multi-task (QA + knowledge recovery) outer objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ProofWriter; CLUTRR-SG</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-hop deductive/inductive reasoning tasks used for RECKONING: ProofWriter (hypothesis classification with proof depths) and CLUTRR-SG (family-graph k-hop relation induction).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning where knowledge K and question x are concatenated into model input and model is trained to predict the answer (in some settings also trained to reproduce the relevant facts — multi-task objective). No parameter editing at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FT-ICR single-task (ST) and multi-task (MT) on GPT-2-base: ProofWriter FT-ICR ST: 2-hop 98.4%, 3-hop 98.8%, 5-hop 97.8%; FT-ICR MT: 2-hop 99.4%, 3-hop 99.2%, 5-hop 99.6%. CLUTRR-SG FT-ICR ST: 2-hop 97.4%, 4-hop 91.3%, 6-hop 89.1%; FT-ICR MT: 2-hop 98.1%, 4-hop 96.9%, 6-hop 90.3%. With distractors (ProofWriter) FT-ICR MT label accuracies drop dramatically (e.g., 2-hop 42.3%, 3-hop 50.3%, 5-hop 55.6%), showing high sensitivity to irrelevant context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High sensitivity to distractor facts in the provided context — adding irrelevant facts causes large performance drops (e.g., ProofWriter accuracy drops from ~99% to ~71% or lower depending on hops). Generalization to longer unseen reasoning chains is weaker than RECKONING in some settings. Requires reprocessing the full context for each question (inefficient when multiple questions share the same knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>FT-ICR achieves strong performance when contexts are clean (comparable to RECKONING in no-distractor settings) and benefits from multi-task training, but under distractors RECKONING outperforms FT-ICR by a large margin (e.g., ProofWriter with all distractors: RECKONING MT avg 82.5% vs FT-ICR MT 70.9%). RECKONING also generalizes better to longer hops and becomes more efficient for multi-question scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Multi-task outer objective improves FT-ICR performance modestly (average +1.8% reported) relative to FT-ICR ST, but even with MT FT-ICR remains much more vulnerable to distractors than RECKONING. The paper also shows that FT-ICR achieves near-perfect knowledge reproduction by direct copying when no distractors are present, but drops to avg exact match ≈49.4% with distractors whereas RECKONING drops less (to ≈73.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RECKONING: Reasoning through Dynamic Knowledge Encoding', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4952.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4952.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The transformer-based causal language model used as the backbone model for both RECKONING and FT-ICR experiments in this paper; ~124M parameter GPT-2 implementation from HuggingFace.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Causal transformer language model (decoder-only) pretrained with causal language modeling objective; used here as the parameterized function f_θ that is adapted in the inner loop (via CLM loss on facts) and fine-tuned in outer-loop training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ProofWriter; CLUTRR-SG</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-hop logical reasoning tasks (deductive and inductive) described above; requires chaining facts/rules to classify hypotheses or infer relations across multiple hops.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Used both as the base model for FT-ICR (concatenate facts+question and forward pass) and as the base for RECKONING (inner-loop parameter edits via gradient descent on CLM loss to encode facts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>As reported under RECKONING and FT-ICR entries: baseline GPT-2-base when fine-tuned (FT-ICR MT) attains near-99% on ProofWriter and high performance on CLUTRR-SG, and when used with RECKONING yields the RECKONING MT performance numbers (see RECKONING entry).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As backbone model, GPT-2-base inherits capacity limitations for large/complex tasks; experiments limited to 124M parameter scale so extrapolation to much larger LLMs is not directly evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Used as identical backbone for fair comparison between methods (RECKONING vs FT-ICR). Results show method-level differences rather than backbone differences since backbone is the same.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>See RECKONING ablations (inner-loop steps, multi-task objective, adaptive learning rate) — these analyses were performed with GPT-2-base as the substrate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RECKONING: Reasoning through Dynamic Knowledge Encoding', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4952.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4952.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-tuned LLM (GPT-3.5 family) evaluated in zero-shot and few-shot (8-shot) prompting for the same multi-hop reasoning datasets as a comparison point to RECKONING on GPT-2-base.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language model from OpenAI with instruction tuning and RLHF variants; exact parameter count not specified in paper. Evaluated via prompting (zero-shot and 8-shot) on ProofWriter and CLUTRR-SG without any RECKONING-style parameter edits.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ProofWriter; CLUTRR-SG</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same synthetic multi-hop reasoning benchmarks used to evaluate RECKONING and FT-ICR.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot prompting (8-shot in-context learning) to elicit reasoning capability without fine-tuning; no parameter updates are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that zero-shot GPT-3.5 significantly underperforms RECKONING on the evaluated benchmarks. GPT-3.5 improves on ProofWriter without distractors but still lags behind RECKONING; with distractors RECKONING substantially outperforms both zero-shot and few-shot GPT-3.5. Exact numeric GPT-3.5 results are not fully enumerated in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-3.5 zero/few-shot prompting struggles comparatively on these synthetic, structured multi-hop reasoning benchmarks and is sensitive to distractors; it does not benefit from the RECKONING inner-loop parameter encoding mechanism as presented.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>RECKONING (on GPT-2-base) outperforms zero-shot and 8-shot GPT-3.5 on ProofWriter and CLUTRR-SG in the reported evaluations, especially in the presence of distractors; GPT-3.5 can improve with few-shot prompting but remains behind RECKONING in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No ablation performed on GPT-3.5 in this paper beyond comparing zero-shot vs 8-shot prompting; main analyses focus on RECKONING components (inner steps, multi-task objective, adaptive LR) on the GPT-2 substrate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RECKONING: Reasoning through Dynamic Knowledge Encoding', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>CLUTRR: A diagnostic benchmark for inductive reasoning from text <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>How to train your MAML <em>(Rating: 1)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 1)</em></li>
                <li>Locating and editing factual associations in GPT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4952",
    "paper_id": "paper-258588316",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "RECKONING",
            "name_full": "RECKONING: Reasoning through Dynamic Knowledge Encoding",
            "brief_description": "A bi-level learning algorithm that teaches transformer language models to reason by encoding contextual knowledge into model parameters via a small number of gradient updates (inner loop) and optimizing meta-parameters (outer loop) so the updated model can answer downstream logical questions without re-reading the context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (base) with RECKONING",
            "model_description": "The authors instantiate RECKONING on GPT-2-base (causal transformer) by performing N inner-loop gradient steps on a CLM loss over contextual facts to produce updated weights, then applying the updated model to answer the question; outer loop optimizes meta-parameters and a per-step-per-layer adaptive inner-loop learning rate. Multi-task outer objective (QA + knowledge recovery) is used.",
            "model_size": "124M",
            "logical_reasoning_task": "ProofWriter; CLUTRR-Systematic-Generalization (CLUTRR-SG)",
            "task_description": "ProofWriter: synthetic multi-hop deductive reasoning over natural-language facts and rules (hypothesis classification and proof generation) with proof depths (hops) up to 5; CLUTRR-SG: inductive / family-relationship multi-hop reasoning (k-hop relations) evaluating systematic generalization.",
            "method_or_approach": "Bi-level optimization: inner loop performs a small number of gradient updates (minimizing CLM loss) to encode facts into parameters; outer loop trains meta-parameters θ so that the updated model answers questions correctly. Multi-task outer loss adds a CLM-based knowledge-recovery objective. Per-step-per-layer adaptive inner-loop learning rates are learned.",
            "performance": "RECKONING with multi-task outer objective (RECKONING MT) on GPT-2-base: ProofWriter label accuracy by hop: 2-hop 99.5%, 3-hop 99.7%, 5-hop 99.8%. CLUTRR-SG label accuracy by hop: 2-hop 98.3%, 4-hop 97.6%, 6-hop 94.8%. With distractors (ProofWriter) RECKONING MT average label accuracy across hops = 82.5% (when all distractors included) and exact-match fact-reproduction: without distractors avg 99.3%, with distractors avg 73.6%. Run-time: single-question inference slower than FT-ICR (e.g., FT-ICR 0.1887s vs RECKONING 1-step 0.2532s and 4-step 0.9664s), but multi-question setting faster (FT-ICR 18 questions 2.0436s vs RECKONING 1-step 0.6228s and 4-step 1.4839s).",
            "limitations_or_failure_cases": "Slower per-question inference due to inner-loop gradient updates (single-question slower than forward-pass ICR); experiments are on synthetic multi-hop datasets (ProofWriter and CLUTRR-SG) using GPT-2-base so scaling/generalization to large real-world LLMs and natural corpora is not demonstrated; performance still degrades with many distractors (though less than ICR); memory and compute constraints (training batch=2) noted; caching of hidden states for ICR not handled (may affect run-time comparisons).",
            "comparison": "Compared to a fine-tuned in-context reasoning (FT-ICR) baseline on the same GPT-2-base backbone, RECKONING MT yields consistent gains (average ~1% across ProofWriter and CLUTRR-SG in the base no-distractor setting) and substantially better robustness to distractors (ProofWriter with all distractors: RECKONING MT avg label accuracy 82.5% vs FT-ICR MT 70.9%). RECKONING generalizes better to longer/extrapolated reasoning chains and is more efficient than ICR when answering many questions about the same knowledge because knowledge is encoded once in parameters.",
            "ablation_or_analysis_results": "Inner-loop steps: increasing inner-loop gradient steps improves multi-hop performance (e.g., performance gap between 6 vs 1 inner steps is large for 4-hop (≈+42.3%) and 6-hop (≈+34.7%) problems, but small for 2-hop (≈+5.9%)). Multi-task outer objective (QA + knowledge recovery) is critical: RECKONING MT outperforms RECKONING ST (single-task) by average 2.8% across datasets and hops, and ST fails to learn effective inner-loop memorization. Adaptive per-step-per-layer inner-loop learning rates are essential: replacing dynamic rates with a fixed shared rate causes large drop in performance (average drop ≈34.2%; for 4-hop ≈45.5% and 6-hop ≈39.5%). Exact-match reproduction ablation: RECKONING MT achieves avg exact match 99.3% without distractors vs 73.6% with distractors, showing memorization trade-offs.",
            "uuid": "e4952.0",
            "source_info": {
                "paper_title": "RECKONING: Reasoning through Dynamic Knowledge Encoding",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "FT-ICR",
            "name_full": "Fine-Tuned In-Context Reasoning (FT-ICR)",
            "brief_description": "A baseline approach where a transformer (GPT-2-base) is fine-tuned to perform reasoning by concatenating the contextual facts and the question into a single input and predicting the answer via a forward pass (standard in-context reasoning fine-tuning).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (base) with FT-ICR",
            "model_description": "GPT-2-base model fine-tuned in a supervised manner to predict answers from concatenated context [K; x] (standard in-context reasoning). Evaluated with both single-task (QA only) and multi-task (QA + knowledge recovery) outer objectives.",
            "model_size": "124M",
            "logical_reasoning_task": "ProofWriter; CLUTRR-SG",
            "task_description": "Same multi-hop deductive/inductive reasoning tasks used for RECKONING: ProofWriter (hypothesis classification with proof depths) and CLUTRR-SG (family-graph k-hop relation induction).",
            "method_or_approach": "Supervised fine-tuning where knowledge K and question x are concatenated into model input and model is trained to predict the answer (in some settings also trained to reproduce the relevant facts — multi-task objective). No parameter editing at inference time.",
            "performance": "FT-ICR single-task (ST) and multi-task (MT) on GPT-2-base: ProofWriter FT-ICR ST: 2-hop 98.4%, 3-hop 98.8%, 5-hop 97.8%; FT-ICR MT: 2-hop 99.4%, 3-hop 99.2%, 5-hop 99.6%. CLUTRR-SG FT-ICR ST: 2-hop 97.4%, 4-hop 91.3%, 6-hop 89.1%; FT-ICR MT: 2-hop 98.1%, 4-hop 96.9%, 6-hop 90.3%. With distractors (ProofWriter) FT-ICR MT label accuracies drop dramatically (e.g., 2-hop 42.3%, 3-hop 50.3%, 5-hop 55.6%), showing high sensitivity to irrelevant context.",
            "limitations_or_failure_cases": "High sensitivity to distractor facts in the provided context — adding irrelevant facts causes large performance drops (e.g., ProofWriter accuracy drops from ~99% to ~71% or lower depending on hops). Generalization to longer unseen reasoning chains is weaker than RECKONING in some settings. Requires reprocessing the full context for each question (inefficient when multiple questions share the same knowledge).",
            "comparison": "FT-ICR achieves strong performance when contexts are clean (comparable to RECKONING in no-distractor settings) and benefits from multi-task training, but under distractors RECKONING outperforms FT-ICR by a large margin (e.g., ProofWriter with all distractors: RECKONING MT avg 82.5% vs FT-ICR MT 70.9%). RECKONING also generalizes better to longer hops and becomes more efficient for multi-question scenarios.",
            "ablation_or_analysis_results": "Multi-task outer objective improves FT-ICR performance modestly (average +1.8% reported) relative to FT-ICR ST, but even with MT FT-ICR remains much more vulnerable to distractors than RECKONING. The paper also shows that FT-ICR achieves near-perfect knowledge reproduction by direct copying when no distractors are present, but drops to avg exact match ≈49.4% with distractors whereas RECKONING drops less (to ≈73.6%).",
            "uuid": "e4952.1",
            "source_info": {
                "paper_title": "RECKONING: Reasoning through Dynamic Knowledge Encoding",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-2-base",
            "name_full": "GPT-2 (base)",
            "brief_description": "The transformer-based causal language model used as the backbone model for both RECKONING and FT-ICR experiments in this paper; ~124M parameter GPT-2 implementation from HuggingFace.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (base)",
            "model_description": "Causal transformer language model (decoder-only) pretrained with causal language modeling objective; used here as the parameterized function f_θ that is adapted in the inner loop (via CLM loss on facts) and fine-tuned in outer-loop training.",
            "model_size": "124M",
            "logical_reasoning_task": "ProofWriter; CLUTRR-SG",
            "task_description": "Same multi-hop logical reasoning tasks (deductive and inductive) described above; requires chaining facts/rules to classify hypotheses or infer relations across multiple hops.",
            "method_or_approach": "Used both as the base model for FT-ICR (concatenate facts+question and forward pass) and as the base for RECKONING (inner-loop parameter edits via gradient descent on CLM loss to encode facts).",
            "performance": "As reported under RECKONING and FT-ICR entries: baseline GPT-2-base when fine-tuned (FT-ICR MT) attains near-99% on ProofWriter and high performance on CLUTRR-SG, and when used with RECKONING yields the RECKONING MT performance numbers (see RECKONING entry).",
            "limitations_or_failure_cases": "As backbone model, GPT-2-base inherits capacity limitations for large/complex tasks; experiments limited to 124M parameter scale so extrapolation to much larger LLMs is not directly evaluated in this work.",
            "comparison": "Used as identical backbone for fair comparison between methods (RECKONING vs FT-ICR). Results show method-level differences rather than backbone differences since backbone is the same.",
            "ablation_or_analysis_results": "See RECKONING ablations (inner-loop steps, multi-task objective, adaptive learning rate) — these analyses were performed with GPT-2-base as the substrate.",
            "uuid": "e4952.2",
            "source_info": {
                "paper_title": "RECKONING: Reasoning through Dynamic Knowledge Encoding",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5 (evaluated)",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "A large instruction-tuned LLM (GPT-3.5 family) evaluated in zero-shot and few-shot (8-shot) prompting for the same multi-hop reasoning datasets as a comparison point to RECKONING on GPT-2-base.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Large language model from OpenAI with instruction tuning and RLHF variants; exact parameter count not specified in paper. Evaluated via prompting (zero-shot and 8-shot) on ProofWriter and CLUTRR-SG without any RECKONING-style parameter edits.",
            "model_size": null,
            "logical_reasoning_task": "ProofWriter; CLUTRR-SG",
            "task_description": "Same synthetic multi-hop reasoning benchmarks used to evaluate RECKONING and FT-ICR.",
            "method_or_approach": "Zero-shot and few-shot prompting (8-shot in-context learning) to elicit reasoning capability without fine-tuning; no parameter updates are applied.",
            "performance": "Paper reports that zero-shot GPT-3.5 significantly underperforms RECKONING on the evaluated benchmarks. GPT-3.5 improves on ProofWriter without distractors but still lags behind RECKONING; with distractors RECKONING substantially outperforms both zero-shot and few-shot GPT-3.5. Exact numeric GPT-3.5 results are not fully enumerated in the provided text.",
            "limitations_or_failure_cases": "GPT-3.5 zero/few-shot prompting struggles comparatively on these synthetic, structured multi-hop reasoning benchmarks and is sensitive to distractors; it does not benefit from the RECKONING inner-loop parameter encoding mechanism as presented.",
            "comparison": "RECKONING (on GPT-2-base) outperforms zero-shot and 8-shot GPT-3.5 on ProofWriter and CLUTRR-SG in the reported evaluations, especially in the presence of distractors; GPT-3.5 can improve with few-shot prompting but remains behind RECKONING in these experiments.",
            "ablation_or_analysis_results": "No ablation performed on GPT-3.5 in this paper beyond comparing zero-shot vs 8-shot prompting; main analyses focus on RECKONING components (inner steps, multi-task objective, adaptive LR) on the GPT-2 substrate.",
            "uuid": "e4952.3",
            "source_info": {
                "paper_title": "RECKONING: Reasoning through Dynamic Knowledge Encoding",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
            "rating": 2,
            "sanitized_title": "clutrr_a_diagnostic_benchmark_for_inductive_reasoning_from_text"
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "How to train your MAML",
            "rating": 1,
            "sanitized_title": "how_to_train_your_maml"
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 1,
            "sanitized_title": "language_models_as_knowledge_bases"
        },
        {
            "paper_title": "Locating and editing factual associations in GPT",
            "rating": 1,
            "sanitized_title": "locating_and_editing_factual_associations_in_gpt"
        }
    ],
    "cost": 0.015071999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RECKONING: Reasoning through Dynamic Knowledge Encoding</p>
<p>Zeming Chen zeming.chen@epfl.ch 
Natural Language Processing Lab
EPFL</p>
<p>Stanford University</p>
<p>Gail Weiss gail.weiss@epfl.ch 
Natural Language Processing Lab
EPFL</p>
<p>Stanford University</p>
<p>Eric Mitchell eric.mitchell@cs.stanford.edu 
Meta AI Research 3</p>
<p>Asli Celikyilmaz aslic@meta.com 
Antoine Bosselut antoine.bosselut@epfl.ch 
Kyle Kyle 
Griffin Amy 
RECKONING: Reasoning through Dynamic Knowledge Encoding
81A10248EBA6CCFBBA724B5AB7114C4F
Recent studies on transformer-based language models show that they can answer questions by reasoning over knowledge provided as part of the context (i.e., incontext reasoning).However, since the available knowledge is often not filtered for a particular question, in-context reasoning can be sensitive to distractor facts, additional content that is irrelevant to a question but that may be relevant for a different question (i.e., not necessarily random noise).In these situations, the model fails to distinguish the knowledge that is necessary to answer the question, leading to spurious reasoning and degraded performance.This reasoning failure contrasts with the model's apparent ability to distinguish its contextual knowledge from all the knowledge it has memorized during pre-training.Following this observation, we propose teaching the model to reason more robustly by folding the provided contextual knowledge into the model's parameters before presenting it with a question.Our method, RECKONING, is a bi-level learning algorithm that teaches language models to reason by updating their parametric knowledge through backpropagation, allowing them to then answer questions using the updated parameters.During training, the inner loop rapidly adapts a copy of the model weights to encode contextual knowledge into its parameters.In the outer loop, the model learns to use the updated weights to reproduce and answer reasoning questions about the memorized knowledge.Our experiments on two multi-hop reasoning datasets show that RECKONING's performance improves over the in-context reasoning baseline (by up to 4.5%).We also find that compared to in-context reasoning, RECKONING generalizes better to longer reasoning chains unseen during training, is more robust to distractors in the context, and is more computationally efficient when multiple questions are asked about the same knowledge.Preprint.Under review.</p>
<p>Introduction</p>
<p>Consider the sentence: "John is David's dad, and Tom is John's dad".Concluding that Tom is David's grandfather involves reasoning about the information in the sentence.Specifically, it requires understanding the direct information, or contextual knowledge, given in the sentence: the stated relationships between John, David, and Tom; and combining it with our existing, commonsense knowledge of the world: someone's dad's dad is their grandfather.Achieving such logical reasoning automatically has long been a goal of AI [51,16,71,79].</p>
<p>The example above demonstrates two necessary abilities required for successful reasoning: first, holding large amounts of commonsense or general knowledge about the world, and second, processing and combining new information with existing knowledge.Transformer-based large language models Figure 1: Our algorithm, RECKONING, solves reasoning problems by encoding external contextual knowledge into a model's parameters through gradient updates.At inference time, RECKONING performs a few parameter updates using the gradients of a language modeling loss to encode the relevant facts.Then, the updated model answers the question using only its implicit knowledge.</p>
<p>have shown a remarkable capacity for the first of these abilities, repeatedly being demonstrated to memorize large amounts of data, or parametric knowledge, in their weights [61,7,10,48].</p>
<p>For the second, recent work showed that transformers fine-tuned to predict answers over a concatenated context ("The cow is big; If something is big then it chases the dog; If the cow chases the dog then the cow sees the rabbit") and question ("Did the cow see the rabbit?")achieve high performance on reasoning tasks where all necessary knowledge is given in the context [16].We refer to this general setting as in-context reasoning (ICR), and differentiate by amount and type of knowledge given [30].</p>
<p>In real-world question-answering settings [38,21,40,15], large amounts of contextual knowledge may be provided at once, and the information may not be perfectly filtered for a specific question.Unfortunately, in-context reasoning is highly sensitive to distractors [67]: additional facts that are not relevant to a question (e.g., "The cow is round" for the above example).Indeed, when fine-tuning and evaluating GPT-2 [57] for ICR, we find that adding distractors to the context drops performance from 99.4% to only 70.9% accuracy for the same questions ( §4.2).This sensitivity to distractors in contextual knowledge contrasts with GPT-2's apparent robustness to distractors in parametric knowledge: for any specific example, most of the training data seen by GPT-2-which forms its parameters-is likely to be completely irrelevant to that example.Naturally, we wonder whether presenting contextual knowledge in the same way as memorized knowledge, by encoding it into a model's parameters, will improve the reasoning abilities of transformer-based language models.</p>
<p>In this work, we propose a novel bi-level optimization algorithm, RECKONING, that learns to memorize (and reason) over facts (i.e., knowledge) by performing inference-time parameter updates using gradients computed from a language modeling loss on those facts.The updated model is then used to answer any questions about those facts.Our training framework involves two nested loops: the inner loop performs fast adaptations from a set of initial weights to memorize a set of external knowledge through a few gradient updates, and the outer loop optimizes those same initial weights such that the updated model will solve reasoning problems associated with the memorized knowledge.In other words, the outer loop learns optimal meta-parameters that can rapidly memorize and successfully reason over contextual knowledge, allowing knowledge memorization to be optimized directly for downstream reasoning.At inference time, instead of including external knowledge in the input sequence as the prefix to a question prompt, the model can encode it in its parameters through gradient updates and then reason over its updated parametric knowledge to reach a conclusion.</p>
<p>We evaluate RECKONING on two synthetic multi-hop reasoning datasets: ProofWriter [71] and CLUTRR-Systematic-Generalization (CLUTRR-SG) [27], comparing against a fine-tuned ICR (FT-ICR) baseline that uses the same underlying model.Our results show that RECKONING consistently outperforms the FT-ICR baseline on each benchmark, demonstrating that it successfully learns to answer multi-hop reasoning questions as desired.In particular, we find that RECKONING more successfully generalizes to adversarial settings, such as the presence of distractor facts and the introduction of longer reasoning chains at inference time.Finally, while the inference-time gradient updates make RECKONING slower to process new knowledge than a typical ICR forward pass, our run-time analysis shows that RECKONING is more efficient when answering multiple questions about a shared knowledge set.This is because RECKONING only needs to encode the knowledge once to answer multiple questions about it.Overall, we demonstrate that RECKONING is an effective  algorithm for reasoning through dynamic and controllable knowledge encoding, overcoming an observed weakness in the common reasoning setting and providing multiple additional benefits.</p>
<p>Background</p>
<p>Notation We use f : X × θ → Y to refer to parameterised functions in which X is the set of possible inputs and θ are their possible weights (parameters).We use f θ : x → f (x, θ) to easily refer to any f with a given set of parameters θ.We describe reasoning problems using tuples (K, x, y * , Y ) such that y ∈ Y is the correct answer for the question x given facts K, and use D to refer to sets of such problems.When it is clear from context, we drop Y and use only (K, x, y * ).</p>
<p>Language Modeling and Memorization</p>
<p>In the causal language modeling (CLM) objective, a parameterized model f θ is trained to estimate the conditional probabilities of each token in a sequence given its predecessors: p(x t |x &lt;t ) .Specifically, we train f θ to approximate p using the CLM loss:
L CLM (f θ , x) = − T t=1 log f θ (x t |x 1 , ..., x t−1 ).(1)
This training objective allows language models to memorize individual training examples [10,9], and we will exploit this ability in order to memorize and draw on contextual knowledge in our work.</p>
<p>Transformers as Soft Reasoners</p>
<p>In natural language reasoning tasks, we are given reasoning problems (K, x, y * , Y ) in natural language and attempt to recover the correct answer y * from the context K, question x, and possible answers Y alone.In in-context reasoning, language models f θ trained with a CLM objective are applied to this task by selecting as the response the answer y ∈ Y with a maximum probability according to the model's next-token prediction from the concatenated context and question:
y = arg max y ′ ∈Y f θ (y ′ |[K; x]
).Previous works show that, after relevant supervised fine-tuning, transformer language models can achieve high performance in this setting [16,71,27], though this degrades significantly in the presence of irrelevant facts (distractors) [67].</p>
<p>Method</p>
<p>Addressing these challenges, we propose RECKONING (REasoning through dynamiC KnOwledge eNcodING), which solves reasoning problems by memorizing the provided contextual knowledge, and then using this encoded knowledge when prompted with downstream questions.Specifically, RECKONING uses bi-level optimization to learn a set of meta-parameters primed to encode relevant knowledge in a limited number of gradient steps.The model can then use its updated weights to solve reasoning problems over this knowledge, without further presentation of the knowledge itself.</p>
<p>Overview: Inference Given a reasoning problem (K, x, y, Y ), we initialize our model with weights copied from a set of meta-parameters θ and perform a constant number N of gradient descent steps on these with the goal of minimizing the CLM objective on the knowledge set K. This allows the model to memorize K in its updated parameters, which we refer to as θK N .Next, we pass the question x to the model, using f θK N to obtain a distribution over Y , and taking as output the answer y ∈ Y with the highest probability.For this method to consistently output the ground truth y * , we seek a set of optimal meta-parameters θ * that can quickly memorize (i.e., learn) the given knowledge in a way that then allows accurate reasoning when queried about the knowledge downstream.</p>
<p>Training RECKONING Given a distribution p(D) of reasoning problems, our proposed bi-level optimization framework RECKONING (seen in Figure 2) optimizes the following objective:
θ * ∈ arg min θ E (K,x,y)∼p(D) <a href="2">L CE (f θK N (x), y)</a>
where for all K, n ∈ N, and θ: θK 0 = θ, and
θK n+1 = θK n − α∇L CLM (f θK n , K).(3)
Here, L CE (f (x), y) denotes the cross-entropy (CE) loss, which we apply with the relevant parameters for each reasoning question in D, L CLM (f, K) = 1 |K| k∈K L CLM (f, k) denotes the causal language modeling loss, and N and α are pre-defined hyperparameters of the fine-tuning.We seek our actual meta-parameters θ through gradient descent.In particular, denoting by θ 0 our initial meta-parameters, and θK N,i the parameters θK N obtained when initializing θK 0 with θ i , we iteratively compute
θ i+1 = θ i − η∇ 1 |D i | (K,x,y)∈Di L Total (f θK N,i , K, x, y),(4)L D ′ ← 0 4:
for each (K, x, y) ∈ D ′ do 5:</p>
<p>Initialize θK 0 = θ 6:</p>
<p>for n := 0 to N − 1 do ▷ inner loop 7:
θK n+1 ← θK n − α∇LCLM (f θK n , K) 8:
end for 9:
L D ′ ← L D ′ + LTotal(f θK N , K, x, y) 10:
end for 11:
θ ← θ − η∇ 1 |D ′ | L D ′ 12: end while
where L Total (f, K, x, y) = L CE (f (x), y) and for each i, D i is randomly sampled from p(D).This continues until L Total converges.</p>
<p>The training can be seen as two nested loops: at each iteration, the outer loop (Equation (4)) samples a random batch D i ⊆ D of reasoning problems for evaluating (in order to update) the current meta-parameters θ i , after the inner loop (Equation (3)) adapts them to encode the associated knowledge through N steps of gradient updates.</p>
<p>Multi-Task Objective Through our experiments, we find that adding a knowledgerecovery objective to the outer loop-such that the model must also state all of K when prompted with x-improves the model's reasoning performance.We evaluate knowledge recovery with a CLM loss and combine the two losses by simple addition, following prior works [23,75,74].The entire change is achieved by redefining the total loss in our outer loop (Equation ( 4)) as:
L Total (f, K, x, y) = L CE (f (x), y) + L CLM (f, x, K)(5)
where L CLM (f, x, K) is the language modeling loss on K, as in Equation ( 3), but this time conditioned on the question x.The overall process for training RECKONING is depicted in Algorithm 1 and Figure 2. Additionally, we dynamically learn a per-step-per-layer learning rate to replace the shared constant learning rate in the inner loop.We give more details Appendix D.</p>
<p>Experiments</p>
<p>Setup We conduct our experiments on two datasets focusing on multi-hop logical reasoning over natural language knowledge: ProofWriter [71], which measures the model's ability to emulate reasoning over facts and rules expressed in natural language, and CLUTRR-SG [27], which is generated from the CLUTRR [69] benchmark, a logical reasoning task that involves reasoning over family relationships between entities grounded in first-order logical proofs.For these datasets, each problem requires multiple reasoning hops to reach an answer. 1e compare our method against the following baselines: (1) a fine-tuned model that performs a forward pass on only the question without access to the knowledge (No-Facts), (2) a fine-tuned model that performs a forward pass on only the knowledge without access to the question (No-Question),</p>
<p>(3) a model trained using RECKONING with random knowledge that is not relevant to the questions (Random-Facts), and (4) an ICR baseline that concatenates the knowledge K with the question x in a single context and is trained using supervised learning to predict the answer (FT-ICR).Our first three baselines sanity-check whether any surface-level patterns in the questions and facts can be exploited to make accurate predictions.The last baseline compares RECKONING to the conventional way of reasoning with language models.In all experiments, we use the base GPT-2 [57] model (∼124M parameters) as our initialization.We compute each score from the average across three different runs.</p>
<p>Unless stated otherwise, we refer by RECKONING to our method trained with the multi-task objective.</p>
<p>For more details on the implementation, datasets, and examples, see Appendix A and Appendix C.  1: Label accuracy of RECKONING on ProofWriter and CLUTRR-SG, compared to FT-ICR baselines where the supporting facts are given as part of the input.MT marks models trained with the multi-task objective, which optimizes both question answering and knowledge memorization.</p>
<p>Main Results</p>
<p>We first evaluate whether RECKONING learns to perform reasoning in the base setting.A model is given a set of supporting facts (without distractors) and a question (or hypothesis) as input and begins by performing a few CLM learning steps on the facts.Then, the updated model reads only the question and generates an answer.To answer correctly, the model must reason over both facts and the question, meaning it must encode the facts during the inner loop such that multi-hop reasoning can be performed over them later.</p>
<p>We train our models and the fine-tuned ICR (FT-ICR) baselines with both the single-task (L CE ) and multi-task (L CE + L CLM ) objectives.For multi-task (MT) training, the model learns to answer the question and generate its relevant knowledge in the outer loop.Table 1 shows the evaluation results on question answering (or hypothesis classification).For all hop numbers in ProofWriter and in CLUTRR-SG, multi-task RECKONING outperforms the best result of all baselines (consistently obtained by multi-task FT-ICR) by an average of 1%.We conclude that RECKONING can effectively solve reasoning problems through its updated parametric knowledge, and do so better than existing baselines.The multi-task objective is crucial for this success: not only is RECKONING's performance consistently higher (by an average of 2.8% over the two datasets and their hop counts) when using the multi-task rather than single-task (ST) objective, it also under-performs both FT-ICR baselines when trained with only the single-task objective.The multi-task objective also improves FT-ICR consistently (average 1.8%), though it is not enough to beat the multi-task RECKONING.In all further experiments, we consider only RECKONING and FT-ICR with a multi-task objective.</p>
<p>Generalizing to Longer Reasoning Chains Our first experiments assume an alignment between the number of reasoning hops in the questions in the training and test set.However, we may not be able to train on all n-hop reasoning questions we encounter in the wild, and we rarely know the number of reasoning hops in a question a priori.Consequently, we also measure the generalization capacity of our model to questions with hop numbers unseen during training.We compile interpolation (fewer hops than the train set) and extrapolation (more hops than the train set) test sets from the CLUTRR-SG dataset.Again, we train models individually on 2-hop, 4-hop, and 6-hop examples and evaluate these three sets of models on the test sets, which contain 2-10-hop reasoning questions. Figure 3 shows that both RECKONING models and ICR baselines retain high performance on the interpolation</p>
<p>Number of Test Hops</p>
<p>Training on 4-hop Questions</p>
<p>FT-ICR RECKONING</p>
<p>Figure 3: System generalization evaluation on CLUTRR-SG.From left to right, the models are trained on 2-hop, 4-hop, and 6-hop CLUTRR-SG data portions.We evaluate the model on 2-10 hop test sets.The higher the hops, the more facts a question has, and the more difficult that question is.Does RECKONING's performance depend on the number of inner loop gradient steps?In RECK-ONING, the model performs multi-hop reasoning over facts by encoding facts using multiple gradient steps in the inner loop optimization ( §3).Naturally, this process prompts the question of whether there is a correlation between the number of reasoning hops and the number of gradient steps needed to reliably encode the knowledge (i.e., problems with more reasoning hops require more gradient steps in the inner loop to encode the facts).In Figure 4, we show for CLUTRR-SG that as the number of inner loop steps increases, the label accuracy of the outer-loop task also increases.Furthermore, when considering the performance gains for reasoning with 6 inner loop steps (i.e., knowledge encoding) steps as opposed to one, we observe that this gap is much more pronounced for 4-hop (42.3%) and 6-hop (34.7%) reasoning than it is for 2-hop reasoning (5.9%).These results show that problems requiring more hops of reasoning also greatly benefit from more steps of inner loop knowledge encoding.</p>
<p>Reasoning with Distractors</p>
<p>In cases where multiple questions must be answered about the same knowledge set, some knowledge that is relevant to one question will likely be irrelevant to another question.For example, in Table 6, the fact "Charlie is White." is not needed to answer the question "Harry is red?".Thus, it is important to evaluate the robustness of RECKONING when there exists irrelevant information (i.e., distractors) in the knowledge set.In this experiment, we analyze RECKONING's ability to focus on the correct knowledge and ignore distractors when answering questions.We use ProofWriter as the evaluation dataset since it already has a setting with distractors included in the knowledge.For systematic analysis, we gradually add distractors to the context (starting from 2 and finishing at all possible distractors, of which there are an average of 7 per question).We train RECKONING and the baseline using the multi-task objective, where the model must (1) recall all of the facts and rules relevant to the question and (2) predict the conclusion based on the correct knowledge.In this case, we adapt training such that for each question x, the outer-loop (Equation ( 5)) CLM loss is only computed with respect to the relevant facts from K, thereby learning to recall only relevant facts during training.In Figure 5, we see that RECKONING's performance is consistently more robust under distractors than the FT-ICR baseline.When we include all of the distractors in the context, RECKONING achieves a significantly higher average label accuracy (82.5%) across hops than the baseline (70.9%), as computed by the average of the 3 considered hop depths.Additionally, compared to performance with no distractors, RECKONING's performance only drops 17.1% while the baseline performance drops 28.6%, thereby exhibiting a better ability to disentangle the correct knowledge from the distractors.One of the advantages of RECKONING is the ability to memorize a large set of knowledge K and answer multiple related questions about that knowledge at a little extra cost per question.Specifically, in contrast to ICR, RECKONING can encode K once and answer multiple questions without needing to reprocess it for each question asked.To test whether RECKONING could be a more efficient method for inference in this setting, we measure the wall-clock time (in seconds) of the complete inference pipeline of RECKONING vs. ICR.For this experiment, we use a synthetic reasoning dataset in which K is a sequence of random letters, and the question x asks for the most frequent letter in the context.The total number of tokens in each example is 1024: 7 for x, 1 for the answer, and the remaining 1016 for K, broken into 8 "facts".The FT-ICR baseline receives a sequence including all 8 facts and the question.In contrast, RECKONING receives the 8 facts as a batch of eight segments of 127 tokens and encodes them in parallel in the inner loop.In the outer loop, the model only receives the question or a batch of questions.We focus on two settings: (1) inference time for a single question and (2) inference time when answering multiple questions.In the multiple-question setting, we set the number of questions to 18 (the same as in ProofWriter).For RECKONING, the inference process includes the inner-loop knowledge encoding and the final forward pass to encode the question.We set the number of inner loop gradient steps to 1 and 4. In Table 2, we see that when answering a single question, RECKONING does not perform inference faster compared to in-context reasoning.However, RECKONING shows significant advantages under a multi-question setting.Both the 1-step inner loop and the 4-step inner loop are faster than the baseline.Since RECKONING encodes the knowledge in model parameters, it does not need to reprocess the knowledge for a related question and is more efficient.We run this experiment on 1 RTX 3090 GPU. 2  Next, we report in Table 4 the model's ability to reproduce memorized facts correctly under a multi-task setting, as measured by an exact match score between the reproduced facts and the gold facts. 3We evaluate on the ProofWriter dataset both with and without distractors in the context and compare the results to the FT-ICR baseline.The results show that RECKONING MT can successfully (average exact match score of 99.3%) recover the relevant facts from its model parameters when the context does not include any distractors.Note that this is comparable to the FT-ICR baseline, for which the task is much easier as it can directly attend to and copy the facts from input, while RECKONING MT no longer has direct access to them.When the context includes distractors, both RECKONING and FT-ICR struggle to identify and reproduce only the relevant facts.However, the performance for FT-ICR (average 49.4%) drops far below that of RECKONING (73.6%), demonstrating that RECKONING is much better at disentangling the relevant knowledge from the distractors.</p>
<p>Run-time Analysis</p>
<p>Memorizing Knowledge</p>
<p>Finally, we show that RECKONING with a multi-task objective is also more robust to distractors as it trains the model to only reproduce the facts that would be relevant to a particular question we ask in the outer loop.As in Section 4.2, we use the ProofWriter dataset and, for each question, add all the distractors to the context.We train the model using the multi-task objective, and we report the label accuracy.While in Table 1, we originally saw a ∼ 1% improvement from training with a multi-task objective on ProofWriter with no distractors, we see a much more significant performance gap in Figure 6 (∼ 18.2%) when distractors are available.We also note that the performance of the single-task model is essentially random (see the Random-Facts baseline from Table 1).By learning how to memorize knowledge in the inner loop so that it can recall relevant facts in the outer loop, the model also learns how to encode facts more robustly over them.</p>
<p>Related Work</p>
<p>Logical Reasoning Datasets and Benchmarks As a central building block of human cognition and intelligence [26], logical reasoning has been a long-pursued topic in the field of AI [53,46,8,2,16,12,70,44]. Logical reasoning, in general, can be categorized in a trichotomy of deductive, inductive, and abductive reasoning [24].Multiple datasets have been published that evaluate neural models' ability on these three types of logical reasoning [16,5,69].Initially, logical reasoning tasks focused on hypothesis classification, where, given a theory consisting of multiple facts and rules, a model would determine whether the hypothesis was correct.Recently, transformer-based language models have been directly used to solve this task in synthetic [16,63], real-world [28], and adversarial [59,25,65] settings.However, simply predicting whether the hypothesis is valid does not elucidate whether the model correctly reasons over the provided knowledge.To better analyze and interpret the reasoning process of language models, new tasks focus on generating the valid proof that explains the model's decision [71,19].Our proposed method, RECKONING, is optimized for the hypothesis classification reasoning task and evaluates on many of these datasets [71,27].</p>
<p>Logical Reasoning over Natural Language Historically, automatic logical reasoners used symbolic systems and formal languages as a knowledge representation [41,53,50,1,47,78].However, these systems were hard to scale up due to the knowledge-acquisition bottleneck and the brittleness of formal representation [33,81].With recent advances in transformer-based language modeling [73] and self-supervised pre-training [20,57,58], a novel paradigm for logical reasoning emerged, where pre-trained language models (PLMs) could be used as soft reasoners over knowledge expressed in natural language.Natural language as a knowledge representation allowed PLMs to handle raw input with diverse formats [31,14], resulting in PLMs being applied to various types of deductive [16], abductive [5], and inductive [27] reasoning tasks.However, language models as soft reasoners also showed structural weaknesses, as their performance dropped on complex logical operations [77,12], and their reasoning process was not interpretable [62,43].Consequently, a new line of work uses neuro-symbolic methods to combine the best of both language models and symbolic reasoning [34,42,13,6,39].Specifically, the interpretability gap motivated modular and step-wise reasoning systems that use PLMs as intermediate modules [64,72,32,66,56,80] to generate reasoning steps (e.g., proofs).In contrast to these works, our method RECKONING dynamically encodes natural language knowledge into the model parameters, thereby reasoning by mixing contextual knowledge with pre-encoded parametric knowledge and allowing the model to determine a conclusion based on its updated parametric knowledge.</p>
<p>Model Editing While our motivations are grounded in research on machine reasoning, our methods are more often used in the area of model editing.Model editing is a method to edit a model's parameters to correct its errors or update the model.Several works propose hypernetwork-based methods to edit knowledge in a model by predicting updates conditioned on new factual statements [29] or transforming the gradients from new provided facts [52] to make local edits to a model.Other approaches focus on more direct edits of model behavior, such as directly modifying neuron outputs [18,82], localizing distinct feed-forward layers that are responsible for factual recall, and modifying these weights [48], and performing weight updates across multiple layers to perform simultaneous edits [49].Similarly, our method also rapidly edits the model parameters to add knowledge.However, our bi-level framework optimizes model edits for the reasoning task in the outer loop, allowing the model to learn to do fast memorization of knowledge that can support the model's reasoning ability.</p>
<p>Language Models as Knowledge Bases Our work learns to reason by dynamically encoding contextual knowledge in the parameters of language models before answering questions about them.Previous studies have found that LLMs can store real-world facts learned during pre-training [61,10,48,9].Learning these facts during pre-training allows language models to be prompted [55,37,68,83] or adapted [7,60,35,36] to produce these facts on-demand.However, LLM knowledge is latent and hard to identify or control.The model generation is sensitive to specific words or phrases.LLMs emit knowledge encoded in the parameters only when prompted appropriately [54,22,17,9].It is also difficult to inject or update knowledge for LLMs [48], and the memorization of knowledge in LLMs is not optimized toward their reasoning ability.In our work, we seek to find a way to add knowledge to LLMs in a controllable and adaptive way that can be beneficial to downstream reasoning applications.</p>
<p>Conclusion</p>
<p>We present RECKONING, a bi-level learning framework for multi-hop reasoning that encodes knowledge verbalized using natural language into a model's parameters through gradient updates.During training, the inner loop encodes the contextual knowledge into the model parameters by backpropagating a language modeling loss.In the outer loop, given only the question as input, the model solves reasoning problems using the memorized knowledge.Through bi-level optimization, RECKONING finds a set of meta-parameters that allows it to perform quick knowledge-based updates for reasoning.Our experiments show that RECKONING learns to reason only by relying on its parametric knowledge after the external knowledge has been encoded.Using a multi-task objective that jointly optimizes reasoning and knowledge memorization in the outer loop, RECKONING outperforms ICR baselines that are trained to encode external knowledge as part of the context.Through our analysis, we show that RECKONING is more generalizable to problems with longer reasoning chains, less susceptible to irrelevant distractor knowledge, and that RECKONING is more efficient than the baseline when answering multiple questions that require common knowledge.</p>
<p>American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3204-3219, Seattle, United States, July 2022.Association for Computational Linguistics.</p>
<p>A Dataset</p>
<p>ProofWriter The ProofWriter [71] dataset has 500k pairs of questions, answers, and proofs over natural-language rule bases.Each example in the dataset contains a set of facts, a set of rules, a hypothesis, and a label indicating whether the hypothesis is true, false, or unknown.The dataset comprise five datasets named D0, D1, D2, D3, D5, each with 100k examples.Each dataset's questions require reasoning up to depths D (D = 0, 1, 2, 3, 5) to determine their answers.In our experiments, we only focus on the datasets that require more reasoning depths (D2, D3, D5).We show an example from the dataset in Table 6.In these datasets, a set of facts and rules are mapped to 18 questions, where the questions can be answered based on a subset of the facts and rules.Thus, some of the facts or rules can be irrelevant to some questions, and we call them distractors in Section 4.2.In the experiment for knowledge encoding with distractors, we encode all the facts in the model parameters and evaluate its ability to reproduce and reason over the correct facts.We show an example of distractor and relevant knowledge of a question in Table 8.For detailed statistics on the two datasets, please see Table 5. CLUTRR-SG The CLUTRR-SG [27] is an evaluation dataset for inductive reasoning on family relations adapted from the [69] dataset for measuring systematic generalization.Each example in the dataset contains (i) a set of facts representing a family graph G = (V, E) where nodes (V ) are entities and edges (E) are the relationships.(ii) a question asking the relationship between two entities (v 1 , v n ∈ V ), and (iii) a target relationship e * ∈ E as the answer for the question.The facts are expressed as a list of (v i , e j , v k ) tuples.The two entities in the question are separated by more than one hop in the graph.There are 272 unique entities, 20 relationship types, and nearly 1.5M possible facts in the dataset.Following the authors, we define the difficulty of examples based on the number of family graph edges (i.e., the number of reasoning hops required to determine a relation), in which k edges (k-hop) correspond to k facts.We show an example from the dataset in Table 7.To motivate the advantage of RECKONING on mitigating interference from distractors, we analyze the performance change of fine-tuned incontext reasoning with and without distractors present in the context of the questions.We define distractors as additional facts or rules present in a question's context that is not directly relevant to the questions.A model should not be able to use only these distractors to answer a question correctly.For an example of distractors in a question's context, please see Table 8.We evaluate the baseline on the ProofWriter dataset since it naturally contains contexts including distractors (Table 8).Recall that we have two training objectives.The single-task objective only trains the model to predict an answer for each question given their contexts.The multitask objective (MT) trains the model to not only predict an answer but also reproduce the correct facts and rules (in contrast to distractors) based on the contexts.We evaluate the baseline on 2, 3, and 5-hop datasets with both training objectives, and we report the average label accuracy across hops in Figure 7. Compared to the baseline's performance without distractors in the context, the performance with distractors decreases significantly.For single-task, the performance drops 23.2% when adding distractors to the contexts, and the performance with the multi-task objective drops 28.6%.The results highlight in-context reasoning's high sensitivity to the interference of irrelevant information in the contexts.</p>
<p>B In-context Reasoning with Distractors</p>
<p>C Implementation Details</p>
<p>We select GPT-2-base [57] as the model for our method and all the baselines.We use the version implemented by the Huggingface Transformers library [76].6: An example from the dataset ProofWriter.There are 6 facts and 6 rules mapped to three question-answer pairs.Each question can be answered based on the given facts and rules.</p>
<p>are conducted on a cluster with NVIDIA A100 (40GB) GPUs.All the baseline experiments are conducted on a local machine with NVIDIA RTX 3090 GPU (24GB).</p>
<p>Fine-tuned In-context Reasoning We set the train batch size to 16 and train the model for 6 epochs with early stopping based on the validation label accuracy.We set the learning rate to 3e-5 and use the AdamW optimizer with ϵ set to 1e-8.We validate the model on the development set for every epoch and select the best checkpoint using the validation accuracy as the metric.</p>
<p>RECKONING In the inner loop, we generally perform 4 gradient steps for lower-hop questions (2, 3, 4-hop) and 5 gradient steps for higher-hop questions (5 and 6-hop).We select the AdamW [45] as the optimizer for the inner loop since the main task is language modeling.The inner-loop learning rate is set to 3e-5 before training and the algorithm dynamically learns a set of optimal learning rates when converged.In our experiments and analysis, we only report the results from RECKONING with a multi-task objective since its performance is better than the single-task objective.In the outer loop, we also use the AdamW with a learning rate of 3e-5.For both optimizers, we set ϵ to 1e-8.We set the train batch size to 2 due to memory limitations.We apply the technique of gradient accumulation and set the accumulation step to 2. We train the model for 6 epochs with early stopping.For each epoch, we validate the model twice: once in the middle and once at the end.We select the best model checkpoint based on the validation label accuracy.</p>
<p>D Adaptive Learning Rate</p>
<p>Prior works [3,4] show that a fixed learning rate shared across steps and across parameters does not benefit the generalization performance of the system.Instead, [3]   We study how much the dynamic learning rate in the inner loop contributes to the outer loop performance.We fix all the hyperparameters except the option of using the dynamic or fixed learning rate.We conduct the analysis using the CLUTRR-SG dataset since it is more complex and difficult (lower random performance).</p>
<p>Are dynamic learning rates necessary for RECK-ONING's performance?Following prior works on meta-learning [3,4], we dynamically learn a set of per-step-per-layer learning rates for RECKONING.</p>
<p>In this ablation study, we analyze whether dynamic learning rates for the inner loop are effective in improving the outer loop reasoning performance.Similarly, we fix other experimental settings and set the number of inner loop steps to 4. As Figure 8 shows, when using a static learning rate (i.e., all layers and inner loop steps share a constant learning rate), the performance drops by a large margin (average drop of 34.2%).The performance drop becomes more significant on questions requiring more reasoning hops (45.5% drop for 4-hop and 39.5% drop for 6-hop), demonstrating the importance of using a dynamic learning rate in the inner loop of our framework.Table 9: Label accuracy of RECKONING on ProofWriter and CLUTRR-SG compared against a popular Large Language Model (LLM), GPT-3.5.We prompt GPT-3.5 in the zero-shot setting and also the 8-shot in-context learning setting.Models with MT are trained with the multi-task objective in the outer loop.</p>
<p>E Experiments with Large Language Models
ProofWriter
Recently, Large Language Models (LLMs) with large parameter sizes learned from human preferences have shown remarkable performance in language understanding and generation.These LLMs are powerful zero-shot and few-shot reasoners.Recent works find that LLMs learn to perform multistep reasoning by first generating new reasoning chains and then predicting the answers.In this experiment, we benchmark the performance of a popular new LLM, GPT-3.5, on the two multi-hop reasoning datasets we used in our paper.We first evaluate GPT-3.5'szero-shot reasoning performance in predicting the correct answers.As Table 9 shows, zero-shot prompting GPT-3.5 significantly underperforms RECKONING's performance.GPT-3.5'sperformance improves on ProofWriter without distractors, but still is behind the performance of RECKONING.When distractors are present in the context, RECKONING performs much better than zero-shot and few-shot GPT-3.5 prompting.This</p>
<p>Figure 2 :
2
Figure 2: The two-stage training process of RECKONING with an inner and outer loop.</p>
<p>Figure 4 :
4
Figure 4: Multi-hop reasoning performance as a function of the number of inner loop steps (x-axis), with each line focusing (by training and testing) on CLUTRR-SG with a different number of hops.</p>
<p>Figure 6 :
6
Figure 6: Performance comparison between models trained with a single-task and a multi-task objective under distractors.With the multi-task objective, the model learns to memorize the relevant facts and perform reasoning over them.</p>
<p>Figure 7 :
7
Figure 7: Label accuracy of fine-tuned in-context reasoning on questions with and without distractors in the context.With the same questions, adding distractors to contexts significantly lower the performance of in-context reasoning, both in the singletask and multi-task setting.</p>
<p>Figure 8 :
8
Figure8: We study how much the dynamic learning rate in the inner loop contributes to the outer loop performance.We fix all the hyperparameters except the option of using the dynamic or fixed learning rate.We conduct the analysis using the CLUTRR-SG dataset since it is more complex and difficult (lower random performance).</p>
<p>Table
4.1 Multi-hop Reasoning PerformanceProofWriterCLUTRR-SGMethod2-h3-h5-h2-h4-h6-hNo-Facts64.1 63.0 64.20.08.88.9No-Question66.2 67.0 65.2 35.7 36.4 28.7Random-Facts64.1 63.0 64.20.01.32.5FT-ICRST98.4 98.8 97.8 97.4 91.3 89.1FT-ICRMT99.4 99.2 99.6 98.1 96.9 90.3RECKONINGST98.3 98.3 99.1 96.0 90.2 91.2RECKONINGMT 99.5 99.7 99.8 98.3 97.6 94.8</p>
<p>Table 2 :
2
Wall clock run-time, in seconds, of the fine-tuned ICR baseline and RECKONING.
ModelWall-clock Time (s)Single questionFT-ICR0.1887RECKONING1step0.2532RECKONING4step0.9664Multiple questions (18)FT-ICR2.0436RECKONING1step0.6228RECKONING4step1.4839</p>
<p>Table 3 :
3
Average inner loop validation loss: final (L CLM ) and difference from start to finish (∆L CLM ).In Table1, we saw that training RECKONING with a multitask (MT) outer loop objective improved over training with the single-task (ST) objective, potentially because the MT objective improves the model's ability to memorize the knowledge in the inner loop.To validate our hypothesis, we analyze RECKONING's performance in reproducing memorized knowledge.First, we show in Table3the inner loop average loss (L CLM ) and average change (∆L CLM ) (from first inner loop evaluation to last) on validation examples from the 5-hop ProofWriter data.We see that the average inner loop loss for RECKONING ST is much higher than RECKONING MT , and indeed starts out much higher as well.This shows that the ST outer loop objective, which optimizes the model only for question answering, does not learn to encode the knowledge in the inner loop by memorizing it.In contrast, the MT objective forces the model to learn to memorize the knowledge too: we observe that RECKONING MT minimizes the inner loop loss as it processes the knowledge.This pattern is also shown in the average inner-loss difference (∆L CLM ): the inner loop loss decreases more after the gradient updates when trained with the MT objective.
ProofWriterProofWriterdistractorMethod2-h3-h5-h2-h3-h5-hFT-ICRMT99.8 99.0 98.7 42.3 50.3 55.6RECKONINGMT 98.9 98.6 98.2 71.2 74.4 75.1</p>
<p>Table 4 :
4
Exact match score for reproducing memorized knowledge.In contrast to in-context reasoning, RECKON-ING does not have direct access to the knowledge.</p>
<p>Table 5 :
5
All the experiments for RECKONING Dataset splits and statistics for our experiments
Dataset#Train #Validation #TestCLUTRR-SG (2-hop) 96,01210,9723,102CLUTRR-SG (4-hop) 89,97210,0869,946CLUTRR-SG (6-hop) 90,92210,2908,788ProofWriter (2-hop)6,9961,0982,013ProofWriter (3-hop)10,8541,6413,057ProofWriter (5-hop)18,5252,5535,175IdentifierContentfact 1Harry is nice.fact 2Fiona is quite Nice.fact 3Fiona is round.fact 4Fiona is white.fact 5Dave is furry.fact 6Charlie is white.rule 1Furry people are green.rule 2Round, green people are red.rule 3All red people are white.rule 4Nice, round people are furry.rule 5If someone is nice, then they are round.rule 6If Charlie is round and Charlie is nice, then Charlie is white.
question-answer 1 Harry is red?True question-answer 2 Harry is not red?False question-answer 3 Dave is not white?Unknown Table</p>
<p>recommend learning a learning rate Algorithm 2 Dynamic Knowledge Encoding for Reasoning Require: An example distribution p(D), a transformer language model f , initial meta-parameters θ, outer step size η, initial inner step size α, inner loop length N .1: while not converged do
▷ outer loop2:Sample D ′ ∼ p(D)3: 4:L D ′ ← 0 for each (K, x, y) ∈ D ′ do5:Initialize θK 0 = θ6:for n := 0 to N − 1 do▷ inner loop7:θK n+1 ← θK n − α ⊙ ∇LCLM (f θK n, K)8:end for9:N L D ′ ← L D ′ + LTotal(f θK, K, x, y)10:end for11:α ← α − η∇L D ′▷ Update inner step size12:θ ← θ − η∇L D ′13: end whileDynamic LR vs. Static LRLR-fixLR-dyna1009996Label Accuracy (%)60 8081.550.584.745.2402-hop4-hop6-hop
In ProofWriter, the number of reasoning hops is called the proof depth. To unify the presentation of the results, we use the term "hop" to describe the number of reasoning steps for both datasets.
We perform this experiment in a limited setting and do not handle the case where hidden states could be cached for the forward pass of in-context reasoning, likely speeding up multi-question inference[11].
This is done by prompting the model with the question and comparing its output (after its answer to the question) to the concatenation of all of the facts. The model is able to produce these facts in the expected order due to an implementation detail: they are numbered and labeled when given to the inner loop.
for each layer of the network and for each adaptation step in the inner loop.The layer parameters have the freedom to learn to adjust the learning rates at each step.To control the learning rate α in the inner loop adaptively, we define α as a set of adjustable variable: α = {α 0 , α 1 , ...α L }, where L is the number of layers and for every l = 0, ..., L, α l is a vector with N elements given a pre-defined inner loop step number N .The inner loop update equation then becomeswhere ⊙ is an element-wise product and θ(l) n is the parameters for layer l at the inner step n.We learn the set of optimal inner loop learning rates α * by optimizing the parameters in the outer loop:where η is the outer loop learning rate and θ is the updated parameters from inner loop.Below, we show the final algorithm of RECKONING in Algorithm 2.
A tableau prover for natural logic and language. Lasha Abzianidze, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSeptember 2015</p>
<p>LangPro: Natural language theorem prover. Lasha Abzianidze, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2017 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsCopenhagen, DenmarkAssociation for Computational LinguisticsSeptember 2017</p>
<p>How to train your MAML. Antreas Antoniou, Harrison Edwards, Amos Storkey, International Conference on Learning Representations. 2019</p>
<p>Metalearning with adaptive hyperparameters. Sungyong Baik, Myungsub Choi, Janghoon Choi, Heewon Kim, Kyoung Mu, Lee , ArXiv, abs/2011.002092020</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Scott Downey, Yejin Wen Tau Yih, Choi, 2020</p>
<p>Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering. Antoine Bosselut, , Ronan Le Bras, Yejin Choi, Proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI). the 35th AAAI Conference on Artificial Intelligence (AAAI)2021</p>
<p>Comet: Commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Recursive neural networks can learn logical semantics. R Samuel, Christopher Bowman, Christopher D Potts, Manning, Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. the 3rd Workshop on Continuous Vector Space Models and their CompositionalityBeijing, ChinaAssociation for Computational LinguisticsJuly 2015</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. 2021</p>
<p>Transformer inference arithmetic. Carol Chen, 2022</p>
<p>Curriculum: A broad-coverage benchmark for linguistic phenomena in natural language understanding. Zeming Chen, Qiyue Gao, Proceedings of the 2022 Conference of the North. the 2022 Conference of the North</p>
<p>NeuralLog: Natural language inference with joint neural and logical reasoning. Zeming Chen, Qiyue Gao, Lawrence S Moss, Proceedings of <em>SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics. </em>SEM 2021: The Tenth Joint Conference on Lexical and Computational SemanticsAssociation for Computational LinguisticsAugust 2021</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck2022Jeff Dean, Slav Petrovand Noah Fiedel. Palm: Scaling language modeling with pathways</p>
<p>Perhaps PTLMs should go to school -a task to assess open book and closed book QA. Manuel Ciosici, Joe Cecil, Dong-Ho Lee, Alex Hedges, Marjorie Freedman, Ralph Weischedel, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. Christian Bessiere, the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-2072020Main track</p>
<p>Analyzing commonsense emergence in few-shot knowledge models. Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut, Proceedings of the Conference on Automated Knowledge Base Construction (AKBC). the Conference on Automated Knowledge Base Construction (AKBC)2021</p>
<p>Knowledge neurons in pretrained transformers. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Explaining answers with entailment trees. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Searchqa: A new q&amp;a dataset augmented with context from a search engine. Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, Kyunghyun Cho, 2017</p>
<p>Measuring and improving consistency in pretrained language models. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, Yoav Goldberg, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Measuring and harnessing transference in multi-task learning. Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, Chelsea Finn, 2021</p>
<p>Abductive and inductive reasoning: Background and issues. Peter A Flach, Antonis C Kakas, Applied Logic Series. NetherlandsSpringer2000</p>
<p>Logically consistent adversarial attacks for soft theorem provers. Alexander Gaskell, Yishu Miao, Francesca Toni, Lucia Specia, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Thirty-First International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence OrganizationJuly 2022</p>
<p>Editorial: The reasoning brain: The interplay between cognitive neuroscience and theories of reasoning. Vinod Goel, Gorka Navarrete, Ira A Noveck, Jérôme Prado, Frontiers in Human Neuroscience. 10January 2017</p>
<p>Measuring systematic generalization in neural proof generation with transformers. Nicolas Gontier, Koustuv Sinha, Siva Reddy, Christopher Joseph Pal, ArXiv, abs/2009.147862020</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R Fabbri, Wojciech Kryscinski, Caiming Xiong, and Dragomir Radev. Folio: Natural language reasoning with first-order logic. Xi Victoria Lin2022</p>
<p>Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srinivasan Iyer, Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. 2021</p>
<p>Reasoning with transformer-based models: Deep learning, but shallow reasoning. Chadi Helwe, Chloé Clavel, Fabian M Suchanek, 3rd Conference on Automated Knowledge Base Construction, AKBC 2021, Virtual. Danqi Chen, Jonathan Berant, Andrew Mccallum, Sameer Singh, October 4-8, 2021, 2021</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 2021</p>
<p>METGEN: A modulebased entailment tree generation framework for answer explanation. Ruixin Hong, Hongming Zhang, Xintong Yu, Changshui Zhang, Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>MonaLog: a lightweight system for natural language inference based on monotonicity. Hai Hu, Qi Chen, Kyle Richardson, Atreyee Mukherjee, Lawrence S Moss, Sandra Kuebler, Proceedings of the Society for Computation in Linguistics 2020. the Society for Computation in Linguistics 2020New York, New YorkAssociation for Computational LinguisticsJanuary 2020</p>
<p>Harnessing deep neural networks with logic rules. Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric Xing, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational LinguisticsAugust 20161</p>
<p>(comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs. Jena D Hwang, Chandra Bhagavatula, Le Ronan, Jeff Bras, Keisuke Da, Antoine Sakaguchi, Yejin Bosselut, Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>I'm not mad": Commonsense implications of negation and contradiction. Liwei Jiang, Antoine Bosselut, Chandra Bhagavatula, Yejin Choi, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterAssociation for Computational LinguisticsJune 2021</p>
<p>How can we know what language models know? Transactions of the. Zhengbao Jiang, Frank F Xu, Jun Araki, Graham Neubig, 2020Association for Computational Linguistics8</p>
<p>TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171</p>
<p>Maieutic prompting: Logically consistent reasoning with recursive explanations. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Natural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 2019Transactions of the Association for Computational Linguistics7</p>
<p>Cyc: Using common sense knowledge to overcome brittleness and knowledge acquisition bottlenecks. Douglas B Lenat, Mayank Prakash, Mary Shepherd, AI Magazine. 6465Mar. 1985</p>
<p>A logic-driven framework for consistency of neural models. Tao Li, Vivek Gupta, Maitrey Mehta, Vivek Srikumar, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Explainable multi-hop verbal reasoning through internal monologue. Zhengzhong Liang, Steven Bethard, Mihai Surdeanu, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 2017</p>
<p>Natural logic for textual inference. Bill Maccartney, Christopher D Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and ParaphrasingPragueAssociation for Computational LinguisticsJune 2007</p>
<p>On-demand injection of lexical knowledge for recognising textual entailment. Pascual Martínez-Gómez, Koji Mineshima, Yusuke Miyao, Daisuke Bekki, Proceedings of the 15th Conference of the European Chapter. Long Papers. the 15th Conference of the European ChapterValencia, SpainAssociation for Computational LinguisticsApril 20171</p>
<p>Locating and editing factual associations in GPT. Kevin Meng, David Bau, Alex J Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Massediting memory in a transformer. Kevin Meng, Sen Arnab, Alex J Sharma, Yonatan Andonian, David Belinkov, Bau, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Expert systems in production planning and scheduling: A state-of-the-art survey. K S Metaxiotis, Dimitris Askounis, John Psarras, Journal of Intelligent Manufacturing. 1342002</p>
<p>Expert systems in production planning and scheduling: A state-of-the-art survey. Kostas S Metaxiotis, Dimitris Askounis, John E Psarras, Journal of Intelligent Manufacturing. 132002</p>
<p>. Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D Manning, 2021Fast model editing at scale</p>
<p>Inductive logic programming: Theory and methods. Stephen Muggleton, Luc De Raedt, The Journal of Logic Programming. 1920May 1994</p>
<p>How context affects language models' factual predictions. Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, 2020</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Interpretable proof generation via iterative backward reasoning. Hanhao Qu, Yu Cao, Jun Gao, Liang Ding, Ruifeng Xu, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, 2020</p>
<p>Pushing the limits of rule reasoning in transformers through natural language satisfiability. Kyle Richardson, Ashish Sabharwal, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceJune 202236</p>
<p>How much knowledge can you pack into the parameters of a language model?. Adam Roberts, Colin Raffel, Noam Shazeer, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>A primer in BERTology: What we know about how BERT works. Anna Rogers, Olga Kovaleva, Anna Rumshisky, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Cynthia Rudin, Nature Machine Intelligence. 15May 2019</p>
<p>RuleBERT: Teaching soft rules to pre-trained language models. Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>multiPRover: Generating multiple proofs for improved interpretability in rule reasoning. Swarnadeep Saha, Prateek Yadav, Mohit Bansal, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021</p>
<p>RobustLR: A diagnostic benchmark for evaluating logical robustness of deductive reasoners. Soumya Sanyal, Zeyi Liao, Xiang Ren, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>FaiRR: Faithful and robust deductive reasoning over natural language. Soumya Sanyal, Harman Singh, Xiang Ren, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, Denny Zhou, CoRR, abs/2302.000932023</p>
<p>Auto-Prompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard De Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U Balis, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B Tenenbaum, Joshua S Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, D Kaustubh, Kevin Dhole, Kevin Gimpel, Kory Omondi, Kristen Mathewson, Ksenia Chiafullo, Kumar Shkaruta, Kyle Shridhar, Kyle Mcdonell, Laria Richardson, Leo Reynolds, Li Gao, Liam Zhang, Lianhui Dugan, Lidia Qin, Louis-Philippe Contreras-Ochando, Luca Morency, Lucas Moschella, Lucy Lam, Ludwig Noble, Luheng Schmidt, Luis He, Luke Oliveros Colón, Metz ; Maheen, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Maru, Jose Ramírez, Marie Quintana, Mario Tolkiehn, Martha Giulianelli, Martin Lewis, Matthew L Potthast, Matthias Leavitt, Mátyás Hagen, Medina Schubert, Melody Orduna Baitemirova, Melvin Arnaud, Michael A Mcelrath, Michael Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michał Strube, Michele Swędrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mo Suzgun, Mohit Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T Varma, Nanyun Peng, Nathan Chi, Nayeon Lee, Neta Gur-, Ari Krakover, ; Niveditha, S Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi ; Ryan Teehan, Rylan Yang, Sahib Singh, Saif M Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, R Samuel, Samuel S Bowman, Sanghyun Schoenholz, Sanjeev Han, Sarah A Kwatra, Sarik Rous, Sayan Ghazarian, Sean Ghosh, Sebastian Casey, Sebastian Bischoff, Sebastian Gehrmann, Sepideh Schuster, Shadi Sadeghi, Sharon Hamdan, Shashank Zhou, Sherry Srivastava, Shikhar Shi, Shima Singh, Asaadi, Shane Shixiang, Shubh Gu, Shubham Pachchigar, Shyam Toshniwal, Upadhyay, Shyamolima, Siamak Debnath, Simon Shakeri, Simone Thormeyer, Siva Melzi, Reddy, Priscilla Sneha, Soo-Hwan Makini, Spencer Lee, Sriharsha Torene, Stanislas Hatwar, Stefan Dehaene, Stefano Divic, Stella Ermon, Stephanie Biderman, Stephen Lin, Steven T Prasad, Stuart M Piantadosi, Summer Shieber, Svetlana Misherghi, Swaroop Kiritchenko, Mishra ; Xinran, Xinyi Zhao, Xudong Wu, Yadollah Shen, Yair Yaghoobzadeh, Yangqiu Lakretz, Yasaman Song, Yejin Bahri, Yichi Choi, Yiding Yang, Yifu Hao, Yonatan Chen, Yu Belinkov, Yufang Hou, Yuntao Hou, Zachary Bai, Zhuoye Seid, Zijian Zhao, Zijie J Wang, Zirui Wang, Ziyi Wang, Wu, Lütfi Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve. Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Omer Levy, Owain Evans, Pablo Antonio Moreno Casares; Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto; Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton ChangTe-Lin Wu2022Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu. Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong,. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational LinguisticsAugust 2021</p>
<p>Entailer: Answering questions with faithful and truthful chains of reasoning. Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. I Guyon, U Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc201730</p>
<p>Balancing training for multilingual neural machine translation. Xinyi Wang, Yulia Tsvetkov, Graham Neubig, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>On negative interference in multilingual models: Findings and a meta-learning treatment. Zirui Wang, Zachary C Lipton, Yulia Tsvetkov, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush2020</p>
<p>Do neural models learn systematicity of monotonicity inference in natural language?. Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Acquisition of phrase correspondences using natural deduction proofs. Hitomi Yanaka, Koji Mineshima, Pascual Martínez-Gómez, Daisuke Bekki, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational LinguisticsJune 20181</p>
<p>Generating natural language proofs with verifier-guided search. Kaiyu Yang, Jia Deng, Danqi Chen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Generating natural language proofs with verifier-guided search. Kaiyu Yang, Jia Deng, Danqi Chen, 2022</p>
<p>Logical reasoning over natural language as knowledge representation: A survey. Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria, 2023</p>
<p>Kformer: Knowledge injection in transformer feed-forward layers. Yunzhi Yao, Shaohan Huang, Li Dong, Furu Wei, Huajun Chen, Ningyu Zhang, 2022</p>
<p>Generate rather than retrieve: Large language models are strong context generators. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang, The Eleventh International Conference on Learning Representations, 2023. highlights RECKONING's strength in disentangling irrelevant information from useful knowledge. and ability that even powerful LLMs like GPT-3.5 lacks</p>            </div>
        </div>

    </div>
</body>
</html>
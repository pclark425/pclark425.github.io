<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5000 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5000</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5000</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-268857112</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.01869v1.pdf" target="_blank">Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs’ reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance , measured through shallow accuracy metrics, rather than a thorough investigation of the models’ reasoning behavior . This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models’ reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rather than on sophisticated reasoning abilities. Additionally, we identify the need for further research that delineates the key differences between human and LLM-based reasoning. Through this survey, we aim to shed light on the complex reasoning processes within LLMs.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5000.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5000.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Saparov&He2023_CoT_FOL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Saparov & He (2023) — Chain-of-thought rationales parsed to first-order logic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic assessment of chain-of-thought (CoT) rationales from GPT-3 iterations by parsing generated steps into first-order logic and measuring validity, atomicity and utility of each proof step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (various iterations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer-based language model family (GPT-3 series) evaluated via chain-of-thought prompting; survey cites Brown et al. (2020) for GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Deductive reasoning via chain-of-thought rationales parsed into first-order logic</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step deductive reasoning problems where the model is prompted to produce step-by-step rationales; each generated step is converted to first-order logic to assess logical validity, atomicity (single deduction per step) and utility (contribution toward final derivation).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Chain-of-thought prompting; parse each generated CoT step into first-order logic and evaluate step-level validity, atomicity, and utility.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Larger GPT-3 iterations produce more valid and atomic steps than smaller counterparts, but many steps have low utility; models frequently produce misleading intermediate steps even when final answer can be correct.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low utility of steps (steps that do not meaningfully contribute), misleading intermediate steps; autoregressive generation causes early mistakes to compound and reduce probability of later correct steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Larger GPT-3 variants outperform smaller ones in step validity and atomicity, but even larger models struggle with utility and recovery from mistaken steps.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Step-level analyses (validity/atomicity/utility) across model iterations show improvements in validity/atomicity with scale but persistent low utility; theoretical analysis links autoregressive decoding to compounding errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5000.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5000.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dziri2023_Einstein_ComputationGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dziri et al. (2023) — Computation-graph analysis of rationales (Einstein's puzzle)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parses model rationales into computation graphs (nodes = partial solutions) and finds multi-step reasoning often reduced to shortcut pattern matching across GPT-3, ChatGPT, and GPT-4 on Einstein's puzzle.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Faith and fate: Limits of transformers on compositionality</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, ChatGPT, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer-based models from OpenAI (GPT-3 family, ChatGPT, GPT-4) evaluated on a structured multi-step puzzle (Einstein's puzzle) by converting rationales to computation graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Einstein's puzzle (multi-step deductive puzzle) analyzed via computation graph representation of reasoning trace</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A multi-step logic puzzle requiring chaining of constraints; rationales are parsed into computation graphs where nodes are intermediate partial solutions and edges are functional mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Parse model rationales into computation graphs and analyze patterns (shortcut matching vs systematic multistep inference); inspect whether reasoning reduces to memorized patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Models sometimes produce correct final answers but analysis shows reliance on shortcut pattern matching rather than systematic multi-step reasoning; success often hinges on exposure to similar patterns during training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to generalize to novel/complex instances that deviate from training patterns; early-step mistakes compound; multi-step reasoning reduced to shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Across GPT-3, ChatGPT, GPT-4 similar shortcut behaviors observed; performance can be superficially high when training overlap exists but generalization is poor.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Computation-graph analyses reveal pattern-matching shortcuts; theoretical considerations highlight autoregressive compounding of errors across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5000.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5000.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chen2024_PremiseOrder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chen et al. (2024b) — Premise order sensitivity in propositional reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstrates that premise ordering significantly affects LLM behavior on propositional reasoning tasks: presenting premises in orders misaligned with ground-truth proofs impairs models including ChatGPT, GPT-4, PaLM 2-L and Gemini Pro.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Premise order matters in reasoning with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT, GPT-4, PaLM 2-L, Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned and foundation LLMs from OpenAI, Google DeepMind/PaLM 2 family and Google/DeepMind/Google-affiliated Gemini family evaluated on propositional reasoning with premise-order perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Propositional logic reasoning with perturbed premise order</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Propositional logic tasks where premises are logically equivalent but presented in an order that does not match the ground-truth proof sequence; tests sensitivity to surface ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Systematic perturbation of premise order while keeping logical content identical; measure impact on reasoning/rationales and final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Models show marked performance degradation when premises are presented in non-proof-aligned order despite logical equivalence; substantial difficulties across strong, instruction-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High sensitivity to irrelevant surface cues (premise order) causing failures despite unchanged logical relations; indicates reliance on training distribution patterns rather than abstract logical structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>All tested models (ChatGPT, GPT-4, PaLM 2-L, Gemini Pro) exhibited sensitivity; larger/instruction-tuned models not immune.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Perturbation experiments reveal that changing order (a purely surface-level modification) strongly impacts reasoning behavior, implicating training-distribution heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5000.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5000.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhang2023_RemoveStats_BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zhang et al. (2023) — Effect of removing statistical cues from training on BERT's logical generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Shows that eliminating certain statistical cues from pretraining data improves BERT's generalization on propositional logic tasks, suggesting over-reliance on superficial features impedes reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the paradox of learning to reason from data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer pre-trained for masked-language modeling (BERT family) analyzed for generalization in propositional logic after altering training cues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Propositional logic generalization tests</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks probing propositional logical entailment/generalization where models must apply logical rules beyond surface statistical patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Modify/eliminate statistical cues in training data and evaluate downstream logical generalization performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Removing specific statistical cues from pretraining data led to improved generalization on propositional logic tasks for BERT-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>When trained with unmodified corpora, models overfit to statistical patterns and fail to generalize; altering data helps but is not a full remedy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Demonstrates that data-driven heuristics can be a core limiter compared to models trained with reduced superficial cues.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations on pretraining data composition show direct impact of statistical cues on downstream logical generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5000.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5000.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pirozelli2023_RoBERTa_probe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pirozelli et al. (2023) — Layer probing of fine-tuned RoBERTa for deductive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Layer-wise probes of a fine-tuned RoBERTa-large show higher layers are pivotal for reasoning; fine-tuned models struggle to transfer deductive reasoning ability to unseen tasks when cross-probed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing logical reasoning capabilities of encoder-only transformer models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer model (RoBERTa-large) fine-tuned for deductive reasoning tasks and investigated via layer-wise probing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Deductive reasoning and transfer (cross-probing) across deductive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Deductive logic problems used for fine-tuning and cross-probing to assess transfer to unseen deductive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning on specific deductive datasets and layer-wise probing to determine where reasoning-relevant representations arise.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Fine-tuned RoBERTa-large acquires task-specific deductive capabilities but shows limited transfer when cross-probed on unseen deductive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor cross-task transfer; reasoning capabilities appear localized and brittle, requiring task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Higher layers more involved in reasoning than lower layers; transfer worse compared to in-distribution fine-tuned performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Layer probes indicate higher layers encode reasoning-relevant features; cross-task ablations highlight brittleness of learned deductive strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5000.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5000.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sanyal2022_RobustLR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sanyal et al. (2022) — RobustLR diagnostic benchmark for logical robustness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Introduces RobustLR, a diagnostic benchmark to evaluate logical robustness of deductive reasoners and demonstrates deficits of models (GPT-3, RoBERTa, T5) on negation and other deductive phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RobustLR: A diagnostic benchmark for evaluating logical robustness of deductive reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, RoBERTa, T5 (evaluated in RobustLR)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variety of encoder-only and autoregressive models evaluated on a diagnostic suite targeting logical robustness (deductive rules, negation, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>RobustLR diagnostic deductive reasoning tasks (logical robustness, negation comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Unit-test-like deductive reasoning problems designed to probe minimal logical competencies (e.g., correct handling of negation, deductive inference robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Diagnostic benchmark (robustness tests) that isolates logical constructs (negation, entailment, etc.) and measures model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Models (including GPT-3, RoBERTa, T5 variants) exhibit notable deficiencies, particularly in comprehending logical negation and robust deductive inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failing to correctly reason about negation and certain deductive patterns; brittle to minimal perturbations of logical structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Across model families both encoder-only and autoregressive models struggle; no class of models is immune.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Diagnostic tests reveal specific operator-level weaknesses (negation) and sensitivity to small structural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5000.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5000.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wan2024_MFTs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wan et al. (2024) — Minimum functionality tests for formal logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposes MFTs (logical unit tests) applied to a suite of models (GPT-3, ChatGPT, GPT-4, Bard/PaLM 2, Vicuna, Guanaco) and uncovers common difficulties in identifying logical fallacies and with specific logical laws (e.g., De Morgan).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A & b == b & a: Triggering logical reasoning failures in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, ChatGPT, GPT-4, Bard (PaLM 2), Vicuna, Guanaco</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture of closed-source (OpenAI, Google) and open-source instruction-tuned chat / foundation models evaluated with unit-test style logical probes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Minimum functionality tests (MFTs) covering formal reasoning scenarios and logical unit tests</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Small, focused logical tests that evaluate minimal competencies (fallacy detection, operator understanding, De Morgan's laws, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Design and run MFTs — logical unit tests — on model rationales and outputs; analyze pass/fail patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Common difficulty across models in identifying logical fallacies; GPT-4 specifically noted to struggle with De Morgan's Laws in the evaluated MFTs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Weaknesses in detecting fallacies and applying certain formal logical transformations (e.g., De Morgan); unit-test failures highlight operator-level gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Failures observed across closed- and open-source models, including stronger models like GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>MFT analyses isolate specific logical operators and fallacies that consistently cause failures across models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5000.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5000.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eisape2024_Syllogisms</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eisape et al. (2024) — Systematic comparison of syllogistic reasoning in humans and LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Compares human and LLM syllogistic reasoning (PaLM 2 series among models); finds LLMs show human-like biases and fallacies, larger models more deliberate but rarely output 'nothing follows'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A systematic comparison of syllogistic reasoning in humans and language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2 series (and other LLMs compared)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 2 family (Google) and other LLMs evaluated on classical syllogistic reasoning tasks alongside human participants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Syllogistic reasoning (standard syllogisms and classical reasoning problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems from syllogistic reasoning paradigms where models/humans must infer valid conclusions from quantified premises; includes belief-consistent/inconsistent variants.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Compare model responses to human judgments on syllogisms, analyze patterns of fallacies and biases (e.g., belief bias, ordering effects).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LLMs exhibit susceptibility to common human logical fallacies and cognitive biases; larger models show more deliberate reasoning and reduced sensitivity but still biased; models rarely respond with 'nothing follows' even when correct.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Belief bias (semantic content influences logical judgments), ordering effects, conversion and atmosphere errors; LLMs differ from humans in some response patterns (e.g., underreporting 'no valid conclusion').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>LLM behavior parallels many human error patterns but differs in certain normative responses; larger models less error-prone but still imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Analyses across model sizes reveal inverse scaling-like patterns for some errors and decreased sensitivity to certain biases with scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5000.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5000.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MondorfPlank2024_PropositionalManual</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mondorf & Plank (2024) — Manual evaluation of open-access LLM rationales on propositional logic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual, inferential-strategy-level analysis of open-access LLMs on propositional logic problems showing human-like inferential strategies but persistent difficulties with negation and logical fallacies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comparing inferential strategies of humans and large language models in deductive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Open-access LLMs (unspecified set evaluated by Mondorf & Plank)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A set of open-access language models evaluated through manual annotation of their verbalized rationales on propositional logic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Propositional logic reasoning with manual rationale evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Propositional logic problems where models provide step-by-step rationales which are manually annotated to reveal inferential strategies and step-level correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Qualitative manual inspection of generated rationales to identify inferential strategies, operator handling (e.g., negation), and fallacy-susceptibility.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Open-access LLMs often adopt inferential strategies similar to humans; however, they display notable weaknesses with logical negation and are vulnerable to typical logical fallacies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Difficulties handling logical negations; vulnerability to fallacies and ordering/contextual cues; potential mismatch between verbalized rationale and final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Observed parallels to human reasoning errors; open-access models show similar failure modes as closed models in key operator handling.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Manual analyses highlight specific operator-level problems (negation) and demonstrate that final answers may not align with the stated rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5000.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5000.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dutta2024_LLaMA2_7B_mechanistic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dutta et al. (2024) — Mechanistic analysis of LLaMA 2-7B under chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mechanistic interpretability study showing a functional rift in LLaMA 2-7B's middle layers where representations shift from pre-training priors to in-context priors during CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards a mechanistic interpretation of multistep reasoning capabilities of language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open foundation language model (LLaMA 2 family) with 7B parameters analyzed via mechanistic interventions (activation patching, layer inspection) under chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Chain-of-thought prompted multi-step reasoning (mechanistic probing rather than a specific named benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multistep reasoning induced via chain-of-thought prompts; analysis focused on how internal representations evolve layer-wise during reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Layer-wise mechanistic analysis including activation patching and probing to observe shifts between pre-training priors and in-context priors across middle layers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not a conventional accuracy report — analysis finds a representational shift in mid-layers: early middle layers biased to pre-training priors, latter middle layers abruptly shift to in-context priors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Findings indicate non-uniform internal processing pathways and potential fragility in how in-context information is integrated; results differ from other mechanistic studies, suggesting model- and task-specific behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Contrasts with other studies that report smooth step-wise processing; suggests multiple concurrent pathways rather than single linear reasoning circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Activation patching and layer probes reveal a functional rift in middle layers; interventions show where in-context priors are introduced and how they alter downstream behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought <em>(Rating: 2)</em></li>
                <li>Faith and fate: Limits of transformers on compositionality <em>(Rating: 2)</em></li>
                <li>Premise order matters in reasoning with large language models <em>(Rating: 2)</em></li>
                <li>On the paradox of learning to reason from data <em>(Rating: 2)</em></li>
                <li>Assessing logical reasoning capabilities of encoder-only transformer models <em>(Rating: 2)</em></li>
                <li>RobustLR: A diagnostic benchmark for evaluating logical robustness of deductive reasoners <em>(Rating: 2)</em></li>
                <li>A & b == b & a: Triggering logical reasoning failures in large language models <em>(Rating: 2)</em></li>
                <li>A systematic comparison of syllogistic reasoning in humans and language models <em>(Rating: 2)</em></li>
                <li>Comparing inferential strategies of humans and large language models in deductive reasoning <em>(Rating: 2)</em></li>
                <li>Towards a mechanistic interpretation of multistep reasoning capabilities of language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5000",
    "paper_id": "paper-268857112",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "Saparov&He2023_CoT_FOL",
            "name_full": "Saparov & He (2023) — Chain-of-thought rationales parsed to first-order logic",
            "brief_description": "Systematic assessment of chain-of-thought (CoT) rationales from GPT-3 iterations by parsing generated steps into first-order logic and measuring validity, atomicity and utility of each proof step.",
            "citation_title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (various iterations)",
            "model_description": "Autoregressive transformer-based language model family (GPT-3 series) evaluated via chain-of-thought prompting; survey cites Brown et al. (2020) for GPT-3.",
            "model_size": null,
            "logical_reasoning_task": "Deductive reasoning via chain-of-thought rationales parsed into first-order logic",
            "task_description": "Multi-step deductive reasoning problems where the model is prompted to produce step-by-step rationales; each generated step is converted to first-order logic to assess logical validity, atomicity (single deduction per step) and utility (contribution toward final derivation).",
            "method_or_approach": "Chain-of-thought prompting; parse each generated CoT step into first-order logic and evaluate step-level validity, atomicity, and utility.",
            "performance": "Larger GPT-3 iterations produce more valid and atomic steps than smaller counterparts, but many steps have low utility; models frequently produce misleading intermediate steps even when final answer can be correct.",
            "limitations_or_failure_cases": "Low utility of steps (steps that do not meaningfully contribute), misleading intermediate steps; autoregressive generation causes early mistakes to compound and reduce probability of later correct steps.",
            "comparison": "Larger GPT-3 variants outperform smaller ones in step validity and atomicity, but even larger models struggle with utility and recovery from mistaken steps.",
            "ablation_or_analysis_results": "Step-level analyses (validity/atomicity/utility) across model iterations show improvements in validity/atomicity with scale but persistent low utility; theoretical analysis links autoregressive decoding to compounding errors.",
            "uuid": "e5000.0",
            "source_info": {
                "paper_title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Dziri2023_Einstein_ComputationGraph",
            "name_full": "Dziri et al. (2023) — Computation-graph analysis of rationales (Einstein's puzzle)",
            "brief_description": "Parses model rationales into computation graphs (nodes = partial solutions) and finds multi-step reasoning often reduced to shortcut pattern matching across GPT-3, ChatGPT, and GPT-4 on Einstein's puzzle.",
            "citation_title": "Faith and fate: Limits of transformers on compositionality",
            "mention_or_use": "mention",
            "model_name": "GPT-3, ChatGPT, GPT-4",
            "model_description": "Autoregressive transformer-based models from OpenAI (GPT-3 family, ChatGPT, GPT-4) evaluated on a structured multi-step puzzle (Einstein's puzzle) by converting rationales to computation graphs.",
            "model_size": null,
            "logical_reasoning_task": "Einstein's puzzle (multi-step deductive puzzle) analyzed via computation graph representation of reasoning trace",
            "task_description": "A multi-step logic puzzle requiring chaining of constraints; rationales are parsed into computation graphs where nodes are intermediate partial solutions and edges are functional mappings.",
            "method_or_approach": "Parse model rationales into computation graphs and analyze patterns (shortcut matching vs systematic multistep inference); inspect whether reasoning reduces to memorized patterns.",
            "performance": "Models sometimes produce correct final answers but analysis shows reliance on shortcut pattern matching rather than systematic multi-step reasoning; success often hinges on exposure to similar patterns during training.",
            "limitations_or_failure_cases": "Fails to generalize to novel/complex instances that deviate from training patterns; early-step mistakes compound; multi-step reasoning reduced to shortcuts.",
            "comparison": "Across GPT-3, ChatGPT, GPT-4 similar shortcut behaviors observed; performance can be superficially high when training overlap exists but generalization is poor.",
            "ablation_or_analysis_results": "Computation-graph analyses reveal pattern-matching shortcuts; theoretical considerations highlight autoregressive compounding of errors across steps.",
            "uuid": "e5000.1",
            "source_info": {
                "paper_title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Chen2024_PremiseOrder",
            "name_full": "Chen et al. (2024b) — Premise order sensitivity in propositional reasoning",
            "brief_description": "Demonstrates that premise ordering significantly affects LLM behavior on propositional reasoning tasks: presenting premises in orders misaligned with ground-truth proofs impairs models including ChatGPT, GPT-4, PaLM 2-L and Gemini Pro.",
            "citation_title": "Premise order matters in reasoning with large language models",
            "mention_or_use": "mention",
            "model_name": "ChatGPT, GPT-4, PaLM 2-L, Gemini Pro",
            "model_description": "Instruction-tuned and foundation LLMs from OpenAI, Google DeepMind/PaLM 2 family and Google/DeepMind/Google-affiliated Gemini family evaluated on propositional reasoning with premise-order perturbations.",
            "model_size": null,
            "logical_reasoning_task": "Propositional logic reasoning with perturbed premise order",
            "task_description": "Propositional logic tasks where premises are logically equivalent but presented in an order that does not match the ground-truth proof sequence; tests sensitivity to surface ordering.",
            "method_or_approach": "Systematic perturbation of premise order while keeping logical content identical; measure impact on reasoning/rationales and final answers.",
            "performance": "Models show marked performance degradation when premises are presented in non-proof-aligned order despite logical equivalence; substantial difficulties across strong, instruction-tuned models.",
            "limitations_or_failure_cases": "High sensitivity to irrelevant surface cues (premise order) causing failures despite unchanged logical relations; indicates reliance on training distribution patterns rather than abstract logical structure.",
            "comparison": "All tested models (ChatGPT, GPT-4, PaLM 2-L, Gemini Pro) exhibited sensitivity; larger/instruction-tuned models not immune.",
            "ablation_or_analysis_results": "Perturbation experiments reveal that changing order (a purely surface-level modification) strongly impacts reasoning behavior, implicating training-distribution heuristics.",
            "uuid": "e5000.2",
            "source_info": {
                "paper_title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Zhang2023_RemoveStats_BERT",
            "name_full": "Zhang et al. (2023) — Effect of removing statistical cues from training on BERT's logical generalization",
            "brief_description": "Shows that eliminating certain statistical cues from pretraining data improves BERT's generalization on propositional logic tasks, suggesting over-reliance on superficial features impedes reasoning.",
            "citation_title": "On the paradox of learning to reason from data",
            "mention_or_use": "mention",
            "model_name": "BERT",
            "model_description": "Encoder-only transformer pre-trained for masked-language modeling (BERT family) analyzed for generalization in propositional logic after altering training cues.",
            "model_size": null,
            "logical_reasoning_task": "Propositional logic generalization tests",
            "task_description": "Benchmarks probing propositional logical entailment/generalization where models must apply logical rules beyond surface statistical patterns.",
            "method_or_approach": "Modify/eliminate statistical cues in training data and evaluate downstream logical generalization performance.",
            "performance": "Removing specific statistical cues from pretraining data led to improved generalization on propositional logic tasks for BERT-based models.",
            "limitations_or_failure_cases": "When trained with unmodified corpora, models overfit to statistical patterns and fail to generalize; altering data helps but is not a full remedy.",
            "comparison": "Demonstrates that data-driven heuristics can be a core limiter compared to models trained with reduced superficial cues.",
            "ablation_or_analysis_results": "Ablations on pretraining data composition show direct impact of statistical cues on downstream logical generalization.",
            "uuid": "e5000.3",
            "source_info": {
                "paper_title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Pirozelli2023_RoBERTa_probe",
            "name_full": "Pirozelli et al. (2023) — Layer probing of fine-tuned RoBERTa for deductive reasoning",
            "brief_description": "Layer-wise probes of a fine-tuned RoBERTa-large show higher layers are pivotal for reasoning; fine-tuned models struggle to transfer deductive reasoning ability to unseen tasks when cross-probed.",
            "citation_title": "Assessing logical reasoning capabilities of encoder-only transformer models",
            "mention_or_use": "mention",
            "model_name": "RoBERTa-large (fine-tuned)",
            "model_description": "Encoder-only transformer model (RoBERTa-large) fine-tuned for deductive reasoning tasks and investigated via layer-wise probing.",
            "model_size": null,
            "logical_reasoning_task": "Deductive reasoning and transfer (cross-probing) across deductive tasks",
            "task_description": "Deductive logic problems used for fine-tuning and cross-probing to assess transfer to unseen deductive tasks.",
            "method_or_approach": "Fine-tuning on specific deductive datasets and layer-wise probing to determine where reasoning-relevant representations arise.",
            "performance": "Fine-tuned RoBERTa-large acquires task-specific deductive capabilities but shows limited transfer when cross-probed on unseen deductive tasks.",
            "limitations_or_failure_cases": "Poor cross-task transfer; reasoning capabilities appear localized and brittle, requiring task-specific fine-tuning.",
            "comparison": "Higher layers more involved in reasoning than lower layers; transfer worse compared to in-distribution fine-tuned performance.",
            "ablation_or_analysis_results": "Layer probes indicate higher layers encode reasoning-relevant features; cross-task ablations highlight brittleness of learned deductive strategies.",
            "uuid": "e5000.4",
            "source_info": {
                "paper_title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Sanyal2022_RobustLR",
            "name_full": "Sanyal et al. (2022) — RobustLR diagnostic benchmark for logical robustness",
            "brief_description": "Introduces RobustLR, a diagnostic benchmark to evaluate logical robustness of deductive reasoners and demonstrates deficits of models (GPT-3, RoBERTa, T5) on negation and other deductive phenomena.",
            "citation_title": "RobustLR: A diagnostic benchmark for evaluating logical robustness of deductive reasoners",
            "mention_or_use": "mention",
            "model_name": "GPT-3, RoBERTa, T5 (evaluated in RobustLR)",
            "model_description": "Variety of encoder-only and autoregressive models evaluated on a diagnostic suite targeting logical robustness (deductive rules, negation, etc.).",
            "model_size": null,
            "logical_reasoning_task": "RobustLR diagnostic deductive reasoning tasks (logical robustness, negation comprehension)",
            "task_description": "Unit-test-like deductive reasoning problems designed to probe minimal logical competencies (e.g., correct handling of negation, deductive inference robustness).",
            "method_or_approach": "Diagnostic benchmark (robustness tests) that isolates logical constructs (negation, entailment, etc.) and measures model behavior.",
            "performance": "Models (including GPT-3, RoBERTa, T5 variants) exhibit notable deficiencies, particularly in comprehending logical negation and robust deductive inference.",
            "limitations_or_failure_cases": "Failing to correctly reason about negation and certain deductive patterns; brittle to minimal perturbations of logical structure.",
            "comparison": "Across model families both encoder-only and autoregressive models struggle; no class of models is immune.",
            "ablation_or_analysis_results": "Diagnostic tests reveal specific operator-level weaknesses (negation) and sensitivity to small structural changes.",
            "uuid": "e5000.5",
            "source_info": {
                "paper_title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Wan2024_MFTs",
            "name_full": "Wan et al. (2024) — Minimum functionality tests for formal logical reasoning",
            "brief_description": "Proposes MFTs (logical unit tests) applied to a suite of models (GPT-3, ChatGPT, GPT-4, Bard/PaLM 2, Vicuna, Guanaco) and uncovers common difficulties in identifying logical fallacies and with specific logical laws (e.g., De Morgan).",
            "citation_title": "A & b == b & a: Triggering logical reasoning failures in large language models",
            "mention_or_use": "mention",
            "model_name": "GPT-3, ChatGPT, GPT-4, Bard (PaLM 2), Vicuna, Guanaco",
            "model_description": "Mixture of closed-source (OpenAI, Google) and open-source instruction-tuned chat / foundation models evaluated with unit-test style logical probes.",
            "model_size": null,
            "logical_reasoning_task": "Minimum functionality tests (MFTs) covering formal reasoning scenarios and logical unit tests",
            "task_description": "Small, focused logical tests that evaluate minimal competencies (fallacy detection, operator understanding, De Morgan's laws, etc.).",
            "method_or_approach": "Design and run MFTs — logical unit tests — on model rationales and outputs; analyze pass/fail patterns.",
            "performance": "Common difficulty across models in identifying logical fallacies; GPT-4 specifically noted to struggle with De Morgan's Laws in the evaluated MFTs.",
            "limitations_or_failure_cases": "Weaknesses in detecting fallacies and applying certain formal logical transformations (e.g., De Morgan); unit-test failures highlight operator-level gaps.",
            "comparison": "Failures observed across closed- and open-source models, including stronger models like GPT-4.",
            "ablation_or_analysis_results": "MFT analyses isolate specific logical operators and fallacies that consistently cause failures across models.",
            "uuid": "e5000.6",
            "source_info": {
                "paper_title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Eisape2024_Syllogisms",
            "name_full": "Eisape et al. (2024) — Systematic comparison of syllogistic reasoning in humans and LLMs",
            "brief_description": "Compares human and LLM syllogistic reasoning (PaLM 2 series among models); finds LLMs show human-like biases and fallacies, larger models more deliberate but rarely output 'nothing follows'.",
            "citation_title": "A systematic comparison of syllogistic reasoning in humans and language models",
            "mention_or_use": "mention",
            "model_name": "PaLM 2 series (and other LLMs compared)",
            "model_description": "PaLM 2 family (Google) and other LLMs evaluated on classical syllogistic reasoning tasks alongside human participants.",
            "model_size": null,
            "logical_reasoning_task": "Syllogistic reasoning (standard syllogisms and classical reasoning problems)",
            "task_description": "Problems from syllogistic reasoning paradigms where models/humans must infer valid conclusions from quantified premises; includes belief-consistent/inconsistent variants.",
            "method_or_approach": "Compare model responses to human judgments on syllogisms, analyze patterns of fallacies and biases (e.g., belief bias, ordering effects).",
            "performance": "LLMs exhibit susceptibility to common human logical fallacies and cognitive biases; larger models show more deliberate reasoning and reduced sensitivity but still biased; models rarely respond with 'nothing follows' even when correct.",
            "limitations_or_failure_cases": "Belief bias (semantic content influences logical judgments), ordering effects, conversion and atmosphere errors; LLMs differ from humans in some response patterns (e.g., underreporting 'no valid conclusion').",
            "comparison": "LLM behavior parallels many human error patterns but differs in certain normative responses; larger models less error-prone but still imperfect.",
            "ablation_or_analysis_results": "Analyses across model sizes reveal inverse scaling-like patterns for some errors and decreased sensitivity to certain biases with scale.",
            "uuid": "e5000.7",
            "source_info": {
                "paper_title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MondorfPlank2024_PropositionalManual",
            "name_full": "Mondorf & Plank (2024) — Manual evaluation of open-access LLM rationales on propositional logic",
            "brief_description": "Manual, inferential-strategy-level analysis of open-access LLMs on propositional logic problems showing human-like inferential strategies but persistent difficulties with negation and logical fallacies.",
            "citation_title": "Comparing inferential strategies of humans and large language models in deductive reasoning",
            "mention_or_use": "mention",
            "model_name": "Open-access LLMs (unspecified set evaluated by Mondorf & Plank)",
            "model_description": "A set of open-access language models evaluated through manual annotation of their verbalized rationales on propositional logic tasks.",
            "model_size": null,
            "logical_reasoning_task": "Propositional logic reasoning with manual rationale evaluation",
            "task_description": "Propositional logic problems where models provide step-by-step rationales which are manually annotated to reveal inferential strategies and step-level correctness.",
            "method_or_approach": "Qualitative manual inspection of generated rationales to identify inferential strategies, operator handling (e.g., negation), and fallacy-susceptibility.",
            "performance": "Open-access LLMs often adopt inferential strategies similar to humans; however, they display notable weaknesses with logical negation and are vulnerable to typical logical fallacies.",
            "limitations_or_failure_cases": "Difficulties handling logical negations; vulnerability to fallacies and ordering/contextual cues; potential mismatch between verbalized rationale and final answer.",
            "comparison": "Observed parallels to human reasoning errors; open-access models show similar failure modes as closed models in key operator handling.",
            "ablation_or_analysis_results": "Manual analyses highlight specific operator-level problems (negation) and demonstrate that final answers may not align with the stated rationale.",
            "uuid": "e5000.8",
            "source_info": {
                "paper_title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Dutta2024_LLaMA2_7B_mechanistic",
            "name_full": "Dutta et al. (2024) — Mechanistic analysis of LLaMA 2-7B under chain-of-thought prompting",
            "brief_description": "Mechanistic interpretability study showing a functional rift in LLaMA 2-7B's middle layers where representations shift from pre-training priors to in-context priors during CoT prompting.",
            "citation_title": "Towards a mechanistic interpretation of multistep reasoning capabilities of language models",
            "mention_or_use": "mention",
            "model_name": "LLaMA 2-7B",
            "model_description": "Open foundation language model (LLaMA 2 family) with 7B parameters analyzed via mechanistic interventions (activation patching, layer inspection) under chain-of-thought prompting.",
            "model_size": "7B",
            "logical_reasoning_task": "Chain-of-thought prompted multi-step reasoning (mechanistic probing rather than a specific named benchmark)",
            "task_description": "Multistep reasoning induced via chain-of-thought prompts; analysis focused on how internal representations evolve layer-wise during reasoning.",
            "method_or_approach": "Layer-wise mechanistic analysis including activation patching and probing to observe shifts between pre-training priors and in-context priors across middle layers.",
            "performance": "Not a conventional accuracy report — analysis finds a representational shift in mid-layers: early middle layers biased to pre-training priors, latter middle layers abruptly shift to in-context priors.",
            "limitations_or_failure_cases": "Findings indicate non-uniform internal processing pathways and potential fragility in how in-context information is integrated; results differ from other mechanistic studies, suggesting model- and task-specific behaviors.",
            "comparison": "Contrasts with other studies that report smooth step-wise processing; suggests multiple concurrent pathways rather than single linear reasoning circuits.",
            "ablation_or_analysis_results": "Activation patching and layer probes reveal a functional rift in middle layers; interventions show where in-context priors are introduced and how they alter downstream behavior.",
            "uuid": "e5000.9",
            "source_info": {
                "paper_title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "rating": 2,
            "sanitized_title": "language_models_are_greedy_reasoners_a_systematic_formal_analysis_of_chainofthought"
        },
        {
            "paper_title": "Faith and fate: Limits of transformers on compositionality",
            "rating": 2,
            "sanitized_title": "faith_and_fate_limits_of_transformers_on_compositionality"
        },
        {
            "paper_title": "Premise order matters in reasoning with large language models",
            "rating": 2,
            "sanitized_title": "premise_order_matters_in_reasoning_with_large_language_models"
        },
        {
            "paper_title": "On the paradox of learning to reason from data",
            "rating": 2,
            "sanitized_title": "on_the_paradox_of_learning_to_reason_from_data"
        },
        {
            "paper_title": "Assessing logical reasoning capabilities of encoder-only transformer models",
            "rating": 2,
            "sanitized_title": "assessing_logical_reasoning_capabilities_of_encoderonly_transformer_models"
        },
        {
            "paper_title": "RobustLR: A diagnostic benchmark for evaluating logical robustness of deductive reasoners",
            "rating": 2,
            "sanitized_title": "robustlr_a_diagnostic_benchmark_for_evaluating_logical_robustness_of_deductive_reasoners"
        },
        {
            "paper_title": "A & b == b & a: Triggering logical reasoning failures in large language models",
            "rating": 2,
            "sanitized_title": "a_b_b_a_triggering_logical_reasoning_failures_in_large_language_models"
        },
        {
            "paper_title": "A systematic comparison of syllogistic reasoning in humans and language models",
            "rating": 2,
            "sanitized_title": "a_systematic_comparison_of_syllogistic_reasoning_in_humans_and_language_models"
        },
        {
            "paper_title": "Comparing inferential strategies of humans and large language models in deductive reasoning",
            "rating": 2,
            "sanitized_title": "comparing_inferential_strategies_of_humans_and_large_language_models_in_deductive_reasoning"
        },
        {
            "paper_title": "Towards a mechanistic interpretation of multistep reasoning capabilities of language models",
            "rating": 2,
            "sanitized_title": "towards_a_mechanistic_interpretation_of_multistep_reasoning_capabilities_of_language_models"
        }
    ],
    "cost": 0.021572749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -A Survey
6 Aug 2024</p>
<p>Philipp Mondorf p.mondorf@lmu.de 
Barbara Plank Mainlp </p>
<p>Center for Information and Language Processing
LMU Munich
Germany</p>
<p>Munich Center for Machine Learning (MCML)
MunichGermany</p>
<p>Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -A Survey
6 Aug 2024F53E87AA20B68C81120BA17BF941CA43arXiv:2404.01869v2[cs.CL]Published as a conference paper at COLM 2024 Reasoning Tasks Core Reasoning Tasks Integrated Reasoning Tasks Logical Reasoning Task Mathematical Reasoning Task Causal Reasoning Task Commonsense Reasoning Task Scientific Reasoning Task Social Reasoning Task Logical Reasoning Ability Mathematical Reasoning Ability
Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans.However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain.This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior.This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes.Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses.Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rather than on sophisticated reasoning abilities.Additionally, we identify the need for further research that delineates the key differences between human and LLM-based reasoning.Through this survey, we aim to shed light on the complex reasoning processes within LLMs.</p>
<p>Introduction</p>
<p>"These models are castles in the air.They have no foundations whatsoever."</p>
<p>-Jitendra Malik (2021) Reasoning is an integral aspect of human intelligence and deliberate, rational thought (Holyoak &amp; Morrison, 2005).It allows individuals to draw conclusions from available information and move beyond their current knowledge (Lohman &amp; Lakin, 2011).As such, reasoning plays a fundamental role in problem-solving and decision-making, and has been a long-standing goal within the field of artificial intelligence (Robinson &amp; Voronkov, 2001).</p>
<p>In recent years, large language models have demonstrated remarkable performance on tasks that require reasoning (Bubeck et al., 2023;Wei et al., 2022;Kojima et al., 2022).This has sparked a vigorous debate about the extent to which these models possess reasoning abilities similar to humans (Mitchell &amp; Krakauer, 2023;Mitchell, 2023;Borji, 2023).While proponents argue that reasoning capabilities emerge with scale, referring to LLMs as foundation models (Bommasani et al., 2021;Kaplan et al., 2020), skeptics contend that the models' performance primarily reflects their capacity to memorize the vast amount of data they have been trained on (Wu et al., 2024;Dziri et al., 2023;Razeghi et al., 2022;Zhang et al., 2023).Thus, the question arises: are these models simply "castles in the air" with "no foundations whatsoever," as Malik (2021) once stated, or do they possess genuine reasoning capacities?One of the major challenges in this debate is the immense size, complexity, and closed-source nature of popular LLMs and their underlying training data.Moreover, the focus often lies on the models' performance on downstream reasoning tasks (Fu et al., 2023;Liu et al., 2023), overshadowing in-depth analyses of their reasoning behavior.In this study, we seek to shed light on the ongoing debate by providing a comprehensive overview of research that goes Published as a conference paper at COLM 2024 beyond task accuracy, offering more nuanced insights into the models' reasoning processes.Specifically, we address the following research questions:</p>
<p>RQ1: How do current LLMs behave across diverse reasoning tasks?RQ2: What are the prevalent evaluation methods employed to assess the reasoning behavior of large language models?</p>
<p>Other Surveys.While various surveys on reasoning within large language models have emerged in recent years (Huang &amp; Chang, 2023;Yu et al., 2024;Sun et al., 2024;Yang et al., 2023;Qiao et al., 2023;Chu et al., 2023;Mialon et al., 2023;Liu et al., 2024;Ahn et al., 2024), these studies predominantly focus on techniques that seek to enhance or benchmark the performance of LLMs on downstream reasoning tasks.However, there seems to exist no review of work yet that assesses the reasoning behavior of LLMs, rather than their final task performance.Hence, the contributions of this survey are as follows: (i) To the best of our knowledge, we present the first comprehensive review of literature that evaluates LLMbased reasoning beyond task accuracy, offering deeper insights into the models' reasoning behavior, and (ii) we suggest a taxonomy that categorizes prevalent evaluation methods designed to analyze the reasoning dynamics of LLMs.</p>
<p>Terminology</p>
<p>We begin this survey with an introduction to fundamental concepts and terminologies pertinent to reasoning in both humans and LLMs.The study of reasoning has captivated scholarly interest for centuries, with insights from diverse disciplines such as philosophy, logic, psychology, neuroscience and artificial intelligence (Holyoak &amp; Morrison, 2005).Over the years, various definitions of reasoning have been proposed, with each discipline offering its unique perspective.In this survey, we define reasoning in the broadest sense (Leighton, 2003;Holyoak &amp; Morrison, 2005;Byrne et al., 1993), and direct the interested reader to the work by Yu et al. (2024) and Sun et al. (2024) for more domain-specific definitions.Definition 2.1 (Reasoning).The process of drawing conclusions based on available information (usually a set of premises).</p>
<p>It is imperative to note that reasoning is a fundamentally process-oriented activity, rather than a singular endpoint (Leighton, 2003;Johnson-Laird, 2006).Although this process often remains hidden in humans, manifesting predominantly through the final conclusions inferred, cognitive psychology leverages methodologies like "think aloud" protocols to unveil the cognitive mechanisms underpinning reasoning (Wolcott &amp; Lobczowski, 2021;Van der Henst et al., 2002;Rips, 1989).Similarly, to understand the reasoning capabilities of large language models, it is crucial to consider not merely the end result, but the reasoning process itself.Thus, we differentiate between reasoning behavior and reasoning performance.Drawing from behavioral psychology, which views behavior as an organism's response to stimuli (American Psychological Association, n.d.), we define reasoning behavior as follows: Definition 2.2 (Reasoning Behavior).The system's computed response to a reasoning task (the stimulus), particularly its actions, expressions and underlying mechanisms exhibited during the reasoning process.</p>
<p>Our working definition highlights the procedural aspects of reasoning, rather than its final outcome.Understanding a model's reasoning behavior involves analyzing how it arrives at its conclusions.Conversely, reasoning performance is outcome-oriented.It is typically measured in terms of accuracy or the efficiency with which relevant conclusions are drawn (Barredo Arrieta et al., 2020).While performance can be helpful in evaluating a system's capacities to tackle specific reasoning tasks, an analysis of reasoning behavior can yield deeper insights into the process itself, thereby offering a more comprehensive understanding.</p>
<p>A Categorization of Reasoning Tasks</p>
<p>Analogous to the assessment of human reasoning within the field of cognitive psychology (WOOD, 1969;Byrne et al., 1993;Dewey, 2022), the evaluation of reasoning capabilities in large language models predominantly occurs through their engagement with designated reasoning tasks (Sun et al., 2024).These tasks are designed to elicit the system's capability of drawing conclusions relevant to the problem at hand.In this survey, we distinguish between core and integrated reasoning tasks.Core reasoning tasks are designed to assess fundamental reasoning skills in an isolated manner.They typically aim to test a single type of reasoning, such as logical, mathematical or causal reasoning.Examples of such tasks may include syllogisms, basic arithmetic problems or structured causal-graph predictions.Conversely, integrated reasoning tasks require the concurrent use of various reasoning types, thereby assessing a combination of fundamental reasoning skills.Examples are commonsense or scientific reasoning tasks.Such problems often reflect the complex cognitive challenges humans encounter in everyday life and professional settings.A schematic representation distinguishing these task categories is provided in Figure 1.</p>
<p>For the purpose of this survey, our focus is confined to examining the behaviors of LLMs in the context of core reasoning tasks, specifically logical, mathematical, and causal reasoning tasks.We leave a review of the models' reasoning behaviors within the context of integrated reasoning tasks to future work.</p>
<p>Reasoning Behavior of LLMs</p>
<p>This section reviews studies that extend beyond mere task accuracy, focusing instead on evaluating the reasoning behavior of large language models.1 Through this review, we intend to address RQ1 and shed light on how these models currently behave across three core reasoning tasks: logical reasoning (Section 3.1), mathematical reasoning (Section 3.2), and causal reasoning (Section 3.3).</p>
<p>Behavior in Logical Reasoning Tasks</p>
<p>The study of logical reasoning evolves around the question of how individuals infer valid conclusions from a set of given premises within a structured framework of logical rules and principles (Holyoak &amp; Morrison, 2005).Based on how conclusions are inferred, logical reasoning can be classified into three types: deductive, inductive, and abductive reasoning (Johnson-Laird, 2008;Flach &amp; Kakas, 2000).While in deductive reasoning, conclusions necessarily follow from the premises' truth, inductive and abductive reasoning are considered defeasible, i.e. conclusions are at most probable, but never necessary (Koons, 2022).Inductive reasoning is concerned with deriving general conclusions from specific instances, whereas abductive reasoning entails formulating plausible hypotheses that explain the data observed.</p>
<p>For additional details on the distinction between the three types of logical reasoning, we refer to prior surveys on reasoning performance within LLMs, such as the ones by Yu et al. (2024) or Sun et al. (2024).</p>
<p>Behavior in Deductive Reasoning Tasks</p>
<p>Extensive research has been dedicated to examining the reasoning behavior of LLMs in the context of deductive reasoning tasks.Various studies analyze the validity and consistency of logical proofs generated by LLMs within deductive reasoning problems.For instance, Saparov &amp; He (2023) conduct a systematic assessment of the rationales produced by various GPT-3 iterations (Brown et al., 2020), through chain-of-thought (CoT) prompting (Wei et al., 2022).By parsing each step of the generated reasoning trace into first-order-logic, its validity, atomicity, and utility2 is measured.Findings indicate that, in comparison to their smaller counterparts, larger models are more adept at generating both valid and atomic steps.However, the utility of these steps is often low, resulting in misleading steps from which the models struggle to recover.In a similar vein, Dziri et al. (2023) study rationales of LLMs by parsing them into computation graphs, where each node represents a partial solution of the multi-step reasoning process.Evaluations on Einstein's puzzle (Vassberg &amp; Vassberg, 2009) indicate that GPT-3, ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI et al., 2024) solve the task by reducing multi-step reasoning into shortcut pattern matching.This shortcut behavior can yield correct answers when the model has been exposed to similar patterns during training, but falls short in generalizing to novel or more complex instances.Furthermore, theoretical findings suggest that due to the models' autoregressive nature, mistakes in early stages of the reasoning process can lead to compounding errors that exponentially diminish the likelihood of arriving at the correct solution in subsequent steps.</p>
<p>Further studies highlight that LLMs tend to rely on superficial statistical features, rather than engaging in systematic reasoning.Chen et al. (2024b) illustrate that the premise order markedly influences the LLMs' behavior in propositional reasoning tasks.Specifically, when premises are presented in an order that does not align with the ground-truth proof, models such as ChatGPT, GPT-4, PaLM 2-L (Anil et al., 2023) and Gemini Pro (Team et al., 2023) encounter significant difficulties within their reasoning, even though such an ordering does not change the underlying logic.Zhang et al. (2023) show that an over-reliance on statistical features in the training data can hinder a model's reasoning and generalization capacity.By eliminating certain statistical cues from its training dataset, BERT (Devlin et al., 2019) demonstrates enhanced generalization capabilities in propositional logic.Similarly, Pirozelli et al. (2023) indicate that various fine-tuned language models show difficulties in transferring their logical reasoning ability when cross-probed on unseen deductive reasoning tasks.</p>
<p>Additional research points to the difficulties of LLMs in understanding specific logical operators.Sanyal et al. (2022) show that GPT-3, RoBERTa (Liu et al., 2019), and models from the T5 series (Raffel et al., 2020) exhibit deficiencies in comprehending logical negations, often failing to correctly deduce the implications of negated statements and rules.Similar findings have been reported by Truong et al. (2023), who demonstrate that models lack a sensitivity to negations within a broad range of natural language inference tasks.Wan et al. (2024) comprehensively assess a suite of formal reasoning scenarios, subjecting GPT-3, ChatGPT, GPT-4, Bard (PaLM 2), Vicuna (Vicuna, 2023) and Guanaco (Dettmers et al., 2023) to minimum functionality tests (MFTs), serving as logical reasoning unit tests that gauge the models' inherent logic comprehension.Their analysis uncovers a common difficulty among models in identifying logical fallacies.In addition, GPT-4 appears to struggle with De Morgan's Laws, which relate conjunctions and disjunctions through logical negation.</p>
<p>A growing body of research investigates the extent to which LLMs exhibit human-like reasoning patterns, particularly in deductive reasoning tasks.For example, Eisape et al. (2024) explore the parallels between human reasoning and that of LLMs in syllogisms.Their findings suggest that LLMs, much like their human counterparts, are susceptible to common logical fallacies and cognitive biases.Similarly, Dasgupta et al. (2022) find that LLMs, akin to humans, display content effects, indicating that the problem's semantic content can significantly influence the models' reasoning behavior.Further research corroborates the manifestation of human reasoning patterns in LLMs, as outlined in Appendix A.</p>
<p>Mechanistic Evaluation. Additional research evaluates the reasoning behavior of LLMs by</p>
<p>inspecting the models' internal mechanisms during the reasoning process.For example, Hou et al. (2023) analyze the models' attention patterns, specifically those of GPT-2 (Radford et al., 2019) and LLaMA (Touvron et al., 2023), to uncover if and how these models perform multistep reasoning internally.Findings indicate a structured, step-wise information processing within the models.Furthermore, layer-wise probing reveals that LLMs tend to focus on identifying relevant information from the task in lower layers, transitioning to more intricate reasoning processes in higher layers.Pirozelli et al. (2023) similarly probe individual layers of a fine-tuned RoBERTa-large model, corroborating the pivotal role of higher layers in the reasoning process.(2023), the study suggests that LLaMA 2-7B employs multiple, concurrent pathways, instead of following a singular path of reasoning.</p>
<p>Behavior in Inductive Reasoning Tasks</p>
<p>In contrast to the well-examined domain of deductive reasoning, the reasoning behavior of LLMs in inductive reasoning tasks remains comparatively underexplored.Nevertheless, research exists that seeks to evaluate the capability of LLMs to infer general conclusions from specific examples.For instance, Yang et al. (2024) investigate how LLMs, such as GPT-J (Wang &amp; Komatsuzaki, 2021) and LLaMA 7B, induce general rules from given facts.While in principle the models seem able to infer general rules from the data provided, challenges arise in ensuring that the rules are consistent with the premises, extend beyond the given information, align with real-world knowledge, and are pertinent to the given task.Similarly, Qiu et al. (2024) find that LLMs such as GPT-3.5,GPT-4, Claude 2 (Anthropic, 2023), and LLaMA 2-70B are capable of inferring rules from given data.However, the models frequently err in the application of these rules, highlighting a gap between their ability to generate and apply rules.Moreover, the rules derived often diverge significantly from those humans might produce, exhibiting a tendency towards verbosity and an inability to concentrate on the fundamental patterns for generalization.In addition, the study finds that LLMs display a pronounced sensitivity to alterations of the task descriptions.Han et al.</p>
<p>(2024) examine the reasoning behaviors of GPT-3.5 and GPT-4 in property induction tasks, where properties common among different categories need to be induced.Findings suggest that GPT-4's behavior closely aligns with human judgments.However, challenges arise when the models need to handle premise non-monotonicity, a scenario in which adding more information to an argument can actually decrease its likelihood.For instance, Han et al. ( 2024) illustrate that the three-premise argument {crow, peacock, rabbit} → bird is considered weaker by humans than the two-premise argument {crow, peacock} → bird, a line of reasoning that both GPT-3.5 and GPT-4 seem to struggle with.</p>
<p>Behavior in Abductive Reasoning Tasks</p>
<p>Some efforts have been made to evaluate the behavior of LLMs in abductive reasoning tasks.Collins et al. ( 2022) ask both humans and GPT-3 to generate plausible explanations for unexpected counterfactual scenarios, for instance why a window did not break despite being struck by a rock.Analyses indicate that GPT-3's ability to generate plausible and coherent explanations for scenarios that require reasoning beyond established patterns is limited.In scenarios demanding inventive, coherent, and context-sensitive responses, especially when the usage of common nouns is restricted, human performance distinctly surpasses that of GPT-3, underscoring a pronounced deficiency in the model's ability to reason beyond its training distribution.Further, Rudinger et al. (2020) reveal that language models such as GPT-2, BART, and various T5 variants exhibit difficulties in identifying or generating arguments that either weaken or strengthen a given hypothesis.Even after fine-tuning, models tend to struggle with the task, often contradicting themselves by producing identical statements to strengthen and weaken the same premise-hypothesis pair.Xu et al. (2023) explore the behavior of GPT-3.5, ChatGPT, and PaLM 2 on various abductive reasoning tasks, paying particular attention to the models' rationales and errors manifested during reasoning.The paper highlights a tendency for LLMs to incorporate redundant information in their explanations and to generate content not grounded in the context, resulting in hallucinations.In comparison to deductive reasoning scenarios, findings suggest that models seem to particularly struggle with multi-hop reasoning in abductive reasoning tasks.</p>
<p>Behavior in Mathematical Reasoning Tasks</p>
<p>Mathematical reasoning encompasses the structured process of arriving at conclusions based on established mathematical principles and logical deduction (Horsten, 2023).Several studies explore the behavior of LLMs in the context mathematical reasoning tasks, especially in arithmetic and math word problems (MWPs).For instance, Srivastava et al. (2024) evaluate a set of LLMs on a functional variant of the MATH dataset (Hendrycks et al., 2021), where the underlying mathematical principles of each problem are captured rather than a static problem formulation.This allows for evaluating models on different dataset snapshots, each comprising unique problem formulations but the same underlying reasoning process.Results reveal inconsistencies across varying snapshots, indicating a tendency of models to rely on memorization rather than reasoning.Razeghi et al. (2022) further highlight the impact of how a problem is formulated, noting a marked correlation between the frequency of terms in the models' pre-training data and their ability to solve arithmetic tasks.Similarly, Wu et al. (2024) observe that models struggle with two-digit additions expressed in mathematical bases that are less represented in the models' training data.</p>
<p>Further studies underline the models' susceptibility to perturbations of the reasoning task.Other studies investigate whether human-like reasoning behavior in mathematical tasks manifests in LLMs.Hagendorff et al. (2023) show that, akin to humans, GPT-3 variants tend to offer intuitive yet incorrect answers to cognitive reflection tests (Frederick, 2005).Notably, GPT-3.5 and GPT-4 provide more deliberate responses, outperforming humans in avoiding intuitive errors.At the same time, McKenzie et al. (2023) demonstrate a form of goal misgeneralization, where models, tasked with rounding numbers to a specific number of significant digits, consistently round based on the number of decimal places.This reflects a cognitive bias known as attribute substitution (Morewedge &amp; Kahneman, 2010), where a more challenging task is replaced with a simpler, related task.In a mechanistic evaluation, Chen et al. (2024a) conduct a layer-wise analysis of LLaMA's mathematical reasoning capabilities, discovering that higher layers exhibit superior mathematical problem-solving abilities, while lower layers seem to lack basic arithmetic and factual knowledge.</p>
<p>Behavior in Causal Reasoning Tasks</p>
<p>Causal reasoning is the process of discerning the cause-and-effect relationships that govern the dynamics of our environment (Sloman, 2005) 2023)'s evaluation of GPT-3 and PaLM on the blicket detector task (Gopnik &amp; Sobel, 2000), where models need to infer which objects cause a light to switch on, further highlights challenges in causal reasoning with LLMs.While models can identify the correct causal structure when a set of causal hypotheses is provided, they struggle in conditions where the hypotheses are not given, underscoring a limitation in their ability to infer causal relationships from limited data.2022), focus on the ability of LLMs to predict outcomes in hypothetical setups, identifying a significant gap between the capabilities of humans and LLMs, particularly in highly unrealistic scenarios.Li et al. (2023) further illustrate that while models like GPT-3 can produce outcomes that seem to align with counterfactual propositions, these models heavily rely on simple lexical cues within the given context, rather than demonstrating a profound grasp of the scenarios' hypothetical essence.Yu et al. (2023) evaluate LLMs on questions embedded in counterfactual presuppositions, pushing the models beyond simple fact retrieval.Their findings suggest that "closed-book" models such as GPT-3 and code-davinci-002 may fabricate facts or base their responses on flawed premises when answering counterfactual questions.Huang et al. (2024) explore a different angle by asking models to modify a given piece of argumentative text so that it upholds a specified logical relationship under new premises.Findings indicate that while LLMs like GPT-3.5 and GPT-4 demonstrate a capacity for solving such tasks, they struggle with modifying arguments such that a new premise makes the argument less likely to be true, an observation also made by Rudinger et al. (2020) in the context of abductive reasoning.</p>
<p>Behavior in</p>
<p>Summary.Regarding RQ1, our review suggests that the reasoning behaviors of LLMs are nuanced and diverse, yet a significant trend emerges: while current LLMs demonstrate proficiency in reasoning problems that align with their training distribution, they frequently encounter substantial conceptual difficulties in out-of-distribution scenarios.Notably, slight alterations in the task context can markedly impair the models' reasoning capabilities.Multi-step reasoning is often reduced to shortcut pattern matching, and fundamental conceptual errors highlight the models' deficiency in understanding basic principles of logic, mathematics, and causal reasoning, particularly in counterfactual setups.</p>
<p>Evaluation Methods</p>
<p>So far, we have provided an overview of studies that evaluate the behavior of LLMs on three core reasoning tasks.However, to date, a standardized methodology for assessing the reasoning capabilities of large language models is lacking.To address RQ2, we review and categorize predominant evaluation frameworks for analyzing the behavior of LLMs in reasoning tasks.As depicted in Figure 2, we categorize evaluation methodologies into four distinct groups: (i) conclusion-based, (ii) rationale-based, (iii) interactive, (iv) and mechanistic evaluations.In the following sections, we further delineate each category by discussing prevalent techniques.An overview of the advantages and disadvantages of each evaluation procedure can be found in Table 1.Additional details are provided in Appendix B.</p>
<p>Conclusion-Based Evaluation</p>
<p>In conclusion-based evaluation schemes, emphasis is placed on the model's final answer rather than the process by which the model arrives at its conclusion.This outcome-oriented approach, despite its limitation of overlooking the model's rationales, can nonetheless provide insights into the model's reasoning behavior.For instance, a thorough error analysis can unveil conceptual errors (Sanyal et al., 2022), reflections of cognitive biases (Dasgupta et al., 2022), or sensitivities to the task context (Wu et al., 2024).Similarly, an examination of the model's output distribution may reveal inherent predispositions towards certain outcomes (Itzhak et al., 2024), or serve as an indicator of the model's confidence in specific conclusions (Frohberg &amp; Binder, 2022).However, relying solely on the model's final conclusion can be less reliable than also considering how the model arrives at its conclusion.For example, candidate answers derived from first token probabilities in multiple-choice setups are often not robust (Wang et al., 2024b), and final answers might not always align with the model's verbalized reasoning (Mondorf &amp; Plank, 2024;Ye &amp; Durrett, 2022).Moreover, answers to benchmark datasets might have been compromised by data leakage during the model's training process, limiting the insights that can be drawn from a correct conclusion (Balloccu et al., 2024;Xu et al., 2024).To address the issue of data contamination and test the model's capacity to generalize, dynamic benchmarks can be employed that update over time.For instance, functional benchmarks capture the underlying reasoning process rather than a static problem formulation, probing the model's reasoning capabilities on dataset snapshots released quarterly (Srivastava et al., 2024).</p>
<p>Rationale-Based Evaluation</p>
<p>In contrast to conclusion-based evaluation schemes, rationale-based evaluation methods focus on examining the reasoning trace generated by the model, typically assessing its logical validity and coherence.While this approach allows for more nuanced insights into the model's reasoning behavior, rationale-based evaluation schemes are often more challenging to automate and scale.Given the variability in the model's reasoning, a spectrum of assessment techniques exist.For rationales following a highly consistent format-either through structured contexts or methods that guide the model's response style (Wan et al., 2024)-rationales can be parsed into more formalized representations such as first-order logic (Saparov &amp; He, 2023), computation graphs (Dziri et al., 2023), or causal graphs (Willig et al., 2022), facilitating a more granular examination.Alternatively, interpretable quantitative metrics, such as ROSCOE (Golovneva et al., 2023) or RECEVAL (Prasad et al., 2023), may be utilized to evaluate the rationales' semantic alignment with the reasoning task, their coherence, factual consistency, and logical validity.In instances where rationales are less structured, qualitative inspections are commonly employed, either through diagnostic agents (Huang et al., 2024), or human judgments (Mondorf &amp; Plank, 2024).</p>
<p>Interactive Evaluation</p>
<p>Similar to the principle of dynamic assessment within psychology (Haywood &amp; Tzuriel, 2002), interactive evaluation offers a framework to engage with LLMs during the evaluation.This approach allows for more flexible assessments tailored to the model's specific reasoning behavior.Although such evaluations are often costly and challenging to scale, several techniques have been developed to automate the process.For instance, adaptive evaluations dynamically select reasoning tasks based on the model's responses, thus providing deeper insights into its capabilities and limitations beyond what static questionnaires can reveal (Zhuang et al., 2023).Dialectic evaluation methods assess the model's reasoning in dialogue form, for example, by challenging the model's conclusions (Wang et al., 2023), or engaging it in game-theoretical scenarios (Bertolazzi et al., 2023).While interactive evaluations yield nuanced insights, they lack the structure of traditional methods, posing challenges in terms of standardization and reproducibility.</p>
<p>Mechanistic Evaluation</p>
<p>Mechanistic evaluations of LLMs delve into the underlying processes that drive the model's responses, aiming to uncover the "how" and "why" within their reasoning processes.By analyzing the internal mechanisms such as attention patterns (Hou et al., 2023), activation flows (Dutta et al., 2024), and the functional attributes of individual layers (Pirozelli et al., 2023), deeper insights into the model's operational logic can be gained, as illustrated in Section 3.1.1.Focusing on the model's intrinsic processes, this framework contrasts with previous approaches, drawing parallels to the study of human reasoning from a neuroscientific perspective (Papo, 2015).Nonetheless, current methods remain computeintensive, and their findings may not always generalize across different models and tasks (Bereska &amp; Gavves, 2024;Ferrando et al., 2024).</p>
<p>Discussion</p>
<p>Despite the notable performance of large language models in prominent reasoning tasks (Bubeck et al., 2023;Fu et al., 2023), our review suggests that current models more closely resemble stochastic parrots (Bender et al., 2021) than systematic reasoners.As discussed in Section 3, we find that although many LLMs demonstrate proficiency in reasoning problems that align with their training data, the models' reasoning behavior reveals significant conceptual errors and limitations in out-of-distribution scenarios.As highlighted by Mahowald et al. (2024), this suggests a limited functional linguistic competence in LLMs.It is likely that the apparent success of LLMs in reasoning tasks predominantly reflects their ability to memorize the extensive data they have been trained on (Wu et al., 2024;Dziri et al., 2023).</p>
<p>Recent studies indicate that a substantial amount of benchmark datasets has been leaked to current LLMs (Balloccu et al., 2024;Xu et al., 2024), raising concerns about the insights derived from their performance on such benchmarks.Therefore, we advocate for more nuanced analyses of the models' reasoning behavior, particularly in novel scenarios that the models have not previously encountered.</p>
<p>While human reasoning is not infallible (Johnson-Laird, 2010), the human capacity for robust reasoning and generalization from limited data remains unmatched by current LLMs.Various studies point to the fundamental differences between human reasoning and that of LLMs, especially the models' restrictive autoregressive pre-training objective (McCoy et al., 2023;Shanahan, 2024;Lenci, 2023).We call for further research-particularly on reasoning behavior-within both humans and LLMs to better discern and comprehend the essential components missing in LLMs, which are crucial for robust and systematic reasoning.</p>
<p>Conclusion</p>
<p>This survey provides a comprehensive review of literature that evaluates LLM-based reasoning beyond mere task accuracy, offering deeper insights into the reasoning behavior of large language models.We discuss the behavior of LLMs across three core reasoning tasks (RQ1), assessing logical, mathematical and causal reasoning.Furthermore, we outline predominant evaluation frameworks and compare their respective strengths and limitations (RQ2).Our findings indicate that although LLMs demonstrate proficiency in reasoning problems that align with their training data, they often encounter significant conceptual challenges in out-of-distribution scenarios.Considering these insights and the recent issue of benchmark dataset leakage (Balloccu et al., 2024;Xu et al., 2024), we caution against relying solely on shallow accuracy metrics for static reasoning tasks.Instead, we advocate for more sophisticated reasoning assessments, such as those discussed in Section 4.</p>
<p>B Further Details on Evaluation Methods</p>
<p>In this section, we offer a more detailed overview of the evaluation frameworks commonly employed to examine the reasoning behavior of large language models.Expanding on the taxonomy depicted in Figure 2, we present conclusion-based evaluation methods in Table 2, provide an overview of rationale-based evaluation approaches in Table 3</p>
<p>Figure 1 :
1
Figure 1: Schematic overview of the two types of reasoning tasks distinguished in this survey.Core reasoning tasks are designed to assess a particular reasoning ability within a given context.Conversely, integrated reasoning tasks involve the concurrent use of various reasoning skills.Tasks and abilities listed are not exhaustive.</p>
<p>Dutta et al. (2024) present an in-depth investigation into the internal mechanisms of LLaMA 2-7B(Touvron et al., 2023) when instructed via chain-of-thought prompting.Findings indicate a notable functional rift in the model's middle layers, where token representations in the initial half are biased towards pre-training priors, and an abrupt shift to in-context priors occurs in the latter half.As opposed to the findings byHou et al.</p>
<p>Shi et al. (2023) find that LLMs such as code-davinci-002 and GPT-3.5 can be distracted by context irrelevant to the MWP solution, especially when the irrelevant context shares lexical similarities with the original problem formulation.In a similar vein,Stolfo et al. (2023) report that LLMs are sensitive to interventions on MWPs, such as changing numerical values or altering the textual framing of the problem.</p>
<p>Figure 2 :
2
Figure 2: A taxonomy of evaluation methods to analyze the reasoning behaviors of LLMs.Only representative approaches for each method are listed.</p>
<p>Jin et al. (2024))23)rrelation, it offers a more nuanced understanding of how changes in one variable can bring about changes in another(Pearl, 2009).For a comprehensive introduction to causal reasoning, we strongly recommend the work ofPearl (2009).While research on the behavior of LLMs in causal reasoning tasks is still in its early stages, the field is receiving growing attention.Various studies to date suggest that LLMs are capable of reciting causal facts from their training data, but lack an intrinsic ability to comprehend or construct causal relationships.For instance,Zečević et al. (2023)indicate that while LLMs, including GPT-3, Meta AI's Opt, and AlephAlpha's Luminous(AlephAlpha, 2022), demonstrate some proficiency in causal reasoning tasks that align with causal facts seen during training, they exhibit limited ability to accurately discern and apply causal relationships in scenarios that demand more than associative recalls from their training data.Jin et al. (2023)probe the models' behavior across three levels of causation: (1) the associational, (2) the interventional, and (3) the counterfactual, as outlined by Pearl &amp; Mackenzie (2018)'s Ladder of Causation.Similarly, their findings indicate that LLMs are more adept at answering associational queries than tackling interventional or counterfactual tasks.Models seem to particularly struggle with causal relationships that deviate from commonsense or are unlikely to be part of their training corpora.Jin et al. (2024)evaluate the ability of LLMs to infer causation from statements that describe correlations between variables.Analyses reveal significant challenges in solving the given task across seventeen LLMs.Although fine-tuning offers substantial improvements, models still fail to generalize in out-of-distribution scenarios.Kosoy et al. (</p>
<p>Table 1 :
1
Comparison of strengths and limitations of the different evaluation methods.
Evaluation MethodAdvantagesDisadvantagesConclusion-based evaluationAllows for controlled setupsLimited insightsProvides metrics for comparisonLess reliableEasy to automate and scaleEasy to reproduceRationale-based evaluationOffers more nuanced insightsDifficult to automate and scaleMore robust in certain scenariosMight require expert interpretationInteractive evaluationHighly flexibleExpensiveCustomizable to model behaviorDifficult to automate and scaleLess standardized and reproducibleMechanistic evaluationIdentifies features or circuits re-Findings may not generalize acrosssponsible for specific behaviorstasks or modelsSupports direct interventions onResults may be hard to interpretmodel internalsCompute-intensive</p>
<p>, discuss interactive evaluation techniques in Table4, and delineate mechanistic evaluation strategies in Table5. in question-answer format into functional form, where triplets of (problem, solution, input) are defined that represent the underlying reasoning process of the task, and thus allow to produce flexible question-answer pairs.
CategoryEvaluation MethodDescriptionExemplary ReferenceOutput Distribution AnalysisModel Preferencea predefined set to assess the model's tendency towards Evaluate the likelihood of various candidate answers fromItzhak et al. (2024)specific answers.Model ConfidenceInterpret the probability assigned to an answer as a mea-Dasgupta et al. (2022)sure of confidence towards that answer.Error AnalysisConceptual ErrorsAssess the model's errors with respect to fundamentalSanyal et al. (2022)principles and concepts within the domain of reasoning.Context SensitivityEvaluate the model's robustness towards perturbations ofWu et al. (2024)the task's context.Cognitive BiasesProbe the model with respect to cognitive biases or heuris-Eisape et al. (2024)tics commonly encountered in human reasoning.Inverse ScalingAnalyze scenarios in which larger models tend to exhibitMcKenzie et al. (2023)more pronounced errors than smaller models.Dynamic BenchmarksFunctional BenchmarksTransform static benchmarks Srivastava et al. (2024)</p>
<p>Table 2 :
2
Overview of conclusion-based evaluation methods that assess the model's reasoning behavior by focusing on the final answers they produced.ReCEvalAnalyze the model's rationale using metrics that quantify its intra-step and inter-step logical validity, as well as the informativeness of each reasoning step.
CategoryEvaluation MethodDescriptionExemplary ReferenceStructured ParsingFOL ConversionTranslate the model's rationale into first-order-logic (FOL)Saparov &amp; He (2023)language and evaluate the logical expressions.Computational Graphstices represent intermediate results and edges denote func-Parse the rationale into a computation graph where ver-Dziri et al. (2023)tion mappings.FAC 2 EUse capability-specific instructions to elicit intermediateWang et al. (2024a)structured reasoning steps (crystallized and fluid reason-ing). Evaluate each step separately, using ground truthresponses and the BARTScore-Recall metric.Interpretable Quantitative MetricsROSCOEmetrics, quantifying its semantic alignment, semantic sim-Assess the model's rationale using a suite of nuancedGolovneva et al. (2023)ilarity, logical inference, and language coherence.Prasad et al. (2023)Qualitative InspectionHuman EvaluatorInspect rationales manually through human annotators.Mondorf &amp; Plank (2024)Diagnostic AgentsAssess rationales by means of additional language models,Huang et al. (2024)acting as diagnostic agents.</p>
<p>Table 3 :
3
Rationale-based evaluation methods that examine the model's reasoning behavior through an analysis of its underlying rationale.
CategoryEvaluation MethodDescriptionExemplary ReferenceComputerizedAdaptive EvaluationAdaptive TestingDynamically adapt questions presented during the evalu-Zhuang et al. (2023)(CAT)ation procedure based on the model's responses.Dialectic EvaluationBelief DefenseTest the model's reasoning through challenging its priorWang et al. (2023)response in a conversational discourse.Game-Theoretic Analysisgame-theoretic scenario. Evaluate the model's reasoning by engaging with it in aBertolazzi et al. (2023)</p>
<p>Table 4 :
4
Interactive evaluation techniques designed to assess the model's reasoning behavior through dynamic interaction.
Evaluation MethodDescriptionExemplary ReferenceLayer ProbingProbe the functional role of different layers of the model's architecture.Pirozelli et al. (2023)Attention Pattern Analysistion flow within the model. Analyze the model's attention matrices to gain insights into the underlying informa-Hou et al. (2023)Activation PatchingAlter specific neuron activations within a model and observe its impact on the model'sDutta et al. (2024)output.</p>
<p>Table 5 :
5
Overview of mechanistic evaluation methods that assess the model's reasoning by delving into its internal mechanisms throughout the reasoning process.</p>
<p>We consistently specify when a particular technique, other than standard prompting, is utilized. For a review of prompting approaches designed to improve the models' reasoning performance, we recommend consulting prior surveys byHuang &amp; Chang (2023),Yu et al. (2024), orSun et al. (2024).
Validity denotes whether the proof step logically follows from preceding steps, atomicity reflects whether it can be proven with exactly one application of a deduction rule, and utility measures its direct contribution towards the derivation of the final conclusion.
AcknowledgmentsWe express our gratitude to the anonymous reviewers for their insightful comments and suggestions.Furthermore, we would like to acknowledge that the emojis featured in Figure1are designed by OpenMoji -the open-source emoji and icon project (License: CC BY-SA 4.0).Finally, we gratefully recognize the support for BP through the ERC Consolidator Grant DIALECT 101043235.A Human-Like Reasoning Behavior in LLMsRecent research has begun to explore the extent to which human reasoning behaviors are reflected in large language models, given their training on human-generated data.Various studies delve into the manifestation of human-like reasoning behaviors within LLMs in the context of deductive reasoning tasks.For instance,Eisape et al. (2024)compare the behaviors of humans and LLMs, particularly those from the PaLM 2 series, in syllogistic reasoning tasks.The findings reveal that, similar to humans, LLMs are prone to logical fallacies and cognitive biases such as ordering effects.This susceptibility persists across model sizes, though larger models tend to engage in more deliberate reasoning, showing a reduced sensitivity towards these errors.A notable finding indicates that LLMs, unlike humans, rarely produce the "nothing follows" response, even when it represents the accurate deduction.In a similar vein,Dasgupta et al. (2022)demonstrate that LLMs, akin to humans, exhibit content effects in predicate logic problems, i.e. their reasoning is influenced by the semantic content of the task.This effect is evident in various LLMs including ChatGPT, PaLM 2, and Chinchilla(Hoffmann et al., 2022).In particular, analyses indicate that the models' reasoning in syllogisms is biased by the believability of the conclusion, a behavior known as belief bias(Klauer et al., 2000).Further research corroborates the influence of semantic content on the models' reasoning.Ando et al. (2023)demonstrate a belief-bias in models like ChatGPT, RoBERTa and BART(Lewis et al., 2020)by comparing the models' behavior on abstract, belief-consistent, and belief-inconsistent syllogisms.Their analyses also uncover a predisposition towards conversion errors and atmosphere effects, where logical quantifiers are either misinterpreted or lead to uninformed inferences, respectively(Tversky &amp; Kahneman, 1974).Itzhak et al. (2024)further show that methods such as instruction tuning and reinforcement learning from human feedback (RLHF)(Ouyang et al., 2022)may amplify cognitive biases within LLMs.Mondorf &amp; Plank (2024)delve into the inferential strategies of open-access LLMs in problems of propositional logic.Comprehensive manual evaluations of the models' rationales reveal that LLMs utilize inferential strategies similar to those employed by human reasoners.Their evaluations further underline difficulties of LLMs with logical negations and a vulnerability to logical fallacies commonly observed in human reasoning.Similarly, McKenzie et al. (2023)indicate that LLMs tend to replicate human-like logical errors when engaging with the logical principle of modus tollens, suggesting an imitation of flawed reasoning patterns from their training data.Notably, this trend becomes more pronounced with increasing model size, a phenomenon denoted as inverse scaling.
Large language models for mathematical reasoning: Progresses and challenges. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop. Neele Falk, Sara Papi, Mike Zhang, the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research WorkshopSt. Julian's, MaltaAssociation for Computational LinguisticsMarch 2024</p>
<p>Luminous language model. Alephalpha , 2022</p>
<p>APA Dictionary of Psychology. American Psychological Association. BehaviorMarch 15, 2024</p>
<p>Evaluating large language models with NeuBAROCO: Syllogistic reasoning ability and human-like biases. Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada, Proceedings of the 4th Natural Logic Meets Machine Learning Workshop. Stergios Chatzikyriakidis, Valeria De Paiva, the 4th Natural Logic Meets Machine Learning WorkshopNancy, FranceAssociation for Computational LinguisticsJune 2023</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, Yaguang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Daniel Smilkov, David R So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, arXiv:2305.10403Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone,2023Slav PetrovarXiv preprintand Yonghui Wu. Palm 2 technical report</p>
<p>Model card and evaluations for claude models. Anthropic, 2023Technical Report</p>
<p>Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source LLMs. Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, Ondrej Dusek, Proceedings of the 18th Conference of the European Chapter. Long Papers. Yvette Graham, Matthew Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational LinguisticsMarch 20241</p>
<p>Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information Fusion. Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera, 10.1016/j.inffus.2019.12.012.URLhttps://www.sciencedirect.com/science/article/pii/S1566253519308103202058</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21. the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Leonard Bereska, Efstratios Gavves, arXiv:2404.14082Mechanistic interpretability for ai safety-a review. 2024arXiv preprint</p>
<p>Chat-GPT's information seeking strategy: Insights from the 20-questions game. Leonardo Bertolazzi, Davide Mazzaccara, Filippo Merlo, Raffaella Bernardi, 10.18653/v1/2023.inlg-main.11Proceedings of the 16th International Natural Language Generation Conference. C , Maria Keet, Hung-Yi Lee, Sina Zarrieß, the 16th International Natural Language Generation ConferencePrague, CzechiaSeptember 2023Association for Computational Linguistics</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, S Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri S Castellon, Annie S Chatterji, Kathleen A Chen, Jared Creel, Dora Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren E Gale, Karan Gillespie, Noah D Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas F Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, O Khani, Pang Wei Khattab, Mark S Koh, Ranjay Krass, Rohith Krishna, Ananya Kuditipudi, Faisal Kumar, Mina Ladhak, Tony Lee, Jure Lee, Isabelle Leskovec, Levent, Lisa Xiang, Xuechen Li, Tengyu Li, Ali Ma, Christopher D Malik, Manning, P Suvir, Eric Mirchandani, Zanele Mitchell, Suraj Munyikwa, Avanika Nair, Deepak Narayan, Benjamin Narayanan, Allen Newman, Juan Carlos Nie, Niebles, J F Hamed Nilforoshan, Giray Nyarko, Andy Ogut, Krishna Parasuram Shih, Alex Srinivasan, Rohan Tamkin, Armin W Taori, Florian Thomas, Rose E Tramèr, William Wang, Bohan Wang, Jiajun Wu, Yuhuai Wu, Sang Wu, Michihiro Michael Xie, Jiaxuan Yasunaga, You, A Matei, Michael Zaharia, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zhang, Kaitlyn Zheng, Percy Zhou, Liang, Joon Sung Park. Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, H Yusuf, Camilo Roohani, Jack Ruiz, Ryan, Dorsa Christopher R'e, Shiori Sadigh, Keshav Sagawa, Santhanam, Laurel Orr, Isabel Papadimitriou2021</p>
<p>Stochastic parrots or intelligent systems? a perspective on true depth of understanding in llms. Ali Borji, 10.2139/ssrn.4507038SSRN. July 2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Human Reasoning: The Psychology Of Deduction. R M J Byrne, J S B T Evans, S E Newstead, 10.4324/97813157850281993Psychology Press1st edition</p>
<p>Is bigger and deeper always better? probing llama across scales and layers. Nuo Chen, Ning Wu, Shining Liang, Ming Gong, Linjun Shou, Dongmei Zhang, Jia Li, arXiv:2312.043332024aarXiv preprint</p>
<p>Premise order matters in reasoning with large language models. Xinyun Chen, Ryan A Chi, Xuezhi Wang, Denny Zhou, arXiv:2402.089392024barXiv preprint</p>
<p>Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu, arXiv:2309.15402A survey of chain of thought reasoning: Advances, frontiers and future. 2023arXiv preprint</p>
<p>Structured, flexible, and robust: Benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks. Catherine Kelsey M Collins, Jessica Wong, Michelle Feng, Joshua Wei, Tenenbaum, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society202244</p>
<p>Language models show human-like content effects on reasoning. Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv:2207.070512022arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Arbitrating norms for reasoning tasks. R Aliya, Dewey, 10.1007/s11229-022-03981-8Synthese. 1573-09642006502Nov 2022</p>
<p>How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning. Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, Tanmoy Chakraborty, arXiv:2402.183122024arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, ( Xiang, ) Lorraine, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Le Ronan, Jena Bras, Soumya Hwang, Xiang Sanyal, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>A systematic comparison of syllogistic reasoning in humans and language models. Tiwalayo Eisape, Michael Tessler, Ishita Dasgupta, Fei Sha, Sjoerd Steenkiste, Tal Linzen, 10.18653/v1/2024.naacl-long.466Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241</p>
<p>Javier Ferrando, Gabriele Sarti, Arianna Bisazza, Marta R Costa-Jussà, arXiv:2405.00208A primer on the inner workings of transformer-based language models. 2024arXiv preprint</p>
<p>Peter A Flach, Antonis C Kakas, 10.1007/978-94-017-0606-3_1https://doi.org/10.1007/978-94-017-0606-3 1Abductive and Inductive Reasoning: Background and Issues. Netherlands, DordrechtSpringer2000</p>
<p>Cognitive reflection and decision making. Shane Frederick, https://www.aeaweb.org/articles?id=10.1257/089533005775196732Journal of Economic Perspectives. 194December 2005</p>
<p>CRASS: A novel data set and benchmark to test counterfactual reasoning of large language models. J Örg, Frohberg , Frank Binder ; Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Proceedings of the Thirteenth Language Resources and Evaluation Conference. Stelios Odijk, Piperidis, the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources AssociationJan. June 2022</p>
<p>Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot, arXiv:2305.173062023arXiv preprint</p>
<p>ROSCOE: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Detecting blickets: How young children use information about novel causal powers in categorization and induction. Alison Gopnik, David M Sobel, Child Development. 00093920, 146786247152000</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. Thilo Hagendorff, Sarah Fabi, Michal Kosinski, 10.1038/s43588-023-00527-xNature Computational Science. 2662-8457310Oct 2023</p>
<p>Inductive reasoning in humans and large language models. Simon Jerome Han, Keith J Ransom, Andrew Perfors, Charles Kemp, 10.1016/j.cogsys.2023.101155.URLhttps://www.sciencedirect.com/science/article/pii/S1389041723000839Cognitive Systems Research. 1389-041783101155. 2024</p>
<p>Applications and challenges in dynamic assessment. H , Carl Haywood, David Tzuriel, 10.1207/S15327930PJE7702_5Peabody Journal of Education. 7722002</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). 2021</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Erich Simonyan, Jack W Elsen, Rae, arXiv:2203.15556Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. 2022arXiv preprint</p>
<p>The Cambridge Handbook of Thinking and Reasoning. Cambridge Handbooks in Psychology. K J Holyoak, R G Morrison, 2005Cambridge University Press</p>
<p>Philosophy of Mathematics. Leon Horsten, The Stanford Encyclopedia of Philosophy. Edward N Zalta, Uri Nodelman, 2023Metaphysics Research Lab, Stanford UniversityWinter 2023 edition</p>
<p>Towards a mechanistic interpretation of multistep reasoning capabilities of language models. Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, Mrinmaya Sachan, 10.18653/v1/2023.emnlp-main.299Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Yinya Huang, Ruixin Hong, Hongming Zhang, Wei Shao, Zhicheng Yang, Dong Yu, Changshui Zhang, Xiaodan Liang, Linqi Song, Clomo, arXiv:2311.17438Counterfactual logical modification with large language models. 2024arXiv preprint</p>
<p>Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, Yonatan Belinkov, 10.1162/tacl_a_00673Transactions of the Association for Computational Linguistics. 2307-387X1206 2024</p>
<p>CLadder: A benchmark to assess causal reasoning capabilities of language models. Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Lyu Zhiheng, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, Bernhard Sch Ölkopf, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Can large language models infer causation from correlation?. Zhijing Jin, Jiarui Liu, Lyu Zhiheng, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T Diab, Bernhard Sch Ölkopf, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Mental models and human reasoning. P N Johnson-Laird, 10.1073/pnas.1012933107Proc Natl Acad Sci U S A. 107432010</p>
<p>How We Reason. Philip Johnson-Laird, 10.1093/acprof:oso/9780199551330.001.00012008Oxford University Press10</p>
<p>How We Reason. P N Johnson-Laird, 2006Oxford University Press</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>On belief bias in syllogistic reasoning. Christoph Karl, Jochen Klauer, Barbara Musch, Naumer, 10.1037/0033-295X.107.4.852Psychological Review. 10742000</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Defeasible Reasoning. Robert Koons, The Stanford Encyclopedia of Philosophy. Edward N Zalta, 2022Metaphysics Research Lab, Stanford UniversitySummer 2022 edition</p>
<p>Towards understanding how machines can learn causal overhypotheses. E Kosoy, D M Chan, A Liu, J Collins, J Hamrick, S H Huang, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society202345</p>
<p>Defining and Describing Reason. Jacqueline P Leighton, 2003Cambridge University Press</p>
<p>Understanding natural language understanding systems. a critical analysis. Alessandro Lenci, arXiv:2303.042292023arXiv preprint</p>
<p>BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios. Jiaxuan Li, Lang Yu, Allyson Ettinger, 10.18653/v1/2023.acl-short.70Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20232Short Papers)</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023arXiv preprint</p>
<p>Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen, Bo Jiang, arXiv:2312.07622Aimin Zhou, and Liang He. Mathematical language models: A survey. 2024arXiv preprint</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Intelligence and Reasoning. David F Lohman, Joni M Lakin, Cambridge Handbooks in Psychology. Cambridge University Press2011</p>
<p>Dissociating language and thought in large language models. Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, Trends in Cognitive Sciences. 2024</p>
<p>Jitendra Malik, Workshop on foundation models. August 2021</p>
<p>Embers of autoregression: Understanding large language models through the problem they are trained to solve. Thomas Mccoy, Shunyu Yao, Dan Friedman, Matthew Hardy, Thomas L Griffiths, arXiv:2309.136382023arXiv preprint</p>
<p>Inverse scaling: When bigger isn't better. Ian R Mckenzie, Alexander Lyzhov, Michael Martin Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan Mclean, Xudong Shen, Joe Cavanagh, Andrew George Gritsevskiy, Derik Kauffman, Aaron T Kirtland, Zhengping Zhou, Yuhui Zhang, Sicong Huang, Daniel Wurgaft, Max Weiss, Alexis Ross, Gabriel Recchia, Alisa Liu, Jiacheng Liu, Tom Tseng, Tomasz Korbak, Najoung Kim, Samuel R Bowman, Ethan Perez, Transactions on Machine Learning Research. 2835-88562023</p>
<p>Augmented language models: a survey. Grégoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Timo Baptiste Roziere, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, Transactions on Machine Learning Research. 2835-88562023Published as a conference paper at COLM 2024</p>
<p>Can large language models reason? AI: A Guide for Thinking Humans. Melanie Mitchell, September 2023. March 202413</p>
<p>The debate over understanding in ai's large language models. Melanie Mitchell, David C Krakauer, 10.1073/pnas.2215907120Proceedings of the National Academy of Sciences. 120132023</p>
<p>Comparing inferential strategies of humans and large language models in deductive reasoning. Philipp Mondorf, Barbara Plank, arXiv:2402.148562024arXiv preprint</p>
<p>Associative processes in intuitive judgment. Carey K Morewedge, Daniel Kahneman, 10.1016/j.tics.2010.07.004Trends in Cognitive Sciences. 14102010</p>
<p>Optimizing language models for dialogue. Openai, Chatgpt, 2022</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brockman, Miles Brooks, Kevin Brundage, Trevor Button, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chen, Chester Chess, Casey Cho, Hyung Won Chu, Dave Chung, Jeremiah Cummings, Yunxing Currier, Cory Dai, Thomas Decareaux, Noah Degry, Damien Deutsch, Arka Deville, David Dhar, Steve Dohan, Sheila Dowling, Adrien Dunning, Atty Ecoffet, Tyna Eleti, David Eloundou, Liam Farhi, Niko Fedus, Felix, Juston Sim Ón Posada Fishman, Isabella Forte, Leo Fulford, Elie Gao, Christian Georges, Vik Gibson, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Jan Hendrik Kim, Jamie Kirchner, Matt Kiros, Daniel Knight, Łukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Mayne ; Aalok, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Ashvin Mishkin ; Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Ouyang, O' Cullen, Jakub Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Giambattista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Daniel Perelman ; John Schulman, Kyla Selsam, Toki Sheppard, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin Sohl, Yang Sokolowsky, Natalie Song, Staudacher, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Wojciech Wong ; Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Tianhao Zhao, Juntang Zheng, William Zhuang, Barret Zhuk, Zoph, Felipe Petroski Such. Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,2024Amin TootoonchianFilipe de Avila Belbute Peres ; Juan Felipe Cer ón Uribe, Andrea Vallone, Arun VijayvergiyaSherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu. Gpt-4 technical report</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Jan Paul F Christiano, Ryan Leike, Lowe, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>How can we study reasoning in the brain?. David Papo, 10.3389/fnhum.2015.00222Frontiers in Human Neuroscience. 92222015</p>
<p>Judea Pearl and Dana Mackenzie. The book of why: the new science of cause and effect. 2009. 2018Judea Pearl. Causality. Cambridge University Press2 edition. Basic books</p>
<p>Assessing logical reasoning capabilities of encoder-only transformer models. Paulo Pirozelli, Marcos M José, A F Anarosa, Fabio G Brandão, Cozman, arXiv:2312.117202023arXiv preprint</p>
<p>ReCEval: Evaluating reasoning chains via correctness and informativeness. Archiki Prasad, Swarnadeep Saha, Xiang Zhou, Mohit Bansal, 10.18653/v1/2023.emnlp-main.622Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, 10.18653/v1/2023.acl-long.294Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019160025533</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Impact of pretraining term frequencies on few-shot numerical reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, 10.18653/v1/2022.findings-emnlp.59Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesDecember 2022Association for Computational Linguistics</p>
<p>The psychology of knights and knaves. Lance J Rips, org/10.1016/0010-0277(89)90019-XCognition. 0010-02773121989</p>
<p>Handbook of Automated Reasoning (in 2 volumes). John Alan Robinson and Andrei Voronkov2001Elsevier and MIT Press</p>
<p>Thinking like a skeptic: Defeasible inference in natural language. Rachel Rudinger, Vered Shwartz, Jena D Hwang, Chandra Bhagavatula, Maxwell Forbes, Le Ronan, Noah A Bras, Yejin Smith, Choi, doi: 10.18653Findings of the Association for Computational Linguistics: EMNLP 2020. Trevor Cohn, Yulan He, Yang Liu, Association for Computational LinguisticsNovember 2020</p>
<p>URL. </p>
<p>RobustLR: A diagnostic benchmark for evaluating logical robustness of deductive reasoners. Soumya Sanyal, Zeyi Liao, Xiang Ren, 10.18653/v1/2022.emnlp-main.653Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, International Conference on Learning Representations. 2023</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Mehran Kazemi, Najoung Kim, He He, Advances in Neural Information Processing Systems. 202436</p>
<p>Talking about large language models. Murray Shanahan, 10.1145/3624724Commun. ACM. 0001-0782672jan 2024</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Huai Hsin Chi, Nathanael Scharli, Denny Zhou, International Conference on Machine Learning. 2023</p>
<p>Steven Sloman, 10.1093/acprof:oso/9780195183115.001.0001Causal Models: How People Think about the World and Its Alternatives. Oxford University Press08 2005</p>
<p>Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. Saurabh Srivastava, P V Anto, Shashank Menon, Ajay Sukumar, Alan Philipose, Stevin Prince, Sooraj Thomas, arXiv:2402.194502024arXiv preprint</p>
<p>A causal framework to quantify the robustness of mathematical reasoning with language models. Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schoelkopf, Mrinmaya Sachan, 10.18653/v1/2023.acl-long.32Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, Pheng , Ann Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Ji-Rong Wen, Xipeng Qiu, Yike Guo, Hui Xiong, Qun Liu, Zhenguo Li, arXiv:2312.11562A survey of reasoning with foundation models. 2024arXiv preprint</p>
<p>. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Lakshman Tamara Von Glehn, Mehran Yagati, Lucas Kazemi, Misha Gonzalez, Jakub Khalman, Alexandre Sygnowski, Charlotte Frechette, Laura Smith, Lev Culp, Yi Proleev, Xi Luan, James Chen, Nathan Lottes, Federico Schucher, Alban Lebron, Natalie Rrustemi, Phil Clay, Tomas Crone, Jeffrey Kocisky, Bartek Zhao, Dian Perz, Heidi Yu, Adam Howard, Jack W Bloniarz, Han Rae, Laurent Lu, Marcello Sifre, Fred Maggioni, Dan Alcober, Megan Garrette, Shantanu Barnes, Jacob Thakoor, Gabriel Austin, William Barth-Maron, Rishabh Wong, Rahma Joshi, Deeni Chaabouni, Arun Fatiha, Ruibo Ahuja, Yunxuan Liu, Sarah Li, Jeremy Cogan, Chao Chen, Chenjie Jia, Qiao Gu, Jordan Zhang, Ale Jakse Grimstad, Martin Hartman, Gaurav Chadwick, Xavier Singh Tomar, Evan Garcia, Emanuel Senter, Thanumalayan Taropa, Jacob Sankaranarayana Pillai, Michael Devlin, Diego Laskin, De Las, Dasha Casas, Connie Valter, Lorenzo Tao, Adrià Blanco, David Puigdomènech Badia, Mianna Reitter, Jenny Chen, Clara Brennan, Sergey Rivera, Shariq Brin, Gabriela Iqbal, Jane Surita, Abhi Labanowski, Stephanie Rao, Emilio Winkler, Yiming Parisotto, Kate Gu, Yujing Olszewska, Ravi Zhang, Antoine Addanki, Annie Miech, Laurent El Louis, Denis Shafey, Geoff Teplyashin, Elliot Brown, Nithya Catt, Jan Attaluri, Jackie Balaguer, Pidong Xiang, Zoe Wang, Anton Ashwood, Albert Briukhov, Sanjay Webson, Smit Ganapathy, Ajay Sanghavi, Ming-Wei Kannan, Axel Chang, Josip Stjerngren, Yuting Djolonga, Ankur Sun, Matthew Bapna, Pedram Aitchison, Henryk Pejman, Tianhe Michalewski, Cindy Yu, Juliette Wang, Junwhan Love, Dawn Ahn, Kehang Bloxwich, Peter Han, Thibault Humphreys, James Sellam, Varun Bradbury, Sina Godbole, Bogdan Samangooei, Alex Damoc, Kaskasoli, M R Sébastien, Vijay Arnold, Shubham Vasudevan, Jason Agrawal, Dmitry Riesa, Richard Lepikhin, Srivatsan Tanburn, Hyeontaek Srinivasan, Sarah Lim, Pranav Hodkinson, Johan Shyam, Steven Ferret, Ankush Hand, Tom Le Garg, Jian Paine, Yujia Li, Minh Li, Alexander Giang, Zaheer Neitz, Sarah Abbas, Machel York, Elizabeth Reid, Aakanksha Cole, Dipanjan Chowdhery, Dominika Das, Vitaly Rogozi Ńska, Pablo Nikolaev, Zachary Sprechmann, Lukas Nado, Flavien Zilka, Luheng Prost, Marianne He, Gaurav Monteiro, Chris Mishra, Josh Welty, Dawei Newlan, Miltiadis Jia, Clara Huiyi Allamanis, Raoul Hu, Justin De Liedekerke, Carl Gilmer, Shruti Saroufim, Shaobo Rijhwani, Disha Hou, Anirudh Shrivastava, Alex Baddepudi, Adnan Goldin, Albin Ozturel, Yunhan Cassirer, Daniel Xu, Devendra Sohn, Reinald Sachan, Craig Kim Amplayo, Dessie Swanson, Shashi Petrova, Arthur Narayan, Siddhartha Guez, Jessica Brahma, Miteyan Landon, Ruizhe Patel, Kevin Zhao, Luyu Villela, Wenhao Wang, Matthew Jia, Mai Rahtz, Legg Giménez, Hanzhao Yeung, James Lin, Petko Keeling, Diana Georgiev, Boxi Mincu, Salem Wu, Rachel Haykal, Kiran Saputro, James Vodrahalli, Zeynep Qin, Abhanshu Cankara, Nick Sharma, Will Fernando, Behnam Hawkins, Solomon Neyshabur, Adrian Kim, Priyanka Hutter, Alex Agrawal, George Castro-Ros, Tao Van Den Driessche, Fan Wang, Yang, Paul Shuo Yiin Chang, Ross Komarek, Mario Mcilroy, Guodong Lučić, Wael Zhang, Michael Farhan, Paul Sharman, Paul Natsev, Yong Michel, Yamini Cheng, Siyuan Bansal, Kris Qiao, Siamak Cao, Christina Shakeri, Justin Butterfield, Paul Chung, Shivani Kishan Rubenstein, Arthur Agrawal, Kedar Mensch, Karel Soparkar, Timothy Lenc, Aedan Chung, Loren Pope, Jackie Maggiore, Priya Kay, Shibo Jhakra, Joshua Wang, Mary Maynez, Taylor Phuong, Andrea Tobin, Maja Tacchetti, Kevin Trebacz, Yash Robinson, Sebastian Katariya, Paige Riedel, Kefan Bailey, Nimesh Xiao, Lora Ghelani, Ambrose Aroyo, Neil Slone, Xuehan Houlsby, Zhen Xiong, Elena Yang, Jonas Gribovskaya, Mateo Adler, Lisa Wirth, Music Lee, Thais Li, Jay Kagohara, Sophie Pavagadhi, Anna Bridgers, Sanjay Bortsova, Zafarali Ghemawat, Tianqi Ahmed, Richard Liu, Vijay Powell, Mariko Bolina, Polina Iinuma, James Zablotskaia, Besley, Da-Woon, Timothy Chung, Ramona Dozat, Xiance Comanescu, Jeremy Si, Guolong Greer, Martin Su, Raphaël Polacek, Simon Lopez Kaufman, Hexiang Tokumine, Elena Hu, Yingjie Buchatskaya, Mohamed Miao, Aditya Elhawaty, Nenad Siddhant, Jinwei Tomasev, Christina Xing, Helen Greer, Shereen Miller, Aurko Ashraf, Zizhao Roy, Ada Zhang, Angelos Ma, Milos Filos, Rory Besta, Ted Blevins, Chih-Kuan Klimenko, Soravit Yeh, Jiaqi Changpinyo, Oscar Mu, Mantas Chang, Carrie Pajarskas, Vered Muir, Charline Le Cohen, Krishna Lan, Amit Haridasan, Steven Marathe, Sholto Hansen, Rajkumar Douglas, Mingqiu Samuel, Sophia Wang, Chang Austin, Jiepu Lan, Justin Jiang, Jaime Alonso Chiu, Lars Lorenzo, Sébastien Lowe Sj Ösund, Zach Cevey, Thi Gleicher, Anudhyan Avrahami, Hansa Boral, Vittorio Srinivasan, Rhys Selo, Konstantinos May, Léonard Aisopos, Hussenot, Baldini Livio, Kate Soares, Michael B Baumli, Adrià Chang, Ben Recasens, Alexander Caine, Filip Pritzel, Fabio Pavetic, Anita Pardo, Justin Gergely, Vinay Frye, Dan Ramasesh, Kartikeya Horgan, Nora Badola, Subhrajit Kassner, Ethan Roy, Víctor Dyer, Alex Campos, Yunhao Tomala, Dalia El Tang, Elspeth Badawy, Basil White, Oran Mustafa, Abhishek Lang, Sharad Jindal, Zhitao Vikram, Sergi Gong, Ross Caelles, Gregory Hemsley, Fangxiaoyu Thornton, Wojciech Feng, Ce Stokowiec, Phoebe Zheng, C Thacker, Zhishuai ¸a Glar Ünl Ü, Mohammad Zhang, James Saleh, Max Svensson, Piyush Bileschi, Ankesh Patil, Roman Anand, Katerina Ring, Arpi Tsihlas, Marco Vezer, Toby Selvi, Mikel Shevlane, Tom Rodriguez, Samira Kwiatkowski, Keran Daruki, Allan Rong, Nicholas Dafoe, Keren Fitzgerald, Mina Gu-Lemberg, Lisa Anne Khan, Marie Hendricks, Vladimir Pellat, James Feinberg, Tara Cobon-Kerr, Maribeth Sainath, Rauh, Hadi Sayed, Richard Hashemi, Yana Ives, Yaguang Hasson, Eric Li, Yuan Noland, Nathan Cao, Le Byrd, Qingze Hou, Thibault Wang, Michela Sottiaux, Jean-Baptiste Paganini, Alexandre Lespiau, Samer Moufarek, Kaushik Hassan, Shivakumar, Amol Joost Van Amersfoort, Pratik Mandhane, Anirudh Joshi, Matthew Goyal, Andrew Tung, Hannah Brock, Vedant Sheahan, Cheng Misra, Nemanja Li, Mostafa Rakićević, Fangyu Dehghani, Sid Liu, Junhyuk Mittal, Seb Oh, Eren Noury, Fantine Sezener, Matthew Huot, Nicola De Lamm, Charlie Cao, Gamaleldin Chen, Ed Elsayed, Mahdis Chi, Ian Mahdieh, Nan Tenney, Ivan Hua, Patrick Petrychenko, Dylan Kane, Rishub Scandinaro, Jonathan Jain, Romina Uesato, Adam Datta, Oskar Sadovsky, Dominik Bunyan, Shimu Rabiej, John Wu, Gautam Zhang, Edouard Vasudevan, Mahmoud Leurent, Ionut Alnahlawi, Nan Georgescu, Ivy Wei, Betty Zheng, Pam G Chan, Piotr Rabinovitch, Ye Stanczyk, David Zhang, Subhajit Steiner, Michael Naskar, Matthew Azzam, Adam Johnson, Chung-Cheng Paszke, Jaume Chiu, Afroz Sanchez Elias, Faizan Mohiuddin, Jin Muhammad, Andrew Miao, Nino Lee, Sahitya Vieillard, Jane Potluri, Elnaz Park, Jiageng Davoodi, Jeff Zhang, Drew Stanway, Abhijit Garmon, Zhe Karmarkar, Jong Dong, Aviral Lee, Luowei Kumar, Jonathan Zhou, William Evens, Zhe Isaac, Johnson Chen, Anselm Jia, Zhenkai Levskaya, Chris Zhu, Peter Gorgolewski, Yu Grabowski, Alberto Mao, Kaisheng Magni, Javier Yao, Norman Snaider, Paul Casagrande, Evan Suganthan, Geoffrey Palmer, Edward Irving, Manaal Loper, Isha Faruqui, Nanxin Arkatkar, Izhak Chen, Michael Shafran, Alfonso Fink, Irene Casta Ño, Wooyeol Giannoumis, Mikołaj Kim, Ashwin Rybi Ński, Jennifer Sreevatsa, David Prendki, Adrian Soergel, Willi Goedeckemeyer, Mohsen Gierke, Meenu Jafari, Jeremy Gaba, Diana Gage Wiesner, Yawen Wright, Harsha Wei, Yana Vashisht, Jay Kulizhskaya, Maigo Hoover, Lu Le, Chimezie Li, Lu Iwuanyanwu, Kevin Liu, Andrey Ramirez, Albert Khorlin, Cui, Lin Tian, Marin Georgiev, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom Van Der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Dung Duc, Paula Nguyen, Sarmishta Kurylowicz, Sebastian Velury, Cassidy Krause, Lucas Hardin, Lili Dixon, Kiam Janzer, Ziqiang Choo, Biao Feng, Achintya Zhang, Tejasi Singhal, Mingyang Latkar, Quoc Zhang, Elena Le, Dayou Allica Abellan, Dan Du, Natasha Mckinnon, Tolga Antropova, Orgad Bolukbasi, David Keller, Daniel Reid, Maria Finchelstein, Remi Abi Raad, Peter Crocker, Robert Hawkins, Colin Dadashi, Sid Gaffney, Ken Lall, Egor Franko, Anna Filonov, Rémi Bulanova, Vikas Leblond, Shirley Yadav, Harry Chung, Luis C Askham, Kelvin Cobo, Felix Xu, Jun Fischer, Christina Xu, Chris Sorokin, Chu-Cheng Alberti, Colin Lin, Hao Evans, Alek Zhou, Hannah Dimitriev, Dylan Forbes, Zora Banarse, Jing Tung, Sabaer Li, John Fatehi, Omar Wieting, Benigno Ajmeri, Tao Uria, Yeongil Zhu, Laura Ko, Amélie Knight, Ning Héliou, Shane Niu, Chenxi Gu, Dustin Pang, Yeqing Tran, Nir Li, Ariel Levine, Norbert Stolovich, Rebeca Kalb, Sonam Santamaria-Fernandez, Wenny Goenka, Robin Yustalim, Ali Strudel, Balaji Elqursh, Charlie Lakshminarayanan, Shyam Deck, Hyo Upadhyay, Mike Lee, Zonglin Dusenberry, Xuezhi Li, Kyle Wang, Raphael Levin, Dan Hoffmann, Olivier Holtmann-Rice, Summer Bachem, Sho Yue, Eric Arora, Daniil Malmi, Qijun Mirylenka, Christy Tan, Soheil Koh, Hassas Yeganeh, Steven Siim P Õder, Francesco Zheng, Mukarram Pongetti, Yanhua Tariq, Lucian Sun, Mojtaba Ionita, Pouya Seyedhosseini, Ragha Tafti, Zhiyu Kotikalapudi, Anmol Liu, Jasmine Gulati, Liu ; Nikhil, Tianrun Sethi, Ben Li, Shreya Brown, Wei Singh, Aaron Fan, Joe Parisi, Chenkai Stanton, Vinod Kuang, Koverkathu, A Christopher, Yunjie Choquette-Choo, Li, Abe Lu, Prakash Ittycheriah, Pei Shroff, Mani Sun, Sanaz Varadarajan, Rob Bahargam, David Willoughby, Ishita Gaddy, Dasgupta ; Brennan, Tyler Saeta, Yi Liechty, Yao Sun, Stephan Zhao, Pandu Lee, Doug Nayak, Manish Reddy Fritz, John Vuyyuru, Nidhi Aslanides, Martin Vyas, Xiao Wicke, Ma ; Hardie, James Cate, Keyvan Manyika, Yelin Amiri, Xi Kim, Kai Xiong, Florian Kang, Nilesh Luisier, David Tripuraneni, Mandy Madras, Guo ; Han, Garima Zhang, Jakob Pruthi, Feng Bauer, Christof Yang, Xiaowei Angermueller, Weiren Li, Julia Wang, Emmanouil Wiesinger, Yuan Koukoumidis, Anand Tian, Madhu Iyer, Mark Gurumurthy, Parashar Goldenson, Shah, Hongkun Blake, Anthony Yu, Jennimaria Urbanowicz, Chrisantha Palomaki, Fernando, Taylan Bilal, Evgenii Eltyshev. Amy Shen, Amir Globerson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi2023Guillaume Desjardins, Marco Cornero, Brona RobenekClaudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Pluci ńska, David Bridson, Dario de Cesare ; Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe ; Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang,Jordi Pont-Tuset. Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu,. Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker Schuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. Gemini: A family of highly capable multimodal models</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Language models are not naysayers: an analysis of language models on negation benchmarks. Hung Thinh, Timothy Truong, Karin Baldwin, Trevor Verspoor, Cohn, 10.18653/v1/2023.starsem-1.10Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (<em>SEM 2023). Alexis Palmer, Jose Camacho-Collados, the 12th Joint Conference on Lexical and Computational Semantics (</em>SEM 2023)Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Judgment under uncertainty: Heuristics and biases. Amos Tversky, Daniel Kahneman, 10.1126/science.185.4157.1124Science. 18541571974</p>
<p>Strategies in sentential reasoning. Jean-Baptiste Van Der Henst, Yingrui Yang, P N Johnson-Laird, Cognitive Science. 0364-02132642002</p>
<p>Is einstein's puzzle over-specified?. Dylan Vassberg, John Vassberg, AJ75th Symposium: 21st Century Challenges in. Princeton, NJNovember 2009Computational Engineering &amp; Science, Princeton University</p>
<p>Vicuna, Vicuna, An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality. 2023</p>
<p>Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen Tse Huang, Pinjia He, Wenxiang Jiao, Michael R Lyu, arXiv:2401.00757A &amp; b == b &amp; a: Triggering logical reasoning failures in large language models. 2024arXiv preprint</p>
<p>Ben Wang, Aran Komatsuzaki, Gpt-j-6b: A 6 billion parameter autoregressive language model. May 2021</p>
<p>Can ChatGPT defend its belief in truth? evaluating LLM reasoning via debate. Boshi Wang, Xiang Yue, Huan Sun, 10.18653/v1/2023.findings-emnlp.795Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Fac 2 e: Better understanding large language model capabilities by dissociating language and cognition. Xiaoqiang Wang, Bang Liu, Lingfei Wu, arXiv:2403.001262024aarXiv preprint</p>
<p>Look at the text: Instruction-tuned language models are more robust multiple choice selectors than you think. Xinpeng Wang, Chengzhi Hu, Bolei Ma, Barbara Paul R Öttger, Plank, arXiv:2404.083822024barXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Can foundation models talk causality. Moritz Willig, Matej Zečević, Devendra Singh Dhami, Kristian Kersting, UAI 2022 Workshop on Causal Representation Learning. 2022</p>
<p>Using cognitive interviews and think-aloud protocols to understand thought processes. D Michael, Nikki G Wolcott, Lobczowski, 10.1016/j.cptl.2020.09.005.URLhttps://www.sciencedirect.com/science/article/pii/S1877129720303026Currents in Pharmacy Teaching and Learning. 202113</p>
<p>Approach to the study of human reasoning. D J Wood, 10.1038/223101a0Nature. 1476-46872235201Jul 1969</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky Ürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim, 10.18653/v1/2024.naacl-long.102Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoJune 20241</p>
<p>Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, Erik Cambria, arXiv:2306.09841Are large language models really good logical reasoners? a comprehensive evaluation from deductive, inductive and abductive views. 2023arXiv preprint</p>
<p>Benchmarking benchmark leakage in large language models. Ruijie Xu, Zengzhi Wang, Run-Ze Fan, Pengfei Liu, arXiv:2404.188242024arXiv preprint</p>
<p>Language models as inductive reasoners. Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria ; Zonglin, Li Yang, Xinya Dong, Hao Du, Erik Cheng, Xiaodong Cambria, Jianfeng Liu, Furu Gao, Wei, arXiv:2303.12023Logical reasoning over natural language as knowledge representation: A survey. Long Papers. Yvette Graham, Matthew Purver, St. Julian's, MaltaAssociation for Computational Linguistics2023. March 20241arXiv preprintProceedings of the 18th Conference of the European Chapter</p>
<p>The unreliability of explanations in few-shot prompting for textual reasoning. Xi Ye, Greg Durrett, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Natural language reasoning, a survey. Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang, 10.1145/3664194ACM Comput. Surv. 0360-0300may 2024</p>
<p>IfQA: A dataset for opendomain question answering under counterfactual presuppositions. Wenhao Yu, Meng Jiang, Peter Clark, Ashish Sabharwal, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Causal parrots: Large language models may talk causality but are not causal. Matej Zečević, Moritz Willig, Devendra Singh Dhami, Kristian Kersting, Transactions on Machine Learning Research. 2835-88562023</p>
<p>On the paradox of learning to reason from data. Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den, Broeck, 10.24963/ijcai.2023/375Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI '23. the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI '232023</p>
<p>Yan Zhuang, Qi Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, Guanhao Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, arXiv:2306.10512Efficiently measuring the cognitive ability of llms: An adaptive testing perspective. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
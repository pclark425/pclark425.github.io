<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8395 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8395</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8395</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-f843233f76a5dff07bfa93a71a1cf13d8aa6a94a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a" target="_blank">Exploring Length Generalization in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper establishes that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale, and shows that combining pretrained large language models' in-context learning abilities with scratchpad prompting results in a dramatic improvement in lengthgeneralization.</p>
                <p><strong>Paper Abstract:</strong> The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8395.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8395.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parity task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic Parity (bit-string parity / coin-flip) task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic task where the model must determine whether a bit-string (or sequence of coin flips) has an even or odd number of ones/flips; used to probe sequential state-tracking and length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer checkpoints (LaMDA family) pretrained on general natural language data; finetuned and prompted in experiments at scales including 244M, 422M, 1B, 64B, 128B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity (boolean arithmetic over bit-strings); framed also as coin-flip natural language instances.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Sequential state representation (parity as left-to-right running parity) versus non-sequential pooled/counting strategies (counting number of 1s and thresholding).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Finetuning on in-distribution lengths; few-shot prompting with scratchpad; scratchpad finetuning; padded-scratchpad (position-padding) intervention; per-step scratchpad error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Finetuned models: near-perfect in-distribution accuracy (reported 100% in one setup) but rapid degradation OOD; in the 'vary number of ones' split in-distribution = 100% while OOD ≈ random (≈50%). Few-shot scratchpad prompting: substantial OOD improvement, able to extrapolate from short exemplars to much longer instances (qualitative/high accuracy up to 20 flips in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Learned parallel 'counting' shortcuts that don't transfer to different-length regimes; attention fails to focus on relevant token when input length is OOD; failures exacerbated by distractor tokens in input or preceding scratchpad; sensitivity to finetuning hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>When training fixes token count but varies number of ones, transformer performance collapses OOD which indicates counting/pooling strategy rather than sequential algorithm; per-step scratchpad error curves show that OOD input length causes mistakes even on early (in-distribution) scratchpad steps — implicating attention-based misallocation; masking distractors yields perfect generalization (see distractor analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Few-shot scratchpad prompting (in-context learning) enables the pretrained model to apply a sequential scratchpad template to much longer instances, contradicting the conclusion that transformer cannot extrapolate — showing that the failure is training-regime dependent rather than architectural impossibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8395.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8395.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Variable Assignment task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Boolean Variable Assignment (program execution) task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic Python-like programs consisting of boolean assignment operations; the model must output the value of the queried variable after executing the program, testing state-tracking across dependent operations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer checkpoints (LaMDA family) used in finetuning and prompting experiments across model scales from hundreds of millions to >100 billion parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Boolean operations and program execution (assign, and/or/xor/negate, conditional assigns) — logical/arithmetic-like operator composition over variables.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Representations align with computational graph depth (longest dependency chain) rather than raw number of operations; transformers tend to adopt parallel strategies resolving low-depth dependencies first rather than executing strictly sequential left-to-right.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Finetuning on 'chain-like' and 'diverse' splits; shuffle-ops baseline (operations randomized) to detect parallel shortcuts; few-shot scratchpad prompting and scratchpad finetuning; tracking performance over computational graph depth.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Finetuned models achieve near-perfect in-distribution accuracy (short lengths) but degrade rapidly OOD as program length increases; models perform much better on OOD examples whose computational graph depth remains in-distribution, indicating depth-driven difficulty rather than raw operation count.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Preference for non-sequential spurious correlations / parallel strategies that don't generalize; failure correlates with increased computational graph depth; distractor tokens in input reduce ability to attend to relevant variable dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Shuffled-ops baseline attains similar OOD performance as clean training, implying model relies on non-sequential correlations; experiments controlling computational graph depth show accuracy is primarily a function of depth (plots in SM), and fixing operation count while varying depth preserves difficulty ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Few-shot scratchpad prompting did not uniformly rescue variable-assignment generalization when pretrained (zero-shot) performance was poor — indicating that few-shot benefits depend on base-model capabilities; scale alone did not resolve depth-driven failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8395.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8395.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Finetuning (vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard supervised finetuning on length-generalization tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training pretrained transformer weights on task-specific input-target pairs (loss computed only on target tokens) to teach the task; used as baseline for length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained decoder-only transformers finetuned with AdaFactor optimizer, hyperparameter sweeps over learning rate, batch size, and small dropout; trained until in-distribution validation loss settled (e.g., 20000 steps parity).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity and Boolean variable assignment as synthetic algorithmic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Often learns non-sequential, parallelized shortcuts (e.g., pooling/counting) rather than true sequential state-tracking algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Finetuning experiments at multiple scales; shuffle-ops baseline to probe learned strategy; hyperparameter sweeps to test robustness; monitored in- vs out-of-distribution accuracy curves.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Near-perfect in-distribution accuracy across model sizes; out-of-distribution performance rapidly degrades with length — scale does not substantially improve OOD length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Learns spurious correlations and parallel strategies that do not extrapolate to longer instances; high sensitivity to finetuning hyperparameters (different hyperparameters yielded similar in-distribution loss but very different OOD behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Shuffled-ops baseline produces similar OOD performance, implicating non-sequential strategy; experiments fixing token count but varying number-of-ones show collapse of OOD performance (pooling/counting behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Combining finetuning with few-shot scratchpad sometimes improves parity generalization; however the improvement is task-dependent and not universal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8395.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8395.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Finetuning with intermediate step (scratchpad / chain-of-thought) targets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finetuning models to generate step-by-step intermediate computations (scratchpad) before the final answer, intended to teach sequential algorithmic decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LaMDA family models finetuned to produce explicit scratchpad intermediate tokens corresponding to stepwise execution (e.g., parity running state or commented program traces).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity step-by-step state, and line-by-line variable value comments for program execution.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Attempts to induce an explicit sequential state representation in output tokens (scratchpad) but internal attention patterns often fail to generalize; per-step error rate used as a diagnostic of the sequential strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Scratchpad finetuning; padded scratchpad (position padding) to control positional biases; per-step scratchpad error analysis; masking distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Scratchpad finetuning shows strong in-distribution performance but still poor OOD length generalization similar to vanilla finetuning (Figure 4); per-step error rates increase for OOD lengths in zero-shot finetuned models, whereas few-shot finetuned per-step error remained roughly constant across steps in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Attention fails to attend to correct input tokens when input length is OOD; distractor tokens (input or preceding scratchpad) degrade performance; positional encoding / EOS issues only partially explain failures.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Padded-scratchpad (equalizing T5 position bias bins) improved but did not fix OOD failures; per-step scratchpad error plots show OOD input length causes errors even on in-distribution scratchpad steps — indicating attention pattern mismatch rather than purely positional parameter gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Few-shot scratchpad prompting (no finetuning) often allows sequential scratchpad templates to generalize to much longer instances, demonstrating that finetuning to scratchpad does not necessarily induce the same robust in-context extrapolation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8395.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8395.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot scratchpad prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context few-shot prompting with scratchpad (chain-of-thought) exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing a small number of worked examples that include intermediate scratchpad steps in the prompt (no weight updates) to induce the model to produce stepwise solutions and generalize to longer instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA 128B (pretrained), other LaMDA sizes tested</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained decoder-only LLMs (no finetuning) conditioned with few-shot exemplars that include scratchpad steps; tested on coin-flip parity and variable assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity / coin-flip step-by-step reasoning and variable-assignment program execution with commented scratchpad.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Template-based variable-length pattern matching: the model infers the scratchpad solution template from short exemplars and applies it to longer inputs, effectively implementing the sequential algorithm in generation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Zero-shot vs few-shot prompting comparisons, natural-language coin-flip prompts, scratchpad template design variations, few-shot finetuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Pretrained LLMs with suitably designed few-shot scratchpad prompts show dramatic improvements in length generalization — able to map a length-3 scratchpad exemplar to correct 20-step scratchpad outputs (qualitative/high accuracy reported); performance scales positively with model size (cites Wei et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Dependence on prompt style: alternative prompt styles that yield poor zero-shot performance also lead to poor few-shot finetuned generalization; when base (zero-shot) performance is weak, few-shot does not reliably rescue generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Demonstrations (Figure 8) of long correct scratchpad generation from a few short exemplars; comparison to Wei et al.'s chain-of-thought results; observed scaling with model size.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>On variable assignment where base model non-finetuned performance was poor, few-shot scratchpad provided limited improvement — indicating mechanism effectiveness depends on preexisting model capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8395.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8395.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parallel pooling/counting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-sequential pooling/counting shortcut (transformer parallel strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned shortcut where the transformer aggregates (pools/counts) across the input (e.g., counts number of ones) rather than executing a sequential running-state algorithm, producing solutions that fail to generalize to different-length distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformers able to perform permutation-invariant/pooling computations via self-attention, leading to bottom-up counting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Counting number of ones (parity), deriving outputs from global statistics rather than sequential state.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Pooling/aggregation across positions enabled by self-attention (equivariant), forming a top-level statistic used for final prediction rather than stepwise state vector.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Data distribution manipulation (fix token count, vary number of ones); shuffle-ops baseline for variable assignment; analyzing OOD performance when a pooled statistic changes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Models trained under distributions that allow pooling show near-perfect in-distribution results but OOD performance collapses to random when global statistics differ (e.g., in the 'varied number of ones' split OOD ≈ random).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Does not support length extrapolation because pooled statistics do not map across different-length regimes; leads to periodic high/low accuracies depending on majority bit biases.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Experiments fixing token length but varying number-of-ones show models learned to count ones and threshold (in-distribution 100% but OOD ≈ random); shuffled baseline shows similar OOD behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Scratchpad few-shot prompting can induce sequential behavior despite the transformer's pooling capacity, demonstrating that the architecture can be coaxed into sequential strategies via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8395.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8395.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Computational graph depth</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computational graph depth (longest dependency chain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A notion of 'length' for variable-assignment tasks defined as the length of the longest dependency chain linking the queried variable; found to better predict transformer difficulty than raw operation count.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same transformer models evaluated with respect to dependency-chain depth of input programs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Program dependency depth as complexity proxy in boolean variable assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Transformer difficulty scales with dependency depth; models resolve low-depth dependencies first, showing bias toward handling parallelizable dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Evaluation controlling for number-of-operations while varying computational graph depth; plotting accuracy vs depth across training iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy is primarily a function of computational graph depth — models handle OOD operation counts well if depth remains in-distribution; training progression shows models learn small depths first then larger depths.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>High-depth dependency chains cause failure even when number of operations is in-distribution; reveals that simple operation-count baselines are insufficient to characterize difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Plots (Figure 13) showing that accuracy vs depth remains consistent even when operation counts are fixed at OOD values; models initially learn small depths and progress to larger depths.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>None reported that invalidate depth as a better difficulty predictor; however few-shot prompting effects may vary depending on pretrained competence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8395.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8395.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Padded scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Padded scratchpad (position-equalized scratchpad targets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention that pads inputs and scratchpad targets left and right so that the token index requiring attention falls into the same positional bias bin across different input lengths (controls for positional encoding gaps).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various sizes) with T5 position biases</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LaMDA models using T5-style position biases; training data augmented with left/right padding to make token distances consistent across lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity scratchpad generation and variable-assignment scratchpad targets.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Aimed to test whether untrained positional bins explain OOD failures by forcing the same positional relationships across lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Data augmentation by adding random left/right padding to input and scratchpad target pairs; then finetuning scratchpad model and evaluating OOD.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Padded-scratchpad helped relative to unpadded scratchpad but models still displayed significant length generalization issues — padding alone did not solve OOD failures.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Indicates that positional bias under-training is not the main cause; remaining errors point to attention/distractor problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Padded-scratchpad intervention produced partial improvement but not full generalization; combined with per-step error analyses this suggests attention misallocation rather than solely positional encoding gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Masking distractors produced full generalization whereas padding did not, challenging a positional-encoding-only explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8395.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8395.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distractor masking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masking distracting input and/or preceding scratchpad tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention that replaces tokens not required for the sequential algorithm with dummy padding so the model must attend only to the minimally relevant tokens; used to identify source of scratchpad finetuning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformers trained with modified inputs/scratchpads where distracting tokens are manually masked during training/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity scratchpad generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Removes distractors so attention can focus on essential sequential cues, testing whether distractors cause OOD failures.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Four experimental conditions: (1) padded scratchpad baseline, (2) mask preceding scratchpad distractors, (3) mask input distractors, (4) mask both input and scratchpad distractors; measure per-step scratchpad accuracy across input lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Masking all distractors produced perfect length generalization; masking input distractors had a larger positive effect than masking preceding scratchpad distractors, showing input distractors hurt most.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Distractor tokens (primarily in the input) cause attention misallocation and breakdown of sequential scratchpad strategies, leading to OOD failures.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Figure 15 shows that removing distractors yields perfect generalization and isolating input vs scratchpad masking demonstrates input distractors contribute more to failure.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Practical models cannot rely on manual masking; this result indicates architectural or training-objective fixes are needed to mitigate distractor sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8395.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8395.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EOS / Position bias investigations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-of-sequence (EOS) prediction and T5 position-bias considerations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hypothesized sources of length generalization failure: (1) under-trained positional bias bins (T5 biases) for long distances, and (2) EOS token prediction causing premature termination; tested via padding and data modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (T5 position biases used)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LaMDA models that incorporate T5-style position biases to encode positions; experiments examine whether these architectural choices explain length generalization failure.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity scratchpad generation and variable assignment scratchpad.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>If positional bias bins or EOS training cause OOD failures, equalizing token distances (padding) or removing EOS signals should mitigate failures.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Padded scratchpad to equalize positional bins; analysis informed by Newman et al.'s EOS token observations; experiments controlling for EOS-related artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Padded intervention improved but did not eliminate OOD failures; EOS token issues noted as plausible but not sole cause.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>EOS prediction can cause premature termination or influence learned representations adversely, but in these experiments distractors and attention misallocation remained principal issues.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Padding tests (position-equalization) produced partial improvements; prior work (Newman et al.) suggests EOS issues can matter, but current experiments show distractor sensitivity dominates.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Masking distractors fully corrected length generalization, arguing that EOS/positional effects are not the dominant failure mode in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8395.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8395.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Per-step scratchpad error rate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-step scratchpad accuracy / error-rate diagnostic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diagnostic measuring accuracy of predicted intermediate scratchpad tokens as a function of step index and input length, used to study where sequential computation fails.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various sizes, including 128B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Finetuned and prompted models evaluated for per-step correctness of scratchpad intermediate outputs for parity and variable-assignment tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Stepwise parity or variable-value comments generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>If model implements a sequential algorithm, per-step error should grow predictably with step index or input length; deviations reveal attention/representation failures.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Plot per-step accuracies across step index for varying input (in-distribution and OOD) lengths; compare zero-shot scratchpad finetuned models to few-shot finetuned ones.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot scratchpad finetuned models show abrupt per-step error increase when evaluated on OOD lengths; few-shot finetuned 128B model displayed roughly constant per-step error rate across steps in some settings (indicating better sequential behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Per-step errors increasing with OOD length indicate inability to maintain correct attention/state across many steps; constant per-step error suggests per-step error rate as bottleneck for long extrapolation (geometric decay of success).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Per-step accuracy plots (Figure 6) demonstrate that OOD input length causes errors even on early steps and that few-shot conditioning can alter per-step error dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Even when per-step error is roughly constant, overall success declines exponentially with number of steps; thus reducing per-step error is essential but not always sufficient for long extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8395.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8395.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shuffle-ops baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shuffled-operations baseline for variable assignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline dataset variant where the order of program operations is randomized to remove sequential dependency, used to test whether models rely on sequential execution or spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (422M example reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformers finetuned on variable assignment data where operation order was shuffled during training; compared to models trained on clean ordered data.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Variable assignment predictions without sequential dependency.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>If model attains similar OOD performance when trained on shuffled ops, it suggests reliance on non-sequential correlations rather than sequential execution.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Train models on shuffled-ops dataset and compare OOD accuracy/length-generalization curves with those trained on ordered dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Shuffled-ops baseline displays OOD performance on par with models trained on clean ordered data; interestingly the shuffled baseline sometimes shows even more dramatic length-generalization deficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Models trained on shuffled ops can only exploit spurious, non-sequential correlations and therefore fail to generalize to longer sequences requiring true sequential computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>The parity of OOD performance between shuffled and clean models indicates learned solutions rely on parallel/spurious features and not sequential algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>This baseline confirms the hypothesis that transformers prefer parallel strategies; however few-shot prompting can still elicit sequential behavior in pretrained models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>The eos decision and length extrapolation <em>(Rating: 1)</em></li>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer <em>(Rating: 1)</em></li>
                <li>Impact of pretraining term frequencies on few-shot reasoning <em>(Rating: 1)</em></li>
                <li>Unveiling transformers with lego: a synthetic reasoning task <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8395",
    "paper_id": "paper-f843233f76a5dff07bfa93a71a1cf13d8aa6a94a",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Parity task",
            "name_full": "Synthetic Parity (bit-string parity / coin-flip) task",
            "brief_description": "A synthetic task where the model must determine whether a bit-string (or sequence of coin flips) has an even or odd number of ones/flips; used to probe sequential state-tracking and length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (various sizes)",
            "model_description": "Decoder-only transformer checkpoints (LaMDA family) pretrained on general natural language data; finetuned and prompted in experiments at scales including 244M, 422M, 1B, 64B, 128B parameters.",
            "arithmetic_task_type": "Parity (boolean arithmetic over bit-strings); framed also as coin-flip natural language instances.",
            "mechanism_or_representation": "Sequential state representation (parity as left-to-right running parity) versus non-sequential pooled/counting strategies (counting number of 1s and thresholding).",
            "probing_or_intervention_method": "Finetuning on in-distribution lengths; few-shot prompting with scratchpad; scratchpad finetuning; padded-scratchpad (position-padding) intervention; per-step scratchpad error analysis.",
            "performance_metrics": "Finetuned models: near-perfect in-distribution accuracy (reported 100% in one setup) but rapid degradation OOD; in the 'vary number of ones' split in-distribution = 100% while OOD ≈ random (≈50%). Few-shot scratchpad prompting: substantial OOD improvement, able to extrapolate from short exemplars to much longer instances (qualitative/high accuracy up to 20 flips in experiments).",
            "error_types_or_failure_modes": "Learned parallel 'counting' shortcuts that don't transfer to different-length regimes; attention fails to focus on relevant token when input length is OOD; failures exacerbated by distractor tokens in input or preceding scratchpad; sensitivity to finetuning hyperparameters.",
            "evidence_for_mechanism": "When training fixes token count but varies number of ones, transformer performance collapses OOD which indicates counting/pooling strategy rather than sequential algorithm; per-step scratchpad error curves show that OOD input length causes mistakes even on early (in-distribution) scratchpad steps — implicating attention-based misallocation; masking distractors yields perfect generalization (see distractor analysis).",
            "counterexamples_or_challenges": "Few-shot scratchpad prompting (in-context learning) enables the pretrained model to apply a sequential scratchpad template to much longer instances, contradicting the conclusion that transformer cannot extrapolate — showing that the failure is training-regime dependent rather than architectural impossibility.",
            "uuid": "e8395.0",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Variable Assignment task",
            "name_full": "Boolean Variable Assignment (program execution) task",
            "brief_description": "Synthetic Python-like programs consisting of boolean assignment operations; the model must output the value of the queried variable after executing the program, testing state-tracking across dependent operations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (various sizes)",
            "model_description": "Decoder-only transformer checkpoints (LaMDA family) used in finetuning and prompting experiments across model scales from hundreds of millions to &gt;100 billion parameters.",
            "arithmetic_task_type": "Boolean operations and program execution (assign, and/or/xor/negate, conditional assigns) — logical/arithmetic-like operator composition over variables.",
            "mechanism_or_representation": "Representations align with computational graph depth (longest dependency chain) rather than raw number of operations; transformers tend to adopt parallel strategies resolving low-depth dependencies first rather than executing strictly sequential left-to-right.",
            "probing_or_intervention_method": "Finetuning on 'chain-like' and 'diverse' splits; shuffle-ops baseline (operations randomized) to detect parallel shortcuts; few-shot scratchpad prompting and scratchpad finetuning; tracking performance over computational graph depth.",
            "performance_metrics": "Finetuned models achieve near-perfect in-distribution accuracy (short lengths) but degrade rapidly OOD as program length increases; models perform much better on OOD examples whose computational graph depth remains in-distribution, indicating depth-driven difficulty rather than raw operation count.",
            "error_types_or_failure_modes": "Preference for non-sequential spurious correlations / parallel strategies that don't generalize; failure correlates with increased computational graph depth; distractor tokens in input reduce ability to attend to relevant variable dependencies.",
            "evidence_for_mechanism": "Shuffled-ops baseline attains similar OOD performance as clean training, implying model relies on non-sequential correlations; experiments controlling computational graph depth show accuracy is primarily a function of depth (plots in SM), and fixing operation count while varying depth preserves difficulty ordering.",
            "counterexamples_or_challenges": "Few-shot scratchpad prompting did not uniformly rescue variable-assignment generalization when pretrained (zero-shot) performance was poor — indicating that few-shot benefits depend on base-model capabilities; scale alone did not resolve depth-driven failures.",
            "uuid": "e8395.1",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Finetuning (vanilla)",
            "name_full": "Standard supervised finetuning on length-generalization tasks",
            "brief_description": "Training pretrained transformer weights on task-specific input-target pairs (loss computed only on target tokens) to teach the task; used as baseline for length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (various sizes)",
            "model_description": "Pretrained decoder-only transformers finetuned with AdaFactor optimizer, hyperparameter sweeps over learning rate, batch size, and small dropout; trained until in-distribution validation loss settled (e.g., 20000 steps parity).",
            "arithmetic_task_type": "Parity and Boolean variable assignment as synthetic algorithmic tasks.",
            "mechanism_or_representation": "Often learns non-sequential, parallelized shortcuts (e.g., pooling/counting) rather than true sequential state-tracking algorithm.",
            "probing_or_intervention_method": "Finetuning experiments at multiple scales; shuffle-ops baseline to probe learned strategy; hyperparameter sweeps to test robustness; monitored in- vs out-of-distribution accuracy curves.",
            "performance_metrics": "Near-perfect in-distribution accuracy across model sizes; out-of-distribution performance rapidly degrades with length — scale does not substantially improve OOD length generalization.",
            "error_types_or_failure_modes": "Learns spurious correlations and parallel strategies that do not extrapolate to longer instances; high sensitivity to finetuning hyperparameters (different hyperparameters yielded similar in-distribution loss but very different OOD behavior).",
            "evidence_for_mechanism": "Shuffled-ops baseline produces similar OOD performance, implicating non-sequential strategy; experiments fixing token count but varying number-of-ones show collapse of OOD performance (pooling/counting behavior).",
            "counterexamples_or_challenges": "Combining finetuning with few-shot scratchpad sometimes improves parity generalization; however the improvement is task-dependent and not universal.",
            "uuid": "e8395.2",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Scratchpad finetuning",
            "name_full": "Finetuning with intermediate step (scratchpad / chain-of-thought) targets",
            "brief_description": "Finetuning models to generate step-by-step intermediate computations (scratchpad) before the final answer, intended to teach sequential algorithmic decomposition.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (various sizes)",
            "model_description": "Same LaMDA family models finetuned to produce explicit scratchpad intermediate tokens corresponding to stepwise execution (e.g., parity running state or commented program traces).",
            "arithmetic_task_type": "Parity step-by-step state, and line-by-line variable value comments for program execution.",
            "mechanism_or_representation": "Attempts to induce an explicit sequential state representation in output tokens (scratchpad) but internal attention patterns often fail to generalize; per-step error rate used as a diagnostic of the sequential strategy.",
            "probing_or_intervention_method": "Scratchpad finetuning; padded scratchpad (position padding) to control positional biases; per-step scratchpad error analysis; masking distractors.",
            "performance_metrics": "Scratchpad finetuning shows strong in-distribution performance but still poor OOD length generalization similar to vanilla finetuning (Figure 4); per-step error rates increase for OOD lengths in zero-shot finetuned models, whereas few-shot finetuned per-step error remained roughly constant across steps in some cases.",
            "error_types_or_failure_modes": "Attention fails to attend to correct input tokens when input length is OOD; distractor tokens (input or preceding scratchpad) degrade performance; positional encoding / EOS issues only partially explain failures.",
            "evidence_for_mechanism": "Padded-scratchpad (equalizing T5 position bias bins) improved but did not fix OOD failures; per-step scratchpad error plots show OOD input length causes errors even on in-distribution scratchpad steps — indicating attention pattern mismatch rather than purely positional parameter gaps.",
            "counterexamples_or_challenges": "Few-shot scratchpad prompting (no finetuning) often allows sequential scratchpad templates to generalize to much longer instances, demonstrating that finetuning to scratchpad does not necessarily induce the same robust in-context extrapolation behavior.",
            "uuid": "e8395.3",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Few-shot scratchpad prompting",
            "name_full": "In-context few-shot prompting with scratchpad (chain-of-thought) exemplars",
            "brief_description": "Providing a small number of worked examples that include intermediate scratchpad steps in the prompt (no weight updates) to induce the model to produce stepwise solutions and generalize to longer instances.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA 128B (pretrained), other LaMDA sizes tested",
            "model_description": "Pretrained decoder-only LLMs (no finetuning) conditioned with few-shot exemplars that include scratchpad steps; tested on coin-flip parity and variable assignment.",
            "arithmetic_task_type": "Parity / coin-flip step-by-step reasoning and variable-assignment program execution with commented scratchpad.",
            "mechanism_or_representation": "Template-based variable-length pattern matching: the model infers the scratchpad solution template from short exemplars and applies it to longer inputs, effectively implementing the sequential algorithm in generation behavior.",
            "probing_or_intervention_method": "Zero-shot vs few-shot prompting comparisons, natural-language coin-flip prompts, scratchpad template design variations, few-shot finetuning experiments.",
            "performance_metrics": "Pretrained LLMs with suitably designed few-shot scratchpad prompts show dramatic improvements in length generalization — able to map a length-3 scratchpad exemplar to correct 20-step scratchpad outputs (qualitative/high accuracy reported); performance scales positively with model size (cites Wei et al.).",
            "error_types_or_failure_modes": "Dependence on prompt style: alternative prompt styles that yield poor zero-shot performance also lead to poor few-shot finetuned generalization; when base (zero-shot) performance is weak, few-shot does not reliably rescue generalization.",
            "evidence_for_mechanism": "Demonstrations (Figure 8) of long correct scratchpad generation from a few short exemplars; comparison to Wei et al.'s chain-of-thought results; observed scaling with model size.",
            "counterexamples_or_challenges": "On variable assignment where base model non-finetuned performance was poor, few-shot scratchpad provided limited improvement — indicating mechanism effectiveness depends on preexisting model capabilities.",
            "uuid": "e8395.4",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Parallel pooling/counting strategy",
            "name_full": "Non-sequential pooling/counting shortcut (transformer parallel strategy)",
            "brief_description": "A learned shortcut where the transformer aggregates (pools/counts) across the input (e.g., counts number of ones) rather than executing a sequential running-state algorithm, producing solutions that fail to generalize to different-length distributions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (various sizes)",
            "model_description": "Decoder-only transformers able to perform permutation-invariant/pooling computations via self-attention, leading to bottom-up counting strategies.",
            "arithmetic_task_type": "Counting number of ones (parity), deriving outputs from global statistics rather than sequential state.",
            "mechanism_or_representation": "Pooling/aggregation across positions enabled by self-attention (equivariant), forming a top-level statistic used for final prediction rather than stepwise state vector.",
            "probing_or_intervention_method": "Data distribution manipulation (fix token count, vary number of ones); shuffle-ops baseline for variable assignment; analyzing OOD performance when a pooled statistic changes.",
            "performance_metrics": "Models trained under distributions that allow pooling show near-perfect in-distribution results but OOD performance collapses to random when global statistics differ (e.g., in the 'varied number of ones' split OOD ≈ random).",
            "error_types_or_failure_modes": "Does not support length extrapolation because pooled statistics do not map across different-length regimes; leads to periodic high/low accuracies depending on majority bit biases.",
            "evidence_for_mechanism": "Experiments fixing token length but varying number-of-ones show models learned to count ones and threshold (in-distribution 100% but OOD ≈ random); shuffled baseline shows similar OOD behavior.",
            "counterexamples_or_challenges": "Scratchpad few-shot prompting can induce sequential behavior despite the transformer's pooling capacity, demonstrating that the architecture can be coaxed into sequential strategies via prompting.",
            "uuid": "e8395.5",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Computational graph depth",
            "name_full": "Computational graph depth (longest dependency chain)",
            "brief_description": "A notion of 'length' for variable-assignment tasks defined as the length of the longest dependency chain linking the queried variable; found to better predict transformer difficulty than raw operation count.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (various sizes)",
            "model_description": "Same transformer models evaluated with respect to dependency-chain depth of input programs.",
            "arithmetic_task_type": "Program dependency depth as complexity proxy in boolean variable assignment.",
            "mechanism_or_representation": "Transformer difficulty scales with dependency depth; models resolve low-depth dependencies first, showing bias toward handling parallelizable dependencies.",
            "probing_or_intervention_method": "Evaluation controlling for number-of-operations while varying computational graph depth; plotting accuracy vs depth across training iterations.",
            "performance_metrics": "Accuracy is primarily a function of computational graph depth — models handle OOD operation counts well if depth remains in-distribution; training progression shows models learn small depths first then larger depths.",
            "error_types_or_failure_modes": "High-depth dependency chains cause failure even when number of operations is in-distribution; reveals that simple operation-count baselines are insufficient to characterize difficulty.",
            "evidence_for_mechanism": "Plots (Figure 13) showing that accuracy vs depth remains consistent even when operation counts are fixed at OOD values; models initially learn small depths and progress to larger depths.",
            "counterexamples_or_challenges": "None reported that invalidate depth as a better difficulty predictor; however few-shot prompting effects may vary depending on pretrained competence.",
            "uuid": "e8395.6",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Padded scratchpad",
            "name_full": "Padded scratchpad (position-equalized scratchpad targets)",
            "brief_description": "An intervention that pads inputs and scratchpad targets left and right so that the token index requiring attention falls into the same positional bias bin across different input lengths (controls for positional encoding gaps).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (various sizes) with T5 position biases",
            "model_description": "LaMDA models using T5-style position biases; training data augmented with left/right padding to make token distances consistent across lengths.",
            "arithmetic_task_type": "Parity scratchpad generation and variable-assignment scratchpad targets.",
            "mechanism_or_representation": "Aimed to test whether untrained positional bins explain OOD failures by forcing the same positional relationships across lengths.",
            "probing_or_intervention_method": "Data augmentation by adding random left/right padding to input and scratchpad target pairs; then finetuning scratchpad model and evaluating OOD.",
            "performance_metrics": "Padded-scratchpad helped relative to unpadded scratchpad but models still displayed significant length generalization issues — padding alone did not solve OOD failures.",
            "error_types_or_failure_modes": "Indicates that positional bias under-training is not the main cause; remaining errors point to attention/distractor problems.",
            "evidence_for_mechanism": "Padded-scratchpad intervention produced partial improvement but not full generalization; combined with per-step error analyses this suggests attention misallocation rather than solely positional encoding gaps.",
            "counterexamples_or_challenges": "Masking distractors produced full generalization whereas padding did not, challenging a positional-encoding-only explanation.",
            "uuid": "e8395.7",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Distractor masking",
            "name_full": "Masking distracting input and/or preceding scratchpad tokens",
            "brief_description": "An intervention that replaces tokens not required for the sequential algorithm with dummy padding so the model must attend only to the minimally relevant tokens; used to identify source of scratchpad finetuning failures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (various sizes)",
            "model_description": "Transformers trained with modified inputs/scratchpads where distracting tokens are manually masked during training/evaluation.",
            "arithmetic_task_type": "Parity scratchpad generation.",
            "mechanism_or_representation": "Removes distractors so attention can focus on essential sequential cues, testing whether distractors cause OOD failures.",
            "probing_or_intervention_method": "Four experimental conditions: (1) padded scratchpad baseline, (2) mask preceding scratchpad distractors, (3) mask input distractors, (4) mask both input and scratchpad distractors; measure per-step scratchpad accuracy across input lengths.",
            "performance_metrics": "Masking all distractors produced perfect length generalization; masking input distractors had a larger positive effect than masking preceding scratchpad distractors, showing input distractors hurt most.",
            "error_types_or_failure_modes": "Distractor tokens (primarily in the input) cause attention misallocation and breakdown of sequential scratchpad strategies, leading to OOD failures.",
            "evidence_for_mechanism": "Figure 15 shows that removing distractors yields perfect generalization and isolating input vs scratchpad masking demonstrates input distractors contribute more to failure.",
            "counterexamples_or_challenges": "Practical models cannot rely on manual masking; this result indicates architectural or training-objective fixes are needed to mitigate distractor sensitivity.",
            "uuid": "e8395.8",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "EOS / Position bias investigations",
            "name_full": "End-of-sequence (EOS) prediction and T5 position-bias considerations",
            "brief_description": "Hypothesized sources of length generalization failure: (1) under-trained positional bias bins (T5 biases) for long distances, and (2) EOS token prediction causing premature termination; tested via padding and data modifications.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
            "mention_or_use": "use",
            "model_name": "LaMDA (T5 position biases used)",
            "model_description": "LaMDA models that incorporate T5-style position biases to encode positions; experiments examine whether these architectural choices explain length generalization failure.",
            "arithmetic_task_type": "Parity scratchpad generation and variable assignment scratchpad.",
            "mechanism_or_representation": "If positional bias bins or EOS training cause OOD failures, equalizing token distances (padding) or removing EOS signals should mitigate failures.",
            "probing_or_intervention_method": "Padded scratchpad to equalize positional bins; analysis informed by Newman et al.'s EOS token observations; experiments controlling for EOS-related artifacts.",
            "performance_metrics": "Padded intervention improved but did not eliminate OOD failures; EOS token issues noted as plausible but not sole cause.",
            "error_types_or_failure_modes": "EOS prediction can cause premature termination or influence learned representations adversely, but in these experiments distractors and attention misallocation remained principal issues.",
            "evidence_for_mechanism": "Padding tests (position-equalization) produced partial improvements; prior work (Newman et al.) suggests EOS issues can matter, but current experiments show distractor sensitivity dominates.",
            "counterexamples_or_challenges": "Masking distractors fully corrected length generalization, arguing that EOS/positional effects are not the dominant failure mode in these tasks.",
            "uuid": "e8395.9",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Per-step scratchpad error rate",
            "name_full": "Per-step scratchpad accuracy / error-rate diagnostic",
            "brief_description": "A diagnostic measuring accuracy of predicted intermediate scratchpad tokens as a function of step index and input length, used to study where sequential computation fails.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (various sizes, including 128B)",
            "model_description": "Finetuned and prompted models evaluated for per-step correctness of scratchpad intermediate outputs for parity and variable-assignment tasks.",
            "arithmetic_task_type": "Stepwise parity or variable-value comments generation.",
            "mechanism_or_representation": "If model implements a sequential algorithm, per-step error should grow predictably with step index or input length; deviations reveal attention/representation failures.",
            "probing_or_intervention_method": "Plot per-step accuracies across step index for varying input (in-distribution and OOD) lengths; compare zero-shot scratchpad finetuned models to few-shot finetuned ones.",
            "performance_metrics": "Zero-shot scratchpad finetuned models show abrupt per-step error increase when evaluated on OOD lengths; few-shot finetuned 128B model displayed roughly constant per-step error rate across steps in some settings (indicating better sequential behavior).",
            "error_types_or_failure_modes": "Per-step errors increasing with OOD length indicate inability to maintain correct attention/state across many steps; constant per-step error suggests per-step error rate as bottleneck for long extrapolation (geometric decay of success).",
            "evidence_for_mechanism": "Per-step accuracy plots (Figure 6) demonstrate that OOD input length causes errors even on early steps and that few-shot conditioning can alter per-step error dynamics.",
            "counterexamples_or_challenges": "Even when per-step error is roughly constant, overall success declines exponentially with number of steps; thus reducing per-step error is essential but not always sufficient for long extrapolation.",
            "uuid": "e8395.10",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Shuffle-ops baseline",
            "name_full": "Shuffled-operations baseline for variable assignment",
            "brief_description": "A baseline dataset variant where the order of program operations is randomized to remove sequential dependency, used to test whether models rely on sequential execution or spurious correlations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (422M example reported)",
            "model_description": "Transformers finetuned on variable assignment data where operation order was shuffled during training; compared to models trained on clean ordered data.",
            "arithmetic_task_type": "Variable assignment predictions without sequential dependency.",
            "mechanism_or_representation": "If model attains similar OOD performance when trained on shuffled ops, it suggests reliance on non-sequential correlations rather than sequential execution.",
            "probing_or_intervention_method": "Train models on shuffled-ops dataset and compare OOD accuracy/length-generalization curves with those trained on ordered dataset.",
            "performance_metrics": "Shuffled-ops baseline displays OOD performance on par with models trained on clean ordered data; interestingly the shuffled baseline sometimes shows even more dramatic length-generalization deficiency.",
            "error_types_or_failure_modes": "Models trained on shuffled ops can only exploit spurious, non-sequential correlations and therefore fail to generalize to longer sequences requiring true sequential computation.",
            "evidence_for_mechanism": "The parity of OOD performance between shuffled and clean models indicates learned solutions rely on parallel/spurious features and not sequential algorithms.",
            "counterexamples_or_challenges": "This baseline confirms the hypothesis that transformers prefer parallel strategies; however few-shot prompting can still elicit sequential behavior in pretrained models.",
            "uuid": "e8395.11",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "The eos decision and length extrapolation",
            "rating": 1
        },
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "rating": 1
        },
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot reasoning",
            "rating": 1
        },
        {
            "paper_title": "Unveiling transformers with lego: a synthetic reasoning task",
            "rating": 1
        }
    ],
    "cost": 0.017433499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring Length Generalization in Large Language Models</h1>
<p>Cem Anil ${ }^{\text {1, }}$, Yuhuai $\mathrm{Wu}^{2}$, Anders Andreassen ${ }^{1}$, Aitor Lewkowycz ${ }^{1}$<br>Vedant Misra ${ }^{1}$, Vinay Ramasesh ${ }^{1}$, Ambrose Slone ${ }^{1}$, Guy Gur-Ari ${ }^{1}$, Ethan Dyer ${ }^{1}$, Behnam Neyshabur ${ }^{1}$<br>${ }^{1}$ Google Research, Blueshift Team<br>${ }^{2}$ Google Research<br>${ }^{3}$ University of Toronto, Vector Institute</p>
<h4>Abstract</h4>
<p>The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.</p>
<h2>1 Introduction</h2>
<p>Many natural problems, such as theorem proving and program synthesis, have a notion of length that strongly correlates with the difficulty of the task. However, in these domains, the number of available problems typically drops rapidly as a function of problem length (e.g. Figure 2). Hence, it is desirable to learn from examples of shorter lengths to generalize to longer ones or at least reduce the number of samples required for longer examples. We refer to this type of problem as length generalization.</p>
<p>Recent work on large language models (LLMs) has shown consistent improvement in their performance by scaling model and dataset size. However, such models are still incapable of length generalization. For example, [1] shows that even though scale helps with solving arithmetic problems, scale alone is likely insufficient for learning to solve instances of arbitrary lengths. This implies that models fail to learn the general algorithms that would enable this kind of generalization. Indeed, Razeghi et al. [2] showed that the performance of LLMs on mathematical calculations correlates with term frequency in the training data. This suggests that LLMs might have gained their current performance from surface-level memorization instead of learning to apply the correct algorithm.</p>
<p>A recent line of work proposes to use a scratchpad, or chain-of-thought reasoning, when prompting LLMs [3, 4, 5] on multi-step tasks. Breaking down tasks into multiple small steps and presenting these steps to the model leads to improved performance across a variety of reasoning tasks including word problems, arithmetic, and code execution.</p>
<p>We perform a systematic study of length generalization with transformer-based large language models. We consider problems in which learning an algorithm can in principle enable a model to extrapolate</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples of variable assignment problems: Can transformer language models learn from short instances of the Variable Assignment task (left) to extrapolate to much longer instances (right)? Length generalization is the ability to learn from shorter/easier instances of a problem to handle longer/harder instances.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Techniques</th>
<th style="text-align: center;">In-distribution</th>
<th style="text-align: center;">Out-of-distribution</th>
<th style="text-align: center;">Improves with scale</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fine-tune</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Prompting</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune + Prompting</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune + Scratchpad</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Prompting + Scratchpad</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune + Prompting + Scratchpad</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark^{+}$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance on length generalization tasks of three techniques that language models admit: (1) Finetuning, (2) Prompting (or in-context few-shot learning) and (3) Scratchpad (Chain-of-Thought reasoning). We find that each technique (and the combinations thereof) have different modes of failure and present different trade-offs regarding in and out-of-distribution coverage. $\times$ signifies poor $\checkmark$ signifies nontrivial, $\checkmark \checkmark$ signifies near-perfect performance. (*) Refers to task-dependency.
from short examples to problems of arbitrary length. In particular, we focus on two simple algorithmic tasks, parity and variable assignment, in which the model needs to keep track of a state in order to extrapolate to longer lengths (see Figure 1). These problems are illuminating because their simplicity allows us to probe the failure modes as well as contrast the learned solutions with the ground truth algorithm. They provide us with a setting to study how/when these large language models start to fail.</p>
<p>We study combinations of three kinds of techniques for LLMs: finetuning, few shot prompting (also referred to as in-context learning), and use of a scratchpad (also referred to as chain-of-thought), to understand the role of each method and the interplay among the three in length generalization. Interestingly, we observe non-trivial interactions among the three techniques; see Table 1.</p>
<p>Contributions Our main contributions are as follows:</p>
<ul>
<li>We define and characterize the problem of length generalization using notions such as state tracking, execution depth, and per-step error rate. We study and carefully design two tasks, parity and variable assignment, that measure length generalization (Section 2).</li>
<li>We find that in the finetuning regime, scaling data, model sizes, and compute does not improve length generalization (Section 3.1). We also observe that even when the model attains perfect in-distribution accuracy, it performs poorly in out-of-distribution domains. Surprisingly, different hyperparameter choices for finetuning have a large effect on length generalization performance, while having minimal effect on the final in-distribution performance (Section 3.3).</li>
<li>We establish finetuning with scratchpad also fails to generalize to longer problems, in contrast to what is suggested by previous works [3]. We look into three potential failure cases: positional encoding, the presence of distractors, and end of token prediction, and conclude that distractors are the main culprit of failures for length generalization (Section 4).</li>
<li>We show that in the in-context learning regime, use of a scratchpad shows a qualitatively different behavior and significantly alleviates the decay of performance on longer problems. This capability is significant, as it implies that for LLMs, there are certain skills, like length generalization, that can be learned through in-context learning rather than through finetuning even in the presence of infinite data. This is in stark contrast to the common norms of machine learning (Section 5).</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Real world datasets have heavy tails in length: (left) Histogram of lengths for proofs presented in the Archive of Formal Proofs (right) Histogram of the number of tokens for solutions in the MATH dataset. [6]</p>
<h1>2 Length Generalization</h1>
<p>Many sequence tasks-especially ones that require reasoning capabilities-have problem instances that differ in terms of their lengths. Shorter instances are often easier to state, process, and handle, and require less compute to find the answer. By contrast, longer instances are more challenging to parse and require more compute to solve. Tasks that have a reasoning component are especially well represented in this category - multi-hop reasoning [7], program execution [8], deductive reasoning [9] and theorem proving [10], to name a few. Note that having to deal with differing problem lengths poses two significant challenges. First, it is often the case that one encounters longer problem instances than the ones ever encountered during training, and is required to extrapolate. Second, even though longer problem instances have much more variety, real-world datasets often contain few long instances (see Figure 2). Both of these challenges are exacerbated if learning agents are not able to generalize across and beyond the lengths they learn from during training. This paper is about investigating to what extent transformer based language models are able to observe short problem instances and extrapolate to longer ones.
Instance Length as Number of Steps in a Markov Process It is possible to define problem length in many different ways to capture different aspects of problem difficulty. Does there exist a notion of length that would expose the same length-generalization-related problem structure observed in qualitatively very different settings? Such a framing would enable researchers to design algorithms and interventions that have the potential to generalize across a broad range of tasks. To this end, we take the approach of characterizing length in the context of a deterministic Markov process. From this perspective, length is simply the number of state transitions experienced by an initial world state. In other words, the data-generation process can be described as sampling an (1) initial state and a (2) variable number of state transformations to be applied sequentially on the initial state. The agent is provided both the initial state and the transformations, and is asked to predict the final state. This framing applies to a wide range of sequence problems, if not all of them-ranging from more mechanical tasks such as code and algorithm execution and theorem proving, to less structured tasks, such as solving math problems and summarizing novels.
In our empirical investigation we focus on two synthetic tasks: parity and variable assignment. These tasks avoid problem-specific subtleties that could mislead our analyses, while strongly capturing the deterministic Markov process structure.</p>
<h3>2.1 Tasks</h3>
<p>Parity: The parity task is an age-old learning problem that requires the trained agent to predict whether a bit-string has an even or odd number of ones in it. For example, the parity of the bitstring $[0,1,1,0,1]$ is "odd" (or 1) as opposed to "even" (or 0 ), because there is an odd number of 1 s in the bit-string. The parity task admits a sequential solution that enables length generalization in a straightforward way: simply process the bits left-to-right and record the parity of the bits processed so far as the state. The default notion of length in the parity task is the number of bits in the input. However, we also experiment with a version where the number of bits is kept constant, and the number of 1 s (i.e. the parity flipping bit) is systematically varied. The number of 1 s stands for the number of state changes contained in the input bit-string, and actually appears to capture a more relevant notion of length for transformer models (see Section 3.1).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Finetuned Length Generalization performance doesn't improve with scale: Models of vastly different scales fail at length generalization on both Parity and Variable Assignment tasks, displaying identical generalization pathologies. The x -axis represents problem length and the y -axis represents the accuracy attained at that problem length. The training lengths are highlighted in grey.</p>
<p>Boolean Variable Assignment Task: The Boolean Variable Assignment task is designed to capture arbitrarily long, potentially branching unidirectional execution flows. An instance of this task can be seen in Figure 1. The inputs consist of semantically correct (i.e. bug-free) Python programs in which each line contains a boolean variable assignment operation. The output is simply the value of the variable presented in the final line of the program. The sequential solution to this task is to simply execute the program line by line while keeping track of the state of all variables.</p>
<p>The data generation procedure involves randomly generating execution flows that involve Boolean operations; see Supplementary Material (SM) for details.</p>
<p>We focus our evaluations on two variants of this dataset. (1) The diverse variable assignment split consists of a wide range of boolean operators available and is intended to contain maximally diverse programs. (2) The chain-like variable assignment split consists only of operations that compose the values of already defined variables. This results in long chains of dependencies between the initial values of the variables and the queried one, ensuring that there are almost no redundant operations in the program (i.e. operations that can be removed without affecting the output of the program). This split emphasizes the sequential nature of the variable assignment problem.</p>
<h1>3 Standard Finetuning Fails at Length Generalization</h1>
<p>We begin by demonstrating that finetuning transformer models on length-generalization tasks results in poor out-of-distribution performance. In experiments we use LaMDA ${ }^{2}$ decoder-only models. These checkpoints were trained using general natural language data. We use the AdaFactor optimizer [11] during finetuning, and tune the learning rate, batch size and dropout. We trained the networks until the in-distribution validation accuracy settles (20000 gradient steps for parity and 18000 gradient steps for variable assignment). The loss was only computed on the target tokens (i.e. the model wasn't trained to model the input questions).</p>
<h3>3.1 Scale Doesn't Improve Length Generalization</h3>
<p>Parity: We finetuned four pretrained LaMDA models with $244 \mathrm{~m}, 422 \mathrm{~m}, 1 \mathrm{~b}$ and 64 b parameters on the parity task, where the training distribution included randomly sampled bitstrings of length 10 to 21. We then evaluated the performance on bitstrings of length 3 to 40; see Figure 3. We find that model scale has a little effect on length generalization.
Variable Assignment: We finetuned the same models on the chain-like Variable Assignment Task, described in Section 2. We kept the in-distribution lengths at 3 to 8 , and evaluated the test performance on lengths 3 to 19. The results can be seen in Figure 3. Just like in the parity task, while the indistribution performance is (near) perfect, out-of-distribution performance degrades rapidly as length increases. To get a sense of just how weak the out-of-distribution performance is, we also trained a 422 m model on the same dataset, except we shuffled the operations before feeding it to the model. This removes the sequential dependency between the operations, and helps us establish a</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Scratchpad finetuning displays poor length generalization: Scratchpad finetuning displays qualitatively similar length generalization pathologies as vanilla finetuning. The x-axis represents problem length and the y-axis represents the accuracy attained at that problem length. The training lengths are highlighted in grey.</p>
<p>strong baseline that only predicts the answers based on non-sequential, spurious correlations. The accuracy-length curves for the baseline can be found in SM.</p>
<h3>3.2 Transformers Prefer Parallel Strategies over Sequential Ones</h3>
<p>The results presented in Section 3.1 establish that, when presented with sequential length generalization problems, transformers are biased toward learning non-sequential "shortcut" solutions that fail at longer problem instances. We ran additional experiments to gain a better understanding of the nature of this generalization pattern.</p>
<p>On parity, we ran finetuning on a different distribution of bit-strings: Instead of first randomly sampling the number of bits in the input bit-string, then sampling the values of the bits, we fixed the total number of bits in the input, and only varied the <em>number of ones</em> in the bit-string uniformly. We trained with 10 to 20 ones in the input distribution and tested on an interval containing 1 to 30 ones. This makes sure that the number of tokens (now fixed at 30) is now disambiguated from number of state changes, which for parity is equal to the number of ones. The difference between in and out-of-distribution performance is even starker for this data distribution (Figure 5): while in-distribution performance was 100%, OOD performance was roughly equivalent to random prediction<sup>3</sup>. This suggests that the transformers are learning a non-sequential solution that involves counting the number of ones in the input, and then thresholding the output. This is not surprising, given that self-attention is an equivariant transformation capable of performing pooling operations like max-pooling [12]. This strategy doesn't allow for knowledge transfer between problems of different lengths. Note that this bottom-up counting behaviour is complementary to the left-to-right counting behaviour displayed by recurrent models Suzgun et al. [13]. On the variable assignment dataset, we finetuned a 255m LaMDA model on the <em>diverse</em> split of the variable assignment dataset of programs up to 16 lines, and evaluated on the same data generating distribution up to 32 lines. We measured the evolution of the model's accuracy with respect to training iterations on different program lengths (quantified by number of lines). The results are in SM.</p>
<p>We again observed that a different notion of length (which we call <em>computational graph depth</em>) captures the difficulty of problem instances better than number of program operations. A variable assignment program can be represented as a computational graph where each node corresponds to a variable, and each edge corresponds to an operation. Computational graph depth is the length of the longest dependency chain that connects to the queried variable node. This notion of length corresponds to the highly parallelizable strategy of executing programs by iteratively resolving computational graph dependencies. We present two results that suggest that computational graph depth is a more relevant notion of length for transformers. (1) Inspecting the order of problem instances in which the trained transformer correctly solves this task, we find that performance is strongest on examples with small computational graph depth, even if these examples are long in</p>
<p><sup>3</sup>The periodic 0% and near 100% performance on OOD lengths is due to the models' tendency to output 0 or 1 depending on whether the input has a significantly higher ratio of 0s or 1s. On average, the accuracy on OOD length is not better than random guess.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: (left) Complete lack of length generalization: Transformers trained on the parity task have difficulty generalizing to bit-strings that have a different number of $1 s$. (right) Sensitivity to hyperparameters: Trained networks sharing architecture, data and in-distribution loss can have very different length generalization performances. $l r$ stands "learning rate" and $b s$ stands for "batch size".
terms of number of operations. (2) The transformer does a good job of handling programs with an out-of-distribution number of operations, but for which computational graph depth is in-distribution.</p>
<h1>3.3 In-Distribution Generalization Doesn't Predict OOD Generalization on Length Generalization Tasks</h1>
<p>Prior work on out-of-distribution generalization establishes that in many tasks, in-distribution loss is a strong predictor of out-of-distribution generalization [14]. Our experiments on the parity task indicate that the distribution shift induced by changing problem lengths falls outside of the this category. Figure 5 shows how the same model trained on the same data achieving roughly the same in-distribution cross entropy loss behaves on OOD data, where the difference is solely induced by the choice of different hyperparameters.</p>
<h2>4 Scratchpad Finetuning Still Fails at Length Generalization</h2>
<p>It has been shown in prior work that it's possible to get pretrained LLMs to solve a given task by not only outputting the answer, but also the solution steps behind it. Nye et al. [15] use scratchpad finetuning to achieve strong in-distribution performance on execution based tasks such as code execution and computing polynomials. While they also report modest length generalization results on integer arithmetic, we find that scratchpad finetuning suffers from similar length generalization pathologies than vanilla finetuning does. The results on parity and variable assignment tasks can be seen in Figure 4. The precise scratchpad strategies used for these tasks are described in detail in SM.</p>
<p>Error analysis: To understand the causes of failure in training scratchpad strategies, we focused on two architectural choices that could account for the poor performance: (1) how transformers encode position information, and (2) whether the transformers are trained to predict an end-of-sequence (EOS) token. LaMDA models use T5 position biases [16] to handle position information. If the network is only trained with short instances, position biases that handle longer positional distances might not be trained, explaining poor length generalization. Similarly, Newman et al. [17] report that networks trained with EOS token prediction often suffer from generalizing to longer problem instances, because of the models' tendency to emit EOS tokens prematurely, as well as the EOS tokens' effect on the representations that get learned.</p>
<p>We tested the extent to which these effects can explain lack of length generalization as follows. We padded both the input bit-strings and the scratchpad content with dummy padding tokens to make the token count the same. We also augmented the input and scratchpad targets with the same number of padding tokens on the left and right so that the relevant bit to attend to when executing the sequential scratchpad strategy corresponds to the same T5 position bias bin. Examples of the updated input-target pairs can be seen in SM. While this intervention helps, the trained models still display significant length generalization issues.</p>
<p>To gain further insight about the source of the problem, we plotted how the scratchpad target prediction error rates change as a function of (1) how far along one is in constructing the scratchpad, and (2) the length of the input bit-string. The results can be seen in Figure 6. The fact that the model makes</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: (left) Effect of input length on per-step scratchpad accuracy: Points corresponds to the accuracy (y-axis) of the first $x$ scratchpad steps (x-axis) on parity instances of variable length (color). If the input length is out-of-distribution, even in-distribution scratchpad steps are inaccurate, implying the model hasn't learned an attention pattern that generalizes to longer bit-strings. (right) Roughly constant per-step error rate: The per-step error rates of the LaMDA 128b model, few-shot finetuned on the coin-flip version of the parity task remain roughly constant across the scratchpad steps. This is in stark contrast with zero-shot scratchpad finetuned models, where the per-step error rates increase abruptly when the model is evaluated on OOD lengths.
mistakes in in-distribution scratchpad steps when the input has an OOD length implies that the attention mechanism isn't capturing the relevant part of the input to form the scratchpad output. See SM for additional analysis.</p>
<h1>5 Scratchpad Prompting Significantly Improves Length Generalization</h1>
<p>Wei et al. [4], Nye et al. [15] and Lewkowycz et al. [5] showed that combining prompting (i.e. in-context learning) with scratchpad strategies present a powerful combination. They demonstrate that pretrained LLMs, without the help of any finetuning, can solve grade school math word problems and execute pieces of code with nontrivial correctness [15], when prompted with the right scratchpad strategy. We corroborate these findings, and report that scratchpad prompting endows pretrained LLMs with the capability of variable length template matching (see Figure 8). That is, in-context learning enables the model to "learn" solution steps from a small number of short instances, and apply the same template on significantly longer instances with a high degree of accuracy.</p>
<h3>5.1 Few-shot scratchpad</h3>
<p>Contrary to vanilla and scratchpad finetuning, we find that under the right conditions, few-shot scratchpad strategies sometimes significantly improves LLMs' capability to extrapolate to lengths much further than what pretraining weights grant them.</p>
<p>To evaluate the performance of few-shot conditioning with scratchpad inputs without any finetuning, we phrase the parity problem in natural language as a coin flipping task. An example for the few-shot prompts we used can be seen in Figure 8. Wei et al. [4] also report results on the coin-flip task: the scratchpad format we used differs from theirs in that while ours respects the sequential nature of the task (i.e. each coin flip corresponds to a step in the scratchpad solution), Wei et al. [4]'s scratchpad strategy involves summing up the number of coin flips, then deciding on the final output based on the evenness/oddness of the sum. Also, while they only test up to 4 flips, we go up to 20 flips while still attaining highly nontrivial accuracy levels.</p>
<p>For the variable assignment task, our scratchpad strategy involves copying over the program that's being executed, with comments added in between lines specifying the value of the variable that was assigned in the line above. Instances of this scratchpad strategy can be seen in SM.</p>
<p>Figure 7 shows the performance of the pretrained LaMDA 128b model on the coin-flip version of the parity task. Figure 8 shows an instance of how a length 3 prompt can induce the model to correctly output a 20 step scratchpad. We find that with the right scratchpad prompt, LLMs are able to generate correct scratchpad solutions. This reduces the problem to simply filling in the content of</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Few-shot finetuning with scratchpad displays qualitatively different behaviour on parity and variable assignment tasks. On parity, where the non-finetuned model already performs very well, few-shotfinetuning with scratchpad leads to a significant performance boost over zero-shot finetuning with scratchpad. On variable assignment, where the base model doesn't perform poorly, there's not a significant gap between few-shot finetuning and zero-shot finetuning with scrathpad. The performance of OpenAI's Codex model [18] on the variable assignment task is also provided.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Few-shot length generalization: The largest LaMDA model is able to map the scratchpad solution template from a few short exemplars onto much longer queries.</p>
<p>the generation correctly by inferring the right state transitions without having to figure out how to extrapolate the solution template.</p>
<p>Few-Shot Finetuning with Scratchpad Strategies: Does combining finetuning, few-shot prompting, and scratchpad strategies improve length generalization?</p>
<p>We find that the answer is <strong>yes</strong> in the case of parity. As seen in Figure 7, few-shot finetuning performs significantly better than the baseline model, both on in- and out-of-distribution lengths. Note that the vanilla (i.e. no shot) finetuning baseline also outperforms the no-finetuning baseline, it actually does worse on the larger lengths — a pathology that doesn't appear with few-shot finetuning.</p>
<p>The results point to a qualitatively different picture for the variable assignment task. Both few-shot finetuning and vanilla finetuning result in similar length generalization behavior (Figure 7). We hypothesize that this distinction is caused by the different pretrained performances that the model displays on these tasks: while length generalization is already strong with no finetuning on parity, that's not the case for variable assignment. In the latter case, the model is forced to acquire a new skill via finetuning, which displays the same pathologies as zero-shot finetuning with scratchpad. As a sanity check, we evaluated the (few-shot) finetuned performance of the pretrained model on an alternative, synthetic prompt style that yields poor performance without any pretraining: As expected by the aforementioned hypothesis, we observed that the few-shot finetuned model on this task also shows significant length generalization pathologies. The results can be found in SM. We leave a more rigorous evaluation of this hypothesis as future work.</p>
<h1>6 Related Works</h1>
<p>There have been many attempts to study generalization from shorter/easier to longer/harder examples.
Challenges in length generalization: Several existing works have investigated pathologies that arise when models are asked to generalize to processing and generating longer (measured by number of tokens) sequences. Newman et al. [17] find that sequence models trained with and in the absence of the end-of-sequence token display qualitatively different length extrapolation behaviour and learn different representations. Dubois et al. [19] proposes modifications to the commonly used dot-product attention to improve the models' ability to extrapolate to longer sequences. Murray and Chiang [20] demonstrate that neural machine translation models tend to have a bias towards generating shorter-than-desired translations. Yehudai et al. [21] show that length generalization issues are also present in training graph neural networks, where extrapolating across graph size presents a challenge. Ju et al. [22] propose a new attention mechanism to facilitate recurrent processing in transformer models. Press et al. [23] propose modifying transformer attention biases to facilitate generalization beyond the training context length. Concurrent work [24] propose a synthetic dataset named LEGO (Learning Equality and Group Operations), an instantiation of which resembles our variable assignment task where the only boolean operations allowed are assign and negate and assign, and overriding the values of variables is not allowed. Their analyses on OOD generalization largely complement ours: while we focus on decoder-only architectures and scratchpad strategies as a way of carrying over state, they focus on encoder-only architectures, and investigate the effect of weight-sharing.
Easy-to-Hard generalization: Schwarzschild et al. [25] and Bansal et al. [26] use weight-tied neural networks to generalize from easy to hard examples. Schwarzschild et al. [25] also provide three tasks to benchmark easy-to-hard generalization. Dehghani et al. [27] and Kaiser and Sutskever [28] assess the capabilities of their proposed architectures on easy-to-hard generalization problems.
Inductive Biases Related to Lenght Generalization: McCoy et al. [29] study the inductive bias of seq-to-seq learners on English question formation and English tense reinflection tasks and find that LSTM and GRU networks often display differing strategies, caused by the use of differing activation functions. Suzgun et al. [13] find that recurrent networks can perform dynamical counting, and encode hierarchical representations, which enables them to solve nontrivial Dyck tasks using k-counters. Kharitonov and Chaabouni [30] also study the inductive bias of different architectures, and conclude that transformer and LSTM architectural have a tendency to learn hierarchical strategies, whereas CNN based strategies display more linear structure. He et al. [31] propose a method to learn natural inference models that are not biased on spurious correlations. McCoy et al. [32] show that transformer models that display strong performance in natural language inference can have superficial biases that fool them in systematic ways and proposes a framework to think about these biases.</p>
<h2>7 Conclusion</h2>
<p>The ability to learn from shorter/easier problem instances to generalize to longer/harder ones is a key capability in a large number of tasks, especially ones requiring reasoning. We defined the concept of length generalization and measured language models' length generalization capabilities. After conducting careful experiments using finetuning, scratchpads, and few-shot prompting, we reached the following conclusions: (1) Generalizing in length is a challenge for language models at least up to the 100B parameter scale. Both vanilla finetuning and finetuning with scratchpads suffer from a lack of length generalization caused by models' tendency to pick up non-sequential pattern that don't apply to longer problem instances. (2) Few-shot scratchpad prompting enables pretrained large language models to pick up scratchpad-templates that extrapolate to arbitrary lengths, leading to dramatic improvements on longer problem instances. Unlike raw finetuning, this approach does scale with model size [4]. (3) Trying to further enhance the performance of few-shot scratchpad prompted LLMs via finetuning yields mixed results, depending on the non-finetuned performance of the base model at the target task. We emphasize that the aforementioned few-shot variable length pattern matching capability - something that doesn't require changing model architecture - offers a qualitatively different approach to handle length generalization in contrast to prior art that introduced architectural modifications to achieve the same goal. This capability is also significant in that it implies that for LLMs, there are certain skills, like length generalization, that can be learned better through in-context learning rather than through finetuning, even in the presence of infinite data.</p>
<h1>References</h1>
<p>[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[2] Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.
[3] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models, 2022. URL https://arxiv.org/abs/2201.11903.
[5] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.
[6] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[7] Haoyu Wang, Mo Yu, Xiaoxiao Guo, Rajarshi Das, Wenhan Xiong, and Tian Gao. Do multi-hop readers dream of reasoning chains? arXiv preprint arXiv:1910.14520, 2019.
[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.
[9] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. arXiv preprint arXiv:2002.05867, 2020.
[10] Yuhuai Wu, Albert Qiaochu Jiang, Jimmy Ba, and Roger Grosse. Int: An inequality benchmark for evaluating generalization in theorem proving. arXiv preprint arXiv:2007.02924, 2020.
[11] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR, 2018.
[12] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, pages 3744-3753. PMLR, 2019.
[13] Mirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov, and Stuart M Shieber. Lstm networks can perform dynamic counting. arXiv preprint arXiv:1906.03648, 2019.
[14] Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure modes of out-of-distribution generalization. arXiv preprint arXiv:2010.15775, 2020.
[15] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.
[17] Benjamin Newman, John Hewitt, Percy Liang, and Christopher D Manning. The eos decision and length extrapolation. arXiv preprint arXiv:2010.07174, 2020.</p>
<p>[18] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[19] Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to longer sequences. arXiv preprint arXiv:1911.03872, 2019.
[20] Kenton Murray and David Chiang. Correcting length bias in neural machine translation. arXiv preprint arXiv:1808.10006, 2018.
[21] Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, and Haggai Maron. From local structures to size generalization in graph neural networks. In International Conference on Machine Learning, pages 11975-11986. PMLR, 2021.
[22] Da Ju, Stephen Roller, Sainbayar Sukhbaatar, and Jason Weston. Staircase attention for recurrent processing of sequences. arXiv preprint arXiv:2106.04279, 2021.
[23] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0.
[24] Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022.
[25] Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural Information Processing Systems, 34, 2021.
[26] Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Logical extrapolation without overthinking. arXiv preprint arXiv:2202.05826, 2022.
[27] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.
[28] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.
[29] R Thomas McCoy, Robert Frank, and Tal Linzen. Does syntax need to grow on trees? sources of hierarchical inductive bias in sequence-to-sequence networks. Transactions of the Association for Computational Linguistics, 8:125-140, 2020.
[30] Eugene Kharitonov and Rahma Chaabouni. What they do when in doubt: a study of inductive biases in seq2seq learners. arXiv preprint arXiv:2006.14953, 2020.
[31] He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting the residual. arXiv preprint arXiv:1908.10763, 2019.
[32] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019.
[33] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Parity problem instances: (left) A sample 8-bit parity problem instance, along with the scratchpad targets. The scratchpad represent the intermediate parity state as the sequence is processed left to right. (right) A parity problem instance with the padded scratchpad strategy. Both the input and the scratchpad targets are padded left and right with the same number of padding tokens, such that the relevant bit to attend to while constructing the scratchpad bits is always equidistant even when there are different number of bits in the input.</p>
<h1>A Data Generation Details</h1>
<p>We describe the data generation procedures for the parity and variable assignment tasks in detail.</p>
<h2>A. 1 Parity Datasets:</h2>
<p>Synthetic Parity Dataset: A 8-bit example of the synthetic parity example, along with the corresponding scratchpad targets, can be seen in Figure 9. We added the prefix " $&gt;&gt;&gt;$ " to signify the start of the parity sequence, and the suffix "==" to signify the start of the target or scratchpad tokens. There's no special meaning associated with the particular prefixes and suffixed used.</p>
<p>We experimented with two version of the synthetic parity dataset: In one split, we varied the number of bits in the input, and in the other one, we varied the number of ones.</p>
<ul>
<li>Varied number of bit split: To generate the samples in this split, we first sampled the number of bits, then sampled each bit individually from a uniform Bernoulli distribution. For training, we used lengths between 3 and 20, and for validation/testing, we used lengths between 3 and 40 .</li>
<li>Varied number of ones split: Here, we fixed the number of bits at 30. To sample each instance, we first uniformly sampled the number of ones, then randomly placed each one in the fixed-length bitstring by randomly shuffling the bits. We used 10 to 20 ones in the training split, and 1 to 30 ones in the validation/test splits.
Padded scratchpad: The padded scratchpad format can be seen in 9. Both the input and the scratchpad targets are padded left and right with the same number of padding tokens respectively, such that the relevant bit to attend to while constructing the scratchpad bits is always equidistant. Moreover the total number of characters/tokens is also kept constant. The number of tokens to pad on the left and right is determined (uniformly) randomly.</li>
</ul>
<p>The parity datasets contain 1000000 samples.
Natural Language Parity Dataset: In order to tap into the natural language understanding capabilities of pretrained language models, we situated the parity task as a "coin flip problem". In this framing, flipping a coin corresponds to 1 and not flipping a coin corresponds to 0 . To make the inputs as close as possible to English without occupying too many tokens, we used the sentence templates "Then <NAME> flips." and "Then <NAME> doesn't flip." to represent whether the coin was flipped or not respectively, where "<NAME>" refers to a randomly sampled given name. We also prepended each step with an integer id that count backwards from the total number of steps there are in the input sequence. We've experimented with versions where the integer ids are incremented. This didn't lead to a significant difference in the overall performance.</p>
<p>Two representative sample input-target pairs (including the exemplars) are provided in Figure 10.</p>
<h2>A. 2 Boolean Variable Assignment Dataset:</h2>
<p>An instance of the variable assignment dataset can be seen in Figure 11. The data generation procedure is aimed at synthesizing large number of qualitatively different programs: (1) A subset of boolean variable assignment operations is uniformly sampled from a large pool of operations. (2) The number</p>
<p>Input:
Question The coin is heads up. (4) Then Williams flips. (3) Then Ward flips. (2) Then Valentine doesn't flip. (1) Then Son doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Williams flips, coin turns to tails. (3) After Ward flips, coin becomes heads. (2) Valentine doesn't flip, so coin stays heads. (1) Son doesn't flip, so coin remains heads. DONE
#
Question The coin is heads up. (4) Then Shade flips. (3) Then Kong doesn't flip. (2) Then Kodi flips. (1) Then Charleston flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Shade flips, coin turns to tails. (3) Kong doesn't flip, so coin stays tails. (2) After Kodi flips, coin becomes heads. (1) After Charleston flips, coin turns to tails. DONE
#
Question The coin is heads up. (4) Then Ka doesn't flip. (3) Then Justice flips. (2) Then Johan flips. (1) Then Jamaica flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) Ka doesn't flip, so coin remains heads. (3) After Justice flips, coin becomes tails. (2) After Johan flips, coin turns to heads. (1) After Jamaica flips, coin becomes tails. DONE
#
Question The coin is heads up. (4) Then Cy flips. (3) Then Booker flips. (2) Then Ace doesn't flip. (1) Then Ren flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Cy flips, coin turns to tails. (3) After Booker flips, coin becomes heads. (2) Ace doesn't flip, so coin stays heads. (1) After Ren flips, coin turns to tails. DONE #
Question The coin is heads up. (6) Then Tristan flips. (5) Then Hillary flips. (4) Then Olivia flips. (3) Then Rosa doesn't flip. (2) Then Kurt flips. (1) Then Glenn doesn't flip. Is the coin still heads up?"</p>
<p>Scratchpad targets:
"Solution Coin is initially heads up. (6) After Tristan flips, coin becomes tails. (5) After Hillary flips, coin turns to heads. (4) After Olivia flips, coin becomes tails. (3) Rosa doesn't flip, so coin remains tails. (2) After Kurt flips, coin turns to heads. (1) Glenn doesn't flip, so coin stays heads. DONE</p>
<p>Input:
Question The coin is heads up. (4) Then Katarina doesn't flip. (3) Then January flips. (2) Then Duke flips. (1) Then Cal doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) Katarina doesn't flip, so coin remains heads. (3) After January flips, coin becomes tails. (2) After Duke flips, coin turns to heads. (1) Cal doesn't flip, so coin stays heads. DONE
#
Question The coin is heads up. (4) Then Berry flips. (3) Then Abbi flips. (2) Then Tam flips. (1) Then Ikea doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Berry flips, coin becomes tails. (3) After Abbi flips, coin turns to heads. (2) After Tam flips, coin becomes tails. (1) Ikea doesn't flip, so coin remains tails. DONE #
Question The coin is heads up. (4) Then Cain doesn't flip. (3) Then Woody flips. (2) Then Von doesn't flip. (1) Then Thu doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) Cain doesn't flip, so coin stays heads. (3) After Woody flips, coin turns to tails. (2) Von doesn't flip, so coin remains tails. (1) Thu doesn't flip, so coin stays tails. DONE
#
Question The coin is heads up. (4) Then Russ flips. (3) Then Williams flips. (2) Then Ward doesn't flip. (1) Then Valentine flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Russ flips, coin becomes tails. (3) After Williams flips, coin turns to heads. (2) Ward doesn't flip, so coin remains heads. (1) After Valentine flips, coin becomes tails. DONE
#
Question The coin is heads up. (8) Then Melissa flips. (7) Then Kevin doesn't flip. (6) Then Steven flips. (5) Then Thomas flips. (4) Then Timothy doesn't flip. (3) Then Kyle doesn't flip. (2) Then Rachel doesn't flip. (1) Then Laura doesn't flip. Is the coin still heads up?</p>
<h1>Scratchpad targets:</h1>
<p>Solution Coin is initially heads up. (8) After Melissa flips, coin turns to tails. (7) Kevin doesn't flip, so coin stays tails. (6) After Steven flips, coin becomes heads. (5) After Thomas flips, coin turns to tails. (4) Timothy doesn't flip, so coin remains tails. (3) Kyle doesn't flip, so coin stays tails. (2) Rachel doesn't flip, so coin remains tails. (1) Laura doesn't flip, so coin stays tails. DONE</p>
<p>Figure 10: Natural Language parity problem instances: The coin flip task - which consists of tracking the state of a coin as it undergoes a number of flip or no-flip operations - shares the same underlying problem structure as the parity task.
of operations and number of variables (along with their names, which are single-letter characters) are uniformly sampled based on pre-set hyperparameters. (3) One by one, operations and the variables included in the operations are sampled, while making sure that each added operations retains the semantic correctness of the program.
We now outline the hyperparameters used to generate the chain-like and diverse splits.</p>
<h2>Chain-like split:</h2>
<ul>
<li>Boolean operators: assign to and with another variable, assign to or with another variable, assign to xor with another variable, negate</li>
<li>Minimum/maximum number of operations: 3, 19</li>
<li>Minimum/maximum number of variables in the program: 2, 3</li>
</ul>
<h2>Diverse split:</h2>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Variable assignment problem instance: A problem instance from the Boolean Variable Assignment dataset. The scratchpad strategy consists of outputting the value of the recenly updated variable in form of comments. Note that both the input and the scratchpad are valid Python programs.</p>
<ul>
<li>Boolean operators: assign to and with another variable, assign to or with another variable, assign to xor with another variable, negate, assign to and with boolean, assign to or with boolean, assign to xor with a boolean, conditional assign to a boolean, assign to another variable, conditional assign to another variable</li>
<li>Minimum/maximum number of operations: 8,32</li>
<li>Minimum/maximum number of variables in the program: 4, 10</li>
</ul>
<p>The scratchpad format (seen in Figure 11) consists of copying over the input program with comments after each line specifying the value of the recently updated variable. This ensures that the scratchpad itself is a valid Python program and can be used with models pretrained with Python data.
Both splits have 1500000 samples.</p>
<h1>B Baseline for Vanilla Finetuning on Variable Assignment</h1>
<p>Just how weak are the vanilla finetuned models on the OOD lengths on the variable assignment task? We trained baseline models with the same parameter count on a modified version of the variable assignment dataset where the order of the operations were randomly shuffled. While this leaves in some of the spurious features that correlate with the right answer, it completely eliminates the possibility of running a sequential algorithm to get to the final answer.
The results can be found in Figure 12. On OOD data, the performance of the shuffled-ops baseline is on par with the models trained with the clean version of the dataset. Note that the length generalization deficiency that the shuffled ops baselines display is even more dramatic than that of the models'</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Results of finetuning on the shuffled-ops variable assignment dataset: We trained baseline models with the same parameter counts on a modified version of the variable assignment dataset where the order of the operations were randomly shuffled. The generalization gap between in and out-of-distribution data persists here as well, due to transformers' tendency to prefer parallel strategies that don't generalize to larger lengths.
trained with clean data. This is not surprising, as the primary source of lack of length generalization is the transformers' tendency to prefer parallel strategies that don't generalize to larger lengths over picking up sequential algorithm</p>
<h1>C Experimental Conditions</h1>
<p>shuff We outline the training conditions and hyperparameter used in the main experiments.
In our experiments on different datasets, we first did a learning rate sweep over different model sizes, and preferred the largest learning rates that ensured training stability. This is due to the fact that we observed the best length generalization when we used larger learning rates (see Figure 5). We used the AdaFactor optimizer in all of our finetuning experiments [11]. We didn't use dropout [33] in the parity experiments and used a dropout rate of 0.05 in the variable assignment experiments. Our initial experiments suggest that one can often reach similar in and out-of-distribution performance either by training the weights from scratch, or finetuning from the pretrained weights. To keep the experiments consistent, we always initialized training using the pretrained weights. In variable assignment, we did a learning rate sweep over $0.0033,0.00033$ and 0.000033 . For parity, we search over learning rate velus of $0.002,0.0002$ and 0.00002 . We used a constant learning rate profile all throughout learning. Due to memory constraints, we used a batch size of 32 when training the $64 b$ and $128 b$ models.</p>
<p>We used greedy decoding in all of our experiments (including few-shot scratchpad ones). We experimented with temperature sampling with reranking based on sentence likelihoods, but found that doing this doesn't lead to qualitatively different results.</p>
<h2>D Computational Graph Depth is the Relevant Notion of Difficulty on Variable Assignment</h2>
<p>Computational graph depth captures a more relevant notion of difficulty on the variable assignment task for transformers. In Figure 13, we show how the accuracy of a transformer model evolves on samples of problem instances with different computational graph depths (left), and how the same</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: Evolution of Performance over Training Iterations on Different Computational Graph Depths: Computational graph depth corresponds to the length of the longest dependency chain linking to the queried variable in the variable assignment task. This quantity captures a more suitable notion of length/difficulty for transformer models. On the left plot, we show how the accuracy of a transformer model evolves for problem instances with different computational graph depths. In the middle, we show the same, except that we constrain the number of operations in the program to an out-of-distribution number. The accuracy values roughly remain unchanged, indicating that it's not the number of operations, but computational graph depth that determines the difficulty of a problems instance. On the right, we show the evolution of accuracy on instances with different number of operations for reference.
quantity behaves if we fix the number of operations in a program at an out-of-distribution length (middle) ${ }^{4}$. Two takeaways from this analysis are:</p>
<ul>
<li>The fact that the accuracy values on samples with different computational graph depths roughly remain unchanged on OOD program lengths indicates that it's not the number of operations, but computational graph depth that captures a more relevant notion of difficulty.</li>
<li>Transformers initially pick up how to handle programs with a small computational graph depth throughout training and then move to more difficult programs.</li>
</ul>
<h1>E Effect of Prompt Style on Few-Shot Finetuning Performance</h1>
<p>In Section 5.1, we hypothesized that few-shot finetuning only leads to significant improvements in length generalization performance if the non-finetuned performance on the same task already at a nontrivial level. To provide a sanity check for this, we ran few-shot finetuning using an alternative prompt style for the coin-flip task that yields poor non-finetuned performance. As can be seen in Figure 14, the few-shot finetuned performance shows significant length generalization pathologies. We leave a systematic study of how prompt style affects length generalization as future work.</p>
<h2>F Distractor Analysis for Scratchpad Strategies</h2>
<p>Our analysis in Section 4 indicates that length generalization pathologies persist even when we use the padded scratchpad strategy that makes sure that it's not untrained position encodings and/or the EOS token prediction that causes the aforementioned pathologies. This points to the fact that the transformer doesn't learn to attend to the "right" section of the input and scratchpad that implements the sequential strategy that generalizes to longer lengths - it's thrown off by distractor tokens in the input and/or the preceding scratchpad targets. The distractor tokens at which section of the transformer context window (input or scratchpad) contribute more to the performance deterioration? If we remove all the distractor tokens, can we achieve perfect length generalization?
To answer these questions, we trained four transformer models under the following conditions: (1) We used the padded scratchpad strategy described in Section 4 with no modification, (2) We manually masked the preceding scratchpad tokens that don't contribute to the correct sequential algorithm (i.e.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: Effect of prompt style on few-shot finetuning performance: we evaluated the (few-shot) finetuned performance of the pretrained model on an alternative, synthetic prompt style that yields poor performance without any pretraining. We observed that the few-shot finetuned model on this task also shows significant length generalization pathologies. This is in line with our hypothesis that the non-finetuned performance of the base model should be non-trivial for few-shot finetuning to consistently yield strong length generalization results.
we masked the distractor tokens in the input), (3) We masked the input tokens that don't contribute to the correct sequential algorithm, (4) we masked the distractor tokens in both the input and the preceding distractor tokens. To give an example, let's say the input bitstring is "[1 1011 ]", and the model has emitted the scratchpad tokens "[100]" so far. Masking the distracting sratchpad tokens simply means replacing all but the last scratchpad token with dummy padding tokens: "[x x 0]". Similarly, removing the distracting input tokens corresponds to masking out the part of the input that the network doesn't need to attend to while predicting the next bit: "[x x x 1 x]". Masking both corresponds to removing the distractor tokens in both the input and the target, so that the input and the preceding scratchpad become "[x x x 1 x]" and "[x x 0]" respectively.</p>
<p>The results can be seen in Figure 15. For all four experimental conditions, we plotted the accuracy of the trained models on predicting the scratchpad tokens at different steps for inputs of varying (in and OOD) lengths. We conclude from this experiment that:</p>
<ul>
<li>Removing all distractor tokens does result in perfect length generalization.</li>
<li>The distracting input tokens are hurt length generalization performance more.</li>
</ul>
<p>Based on this analysis, we conclude that innovations in transformer architectures and/or training methodology/objective that alleviate the issues caused by distractor tokens have a chance at significantly improving length generalization.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: Distractor analysis: We trained scratchpad-augmented transformers to solve the parity task, where we systematically masked out the tokens in the input (left bottom) and the scratchpad (right top) that don't need to be attended to while implementing the correct sequential algorithm that solves parity. The plots illustrate the accuracy of the trained models on predicting the scratchpad tokens at different steps for inputs of varying (in and OOD) lengths. This analysis shows that (1) removing all distracting tokens leads to perfect length generalization (right bottom), (2) the distractor tokens in the input contribute more to the length generalization pathologies (right top versus left bottom).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ The longest in-distribution number of operations is 15 , and we fix this quantity at 20 in the middle plot.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
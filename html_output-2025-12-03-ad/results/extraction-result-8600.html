<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8600 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8600</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8600</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278740960</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.11854v1.pdf" target="_blank">Evaluating the Logical Reasoning Abilities of Large Reasoning Models</a></p>
                <p><strong>Paper Abstract:</strong> Large reasoning models, often post-trained on long chain-of-thought (long CoT) data with reinforcement learning, achieve state-of-the-art performance on mathematical, coding, and domain-specific reasoning benchmarks. However, their logical reasoning capabilities - fundamental to human cognition and independent of domain knowledge - remain understudied. To address this gap, we introduce LogiEval, a holistic benchmark for evaluating logical reasoning in large reasoning models. LogiEval spans diverse reasoning types (deductive, inductive, analogical, and abductive) and task formats (e.g., logical sequence, argument analysis), sourced from high-quality human examinations (e.g., LSAT, GMAT). Our experiments demonstrate that modern reasoning models excel at 4-choice argument analysis problems and analogical reasoning, surpassing human performance, yet exhibit uneven capabilities across reasoning types and formats, highlighting limitations in their generalization. Our analysis reveals that human performance does not mirror model failure distributions. To foster further research, we curate LogiEval-Hard, a challenging subset identified through a novel screening paradigm where small-model failures (Qwen3-30B-A3B) reliably predict difficulties for larger models. Modern models show striking, consistent failures on LogiEval-Hard. This demonstrates that fundamental reasoning bottlenecks persist across model scales, and establishes LogiEval-Hard as both a diagnostic tool and a rigorous testbed for advancing logical reasoning in LLMs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8600.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8600.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 671B-parameter Mixture-of-Experts reasoning model trained with heavy reinforcement learning on long chain-of-thought data, evaluated on the LogiEval logical-reasoning benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MoE architecture (671B total parameters) built on DeepSeek V3; key training innovation is extensive reinforcement learning for long chain-of-thought (long-CoT) reasoning and formal verification style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>671B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiEval (comprehensive logical reasoning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A 6,235-instance bilingual benchmark assembled from high-stakes human examinations (LSAT, GMAT, Civil Service, etc.) covering four reasoning types (deductive, inductive, analogical, abductive) and 10 task formats (e.g., syllogism, argument analysis, artificial language).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with minimal-design few-shot prompts; models trained with reinforcement learning on long chain-of-thought data (post-training) are assessed; answers extracted by regex exact-string matching. DeepSeek-R1 benefits from RL-based long-CoT and formal-verification style training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Total accuracy on LogiEval: 81.41% (Table 2). By reasoning type: Abductive 77.32%, Analogical 89.60%, Deductive 78.16%, Inductive 81.49% (Table 3). On selected task formats: Textual entailment 83.77%, Argument analysis 81.20%, Syllogism 73.38% (Table 2/3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms several other evaluated models on total score (highest reported total in Table 3) and shows clear gains on textual entailment and analogical tasks compared to smaller models; compared to human baselines, models exceed humans on some 4-choice argument-analysis items but differ non-monotonically across difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Uneven performance across formats: formal deductive tasks (syllogisms) and some abductive/situational items remain challenging. Exhibits the benchmark's reported inverse difficulty correlation (solving some problems humans find hard while failing mid-difficulty items).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Reinforcement-learning long-CoT training yields strong, balanced performance across reasoning types, but RL+scaling does not eliminate systematic failures in formal deductive reasoning; LogiEval reveals remaining bottlenecks despite high aggregate accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Abilities of Large Reasoning Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8600.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8600.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QWEN3-235B-A22B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QWEN3-235B-A22B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 235B-total / 22B-activated-parameter Mixture-of-Experts model with hybrid thinking modes that allow variable amounts of internal 'thinking', evaluated on LogiEval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QWEN3-235B-A22B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MoE model (235B total parameters, 22B activated) from the Qwen family; supports hybrid thinking modes to control amount of internal computation/chain-of-thought at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>235B (22B active)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same comprehensive human-exam-derived benchmark covering multiple logical reasoning types and task formats.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with minimal-design prompts and few-shot development examples; uses hybrid thinking modes at inference to adjust internal reasoning depth.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Total accuracy on LogiEval: 78.74% (Table 2). By reasoning type: Abductive 85.95%, Analogical 90.54% (best analogical in Table 3), Deductive 72.57%, Inductive 78.61% (Table 3). Task-format highlights: Argument analysis 87.72%, Artificial language 80.99%, Syllogism 56.93% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Competitive with other large models on analogical and argument-analysis tasks (top analogical performer), but lower on deductive tasks relative to DeepSeek-R1. Shows that hybrid thinking modes can yield strengths on analogy and abductive tasks but do not uniformly improve deductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relatively weak on formal deductive formats (syllogisms and some multi-option deductive items) compared to its analogical strength; evidence of format-specific saturation and brittle mid-difficulty behavior vs. humans.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Hybrid thinking-mode control supports strong analogical/abductive performance but does not resolve systemic deductive/syllogistic weaknesses; scaling and thinking-mode controls produce uneven gains across logical subdomains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Abilities of Large Reasoning Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8600.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8600.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 32B-parameter reasoning model from Team Qwen notable for strong few-shot reasoning performance despite relatively smaller size; used as one of the evaluated systems on LogiEval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>32B-parameter reasoning model (QwQ-32B) that uses reinforcement-learning based techniques for reasoning capabilities; marketed as first strong reasoning model from Qwen.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Comprehensive human-exam-derived logical reasoning benchmark across four reasoning types and ten task formats.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with standardized minimal prompts and regex-based answer extraction; benefits from RL-based post-training (per family description).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Total accuracy on LogiEval: 80.34% (Table 2). By reasoning type: Abductive 82.68%, Analogical 87.85%, Deductive 77.21%, Inductive 80.69% (Table 3). Task-format highlights: Argument analysis 88.19%, Artificial language 54.52% (lowest on that format), Syllogism 73.46% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performs near other large models in total score despite being smaller than the largest MoE models, supporting that architecture and training (RL) strongly influence logical performance beyond raw parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Large variance across formats—very low on artificial language in this evaluation—and shows similar vulnerabilities in deductive/syllogistic items and the LogiEval-Hard subset (paper reports persistent cross-model failures identified via small-model screening).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>A mid-sized RL-tuned model can reach competitive aggregate reasoning performance, but persistent format-specific failures suggest architectural/training limits not solved by moderate scaling alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Abilities of Large Reasoning Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8600.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8600.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen3-30B-A3B (diagnostic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen3-30B-A3B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compact diagnostic Qwen model (3B active parameters) used as a small-model probe to screen LogiEval problems and construct LogiEval-Hard by selecting items the small model consistently fails.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen3-30B-A3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A compact Qwen model variant with 3B active parameters used in this work as a diagnostic probe (hosted locally for extended experiments) to identify universally hard examples via repeated failure.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B (active)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiEval (screening to build LogiEval-Hard)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to identify instances that reliably trigger failures (3 trials with majority-wrong consensus) to produce LogiEval-Hard — a subset of LogiEval where large models also fail.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Small-model screening methodology: run multiple trials, mark examples with consistent failures, then aggregate these to form a 'hard' diagnostic subset (LogiEval-Hard).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported as a single aggregate in the paper; used diagnostically. The model's consistent failures were used to select 1,617 hard examples (LogiEval-Hard).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Its failure-patterns aligned strongly with larger models' failures: the paper reports that a high fraction of these small-model failures (e.g., 82.3% alignment reported with GEMINI2.0) also perplexed larger models, demonstrating predictive power versus purely scaling-based baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>As a compact diagnostic model it fails many logically hard items (by design for screening); specific per-format accuracy not provided. The method assumes small-model failures indicate structural difficulty rather than dataset artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Small-model screening is an effective a priori technique to identify universal reasoning bottlenecks that persist across scales; this suggests some logical difficulties are architectural/training blind spots not remedied by scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Abilities of Large Reasoning Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8600.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8600.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEMINI2.0 Flash Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GEMINI2.0 Flash Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's 32B-parameter reasoning model in 'Flash Thinking' mode, evaluated on LogiEval and the LogiEval-Hard subset; shows high standard-set performance but severe failures on the Hard subset in formal logic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GEMINI2.0 Flash Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary 32B-parameter reasoning model from Google with a 'Flash Thinking' mode designed for stronger reasoning; used via API/ChatUI for evaluation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiEval and LogiEval-Hard</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>LogiEval: broad exam-derived benchmark covering multiple logical forms. LogiEval-Hard: a 1,617-instance subset identified by small-model screening intended to expose universal reasoning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with minimal-design prompts, few-shot examples per task-format on LogiEval; specifically evaluated on the LogiEval-Hard subset to reveal persistent failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LogiEval total accuracy: 80.77% (Table 2). On LogiEval-Hard (1,617 items) GEMINI2.0 achieved 37.97% overall; on specific formats in the Hard set it scored 16.00% on syllogisms and 22.73% on blood relations; textual-entailment subset accuracy 28.00% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Although GEMINI2.0 performs near other SOTA models on standard LogiEval (∼80%), it collapses on LogiEval-Hard, indicating that standard benchmarks can mask core deductive weaknesses; its 37.97% on Hard is far below its standard benchmark accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Striking failures on formally-structured deductive tasks (e.g., syllogisms) and blood-relation problems within the Hard set; consistent with the paper's claim that scaling and high standard-set scores don't eliminate certain logical bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Demonstrates that a model can show strong aggregate exam-style performance yet fail systematically on logically-structured hard examples; supports the paper's conclusion that small-model screening reveals cross-scale logical bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Abilities of Large Reasoning Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8600.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8600.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.7 Sonnet Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.7 Sonnet Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's most recent Claude model variant with a 'Sonnet Thinking' mode, evaluated on LogiEval across formats and reasoning types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.7 Sonnet Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary Anthropic model (parameter count not disclosed) with an explicit thinking mode; accessed via API/ChatUI for evaluation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Comprehensive bilingual exam-derived logical reasoning benchmark spanning deductive, inductive, analogical, and abductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with minimal-design prompting and few-shot dev examples; relies on provider API to run reasoning-mode responses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Total accuracy on LogiEval: 79.06% (Table 2). By reasoning type: Abductive 81.10%, Analogical 85.22%, Deductive 77.36%, Inductive 79.17% (Table 3). Task-format highlights: Argument analysis 85.70%, Essential part 100%, Syllogism 70.61% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Comparable to other proprietary SOTA models; outperforms humans on some 4-choice argument-analysis items but shows gaps on structured deductive formats relative to the best-performing systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lower performance on certain deductive and abductive Hard-set items; exhibits the overall unevenness across formats noted in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Proprietary thinking modes deliver high performance on many exam-derived tasks, but like other models, Claude-3.7 shows systematic gaps in formal deductive reasoning and on the LogiEval-Hard subset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Abilities of Large Reasoning Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8600.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8600.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grok3 (Think)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grok3 (Think)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>xAI's advanced reasoning model optimized for test-time compute and reasoning ('Think' mode), evaluated on LogiEval and showing high performance in some inductive and artificial-language tasks but poor results on syllogisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grok3 (Think)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary xAI model with 'Think' mode optimized for efficient reasoning at test time; exact size not specified in the paper's evaluation table headings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Human-exam-derived benchmark covering multiple forms of logical reasoning and task formats.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated via provider API using minimal prompts and few-shot dev examples; 'Think' mode emphasizes internal reasoning computation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Total accuracy on LogiEval: 79.59% (Table 2). By reasoning type: Abductive 84.50%, Analogical 87.50%, Deductive 73.74%, Inductive 85.62% (Table 3). Task-format highlights: Artificial language 91.67% (best on that format), Syllogism 51.86% (very low) (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Excels in artificial-language (coding/decoding) and inductive tasks compared to many peers, but underperforms substantially on syllogisms and some deductive formats, highlighting a tradeoff between domain-specific tuning and formal logic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Very poor performance on syllogisms (51.86%) compared to its strong artificial-language scores; indicates specialization can produce brittle performance on formal logical formats.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Specialized reasoning optimizations (e.g., for coding/decoding analogies) can yield strong gains on some logical tasks but leave residual brittleness on formal deductive reasoning; highlights the need for methods beyond format-specific tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Abilities of Large Reasoning Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8600.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8600.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI o4-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o4-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI 'o' series reasoning model tuned for longer thinking before responding, evaluated on LogiEval and achieving high accuracy on multiple-choice argument-analysis but middling results on syllogisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o4-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's recent 'o' series reasoning model intended to produce longer internal thinking chains; exact architecture/parameters not disclosed in the paper (proprietary).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A comprehensive logical reasoning benchmark assembled from human examinations testing deductive, inductive, analogical, and abductive reasoning in diverse formats.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with minimal-design prompts and few-shot dev examples; answer extraction via regex-based exact matching. Model allegedly trained to 'think' longer before responding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Total accuracy on LogiEval: 79.81% (Table 2). By reasoning type: Abductive 87.67% (best abductive in Table 3), Analogical 86.17%, Deductive 74.70%, Inductive 84.06% (Table 3). Task-format highlights: Argument analysis 89.90% (top on that format), Syllogism 54.85% (near-random on 5-option syllogisms) (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms humans on 4-choice argument-analysis items (human 85.2% vs model 89.9% here) and is among the top performers on argument-analysis; however its syllogism/deductive performance lags behind some models like DeepSeek-R1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Notable weakness on multi-option syllogisms (near-random performance on 5-option questions) and uneven mid-difficulty brittleness vs. human baselines; shows the saturation effect for multiple-choice formats that mask deeper reasoning limits.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Tuning to encourage longer internal 'thinking' improves performance on some formats but fails to resolve formal deductive weaknesses; multiple-choice saturation suggests the need for richer, harder diagnostics (e.g., LogiEval-Hard).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Logical Reasoning Abilities of Large Reasoning Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LogiQA <em>(Rating: 2)</em></li>
                <li>ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning <em>(Rating: 2)</em></li>
                <li>RuleTaker <em>(Rating: 2)</em></li>
                <li>PrOntoQA <em>(Rating: 2)</em></li>
                <li>LogicNLI <em>(Rating: 2)</em></li>
                <li>CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text <em>(Rating: 2)</em></li>
                <li>GLoRE: Evaluating Logical Reasoning of Large Language Models <em>(Rating: 1)</em></li>
                <li>LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8600",
    "paper_id": "paper-278740960",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1",
            "brief_description": "A 671B-parameter Mixture-of-Experts reasoning model trained with heavy reinforcement learning on long chain-of-thought data, evaluated on the LogiEval logical-reasoning benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1",
            "model_description": "MoE architecture (671B total parameters) built on DeepSeek V3; key training innovation is extensive reinforcement learning for long chain-of-thought (long-CoT) reasoning and formal verification style tasks.",
            "model_size": "671B",
            "reasoning_task_name": "LogiEval (comprehensive logical reasoning benchmark)",
            "reasoning_task_description": "A 6,235-instance bilingual benchmark assembled from high-stakes human examinations (LSAT, GMAT, Civil Service, etc.) covering four reasoning types (deductive, inductive, analogical, abductive) and 10 task formats (e.g., syllogism, argument analysis, artificial language).",
            "method_or_approach": "Evaluated with minimal-design few-shot prompts; models trained with reinforcement learning on long chain-of-thought data (post-training) are assessed; answers extracted by regex exact-string matching. DeepSeek-R1 benefits from RL-based long-CoT and formal-verification style training.",
            "performance": "Total accuracy on LogiEval: 81.41% (Table 2). By reasoning type: Abductive 77.32%, Analogical 89.60%, Deductive 78.16%, Inductive 81.49% (Table 3). On selected task formats: Textual entailment 83.77%, Argument analysis 81.20%, Syllogism 73.38% (Table 2/3).",
            "baseline_comparison": "Outperforms several other evaluated models on total score (highest reported total in Table 3) and shows clear gains on textual entailment and analogical tasks compared to smaller models; compared to human baselines, models exceed humans on some 4-choice argument-analysis items but differ non-monotonically across difficulty levels.",
            "limitations_or_failures": "Uneven performance across formats: formal deductive tasks (syllogisms) and some abductive/situational items remain challenging. Exhibits the benchmark's reported inverse difficulty correlation (solving some problems humans find hard while failing mid-difficulty items).",
            "insights_or_conclusions": "Reinforcement-learning long-CoT training yields strong, balanced performance across reasoning types, but RL+scaling does not eliminate systematic failures in formal deductive reasoning; LogiEval reveals remaining bottlenecks despite high aggregate accuracy.",
            "uuid": "e8600.0",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "QWEN3-235B-A22B",
            "name_full": "QWEN3-235B-A22B",
            "brief_description": "A 235B-total / 22B-activated-parameter Mixture-of-Experts model with hybrid thinking modes that allow variable amounts of internal 'thinking', evaluated on LogiEval.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QWEN3-235B-A22B",
            "model_description": "MoE model (235B total parameters, 22B activated) from the Qwen family; supports hybrid thinking modes to control amount of internal computation/chain-of-thought at inference time.",
            "model_size": "235B (22B active)",
            "reasoning_task_name": "LogiEval",
            "reasoning_task_description": "Same comprehensive human-exam-derived benchmark covering multiple logical reasoning types and task formats.",
            "method_or_approach": "Evaluated with minimal-design prompts and few-shot development examples; uses hybrid thinking modes at inference to adjust internal reasoning depth.",
            "performance": "Total accuracy on LogiEval: 78.74% (Table 2). By reasoning type: Abductive 85.95%, Analogical 90.54% (best analogical in Table 3), Deductive 72.57%, Inductive 78.61% (Table 3). Task-format highlights: Argument analysis 87.72%, Artificial language 80.99%, Syllogism 56.93% (Table 2).",
            "baseline_comparison": "Competitive with other large models on analogical and argument-analysis tasks (top analogical performer), but lower on deductive tasks relative to DeepSeek-R1. Shows that hybrid thinking modes can yield strengths on analogy and abductive tasks but do not uniformly improve deductive reasoning.",
            "limitations_or_failures": "Relatively weak on formal deductive formats (syllogisms and some multi-option deductive items) compared to its analogical strength; evidence of format-specific saturation and brittle mid-difficulty behavior vs. humans.",
            "insights_or_conclusions": "Hybrid thinking-mode control supports strong analogical/abductive performance but does not resolve systemic deductive/syllogistic weaknesses; scaling and thinking-mode controls produce uneven gains across logical subdomains.",
            "uuid": "e8600.1",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "QwQ-32B",
            "name_full": "QwQ-32B",
            "brief_description": "A 32B-parameter reasoning model from Team Qwen notable for strong few-shot reasoning performance despite relatively smaller size; used as one of the evaluated systems on LogiEval.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QwQ-32B",
            "model_description": "32B-parameter reasoning model (QwQ-32B) that uses reinforcement-learning based techniques for reasoning capabilities; marketed as first strong reasoning model from Qwen.",
            "model_size": "32B",
            "reasoning_task_name": "LogiEval",
            "reasoning_task_description": "Comprehensive human-exam-derived logical reasoning benchmark across four reasoning types and ten task formats.",
            "method_or_approach": "Evaluated with standardized minimal prompts and regex-based answer extraction; benefits from RL-based post-training (per family description).",
            "performance": "Total accuracy on LogiEval: 80.34% (Table 2). By reasoning type: Abductive 82.68%, Analogical 87.85%, Deductive 77.21%, Inductive 80.69% (Table 3). Task-format highlights: Argument analysis 88.19%, Artificial language 54.52% (lowest on that format), Syllogism 73.46% (Table 2).",
            "baseline_comparison": "Performs near other large models in total score despite being smaller than the largest MoE models, supporting that architecture and training (RL) strongly influence logical performance beyond raw parameter count.",
            "limitations_or_failures": "Large variance across formats—very low on artificial language in this evaluation—and shows similar vulnerabilities in deductive/syllogistic items and the LogiEval-Hard subset (paper reports persistent cross-model failures identified via small-model screening).",
            "insights_or_conclusions": "A mid-sized RL-tuned model can reach competitive aggregate reasoning performance, but persistent format-specific failures suggest architectural/training limits not solved by moderate scaling alone.",
            "uuid": "e8600.2",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen3-30B-A3B (diagnostic)",
            "name_full": "Qwen3-30B-A3B",
            "brief_description": "A compact diagnostic Qwen model (3B active parameters) used as a small-model probe to screen LogiEval problems and construct LogiEval-Hard by selecting items the small model consistently fails.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen3-30B-A3B",
            "model_description": "A compact Qwen model variant with 3B active parameters used in this work as a diagnostic probe (hosted locally for extended experiments) to identify universally hard examples via repeated failure.",
            "model_size": "3B (active)",
            "reasoning_task_name": "LogiEval (screening to build LogiEval-Hard)",
            "reasoning_task_description": "Used to identify instances that reliably trigger failures (3 trials with majority-wrong consensus) to produce LogiEval-Hard — a subset of LogiEval where large models also fail.",
            "method_or_approach": "Small-model screening methodology: run multiple trials, mark examples with consistent failures, then aggregate these to form a 'hard' diagnostic subset (LogiEval-Hard).",
            "performance": "Not reported as a single aggregate in the paper; used diagnostically. The model's consistent failures were used to select 1,617 hard examples (LogiEval-Hard).",
            "baseline_comparison": "Its failure-patterns aligned strongly with larger models' failures: the paper reports that a high fraction of these small-model failures (e.g., 82.3% alignment reported with GEMINI2.0) also perplexed larger models, demonstrating predictive power versus purely scaling-based baselines.",
            "limitations_or_failures": "As a compact diagnostic model it fails many logically hard items (by design for screening); specific per-format accuracy not provided. The method assumes small-model failures indicate structural difficulty rather than dataset artifacts.",
            "insights_or_conclusions": "Small-model screening is an effective a priori technique to identify universal reasoning bottlenecks that persist across scales; this suggests some logical difficulties are architectural/training blind spots not remedied by scale.",
            "uuid": "e8600.3",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GEMINI2.0 Flash Thinking",
            "name_full": "GEMINI2.0 Flash Thinking",
            "brief_description": "Google's 32B-parameter reasoning model in 'Flash Thinking' mode, evaluated on LogiEval and the LogiEval-Hard subset; shows high standard-set performance but severe failures on the Hard subset in formal logic tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GEMINI2.0 Flash Thinking",
            "model_description": "Proprietary 32B-parameter reasoning model from Google with a 'Flash Thinking' mode designed for stronger reasoning; used via API/ChatUI for evaluation in this paper.",
            "model_size": "32B",
            "reasoning_task_name": "LogiEval and LogiEval-Hard",
            "reasoning_task_description": "LogiEval: broad exam-derived benchmark covering multiple logical forms. LogiEval-Hard: a 1,617-instance subset identified by small-model screening intended to expose universal reasoning failures.",
            "method_or_approach": "Evaluated with minimal-design prompts, few-shot examples per task-format on LogiEval; specifically evaluated on the LogiEval-Hard subset to reveal persistent failure modes.",
            "performance": "LogiEval total accuracy: 80.77% (Table 2). On LogiEval-Hard (1,617 items) GEMINI2.0 achieved 37.97% overall; on specific formats in the Hard set it scored 16.00% on syllogisms and 22.73% on blood relations; textual-entailment subset accuracy 28.00% (Table 5).",
            "baseline_comparison": "Although GEMINI2.0 performs near other SOTA models on standard LogiEval (∼80%), it collapses on LogiEval-Hard, indicating that standard benchmarks can mask core deductive weaknesses; its 37.97% on Hard is far below its standard benchmark accuracy.",
            "limitations_or_failures": "Striking failures on formally-structured deductive tasks (e.g., syllogisms) and blood-relation problems within the Hard set; consistent with the paper's claim that scaling and high standard-set scores don't eliminate certain logical bottlenecks.",
            "insights_or_conclusions": "Demonstrates that a model can show strong aggregate exam-style performance yet fail systematically on logically-structured hard examples; supports the paper's conclusion that small-model screening reveals cross-scale logical bottlenecks.",
            "uuid": "e8600.4",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Claude-3.7 Sonnet Thinking",
            "name_full": "Claude-3.7 Sonnet Thinking",
            "brief_description": "Anthropic's most recent Claude model variant with a 'Sonnet Thinking' mode, evaluated on LogiEval across formats and reasoning types.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3.7 Sonnet Thinking",
            "model_description": "Proprietary Anthropic model (parameter count not disclosed) with an explicit thinking mode; accessed via API/ChatUI for evaluation in this paper.",
            "model_size": null,
            "reasoning_task_name": "LogiEval",
            "reasoning_task_description": "Comprehensive bilingual exam-derived logical reasoning benchmark spanning deductive, inductive, analogical, and abductive reasoning.",
            "method_or_approach": "Evaluated with minimal-design prompting and few-shot dev examples; relies on provider API to run reasoning-mode responses.",
            "performance": "Total accuracy on LogiEval: 79.06% (Table 2). By reasoning type: Abductive 81.10%, Analogical 85.22%, Deductive 77.36%, Inductive 79.17% (Table 3). Task-format highlights: Argument analysis 85.70%, Essential part 100%, Syllogism 70.61% (Table 2).",
            "baseline_comparison": "Comparable to other proprietary SOTA models; outperforms humans on some 4-choice argument-analysis items but shows gaps on structured deductive formats relative to the best-performing systems.",
            "limitations_or_failures": "Lower performance on certain deductive and abductive Hard-set items; exhibits the overall unevenness across formats noted in the paper.",
            "insights_or_conclusions": "Proprietary thinking modes deliver high performance on many exam-derived tasks, but like other models, Claude-3.7 shows systematic gaps in formal deductive reasoning and on the LogiEval-Hard subset.",
            "uuid": "e8600.5",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Grok3 (Think)",
            "name_full": "Grok3 (Think)",
            "brief_description": "xAI's advanced reasoning model optimized for test-time compute and reasoning ('Think' mode), evaluated on LogiEval and showing high performance in some inductive and artificial-language tasks but poor results on syllogisms.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Grok3 (Think)",
            "model_description": "Proprietary xAI model with 'Think' mode optimized for efficient reasoning at test time; exact size not specified in the paper's evaluation table headings.",
            "model_size": null,
            "reasoning_task_name": "LogiEval",
            "reasoning_task_description": "Human-exam-derived benchmark covering multiple forms of logical reasoning and task formats.",
            "method_or_approach": "Evaluated via provider API using minimal prompts and few-shot dev examples; 'Think' mode emphasizes internal reasoning computation.",
            "performance": "Total accuracy on LogiEval: 79.59% (Table 2). By reasoning type: Abductive 84.50%, Analogical 87.50%, Deductive 73.74%, Inductive 85.62% (Table 3). Task-format highlights: Artificial language 91.67% (best on that format), Syllogism 51.86% (very low) (Table 2).",
            "baseline_comparison": "Excels in artificial-language (coding/decoding) and inductive tasks compared to many peers, but underperforms substantially on syllogisms and some deductive formats, highlighting a tradeoff between domain-specific tuning and formal logic generalization.",
            "limitations_or_failures": "Very poor performance on syllogisms (51.86%) compared to its strong artificial-language scores; indicates specialization can produce brittle performance on formal logical formats.",
            "insights_or_conclusions": "Specialized reasoning optimizations (e.g., for coding/decoding analogies) can yield strong gains on some logical tasks but leave residual brittleness on formal deductive reasoning; highlights the need for methods beyond format-specific tuning.",
            "uuid": "e8600.6",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "OpenAI o4-mini",
            "name_full": "OpenAI o4-mini",
            "brief_description": "An OpenAI 'o' series reasoning model tuned for longer thinking before responding, evaluated on LogiEval and achieving high accuracy on multiple-choice argument-analysis but middling results on syllogisms.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI o4-mini",
            "model_description": "OpenAI's recent 'o' series reasoning model intended to produce longer internal thinking chains; exact architecture/parameters not disclosed in the paper (proprietary).",
            "model_size": null,
            "reasoning_task_name": "LogiEval",
            "reasoning_task_description": "A comprehensive logical reasoning benchmark assembled from human examinations testing deductive, inductive, analogical, and abductive reasoning in diverse formats.",
            "method_or_approach": "Evaluated with minimal-design prompts and few-shot dev examples; answer extraction via regex-based exact matching. Model allegedly trained to 'think' longer before responding.",
            "performance": "Total accuracy on LogiEval: 79.81% (Table 2). By reasoning type: Abductive 87.67% (best abductive in Table 3), Analogical 86.17%, Deductive 74.70%, Inductive 84.06% (Table 3). Task-format highlights: Argument analysis 89.90% (top on that format), Syllogism 54.85% (near-random on 5-option syllogisms) (Table 2).",
            "baseline_comparison": "Outperforms humans on 4-choice argument-analysis items (human 85.2% vs model 89.9% here) and is among the top performers on argument-analysis; however its syllogism/deductive performance lags behind some models like DeepSeek-R1.",
            "limitations_or_failures": "Notable weakness on multi-option syllogisms (near-random performance on 5-option questions) and uneven mid-difficulty brittleness vs. human baselines; shows the saturation effect for multiple-choice formats that mask deeper reasoning limits.",
            "insights_or_conclusions": "Tuning to encourage longer internal 'thinking' improves performance on some formats but fails to resolve formal deductive weaknesses; multiple-choice saturation suggests the need for richer, harder diagnostics (e.g., LogiEval-Hard).",
            "uuid": "e8600.7",
            "source_info": {
                "paper_title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LogiQA",
            "rating": 2
        },
        {
            "paper_title": "ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "RuleTaker",
            "rating": 2
        },
        {
            "paper_title": "PrOntoQA",
            "rating": 2
        },
        {
            "paper_title": "LogicNLI",
            "rating": 2
        },
        {
            "paper_title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text",
            "rating": 2,
            "sanitized_title": "clutrr_a_diagnostic_benchmark_for_inductive_reasoning_from_text"
        },
        {
            "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "rating": 1,
            "sanitized_title": "glore_evaluating_logical_reasoning_of_large_language_models"
        },
        {
            "paper_title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
            "rating": 1,
            "sanitized_title": "logicbench_towards_systematic_evaluation_of_logical_reasoning_ability_of_large_language_models"
        }
    ],
    "cost": 0.016363,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating the Logical Reasoning Abilities of Large Reasoning Models
17 May 2025</p>
<p>Hanmeng Liu liuhanmeng@hainanu.edu.cn 
Yiran Ding dingyiran@westlake.edu.cn 
Zhizhang Fu fuzhizhang@westlake.edu.cn 
Chaoli Zhang 
Xiaozhang Liu 
Yue Zhang zhangyue@westlake.edu.cn </p>
<p>Hainan University</p>
<p>Westlake University</p>
<p>Westlake University</p>
<p>Zhejiang Normal University</p>
<p>Hainan University</p>
<p>Westlake University</p>
<p>Evaluating the Logical Reasoning Abilities of Large Reasoning Models
17 May 2025DE209DBB09B75760362A91769B31D357arXiv:2505.11854v1[cs.AI]
Large reasoning models, often post-trained on long chain-of-thought (long CoT) data with reinforcement learning, achieve state-of-the-art performance on mathematical, coding, and domain-specific reasoning benchmarks.However, their logical reasoning capabilities-fundamental to human cognition and independent of domain knowledge-remain understudied.To address this gap, we introduce LogiEval, a holistic benchmark for evaluating logical reasoning in large reasoning models.LogiEval spans diverse reasoning types (deductive, inductive, analogical, and abductive) and task formats (e.g., logical sequence, argument analysis), sourced from high-quality human examinations (e.g., LSAT, GMAT).Our experiments demonstrate that modern reasoning models excel at 4-choice argument analysis problems and analogical reasoning, surpassing human performance, yet exhibit uneven capabilities across reasoning types and formats, highlighting limitations in their generalization.Our analysis reveals that human performance does not mirror model failure distributions.To foster further research, we curate LogiEval-Hard, a challenging subset identified through a novel screening paradigm where small-model failures (Qwen3-30B-A3B) reliably predict difficulties for larger models.Modern models show striking, consistent failures on LogiEval-Hard.This demonstrates that fundamental reasoning bottlenecks persist across model scales, and establishes LogiEval-Hard as both a diagnostic tool and a rigorous testbed for advancing logical reasoning in LLMs.</p>
<p>Introduction</p>
<p>Large language models (LLMs) with advanced reasoning capabilities-often termed reasoning language models or large reasoning models-have become pivotal in both industry (Guo et al., 2025;Team, 2025b;Anthropic, 2025;xAI, 2025;OpenAI, 2025) and academia (Muennighoff et al., 2025).These models acquire their "thinking" ability through post-training (Kumar et al., 2025) et al., 2025) or generated via reinforcement learning (RL) over action spaces (Xu et al., 2025).Such data enables LLMs to perform multi-step reasoning and plan for complex tasks.This approach, now widely adopted (Team, 2025a,b), enables models to generate intermediate reasoning steps-either implicitly or explicitly-before delivering final outputs.Despite their success, evaluation remains skewed toward domain-specific benchmarks: mathematical reasoning (Ye et al., 2025;Lightman et al., 2023), coding (Penedo et al., 2025;Jimenez et al., 2024), and knowledge-intensive tasks (Wang et al., 2024;Rein et al., 2024).A critical gap persists in assessing logical reasoning-a fundamental capability independent of domain knowledge-leaving the true generalization of these models unclear.</p>
<p>We aim to give a systematic empirical evaluation of reasoning language models on logical reasoning, covering inductive (Sinha et al., 2019), deductive (Saparov, He, 2023;Tian et al., 2021;Sanyal et al., 2022;Parmar et al., 2024), abductive (Del, Fishel, 2023;Nguyen et al., 2023), and analogical (Petersen, Plas van der, 2023;Qin et al., 2024;Wijesiriwardene et al., 2023) reasoning types -the four basic logical reasoning categories (Liu et al., 2025).To this end, several benchmark datasets are available in exam-originated (e.g., LSAT ,Civil Service tests) (Liu et al., 2021;Yu et al., 2020) or synthetically-generated (Sinha et al., 2019;Saparov, He, 2023) forms.However, existing benchmarks typically focus on one aspect of reasoning such as inductive (Sinha et al., 2019) or deductive (Saparov, He, 2023;Tian et al., 2021;Sanyal et al., 2022;Parmar et al., 2024) reasoning.In addition, these challenges are typically represented by a single problem format, mostly multi-choice question answering (Liu et al., 2021;Yu et al., 2020), which may be subject to data artifacts (Ye et al., 2024;Chen et al., 2025b).Finally, existing benchmark suits such as GLoRE (liu et al., 2023) andLogiTorch (Helwe et al., 2022) assemble multiple reasoning benchmarks for increased reliability, yet they lack a unified representation, and recent LLMs have achieved very high performances on these datasets (liu et al., 2025).</p>
<p>To address this gap, we introduce LogiEval, a comprehensive logical reasoning benchmark curated from diverse human examinations (e.g., LSAT, GMAT, Civil Service Exams), covering multiple reasoning types (deductive, inductive, analogical and abductive) and tasks (e.g., essential part, artificial language, syllogism) and problem formats(muti-choice QA, 3-way classification), providing a unified evaluation suite for assessing fundamental reasoning abilities.The benchmark consists of 6,235 instances in total.Some example questions are shown in Figure 2. We evaluate state-of-the-art models-including DeepSeek-R1 (Guo et al., 2025), Qwen3 (Team, 2025b), Claude-3.7-Sonnet(Anthropic, 2025), Grok-3 (xAI, 2025), Gemini-2.0-Flash-thinking,and OpenAI o4-mini (OpenAI, 2025)-revealing two key findings: (1) reasoning LLMs give varying performances across various tasks -while the essential part task is easily solved by most models, artificial language and syllogism tasks remain rather challenging.</p>
<p>(2) Different reasoning LLMs excel on different tasks and reasoning types, without a winner across the board.</p>
<p>(3) LogiEval is challenging to existing reasoning LLMs, with the best-performing model giving around 80% overall accuracy.(4) A subset of the most challenging questions to one model (i.e., QwQ-32B) is also the most challenging to all the other models, showing that there is some common challenge to existing reasoning LLMs.Accordingly, we extract the most difficult subset of LogiEval and name it LogiEval-Hard to further resolve the saturation issue.LogiEval is gated behind manual review to prevent adversarial optimization.We Overall, our contributions are: (1) We introduce LogiEval, a comprehensive logical reasoning benchmark curated from 7 high-stakes human examinations that unifies four fundamental reasoning types across 10 task formats -the first to combine diverse tasks and question formats into one evaluation suite.</p>
<p>(2) Through systematic evaluation of 7 cutting-edge reasoning models (2025 releases), we reveal critical gaps in LLM capabilities: while models exceed human performance on 4-choice argument analysis, they show catastrophic failures on syllogisms and exhibit inverse difficulty correlations, solving "hard" human problems while failing "medium" ones.(3) We develop a novel screening paradigm where small-model failures (Qwen3-30B-A3B) reliably predict universal challenges, producing LogiEval-Hard -the first benchmark subset where all models show striking, consistent failures (avg 37.97% accuracy), exposing fundamental reasoning bottlenecks that persist across model scales.</p>
<p>LogiEval</p>
<p>Our motivation for the benchmark is to cover all the question types from all sorts of examinations concerning logical reasoning.Other than testing only one unique aspect of logical reasoning with one benchmark, our intention is to give the LLM providers the freedom to get an integrated solution with our benchmark.Moreover, compared to the benchmarks that are commonly used for testing new models that require intensive domain knowledge, a key advantage of LogiEval is its domain-agnostic nature.By decoupling reasoning from specialized knowledge, it mirrors the fundamental cognitive abilities shared across humans-regardless of educational background-and provides a purer measure of a model's reasoning capacity.This makes LogiEval particularly valuable for evaluating whether LLMs can generalize logical principles beyond pattern recognition in narrow domains.</p>
<p>Data Collection and Curation</p>
<p>LogiEval is constructed from carefully selected logical reasoning sections of high-stakes human examinations, including the Chinese Civil Service Examination, Law School Admission Test (LSAT), Graduate Management Admission Test (GMAT), Banking Personnel Selection (IBPS), Common Admission Test (CAT), and several standardized IQ and aptitude tests.These examinations were chosen because they each contain dedicated logical reasoning components that have been rigorously developed to assess human reasoning capabilities.All questions were sourced from publicly available practice materials and are used strictly for academic research purposes.We maintain the original language of each examination (either English or Chinese) to preserve the linguistic nuances of logical reasoning tasks, avoiding potential biases introduced by translation (Liu et al., 2023;Song et al., 2025).This approach makes LogiEval a genuinely bilingual benchmark that can evaluate reasoning abilities across languages.The difficulty level of the benchmark faithfully reflects that of the original examinations, ensuring that it maintains the same discriminative power for evaluating reasoning capabilities as the tests demonstrate for human test-takers.By preserving both the content and difficulty characteristics of these established examinations, LogiEval provides an authentic assessment environment for evaluating large language models' logical reasoning abilities.</p>
<p>After de-duplication and validity verification, we obtain 6,235 high-quality problems, each annotated with gold labels.In addition to these labels, our dataset includes supplementary metadata such as difficulty level, human accuracy rate, and explanations, derived directly from post-exam statistics.To enable fine-grained analysis, we further annotate each question with its task format and reasoning type.Annotation details are provided in Appendix A Task formats are categorized into 10 distinct types based on question structure: logical sequence, essential part, artificial language (coding and decoding), blood relation, situation judgment, syllogism, definition matching, argument analysis, odd one out, and textual entailment.To our knowledge, we are the first to cover artificial language questions, essential part questions, and odd one out questions in logical reasoning.For reasoning types, we employ a hybrid annotation pipeline: Qwen3-30B-A3B first proposes one of four reasoning categories (analogical, deductive, inductive, or abductive), followed by human verification to mitigate potential model hallucinations.Three annotators independently review each label, with final assignments determined by majority vote.Figure 2 shows 4 examples from LogiEval representing each reasoning type.</p>
<p>The dataset is partitioned into a few-shot development set and a test set.The development set includes 5 representative examples per task format, accompanied by task-specific instructions to guide model adaptation.The test set comprises 6,174 problems, while the development set contains 65, ensuring robust evaluation across diverse reasoning scenarios.</p>
<p>Data Statistics</p>
<p>As shown in Table 1, LogiEval comprises 6,235 instances distributed across four reasoning types and ten task formats.Deductive reasoning constitutes the largest category with 3,681 instances, followed by inductive (1,214), abductive (961), and analogical reasoning (379).The task format distribution reveals textual entailment as the most prevalent (3,402 instances), followed by argument analysis (1,354), while niche formats like essential part (55) and odd one out (71) represent specialized challenges.The benchmark features diverse answer options ranging from 2 to 6 choices, with 3-option questions dominating (3,766), followed by 4-option (1,575) and 5-option formats (883).This composition ensures comprehensive evaluation across reasoning paradigms while maintaining examination authenticity through varied question structures.</p>
<p>3 Experiments and Results</p>
<p>Experimental Setup</p>
<p>We evaluate state-of-the-art large reasoning models released in 2025, all featuring advanced reasoning capabilities.Despite differences in scale (32B to 671B parameters), architecture (dense vs. MoE),</p>
<p>and training strategies (RL-based long CoT vs. hybrid thinking modes), these models rank among the top 50 in the LMSYS Chatbot Arena Leaderboard2 as of May 2025.</p>
<p>For consistency, we convert each instance into minimal-design prompts and extract answers using regex-based pattern matching (details in Appendix B).Accuracy is computed against gold labels, with task-specific normalization for multi-format evaluation.For consistent evaluation, each data instance is converted into a standardized, minimal-design prompt.</p>
<p>To extract answers from model responses, we apply exact string matching, following the approach of Er, Cicekli (2013).The extracted answers are then compared against the gold labels to compute accuracy.For model evaluation, we use the official API by the LLM provider.Apart from that, we host a Qwen3-30B-A3B model on a server with 4 Nvidia 80G VRAM H100 GPUs for extended experiments.</p>
<p>Open-weighted models Open-weighted models are those that have released their model checkpoints to the public.Users can deploy their reasoning models and have access to the thinking process.</p>
<p>We chose the following reasoning models: OPENAI O4-MINI is OpenAI's most recent release of its reasoning models.The o-series of models is trained to think for longer before responding.However, the original thinking process of these models is not observable to users.Along with o3, OpenAI claims they are the smartest models they've released to date.As no API has been provided by OpenAI for o3, we include o4-mini in our experiment.
DEEPSEEK
Human passing rate Human performance is benchmarked using historical passing rates from source examinations.These rates reflect real-world test-taker performance, providing a robust reference for model comparison.</p>
<p>Results</p>
<p>As shown in Table 2, our comprehensive evaluation reveals several critical insights into the logical reasoning capabilities of state-of-the-art models.Models consistently outperformed human test-takers on 4-choice argument analysis problems, with human accuracy at 85.2% compared to model performance ranging from 81.20% to 89.90%.Proprietary models like OpenAI o4-mini (89.90%) and Grok3-Think (88.22%) led in this category, aligning with prior observations of LLM overfitting to multiple-choice formats.The saturation effect-where all models cluster above 81% accuracy-suggests diminishing returns in using this format to distinguish reasoning capabilities.</p>
<p>Performance varied dramatically across different task formats, exposing fundamental gaps in reasoning skills.Structured deductive reasoning, such as syllogisms, proved particularly challenging, with Grok3-Think (51.86%) and OpenAI o4-mini (54.85%) performing near-random on 5-option questions, while DeepSeek-R1 achieved 73.38%, likely due to its RL-based training on formal verification tasks.Contextual analogical reasoning, like artificial language tasks, showed the highest variance (54.52%-91.67%),with Grok3-Think outperforming others by over 10 percentage points, suggesting specialized training on coding/decoding tasks.Resource-intensive formats like textual entailment (75.20%-83.77%)revealed clear scaling effects, with larger models like DeepSeek-R1 (83.77%) outperforming smaller ones like QwQ-32B (77.50%).</p>
<p>Notably, all models achieved 95%+ accuracy on essential part identification, with four models reaching 100%-surpassing human performance (92.3% historical average).This suggests either an inherent strength in component-based reasoning or that these tasks rely on predictable pattern recognition rather than genuine reasoning.</p>
<p>Error analysis revealed systematic failure patterns, with 18.3% of problems incorrectly answered by all models.Errors concentrated in abductive reasoning (32% of hard subset) and situational judgment tasks (27%), confirming that aggregate metrics mask critical reasoning deficiencies.LogiEval-Hard, our challenging subset, provides a targeted evaluation suite for these gaps, with baseline accuracies below 40% for all evaluated models.</p>
<p>These findings demonstrate that while modern reasoning models achieve strong examination performance through format-specific optimization, their logical reasoning capabilities remain uneven and task-dependent.LogiEval-Hard serves as a critical complement to existing benchmarks by focusing on persistent failure modes.</p>
<p>Discussion</p>
<p>Performance Across Reasoning Types</p>
<p>The results in Table 3 reveal distinct patterns in model performance across different reasoning types.Models demonstrate strong capabilities in abductive reasoning, with OpenAI o4-mini achieving the highest accuracy (87.67%) and Gemini2.0Flash Thinking close behind (86.94%).This suggests that current architectures are particularly adept at inference to the best explanation, a crucial skill for real-world problem-solving.For analogical reasoning, Qwen3-235B-A22B leads with 90.54% accuracy, followed by DeepSeek-R1 (89.60%), indicating that larger models may have an advantage in identifying and applying analogies.The relatively high performance across all models (83.10%-90.54%)suggests that analogical reasoning may be more accessible to current architectures compared to other reasoning types.</p>
<p>Deductive reasoning proves more challenging, with accuracies ranging from 72.57% to 78.16%.DeepSeek-R1 shows the strongest performance (78.16%), potentially benefiting from its reinforcement learning training on formal verification tasks.The narrower performance gap in this category suggests that deductive reasoning presents a more uniform challenge across models.</p>
<p>Inductive reasoning shows significant variation, with Grok3 (Think) performing best (85.62%) and Qwen3-235B-A22B the weakest (78.61%).The 7-point spread between top and bottom performers indicates that inductive reasoning capabilities may be more dependent on specific architectural choices or training approaches.</p>
<p>Overall, DeepSeek-R1 achieves the highest aggregate score (81.41%), demonstrating balanced performance across reasoning types.The close clustering of total scores (78.74%-81.41%)suggests that while individual strengths vary, current state-of-the-art models have reached similar overall levels of logical reasoning capability.However, the persistent gaps in specific reasoning types highlight areas needing further architectural innovation and training improvements.</p>
<p>LLM Reasoning vs. Human Reasoning</p>
<p>As shown in Table 4, we compute Wilson score intervals for binomial proportions and Fisher's exact tests for significance against human baselines.Our analysis reveals statistically significant differences between LLM and human reasoning patterns, demonstrating that models (1) outperform humans on challenging problems (85.71%vs 18% human accuracy, p=0.0032) yet fail at specific mid-difficulty points (0% accuracy at 31% human accuracy, p=1.0), (2) achieve perfect mastery (100% accuracy, p&lt;0.0001) for problems humans solve at 41-85% rates, while (3) showing unexpected vulnerabilities in mid-difficulty ranges (28.57% accuracy at 46% human accuracy, p=0.31), with all comparisons using Wilson score intervals and Fisher's exact tests, collectively indicating that LLMs develop nonmonotonic reasoning strategies that excel on extreme difficulties but exhibit brittleness on specific problem types unexplained by human performance metrics.</p>
<p>LogiEval-Hard: Predicting Universal Reasoning Challenges via Small-Model Screening</p>
<p>Whereas human examination performance doesn't mirror model failure distributions, we test whether small-model error patterns can forecast fundamental reasoning obstacles that persist at larger scales.</p>
<p>To systematically identify universal reasoning challenges independent of model scale, we develop a novel screening methodology using Qwen3-30B-A3B (3B active parameters) as a diagnostic probe.By analyzing problems where this compact model consistently fails across multiple reasoning attempts (3 trials with majority-wrong consensus), we construct LogiEval-Hard -a challenge set that enhances the benchmark's discriminative power.</p>
<p>The creation of LogiEval-Hard addresses a critical need in evaluating modern reasoning models by distinguishing true reasoning capabilities from pattern recognition.Table 5 shows the statistics.Overall, we have 1,617 hard examples.The composition of LogiEval-Hard shows a coverage across reasoning paradigms, with deductive reasoning dominating at 802 problems, reflecting its importance in formal logic applications, while textual entailment constitutes the largest task format with 982 cases that test core language understanding.The distribution presents challenging conditions with varied answer options, including 1,107 three-option and 295 five-option questions.As shown in Table 5, experiments with GEMINI2.0 FLASH THINKING reveal striking alignment: 82.3% of small-model failures simultaneously perplex this 32B-parameter state-of-the-art reasoner.This cross-scale consistency manifests most acutely in formal logic tasks, where GEMINI2.0 achieves only 16.00% accuracy on syllogisms and 22.73% on blood relations, despite its superior performance (87.01%overall) on standard LogiEval.</p>
<p>This approach reveals fundamental reasoning bottlenecks that transcend model scale, as subsequent evaluation shows problems challenging for compact models prove equally formidable for state-ofthe-art large reasoning models.The methodology demonstrates that reasoning difficulties rooted in logical structure rather than parametric capacity manifest consistently across model sizes, establishing small-model screening as an effective a priori technique for identifying universally challenging problems.This finding challenges conventional assumptions about the relationship between model scale and reasoning capability, suggesting that certain cognitive limitations may be intrinsic to current architectural paradigms rather than solvable through scaling alone.</p>
<p>These findings suggest that current models develop non-human reasoning strategies that excel on certain complex problems but fail unexpectedly on others, with the benchmark successfully identifying specific reasoning types like deductive and syllogistic, where models struggle disproportionately.</p>
<p>LogiEval-Hard provides meaningful differentiation between surface-level pattern matching and genuine reasoning capabilities, serving as both a diagnostic tool for identifying model weaknesses and a proving ground for next-generation reasoning architectures.The demonstrated performance patterns underscore the need for continued research into more robust reasoning architectures that can handle the full spectrum of logical challenges, with LogiEval-Hard offering a more discriminating alternative to aggregate metrics that often mask fundamental limitations in current language models.</p>
<p>Related Work</p>
<p>Logical reasoning datasets With the advance of pre-trained language models, logical reasoning has become a booming research area.Multiple logical reasoning datasets are brought up to challenge or probe into the reasoning ability of large language models.LogiQA (Liu et al., 2021) and ReClor (Yu et al., 2020) first introduce multi-choice reading comprehension to the investigation.They are sourced from competitive examinations like the Chinese Civil Service Examination and LSAT.Because of the high-quality nature of these expert-designed questions, they become the most widely used datasets for logical reasoning.Over the years, language models have been tested or even trained on these datasets, making the performance on this question type increase drastically.Similarly, our dataset is sourced from examinations, but we cover a broader type of questions and task formats, making it a holistic benchmark for logical reasoning.Apart from sourcing from exams, researchers also use rule-based methods to synthesize logical reasoning datasets, deductive reasoning in particular, for this type of reasoning is easy to create in massive quantities.Ruletaker (Clark et al., 2020) et al., 2021a) benchmark to solve the saturation issue of the original dataset.It expands the option choices from 4 to 10, drastically decreasing the performance of large language models.The dataset has 12K complex questions across various disciplines like law, physics, and chemistry.Like our dataset, MMLU-pro is multi-task, however, it only contains a small fraction of logic questions under the philosophy subject.On the contrary, all the tasks in LogicEval are focused on logical reasoning.</p>
<p>Conclusion</p>
<p>This work introduces LogiEval, a comprehensive benchmark for evaluating logical reasoning in large language models, revealing that while modern LLMs demonstrate impressive performance on certain tasks like multiple-choice argument analysis, their capabilities remain uneven across reasoning types, which suggests fundamental gaps in formal logical reasoning.Our analysis uncovers an intriguing inverse difficulty relationship where models perform well on problems humans find challenging yet fail unexpectedly on mid-difficulty items, indicating fundamentally different reasoning strategies from human cognition.While the creation of LogiEval-Hard provides a rigorous testbed that exposes current limitations, it also demonstrates that small models can serve as effective predictors of universal reasoning challenges across large language models.While this enables more nuanced assessment beyond traditional domain-specific evaluations and paving the way for developing more robust reasoning architectures capable of handling the full spectrum of logical challenges, practitioners should note risks: (1) Logical perfection doesn't represent factual correctness (2) Training on our data without safeguards could lead to potential misuse for generating persuasive misinformation.We advocate for human oversight when deploying reasoning systems evaluated through LogiEval.</p>
<p>Limitations</p>
<p>LogiEval's current scope has two key limitations: (1) Text-only evaluation excludes multi-modal reasoning challenges; (2) Accuracy metrics overlook reasoning validity and explanation robustness.Future work should expand to multi-modal tasks and develop process-aware evaluation metrics.</p>
<p>R1 is released in January 2025 by DeepSeek AI.It is trained on top of DeepSeek V3 with 671 B MoE parameters.The key innovation is the massive implementation of reinforcement learning for long CoT reasoning.QWQ 32B is a 32B parameter model developed by the Qwen team.As their first reasoning model, QwQ 32B has garnered a lot of attention for its superior performance on various reasoning tasks despite its size.QWEN3-235B-A22B is the latest flagship reasoning model of Qwen.Released in April, 2025, It is a MoE model with 235B total parameters and 22B activated parameters.One of the key features of this model is the hybrid thinking modes, which allow users to control how much "thinking" the model performs based on the task.Proprietary models Proprietary models are less open compared to open-weighted models, we can access to the responses of these models either through a ChatUI or API.We chose the following models, which are claimed to be reasoning models or have a thinking mode: GEMINI2.0FLASH THINKING is a model developed by Google.It was released in 2025 with 32B parameters as the company's most capable reasoning model.CLAUDE3.7 SONNET THINKING is a model developed by Anthropic.The parameter size of this model has not been revealed.It is the company's most recent release to date.GROK3 (THINK) is developed by xAI as its most advanced reasoning model yet.The thinking model is optimized for test-time compute and reasoning.</p>
<p>on long chainof-thought (long CoT) examples (Chen et al., 2025a), which are either human-annotated (Muennighoff
Model Performance on Different Reasoning TasksAbductiveAnalogicalDeductiveInductiveTotalBestAccuracy (%)75 80 85 90 9581.10%85.22%77.36%79.17%79.06%77.32%89.60%78.16%81.49%81.41%86.94%83.10%77.62%84.03%80.77%84.50%87.50%73.74%85.62%79.59%87.67%86.17%74.70%84.06%79.81%85.95%90.54%72.57%78.61%78.74%82.68%87.85%77.21%80.69%80.34%70Cl au de 3. 7 So nn et Th in ki ngD ee pS ee k R1G em in i2 .0 Fl as h Th in ki ngModels G ro k3 (T hi nk )O pe nA I o4 -m in iQ w en 3Q w Q 32 BFigure 1: Model performance comparison on different subtasks</p>
<p>one of the following, if true, most helps to explain the discrepancy between the intended results of the highway improvements and the results revealed in
DeductiveInductiveAnalogicalAbductiveContext: Identify the relationship between the first pair of words and select the answer that best replicates the same relationship in the second pair. Question: Cup is to coffee as bowl is toContext: A research study revealed that, in most cases, once existing highways near urban areas are widened and extended in an attempt to reduce traffic congestion and resulting delays for motorists, these problems actually increase rather than decrease. Question: Which the study? A. Widened and extended roads tend to attract many more motorists than used them before their improvement √ B. Typically, road widening or extension projects are undertaken only after theA. dishpopulation near the road in question has increased and then leveled off, leaving aB. soup √ C. spoon D. foodhigher average population level C. As a general rule, the greater the number of lanes on a given length of highway, the lower the rate of accidents per 100,000 vehicles traveling on it D. Rural, as compared to urban, traffic usually includes a larger proportion of trucksand vehicles used by farmersFigure 2: Examples from the LoigEval benchmarkrelease our dataset on HuggingFace and enable access control to avoid misuse. Researchers whoapply for access should adhere to usage guidelines.</p>
<p>Table 1 :
1
The dataset statistics of LogiEval
Reasoning Type Count Task FormatCount Task FormatCount Options Countabductive961argument analysis1,354 logical sequence135210analogical379artificial language195odd one out7133,766deductive3,681 blood relations91situational judgement 31041,575inductive1,214 definition matching 324syllogism3085879Total6,235 essential part55textual entailment3,402 65</p>
<p>Table 2 :
2
The performance of large reasoning models on different tasks of LogiEval
Task FormatCLAUDE3.7 SONNET THINKINGDEEPSEEK R1GEMINI2.0 FLASH THINKINGGROK3 (THINK)OPENAI O4-MINIQWEN3 -235B A22BQWQ 32BArgument Analysis85.70%81.20%87.01%88.22%89.90% 87.72% 88.19%Artificial Language64.30%72.34%80.93%91.67%82.71% 80.99% 54.52%Blood Relations59.60%74.94%73.79%73.68%72.04% 58.24% 71.62%Definition Matching90.91%91.65%91.23%88.60%87.35% 96.12% 94.71%Essential Part100%100%95.99%95.30%100%100% 94.88%Logical Sequence86.04%89.24%91.45%93.68%95.18% 83.46% 92.44%Odd One Out77.93%79.05%81.28%88.69%85.57% 83.99% 78.26%Situational Judgement77.34%74.43%74.26%73.87%69.14% 72.95% 67.57%Syllogism70.61%73.38%70.19%51.86%54.85% 56.93% 73.46%Textual Entailment79.55%83.77%75.20%77.81%78.48% 82.72% 77.50%Total79.06%81.41%80.77%79.59%79.81% 78.74% 80.34%</p>
<p>Table 3 :
3
The performance of large reasoning models on different reasoning types of LogiEval
Reasoing TypeCLAUDE3.7 SONNET THINKINGDEEPSEEK R1GEMINI2.0 FLASH THINKINGGROK3 (THINK)OPENAI O4-MINIQWEN3 -235B A22BQWQ 32BAbductive81.10%77.32%86.94%84.50%87.67%85.95% 82.68%Analogical85.22%89.60%83.10%87.50%86.17%90.54% 87.85%Deductive77.36%78.16%77.62%73.74%74.70%72.57% 77.21%Inductive79.17%81.49%84.03%85.62%84.06%78.61% 80.69%Total79.06%81.41%80.77%79.59%79.81%78.74% 80.34%</p>
<p>Table 4 :
4
The comparison between human performance and LLM performance.
Human Acc. Model Acc. n95% CIp-value18%85.71%7 [42.13%, 99.64%] 0.003231%0.00%7 [0.00%, 35.43%]1.000041%100.00%7[59.04%, 100%]&lt;0.000163%0.00%7 [0.00%, 35.43%]1.000085%100.00%7[59.04%, 100%]&lt;0.0001</p>
<p>Table 5 :
5
The performance of GEMINI2.0 FLASH THINKING on different task format and reasoning types of LogiEval-Hard.
Reasoning Type Accuracy Task FormatAccuracy Task FormatAccuracyabductive (239) 45.61%argument analysis (263) 65.40%logical sequence (21)61.90%analogical (68)52.94%artificial language (120) 60.00%odd one out (10)50.00%deductive (802) 35.66%blood relations (22)22.73%situational judgment (70) 61.43%inductive (508)36.02%definition matching (28) 42.86%syllogism (100)16.00%Total (1,617)37.97%essential part (1)100.00% textual entailment (982)28.00%</p>
<p>(Jain et al., 2024)5)21b)nd inference engine written in Lisp to generate a set of facts and rules to form a context, and a statement to infer.This forms a 2-way (true, false) classification task with more than 707K data instances.PrOntoQA(Saparov, He, 2023)starts by generating a small hierarchical ontology with a set of concepts and subtype relations between them.It generates proofs from the ontology using tree search and lastly translates FOL into natural language CoT examples.PrOntoQA is also in a true-or-false classification format, and it has 500 examples.LogicNLI (Tian et al., 2021)is a 4-way classification (entailment, contradiction, neutral, paradox) task with more than 30K data instances.Similarly, it generates FOL first and then natural language.Manual revisions are implemented on the initial language expressions.CLUTRR(Sinha et al., 2019)uses a knowledge base to generate a kinship relation inference.It first forms a kinship graph and then uses this graph to make up short stories, which explicitly tests inductive reasoning and systematic generalization.It contains 70K examples that infer the relationship between two family members.The aforementioned datasets have repetitive patterns because of their rule-based generation methodology, which diminishes their applicability to test large reasoning models.On the contrary, our dataset contains the same logical deduction problem sets but was collected from examinations, which are diverse and unique.GLoRE  (liu et al., 2025)reports the performance of DeepSeek R1 and QwQ 32B on a collection of logical reasoning datasets.However, these models' high scores on these datasets suggest the need for a more challenging benchmark.Evaluation benchmarks for large reasoning models The performance on logical reasoning datasets is not reported at the release of large reasoning models.The evaluation benchmarks center on math, coding, and knowledge-based question answering.AIME (Ye et al., 2025) is a challenging mathematical competition held in America each year.There are 30 problem sets released in each examination.Recent large reasoning models report their results on AIME-2024 or AIME-2025.MATH-500 is a subset of the MATH dataset(Hendrycks et al., 2021b).With 500 testing examples, it is the go-to benchmark for testing mathematical reasoning in large reasoning models.Compared to our dataset, mathematical reasoning datasets deal with numbers, calculation, and other mathematical concepts, which are not logical reasoning-intensive.Codeforces(Penedo et al., 2025)contains more than 10K unique programming problems that are hosted on the Codeforces website up to 2025.These challenging algorithmic optimization problems serve as an ideal testbed for complex multi-step reasoning in large reasoning models.LiveCodeBench(Jain et al., 2024)constantly collects new coding tests from LeetCode, AtCoder, and Codeforces.Currently, it contains over 300 high-quality coding problems published between May 2022 and February 2024.Although algorithmic problems are sophisticated reasoning problems that require logical reasoning abilities, they are not logicalreasoning centered.These coding datasets focus more on syntactic patterns of the coding languages, which are highly repetitive.The required logical reasoning abilities in our dataset are diverse and uniquely presented in different contexts.MMLU-pro (Wang et al., 2024)is a benchmark derived from the original MMLU (Hendrycks</p>
<p>https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
Which conclusion follows? A. Only conclusion I follows √ B. Only conclusion II follows C. Either I or II follows D. Neither I nor II follows E. Both I and II followA Annotation DetailsParticipant Recruitment Participants were recruited with the following qualifications: minimum 100 prior approved studies, 95%+ approval rating, native English proficiency, and verified background in formal logic through a screening test.3 qualified annotators were selected for the study.We make sure that the compensation for their work is above local minimum wage.Task Instructions The evaluation instructions stated: "Evaluate whether the conclusion logically follows from the premises.Select from: (1) Valid (2A.1 Quality ControlTen percent of questions served as controls with verified answers.Annotators maintaining below 80% accuracy were excluded from analysis.Inter-annotator agreement measured Fleiss' k = 0.72, indicating substantial reliability.The interface included a tutorial with five practice questions before beginning the actual evaluation tasks.We designed three distinct prompt templates for different experimental phases:Main Evaluation PromptConclude with your answer using the format: Answer: [A-D] Context: {context} Question: {question} Options: Evaluation experiments are conducted with a temperature of 0.7 and a 16k token limit.
Claude 3.7 Sonnet and Claude Code. Anthropic, 2025</p>
<p>Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models. Chen Qiguang, Qin Libo, Liu Jinhao, Peng Dengyun, Guan Jiannan, Wang Peng, Hu Mengkang, Zhou Yuhang, Gao Te, Che Wanxiang, 2025a</p>
<p>Chen Xiaoyang, Du Dai Xinan, Feng Yu, Guo Qian, Gu Naixu, Gao Tingshuo, Gao Yuting, Han Yingyi, Jiang Xudong, Jin Xiang, Lin Yilin, Lin Hongyi, Li Shisheng, Li Xiangnan, Li Yuante, Lai Yixing, Ma Zhentao, Peng Zilu, Qian Yingrong, Sun Jiacheng, Sun Hao-Yu, Wang Jianbo, Wu Zirui, Wang Siwei, Xu Zian, Xu Bin, Yu Jianghao, Yang Yiyang, Zichuan, Zha Hongji, Zhang Ruichong. DeepMath-Creative: Benchmark for Evaluating Mathematical Creativity of Large Language Models. 2025b</p>
<p>Clark Peter, Tafjord Oyvind, Richardson Kyle, Transformers as Soft Reasoners over Language // Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. 7Main track</p>
<p>Del Maksym, Fishel Mark, True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4 // Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023). 2023</p>
<p>A factoid question answering system using answer pattern matching. Er Nagehan Pala, Cicekli Ilyas, Proceedings of the sixth international joint conference on natural language processing. the sixth international joint conference on natural language processing2013</p>
<p>Guo Daya, Yang Dejian, Zhang Haowei, Song Junxiao, Zhang Ruoyu, Zhu Xu Runxin, Ma Qihao, Wang Shirong, Peiyi, arXiv:2501.12948Bi Xiao, others . Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>LogiTorch: A PyTorch-based library for logical. Helwe Chadi, Clavel Chloé, Suchanek Fabian, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2022 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAbu Dhabi, UAE; XIIAssociation for Computational Linguistics2022</p>
<p>Hendrycks Dan, Basart Burns Collin, Zou Steven, Mazeika Andy, Song Mantas, Steinhardt Dawn, Jacob, Measuring Massive Multitask Language Understanding // Proceedings of the International Conference on Learning Representations (ICLR). 2021a</p>
<p>Measuring Mathematical Problem Solving With the MATH Dataset // NeurIPS. Hendrycks Dan, Kadavath Burns Collin, Arora Saurav, Basart Akul, Tang Steven, Song Eric, Steinhardt Dawn, Jacob, 2021b</p>
<p>Han Jain Naman, Gu King, Li Alex, Yan Wen-Ding, Zhang Fanjia, Wang Tianjun, Solar-Lezama Sida, Sen Armando, Koushik, arXiv:2403.07974Stoica Ion. LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code. 2024arXiv preprint</p>
<p>Jimenez Carlos, E , Yang John, Wettig Alexander, Yao Shunyu, Narasimhan Karthik R. SWEbench: Can Language Models Resolve Real-world Github Issues? // The Twelfth International Conference on Learning Representations. Pei Kexin, Press Ofir2024</p>
<p>Kumar Komal, Ashraf Tajamul, Thawakar Omkar, Anwer Rao Muhammad, Cholakkal Hisham, Shah Mubarak, Yang Ming-Hsuan, Torr Phillip, H S Khan, Fahad Shahbaz, Khan Salman, arXiv:2502.21321Llm post-training: A deep dive into reasoning large language models. 2025arXiv preprint</p>
<p>. Lightman Hunter, Kosaraju Vineet, Burda Yura, Edwards Harri, Baker Bowen, Lee Teddy, Leike Jan, Schulman John, Sutskever Ilya, Cobbe Karl, arXiv:2305.200502023Let's Verify Step by Step // arXiv preprint</p>
<p>Liu Hanmeng, Fu Zhizhang, Ding Mengru, Ning Ruoxi, Zhang Chaoli, Liu Xiaozhang, Zhang Yue, arXiv:2502.09100Logical Reasoning in Large Language Models: A Survey. 2025arXiv preprint</p>
<p>LogiQA 2.0-An Improved Dataset for Logical Reasoning in Natural Language Understanding. Liu Hanmeng, Liu Jian, Cui Leyang, Teng Zhiyang, Duan Nan, Ming Zhou, Zhang Yue, Speech, and Language Processing. 202331</p>
<p>LogiQA: a challenge dataset for machine reading comprehension with logical reasoning. Cui Liu Jian, Liu Leyang, Hanmeng, Wang Huang Dandan, Zhang Yile, Yue, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial Intelligence202120</p>
<p>Muennighoff Niklas, Yang Zitong, Shi Weijia, Li Xiang Lisa, Fei-Fei Li, Hajishirzi Hannaneh, Zettlemoyer Luke, Liang Percy, arXiv:2501.19393Candès Emmanuel, Hashimoto Tatsunori. s1: Simple test-time scaling. 2025arXiv preprint</p>
<p>How well do SOTA legal reasoning models support abductive reasoning?. Nguyen Ha-Thanh, Goebel Randy, Toni Francesca, Stathis Kostas, Ken Satoh, 2023</p>
<p>Introducing OpenAI o3 and o4-mini. Openai, 2025</p>
<p>LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models. Parmar Mihir, Patel Nisarg, Varshney Neeraj, Nakamura Mutsumi, Luo Man, Mashetty Santosh, Mitra Arindam, Baral Chitta, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, Thailand; VIIIAssociation for Computational Linguistics20241</p>
<p>Lajarín Agustín Piqueres. Penedo Guilherme, Lozhkov Anton, Kydlíček Hynek, Allal Loubna Ben, Beeching Edward, 2025Werra Leandro von. CodeForcesGallouédec Quentin, Habib Nathan, Tunstall Lewis</p>
<p>Plas Lonneke van der. Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance // Proc. of EMNLP. Molly Petersen, 2023</p>
<p>Hu Yuchen, others . Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?. Qin Chengwei, Xia Wenhan, Wang Tan, Jiao Fangkai, 2024</p>
<p>Rein David, Betty Hou, Li, Asa Stickland, Petty Cooper, Pang Jackson, Dirani Richard Yuanzhe, Michael Julien, Bowman Julian, R Samuel, Gpqa, A Graduate-Level Google-Proof Q&amp;A Benchmark // First Conference on Language Modeling. 2024</p>
<p>RobustLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners. Sanyal Soumya, Liao Zeyi, Ren Xiang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates; XIIAssociation for Computational Linguistics2022</p>
<p>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain. Saparov Abulhair, He He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Sodhani Sinha Koustuv, Dong Shagun, Jin, Joelle Pineau, Hamilton William, L Clutrr, A Diagnostic Benchmark for Inductive Reasoning from Text // Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Hong Kong, China; XIAssociation for Computational Linguistics2019</p>
<p>Song Yueqi, Ou Tianyue, Kong Yibo, Li Zecheng, Neubig Graham, Yue Xiang, arXiv:2504.10342VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge. 2025arXiv preprint</p>
<p>QwQ-32B: Embracing the Power of Reinforcement Learning. Team Qwen, Team Qwen. Qwen3March 2025a. April 2025b</p>
<p>Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI. Tian Jidong, Li Yitian, Chen Wenqing, Xiao Liqiang, He Hao, Jin Yaohui, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic; XIAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>Wang Yubo, Ma Xueguang, Zhang Ge, Ni Yuansheng, Chandra Abhranil, Guo Shiguang, Ren Weiming, Arulraj Aaran, arXiv:2406.01574He Xuan, Jiang Ziyan, others . Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. 2024arXiv preprint</p>
<p>Wijesiriwardene Thilini, Wickramarachchi Ruwan, Gajera Bimal, Gowaikar Shreeyash, Gupta Chandan, others . ANALOGICAL -A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models // Proc. of ACL Findings. 2023</p>
<p>Xu Fengli, Zong Hao Qianyue, Wang Zefang, Zhang Jingwei, Wang Yunke, Lan Jingyi, Gong Xiaochong, Ouyang Jiahui, Tianjian, arXiv:2501.09686Meng Fanjin, others . Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models. 2025arXiv preprint</p>
<p>Ye Wenqian, Zheng Guangtao, Cao Xu, Ma Yunsheng, Zhang Aidong, arXiv:2402.12715.2024Spurious correlations in machine learning: A survey. arXiv preprint</p>
<p>AIME-Preview: A Rigorous and Immediate Evaluation Framework for Advanced Mathematical Reasoning. Ye Yixin, Xiao Yang, Mi Tiantian, Liu Pengfei, 2025GitHub repository</p>
<p>Yu Weihao, Jiang Zihang, Dong Yanfei, Feng Jiashi, ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning // International Conference on Learning Representations (ICLR). April 2020</p>
<p>GLoRE: Evaluating Logical Reasoning of Large Language Models. Teng Liu Hanmeng, Ning Zhiyang, Ding Ruoxi, Li Yiran, Liu Xiulai, Zhang Xiaozhang, Yue, 2025</p>
<p>GLoRE: Evaluating Logical Reasoning of Large Language Models. 2023. xAI . Grok 3 Beta -The Age of Reasoning Agents. Teng Liu Hanmeng, Ning Zhiyang, Liu Ruoxi, Zhou Jian, Zhang Qiji, Yue, 2025</p>            </div>
        </div>

    </div>
</body>
</html>
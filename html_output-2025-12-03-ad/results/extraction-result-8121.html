<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8121 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8121</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8121</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-267750484</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.11917v3.pdf" target="_blank">A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task</a></p>
                <p><strong>Paper Abstract:</strong> Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights into the broader operating principles of transformers and thus provide a basis for understanding more complex models.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8121.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8121.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ModularAdditionCircuit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Circuit for modular addition in small transformers (reverse-engineered)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (cited here) reverse-engineered a small transformer to show it implements modular addition via an interpretable circuit, attributing distinct computations to different components of the model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Progress measures for grokking via mechanistic interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>small transformer (as studied by Nanda et al. 2023a)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A small transformer model analyzed in prior mechanistic-interpretability work; exact size and training details are not specified in this paper (referred to only as a 'small transformer').</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Modular addition (single-operation modular arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Interpretable circuit implementing modular addition: distinct components (attention/MLP circuits) implement parts of the algorithm; representations appear to be localized to specific subnetworks (i.e., circuit motifs).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Mechanistic reverse-engineering / circuit analysis (as referenced); related methods in field include linear probes, attention inspection, activation patching and causal interventions, though exact methods used in that work are not fully detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited reverse-engineering results showing that a small transformer implements modular addition via an identifiable circuit; used as an example that internal mechanisms can be reverse-engineered into human-interpretable computations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not discussed in detail in this paper; only cited as an existence proof that arithmetic-like algorithms can be implemented in small transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8121.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8121.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ArithmeticCausalMediation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mechanistic interpretation of arithmetic reasoning using causal mediation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study (Stolfo et al., 2023) applies causal mediation-style analyses to trace information flow in language models when answering simple mathematical questions, aiming to attribute arithmetic behavior to internal components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified language models (as studied by Stolfo et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Language models studied for arithmetic reasoning in the referenced work; specific architectures/sizes are not specified in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic reasoning on simple mathematical questions (general arithmetic reasoning rather than a single fixed operation)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Investigates information flow and internal representations responsible for arithmetic reasoning (e.g., which components mediate the effect of inputs on outputs), aiming to identify causal pathways rather than only correlational signals.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Causal mediation analysis / causal attribution techniques (referenced as the primary approach in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>The referenced study is described as investigating internal information flow and using causal mediation methods to link internal components to arithmetic outputs; cited here as the closest prior work to the paper's mechanistic approach.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not detailed in this paper; referenced as complementary work but without reported failure modes here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8121.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8121.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InductionHeads</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Induction heads (copying attention mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Attention-head motif discovered in prior work that implements a copying mechanism (induction) enabling in-context learning by copying token patterns; cited here as analogous to deduction heads found in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>In-context learning and induction heads</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>transformer models (studied by Olsson et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer models analyzed in Olsson et al.; induction heads observed across transformer layers and models (exact sizes not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Not an arithmetic task per se, but a copying mechanism that can support arithmetic-like computations (e.g., digit-by-digit copying or retrieval subroutines) when present in models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Copying via attention: head composes query/key to locate prior occurrences and value to copy representations forward (the induction-head motif); in this paper a related motif ('deduction heads') is identified that searches for edges and copies source tokens to target positions.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Attention pattern analysis and circuit-style interpretation (as referenced); induction heads originally identified via attention visualization and mechanistic analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited prior discovery of induction heads and the similarity between induction heads' copying behavior and the deduction heads observed in this paper; used to motivate attention-based copying as a general mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not provided here; the paper notes differences (deduction heads act in reverse/cascaded fashion) and that attention-only analyses become harder when MLPs compose with attention in deeper layers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>In-context learning and induction heads <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8121",
    "paper_id": "paper-267750484",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "ModularAdditionCircuit",
            "name_full": "Circuit for modular addition in small transformers (reverse-engineered)",
            "brief_description": "Prior work (cited here) reverse-engineered a small transformer to show it implements modular addition via an interpretable circuit, attributing distinct computations to different components of the model.",
            "citation_title": "Progress measures for grokking via mechanistic interpretability",
            "mention_or_use": "mention",
            "model_name": "small transformer (as studied by Nanda et al. 2023a)",
            "model_description": "A small transformer model analyzed in prior mechanistic-interpretability work; exact size and training details are not specified in this paper (referred to only as a 'small transformer').",
            "arithmetic_task_type": "Modular addition (single-operation modular arithmetic)",
            "mechanism_or_representation": "Interpretable circuit implementing modular addition: distinct components (attention/MLP circuits) implement parts of the algorithm; representations appear to be localized to specific subnetworks (i.e., circuit motifs).",
            "probing_or_intervention_method": "Mechanistic reverse-engineering / circuit analysis (as referenced); related methods in field include linear probes, attention inspection, activation patching and causal interventions, though exact methods used in that work are not fully detailed in this paper.",
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "Cited reverse-engineering results showing that a small transformer implements modular addition via an identifiable circuit; used as an example that internal mechanisms can be reverse-engineered into human-interpretable computations.",
            "counterexamples_or_challenges": "Not discussed in detail in this paper; only cited as an existence proof that arithmetic-like algorithms can be implemented in small transformers.",
            "uuid": "e8121.0",
            "source_info": {
                "paper_title": "A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ArithmeticCausalMediation",
            "name_full": "Mechanistic interpretation of arithmetic reasoning using causal mediation analysis",
            "brief_description": "A referenced study (Stolfo et al., 2023) applies causal mediation-style analyses to trace information flow in language models when answering simple mathematical questions, aiming to attribute arithmetic behavior to internal components.",
            "citation_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "mention_or_use": "mention",
            "model_name": "unspecified language models (as studied by Stolfo et al. 2023)",
            "model_description": "Language models studied for arithmetic reasoning in the referenced work; specific architectures/sizes are not specified in this paper's discussion.",
            "arithmetic_task_type": "Arithmetic reasoning on simple mathematical questions (general arithmetic reasoning rather than a single fixed operation)",
            "mechanism_or_representation": "Investigates information flow and internal representations responsible for arithmetic reasoning (e.g., which components mediate the effect of inputs on outputs), aiming to identify causal pathways rather than only correlational signals.",
            "probing_or_intervention_method": "Causal mediation analysis / causal attribution techniques (referenced as the primary approach in the cited work).",
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "The referenced study is described as investigating internal information flow and using causal mediation methods to link internal components to arithmetic outputs; cited here as the closest prior work to the paper's mechanistic approach.",
            "counterexamples_or_challenges": "Not detailed in this paper; referenced as complementary work but without reported failure modes here.",
            "uuid": "e8121.1",
            "source_info": {
                "paper_title": "A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "InductionHeads",
            "name_full": "Induction heads (copying attention mechanism)",
            "brief_description": "Attention-head motif discovered in prior work that implements a copying mechanism (induction) enabling in-context learning by copying token patterns; cited here as analogous to deduction heads found in this paper.",
            "citation_title": "In-context learning and induction heads",
            "mention_or_use": "mention",
            "model_name": "transformer models (studied by Olsson et al., 2022)",
            "model_description": "Transformer models analyzed in Olsson et al.; induction heads observed across transformer layers and models (exact sizes not specified in this paper).",
            "arithmetic_task_type": "Not an arithmetic task per se, but a copying mechanism that can support arithmetic-like computations (e.g., digit-by-digit copying or retrieval subroutines) when present in models.",
            "mechanism_or_representation": "Copying via attention: head composes query/key to locate prior occurrences and value to copy representations forward (the induction-head motif); in this paper a related motif ('deduction heads') is identified that searches for edges and copies source tokens to target positions.",
            "probing_or_intervention_method": "Attention pattern analysis and circuit-style interpretation (as referenced); induction heads originally identified via attention visualization and mechanistic analysis.",
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "Cited prior discovery of induction heads and the similarity between induction heads' copying behavior and the deduction heads observed in this paper; used to motivate attention-based copying as a general mechanism.",
            "counterexamples_or_challenges": "Not provided here; the paper notes differences (deduction heads act in reverse/cascaded fashion) and that attention-only analyses become harder when MLPs compose with attention in deeper layers.",
            "uuid": "e8121.2",
            "source_info": {
                "paper_title": "A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2,
            "sanitized_title": "progress_measures_for_grokking_via_mechanistic_interpretability"
        },
        {
            "paper_title": "In-context learning and induction heads",
            "rating": 2,
            "sanitized_title": "incontext_learning_and_induction_heads"
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 1,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        }
    ],
    "cost": 0.010651250000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task</p>
<p>Jannik Brinkmann 
University of Mannheim</p>
<p>Abhay Sheshadri 
Georgia Institute of Technology
3 Independent 4 Heinrich</p>
<p>Heine University Düsseldorf</p>
<p>Victor Levoso 
Paul Swoboda 
Christian Bartelt 
University of Mannheim</p>
<p>A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task
955B7CBD67DE5FBB1997B8DA25256E0E
Transformers demonstrate impressive performance on a range of reasoning benchmarks.To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies.However, these studies do not provide insights into the internal mechanisms driving the observed capabilities.To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task.We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence.Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions.We anticipate that the motifs we identified in our synthetic setting can provide valuable insights into the broader operating principles of transformers and thus provide a basis for understanding more complex models. 1</p>
<p>Introduction</p>
<p>Transformer-based language models (Vaswani et al., 2017) demonstrate impressive performance on reasoning 2 tasks (Kojima et al., 2023), mathematical problem-solving (Cobbe et al., 2021), and planning (Huang et al., 2022).However, despite strong performance on certain reasoning benchmarks, it remains unclear to what extent these abilities are a result of actual reasoning or simple pattern memorization (Huang and Chang, 2023).† Equal contribution.</p>
<p>1 Correspondence to jannik.brinkmann@uni-mannheim.de.The code is available at github.com/backward-chainingcircuits. 2 In this paper, we consider a form of deductive reasoning as studied in Saparov and He (2023).For a discussion of different forms of reasoning, refer to Huang and Chang (2023).
2 → 1 ,i 3 → 2 ,i 0 → 4 ,i 0 → 3 | 1 : 0 3 2 → 1 ,i 3 → 2 ,i 0 → 4 ,i 0 → 3 | 1 : 0 2 → 1 ,i 3 → 2 ,i 0 → 4 ,i 0 → 3 | 1 : 0 2 → 1 ,i 3 → 2 ,i 0 → 4 ,i 0 → 3 | 1 : 0 2 → 1 ,i 3 → 2 ,i 0 → 4 ,i 0 → 3 | 1 : 0 (A) Edge Token Concatenation (B) Goal Movement (C) Backward Chaining
Figure 1: Backward Chaining.Given an input prompt, the model concatenates edge tokens in a single token position (A), and copies the goal node into the final token position (B).The next step is then identified by applying an iterative algorithm that climbs the tree one level per layer (C).</p>
<p>To understand the reasoning capabilities of language models, existing work has focused on developing sophisticated benchmarks for behavioral studies (Tafjord et al., 2021;Saparov and He, 2023;Valmeekam et al., 2023).However, the conclusions drawn from these studies do not provide insights into the internal mechanisms driving the observed capabilities.In contrast, recent work in the field of mechanistic interpretability attempts to understand the algorithms that models implement by reverseengineering their internal mechanisms, and describing them at a certain level of abstraction (Elhage et al., 2021).For example, Nanda et al. (2023a) reverse-engineered how a small transformer model implements modular addition, providing insights into the specific computations performed by different components of the model.Similarly, Olsson et al. (2022) discovered "induction heads" in transformers which enable a distinct copying mechanism that is considered to be crucial for the incontext learning abilities of language models.Contributions This paper studies reasoning in language models by reverse-engineering the inter-arXiv: 2402.11917v3 [cs.LG] 30 Jun 2024 nal mechanisms of a transformer trained on a symbolic multi-step reasoning task.Specifically, we focus on path finding in a tree, a variation of the task proposed in Saparov and He (2023).By analyzing the internal representations of the model, we identify several key mechanisms:</p>
<p>1.A specific type of copying operation implemented in attention heads, which we call deduction heads.These are similar to induction heads as observed in Olsson et al. (2022).</p>
<p>In our task, deduction heads intuitively serve the purpose of moving one level up the tree.These heads are implemented in multiple consecutive layers which allows the model to climb the tree multiple layers in a single forward pass.</p>
<ol>
<li>
<p>A parallelization motif whereby the early layers of the model choose to solve several subproblems in parallel that may be relevant for solving many harder instances of the task.</p>
</li>
<li>
<p>A heuristic for tracking the children of the current node and whether these children are leaf nodes of the tree.This mechanism is used when the model is unable to solve the problem using deduction heads and parallelization.</p>
</li>
</ol>
<p>We validate our findings using correlational and causal evidence, using techniques such as linear probing (Alain and Bengio, 2018), activation patching (Vig and Belinkov, 2019), and causal scrubbing (Chan et al., 2022).</p>
<p>Related Work</p>
<p>Expressiveness of Transformers To understand the capabilities of transformers, one line of work characterizes their theoretical properties (Bhattamishra et al., 2020;Merrill et al., 2021;Pérez et al., 2021;Merrill et al., 2022;Liu et al., 2023).These studies answer questions about the expressiveness of the transformer architecture by treating them as approximators of different classes of functions (Yun et al., 2020), or by characterizing the class of formal languages that a transformer can recognize (Strobl et al., 2023).</p>
<p>Mechanistic Interpretability</p>
<p>In contrast to these theoretical investigations, a number of studies have adopted an empirical approach in order to understand what transformers learn in practice (Elhage et al., 2021;Delétang et al., 2023;Zhang et al., 2024).Our analysis is inspired by existing work in the field of mechanistic interpretability, attempting to discover and understand the algorithms implemented in a model by reverse-engineering its internal mechanisms (Räuker et al., 2023).To explore these internal mechanisms, the field has adopted techniques such as activation patching (Wang et al., 2023b), causal scrubbing (Chan et al., 2022), and circuit discovery (Conmy et al., 2023).In addition, considerable focus has been placed on the study of small models trained on specialized tasks, such as modular addition (Nanda et al., 2023a), or group operations (Chughtai et al., 2023), providing a more manageable framework for understanding complex computational processes.We present an extended discussion of the related work on mechanistic interpretability in Appendix A.</p>
<p>Evaluating Reasoning Capabilities Existing approaches to evaluate the reasoning capabilities of language models focus on their performance on a range of downstream tasks (Huang and Chang, 2023).To enable a more formal analysis of reasoning, a number of studies have developed sophisticated metrics and benchmarks (Han et al., 2022;Golovneva et al., 2023;Wang et al., 2023a).For example, Saparov and He (2023) use a synthetic question-answering dataset based on a world model expressed in first-order logic to parse the generated reasoning processes into symbolic proofs for formal analysis.Their results suggest that language models are capable of making correct individual deduction steps.However, these approaches stop short of exploring the internal mechanisms that enable these capabilities (Huang and Chang, 2023).Hou et al. (2023) investigate whether language models solve reasoning tasks using information the model memorized from pretraining or using information provided in context.To investigate this question, they use linear probes to determine whether the model encodes a reasoning tree.The approach that comes closest to our work is Stolfo et al. (2023), presenting a mechanistic interpretation of arithmetic reasoning by investigating the information flow in the model given simple mathematical questions.</p>
<p>Background</p>
<p>Transformer Notation Transformers (Vaswani et al., 2017) represent input text as a sequence t 1 , t 2 , . . ., t N of tokens, such that t i ∈ V where V is a vocabulary.Each token t i is em-bedded as a vector x 0 i ∈ R d using an embedding matrix W E ∈ R |V |×d , where d is the dimension of the hidden state.These embeddings initialize the residual stream, which is then transformed through a sequence of L transformer blocks, each consisting of a multi-head attention sublayer and an MLP sublayer.The representation of token t i at layer ℓ is obtained by:
x ℓ i = x ℓ−1 i + a ℓ i + m ℓ i (1)
where a ℓ i and m ℓ i are the outputs of the attention and MLP sublayers.To predict the next token in the sequence, it applies an unembedding matrix W U ∈ R |V |×d to the residual stream x L i , translating it into a probability distribution over the vocabulary.</p>
<p>Linear Probes To investigate whether information is encoded in intermediate representations of the model, we use linear probes (Alain and Bengio, 2018) implemented as a linear projection from the residual stream.This involves training a logistic regression model on a dataset of activations x ℓ i to predict an auxiliary classification problem.For example, we use it to determine whether information about an edge is encoded in the activations at a specific token position.If the performance of the linear probe is sufficiently high, we can treat this as evidence for the information being encoded.</p>
<p>Activation Patching To evaluate the importance of a model component for a given prediction, we intervene by patching in the activations it would have had on a different input (also called resampling ablations) (Vig et al., 2020;Meng et al., 2022).This involves using a clean input s with an associated target prediction r, and a corrupted input s ′ with a different target r ′ .Then, we cache the activations of the component on s, and evaluate the effect of patching in these activations when running the model on s ′ .To compute the effect of this intervention, we compute the difference in logits:
LD(r, r ′ ) = Logit(r) − Logit(r ′ ) (2)
Causal Scrubbing To evaluate specific hypotheses about internal mechanisms, we use causal scrubbing which evaluates the effect of behaviorpreserving resampling ablations (Chan et al., 2022).Specifically, given a hypothesis about which component of a model implements a specific behavior, we replace the activations of that component on some input with activations on another input, where our hypothesis predicts that the activations represent the same thing.Then, we evaluate the impact of this intervention by computing how much of the initial performance is recovered:
L CS = L scrubbed − L random L model − L random (3)
where L model is the test loss of the trained model, L random of a model that outputs uniform logits, and L scrubbed of the trained model with the chosen activations resampled.In contrast to activation patching, which provides insights about whether a specific activation is causally linked to the output, causal scrubbing provides stronger evidence about the role of the activations in the model.Recovering the majority of the loss with a given hypothesis demonstrates that the activation corresponds to a specific variable in a high-level causal model.</p>
<p>Experimental Setup</p>
<p>Task Description</p>
<p>We focus on path finding in trees as a modified version of the task proposed by Saparov and He (2023).Our adaptation shifts the focus from reasoning in natural language to abstract symbolic reasoning.This allows us to better understand motifs that the models might be using to solve analogous problems in natural language.In our experimental setup, we generate training samples by generating binary trees T = (V, E) uniformly at random from the set of all trees with 16 nodes, i.e. |V | = 16.</p>
<p>Then, for each tree, a leaf node is randomly selected as the target node (see Figure 2).The model is given the edge list
[A 1 ][B 1 ], [A 2 ][B 2 ], . . . [A n ] [B n ]
of the tree, a specified goal [G], and the root node [P 1 ], and is trained to predict the path from the root node to the goal
[P 2 ][P 3 ] . . . [P m ] such that [P m ] = [G],
as depicted in Figure 3.The path between any two nodes in a tree is unique; therefore, the data generation is not affected by the type of search algorithm used to determine the path.We emphasize that this task is nontrivial, as the model has to perform multiple reasoning steps in a single forward pass to predict the next step.At each step, the model must determine from which node the goal is reachable.Thus, the model must perform multiple steps of reasoning for each next token prediction without relying on techniques such as chain-of-thought or scratchpad (Wei et al., 2022), which would allow the model to roll-out the reasoning steps over multiple next token predictions.</p>
<p>Model Specification and Training Process</p>
<p>In our experiments, we use a 6-layer, decoderonly transformer with an embedding dimension of 128, a single attention head per layer, and a feedforward dimension of 512, resulting in a total of 1.2 million parameters.The training dataset consists of 150,000 generated trees.The edge lists of these trees are shuffled to prevent the model from learning simple heuristics and encourage structural understanding of trees.To evaluate the performance of our model, we compute the accuracy based on the exact match of complete sequences using greedy decoding.Our model achieves 99.7 % accuracy rate on a test set of 15,000 unseen trees, despite seeing just a small fraction of all possible trees during training (see Appendix B).This suggests that generalization is required for meaningful performance and that the model has learned to be capable of solving pathfinding in trees.</p>
<p>Symbolic Reasoning using Backward Chaining</p>
<p>In this section, we present a mechanistic analysis of the internal mechanisms of the model and provide correlational and causal evidence.Our findings suggest that the model uses an interpretable and meaningful backward chaining algorithm to perform pathfinding in a tree.To help guide the reader, we present an intuitive explanation before breaking down the individual algorithmic steps in the following sections.</p>
<p>The Backward Chaining Algorithm First, the model aggregates the source and target nodes of each edge in the edge list into the target node position (see Section 5.1).Then, the model starts at the goal node and moves up the tree one level with each layer of the model.This mechanism is implemented using attention heads, which we term "deduction heads" (see Section 5.2).By the composition of multiple attention heads in consecutive layers, the model can traverse the tree upwards for L − 1 edges, where L is the number of layers in the model.We refer to this mechanism as backward chaining, inspired by the use of the term in the symbolic artificial intelligence literature (Russell and Norvig, 2009).In more complex scenarios, where the required path exceeds the depth of the model, it relies on backward chaining from multiple nodes in the tree in parallel.This creates multiple subpaths that can be used to find the correct next step in cases where the goal node is more than L − 1 edges away (see Section 5.3).In addition, the model uses a simple heuristic as a fallback mechanism, where it identifies child nodes of the current position and evaluates whether these are leaf nodes of the tree.This enables the model to make informed guesses when backward chaining and parallelization are insufficient to solve the problem (see Section 5.4).</p>
<p>Edge Token Concatenation</p>
<p>The attention head in the first layer of the model creates edge embeddings by moving the information about the source token onto the target token for each edge in the context.Specifically, for each edge
[A][B] it copies the information from [A] into the residual stream at position [B]
. This mechanism has some similarities with "Previous Token Heads", as observed in pre-trained language models (Olsson et al., 2022;Wang et al., 2023a).</p>
<p>Experiment: Linear Probes To validate that the model creates edge embeddings, we train a linear probe to predict the associated edge given the activations x 1 at the positions of target nodes.The probe is trained using 8,000 examples and evaluated on a test dataset of the same size.For comparison, we also report the performance of a linear probe given the activations x 0 at the positions of the target nodes and probes given the activations at the positions of the source node.</p>
<p>Results Table 1 reports the performance of the linear probe measured using the F1 score.We find that we can successfully extract the source and target tokens [A][B] from the residual stream activations x 1 at the position of the target tokens after the first layer, providing strong evidence for the edge token concatenation hypothesis as described above.Moreover, it does not encode the complete edge in the position of the source token, attributed to causal masking in the attention mechanism.</p>
<p>Backward Chaining</p>
<p>The most important mechanism the model uses to predict the correct next step is an iterative algorithm, which we refer to as backward chaining.The algorithm starts at the goal node and climbs the tree one level per layer.To this end, the model copies the target node [G] into the final token position [P i ] (see Table 1) and then in each consecutive layer applies what we term "deduction heads".
x 0 i x 1 i Linear {[Ai] → [Ai][Bi]} 0.13 0.19 Linear {[Bi] → [Ai][Bi]} 0.11 1.00 Linear {[Pi] → [G]} 0.03 1.00
Mechanism: Deduction Heads The function of deduction heads is to search for the edge in the context in which the current position is the target node [B] , find the corresponding source token [A], and then copy the source token over to the current position.Thus, deduction heads complete the pattern by mapping:
[A] [B] ... [B] → <a href="4">A</a>
In other words, this mechanism enables the model to go one step up the tree and append [A] after having seen the last [B] in the sequence.This mechanism depends on the edge-token concatenation, which previously copied information about
[A] into [B] (see Appendix C).
This allows the deduction head to search for information about [B] but then copy information about [A] into the final token position.By composition of multiple deduction heads in consecutive layers, the model is capable of climbing several steps up the tree in a single inference step.This creates a subpath at the final token position whose lengths is equivalent to the number of layers involved.In our model, we observe that the attention heads after the first layers can act as deduction heads, resulting in a backward chaining depth of at most L − 1 steps.The role of these heads depends on the proximity of the current position to the goal node; for example, if the root node is just four steps away from the goal node, the model recognizes this and the attention head in the sixth layer does not act as a deduction head.</p>
<p>Experiment: Causal Scrubbing To confirm that the model uses backward chaining to predict the next step for paths up to a depth of L − 1, we use causal scrubbing (see Section 3).Specifically, we hypothesize that the attention head of layer ℓ is responsible for writing the node that is ℓ − 1 edges above the goal into the final token position in the residual stream.This implies that the output of 9 → 4 , 6 → 9 , 1 → 6 , 5 → 12 , 8 → 1 , 14 → 5 , 11 → 8 , 7 → 14 , 15 → 11 , 10 → 7 , 3 → 15 , 13 → 10 , 2 → 3 , 0 → 13 , 0 → 2 | 4 : 0 9 → 4 , 6 → 9 , 1 → 6 , 5 → 12 , 8 → 1 , 14 → 5 , 11 → 8 , 7 → 14 , 15 → 11 , 10 → 7 , 3 → 15 , 13 → 10 , 2 → 3 , 0 → 13 , 0 → 2 | 4 : 0 9 → 4 , 6 → 9 , 1 → 6 , 5 → 12 , 8 → 1 , 14 → 5 , 11 → 8 , 7 → 14 , 15 → 11 , 10 → 7 , 3 → 15 , 13 → 10 , 2 → 3 , 0 → 13 , 0 → 2 | 4 : 0 9 → 4 , 6 → 9 , 1 → 6 , 5 → 12 , 8 → 1 , 14 → 5 , 11 → 8 , 7 → 14 , 15 → 11 , 10 → 7 , 3 → 15 , 13 → 10 , 2 → 3 , 0 → 13 , 0 → 2 | 4 : 0 9 → 4 , 6 → 9 , 1 → 6 , 5 → 12 , 8 → 1 , 14 → 5 , 11 → 8 , 7 → 14 , 15 → 11 , 10 → 7 , 3 → 15 , 13 → 10 , 2 → 3 , 0 → 13 , 0 → 2 | 4 : 0 9 → 4 , 6 → 9 , 1 → 6 , 5 → 12 , 8 → 1 , 14 → 5 , 11 → 8 , 7 → 14 , 15 → 11 , 10 → 7 , 3 → 15 , 13 → 10 , 2 → 3 , 0 → 13 , 0 → 2 | 4 :  Figure 4: Visualization of multi-layer attention patterns on an example input.We show the attention from three selected positions: the path position, register token at position 39, and register token at position 44.We show that the path node starts backward chaining from the specified goal, while the two register tokens start backward chaining from different subgoals.Each token is highlighted by the color of the token that most strongly attends to it.The intensity of the color is based on the magnitude of the attention score.For details on how we select the register tokens and more examples, see Appendix E.</p>
<p>the attention head in layer ℓ should be consistent across trees that share the same node ℓ − 1 edges above the goal.To test this, we generate a clean and a corrupted graph that share the same node ℓ − 1 edges above the goal node.Then, we substitute the output of the head on the clean graph with the output of the head on the corrupted graph, and measure the difference in the loss.To test whether the model predicts the next step using backward chaining, we perform resampling ablations on each head using causal scrubbing.We find that we can recover close to 100 % of the performance of the model for paths up to length L − 1, providing strong evidence for our backward chaining hypothesis.</p>
<p>Results Figure 5 illustrates the effect of causal scrubbing.We find that we can recover most of the performance (close to 100 %) of the model for paths up to length L − 1, providing strong evidence for our hypothesis about backward chaining.We also find that this hypothesis explains most of the behavior of the attention heads in the first four layers of the model even on paths that require more than L − 1 steps; only the attention heads in the final two layers act significantly different, such that the model ends up confidently making incorrect predictions after we apply causal scrubbing.This highlights that the heads in these two layers cannot be accurately described as deduction heads over the entire data distribution.</p>
<p>Path Merging</p>
<p>In cases where the goal is more than L − 1 steps away from the current position, the previously described mechanism is insufficient.To address this, the model does not only perform backward chaining on the final token position, but in parallel on multiple different token positions.We refer to these tokens as "register tokens".The resulting subpaths are then merged on the final token position to facilitate more complex scenarios.</p>
<p>Observation: Register Tokens The role of register tokens is to act as working memory.These are tokens that do not contain any useful information for the actual task; either they do not contain any useful information to begin with, e.g.[,], or are tokens whose information has been copied to other positions and thus contain redundant information.For more details on the role of register tokens in our context, see Appendix D.</p>
<p>Mechanism: Path Merging To compute paths for which backward chaining on the final token position is not sufficient, the model uses register tokens to perform backward chaining in parallel from multiple positions in the tree.To this end, similar to backward chaining from the goal, it copies a node into each of these register tokens which will then be picked up by the deduction heads.This results in a multiple subpaths being stored at different positions in the sequence.Then, the model can merge these by finding overlapping subpaths.</p>
<p>To illustrate, let us assume that a subpath [B]→  Experiment: Linear Probe To validate that the model is indeed generating subpaths in register tokens, we train a linear probe to predict these subpaths given the residual stream at these register token positions.We generate the training set for the linear probe by inspecting the attention patterns (see Figure 4).We observe that these approximate hard-attention, which allows us to read off the token positions from which the head is copying information.We extract the subpath we would expect to be encoded at each of these register tokens and transform them into an adjacency matrix representation.Then, we train the probe on a set of 8,000 examples and evaluate it on a test set of the same size.The probe achieves an F1 score of 92.82 % when trained on the activations of layer four, i.e. up to the the point where the model is performing backward chaining.</p>
<p>Experiment: Register Token Patching To evaluate whether the subpaths stored in the register tokens have a causal effect on the prediction of the model, we perform resampling ablations (see Chapter 3) on the register tokens positions in trees which (i) contain sufficiently long paths such that backward chaining on the final token position is insufficient, and (ii) positions where nodes have multiple child nodes, ensuring that the model has to make a decision between multiple options.The corrupted activations are extracted from another tree in the same class as described above.We compute the effect of this intervention using the logit difference.We perform this intervention after layer four, as we found that the behavior of all previous layers is explained using our backward chaining hypothesis (see Section 5.2).</p>
<p>Results Figure 6 illustrates the impact of patching the register tokens on the model predictions at i .Here, we conducted 10 separate runs, each involving 1000 samples.For each run, we calculate the mean logit difference and report the 95 % confidence interval for the average effects observed across the runs.The results demonstrate that these subpaths are instrumental for paths longer than L − 1 steps.different path lengths.Our results show that the intervention has no effect on performance up to a path depth of 4 and minimal effect at depth 5, which is consistent with our backward chaining hypothesis.Beyond this depth, this intervention has a significant effect on the performance.This suggests that the encoded subpaths are causally relevant for predicting next steps on paths that are more than L − 1 steps away from the goal.However, our findings also indicate that the predictions are not solely dependent on these subpaths derived, but other factors besides the subpaths contribute to the prediction.This includes the influence of a one-step lookahead mechanism, which identifies child nodes of the current position and increases the probabilities of the children that are not leaves.</p>
<p>One-Step Lookahead</p>
<p>We find that the model uses an additional mechanism, which identifies child nodes of the current position and increases the prediction probabilities of the children that are not leaf nodes of the tree.This enables the model to make informed guesses in cases where backward chaining is not sufficient.This mechanism is particularly effective on long paths as these have a lower branching factor in our experimental setup.Thus, it is a pragmatic strategy to minimize the training error.</p>
<p>Experiment: Linear Probes To validate that the model represents the child nodes of the current position, including whether they are leaf nodes, we use linear probes.The probes are trained to predict this information given the activations on the final token position.Table 3 reports the performance of the linear probes measured using the F1 score.Our analysis shows that the model starts to collect information about the children and leaf nodes from the fourth layer and represents both aspects in the fifth layer.</p>
<p>Table 3: F1 score of linear probes trained to predict the children of the current position and whether these are leafs of the tree given the residual stream activations at position [P i ].Mechanism The results from linear probes suggest that the attention heads in the final two layers of the model are responsible for the one-step lookahead mechanism.To better understand this mechanism, we examine how these two attention heads directly compose with the unembedding matrix.Specifically, we compute the contribution of each token to the logits through these attention heads.</p>
<p>Since each edge will be represented in the target node (see Section 5.1), we focus on target node positions (see Figure 7).By inspecting their querykey (QK) circuits (Elhage et al., 2021), we find that these two heads attend to the target node of every edge except those for which the source node is the current path position.Furthermore, we can break the mechanism in their output-value (OV) circuits into three components:</p>
<p>1.Each edge decreases the logit of its target node.</p>
<ol>
<li>
<p>Each edge increases the logit of its source node.</p>
</li>
<li>
<p>Each token in the path decreases its logit.</p>
</li>
</ol>
<p>As a result of these components, the logits of the leaf nodes in the tree will decrease while the logits of the children of the current position will increase.</p>
<p>For the other nodes, the logit increase from being a parent, and the logit decrease from being a child node which roughly cancel out, causing their logit to remain constant.</p>
<p>Discussion</p>
<p>Register Tokens Our model uses some token positions as a form of working memory to store intermediate results.This observation aligns with Darcet et al. (2023) which found that image models use some image patches to accumulate global information while discarding spatial information.Similarly, Goyal et al. (2023) show that adding uninformative tokens at the end of each prompt can enhance language model performance on downstream tasks without introducing additional parameters.Our findings suggest that these techniques enable the model to store intermediate results and perform more computations in parallel.This is consistent with theoretical insights from Merrill et al. (2022) which highlights how the effective state of a transformer depends on the number of tokens in the sequence.</p>
<p>Structural Recursion</p>
<p>Transformers, which are by definition non-recurrent, struggle with emulating structural recursion and extracting recursive rules from data (Zhang et al., 2024).This aspect of learning is crucial in domains such as programming and formal mathematics, where understand-ing complex relationships relies on these abilities.Our analysis provides insights into possible reasons for this limitation.In our setting, training the model using standard objectives for next-token prediction forces the model to unroll the entire recursive structure in a single forward pass.This restricts their abilities to process recursion, leading them to resort to shortcut solutions (Liu et al., 2023).</p>
<p>Reasoning in Transformers</p>
<p>There is an ongoing debate about the reasoning capabilities of transformers (Huang and Chang, 2023).Some argue that these models might just be capable of memorizing patterns without gaining causal understanding, which could lead to diminishing performance on out-of-distribution data (Bender and Koller, 2020;Floridi and Chiriatti, 2020;Bender et al., 2021;Merrill et al., 2021).However, there are several observations that suggest that transformers might be capable of more than just pattern recognition; e.g.Olsson et al. (2022) found a simple algorithm implemented in attention heads that contributes to the in-context learning abilities of transformers and operates independent of the specific tokens.This algorithm is doing more than memorizing patterns and can in some sense work out-of-distribution.In our synthetic setting, we found that the model learned an interpretable and meaningful backward chaining algorithm, supporting the claim that transformers might be capable of a form of reasoning that goes beyond simple pattern memorization.These results further complement the findings of Hou et al. (2023), shedding light on the mechanisms transformer models might use to compute reasoning trees over information provided in context.However, it is important to note that findings from our synthetic settings do not support the boarder claim that transformers possess general reasoning capabilities, highlighting the need for further investigations.</p>
<p>Conclusion</p>
<p>In this paper, we conducted a mechanistic analysis of a transformer trained on pathfinding in trees.</p>
<p>Our results suggest that the model implements a backward chaining algorithm that starts at the goal node and climbs the tree one level per layer.To solve more complex problems, where the required reasoning depth exceeds the number of layers, it executes the same mechanism in parallel across multiple register tokens and combines the resulting subpaths on the final token position.In addition, it performs a simple one-step lookahead in which it finds the child nodes of the current position and evaluates whether they are leaf nodes.Our findings in this synthetic setting demonstrate the ability of a transformer to perform deductive reasoning up to a certain reasoning depth, after which it resorts to simple heuristics.By using parallelized computations to store intermediate results in register tokens and then merging these results in the final token position, the model demonstrates a form of deductive reasoning that, while effective within a given setting, is constrained by its architecture.These observations suggests that transformers may exhibit a inductive bias towards adopting highly parallelized strategies for tasks involving search, planning, or reasoning.</p>
<p>Limitations</p>
<p>Synthetic Task Our experiments were conducted on a symbolic reasoning task (see Chapter 4.1).This allowed us to bypass the complexities associated with natural language, such as multi-token embeddings (Nanda et al., 2023c).In addition, our tokenization distinguishes tokens representing source and target nodes of each edge, such as [15] and [→15].Therefore, our findings are specific to our model and it remains unclear whether large language models trained on natural language use similar mechanisms to solve this task.However, we anticipate that the motifs we discovered in our synthetic setting can provide valuable insights into the broader operating principles of transformers and thus provide a basis for understanding more complex models.</p>
<p>Input Format To prevent the model from learning shortcuts based on the order of the edges in the prompt, we trained our model on shuffled edge lists (see Section 4.2).However, our analysis is limited to sequences in which the edge list is presented in backward order.By backward order we mean a listing of edges that starts with the leaf nodes and ascends level by level to the root node, as opposed to a forward order where the listing starts with the root node and progresses downwards through each level.Our investigation does not extend to a detailed examination of alternative arrangements of the edge list.However, preliminary observations suggest that the model uses similar mechanisms with minor variations, such as the use of different register tokens.</p>
<p>A Extended Discussion of Mechanistic Interpretability</p>
<p>Our work contributes to the emerging field of mechanistic interpretability, which seeks to reverseengineer the internal mechanisms of neural networks into human-understandable algorithms (Elhage et al., 2021).To this end, the model is considered a causal graph (Meng et al., 2022), with the intent of finding interpretable and meaningful subgraphs (circuits) that are responsible for solving the task in question, such as understanding a circuit implementing modular addition (Nanda et al., 2023a), or understanding a circuit responsible for indirect object identification (Wang et al., 2023b).Similar works studied how models perform in-context learning using induction heads, as found by Olsson et al. (2022), or how models perform retrieval tasks using an internal modular decomposition as shown in Variengien and Winsor (2023).Moreover, Merullo et al. (2024) provide evidence that certain mechanisms are reused across different tasks, indicating that models can use similar mechanisms to solve different tasks.However, these studies of specific mechanisms and circuits in language models required a lot of manual effort.In addition, Zhong et al. (2023) demonstrated that small changes in hyperparameters and initializations can lead to the emergence of qualitatively different algorithms.To address this, some researchers attempted to automate the process of finding these circuits using methods such as automated circuit discovery (Conmy et al., 2023) or edge attribution patching (Syed et al., 2023).</p>
<p>Similar research investigates how neural networks process and store information.For example, Geva et al. (2023) explored how models recall factual information, and Meng et al. (2022) studied how this information can be localized and edited without re-training.This also includes works on understanding internal representations.In this context, there is an ongoing debate about the linear representation hypothesis (Mikolov et al., 2013), which is the idea that high-level concepts are represented linearly as directions in the representation space.Here, a model is considered to linearly represent a concept, if it can be probed from the activations with a linear model.For example, Nanda et al. (2023b) and Mini et al. (2023) studied the internal representations of language models trained on a board game and found that it develops a representation of the current board state that can be extracted using linear probes.Importantly, they find that whether the representation can be extracted using a linear model depends on the encoding of the target label.Other linear representations have been found in language models; e.g.Tigges et al. (2023) find a linear representation of sentiment in text and Marks and Tegmark (2023) find a linear representation of the truth value of input text.Ivanitskiy et al. ( 2023) explored a transformer trained to perform a mazesolving task and found that they can successfully extract boundaries of the maze using linear probes.They also observed attention heads that attend to positions in the maze that are one step away from the current position.</p>
<p>B Experimental Setup</p>
<p>B.1 Implementation and Computing</p>
<p>All experiments were carried out on a single NVIDIA RTX A6000 GPU.The total computation time for training the transformer model was less than 24 hours.To generate the trees, we used networkx (Hagberg et al., 2008).For training and execution of all experiments, we used TransformerLens (Nanda and Bloom, 2022).For details on the model and training configuration, see Tables 4 and 5.</p>
<p>B.2 Size of Training Set</p>
<p>Our dataset consists of 150,000 randomly generated examples, each including a labeled binary tree with 16 nodes.The number of possible unlabeled binary trees with n + 1 nodes is given by the n-th
C(n) = (2n)! (n + 1)! • n!
When considering labeled binary trees, this number grows to (n+1)!•C(n) unique trees.This suggests that memorization is infeasible, and generalization is required for meaningful performance.</p>
<p>C Attention Head Composition</p>
<p>In this section, we perform additional experiments to verify that L1.H1 performs edge token concatenation, and L2.H1 is a deduction head.Elhage et al. (2021) show that transformers can learn induction heads in two different ways involving different compositions: the K-composition, where the W K of the head reads from the output of the previous head, and V-composition, where the W V of the head reads from the output of the previous head.Our findings suggest that the deduction heads we found in our model are a result of K-composition.In the following subsections, we adopt the conventions of (Elhage et al., 2021).</p>
<p>Layer 1 -Edge Token Concatenation Head L1.H1 serves as a variation of a previous token head studied in Elhage et al. (2021), effectively transferring source node information to the corresponding target node in each edge.This is captured  8) shows that the attention values are maximized when the query vector corresponds to the position embedding of an incoming node, and the key vector corresponds to the position embedding of the immediately preceding outgoing node.Then, following the goal node at position 45, the head persistently attends to the goal position.
M 0 QK = W P W 0 QK W T P M 0 QK (see Figure
Layer 2 -Deduction Head L2.H1 is a deduction head, which attends to the target node that matches the goal at positions in the path.It then moves information about the source node of that edge into the last position in the window.It can be viewed as a reverse induction head (Olsson et al., 2022) that uses a K-composition (Elhage et al., 2021)
M 1 QK = (MLP 0 (W 0 OV W E )+W 0 OV W E )W 1 QK W T E
This matrix shows the interactions of the embedding of the source and target tokens at layer 2 (see Figure 9).Our analysis is complicated by the fact that our model is not attention-only, as attention heads can compose with each other through the MLP, which makes similar analyses in later layers of the model intractable.However, our causal scrubbing results provide evidence that the attention heads in the subsequent layers implement a similar mechanism to L2.H1, but use the output of the previous layer's attention head to backward chain further up the tree.</p>
<p>D Register Tokens and Subgoals</p>
<p>In this section, we provide more details on the role of the register tokens in our model.From each register token position, the model attends to a random node in the context and starts backward-chaining from that node.The initial selection of a node by the register token can be viewed as identifying a subgoal, from which the model can perform backward chaining.This precomputation occurs before the actual goal is specified and occurs fully parallel to the main backward-chaining mechanism.These findings hint that transformers may exhibit an inductive bias towards learning highly parallelized algorithms when trained to perform search, planning, or reasoning.</p>
<p>To see whether there is any structure in the selection of subgoals, we empirically study which node the register tokens select as subgoals across 1000 samples.The results are illustrated in Figures 10.We observe that the model usually attends to the same tokens, e.g.position 36 attends to token [6] most of the time.However, we observe an interesting dynamic in which the register token selects a different subgoal in two cases:</p>
<p>1.If the node doesn't occur before the register token position, it cannot attend to it due to causal masking.</p>
<ol>
<li>If the node is a leaf node of the tree since it doesn't have a corresponding source token to attend to.</li>
</ol>
<p>To validate this, we again examine the probability of the model selecting subgoals in trees where the most common subgoal occurs before the register token position and is not a leaf node (see Figure 11).Further exploration reveals that the subgoals selected by each register token position can be somewhat understood through an examination of the embedding matrices.We evaluate a selection of seven register token positions that are used on several different examples and show their preferred subgoals.By composing the embedding and position embedding matrices with the QK-circuit of the first layer's attention head, we define R P as:
R P = W P W 1 QK W T E
where W E is the embedding matrix, W P is the position embedding matrix, and W 1 E , W 1 Q are the key and query projection matrices of L1.H1.This explains how the model selects subgoals, by having the key for each positional embedding of a register token match with some specific source node token.</p>
<p>E Attention Patterns</p>
<p>Here, we visualize attention patterns of our model on a few example inputs (see Figures 13 to 16), in Figure 12: Plot of R P addition to the example illustrated in Figure 4, to provide intuition for the backward chaining mechanism.In each example, we highlight the attention from the final token position and the register tokens that are causally relevant for the prediction.To determine these, we use attention knockout (Geva et al., 2023) on the register token positions.This prevents the final token from attending to register tokens by zero-ing out the attention weights.This allows us to test whether critical information propagates from them.More formally, let a, b ∈ [1, N ] be two positions such that a &lt;= b, we block x l b from attending to x ℓ a at layer ℓ &lt; L by updating the attention weights to that layer:
M l+1 ab = −∞∀j ∈ [1, H]
Thus, this restricts the source position from obtaining information from the target position, at that particular layer.To avoid information leakage across positions, we block attention edges in multiple layers rather than a single one.Specifically, we block attention to the register tokens in the final two layers of the model.</p>
<p>F Additional Experiment on the One-Step Lookahead</p>
<p>To evaluate the impact of the one-step lookahead, we perform causal scrubbing that incorporates the described mechanism.We reuse the experimental setup from Section 5.2 but add additional constraints to our resampling scheme.Specifically, we avoid resampling the contributions of the target node and register token positions through the attention heads of the last two layers.We visualize the results in Figure 17.</p>
<p>G Tuned Lens</p>
<p>In this section, we provide an additional piece of evidence in favor of the existence of the backward chaining mechanism.To understand how the predictions of a transformer are built layer-bylayer, Belrose et al. (2023) develop the Tuned Lens, a method that involves training a linear model to translate the activations from an intermediate layer directly to the input of the unembedding layer.</p>
<p>Inspired by this approach, we replace the last n layers of the model with a linear transformation trained to predict the next token from the residual stream activations x L−n .Similar to the Tuned Lens, this method allows us to skip over these layers and see the current best prediction that can be made from the model's residual stream.Intuitively, this allows us to peek at the iterative computations a transformer uses to compute the next token.Here, we present a visualization of some example trees and the results of the iterative computation (see Figures 18 to 21).These figures highlight the current best prediction a linear transformation could make based on the internal activations.We project the logits output by the linear model back onto the tree structure to better visualize the backward-chaining procedure.</p>
<p>Figure 5 :
5
Figure5: To test whether the model predicts the next step using backward chaining, we perform resampling ablations on each head using causal scrubbing.We find that we can recover close to 100 % of the performance of the model for paths up to length L − 1, providing strong evidence for our backward chaining hypothesis.</p>
<p>→[G]  has been stored in the final token position and a different subpath [A]→[E]→[B] has been stored in some register token.Then, during path merging, the model copies the subpath stored in the register token into the final token position, enabling it to move up the tree multiple steps at a time.</p>
<p>Figure 6 :
6
Figure6: To test whether the model relies on subpaths stored in register tokens, we perform resampling ablations on the register token positions at x 4 i .Here, we conducted 10 separate runs, each involving 1000 samples.For each run, we calculate the mean logit difference and report the 95 % confidence interval for the average effects observed across the runs.The results demonstrate that these subpaths are instrumental for paths longer than L − 1 steps.</p>
<p>Figure 7 :
7
Figure 7: Contribution of each target node position to the logits at the path position through the attention heads L5.H1 and L6.H1 on a specific example.</p>
<p>Pi] → [Childreni]} 0.00 47.88 98.20 Linear {[Pi] → [Leafsi]} 0.00 49.71 95.76</p>
<p>Figure 8 :
8
Figure 8: Visualization of M 0 QK</p>
<p>to map a sequence [A] [B] . . .[A] → [B], where [B] represents target nodes and [A] represents source nodes.This can again be verified by looking at the QK-circuit:</p>
<p>Figure 9 :
9
Figure 9: Visualization of a subset of M 1QK showing interactions between source and target node tokens.</p>
<p>Figure</p>
<p>Figure 10: Preferences in Subgoal Selection: Ratio of register tokens attending to different subgoals aggregated across 1000 trees.We consider a register token to select a subgoal based on an attention threshold of 0.3.</p>
<p>10 :
10
Figure 10: Preferences in Subgoal Selection: Ratio of register tokens attending to different subgoals aggregated across 1000 trees.We consider a register token to select a subgoal based on an attention threshold of 0.3.</p>
<p>Figure 11 :
11
Figure11: Preferences in Subgoal Selection: Ratio of register tokens attending to different subgoals aggregated across 1000 trees where the node it attends to most often is not a leaf node of the tree and occurs before the register token.We consider a register token to select a subgoal based on an attention threshold of 0.3.</p>
<p>Figure 14 :Figure 15 :Figure 16 :
141516
Figure 14: Visualization of multi-layer attention patterns on an example input, similar to Figure 4. We show the attention from three different positions: the path position and register token at position 41</p>
<p>Figure 17 :
17
Figure17: To test the impact of the one-step lookahead, we replicate the causal srubbing experiment from Section 5.2 but add additional constraints to our resampling scheme.We find that we can recover most of the model performance across the full training distribution.</p>
<p>Figure 18 :
18
Figure 18: Example 1 (Path Length 5): Results of a linear transformation to predict the next step based on the residual stream activations after each layer, projected onto the tree structure.The yellow border highlights the current best prediction(s) of the linear transformation.</p>
<p>Figure 19 :
19
Figure 19: Example 2 (Path Length 8): Results of a linear transformation to predict the next step based on the residual stream activations after each layer, projected onto the tree structure.The yellow border highlights the current best prediction(s) of the linear transformation.</p>
<p>Figure 20 :
20
Figure 20: Example 3 (Path Length 10): Results of a linear transformation to predict the next step based on the residual stream activations after each layer, projected onto the tree structure.The yellow border highlights the current best prediction(s) of the linear transformation.</p>
<p>Figure 21 :
21
Figure 21: Example 4 (Path Length 13): Results of a linear transformation to predict the next step based on the residual stream activations after each layer, projected onto the tree structure.The yellow border highlights the current best prediction(s) of the linear transformation.</p>
<p>Prompt Format.The model receives input in a structured format, with each box representing a token.The edge list of the tree is denoted as token pairs [A 1 ][B 1 ], . . .[A n ][B n ] followed by the task specification, including the goal [G] and the root node [P 1 ].The model's objective is to predict the nodes in the path [P 2 ] . . .[P m1 ], culminating in the goal node [P m ] = [G].For simplification, our tokenization distinguishes tokens representing source and target nodes of each edge, such as [15] and [→15].
Generate a Random Tree2. Select a Leaf Node as Goal3. Generate Path from Root to GoalFigure 2: Data Generation. To generate our training set, we (1) generate a binary tree, (2) select a leaf node as thegoal node, and (3) determine the path from the root to the goal node.Prompt: 10 → 3 , 10 → 14 , 3 → 7 , 3 → 1 , 7 → 6 , 6 → 13 , 7 → 5 , 1 → 15 , 15 → 9 , 14 → 4 , 14 → 8 , 4 → 0 , 0 → 2 Edge List , 0 → 11 , 8 → 12 | 9 : 10 Goal Node : Root Node Model Output: → 3 → 1 → 15 → 9 PathFigure 3:</p>
<p>Table 1 :
1
F1 score of linear probes trained to predict the edge [A][B] given the residual stream activations at position [A] or [B].In addition, we report the performance of a linear probe to predict the goal node [G] from the positions in the path [P i ].</p>
<p>Table 2 :
2
F1 score of a linear probe trained to predict the adjacency matrix of the subpath we hypothesise to be encoded in the activations x 4 i at register token positions.</p>
<p>Table 4 :
4
Model Configuration
ParameterValue# of layers6# of heads1Residual Stream dim.128Attention Head dim.128Feed-Forward dim.512Activation FunctiongeluVocabulary Size35Context Size63Table 5: Training ConfigurationParameterValueLearning Rate1e-3OptimizerAdamWBatch Size64Betas(0.9, 0.99)Weight Decay0.01Catalan number (Catalan, 1838):
AcknowledgementsJannik Brinkmann is supported by the German Federal Ministry for Digital and Transport (BMDV) and the German Federal Ministry for Economic Affairs and Climate Action (BMWK).Abhay Sheshadri and Victor Levoso have been supported by Lightspeed Grants.
Understanding intermediate layers using linear classifier probes. References Guillaume, Alain , Yoshua Bengio, Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev Mckinney, Stella Biderman, and Jacob Steinhardt. 2018. 2023Eliciting latent predictions from transformers with the tuned lens</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21. the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. Emily M Bender, Alexander Koller, 10.18653/v1/2020.acl-main.463Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>On the Ability and Limitations of Transformers to Recognize Formal Languages. Satwik Bhattamishra, Kabir Ahuja, Navin Goyal, 10.18653/v1/2020.emnlp-main.576Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Note sur une équation aux différences finies. Eugène Charles, Catalan , Journal de Mathématiques Pures et Appliquées. 31838</p>
<p>Causal Scrubbing: a method for rigorously testing interpretability hypotheses. Lawrence Chan, Adrià Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt, Jenny , Ansh Radhakrishnan, Nate Buck, Thomas, 2022</p>
<p>Neural networks learn representation theory: Reverse engineering how networks perform group operations. Bilal Chughtai, Lawrence Chan, Neel Nanda, ICLR 2023 Workshop on Physics for Machine Learning. 2023</p>
<p>. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021Training verifiers to solve math word problems</p>
<p>Towards automated circuit discovery for mechanistic interpretability. Arthur Conmy, Augustine N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, Adrià Garriga-Alonso, 2023</p>
<p>Vision transformers need registers. Timothée Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski, 2023</p>
<p>Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Kevin Li, Elliot Wenliang, Chris Catt, Marcus Cundy, Shane Hutter, Joel Legg, Pedro A Veness, Ortega, Neural networks and the chomsky hierarchy. 2023</p>
<p>Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A mathematical framework for transformer circuits. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack ClarkTransformer Circuits Thread</p>
<p>Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines. Luciano Floridi, Massimo Chiriatti, 10.1007/s11023-020-09548-1202030</p>
<p>Dissecting recall of factual associations in auto-regressive language models. Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson, 2023</p>
<p>Roscoe: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, 2023</p>
<p>Think before you speak: Training language models with pause tokens. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, Vaishnavh, Nagarajan. 2023</p>
<p>Exploring network structure, dynamics, and function using networkx. A Aric, Daniel A Hagberg, Pieter J Schult, Swart, Proceedings of the 7th Python in Science Conference (SciPy2008). the 7th Python in Science Conference (SciPy2008)Pasadena, CA USA2008</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R Fabbri, Wojciech Kryscinski, Caiming Xiong, and Dragomir Radev. 2022. Folio: Natural language reasoning with first-order logic. Xi Victoria Lin</p>
<p>Towards a mechanistic interpretation of multi-step reasoning capabilities of language models. Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, Mrinmaya Sachan, 2023</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLR2022162</p>
<p>Alex F Michael Igorevich Ivanitskiy, Tilman Spies, Guillaume Räuker, Chris Corlouer, Lucia Mathwin, Can Quirke, Rusheb Rager, Dan Shah, Cecilia Valentine, Katsumi Diniz Behn, Samy Wu Inoue, Fung, Structured world representations in maze-solving transformers. 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023</p>
<p>Transformers learn shortcuts to automata. Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Samuel Marks, Max Tegmark, The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. 2023</p>
<p>Locating and editing factual associations in GPT. Kevin Meng, David Bau, Alex J Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. 2022</p>
<p>Provable limitations of acquiring meaning from ungrounded form: What will future language models understand? Transactions of the. William Merrill, Yoav Goldberg, Roy Schwartz, Noah A Smith, 10.1162/tacl_a_004122021Association for Computational Linguistics9</p>
<p>Saturated transformers are constant-depth threshold circuits. William Merrill, Ashish Sabharwal, Noah A Smith, 10.1162/tacl_a_00493Transactions of the Association for Computational Linguistics. 102022</p>
<p>Circuit component reuse across tasks in transformer language models. Jack Merullo, Carsten Eickhoff, Ellie Pavlick, 2024</p>
<p>Linguistic regularities in continuous space word representations. Tomas Mikolov, Wen-Tau Yih, Geoffrey Zweig, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAtlanta, GeorgiaAssociation for Computational Linguistics2013</p>
<p>Understanding and controlling a mazesolving policy network. Ulisse Mini, Peli Grietzer, Mrinank Sharma, Austin Meek, Monte Macdiarmid, Alexander Matt Turner, 2023</p>
<p>Transformerlens. Neel Nanda, Joseph Bloom, 2022</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>Emergent linear representations in world models of self-supervised sequence models. Neel Nanda, Andrew Lee, Martin Wattenberg, 10.18653/v1/2023.blackboxnlp-1.2Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLPSingaporeAssociation for Computational Linguistics2023b</p>
<p>Fact finding: Attempting to reverse-engineer factual recall on the neuron level. Neel Nanda, Senthooran Rajamanoharan, Janos Kramar, Rohin Shah, 2023c</p>
<p>. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlishScott Johnston, Andy Jones, Jackson Kernionand Chris Olah. 2022. In-context learning and induction heads</p>
<p>Attention is turing-complete. Jorge Pérez, Pablo Barceló, Javier Marinkovic, Journal of Machine Learning Research. 22752021</p>
<p>Toward transparent ai: A survey on interpreting the inner structures of deep neural networks. Stuart Russell, Peter Norvig, ; , Dylan Hadfield-Menell, Artificial intelligence. Upper Saddle River, New Jersey. Tilman Räuker, Anson Ho, Stephen Casper,2009. 20233 edition. Pearson</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, 10.18653/v1/2023.emnlp-main.435Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Lena Strobl, William Merrill, Gail Weiss, David Chiang, Dana Angluin, Transformers as recognizers of formal languages: A survey on expressivity. 2023</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Aaquib Syed, Can Rager, Arthur Conmy, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Online. Association for Computational Linguistics2023. 2021Attribution patching outperforms automated circuit discovery</p>
<p>Curt Tigges, John Oskar, Atticus Hollinsworth, Neel Geiger, Nanda, Linear representations of sentiment in large language models. 2023</p>
<p>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Look before you leap: A universal emergent decomposition of retrieval tasks in language models. Alexandre Variengien, Eric Winsor, 2023</p>
<p>Analyzing the structure of attention in a transformer language model. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, 10.18653/v1/W19-4808Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPFlorence, ItalyAssociation for Computational Linguistics2017. 2019Attention is all you need. Jesse Vig and Yonatan Belinkov</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart Shieber, ; Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, 10.18653/v1/2023.acl-long.153Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2020. 2023a33Advances in Neural Information Processing Systems</p>
<p>Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. Kevin Ro, Wang , Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Are transformers universal approximators of sequenceto-sequence functions. Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, Sanjiv Kumar, International Conference on Learning Representations. 2020</p>
<p>Transformer-based models are not yet perfect at learning to emulate structural recursion. Dylan Zhang, Curt Tigges, Zory Zhang, Stella Biderman, Maxim Raginsky, Talia Ringer, 2024</p>
<p>The clock and the pizza: Two stories in mechanistic explanation of neural networks. Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas, 2023</p>
<p>Visualization of multi-layer attention patterns on an example input, similar to Figure 4. We show the attention from three different positions: path position, register token at position 39, and register token at position 45. → 11, 131415 → 13 , 7 → 15 , 2 → 7 , 4 → 2 , 8 → 6 , 0 → 4 , 9 → 8 , 3 → 0 , 12 → 9 , 1 → 3 , 5 → 12 , 11 → 1 , 10 → 5 , 14 → 11 , 14 → 10</p>            </div>
        </div>

    </div>
</body>
</html>
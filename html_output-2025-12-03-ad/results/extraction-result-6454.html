<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6454 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6454</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6454</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-277999605</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.16855v1.pdf" target="_blank">Monte Carlo Planning with Large Language Model for Text-Based Game Agents</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6454.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6454.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC-DML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo planning with Dynamic Memory-guided Large language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An MCTS algorithm that uses an LLM as a prior policy and augments it with two explicit memory mechanisms — in-trial (short-term trajectory window) and cross-trial (episodic reflections) — to dynamically adjust action value estimates during planning in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>MONTE CARLO PLANNING WITH LARGE LANGUAGE MODEL FOR TEXT-BASED GAME AGENTS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MC-DML</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses GPT-3.5 as a prior policy inside a PUCT-style MCTS. The LLM consumes (1) in-trial memory M_i (recent action-observation history) and (2) cross-trial memory M_c (interpretable one-sentence reflections generated from past failed trajectories) to produce action probability priors which are combined with Q(s,a) and visit counts to select actions. Tree search uses dynamic pruning and rollouts; on game failures the LLM writes reflections to cross-trial memory which influence subsequent simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0125</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (multiple games, e.g., Zork1, Deephome, Ludicorp, Pentari, Detective, Library, Temple, Ztuu)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic buffer + short-term trajectory (interpretable cross-trial reflections and in-trial trajectory window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>In-trial memory: discrete action-observation tuples (o_{t-1}, a_{t-1}, o_t); Cross-trial memory: short, human-readable LLM-generated reflections (single-sentence summaries/suggestions derived from failed trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Rule-based LLM reflection generation on terminal/failure trajectories: when a simulation reaches a game failure the LLM is prompted to produce a reflection which is appended to M_c; M_c is capped (K=3 reflections per root) and older reflections beyond the cap are not collected (early termination). In-trial memory is taken as the last part of the current trajectory (no learned writer).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt-based retrieval: M_i and M_c are concatenated into the LLM prompt; the LLM conditions on both to produce action probability estimates (no separate retrieval index or learned retriever).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>No additional RL or fine-tuning of LLM; few-shot/zero-shot prompting inside MCTS (planning-without-learning paradigm).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average game score (mean ± std) across runs; comparisons also reported versus baselines and ablated versions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Examples (mean ± std final score): Zork1: 48.66 ± 1.89; Deephome: 67.0 ± 1.41; Detective: 346.67 ± 9.43; Ztuu: 23.67 ± 1.92. (Tables report per-game average scores with full MC-DML including M_i and M_c.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablations reported: Without cross-trial memory (w.o. M_c): Zork1 38.33 ± 2.89; Deephome 62.66 ± 0.94; Detective 326.67 ± 4.71; Ztuu 20.66 ± 0.47. Without both cross-trial and in-trial memory (w.o. M_c, M_i, DP): Zork1 21.67 ± 4.7; Deephome 51 ± 14.9; Detective 320 ± 0.0; Ztuu 16.33 ± 0.94. (See paper Table 4.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Removing cross-trial memory (M_c) reduces scores across games; removing both M_c and the in-trial window (M_i) produces a substantially larger performance drop. Dynamic pruning (DP) helps in some games (e.g., Ztuu). The paper provides per-game ablation numbers showing consistent benefit from M_c and M_i.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>In-trial memory is implemented as a short trajectory window and may miss clues encountered far earlier ('Needle In a Haystack' problem). Input size is constrained by the LLM context window, requiring truncation/compression. The authors note they used a simplified short-window in-trial memory for current experiments and suggest improved storage/retrieval is future work.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Combine short-term detailed trajectory context (in-trial window) with compact, interpretable cross-trial reflections; cap cross-trial memory (they use K=3) to limit input size; compress/truncate prompts to fit context window; use the LLM's log-probabilities to form action priors (or self-consistency/verbalized probability estimates when token probs aren't available); avoid injecting game-specific hints into prompts (keep general commonsense prior).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Monte Carlo Planning with Large Language Model for Text-Based Game Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6454.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6454.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflection agent (Reflexion)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflection / Reflexion: LLM agent with cross-trial reflections (Shinn et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based baseline that generates reflections after failed rounds and uses those reflections as input for subsequent rounds to guide future behavior; implemented here as a baseline that the authors compare against MC-DML.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>MONTE CARLO PLANNING WITH LARGE LANGUAGE MODEL FOR TEXT-BASED GAME AGENTS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflection agent (Reflexion-style)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The LLM plays multiple rounds; after each round it generates a brief reflection (one-sentence suggestion) describing reasons for failure and improvement; these reflections are fed into subsequent rounds as cross-trial memory to influence action selection. In the paper's baseline, the reflection module is limited to 3 reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0125 (baseline settings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (used as baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>cross-trial episodic reflections (episodic buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>One-sentence reflections summarizing reasons for past failures and suggesting improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>After each round (upon failure) the LLM is prompted to generate a reflection which is appended and passed to the next round; the baseline caps number of reflections (they used up to 3).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt-based: reflections are concatenated into future prompts for the LLM to condition on.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Few-shot/zero-shot prompting over multiple rounds (no RL fine-tuning in this baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final game scores (mean) across rounds; used as a baseline for comparison to MC-DML.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Not reported as a per-game table in the provided text; the paper states MC-DML outperforms or matches LLM/MCTS-based baselines (including the Reflection agent) in 8 of 9 games, but exact Reflection-agent per-game numbers are not included in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not applicable in this paper (the Reflection baseline's defining feature is the cross-trial memory; the paper does not report a direct ablation of that baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>The paper does not include ablations of the Reflection baseline itself; MC-DML ablations (which include removing M_c) show that cross-trial reflections substantially help, supporting the Reflection approach in principle.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Limited number of reflections (they cap at 3) and potential context window constraints; the Reflection baseline is reported heuristically to help but the paper emphasizes MC-DML's integrated use during MCTS is more effective.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Store concise, interpretable reflections and feed them into future rounds; limit number of stored reflections to fit context window (paper used 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Monte Carlo Planning with Large Language Model for Text-Based Game Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6454.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6454.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM agent (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-only interactive agent (direct LLM policy without MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental baseline where the LLM acts directly as the game-playing policy (selects actions from valid action set conditioned on trajectory history), used to test data contamination and LLM raw performance in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>MONTE CARLO PLANNING WITH LARGE LANGUAGE MODEL FOR TEXT-BASED GAME AGENTS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM agent (direct policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM selects actions directly from the valid action list conditioned on the current trajectory; temperature was set low (0.0 or 0.1) and the highest-probability action chosen; includes a heuristic to break loops (switch to random action after 5 repeated same actions in same state). No explicit memory beyond the prompt context window (i.e., trajectory history is used but no cross-trial reflection store unless augmented).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0125</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (multiple games)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-context window / prompt (no explicit memory module beyond immediate prompt history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw observation and action sequence included in prompt (trajectory history) as context; no separate persistent memory structure.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Not applicable beyond appending new observations/actions to the prompt context; no explicit episodic memory writes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Context window / prompt inclusion only.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Zero-shot/few-shot prompting (no training).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final game score and qualitative behavior (looping was noted).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>N/A (this baseline uses only prompt context; the paper reports it performed poorly overall and often fell into action loops).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Authors observed the raw LLM policy tends to loop and struggles to balance exploration/exploitation; they apply a pragmatic loop-breaking heuristic (random action after 5 repeats).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Poor exploration-exploitation balance; repetitive loops in the same state; lacks long-term planning; no capacity to learn from past failed rounds unless explicitly augmented with reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>When using a raw LLM as a policy, add mechanisms to avoid repeated action loops (e.g., randomization after repeats) and consider augmenting with external memory or planning modules for long-horizon games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Monte Carlo Planning with Large Language Model for Text-Based Game Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6454.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6454.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-MCTS (Zhao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM as fixed prior policy in MCTS (Large language models as commonsense knowledge for large-scale task planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that uses an LLM to provide a fixed prior policy for MCTS; the LLM is not updated from experience during planning and does not maintain persistent cross-trial memory in the manner of MC-DML.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as commonsense knowledge for large-scale task planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>MONTE CARLO PLANNING WITH LARGE LANGUAGE MODEL FOR TEXT-BASED GAME AGENTS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-MCTS (fixed prior)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses an LLM to produce action priors for MCTS but treats the LLM as a static prior that does not adapt based on experience or failures; suited for commonsense planning tasks but less effective in highly uncertain text-game environments according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>none (fixed LLM prior)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>LLM used as inference-time prior; PUCT-style planning with static prior (no learning-from-experience of the LLM in that method).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Task-specific metrics in cited work (paper here notes LLM-MCTS is effective on commonsense tasks but lacks experience-driven updating).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>N/A (method does not use persistent memory to update priors from failures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Mentioned as related work; authors argue its static prior nature limits effectiveness in uncertain/text-based games compared to MC-DML's memory-guided dynamic priors.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Cannot improve priors based on past experience or external feedback; less suitable for uncertain environments like complex text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>The paper suggests augmenting fixed LLM priors with mechanisms to learn from failures (e.g., reflections) to handle high-uncertainty text-game settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Monte Carlo Planning with Large Language Model for Text-Based Game Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Large language models as commonsense knowledge for large-scale task planning <em>(Rating: 2)</em></li>
                <li>Monte-carlo planning and learning with language action value estimates <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6454",
    "paper_id": "paper-277999605",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "MC-DML",
            "name_full": "Monte Carlo planning with Dynamic Memory-guided Large language model",
            "brief_description": "An MCTS algorithm that uses an LLM as a prior policy and augments it with two explicit memory mechanisms — in-trial (short-term trajectory window) and cross-trial (episodic reflections) — to dynamically adjust action value estimates during planning in text-based games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "MONTE CARLO PLANNING WITH LARGE LANGUAGE MODEL FOR TEXT-BASED GAME AGENTS",
            "agent_name": "MC-DML",
            "agent_description": "Uses GPT-3.5 as a prior policy inside a PUCT-style MCTS. The LLM consumes (1) in-trial memory M_i (recent action-observation history) and (2) cross-trial memory M_c (interpretable one-sentence reflections generated from past failed trajectories) to produce action probability priors which are combined with Q(s,a) and visit counts to select actions. Tree search uses dynamic pruning and rollouts; on game failures the LLM writes reflections to cross-trial memory which influence subsequent simulations.",
            "model_name": "gpt-3.5-turbo-0125",
            "model_size": null,
            "benchmark_name": "Jericho (multiple games, e.g., Zork1, Deephome, Ludicorp, Pentari, Detective, Library, Temple, Ztuu)",
            "memory_used": true,
            "memory_type": "episodic buffer + short-term trajectory (interpretable cross-trial reflections and in-trial trajectory window)",
            "memory_representation": "In-trial memory: discrete action-observation tuples (o_{t-1}, a_{t-1}, o_t); Cross-trial memory: short, human-readable LLM-generated reflections (single-sentence summaries/suggestions derived from failed trajectories).",
            "memory_update_mechanism": "Rule-based LLM reflection generation on terminal/failure trajectories: when a simulation reaches a game failure the LLM is prompted to produce a reflection which is appended to M_c; M_c is capped (K=3 reflections per root) and older reflections beyond the cap are not collected (early termination). In-trial memory is taken as the last part of the current trajectory (no learned writer).",
            "memory_retrieval_method": "Prompt-based retrieval: M_i and M_c are concatenated into the LLM prompt; the LLM conditions on both to produce action probability estimates (no separate retrieval index or learned retriever).",
            "training_method": "No additional RL or fine-tuning of LLM; few-shot/zero-shot prompting inside MCTS (planning-without-learning paradigm).",
            "evaluation_metric": "Average game score (mean ± std) across runs; comparisons also reported versus baselines and ablated versions.",
            "performance_with_memory": "Examples (mean ± std final score): Zork1: 48.66 ± 1.89; Deephome: 67.0 ± 1.41; Detective: 346.67 ± 9.43; Ztuu: 23.67 ± 1.92. (Tables report per-game average scores with full MC-DML including M_i and M_c.)",
            "performance_without_memory": "Ablations reported: Without cross-trial memory (w.o. M_c): Zork1 38.33 ± 2.89; Deephome 62.66 ± 0.94; Detective 326.67 ± 4.71; Ztuu 20.66 ± 0.47. Without both cross-trial and in-trial memory (w.o. M_c, M_i, DP): Zork1 21.67 ± 4.7; Deephome 51 ± 14.9; Detective 320 ± 0.0; Ztuu 16.33 ± 0.94. (See paper Table 4.)",
            "has_comparative_results": true,
            "ablation_findings": "Removing cross-trial memory (M_c) reduces scores across games; removing both M_c and the in-trial window (M_i) produces a substantially larger performance drop. Dynamic pruning (DP) helps in some games (e.g., Ztuu). The paper provides per-game ablation numbers showing consistent benefit from M_c and M_i.",
            "reported_limitations": "In-trial memory is implemented as a short trajectory window and may miss clues encountered far earlier ('Needle In a Haystack' problem). Input size is constrained by the LLM context window, requiring truncation/compression. The authors note they used a simplified short-window in-trial memory for current experiments and suggest improved storage/retrieval is future work.",
            "best_practices_recommendations": "Combine short-term detailed trajectory context (in-trial window) with compact, interpretable cross-trial reflections; cap cross-trial memory (they use K=3) to limit input size; compress/truncate prompts to fit context window; use the LLM's log-probabilities to form action priors (or self-consistency/verbalized probability estimates when token probs aren't available); avoid injecting game-specific hints into prompts (keep general commonsense prior).",
            "uuid": "e6454.0",
            "source_info": {
                "paper_title": "Monte Carlo Planning with Large Language Model for Text-Based Game Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Reflection agent (Reflexion)",
            "name_full": "Reflection / Reflexion: LLM agent with cross-trial reflections (Shinn et al., 2024)",
            "brief_description": "An LLM-based baseline that generates reflections after failed rounds and uses those reflections as input for subsequent rounds to guide future behavior; implemented here as a baseline that the authors compare against MC-DML.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "paper_title": "MONTE CARLO PLANNING WITH LARGE LANGUAGE MODEL FOR TEXT-BASED GAME AGENTS",
            "agent_name": "Reflection agent (Reflexion-style)",
            "agent_description": "The LLM plays multiple rounds; after each round it generates a brief reflection (one-sentence suggestion) describing reasons for failure and improvement; these reflections are fed into subsequent rounds as cross-trial memory to influence action selection. In the paper's baseline, the reflection module is limited to 3 reflections.",
            "model_name": "gpt-3.5-turbo-0125 (baseline settings)",
            "model_size": null,
            "benchmark_name": "Jericho (used as baseline in experiments)",
            "memory_used": true,
            "memory_type": "cross-trial episodic reflections (episodic buffer)",
            "memory_representation": "One-sentence reflections summarizing reasons for past failures and suggesting improvements.",
            "memory_update_mechanism": "After each round (upon failure) the LLM is prompted to generate a reflection which is appended and passed to the next round; the baseline caps number of reflections (they used up to 3).",
            "memory_retrieval_method": "Prompt-based: reflections are concatenated into future prompts for the LLM to condition on.",
            "training_method": "Few-shot/zero-shot prompting over multiple rounds (no RL fine-tuning in this baseline).",
            "evaluation_metric": "Final game scores (mean) across rounds; used as a baseline for comparison to MC-DML.",
            "performance_with_memory": "Not reported as a per-game table in the provided text; the paper states MC-DML outperforms or matches LLM/MCTS-based baselines (including the Reflection agent) in 8 of 9 games, but exact Reflection-agent per-game numbers are not included in the excerpt.",
            "performance_without_memory": "Not applicable in this paper (the Reflection baseline's defining feature is the cross-trial memory; the paper does not report a direct ablation of that baseline).",
            "has_comparative_results": false,
            "ablation_findings": "The paper does not include ablations of the Reflection baseline itself; MC-DML ablations (which include removing M_c) show that cross-trial reflections substantially help, supporting the Reflection approach in principle.",
            "reported_limitations": "Limited number of reflections (they cap at 3) and potential context window constraints; the Reflection baseline is reported heuristically to help but the paper emphasizes MC-DML's integrated use during MCTS is more effective.",
            "best_practices_recommendations": "Store concise, interpretable reflections and feed them into future rounds; limit number of stored reflections to fit context window (paper used 3).",
            "uuid": "e6454.1",
            "source_info": {
                "paper_title": "Monte Carlo Planning with Large Language Model for Text-Based Game Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLM agent (baseline)",
            "name_full": "LLM-only interactive agent (direct LLM policy without MCTS)",
            "brief_description": "An experimental baseline where the LLM acts directly as the game-playing policy (selects actions from valid action set conditioned on trajectory history), used to test data contamination and LLM raw performance in text games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "MONTE CARLO PLANNING WITH LARGE LANGUAGE MODEL FOR TEXT-BASED GAME AGENTS",
            "agent_name": "LLM agent (direct policy)",
            "agent_description": "LLM selects actions directly from the valid action list conditioned on the current trajectory; temperature was set low (0.0 or 0.1) and the highest-probability action chosen; includes a heuristic to break loops (switch to random action after 5 repeated same actions in same state). No explicit memory beyond the prompt context window (i.e., trajectory history is used but no cross-trial reflection store unless augmented).",
            "model_name": "gpt-3.5-turbo-0125",
            "model_size": null,
            "benchmark_name": "Jericho (multiple games)",
            "memory_used": false,
            "memory_type": "long-context window / prompt (no explicit memory module beyond immediate prompt history)",
            "memory_representation": "Raw observation and action sequence included in prompt (trajectory history) as context; no separate persistent memory structure.",
            "memory_update_mechanism": "Not applicable beyond appending new observations/actions to the prompt context; no explicit episodic memory writes.",
            "memory_retrieval_method": "Context window / prompt inclusion only.",
            "training_method": "Zero-shot/few-shot prompting (no training).",
            "evaluation_metric": "Final game score and qualitative behavior (looping was noted).",
            "performance_with_memory": "N/A (this baseline uses only prompt context; the paper reports it performed poorly overall and often fell into action loops).",
            "performance_without_memory": "N/A",
            "has_comparative_results": false,
            "ablation_findings": "Authors observed the raw LLM policy tends to loop and struggles to balance exploration/exploitation; they apply a pragmatic loop-breaking heuristic (random action after 5 repeats).",
            "reported_limitations": "Poor exploration-exploitation balance; repetitive loops in the same state; lacks long-term planning; no capacity to learn from past failed rounds unless explicitly augmented with reflections.",
            "best_practices_recommendations": "When using a raw LLM as a policy, add mechanisms to avoid repeated action loops (e.g., randomization after repeats) and consider augmenting with external memory or planning modules for long-horizon games.",
            "uuid": "e6454.2",
            "source_info": {
                "paper_title": "Monte Carlo Planning with Large Language Model for Text-Based Game Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLM-MCTS (Zhao et al.)",
            "name_full": "LLM as fixed prior policy in MCTS (Large language models as commonsense knowledge for large-scale task planning)",
            "brief_description": "A prior approach that uses an LLM to provide a fixed prior policy for MCTS; the LLM is not updated from experience during planning and does not maintain persistent cross-trial memory in the manner of MC-DML.",
            "citation_title": "Large language models as commonsense knowledge for large-scale task planning",
            "mention_or_use": "mention",
            "paper_title": "MONTE CARLO PLANNING WITH LARGE LANGUAGE MODEL FOR TEXT-BASED GAME AGENTS",
            "agent_name": "LLM-MCTS (fixed prior)",
            "agent_description": "Uses an LLM to produce action priors for MCTS but treats the LLM as a static prior that does not adapt based on experience or failures; suited for commonsense planning tasks but less effective in highly uncertain text-game environments according to the authors.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": null,
            "memory_used": false,
            "memory_type": "none (fixed LLM prior)",
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "LLM used as inference-time prior; PUCT-style planning with static prior (no learning-from-experience of the LLM in that method).",
            "evaluation_metric": "Task-specific metrics in cited work (paper here notes LLM-MCTS is effective on commonsense tasks but lacks experience-driven updating).",
            "performance_with_memory": "N/A (method does not use persistent memory to update priors from failures).",
            "performance_without_memory": "N/A",
            "has_comparative_results": false,
            "ablation_findings": "Mentioned as related work; authors argue its static prior nature limits effectiveness in uncertain/text-based games compared to MC-DML's memory-guided dynamic priors.",
            "reported_limitations": "Cannot improve priors based on past experience or external feedback; less suitable for uncertain environments like complex text-based games.",
            "best_practices_recommendations": "The paper suggests augmenting fixed LLM priors with mechanisms to learn from failures (e.g., reflections) to handle high-uncertainty text-game settings.",
            "uuid": "e6454.3",
            "source_info": {
                "paper_title": "Monte Carlo Planning with Large Language Model for Text-Based Game Agents",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Large language models as commonsense knowledge for large-scale task planning",
            "rating": 2,
            "sanitized_title": "large_language_models_as_commonsense_knowledge_for_largescale_task_planning"
        },
        {
            "paper_title": "Monte-carlo planning and learning with language action value estimates",
            "rating": 2,
            "sanitized_title": "montecarlo_planning_and_learning_with_language_action_value_estimates"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        }
    ],
    "cost": 0.014608499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MONTE CARLO PLANNING WITH LARGE LANGUAGE MODEL FOR TEXT-BASED GAME AGENTS
23 Apr 2025</p>
<p>Zijing Shi zijing.shi@student.uts.edu.au 
AAII
University of Technology</p>
<p>Meng Fang meng.fang@liverpool.ac.uk 
University of Liverpool</p>
<p>Ling Chen ling.chen@uts.edu.au 
AAII
University of Technology</p>
<p>MONTE CARLO PLANNING WITH LARGE LANGUAGE MODEL FOR TEXT-BASED GAME AGENTS
23 Apr 20257F1F42953678A81E0AB20E0C5DE1C52EarXiv:2504.16855v1[cs.CL]
Text-based games provide valuable environments for language-based autonomous agents.However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations.Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities.In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm.MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms.Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning.We conduct experiments on a series of text-based games from the Jericho benchmark.Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations.This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments. 1</p>
<p>INTRODUCTION</p>
<p>Text-based games serve as valuable environments for studying various natural language processing (NLP) and sequential decision-making problems (Narasimhan et al., 2015;Xu et al., 2020).In these games, agents navigate environments using textual commands and deal with limited observability.Unlike simple synthetic games (Côté et al., 2019), human-designed adventure games feature dynamic state spaces and sparse rewards, presenting significant challenges (Hausknecht et al., 2020).Existing game agents are typically based on reinforcement learning (RL) and employ ϵ-greedy or softmax policies for action exploration.However, they lack long-term planning abilities (Osborne et al., 2022).</p>
<p>Planning for text-based games presents unique challenges, as each language action is treated as a discrete token, making uncertainty-driven exploration without understanding the potential future impacts or the natural language described game state.Previous works that integrate Monte Carlo Tree Search (MCTS) with learning models have shown remarkable proficiency in classical games such as Go and Atari (Browne et al., 2012;Świechowski et al., 2023).These methods are based on the architectures pioneered by AlphaGo and AlphaGo Zero, which utilize policy and value networks to evaluate and prioritize potential moves, thereby significantly enhancing gameplay (Silver et al., 2016;2017).However, MCTS still faces challenges in real-world scenarios.Learning models typically need a substantial warm-up period to learn effectively.Their performance relies on data obtained from MCTS planning, which is confined by the practical limitations of tree size and depth.Specifically, in text-based games, MCTS lacks the necessary language understanding and reasoning abilities.In response, Jang et al. (2020) suggested guiding exploration in MCTS planning through the evaluation of action's semantic similarity.This approach assumes that an action may possess value if its similar actions taken previously have high Q values.While effective in certain games, this assumption may be less reliable in more dynamic settings.</p>
<p>The recent emergence of Large Language Models (LLMs) have shown remarkable capabilities in quickly generating viable initial plans for decision-making tasks, even with minimal or no prior examples.Some research explores various prompting techniques, such as Reflection (Shinn et al., 2024) and Tree-of-Thought (Yao et al., 2024), which further enhance LLM reasoning for interactive tasks.Despite achieving near-saturated performance in simpler environments such as ALFworld (Shridhar et al., 2020), they continue to face challenges in more complex settings.A primary challenge lies in translating the plans generated by LLMs into executable actions.Furthermore, LLMs often struggle to balance exploration and exploitation, which hinders their ability to navigate extensive state spaces efficiently.</p>
<p>In this study, we explore the potential of LLMs to enhance MCTS planning in complex interactive tasks.We aim to answer two questions: (1) Can LLMs, with their encoded knowledge, enhance action exploration within MCTS planning, thereby improving sample efficiency and task performance?(2) Can LLMs, with their few-shot learning capabilities, dynamically adapt action guidance based on past experiences during planning?To address these questions, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML).This algorithm leverages the language understanding and commonsense reasoning capabilities of LLMs and the exploration benefits of tree-search approaches.By integrating both in-trial and cross-trial memory into LLMs, MC-DML enables dynamic adjustments of action evaluation during the MCTS planning.</p>
<p>We conduct experiments using a series of text-based games from the Jericho benchmark (Hausknecht et al., 2020).These games are characterized by numerous branching paths and sparse rewards.The agent, operating under limited observability, must extensively explore the environment to solve complex puzzles.Our results demonstrate that the MC-DML algorithm enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations for policy optimization.Additionally, we perform ablation studies to highlight the role of the memory mechanism in LLM policy.</p>
<p>Our main contributions are summarized as follows: First, we propose an MCTS-based algorithm that integrates an LLM to enhance action exploration in complex textual interactive tasks.Second, we develop an LLM agent equipped with both in-trial and cross-trial memory, enabling dynamic language action value estimation in tree-search planning phrase.Third, our experiments on a series of text-based games demonstrate that the proposed algorithm significantly improves performance across multiple games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations.</p>
<p>PRELIMINARY</p>
<p>MONTE-CARLO TREE SEARCH</p>
<p>Upper Confidence bound for Trees (UCT) MCTS (Kocsis &amp; Szepesvári, 2006;Coulom, 2006) operates by iteratively developing a decision tree through four key phases: selection, expansion, simulation, and backpropagation.Within the search tree, the standard MCTS method uses UCT to choose action a * at each node, balancing exploitation based on the Q-value with exploration driven by uncertainty.The formula for UCT is as follows:
a * = arg max a∈A(s) Q(s, a) + C uct • ln N (s) N (s, a)(1)
where Q(s, a) is the average reward for action a in state s, N (s, a) is the number of times action a is chosen in state s, N (s) is the visit count to state s, C uct is a constant that balances exploration and exploitation.</p>
<p>Predictor UCT (PUCT) One limitation of UCT is its dependence on Monte Carlo averages to estimate state values, which can result in significant search inefficiencies, especially in text-based games with high branching factors.PUCT partially overcomes these challenges by incorporating PUCB, which utilizes a prior action distribution π(•|s) to estimate action values under state s and prioritize exploration (Silver et al., 2017;2018).The formula for PUCT is as follows:
a * = arg max a∈A(s) Q(s, a) + C puct • π(a|s) • N (s) 1 + N (s, a)(2)
Here, π(•|s) is usually a neural network that trained via behavioral cloning using (s, a * ) samples from previous tree search results.However, PUCT still faces several challenges.Firstly, the training data for policy network is sourced from MCTS planning.In practice, due to limitations in tree size and depth, the training data may not be optimal.To achieve effective learning, it often requires multiple iterations using a planning-then-learning paradigm, which is time-consuming.Additionally, this training data is specific to certain environments, limiting the ability of the policy network to generalize across various games.Furthermore, the nature of its supervised learning approach restricts its strategic depth and impairs its long-term planning capabilities.</p>
<p>TEXT-BASED GAMES AS POMDPS</p>
<p>The text-based game can be modeled as a Partially Observable Markov Decision Process (POMDP) (Narasimhan et al., 2015), represented by (S, T, A, O, R, γ).At each time t, the agent cannot directly observe the environmental state s t from S. Instead, it infers the state through a textual observation o t from O. When the agent performs an action a t from A, the environment transitions to the next state according to the hidden transition function T .Meanwhile, the agent receives a reward r t = R(s t , a t ) and the subsequent observation o t+1 from the game environment.The agent's goal is to optimize actions to maximize the expected total discounted rewards
R t = E[ ∞ t=0 γ t r t ]
, where γ ranges from 0 to 1, indicating the discount factor.</p>
<p>METHOD</p>
<p>In this study, we focus on human-designed text adventure games, which present two significant challenges.First, these games feature a vast combinatorial action space.To manage this complexity, benchmarks such as Jericho provide a predefined set of valid actions at each step by filtering out inadmissible commands.However, this still results in a dynamic action space that varies with the game state, leading to numerous game branches.Moreover, these games are characterized by sparse rewards and multiple bottleneck states.Figure 1 illustrates an example of a bottleneck state in the game Zork1.Based on the current observation, the agent selects from the available actions, leading to different game branches.An agent optimized for cumulative rewards might choose open trapdoor, resulting in a significant immediate reward but also leading to subsequent death.To progress in the game, the agent must explore necessary actions without receiving any immediate reward signals.This requires the agent to combine semantic reasoning with long-term planning capabilities.</p>
<p>MC with Dynamic Memory Guided LLM (MC-DML)</p>
<p>Living Room open trapdoor take latern</p>
<p>You ar e pl ayi ng a game.Your cur r ent obser vat i on i s: { I N-TRAI L MEMORY} Your memor y about t hi s game i s: { CROSS-TRI AL MEMORY} .Based on your obser vat i on, pl ease sel ect one act i on f r om { VALI D ACTI ONS} t o expl or e. Thi s i s a l og of unsuccessf ul gamepl ay: { TRAJECTORY} Pl ease i dent i f y t he r easons f or f ai l ur e and suggest i mpr ovement s f or t he next r ound.</p>
<p>Living Room</p>
<p>Movi ng down i nt o t he dar k pl ace wi t hout a l i ght sour ce or any ot her i t ems t o hel p navi gat e. . .This approach more closely aligns with the human thought process.</p>
<p>MONTE CARLO PLANNING WITH DYNAMIC MEMORY-GUIDED LLMS (MC-DML)</p>
<p>MC-DML consists of four stages: selection, expansion, simulation, and backpropagation, and finally predicts an action based on the simulations.During the expansion phase, MC-DML employs an LLM as the prior policy within the PUCT algorithm.Based on the current tree state, the LLM assigns non-uniform search priorities to each optional action.After an action is selected, the expansion phase adds a new node to the search tree.In the simulation phase, MC-DML conducts multiple rollouts from this new node to evaluate the potential outcomes of the chosen action.The simulation results are then backpropagated to update the Q-value estimates and visit counts of the relevant nodes.In text-based games, the current state is not fully observable.Therefore, we equip the LLM with a dynamic memory mechanism, utilizing in-trial memory M i and cross-trial memory M c .M i contains the current trajectory history, representing the game state, while M c includes experiences from previous failure trajectories, used to dynamically adjust the action value estimation.</p>
<p>Learning from In-Trial Memory The in-trial memory M i includes a sequence of past observations and actions.Using this memory, we prompt the LLM to generate a probability distribution of valid actions π(•|s) grounded in commonsense.The probability of an action a is calculated by accumulating the conditional probabilities of its tokens.We use the GPT-3.5 model, which provides log probabilities for the top potential answers.These probabilities are then normalized using the softmax function.For APIs where token probabilities are unavailable, this can be achieved through self-consistency (Wang et al., 2022) and verbalized methods (Lin et al., 2022) 2 .</p>
<p>Reflection on Cross-Trial Memory</p>
<p>In-trial memory is a form of short-term memory that relies on the LLM's commonsense but lacks flexibility.Inspired by Shinn et al. (2024), we develop crosstrial memory M c , an interpretable and enduring form of episodic memory that allows agents to learn from past failures.In MCTS, the agent repeatedly simulates from the root node to explore various paths.This restart mechanism allows agent to reflection on the segment of trajectory and resume play from that save point.Figure 2 illustrates the process of updating and utilizing crosstrial memory in MC-DML.During the tree search, when the agent encounters a terminal state due to game failure, the LLM analyzes this trajectory and generates a reflection.In subsequent simulations under the same root node, we combine in-trial memory and cross-trial memory to adjust the LLM's action estimations.The formula for action selection in MC-DML is as follows:
a * = arg max a∈A Q(s, a) + C puct • LLM (a|M i , M c , p) • N (s) 1 + N (s, a)(3)
where LLM (a|M i , M c , p) is the action probability distribution generated by the LLM policy, p is the given prompt consisting of an instruction and optional actions A, Q(o, a) is the average reward for action a in tree state s.</p>
<p>ALGORITHM</p>
<p>Algorithm 1 Monte Carlo Planning with Dynamic Memory-Guided LLM (MC-DML)
end if 18: [a, rollout] ← SELECTACTION(h) 19: [r, s ′ , o ′ ] ← T (s, a) 20: h ′ ← h + a + o ′ 21: if rollout then 22: R ′ ← ROLLOUT(s ′ , h ′ , t + 1) 23: else 24: R ′ ← SIMULATE(s ′ , h ′ , t + 1) 25: end if 26: R ← r + γ • R ′ 27: N (h) ← N (h) + 1 28: N (h, a) ← N (h, a) + 1 29: Q(h, a) ← Q(h, a) + R−Q(h,a) N (h,a)
30:</p>
<p>return R 31: end procedure 32: procedure SELECTACTION(h) 33:
M i ← LASTPART(h) 34: π(a|s) ← LLM(M i , M c , p action probs ) a * ← arg max a∈A [Q(s, a)+ c puct π(a|s) N (s) N (s,a)+1 ] 35: if N (s, a) =o ← O(s) 52: a ∼ Uniform(A) 53: [r, s ′ , o ′ ] ← T (s, a) 54: return r + γ• ROLLOUT(s ′ , t + 1) 55: end procedure
We now describe the operation of MC-DML in a single planning episode, as outlined in Algorithm 1.For each simulation, MC-DML initializes the root node s 0 and new trajectory h 0 to construct the state (lines 2-3).An action a * is chosen based on the Q value, visit counts, and the LLM policy (lines 34-35).The LLM policy determines the action probability distribution using the in-trial memory M i and the cross-trial memory M c .The in-trial memory M i is a portion of the trajectory h (line 33), while the cross-trial memory M c consists of reflections generated from previous failed trajectories (lines 10-13, 44-47).MC-DML iteratively selects actions to execute and updates the visit counts and estimated Q values (line 26-29).When encountering leaf nodes, it expands the tree and performs a rollout, using a uniform policy to sample actions and returning the discounted reward.Upon completion of the search process, the agent will execute an final action based on the estimated Q value and receive a new observation.</p>
<p>NOVELTY IN COMPARISON TO PRIOR ALGORITHMS</p>
<p>We now more explicitly discuss comparisons to a few other approaches.The approach most related to ours is LLM-MCTS, which uses an LLM as a prior policy in MCTS (Zhao et al., 2024).While the LLM can serve as a good initial policy, it cannot improve the policy based on past experience and external feedback.This makes LLM-MCTS well-suited for commonsense planning tasks, such as object rearrangement in household environments, but less effective in uncertain environments like text-based games.</p>
<p>For text-based games, MC-LAVE-RL is one of the SOTA methods that combines MCTS with RL while considering the semantic sharing between actions (Lee et al., 2021).During MCTS planning, an exploration reward is added to each action a, estimated through the Q-values of its semantically similar actions.This approach addressed the bottleneck state in the game Zork1 (see Figure 1).Actions such as collecting items typically have high value in games, resulting in the action take lantern being assigned a higher exploration bonus than the action open trapdoor.However, its performance beyond games remains to be validated.In this study, rather than relying on item collection within LLM prompts, we focus on developing a more general solution.MC-DML simulates human gameplay by combining in-trial and cross-trial memory, mimicking how humans retain both recent detailed information and significant past experiences.We avoid introducing any prior game knowledge or human-designed hints in the LLM prompts.</p>
<p>EXPERIMENTS</p>
<p>EXPERIMENTAL SETUP</p>
<p>Environments We conduct experiments on 9 text-based games provided by the Jericho benchmark (Hausknecht et al., 2020).These games provide a variety of challenges such as darkness, nonstandard actions, inventory management, and dialog Among them, Zork1, Deephome, and Ludicorp are categorized as difficult games, as their optimal completion paths require over 300 steps, with an average of more than 14 available actions per step.The remaining games are categorized as possible games (Hausknecht et al., 2020).At each step t, the observation from the Jericho game engine includes a description of the state.This is augmented with location and inventory information by issuing the "look" and "inventory" commands, forming o t .Additionally, we utilize the valid action handicap provided by Jericho.For further analysis of these games, refer to Appendix A.</p>
<p>Implementation Details For the LLM policy, we use gpt-3.5-turbo-0125as the backend model with a sampling temperature set to 0. We query the LLM for the index of the optimal action and retrieve the log probabilities for the top 20 tokens at that index.For absent actions, we assign a log probability of -10.These log probabilities are then normalized using softmax with a temperature of 5.The in-trial memory is defined as (o t−1 , a t−1 , o t ), and the size of the cross-trial memory k is set to 3. We allow each root node to store up to k reflections.If the number of reflections exceeds k, cross-memory collection is terminated early due to the sufficient experiences.When necessary, we compress the input information using a truncation function to ensure that it fits within the LLM's input window constraints.</p>
<p>For the tree search component, we adopt a dynamic pruning strategy; the search depth is dynamically adjusted between a minimum depth d min and a maximum depth d max .The algorithm begins with d min .If the highest Q-value of the selected action node is 0, the depth is increased by ∆d and the search is repeated, up to d max .This setting takes into account the uneven distribution of steps with rewards in the game.We provide a statistical analysis of the game's step distribution in Appendix A and an ablation study of this setting in Section 4.3.Further details about the experimental implementation can be found in Appendix B.</p>
<p>Evaluation We first evaluate the performance of MC-DML in comparison with baseline methods on a series of text-based games.Next, we compare MC-DML with the intermediate scores of MCTSbased baselines during multiple iterations in the game Zork1.Then, we conduct ablation studies on a subset of games to evaluate the importance of different memory mechanisms in MC-DML.Finally, we provide further qualitative analysis, including how MC-DML addresses bottleneck states in the game Zork1.</p>
<p>Baseline We consider 10 baselines, including RL-based agents, LLM-based agents, and MCTSbased agents.All these baselines except MC!Q<em>BERT assume access to valid actions from the Jericho benchmark.Among these, PUCT-RL and MC-LAVE-RL algorithms serve as direct comparators to MC-DML.(1) DRRN (He et al., 2015) : The Deep Reinforcement Relevance Network (DRRN) is an RL-based agent that utilizes a Q-based softmax policy.This policy is parameterized with GRU encoders and decoders and is trained using the Temporal-Difference (TD) loss.( 2) KG-A2C-Hard (Ammanabrolu &amp; Hausknecht, 2020): An actor-critic method using a knowledge graph for state representation, with a hard valid action constraint.(3) MC!Q</em>BERT (Ammanabrolu et al., 2020): An extension of KG-A2C that leverages BERT for knowledge graph construction and includes knowledge-graph-based intrinsic rewards.(4) MPRC-DQN (Guo et al., 2020): A DQNbased method that formulates action selection as a multi-paragraph reading comprehension task, retrieving relevant historical observations and jointly encoding them with the current state for Q-value prediction.( 5) RC-DQN (Guo et al., 2020): A DQN-based agent that applies a reading comprehension model to compute Q-values from the current observation alone, without multi-paragraph retrieval.( 6) BiKE+CBR (Atzeni et al., 2021): A knowledge-graph-based A2C agent augmented with a case-based reasoning framework that explicitly stores and reuses successful experiences to improve out-of-distribution generalization.( 7) LLM agent: We employ the LLM directly as the agent to interact with the environment, aiming to assess potential data contamination within the LLM.In this setting, the LLM selects actions from the valid action set based on the current trajectory history complete the game.We set the temperature of the LLM to 0 and select the action with the highest output probability.(8) Reflection agent (Shinn et al., 2024): We allow the LLM to perform reflection, which is then used to guide its interactions with the environment in the next round.( 9) PUCT-RL (Jang et al., 2020): PUCT-RL uses PUCT as a policy improvement operator for DRRN, alternating between PUCT planning and supervised learning of self-generated actions.(10) MC-LAVE-RL (Jang et al., 2020): MC-LAVE is one of the SOTA models on the Jericho benchmark that combines MCTS with RL while considering the semantic sharing between actions.</p>
<p>MAIN RESULTS</p>
<p>Tables 1 and 2 present the performance of MC-DML compared with RL-based baselines and LLM/MCTS-based baselines, respectively, across a set of 9 games.We observe the following key findings.First, when compared to RL-based baselines, MC-DML achieves better or comparable results in 6 out of 9 games.In comparison to LLM/MCTS-based baselines, MC-DML outperforms or matches their performance in 8 out of 9 games.Secondly, in challenging games like Deephome, MC-DML overcomes multiple bottlenecks, achieving nearly double the performance of MC-LAVE-RL.In possible games like Pentari and Detective, MC-DML even completes the games fully.In other possible games, such as Library and Temple, it also approaches the best possible score within the given number of steps.Finally, the LLM policy performs poorly, likely due to its inability to balance exploration and exploitation.This also indicates that LLM does not have knowledge of the game's walkthrough under the current prompting setting.</p>
<p>RELATED WORK</p>
<p>Action Selection in MCTS MCTS-based algorithms thrives in large search spaces by selectively sampling promising actions (Osborne et al., 2022).The prevalent PUCT algorithm enhances this process by predicting action values using prior knowledge, typically obtained from historical data through imitation learning (Silver et al., 2017;2018).Current research on MCTS is directed towards developing contextual action value estimators to further refine action exploration (Lee et al., 2021;Sztyglic et al., 2021).Specifically, in language-ground settings, Branavan et al. ( 2012) utilizes a multi-layer neural network to extract relevant text segments from game documents.This text is then integrated into the Monte-Carlo search framework to facilitate linguistically-informed decisionmaking.Jang et al. ( 2020) introduced MC-LAVE-RL for solving text-based games, a method that incorporates value sharing among actions during the search process.Specifically, an action is encouraged for exploration if similar actions taken previously have high Q-values.While effective in certain games, this assumption may be less reliable in more dynamic settings.</p>
<p>Interactive Planning with LLM It is important to underscore our research focus.While recent studies have introduced search-based prompting approaches to enhance LLMs' reasoning capabilities by exploring generated thoughts (Yao et al., 2024;Ding et al., 2023), our study takes a distinct direction, emphasizing the large-scale planning under limited observability.In this domain, some studies utilize large LLMs as direct policies for interactive tasks, which yield interesting results but also exhibit certain limitations (Huang et al., 2022;Zhu et al., 2023;Fang et al., 2024).One such limitation is the difficulty in translating the plans created by LLMs into executable actions.Another is the inability of LLMs to balance exploration with exploitation.To address these issues, some research efforts use LLMs to formulate high-level plans and guide RL agents in performing specific actions (Shukla et al., 2023;Liu et al., 2024;Dalal et al., 2024;Zhang et al., 2024).However, these RL agents often struggle to conduct long-term planning.The study most closely aligned with ours is Zhao et al. (2024), which employs an LLM as a fixed prior policy within MCTS to address common sense tasks.Whereas these tasks are more intuitive and can be effectively addressed by leveraging the world prior knowledge of LLMs, text-based games present greater uncertainty, thus posing significant challenges.</p>
<p>Text-based Game Playing Agent Recent research has explored RL agents with varying architectures for solving text-based games (He et al., 2015;Narasimhan et al., 2015;Ammanabrolu &amp; Hausknecht, 2020;Xu et al., 2021;Ryu et al., 2022;Tuyls et al., 2022;Shi et al., 2023b;a).Innovations in this field address the problem of combinatorial action spaces (Zahavy et al., 2018;Yao et al., 2020), modeling state space utilising knowledge graphs (Ammanabrolu &amp; Hausknecht, 2020;Adhikari et al., 2020;Xu et al., 2020), integrating question-answering and reading comprehension modules (Ammanabrolu et al., 2020;Xu et al., 2022;Dambekodi et al., 2020).These agents rely on ϵ-greedy or softmax policies, which restrict their capacity long-term planning.To overcome this limitation, Jang et al. (2020) proposed MC-LAVE-RL, which integrated MCTS with RL to solve text-based games, while also considering semantic sharing of actions between nodes.Following this line, our study aims to extend the capabilities of these agents by combining MCTS with LLM, enhancing their strategic depth and adaptability in complex scenarios.</p>
<p>CONCLUSION</p>
<p>In this study, we propose the MC-DML algorithm.MC-DML leverages the prior knowledge embedded in LLM to guide action exploration during MCTS planning.The LLM is equipped with a dynamic memory mechanism to adjust action value estimation based on historical experience.MC-DML simulates human gameplay by mimicking how humans retain both recent detailed information and significant past experiences.Our results demonstrated that the MC-DML enhances performance across various games.</p>
<p>Limitation We utilise an LLM for value estimation during MCTS planning that combines in-trial memory and cross-memory.However, for simplicity in our current setup, we define in-trial memory as the trajectory within a shorter time window.In these games, some puzzles may relate to clues encountered much earlier, such as a spell or an item seen long ago.This places demands on the LLM's "Needle In a Haystack" ability.Future work could explore more efficient in-trial memory storage and retrieval mechanisms.</p>
<p>A GAME STATISTICS</p>
<p>We conduct experiments upon 9 games provided by the Jericho Game Suite (Hausknecht et al., 2020).Different from those generated through pre-defined simple rules (Côté et al., 2019), the games we use are more complex, making them even challenging for the human players.These games have diverse themes and genres.For example, in the game "Ludicorp", the player appears to be a modern citizen being located in an office building.In another game "Zork1", the player is put into a fantasy world that she/he has to find the treasure in the mazes while escaping from the troll.Some games contain nonstandard actions (e.g., the spells), which are unlikely to be understood by the language model pre-trained with commonsense knowledge.</p>
<p>B IMPLEMENTATION DETAILS</p>
<p>MC-DML.We set the discount factor to 0.95 and the number of simulations to 50 multiplied by len(A).We set C puct to 50.Specifically, it is set to 20 for the games Deephome and Library, and to 200 for the game Detective.The above configuration follows the work of Jang et al. (2020).We set d min to 10, d max to 30, and the step increment ∆d to 20.This configuration allows the algorithm to start with a conservative search depth and expand progressively when necessary.The LLM policy uses gpt-3.5-turbo-0125as the backend model with a sampling temperature set to 0. We query the LLM for the index of the optimal action and retrieve the log probabilities for the top 20 tokens at that index.For absent actions, we assign a log probability of -10.These log probabilities are then normalized using softmax with a temperature of 5.The in-trial memory is set to
(o t−1 , a t−1 , o t ).
The size of the cross-trial memory K is set to 3.</p>
<p>LLM agent.For the LLM agent baseline, we use gpt-3.5-turbo-0125with a sampling temperature of 0.1.The agent selects actions based on the highest probability from the model's output.However, we observe that the agent often enters loops due to ineffective exploration.To address this, if the agent repeats the same action in the same state five consecutive times, it switches to a random action.</p>
<p>Reflection agent.For the Reflection agent baseline, we use the same settings as the LLM agent baseline but equip it with a cross-memory module, allowing the agent to learn through reflection and trial-and-error.Specifically, the LLM plays multiple rounds of the game, generating reflections at the end of each round, which are then used as input for the next round.In our experiments, we limit the number of reflections to 3.</p>
<p>C LLM PROMPTS</p>
<p>In this section, we provide the prompts used for action value estimation by the LLM, as well as the prompts used for reflection.</p>
<p>C</p>
<p>C.2 PROMPTS FOR REFLECTION</p>
<p>You will receive a log of unsuccessful gameplay: {TRAJECTORY} Please briefly identify the reasons for failure and provide a suggestion for improvement in the next round.Do not summarize the gameplay trajectory; respond with your suggestion in a single sentence.</p>
<p>D TRAJECTORY EXAMPLE</p>
<p>[OBS] mining center This is the main center of all of the mining that takes place in Deephome.To the east are coal mines, rarely used because of the nature of the generator, and to the south are the ore mines, used for simultaneously mining all kinds of metal ores from the deeps of the mountains.To the north is the main street.</p>
<p>[LOOK] This is the main center of all of the mining that takes place in Deephome.To the east are coal mines, rarely used because of the nature of the generator, and to the south are the ore mines, used for simultaneously mining all kinds of metal ores from the deeps of the mountains.To the north is the main street.</p>
<p>[INV] You are carrying: -king's order -a warning note -a lantern (providing light)
[ACTION] east [REWARD] 1 [GAME SCORE] 15
[OBS] coal mines The coal mines are rather stark, for by the time the dwarves moved into Deephome, they had already developed the technology to use the world's core for energy, what they called geothermal power.The rock on the wall looks like it has been dug in some, however.The mine opens up to the mining center to the west.</p>
<p>[LOOK] coal mines The coal mines are rather stark, for by the time the dwarves moved into Deephome, they had already developed the technology to use the world's core for energy, what they called geothermal power.The rock on the wall looks like it has been dug in some, however.The mine opens up to the mining center to the west.</p>
<p>[INV] You are carrying: -king's order -a warning note -a lantern (providing light)</p>
<p>[ACTION] push coal to warning [REWARD] 10 [GAME SCORE] 25</p>
<p>[OBS] You open the generator furnace.</p>
<p>[LOOK] city generator Here is a large wonder of technology and ingenuity.The generator runs on the natural heat of the world, having pipes that lead all the way to near the core of the world.However, it requires a small fire to start the process of activating the power.The generator itself is large and black, having a small furnace in the very front.A doorway leads to the west, out into the street.</p>
<p>[INV] You are carrying:king's order -a warning note -a lantern (providing light) [ACTION] turn lever on [REWARD] 30 [GAME SCORE] 56</p>
<p>[OBS] extreme south main street [LOOK] extreme south main street Only people working for the city actually ever come down here, unless they are on a tour.A few mining operations lie to the south of this road, and to the east is the main generator for the city.To the southwest, there is a doorway leading to the city water works.The street ends here but continues to the north.</p>
<p>[INV] You are carrying: -king's order -a warning note -a lantern (providing light)
[ACTION] get in southwest [REWARD] 1 [GAME SCORE] 57
[OBS] water works This room is filled with scores and scores of pipes, leading this way and that, all connecting with the walls at one point or another.A large wheel sticks out of a rather significant pipe and there is a sign above it.To the northeast is the main road.</p>
<p>[LOOK] water works This room is filled with scores and scores of pipes, leading this way and that, all connecting with the walls at one point or another.A large wheel sticks out of a rather significant pipe and there is a sign above it.To the northeast is the main road.</p>
<p>Obs:</p>
<p>You ar e i n t he l i v i ng r oom. . .t her e i s a c l os ed t r ap door at y our f eet Obs : I t i s pi t c h bl ac k , y ou ar e l i k el y t o be eat en by a gr ue . . .You hav e di edYou ar e i n a dar k and damp c el l ar wi t h a nar r ow pas s ageway l eadi ng nor t h. . .</p>
<p>Figure 1 :?
1
Figure 1: An example bottleneck state from the game Zork1.</p>
<p>?</p>
<p>Figure 2: A comparison of the PUCT and MC-DML algorithms.PUCT trains its policy through imitation learning from self-play data.In contrast, MC-DML uses a LLM as the initial policy.During planning, the LLM learns from past failure trajectories and adjusts the action value estimates.This approach more closely aligns with the human thought process.</p>
<p>0 then
36:rollout ← true37:else38:rollout ← false39:end if40:43:if GAMEFAIL(s) then44:reflection ← LLM(h, p reflection )45:M c ← M c + reflection46:return 047:end if48:if t = planning horizon H then49:return 050:end if51:
return [a * , rollout] 41: end procedure 42: procedure ROLLOUT(h, s, t)</p>
<p>Table 2 :
2
Comparison of MC-DML with LLM-based and MCTS-based baselines on Jericho benchmark text-based games.</p>
<p>Table 3
3
shows the results of MC-DML alongside the intermediate scores of the MCTS-based baselines during multiple iterations on the game Zork1.We would like to emphasize that for the PUCT-RL and MC-LAVE-RL algorithms, the final result is computed based on the policy and Q-function obtained after 4 iterations, which is when convergence is reached.In each iteration, these algorithms conducted 25 independent planning sessions to collect trajectories and experience replay for policy learning.Unlike these approaches, MC-DML does not require a planning-then-learning paradigm.It can adjust the initial policy and estimated action values guided by dynamic memory.
PUCT-RLMC-LAVE-RLMC-DMLTree SearchRLTree SearchRLTree SearchIteration 131.9 ± 1.436.6 ± 1.030.4 ± 2.036.6 ± 1.0 48.66 ± 1.89Iteration 235.8 ± 0.037.4 ± 1.036.1 ± 0.138.2 ± 0.8N/AIteration 335.3 ± 0.239.0 ± 0.041.2 ± 0.543.0 ± 1.0N/AIteration 435.2 ± 0.438.2 ± 0.843.8 ± 0.145.2 ± 1.2N/A</p>
<p>Table 3 :
3
Jang et al. (2020)ts of MC-DML with the intermediate scores of the MCTS-based baseline during multiple iterations on the game Zork1.Our MC-DML achieves superior results with its initial planning.4.3 ABLATION STUDIESTo evaluate the importance of the memory mechanisms and dynamic pruning strategy in MC-DML, we conduct several ablation studies on a subset of games.We compare the performance of MC-DML without dynamic pruning (DP), without M c , without DP, and without M c and DP, M c , and M i .When disregarding the DP, we follow the experimental setup ofJang et al. (2020), using a fixed search depth for each game.Without M c , the LLM's action estimates are based on the current historical trajectory.Without both M c and M i , the LLM's action estimates at time t rely only on the current state node o t .The results show that using DP significantly improves performance in the game Zutt, but has little effect on other games.Removing M c reduces game scores, and removing both M c and M i results in an even larger drop in scores, highlighting the importance of these memory mechanisms.
MC-DMLw.o. Mcw.o. DPw.o. Mc, DP w.o. Mc, Mi, DPZork148.66 ± 1.8938.33 ± 2.8948 ± 2.4538 ± 5.231.67 ± 4.7Deephome67 ± 1.4162.66 ± 0.9467.4 ± 0.864.33 ± 0.9451 ± 14.9Detective346.67 ± 9.43 326.67 ± 4.71 334 ± 4.9323.33 ± 4.7320 ± 0.0Ztuu23.67 ± 1.920.66 ± 0.477.8 ± 0.567 ± 0.816.33 ± 0.94</p>
<p>Table 4 :
4
Ablation results on a subset of games.For the ablation models, we report the average score over 3 independent runs.Overall, both the M c and M i are crucial to our MC-DML.4.4 ANALYSISTable 5 presents an illustrative example of search results for MC-DML and MC-DML w.o.M c on a bottleneck state in the game Zork1.Without the reflection module M c , the LLM assigns a high value to the action open trap due to its semantic alignment with the current state, which also provides an immediate reward.Although this action eventually leads to failure, the agent, lacking the ability to reflect on its mistakes, continues to explore it, resulting in both a high Q-value and N (s, a).Consequently, the agent ends up selecting this action during the final execution step, which explains why it gets stuck in a bottleneck state.However, in MC-DML, the LLM generates a reflection based on failed trajectories and store it in the memory.The reflection might be, "Ensure you have a light source before entering dark areas," altering the action probability distribution in subsequent simulations.After sufficient exploration, the agent obtains an accurate value estimation and ultimately selects the take lantern action at the current state.Ultimately, the agent selects the optimal action take lantern, even though it does not provide any immediate reward.Similar bottleneck states are also addressed in the game Deephome.Additional trajectory examples of MC-DML playing Deephome are provided in the Appendix D.
MC-DMLopen trap open case take sword take lantern take alleastturn on lanternQ(s, a)4.4111.419.3114.260.00-8.12-1.42LLM (a|Mc, Mi, p)0.160.130.100.220.100.080.17N (s, a)213927252623w.o. Mcopen trap open case take sword take lantern take alleastturn on lanternQ(s, a)13.029.928.3812.663.17-1.854.73LLM (a|Mi, p)0.240.200.210.100.100.050.06N (s, a)17636723417510</p>
<p>Table 5 :
5
An illustrative example of search results for MC-DML and MC-DML w.o.M c on a bottleneck state in the game Zork1.Regarding the differing scales between LLM value and Q(s, a), during simulations, the LLM value is multiplied by a scale factor C P U CT .</p>
<p>Table A
A
shows the game statistics calculated from the walkthrough of each game.The Avg Actions per Step refers to the average number of valid actions available at each step of the game.The Walkthrough Length represents the minimum number of steps required to complete the game optimally, showing the shortest possible solution.The Avg Steps per Reward measures the average distance between two reward-triggering steps, reflecting how frequently rewards are distributed throughout the game.The Max Steps per Reward indicates the maximum number of steps a player might take between two rewards, highlighting the sparsest distribution of rewards.Finally, the Max Score represents the highest possible score an agent can achieve in the game.
Avg ActionsWalkthroughAvg StepMax StepMaxper StepLengthper Rewardper RewardGame ScoreZork115.963969.1251350Deephome19.473275.9053300Ludicorp14.523643.6945150Pentari5.16496.431670Detective7.16511.965360Library7.73523.67630Balances23.1812213.445450Temple15.2518221.384635Ztuu33.96844.5314100</p>
<p>Table 6 :
6
Game statistics on text-based games from Jericho benchmark.</p>
<p>.1 PROMPTS FOR ACTION VALUE ESTIMATES
You are a player in a text-based adventure game. Your task is toevaluate and select actions that are promising based on the givencontext.Your memory of playing this game previously is: {CORSS_TRIAL_MEMORY}You are now facing the following state:{IN_TRIAL_MEMORY}Considering the current state and previous memories, please select theaction most worth exploring from the following list:{VALID_ACTIONS}Respond by providing the index of the action only. Your response shouldbe a single integer, without any extra formatting, spaces,punctuation, or text.</p>
<p>OBS] east royal road This road leads east to an intersection and west toward the castle of the regional lord.It is lined with high walls and gates that open to lesser nobles' palaces.There is a gate open to the south.[LOOK]eastroyalroadThisroadleadseast to an intersection and west toward the castle of the regional lord.It is lined with high walls and gates that open to lesser nobles' palaces.OBS] west royal road This road is quite beautiful, decorated on its sides with fluorescent mosses that feed on the minerals in the stones that line the sides of the roads.Somehow, the mosses do not leave their designated stones.High walls on both sides make the street feel more like a hall than an open passageway, and gates leading to palaces break up the monotony of the stone.A single gate is open to the north.The road continues east and to the west is the outer court of the lord's palace.[LOOK]westroyalroadThisroad is quite beautiful, decorated on its sides with fluorescent mosses that feed on the minerals in the stones that line the sides of the roads.Somehow, the mosses do not leave their designated stones.High walls on both sides make the street feel more like a hall than an open passageway, and gates leading to palaces break up the monotony of the stone.A single gate is open to the north.The road continues east and to the west is the outer court of the lord's LOOK] outer court High walls surround this court, and a long pathway leads from the gates to the east to the throne room to the west.Most of the palace is locked securely, but the courtyard is still open, for the very reason of allowing a dwarven reclaimer to have access to the throne room.[INV]Youarecarrying:LOOK]westroyalroadThis road is quite beautiful, decorated on its sides with fluorescent mosses that feed on the minerals in the stones that line the sides of the roads.Somehow, the mosses do not leave their designated stones.High walls on both sides make the street feel more like a hall than an open passageway, and gates leading to palaces break up the monotony of the stone.A single gate is open to the north.The road continues east and to the west is the outer court of the lord's palace.[INV]OBS]greaternoble'spalaceThispalaceriseshigh, probably consisting of ten or twelve floors, all carved out of the mountain.The walls are covered with beautiful murals and tapestries, the floors are mosaics with fountains and baths interrupting them.Many gold and silver objects sit here gleaming in the light.It is clearly a wealthy abode.To the south is the gate that leads to the main street.[LOOK]greaternoble'spalaceThispalacerises high, probably consisting of ten or twelve floors, all carved out of the mountain.The walls are covered with beautiful murals and tapestries, the floors are mosaics with fountains and baths interrupting them.Many gold and silver objects sit here gleaming in the light.It is clearly a wealthy abode.To the south is the gate LOOK] west royal road This road is quite beautiful, decorated on its sides with fluorescent mosses that feed on the minerals in the stones that line the sides of the roads.Somehow, the mosses do not leave their designated stones.High walls on both sides make the street feel more like a hall than an open passageway, and gates leading to palaces break up the monotony of the stone.A single gate is open to the north.The road continues east and to the west is the outer court of the lord's palace.[INV]OBS]lessernoble'spalaceThis palace is quite beautiful, with many mosaics on the floors and several tapestries hanging on the walls.The columns that support the building are etched with pictures from dwarven lore and are exquisitely done.However, despite the attention to beauty, there is little of anything of extreme cost here, for dwarven craftsmanship is always ornate and admirable.The gates to the wall lead north to the royal road.[LOOK]lessernoble'spalace This palace is quite beautiful, with many mosaics on the floors and several tapestries hanging on the walls.The columns that support the building are etched with pictures from dwarven lore and are exquisitely done.However, despite the attention to beauty, there is little of anything of extreme cost here, for dwarven craftsmanship is always ornate and admirable.The library is quite large: floor after floor of books rises before you, clinging to the walls of this circular room.Stairs lead up the middle, spiraling along and pausing at landings at each floor.At the very center of the building you can look straight up and see the roof, and on it a mural.The exit is to the west.You can see Fresto's Exorcisms and Tasty Hors d'Oeuvres and Leshosh's Encyclopedia here.[LOOK]dwarvenlibraryThe library is quite large: floor after floor of books rises before you, clinging to the walls of this circular room.Stairs lead up the middle, spiraling along and pausing at landings at each floor.At the very center of the building you can look straight up and see the roof, and on it a mural.The exit is to the west.You can see Fresto's Exorcisms and Tasty Hors d'Oeuvres and Leshosh's Encyclopedia here.OBS] s. main street [LOOK] s. main street This section of the main street is dedicated to quieter pursuits.To the east is the library, and to the west is the great temple.Main Street runs north and south from here.[INV]Youare carrying: -king's order -a warning note -a lantern (providing light) [ACTION] west [REWARD] 1 [GAME SCORE] 65 [OBS] great temple of Kraxis The once great and shining temple of the one god, Kraxis, has been desecrated.Animal dung has been smeared on the beautiful wall murals, many of the tiles of the floor have been ripped up and thrown across the room, and the altar, a single large rock, unhewn by hands, has the statue of a pig-headed god on top of it.The large main doors lie to the east.You can see a dark spirit here.[LOOK]greattemple of Kraxis The once great and shining temple of the one god, Kraxis, has been desecrated.Animal dung has been smeared on the beautiful wall murals, many of the tiles of the floor have been ripped up and thrown across the room, and the altar, a single large rock, unhewn by hands, has the statue of a pig-headed god on top of it.The large main doors lie to the east.You can see a dark spirit here.[INV]Youare carrying:king's order -a warning note -a lantern (providing light) [ACTION] east [REWARD] 0 [GAME SCORE] 65 [OBS] s. main street [LOOK] s. main street This section of the main street is dedicated to quieter pursuits.To the east is the library, and to the west is the great temple.Main Street runs north and south from here.[INV]Youare carrying: -king's order -a warning note -a lantern (providing light) [ACTION] north [REWARD] 0 [GAME SCORE] 65 [OBS] n. main street [LOOK] n. main street This was once a bustling trading center with shops lining the sides of the streets, and smaller, less affluent merchants' booths sitting outside of the doorways, offering items of lower quality at a substantially lower rate.At this particular point in the street you see a clothier to the northeast, a bakery to the east, and a scrivener's to the west.To the north is the main hall, and the street runs further south.[INV]Youare carrying: -king's order -a warning note -a lantern (providing light) [ACTION] get in northeast [REWARD] 1 [GAME SCORE] 66 [OBS] clothier In this room you see the tools of a clothier's trade, generally dwarven-shaped wooden mannequins, a table for cutting, and other tools scattered about.The exit is to the southwest.[LOOK]clothierIn this room you see the tools of a clothier's trade, generally dwarven-shaped wooden mannequins, a table for cutting, and other tools scattered about.The exit is to the southwest.[INV]YouOBS] n. main street [LOOK] n. main street This was once a bustling trading center with shops lining the sides of the streets, and smaller, less affluent merchants' booths sitting outside of the doorways, offering items of lower quality at a substantially lower rate.At this particular point in the street you see a clothier to the northeast, a bakery to the east, and a scrivener's to the west.To the north is the main hall, and the street runs further south.[INV]Youare carrying: -king's order -a warning note -a lantern (providing light) [ACTION] west [REWARD] 1 [GAME SCORE] 67 [OBS] scrivener's This room is very plain and has half a dozen desks and chairs where the scriveners would sit and write documents for those who could not read.The exit is to the east.[LOOK]scrivener'sThis room is very plain and has half a dozen desks and chairs where the scriveners would sit and write documents for those who could not read.The exit is to the east.[INV]Youare carrying: -king's order -a warning note -a lantern (providing light) [ACTION] east [REWARD] 0 [GAME SCORE] 67 [OBS] n. main street [LOOK] n. main street This was once a bustling trading center with shops lining the sides of the streets, and smaller, less affluent merchants' booths sitting outside of the doorways, offering items of lower quality at a substantially lower rate.At this particular point in the street you see a clothier to the northeast, a bakery to the east, and a scrivener's to the west.To the north is the main hall, and the street runs further south.[INV]You are carrying: -king's order -a warning note -a lantern
[INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] get in northeast [REWARD] 0 [GAME SCORE] 57 [OBS] extreme south main street [LOOK] extreme south main street Only people working for the city actually ever come down here, unless they are on a tour. A few mining operations lie to the south of this road, and to the east is the main generator for the city. To the southwest, there is a doorway leading to the city water works. The street ends here, but continues to the north. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] north [REWARD] 0 [GAME SCORE] 57 [OBS] intersection [LOOK] intersection Here meet the two major roads of the city proper: Main Street, which runs north and south, and the Royal Road which runs west from here. To the east is a guard post with a sign next to it. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] west [REWARD] 1 [GAME SCORE] 58 [There is a gate open to the south. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] west [REWARD] 1 [GAME SCORE] 59 [palace. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] west [REWARD] 1 [GAME SCORE] 60 [OBS] outer court High walls surround this court, and a long pathway leads from the gates to the east to the throne room to the west. Most of the palace is locked securely, but the courtyard is still open, for the very reason of allowing a dwarven reclaimer to have access to the throne room. [LOOK] outer court High walls surround this court, and a long pathway leads from the gates to the east to the throne room to the west. Most of the palace is locked securely, but the courtyard is still open, for the very reason of allowing a dwarven reclaimer to have access to the throne room. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] west [REWARD] 1 [GAME SCORE] 61 [OBS] throne room The throne room is much more simple and quaint than you might expect. A large tapestry hangs behind the throne itself and a long red carpet leads from the great doorway to the east, and except for the throne itself, there is little here. You can see a luminous spirit here. [LOOK] throne room The throne room is much more simple and quaint than you might expect. A large tapestry hangs behind the throne itself and a long red carpet leads from the great doorway to the east, and except for the throne itself, there is little here. You can see a luminous spirit here. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] east [REWARD] 0 [GAME SCORE] 61 [ACTION] east [REWARD] 0 [GAME SCORE] 61 [OBS] outer court [-king's order -a warning note -a lantern (providing light) [OBS] west royal road [You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] north [REWARD] 1 [GAME SCORE] 62 [that leads to the main street. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] south [REWARD] 0 [GAME SCORE] 62 [OBS] west royal road [You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] east [REWARD] 0 [GAME SCORE] 62 [OBS] east royal road [LOOK] east royal road This road leads east to an intersection and west toward the castle of the regional lord. It is lined with high walls and gates that open to lesser nobles' palaces. There is a gate open to the south. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] south [REWARD] 1 [GAME SCORE] 63 lead north to the royal road. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] north [REWARD] 0 [GAME SCORE] 63 [OBS] east royal road [LOOK] east royal road This road leads east to an intersection and west toward the castle of the regional lord. It is lined with high walls and gates that open to lesser nobles' palaces. There is a gate open to the south. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] east [REWARD] 0 [GAME SCORE] 63 [OBS] intersection [LOOK] intersection Here meet the two major roads of [The gates to the wall the city proper: Main Street, which runs north and south, and the Royal Road which runs west from here. To the east is a guard post with a sign next to it. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] north [REWARD] 0 [GAME SCORE] 63 [OBS] s. main street [LOOK] s. main street This section of the main street is dedicated to quieter pursuits. To the east is the library, and to the west is the great temple. Main Street runs north and south from here. [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] east [REWARD] 1 [GAME SCORE] 64 [OBS] dwarven library [INV] You are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] west [REWARD] 0 [GAME SCORE] 64 [are carrying: -king's order -a warning note -a lantern (providing light) [ACTION] get in southwest [REWARD] 0 [GAME SCORE] 66 [ACTION] east [REWARD] 1 [(providing light) [GAME SCORE] 68
Our code is available at https://textgamer.github.io/mc-dml/.
Self-consistency methods estimate the probability of an answer by sampling multiple responses from the LLM. Verbalized methods leverage a well-designed prompt to instruct the LLM to output the most likely answers along with their corresponding probabilities.
ACKNOWLEDGMENTSThis project is partially supported by ARC DP240101349.We are grateful to the anonymous reviewers for their insightful feedback, which significantly enhanced our work.
Learning dynamic belief graphs to generalize on text-based games. Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, Will Hamilton, Advances in Neural Information Processing Systems. 202033</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, arXiv:2001.088372020arXiv preprint</p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, Mark O Riedl, arXiv:2006.074092020arXiv preprint</p>
<p>Casebased reasoning for better generalization in textual reinforcement learning. Mattia Atzeni, Shehzaad Dhuliawala, Keerthiram Murugesan, Mrinmaya Sachan, arXiv:2110.084702021arXiv preprint</p>
<p>Learning to win by reading manuals in a montecarlo framework. David Srk Branavan, Regina Silver, Barzilay, Journal of Artificial Intelligence Research. 432012</p>
<p>A survey of monte carlo tree search methods. Edward Cameron B Browne, Daniel Powley, Simon M Whitehouse, Peter I Lucas, Philipp Cowling, Stephen Rohlfshagen, Diego Tavener, Spyridon Perez, Simon Samothrakis, Colton, IEEE Transactions on Computational Intelligence and AI in games. 412012</p>
<p>A learning environment for text-based games. Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Textworld Et, Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018. Stockholm, SwedenSpringerJuly 13. 2018. 2019Revised Selected Papers 7</p>
<p>Efficient selectivity and backup operators in monte-carlo tree search. Rémi Coulom, International conference on computers and games. Springer2006</p>
<p>Plan-seqlearn: Language model guided rl for solving long horizon robotics tasks. Murtaza Dalal, Tarun Chiruvolu, Devendra Chaplot, Ruslan Salakhutdinov, arXiv:2405.015342024arXiv preprint</p>
<p>Playing text-based games with common sense. Sahith Dambekodi, Spencer Frazier, Prithviraj Ammanabrolu, Mark O Riedl, arXiv:2012.027572020arXiv preprint</p>
<p>Everything of thoughts: Defying the law of penrose triangle for thought generation. Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang, arXiv:2311.042542023arXiv preprint</p>
<p>Meng Fang, Shilong Deng, Yudi Zhang, Zijing Shi, Ling Chen, Mykola Pechenizkiy, Jun Wang, arXiv:2401.09334Large language models are neurosymbolic reasoners. 2024arXiv preprint</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu Chang, arXiv:2010.023862020arXiv preprint</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, arXiv:1511.046362015arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>Monte-carlo planning and learning with language action value estimates. Youngsoo Jang, Seokin Seo, Jongmin Lee, Kee-Eung Kim, International Conference on Learning Representations. 2020</p>
<p>Bandit based monte-carlo planning. Levente Kocsis, Csaba Szepesvári, European conference on machine learning. Springer2006</p>
<p>Monte-carlo tree search in continuous action spaces with value gradients. Jongmin Lee, Wonseok Jeon, Geon-Hyeong Kim, Kee-Eung Kim, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202134</p>
<p>Teaching models to express their uncertainty in words. Stephanie Lin, Jacob Hilton, Owain Evans, arXiv:2205.143342022arXiv preprint</p>
<p>Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia, arXiv:2402.19299Rl-gpt: Integrating reinforcement learning and code-as-policy. 2024arXiv preprint</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, arXiv:1506.089412015arXiv preprint</p>
<p>A survey of text games for reinforcement learning informed by natural language. Philip Osborne, Heido Nõmm, André Freitas, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Fire burns, sword cuts: Commonsense inductive bias for exploration in text-based games. Dongwon Ryu, Ehsan Shareghi, Meng Fang, Yunqiu Xu, Shirui Pan, Reza Haf, 10.18653/v1/2022.acl-short.56Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandMay 20222Association for Computational Linguistics</p>
<p>Stay moral and explore: Learn to behave morally in text-based games. Zijing Shi, Meng Fang, Yunqiu Xu, Ling Chen, Yali Du, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>Self-imitation learning for action generation in text-based games. Zijing Shi, Yunqiu Xu, Meng Fang, Ling Chen, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics2023b</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.037682020arXiv preprint</p>
<p>Lgts: Dynamic task sampling using llm-generated sub-goals for reinforcement learning agents. Yash Shukla, Wenchang Gao, Vasanth Sarathy, Alvaro Velasquez, Robert Wright, Jivko Sinapov, arXiv:2310.094542023arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, nature. 55076762017</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Science. 36264192018</p>
<p>Monte carlo tree search: A review of recent modifications and applications. Maciej Świechowski, Konrad Godlewski, Bartosz Sawicki, Jacek Mańdziuk, Artificial Intelligence Review. 5632023</p>
<p>Simplified belief-dependent reward mcts planning with guaranteed tree consistency. Ori Sztyglic, Andrey Zhitnikov, Vadim Indelman, arXiv:2105.142392021arXiv preprint</p>
<p>Multi-stage episodic control for strategic exploration in text games. Jens Tuyls, Shunyu Yao, Sham Kakade, Karthik Narasimhan, arXiv:2201.012512022arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang, Advances in Neural Information Processing Systems. 202033</p>
<p>Generalization in text-based games via hierarchical reinforcement learning. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Chengqi Zhang, 10.18653/v1/2021.findings-emnlp.116Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsNovember 2021</p>
<p>Perceiving the world: Question-guided reinforcement learning for text-based games. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Zhou, Chengqi Zhang, 10.18653/v1/2022.acl-long.41Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL). the 60th Annual Meeting of the Association for Computational Linguistics (ACL)2022</p>
<p>Keep CALM and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 10.18653/v1/2020.emnlp-main.704Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2020</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, Shie Mannor, Advances in neural information processing systems. 201831</p>
<p>How can llm guide rl? a value-based approach. Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, Zhaoran Wang, arXiv:2402.161812024arXiv preprint</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Zirui Zhao, Wee Sun Lee, David Hsu, Advances in Neural Information Processing Systems. 202436</p>
<p>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2305.171442023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
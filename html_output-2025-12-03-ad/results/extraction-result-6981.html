<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6981 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6981</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6981</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-268363717</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.07118v1.pdf" target="_blank">Narrating Causal Graphs with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The use of generative AI to create text descriptions from graphs has mostly focused on knowledge graphs, which connect concepts using facts. In this work we explore the capability of large pretrained language models to generate text from causal graphs, where salient concepts are represented as nodes and causality is represented via directed, typed edges. The causal reasoning encoded in these graphs can support applications as diverse as healthcare or marketing. Using two publicly available causal graph datasets, we empirically investigate the performance of four GPT-3 models under various settings. Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings. Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few examples as compared to fine-tuning via a large curated dataset.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6981.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6981.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal-Tagged Linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized Token Sequence with Explicit Causality Tags</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential, token-based encoding that represents each directed causal edge as a linearized relation using special tokens for source/target and explicit polarity tags (e.g., <S>, <H>, <POS>, <NEG>, <T>, <E>), sometimes separated by a pipe '|' delimiter between relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized representation with causal tags</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph is decomposed into small acyclic components and each relation is serialized into a token sequence using special markers: <S> (start), <H> (head/source node), <POS>/<NEG> (positive/negative causal polarity), <T> (tail/target node), and <E> (end). Edges/components are delimited (they additionally used '|' as a delimiter between relations). This representation preserves node labels verbatim and retains polarity via explicit tags.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossless (representation preserves graph information by design via component union)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Decompose graph into acyclic components (per prior algorithm from Shrestha et al., 2022) and serialize each component as an ordered sequence of relation tokens; use special tokens to mark nodes and polarity; optionally add '|' between relations.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Obesity causal map; Suicide causal map</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (causal map verbalization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 variants (Davinci, Curie, Babbage, Ada)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only pretrained language models from OpenAI: Davinci (≈175B parameters), Curie (≈6.7B), Babbage (≈1.3B), Ada (≈350M). Used via OpenAI API for fine-tuning, few-shot, and zero-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ROUGE-L, METEOR, BERTScore, QuestEval; human evaluation: faithfulness and coverage (binary/selection over 20 samples), inter-annotator agreement via Cohen's Kappa</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Providing explicit causal tags improved model outputs in fine-tuned and few-shot settings (higher automatic metrics and human preference), but not in zero-shot where omitting tags produced better results. Fine-tuned and few-shot (3-shot) performance was similar, indicating little need for very large curated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires pre-processing to decompose graphs into acyclic components and to add tags; representation can be verbose (repeated nodes across components); performance still degrades substantially in zero-shot; paragraph-level flow/cohesiveness not captured by sentence-level linearization; average token lengths and canonical ordering not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared against the no-tag variant and against WebNLG factual graphs: for causal datasets, tagged linearization performed better when fine-tuned or few-shot, but in zero-shot WebNLG (fact-based dataset) outperformed causal-tagged inputs. Authors note template-based edge-to-sentence would score perfectly on n-gram overlap metrics but be less natural for humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Narrating Causal Graphs with Large Language Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6981.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6981.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-Tag Linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized Token Sequence without Explicit Polarity Tags (Generic Connector)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of the linearized token sequence where explicit polarity markers (<POS>/<NEG>) are replaced by a generic causal connector token to test whether models infer polarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized representation without explicit polarity tags</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Same decomposition into acyclic components and serialization as the tagged variant, but POS/NEG tokens are replaced by a generic causal connector token (i.e., the polarity label is omitted) while preserving source/target markers and delimiters.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossless (by design of decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Acyclic component decomposition + edge serialization using <S>, <H>, <T>, <E> and a generic connector instead of explicit <POS>/<NEG>; '|' used between relations.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Obesity causal map; Suicide causal map</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (causal map verbalization; polarity inference evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 variants (Davinci, Curie, Babbage, Ada)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3 decoder-only models; experiments run at temperatures 0.6 and 0.8; fine-tuning for 5 epochs when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ROUGE-L, METEOR, BERTScore, QuestEval; human faithfulness and coverage; Cohen's Kappa for agreement</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>In zero-shot settings omitting explicit tags produced better performance (models inferred polarity more accurately without tags), while in fine-tuned and few-shot settings performance was generally inferior to the tagged representation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on model's implicit causal knowledge to infer polarity, which fails in zero-shot vs. fine-tuned nuances; still requires graph decomposition preprocessing; no canonical order reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Directly compared to the causal-tagged linearization: tags help in supervised/few-shot regimes but hinder zero-shot; compared against WebNLG results where factual encoding behaves differently in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Narrating Causal Graphs with Large Language Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6981.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6981.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Acyclic Component Linearization (Shrestha et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decomposition of Graph into Small Acyclic Components for Linearization (algorithm from Shrestha et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph pre-processing method that decomposes cyclic graphs into multiple small acyclic components (paths/subgraphs) so that each component can be linearized into a sentence-length sequence; nodes/edges may be duplicated across components to preserve information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatically explaining a model: Using deep neural networks to generate text from causal maps.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Acyclic component decomposition (small-component linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs (which may contain cycles) are partitioned into a set of acyclic components such that the union of components equals the original graph. Components are kept small (recommendation: <10 nodes) to match NLG sentence-length constraints; nodes/edges can appear in multiple components to avoid information loss.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical preprocessing leading to sequential tokenization (component-based, intended to be lossless at representation level)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph decomposition algorithm (from Shrestha et al., 2022) that generates small acyclic subgraphs (paths/components) which are then serialized; duplication of nodes/edges allowed to preserve full graph information across components.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Obesity causal map; Suicide causal map (used as input graphs for decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>preprocessing for graph‑to‑text generation (linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (preprocessing step used prior to GPT-3 models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not a model output metric, but authors reference prior work noting small component sizes (<10 nodes) yield best NLG results; downstream metrics used after NLG: ROUGE-L, METEOR, BERTScore, QuestEval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables sentence-length inputs for PLMs and preserves graph information for downstream NLG; recommended small components improve generation quality and fit within model context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires duplication of nodes/edges across components (verbosity and token-cost); choice of decomposition algorithm and component size affects how information is presented; canonical ordering not specified, which may introduce nondeterminism in linearized outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Authors used this decomposition to match earlier successful approaches for causal map NLG (Shrestha et al., 2022). Compared implicitly to approaches that encode whole graphs directly into graph-aware encoders; here decomposition enables use of decoder-only PLMs (GPT-3) without specialized graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Narrating Causal Graphs with Large Language Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6981.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6981.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template Edge Verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge-to-Sentence Template-Based Verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple deterministic mapping that converts each directed edge A -> B (with polarity) into a fixed sentence of the form 'A increases/decreases B'; yields perfect overlap scores but low human-acceptability due to repetitiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Template-based edge verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each edge in the graph, generate a templated sentence directly reflecting the edge (e.g., 'A increases B' for positive edges, 'A decreases B' for negative edges), producing outputs that exactly match the data but lack natural variation and readability.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, deterministic, lossy in stylistic variety (but information-preserving for individual edges)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Edge-list iteration with a fixed natural language template applied to each relation; no decomposition beyond iterating edges is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Discussed as a baseline for causal maps (not used as an experimental condition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (baseline deterministic verbalization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (rule-based template, not an LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not experimentally reported with numeric values in this paper, but authors note it would achieve near-perfect scores on reference-based n-gram/overlap metrics (e.g., ROUGE, METEOR) while being suboptimal for human readability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Not applicable to model training; serves as a deterministic baseline that would misleadingly score very high on automatic metrics but would not satisfy human-readability or variety requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Unnatural, repetitive output; cannot combine edges into more fluent sentences or express multi-edge reasoning/paths; would score poorly in human evaluations despite high automatic-metric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Contrasted in the paper with generative AI outputs: template-based yields perfect automatic-metric scores but is inferior in human preference and expressivity compared to GPT-3-generated diverse verbiage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Narrating Causal Graphs with Large Language Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatically explaining a model: Using deep neural networks to generate text from causal maps. <em>(Rating: 2)</em></li>
                <li>The webnlg challenge: Generating text from rdf data. <em>(Rating: 2)</em></li>
                <li>Gpt-too: A language-model-first approach for amr-to-text generation. <em>(Rating: 1)</em></li>
                <li>Investigating pretrained language models for graph-to-text generation. <em>(Rating: 2)</em></li>
                <li>Promoting graph awareness in linearized graph-to-text generation. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6981",
    "paper_id": "paper-268363717",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Causal-Tagged Linearization",
            "name_full": "Linearized Token Sequence with Explicit Causality Tags",
            "brief_description": "A sequential, token-based encoding that represents each directed causal edge as a linearized relation using special tokens for source/target and explicit polarity tags (e.g., &lt;S&gt;, &lt;H&gt;, &lt;POS&gt;, &lt;NEG&gt;, &lt;T&gt;, &lt;E&gt;), sometimes separated by a pipe '|' delimiter between relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Linearized representation with causal tags",
            "representation_description": "Each graph is decomposed into small acyclic components and each relation is serialized into a token sequence using special markers: &lt;S&gt; (start), &lt;H&gt; (head/source node), &lt;POS&gt;/&lt;NEG&gt; (positive/negative causal polarity), &lt;T&gt; (tail/target node), and &lt;E&gt; (end). Edges/components are delimited (they additionally used '|' as a delimiter between relations). This representation preserves node labels verbatim and retains polarity via explicit tags.",
            "representation_type": "sequential, token-based, lossless (representation preserves graph information by design via component union)",
            "encoding_method": "Decompose graph into acyclic components (per prior algorithm from Shrestha et al., 2022) and serialize each component as an ordered sequence of relation tokens; use special tokens to mark nodes and polarity; optionally add '|' between relations.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Obesity causal map; Suicide causal map",
            "task_name": "graph-to-text generation (causal map verbalization)",
            "model_name": "GPT-3 variants (Davinci, Curie, Babbage, Ada)",
            "model_description": "Decoder-only pretrained language models from OpenAI: Davinci (≈175B parameters), Curie (≈6.7B), Babbage (≈1.3B), Ada (≈350M). Used via OpenAI API for fine-tuning, few-shot, and zero-shot experiments.",
            "performance_metric": "ROUGE-L, METEOR, BERTScore, QuestEval; human evaluation: faithfulness and coverage (binary/selection over 20 samples), inter-annotator agreement via Cohen's Kappa",
            "performance_value": null,
            "impact_on_training": "Providing explicit causal tags improved model outputs in fine-tuned and few-shot settings (higher automatic metrics and human preference), but not in zero-shot where omitting tags produced better results. Fine-tuned and few-shot (3-shot) performance was similar, indicating little need for very large curated datasets.",
            "limitations": "Requires pre-processing to decompose graphs into acyclic components and to add tags; representation can be verbose (repeated nodes across components); performance still degrades substantially in zero-shot; paragraph-level flow/cohesiveness not captured by sentence-level linearization; average token lengths and canonical ordering not specified.",
            "comparison_with_other": "Compared against the no-tag variant and against WebNLG factual graphs: for causal datasets, tagged linearization performed better when fine-tuned or few-shot, but in zero-shot WebNLG (fact-based dataset) outperformed causal-tagged inputs. Authors note template-based edge-to-sentence would score perfectly on n-gram overlap metrics but be less natural for humans.",
            "uuid": "e6981.0",
            "source_info": {
                "paper_title": "Narrating Causal Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "No-Tag Linearization",
            "name_full": "Linearized Token Sequence without Explicit Polarity Tags (Generic Connector)",
            "brief_description": "A variant of the linearized token sequence where explicit polarity markers (&lt;POS&gt;/&lt;NEG&gt;) are replaced by a generic causal connector token to test whether models infer polarity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Linearized representation without explicit polarity tags",
            "representation_description": "Same decomposition into acyclic components and serialization as the tagged variant, but POS/NEG tokens are replaced by a generic causal connector token (i.e., the polarity label is omitted) while preserving source/target markers and delimiters.",
            "representation_type": "sequential, token-based, lossless (by design of decomposition)",
            "encoding_method": "Acyclic component decomposition + edge serialization using &lt;S&gt;, &lt;H&gt;, &lt;T&gt;, &lt;E&gt; and a generic connector instead of explicit &lt;POS&gt;/&lt;NEG&gt;; '|' used between relations.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Obesity causal map; Suicide causal map",
            "task_name": "graph-to-text generation (causal map verbalization; polarity inference evaluation)",
            "model_name": "GPT-3 variants (Davinci, Curie, Babbage, Ada)",
            "model_description": "OpenAI GPT-3 decoder-only models; experiments run at temperatures 0.6 and 0.8; fine-tuning for 5 epochs when applicable.",
            "performance_metric": "ROUGE-L, METEOR, BERTScore, QuestEval; human faithfulness and coverage; Cohen's Kappa for agreement",
            "performance_value": null,
            "impact_on_training": "In zero-shot settings omitting explicit tags produced better performance (models inferred polarity more accurately without tags), while in fine-tuned and few-shot settings performance was generally inferior to the tagged representation.",
            "limitations": "Relies on model's implicit causal knowledge to infer polarity, which fails in zero-shot vs. fine-tuned nuances; still requires graph decomposition preprocessing; no canonical order reported.",
            "comparison_with_other": "Directly compared to the causal-tagged linearization: tags help in supervised/few-shot regimes but hinder zero-shot; compared against WebNLG results where factual encoding behaves differently in zero-shot.",
            "uuid": "e6981.1",
            "source_info": {
                "paper_title": "Narrating Causal Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Acyclic Component Linearization (Shrestha et al.)",
            "name_full": "Decomposition of Graph into Small Acyclic Components for Linearization (algorithm from Shrestha et al., 2022)",
            "brief_description": "A graph pre-processing method that decomposes cyclic graphs into multiple small acyclic components (paths/subgraphs) so that each component can be linearized into a sentence-length sequence; nodes/edges may be duplicated across components to preserve information.",
            "citation_title": "Automatically explaining a model: Using deep neural networks to generate text from causal maps.",
            "mention_or_use": "use",
            "representation_name": "Acyclic component decomposition (small-component linearization)",
            "representation_description": "Graphs (which may contain cycles) are partitioned into a set of acyclic components such that the union of components equals the original graph. Components are kept small (recommendation: &lt;10 nodes) to match NLG sentence-length constraints; nodes/edges can appear in multiple components to avoid information loss.",
            "representation_type": "hierarchical preprocessing leading to sequential tokenization (component-based, intended to be lossless at representation level)",
            "encoding_method": "Graph decomposition algorithm (from Shrestha et al., 2022) that generates small acyclic subgraphs (paths/components) which are then serialized; duplication of nodes/edges allowed to preserve full graph information across components.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Obesity causal map; Suicide causal map (used as input graphs for decomposition)",
            "task_name": "preprocessing for graph‑to‑text generation (linearization)",
            "model_name": "N/A (preprocessing step used prior to GPT-3 models)",
            "model_description": null,
            "performance_metric": "Not a model output metric, but authors reference prior work noting small component sizes (&lt;10 nodes) yield best NLG results; downstream metrics used after NLG: ROUGE-L, METEOR, BERTScore, QuestEval.",
            "performance_value": null,
            "impact_on_training": "Enables sentence-length inputs for PLMs and preserves graph information for downstream NLG; recommended small components improve generation quality and fit within model context limits.",
            "limitations": "Requires duplication of nodes/edges across components (verbosity and token-cost); choice of decomposition algorithm and component size affects how information is presented; canonical ordering not specified, which may introduce nondeterminism in linearized outputs.",
            "comparison_with_other": "Authors used this decomposition to match earlier successful approaches for causal map NLG (Shrestha et al., 2022). Compared implicitly to approaches that encode whole graphs directly into graph-aware encoders; here decomposition enables use of decoder-only PLMs (GPT-3) without specialized graph encoders.",
            "uuid": "e6981.2",
            "source_info": {
                "paper_title": "Narrating Causal Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Template Edge Verbalization",
            "name_full": "Edge-to-Sentence Template-Based Verbalization",
            "brief_description": "A simple deterministic mapping that converts each directed edge A -&gt; B (with polarity) into a fixed sentence of the form 'A increases/decreases B'; yields perfect overlap scores but low human-acceptability due to repetitiveness.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "representation_name": "Template-based edge verbalization",
            "representation_description": "For each edge in the graph, generate a templated sentence directly reflecting the edge (e.g., 'A increases B' for positive edges, 'A decreases B' for negative edges), producing outputs that exactly match the data but lack natural variation and readability.",
            "representation_type": "sequential, token-based, deterministic, lossy in stylistic variety (but information-preserving for individual edges)",
            "encoding_method": "Edge-list iteration with a fixed natural language template applied to each relation; no decomposition beyond iterating edges is necessary.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Discussed as a baseline for causal maps (not used as an experimental condition)",
            "task_name": "graph-to-text generation (baseline deterministic verbalization)",
            "model_name": "N/A (rule-based template, not an LLM)",
            "model_description": null,
            "performance_metric": "Not experimentally reported with numeric values in this paper, but authors note it would achieve near-perfect scores on reference-based n-gram/overlap metrics (e.g., ROUGE, METEOR) while being suboptimal for human readability.",
            "performance_value": null,
            "impact_on_training": "Not applicable to model training; serves as a deterministic baseline that would misleadingly score very high on automatic metrics but would not satisfy human-readability or variety requirements.",
            "limitations": "Unnatural, repetitive output; cannot combine edges into more fluent sentences or express multi-edge reasoning/paths; would score poorly in human evaluations despite high automatic-metric scores.",
            "comparison_with_other": "Contrasted in the paper with generative AI outputs: template-based yields perfect automatic-metric scores but is inferior in human preference and expressivity compared to GPT-3-generated diverse verbiage.",
            "uuid": "e6981.3",
            "source_info": {
                "paper_title": "Narrating Causal Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatically explaining a model: Using deep neural networks to generate text from causal maps.",
            "rating": 2,
            "sanitized_title": "automatically_explaining_a_model_using_deep_neural_networks_to_generate_text_from_causal_maps"
        },
        {
            "paper_title": "The webnlg challenge: Generating text from rdf data.",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "Gpt-too: A language-model-first approach for amr-to-text generation.",
            "rating": 1,
            "sanitized_title": "gpttoo_a_languagemodelfirst_approach_for_amrtotext_generation"
        },
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation.",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "Promoting graph awareness in linearized graph-to-text generation.",
            "rating": 2,
            "sanitized_title": "promoting_graph_awareness_in_linearized_graphtotext_generation"
        }
    ],
    "cost": 0.011118749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Narrating Causal Graphs with Large Language Models
11 Mar 2024</p>
<p>Atharva Phatak phataka@lakeheadu.ca 
Vijay K Mago vmago@yorku.ca 
Philippe J Giabbanelli </p>
<p>Lakehead University
Thunder BayONCanada</p>
<p>Ameeta Agrawal
Aravind Inbasekaran Portland State University
PortlandORUSA</p>
<p>York University
TorontoONCanada</p>
<p>Miami University
OxfordOHUSA</p>
<p>Narrating Causal Graphs with Large Language Models
11 Mar 2024B0A890FF6E8D0A4DF99DAF5E37E84E4BarXiv:2403.07118v1[cs.CL]Causal MapGenerative AIGPTPre-Trained Large-Scale Language Model
The use of generative AI to create text descriptions from graphs has mostly focused on knowledge graphs, which connect concepts using facts.In this work we explore the capability of large pretrained language models to generate text from causal graphs, where salient concepts are represented as nodes and causality is represented via directed, typed edges.The causal reasoning encoded in these graphs can support applications as diverse as healthcare or marketing.Using two publicly available causal graph datasets, we empirically investigate the performance of four GPT-3 models under various settings.Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings.Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few examples as compared to fine-tuning via a large curated dataset.</p>
<p>Introduction</p>
<p>Large-scale pre-trained language models (LLMs) such as ChatGPT have recently been at the forefront of generative AI.By accomplishing a variety of tasks, these models save time for human users.They provide an accessible technology, as users do not require expertise in natural language processing (NLP).For example, GPT-based solutions can be integrated in information systems to create summaries (Ma et al., 2023), which is faster than asking humans to read a large textual input and does not require expertise in abstractive summarization algorithms.There is also a strong interest in using these models to perform causal reasoning, as it has potential to improve both customer experience and intention to use chatbots in areas such as healthcare information systems (Yu, 2021) or marketing (Bialkova, 2023).In a classic example, if a user asks "What will happen to my headache if I take an aspirin?" then the chatbot needs a causal model to suggest that the headache will be gone (Bishop, 2021).However, AI practitioners have noted that causality in LLMs is a nascent research field, so companies may currently struggle to effectively integrate LLMs by treating them over-confidently as human-level intelligence (C.Zhang et al., 2023).This has important implications, as exemplified by a recent case in which a user found the causal reasoning of a chatbot so convincing that he followed it to the letter, even when the chatbot encouraged him to commit suicide (Atillah, 2023).It is thus essential to assess and improve causal reasoning in LLMs (Kıcıman et al., 2023), such that we evaluate the limitations of their application and potentially add causal information to their training set.</p>
<p>Given that causality focuses on connecting antecedents to their consequences, a directed graph is a frequently used structure in causal research.Commonly employed types of graphs for generative AI include the following: ontologies (which possess attributes, classes, and events), where edges can be labeled as a subclass or a cause; knowledge graphs or semantic networks as used in WebNLG (T.Wang et al., 2023), where edges are labeled by words (see Figure 2); and causal maps (Shrestha et al., 2022), where edges are typed/labeled as positive or negative (see Figure 1).In this paper, we assess how much (if any) data is necessary to a LLM in generating sentences with the right type of causal effect.</p>
<p>That is, we use GPT-3 to transform causal maps into textual outputs that must have the appropriate causal increase or causal decrease.</p>
<p>Generating sentences from a graph is known as graph-to-text generation, which is a subtask of data-to-text generation.Recent studies in graph-to-text have shown that a causal graph could be transformed into fluent textual outputs (Shrestha et al., 2022), as captured by both automatic metrics and manual assessments (both of which are also employed in the present study).However, these works also relied on many additional pairs of examples (i.e., input graph and desired text) in order to train GPT-3 both on the application domain and on the causality encoded in the graphs.By examining whether this work-intensive fine-tuning can be reduced (few-shot learning) or eliminated altogether (zero-shot), our work contributes to lessening the burden on users and thus makes it possible to turn the wide array of available causal maps (B.Wang and Giabbanelli, 2023) into text.</p>
<p>The main contributions of our work are twofold:</p>
<p>• We evaluate the possibility of transforming causal graphs to text without having to specify causality.Our results are provided on two causal datasets, three different training settings (zero-shot, few-shot, and fine-tuned), and four GPT-3 models.</p>
<p>• We contrast results using both automatic performance metrics and human evaluations.</p>
<p>The remainder of this paper is organized as follows.In section 2, we succinctly review the context leading to the creation of causal maps and provide a brief background on LLMs.We present our methods and datasets in section 3. Our results are presented in section 4, including our performances and an assessment of the differences between our approach and prior works.These results are discussed in section 5 to address our central question: can a large language model such as GPT-3 act as a causal learner, or do we need to manually convey causation?</p>
<p>Background</p>
<p>Causal Maps</p>
<p>A causal map is a representation of a system as a graphical model (de Pinho, 2017), where salient concepts are captured as labeled nodes (e.g., 'financial insecurity', 'homelessness'), and causality is represented via directed, typed edges (e.g., financial insecurity + − → homelessness).While a knowledge graph  describes factual knowledge in the form of relations between entities, a causal map is a specific case in which relations can take two values and knowledge is subjective since it provides the perspectives of an individual.These maps are often produced in the context of participatory modeling (Quimby and Beresford, 2022), where participants (e.g., stakeholders, experts, community members) share their views as individual causal maps which are then aggregated to obtain a comprehensive view.Although the method of causal mapping is often chosen because it allows to elicit perspectives in a transparent manner with participants (Voinov et al., 2018), the product may no longer be transparent as participants struggle to interact with maps (Giabbanelli and Vesuvala, 2023).This has motivated prior works in converting maps into text, as a more universally accessible format (Shrestha et al., 2022).</p>
<p>Graph-to-text Generation</p>
<p>Early neural models to generate text descriptions from graphs were mostly fully supervised requiring large annotated datasets, and included architectures such as sequence-to-sequence, graph transformer (T.Wang et al., 2020), heterogeneous graph transformer (Yao et al., 2020), and graph encoder-decoder (Shi et al., 2020).Recent progress on generative pre-trained language models (PLMs) has achieved impressive results in graph-to-text generation.(Mager et al., 2020) were the first to employ a decoder-only PLM (GPT-2) to transform Abstract Meaning Representation graphs (directed trees that form whole sentences) into text (Radford et al., 2018).This was followed by (Ribeiro et al., 2020) who investigated the impact of different task-adaptive pretraining strategies for two encoder-decoder PLMs including the popular BART (Bidirectional and Auto-Regressive Transformer) and T5 models.In particular, they showed that approaches based on PLMs outperformed those explicitly encoding graph structure.</p>
<p>An emerging research area has been to control the generated text such that it expresses a set of user-desired attributes.(Hu and Li, 2021) focus on controllable text generation from a causal perspective with the primary objective of reducing bias in the text generated by various conditional models.(Z.Li et al., 2021) proposed causal generation models utilizing transformers.They constructed a corpus called CausalBank, which consists of 314 million cause-effect pairs.This corpus was used to train the model, enabling the generation of cause and effect relationships given initial words in a sentence.Note that there are now several studies that examine causality in generative AI and obtain seemingly contradictory results.This is partly explained by the different tasks used across studies (Kıcıman et al., 2023).Some studies strive to generate text that learns the whole causal graph (i.e., counterfactuals) while others may provide all pairs of related concepts and only expect the generator to correctly identify which concept is the antecedent and which one is the consequent.We thus emphasize the importance of being specific with respect to the causal task of interest (Section 3.1).</p>
<p>Recent work has also focused on data-to-text generation under few-shot setting (J.Li et al., 2021), zero-shot setting (Kasner and Dušek, 2022) and any-shot setting (Xiang et al., 2022).These different settings are also explored in the present manuscript.Among notable works, (Chen et al., 2020) proposed a knowledge-grounded pre-training framework and evaluated it under fully-supervised, zero-shot, and few-shot settings.(Hoyle et al., 2020) explored scaffolding objectives in PLMs (T5) and showed gains in low-resource settings.</p>
<p>Most of the previous methods have studied graph-to-text generation through widely-used datasets such as WebNLG (Gardent et al., 2017), hence we include this dataset in our study to allow for comparison.Complementary to prior work, we focus on causal datasets which contain facts that are often not explicitly stated in knowledge graphs.</p>
<p>Methods</p>
<p>Problem Description</p>
<p>A causal graph G = (V, E) consists of a set of entities (the nodes V ) and relations (the edges E).Each entity v ∈ V has a label, which can be composed of multiple words and is usually a noun (e.g., nutrition, consumption of fruits and vegetables).Each relation is directed since it encodes causality and it can be of only two types (positive, negative).Given a causal graph G, our goal is to generate a set of sentences S = {s 1 , s 2 , . . ., s n } that describes the graph in natural language text.For example, given the graph in Figure 1, sentences could be as follows:</p>
<p>Increasing nutrition education improves the consumption of fruits and vegetables, which prevents obesity and provides social support to consume such foods.As individuals eat more fruits and vegetables, there is also a lesser lack of knowledge about the associated benefits.Another consequence of a rise in nutrition education is that more hours will be spent on this topic.</p>
<p>The simplest solution would be to turn every edge A type − − → B into a sentence A increases/decreases B, which would achieve perfect scores in all automatic metrics since the output would be grammatically correct, contain the data present in each edge, and does not create noisy data (i.e., hallucinate).However, such a template-based approach would be unpleasant for humans to read.In contrast, Generative AI solutions are expected to express causality in diverse ways (e.g., improves, lessens, augments), combine edges into single sentences when there is a shared root node, or express a sequence of edges (i.e., a path) in one sentence to form a logical thread.The downside is a potential decrease in various metrics, particularly as approximations may ignore the type of causality or some edges entirely, or hallucinate due to the reliance on deep neural networks.</p>
<p>Data Pre-Processing</p>
<p>Since a text description is linear (read from left-to-right) but a graph can contain cycles, the graph is first decomposed into a series of acyclic components.The decomposition is not a simplification, as the union of all components should be equal to the input graph.Consequently, any loss of information in the text output would be attributable to the NLG step rather than to pre-processing.To preserve information while decomposing the graph into acyclic components, it may be necessary to include some nodes or edges in multiple components.For example, consider A → B → C → A. This could be split into A → B → C and C → A, hence C appears in both components.This redundancy is adequate for NLG tasks, since sentences on a given topic could also repeat some concepts or important causal statements.In our example from section 3.1, nutrition education was present in multiple sentences.Prior research has provided algorithms to obtain such a 'linearized representation' of a graph and showed that small graph sizes (less than 10 nodes) perform best for NLG tasks (Shrestha et al., 2022).We thus used the algorithm published in this prior work and followed their recommendation to create small components.We emphasize that the focus of our work is on generating sentences without having to specify the causality, and with lesser or no training data.</p>
<p>Experimental Approach</p>
<p>While many PLMs can be used for text generation, we use four variants of OpenAI's GPT-3 models1 (Brown et al., 2020): Davinci (175 billion parameters), Curie (6.7B), Babbage (1.3B), and Ada (350M).These models are used via a 2 × 3 experimental approach consisting of 2 versions of the input data (with/without expressed causality) and 3 learning settings (fine-tuning, few shot, zero shot), detailed below.The pseudocode for our methods is provided in the Appendix.</p>
<p>We considered two versions of the input.The first version is produced directly by the pre-processing step explained in section 2.2, which includes causal tags.In our modified second version, we exclude causal tags by replacing ⟨P OS⟩ and ⟨N EG⟩ with a generic causal connector.These two input versions allow us to test whether the generative AI is capable of inferring the type of causality.In each version, we added a pipe character (|) as a delimiter between each entity relation, such that the edges were clearly segmented in the input.</p>
<p>We also evaluated three training settings (see Figure 3), to examine how much data was necessary for a generative AI to infer causality.The most common setting is to create a fine-tuned model by training a PLM using a task-specific dataset, which we constructed for this experiment as detailed below.The effect of fine-tuning is that a model improves its performances by updating its weights.The OpenAI API recommends to fine-tune the models for a short amount of epochs.We observed that smaller GPT models (Ada, Babbage) had poor results for 1 or 2 epochs, hence we used 5 epochs for all models to guarantee that results reach a plateau.</p>
<p>The   The third setting is zero-shot, where the model is only given a natural description of the task along with a test query (see Figure 4).This is the most challenging setting, as it tests whether PLMs have encoded causal relationships between entities without needing any kind of domain-specific support from the user.</p>
<p>GPT models have one key parameter, known as temperature.Intuitively, it controls the 'creative randomness' of the model.When temperature is low, outputs will be less varied because the model will always output the words that have the highest probability.As temperature is increased, the model can select words that have a lower probability, thus leading both to more varied outputs but also to an increased risk of being offtopic.For each of the 3 × 2 configurations and four GPT models, we performed experiments on two different temperature levels (0.6 and 0.8).As described in the next section, each experiment took place on two different causal datasets, to measure the effect of the application domain onto the results.In summary, we have a total of 2 inputs × 3 learning settings × 2 temperatures × 2 datasets, that is, 24 experiments.</p>
<p>Datasets</p>
<p>We used two causal maps provided on open repositories: a map on youth suicide in the U.S. with 361 nodes and 946 edges (Giabbanelli et al., 2022)  and a smaller map on obesity with 98 nodes and 177 edges (Drasic and Giabbanelli, 2015).The maps are available at https://osf.io/7nxp4/and https://osf.io/7ztwu/, respectively.The youth suicide map was created by interviewing 15 experts, while the obesity map was developed by 19 experts.In both cases, experts representing a diverse array of fields were interviewed one-on-one, and their individual maps were merged to arrive at the final map.The merging process ensures that a concept appears only once in the entire map.We divided each map into small parts of 2 up to 4 nodes so that each part can be described in a sentence of tolerable length.To create a training dataset for each map, we employed a team of 8 human annotators who independently wrote descriptive sentences for each part.Sentences were then extracted and the list was reduced to promote variations in style.As a result, the generated dataset includes 588 graph-sentence pairs for the suicide map and 625 pairs for the obesity map.Table 1 presents sample instances from the datasets, while the dataset splits used in our experiments are shown in Table 2.</p>
<p>Evaluation Metrics</p>
<p>We use several automatic metrics as well as human readers to evaluate the quality of the generated text descriptions.</p>
<p>For the automatic evaluation, we relied on widely-used reference-based metrics where the model generated output is compared to the human-written reference text.We considered four evaluation metrics.ROUGE-L calculates the longest Table 3: Results for generated sentences when the input is formatted with and without tags, across different training settings (fine-tuned, few-shot, and zero-shot) for four GPT-3 models sorted from smallest (Ada, 350 million parameters) to largest (Davinci, 175 billion parameters).common subsequence overlap between the human text and model-generated text2 .METEOR (Metric for Evaluation of Translation with Explicit ORdering) is also an n-gram matching metric, but it accounts for semantics 3 .BERTScore (T.Zhang et al., 2019) focuses on semantic similarity between the reference and candidate texts4 .Finally, QuestEval (Scialom et al., 2021) focuses assessing factuality5 , which is an important property for all NLG and especially so for graph-to-text generation; it has a good correlation with human ratings (W.Li et al., 2022).All the evaluation metrics produce scores from 0 to 1, where 1 is the best match between the generated text and the human-written reference text.</p>
<p>We also conduct human evaluation of the outputs to assess two dimensions of quality.First, faithfulness measures how much of the input subgraph is reflected in the generated sentence; this score is negatively impacted when the model hallucinates.</p>
<p>Second, coverage measures how much of the input is preserved in the output; this score is lower when the model operates a simplification by ignoring parts of the input.We invited two annotators and asked them to choose the best sentence for faithfulness and coverage over 20 samples.This was repeated for the two datasets, the two harder training settings (fine-tuned, zero-shot), the two forms of input (with/without causality), and the best model (i.e., the GPT-3 model with best performance on automatic metrics).To reduce the risk that annotators may miss information when reading an input as linearized text, we also generated causal graphs for each input in the same format as Figure 1.We utilized Cohen's Kappa score to calculate the inter-annotator agreement score for faithfulness and coverage in both datasets and for each training setting.</p>
<p>Results</p>
<p>Table 3 shows the main results of our experiment to assess whether formatting the input linearized representation with and without tags improves the model's generated sentences, depending on the dataset, GPT-3 model size, and automatic metric.All results are presented at a temperature setting of 0.6 due to space limitation, while noting that similar results were Tags are most beneficial in fine-tuning and few-shot settings, but not having tags is best in a zero-shot learning situation.This observation is supported across the two datasets and across metrics, at the exception of measuring fine-tuning on obesity with METEOR (performances with and without tags are tied).As expected, results deteriorate as we move from fine-tuning (best results) to few-shot and then zero-shot (worst results).Interestingly, we observe only a minor deterioration when shifting from a full-training dataset to using just three instances, and a more pronounced decline when moving to a zero-shot setting.</p>
<p>We complemented this automatic analysis by performing a human evaluation for the results obtained by the best model, Davinci.</p>
<p>The inter-annotator agreement score (Table 4) shows moderate to very strong agreement under a zero-shot setting, but poor agreement between dimensions (faithfulness and coverage) in both datasets for fine-tuning.These varying levels of agreement are routinely observed in the literature (Ethayarajh and Jurafsky, 2022), as identifying robust mechanisms for manual evaluation remains an active area of research.Within these limitations, results (Table 5) show that human evaluators prefer the text generated by models using tags in all but one case (zero-shot learning for obesity).This manual inspection confirms the takeaway of the automatic metrics: guiding GPT-3 with causal tags leads to higher performance in general, but not necessarily under a zero-shot setting.</p>
<p>To further examine these results, we compared the performances of our best model (Davinci at temperature 0.6) in our two causal datasets of suicide and obesity against performances in the classical WeBNLG dataset for graph-to-text task, which encodes facts between entities rather than the type of causality.The results (Table 6) indicate that causal datasets lead to better performance than WebNLG when fine-tuning or in a few-shot learning setting but, interestingly, WebNLG outperforms in a zero-shot setting.This suggests that while causal relationships may be relatively easier for models to learn with limited training data, they do not appear to be encoded inherently in the PLM.</p>
<p>Discussion and Conclusions</p>
<p>This paper evaluates text generation specifically for causal graph representations.Using several versions of GPT-3 models and two causal datasets (obesity and suicide), we assess the models' ability to generate natural language descriptions with and without causal tags.The generated outputs are evaluated through both automatic and manual evaluation.Our results indicate that the quality of text generated from a causal map is about the same when using a full training set compared to just three examples.This is an important finding, as creating extensive training sets is particularly labor-intensive, and users would thus be able to save significant amounts of time in exchange for a small loss in performance.Zero-shot learning is a very different setting, which shows a sharp deterioration in performance and an interesting reversal since models learned best without using causal tags.Comparing results between causal datasets and the WebNLG dataset suggest that the generative AI tool GPT-3 is able to satisfactorily learn causality with limited training data, but it does not inherently encode causality.</p>
<p>There are three main limitations to this study.First, creating a full training set for a given causal map is a labor intensive process, hence only two datasets were available for the evaluation.As other research groups gradually examine the use of LLMs for causal maps and share their datasets, additional evaluations will become possible and will contribute to assessing generalizability.Second, although our results were in agreement between the two datasets for automatic metrics, we noted one discrepancy when involving human annotators -a process that is itself subject to considerable variability.Third, our results focused on evaluating the quality of sentences, but reports consist of paragraphs.Extending sentence-level scores over paragraphs could be realized by using the average score of individual sentences in a paragraph, but that would not evaluate the flow.Paragraph-level metrics Table 6: Comparing WebNLG against two causal datasets (Obesity, and Suicide) formatted with/without tags, across different training settings (full-shot, few-shot, and zero-shot) for GPT-3 (Davinci, with temperature 0.6).such as Flesch-Kincaid scores have been employed for NLG, but additional metrics are needed to capture cohesiveness and factuality at the paragraph level.There is also a need for causality-specific metrics that go beyond overlap, semantic similarity, or n-gram matching.Existing metrics can score highly for texts expressing opposing causal relationships, hence we need a more precise assessment of the causal direction, type, and relationships between entities.</p>
<p>Our study focused on causal reasoning for general facts, such as the notion that an increase in traumatic events does raise the average risk of suicidal ideation across individuals.Our study of causal reasoning could thus be extended to gain a better understanding of the specific context of a user.For instance, emotional causality allows to relate the feelings expressed by a user to their underlying causes, which results in a more empathetic interaction.This also involves a knowledge graph, automatic evaluations (e.g., BLEU) and manual evaluations (e.g., fluency).The main differences would be about the content of the graph and the incorporation of an addition manual evaluation on the empathy expressed in the generated content (J.Wang et al., 2021).</p>
<p>Finally, we examined whether LLMs could determine the causal type of a specified relation.Mathematically, we provide the structure (including the direction of each edge) and check for the polarity of the edges' labels.The ability of a LLM to perform this task is promising to support text generation focused on intervention (e.g., does taking an aspirin increase or decrease my headache?).However, this is a simpler task than determining the direction of each relation (e.g., A → B vs. B → A), the level of causality (e.g., necessary, sufficient), or even discovering the full graph (Kıcıman et al., 2023).Additional research on such advanced tasks is necessary to support retrospective reasoning (e.g., why is my headache gone?), which is at the forefront of debates on the capacity of generative AI (Bishop, 2021).The function getResponse (Algorithm 2) accepts multiple parameters and queries OpenAI API to get the response for each prompt in the test set.We initialize an empty dictionary (results) which stores the outputs generated by the model.Lines 5-16 show the process of iterating over test samples and querying the OpenAI API with the required parameters.Finally, after the results are generated for all the samples in the test set, the function returns all results in a dictionary.</p>
<p>Figure 1 :
1
Figure 1: Sample graphs from Obesity dataset.The linearized representations of instances would be: <S> <H> nutrition <POS> <T> consumption of fruits and vegetables<H> nutrition <POS> <T> nutrition education hours<H> consumption of fruits and vegetables <NEG> <T> obesity<H> consumption of fruits and vegetables <POS> <T> social support for eating fruits and vegetables<H> consumption of fruits and vegetables <NEG> <T> lack of knowledge of benefits to eating fruits and vegetables <E>.</p>
<p>Figure 2 :
2
Figure 2: Sample graph from WebNLG dataset.</p>
<p>Figure 3 :
3
Figure 3: We had three training settings (fine-tuned, few shot, zero shot) and four variants of GPT-3 models.</p>
<p>second setting reduces the training set via a few-shot approach.Instead of a large number of training samples, only k samples are chosen, where k depends on the model's context size.That is, GPT has a maximum input length limit (i.e., context length of 2048 tokens), so the set of all examples must fit within this limit; the more context is required when providing an example, the fewer examples can fit.Previous works have used very different amounts of examples depending on their length.For instance, one study used 5 examples (Agrawal et al., 2022), another varied from 1 to 16 examples (Yang et al., 2022), and a third tested up to 100 examples before running out of tokens (Moradiet al., 2021).In our case, we use 3 examples randomly selected from the wider pool used in fine-tuning, hence we employ a strict few-shot setting.</p>
<p>Figure 4 :
4
Figure 4: Example of zero-shot prompt.</p>
<p>select from the training data (n), i.e., the number of n-shot samples.The function M akeN ShotSamples generates the n-shot prompt in the format required by the OpenAI API.Lines 2 and 3 show how to read the dataframe from the given path and randomly sample n instances from the dataframe.OpenAI GPT3 models require a statement that indicates what task needs to be performed and is included in line 4. Finally, lines 7-16 populate the prompt variable (line 5) by iterating over the randomly sampled n instances and concatenating them in the format required by the API.</p>
<p>Table 1 :
1
Sample instances from the causal graph datasets
Dataset InputText DescriptionSuicide⟨S⟩⟨H⟩ ACEs of parents ⟨POS⟩ ⟨T⟩ Parental risk factors⟨E⟩More ACEs of parents increases parental risk factors.⟨S⟩⟨H⟩PeersyoucantalkHaving more peers you can talk to can createto⟨POS⟩⟨T⟩Protective environment⟨E⟩protective environment.Obesity⟨S⟩⟨H⟩ Obesity awareness programs⟨POS⟩ ⟨T⟩nutrition education⟨H⟩ObesityObesity awareness programs can develop knowledge about nutrition and also communityawarenessprograms⟨POS⟩communitypartnerships.partnerships⟨E⟩⟨S⟩⟨H⟩ routine practices in hospital ⟨NEG⟩Improving routine practices in hospitals⟨T⟩breastfeeding knowledge⟨E⟩decreases breastfeeding knowledge.</p>
<p>Table 2 :
2
Statistics of the datasets.
Dataset Train Validation TestSuicide32883177Obesity34988188</p>
<p>Table 5 :
5
Results (percentage) of human annotation of comparing Tags vs. NoTags models obtained at a higher temperature of 0.8.Davinci is the best model in all cases.Although results show that providing tags is generally beneficial, these benefits are limited and depend on additional considerations.
FaithfulnessCoverageFTZSFTZSSuicide -0.19 0.33 -0.191Obesity-0.50.5700.39Table 4: Inter-annotator (Cohen's Kappa) agreementscore for faithfulness and coverageFaithfulnessCoverageTagsNoTagsTagsNoTagsObesity FT 69.2330.7765.3834.62Obesity ZS 69.2330.7746.1553.85Suicide FT 57.6942.3261.5438.46Suicide ZS 69.2330.7773.0826.92</p>
<p>Input: testP ath: path to testing data, temperature: setting used to generate the outputs, model: name of openAI model to use, maxT okens: number of new tokens to generate, trainP ath: path to the training dataset, n: number of input instances to select from the dataset Output: results: A dictionary consisting of outputs generated by specified OpenAI GPT3 model. 1 Function getResponse(trainP ath, n, testP ath, temperature, model, maxT okens) /<em> Get few shot input prompt </em>/
Algorithm 2: Generate response of openAIGPT3 models2prompt ←GenerateN ShotSamples(trainP ath, n)/<em> Read dataframe from the path</em>/3testF rame ← pd.read csv(testP ath)/<em> Initialize empty dictionary to storethe results.</em>/4results ← ϕ/<em> Iterate over the test prompts.</em>/completion ←completion.replace("⟨end⟩", "") prompt= prompt + ( "prompt: "10+ sentence + "\n" + "completion: "11+ completion + separator )12end13return statement + prompt
5for testPrompt in testF rame["prompt"]do 6 inputP rompt = prompt + "prompt: " 7 + testP rompt + "\n" +"completion: " 8 + "\n \n" /<em> Generate the output using OpenAI API.</em>/ 9 response ← openai.Completion.create(model=</p>
<p>https://openai.com/api/
https://github.com/google-research/google-research/tree/master/ rouge
https://github.com/wbwseeker/meteor
https://github.com/Tiiiger/bert score
https://github.com/ThomasScialom/QuestEval</p>
<p>Large language models are few-shot clinical information extractors. M Agrawal, Proc. 2022 Conf. on Empirical Methods in Natural Language Processing. 2022 Conf. on Empirical Methods in Natural Language essing2022. 1998-2022</p>
<p>Man ends his life after an ai chatbot 'encouraged' him to sacrifice himself to stop climate change. I E Atillah, Euronews. 2023next</p>
<p>I want to talk to you: Chatbot marketing integration. S Bialkova, Advances in advertising research. 202312</p>
<p>. Springer, </p>
<p>Artificial intelligence is stupid and causal reasoning will not fix it. J M Bishop, Frontiers in Psychology. 1126032021</p>
<p>Language models are few-shot learners. T Brown, Adv. Neural Inf. Process. 332020</p>
<p>Mapping complex systems of population health. Systems Science and Population Health. W Chen, Y Su, X Yan, W Y Wang, arXiv:2010.023072020. 2017de Pinho, HarXiv preprintKgpt: Knowledge-grounded pre-training for data-to-text generation</p>
<p>Exploring the interactions between physical well-being, and obesity. Can. L Drasic, P J Giabbanelli, J. Diabetes. 392015</p>
<p>How human is human evaluation? improving the gold standard for nlg with utility theory. K Ethayarajh, D Jurafsky, arXiv:2205.119302022arXiv preprint</p>
<p>The webnlg challenge: Generating text from rdf data. C Gardent, A Shimorina, S Narayan, L Perez-Beltrachini, Proc. 10th Int. Conf. on Natural Language Generation. 10th Int. Conf. on Natural Language Generation2017</p>
<p>Pathways to suicide or collections of vicious cycles? understanding the complexity of suicide through causal mapping. Social network analysis and mining. P J Giabbanelli, K L Rice, M C Galgoczy, 202212</p>
<p>Human factors in leveraging systems science to shape public policy for obesity: A usability study. P J Giabbanelli, C X Vesuvala, Information. 1431962023</p>
<p>Promoting graph awareness in linearized graph-to-text generation. A Hoyle, A Marasović, N Smith, arXiv:2012.157932020arXiv preprint</p>
<p>A causal lens for controllable text generation. Z Hu, L E Li, Adv. Neural Inf. Process. 342021</p>
<p>Neural pipeline for zero-shot data-to-text generation. Z Kasner, O Dušek, arXiv:2203.162792022arXiv preprint</p>
<p>E Kıcıman, R Ness, A Sharma, C Tan, arXiv:2305.00050Causal reasoning and large language models: Opening a new frontier for causality. 2023arXiv preprint</p>
<p>Few-shot knowledge graph-to-text generation with pretrained language models. J Li, arXiv:2106.016232021arXiv preprint</p>
<p>Faithfulness in natural language generation: A systematic survey of analysis, evaluation and optimization methods. W Li, arXiv:2203.052272022arXiv preprint</p>
<p>Z Li, X Ding, T Liu, J E Hu, B Van Durme, arXiv:2107.09846Guided generation of cause and effect. 2021arXiv preprint</p>
<p>C Ma, Z Wu, J Wang, arXiv:2304.08448Impressiongpt: An iterative optimizing framework for radiology report summarization with chatgpt. 2023arXiv preprint</p>
<p>Gpt-too: A language-model-first approach for amr-to-text generation. M Mager, arXiv:2005.09123arXiv:2109.025552020. 2021arXiv preprintGpt-3 models are poor few-shot learners in the biomedical domain</p>
<p>Participatory modeling: A methodology for engaging stakeholder knowledge and participation in social science research. B Quimby, M Beresford, 2022</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Investigating pretrained language models for graph-to-text generation. L F Ribeiro, M Schmitt, H Schütze, I Gurevych, arXiv:2007.084262020arXiv preprint</p>
<p>Questeval: Summarization asks for fact-based evaluation. T Scialom, arXiv:2103.126932021arXiv preprint</p>
<p>G2t: Generating fluent descriptions for knowledge graph. Y Shi, Proc. 43rd Int. ACM SIGIR Conf. on Research and Development in Information Retrieval. 43rd Int. ACM SIGIR Conf. on Research and Development in Information Retrieval2020</p>
<p>Automatically explaining a model: Using deep neural networks to generate text from causal maps. A Shrestha, K Mielke, T A Nguyen, P J Giabbanelli, Winter Simulation Conf. (WSC). 2022. 2022</p>
<p>Tools and methods in participatory modeling: Selecting the right tool for the job. A Voinov, Environmental Modelling &amp; Software. 1092018</p>
<p>Identifying informative features to evaluate student knowledge as causal maps. B Wang, P J Giabbanelli, Int. J. Artif. Intell. Educ. 2023</p>
<p>Empathetic response generation through graph-based multi-hop reasoning on emotional causality. Knowledge-Based Systems. J Wang, W Li, P Lin, F Mu, 2021107547</p>
<p>Improving plms for graph-to-text generation by relational orientation attention. T Wang, B Shen, J Zhang, Y Zhong, Neural Processing Letters. 2023</p>
<p>Amr-to-text generation with graph transformer. T Wang, X Wan, H Jin, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Asdot: Any-shot data-to-text generation with pretrained language models. J Xiang, Z Liu, Y Zhou, E P Xing, Z Hu, arXiv:2210.043252022arXiv preprint</p>
<p>An empirical study of gpt-3 for few-shot knowledge-based vqa. Z Yang, Proc. AAAI Conf. on Artificial Intelligence. AAAI Conf. on Artificial Intelligence202236</p>
<p>Heterogeneous graph transformer for graph-to-sequence learning. S Yao, T Wang, X Wan, Proc. 58th Annual Meeting of the Association for Computational Linguistics. 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Dynamic causality knowledge graph generation for supporting the chatbot healthcare system. H Q Yu, Proc. Future Technologies Conf. (FTC) 2020. Future Technologies Conf. (FTC) 202020213</p>
<p>Causality in the time of llms: Round table discussion results of clear 2023. C Zhang, Proc. Machine Learning Research. 172023</p>
<p>T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
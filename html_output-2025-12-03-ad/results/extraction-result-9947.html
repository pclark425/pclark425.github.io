<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9947 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9947</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9947</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-3bb853fe6bdbe889a8715c93f78dae01cf1bc65c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3bb853fe6bdbe889a8715c93f78dae01cf1bc65c" target="_blank">Evaluating Mathematical Reasoning Beyond Accuracy</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper Abstract:</strong> The leaderboard of Large Language Models (LLMs) in mathematical tasks has been continuously updated. However, the majority of evaluations focus solely on the final results, neglecting the quality of the intermediate steps. This oversight can mask underlying problems, such as logical errors or unnecessary steps in the reasoning process. To measure reasoning beyond final-answer accuracy, we introduce ReasonEval, a new methodology for evaluating the quality of reasoning steps. ReasonEval employs validity and redundancy to characterize the reasoning quality, as well as accompanying LLMs to assess them automatically. We explore different design options for the LLM-based evaluators and empirically demonstrate that ReasonEval, when instantiated with base models possessing strong mathematical knowledge and trained with high-quality labeled data, consistently outperforms baseline methods in the meta-evaluation datasets. We also highlight the strong generalization capabilities of ReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we find that an increase in final-answer accuracy does not necessarily guarantee an improvement in the overall quality of the reasoning steps for challenging mathematical problems. Additionally, we observe that ReasonEval can play a significant role in data selection. We open-source the best-performing model, meta-evaluation script, and all evaluation results to facilitate future research.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9947.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9947.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting-LLM-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting-based Large Language Model as Judge (e.g., GPT-4, GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using powerful generative LLMs prompted to judge step-by-step mathematical solutions (produce final judgement, locate first error, or identify neutral/redundant steps); used as an automated alternative to human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Mathematical reasoning / multi-step math problem solution evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, GPT-3.5-turbo (prompting-based)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompting with a step-by-step evaluation template (asks for stepwise evaluation, final judgment 'correct'/'wrong', and first-error step or list of neutral steps); prompts adapted from Zeng et al. (2024) and listed in Appendix (Figure 6). Often run directly (no fine-tuning) and requires a powerful model (GPT-4 recommended).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labels used as ground truth in meta-evaluation datasets (e.g., MR-MATH labels collected by undergraduates with solid math background; PRM800K human-annotated step labels used for training/evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Compared against human annotations using macro F1 and AUC. Reported prompting results (Table 1): GPT-3.5-turbo and GPT-4 solution/step-level performance (e.g., GPT-4 solution-level F1 ~73.2 on MR-MATH-invalid; step-level F1 ~61.0; redundancy metrics lower: solution-level ~57.1, step-level ~54.2).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Prompting LLM judges are limited in step-level error localization and in judging redundancy: they struggle to reliably detect fine-grained step errors and ambiguous/neutral (redundant) steps; they incur cost and transparency issues; their judgments depend heavily on model capability and the prompt, and they may miss subtle logical/calculation mistakes humans detect.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>MR-MATH is constructed so many solutions have correct final answers but include incorrect intermediate steps; GPT-4 and GPT-3.5-turbo perform worse at step-level identification and redundancy detection than specialized REASONEVAL evaluators (Table 1). The paper cites prior finding that even SOTA (GPT-4) struggles to detect reasoning errors (footnote referencing Tyen et al. 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Prompting-based GPT-4 attains strong solution-level performance (approaching human-level F1 on some solution-level tasks) and can be effective for coarse correctness checks; however, high cost, lack of transparency, and poorer step-level/localization performance remain important caveats. The paper treats GPT-4 as a strong baseline but shows fine-tuned step-level evaluators can surpass it at locating errors.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>See §2.2 (Prompting-based Method), §4.1 (Evaluators & meta-evaluation setup), §4.2 (Results and Analysis), Table 1, Appendix D (prompts), and footnote referencing Tyen et al. (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Mathematical Reasoning Beyond Accuracy', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9947.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9947.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REASONEVAL (LLM evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REASONEVAL: Fine-tuned LLM-based Step-level Evaluator for Validity and Redundancy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised LLM evaluator that classifies each reasoning step as positive/neutral/negative (valid + useful / valid + redundant / invalid) and aggregates step scores to solution-level validity and redundancy; instantiated by fine-tuning various base LLMs on human-labeled step data (PRM800K).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Mathematical reasoning / step-by-step solution quality evaluation (validity and redundancy)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Multiple fine-tuned backbones: Llama-2-7B, Mistral-7B, Llemma-7B/34B, WizardMath-7B variants, Abel-7B-002 (REASONEVAL is the fine-tuned evaluator derived from these bases)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Supervised fine-tuning on PRM800K (≈800k step-level human labels over 75k solutions) replacing LM head with a classification head producing p_positive/p_neutral/p_negative per step; aggregates step scores using min (validity) and max (redundancy); thresholds used for solution-level judgments (validity 0.5, redundancy 0.15 as in Appendix D).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Training labels derived from PRM800K (human-annotated step labels). Meta-evaluation datasets (MR-MATH, MR-GSM8K) use human annotators (undergraduates with math background) to label step correctness and locate first error for ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Performance measured versus human annotations with macro F1 (solution- and step-level) and AUC where thresholds are needed. REASONEVAL variants outperform prompting and embedding baselines: e.g., REASONEVAL_Llemma-34B achieves high AUC/F1 on MR-MATH (Table 1: solution- and step-level AUCs/F1s; Table 2 shows strong OOD generalization on MR-GSM8K). Exact numbers in paper (e.g., REASONEVAL_Llemma-34B solution-level AUC ~90.8 on MR-MATH-invalid, step-level AUC ~92.8 in Table 1/2).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Even fine-tuned LLM evaluators lose some human-like judgment abilities: (1) they are less reliable at judging redundancy (neutral steps) because redundancy is inherently ambiguous and labels are noisier; (2) their effectiveness strongly depends on high-quality human-labeled training data — noisy synthetic labels (e.g., Math-shepherd) degrade their ability to localize errors; (3) step-level localization remains harder than solution-level correctness (i.e., fine-grained error identification remains challenging compared to humans).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Math-shepherd-trained REASONEVAL (unsupervised/noisy step labels) performs worse at step-level error localization than PRM800K-trained REASONEVAL; identification of redundant steps is consistently more difficult across base models (paper notes redundancy detection is more intricate and ambiguous). The paper also documents that human-annotated datasets are crucial — PRM800K-trained evaluators outperform those trained on noisier labels.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>REASONEVAL fine-tuned on high-quality human labels can surpass prompting-based GPT-4 at step-level localization and approaches GPT-4 on solution-level judgments; REASONEVAL generalizes well OOD (MR-GSM8K) when trained with good data and strong base models (Llemma-34B, WizardMath-7B-v1.1). Thus, with human labels and appropriate model selection, LLM evaluators can match or exceed some human-judgment aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>See §3 (REASONEval design and implementation), §3.4 (Fine-tuning on PRM800K), §4.1 (meta-evaluation setup), §4.2 (results: Table 1), §4.3 (OOD generalization: Table 2), and Appendix D (thresholds/prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Mathematical Reasoning Beyond Accuracy', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9947.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9947.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human annotation (PRM800K, MR-MATH, MR-GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human step-level annotations used as ground truth (PRM800K; undergrad labels for MR-MATH; human labels in MR-GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human annotators (trained/higher-education with math background) labeled step-level correctness and redundancy to provide gold-standard supervision and meta-evaluation ground truth; used both to train evaluators (PRM800K) and to test evaluators (MR-MATH, MR-GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Mathematical reasoning / step-level labeling</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>PRM800K: ≈800k step-level labels across ≈75k solutions (human annotated) used to train REASONEVAL; MR-MATH: undergraduates labeled 83 samples with incorrect steps and 76 without (solutions still reach correct final answer) to test step-level error localization; MR-GSM8K: human-labeled step correctness for GSM8K and reversed problems (≈1.4k samples each) used for OOD tests.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Human labels serve as ground truth; automatic evaluators are compared to these labels using macro F1 and AUC; used to compute false positive rate estimates when applying automated judges.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Humans provide higher-quality, nuanced judgments (especially on ambiguous redundancy), but are costly and slow. The paper emphasizes that models and automated methods can degrade on subtle step-level judgments and ambiguity handling when human labels are unavailable or when training labels are noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Comparisons show that evaluators trained on noisy synthetic labels (e.g., Math-shepherd) perform worse against human labels; many automatic methods (including prompting LLMs) have lower step-level F1/AUC compared to REASONEVAL trained on human PRM800K, illustrating the importance of human labeling for fine-grained judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>When high-quality human labels are used for training, LLM evaluators can approach or exceed prompting baselines at many metrics; nevertheless, human labeling remains the gold standard for ambiguity-rich judgments such as redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>See §3.4 (PRM800K description), §4.1 (construction of MR-MATH and MR-GSM8K with human labels), §4.2 (analysis of training data effects), and Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Mathematical Reasoning Beyond Accuracy', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MRGSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation <em>(Rating: 2)</em></li>
                <li>LLMs cannot find reasoning errors, but can correct them! <em>(Rating: 2)</em></li>
                <li>ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning <em>(Rating: 1)</em></li>
                <li>Let's Verify Step by Step <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9947",
    "paper_id": "paper-3bb853fe6bdbe889a8715c93f78dae01cf1bc65c",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Prompting-LLM-judge",
            "name_full": "Prompting-based Large Language Model as Judge (e.g., GPT-4, GPT-3.5-turbo)",
            "brief_description": "Using powerful generative LLMs prompted to judge step-by-step mathematical solutions (produce final judgement, locate first error, or identify neutral/redundant steps); used as an automated alternative to human raters.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Mathematical reasoning / multi-step math problem solution evaluation",
            "llm_judge_model": "GPT-4, GPT-3.5-turbo (prompting-based)",
            "llm_judge_setup": "Prompting with a step-by-step evaluation template (asks for stepwise evaluation, final judgment 'correct'/'wrong', and first-error step or list of neutral steps); prompts adapted from Zeng et al. (2024) and listed in Appendix (Figure 6). Often run directly (no fine-tuning) and requires a powerful model (GPT-4 recommended).",
            "human_evaluation_setup": "Human labels used as ground truth in meta-evaluation datasets (e.g., MR-MATH labels collected by undergraduates with solid math background; PRM800K human-annotated step labels used for training/evaluation).",
            "agreement_metric": "Compared against human annotations using macro F1 and AUC. Reported prompting results (Table 1): GPT-3.5-turbo and GPT-4 solution/step-level performance (e.g., GPT-4 solution-level F1 ~73.2 on MR-MATH-invalid; step-level F1 ~61.0; redundancy metrics lower: solution-level ~57.1, step-level ~54.2).",
            "losses_identified": "Prompting LLM judges are limited in step-level error localization and in judging redundancy: they struggle to reliably detect fine-grained step errors and ambiguous/neutral (redundant) steps; they incur cost and transparency issues; their judgments depend heavily on model capability and the prompt, and they may miss subtle logical/calculation mistakes humans detect.",
            "examples_of_loss": "MR-MATH is constructed so many solutions have correct final answers but include incorrect intermediate steps; GPT-4 and GPT-3.5-turbo perform worse at step-level identification and redundancy detection than specialized REASONEVAL evaluators (Table 1). The paper cites prior finding that even SOTA (GPT-4) struggles to detect reasoning errors (footnote referencing Tyen et al. 2023).",
            "counterexamples_or_caveats": "Prompting-based GPT-4 attains strong solution-level performance (approaching human-level F1 on some solution-level tasks) and can be effective for coarse correctness checks; however, high cost, lack of transparency, and poorer step-level/localization performance remain important caveats. The paper treats GPT-4 as a strong baseline but shows fine-tuned step-level evaluators can surpass it at locating errors.",
            "paper_reference": "See §2.2 (Prompting-based Method), §4.1 (Evaluators & meta-evaluation setup), §4.2 (Results and Analysis), Table 1, Appendix D (prompts), and footnote referencing Tyen et al. (2023).",
            "uuid": "e9947.0",
            "source_info": {
                "paper_title": "Evaluating Mathematical Reasoning Beyond Accuracy",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "REASONEVAL (LLM evaluator)",
            "name_full": "REASONEVAL: Fine-tuned LLM-based Step-level Evaluator for Validity and Redundancy",
            "brief_description": "A supervised LLM evaluator that classifies each reasoning step as positive/neutral/negative (valid + useful / valid + redundant / invalid) and aggregates step scores to solution-level validity and redundancy; instantiated by fine-tuning various base LLMs on human-labeled step data (PRM800K).",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Mathematical reasoning / step-by-step solution quality evaluation (validity and redundancy)",
            "llm_judge_model": "Multiple fine-tuned backbones: Llama-2-7B, Mistral-7B, Llemma-7B/34B, WizardMath-7B variants, Abel-7B-002 (REASONEVAL is the fine-tuned evaluator derived from these bases)",
            "llm_judge_setup": "Supervised fine-tuning on PRM800K (≈800k step-level human labels over 75k solutions) replacing LM head with a classification head producing p_positive/p_neutral/p_negative per step; aggregates step scores using min (validity) and max (redundancy); thresholds used for solution-level judgments (validity 0.5, redundancy 0.15 as in Appendix D).",
            "human_evaluation_setup": "Training labels derived from PRM800K (human-annotated step labels). Meta-evaluation datasets (MR-MATH, MR-GSM8K) use human annotators (undergraduates with math background) to label step correctness and locate first error for ground truth.",
            "agreement_metric": "Performance measured versus human annotations with macro F1 (solution- and step-level) and AUC where thresholds are needed. REASONEVAL variants outperform prompting and embedding baselines: e.g., REASONEVAL_Llemma-34B achieves high AUC/F1 on MR-MATH (Table 1: solution- and step-level AUCs/F1s; Table 2 shows strong OOD generalization on MR-GSM8K). Exact numbers in paper (e.g., REASONEVAL_Llemma-34B solution-level AUC ~90.8 on MR-MATH-invalid, step-level AUC ~92.8 in Table 1/2).",
            "losses_identified": "Even fine-tuned LLM evaluators lose some human-like judgment abilities: (1) they are less reliable at judging redundancy (neutral steps) because redundancy is inherently ambiguous and labels are noisier; (2) their effectiveness strongly depends on high-quality human-labeled training data — noisy synthetic labels (e.g., Math-shepherd) degrade their ability to localize errors; (3) step-level localization remains harder than solution-level correctness (i.e., fine-grained error identification remains challenging compared to humans).",
            "examples_of_loss": "Math-shepherd-trained REASONEVAL (unsupervised/noisy step labels) performs worse at step-level error localization than PRM800K-trained REASONEVAL; identification of redundant steps is consistently more difficult across base models (paper notes redundancy detection is more intricate and ambiguous). The paper also documents that human-annotated datasets are crucial — PRM800K-trained evaluators outperform those trained on noisier labels.",
            "counterexamples_or_caveats": "REASONEVAL fine-tuned on high-quality human labels can surpass prompting-based GPT-4 at step-level localization and approaches GPT-4 on solution-level judgments; REASONEVAL generalizes well OOD (MR-GSM8K) when trained with good data and strong base models (Llemma-34B, WizardMath-7B-v1.1). Thus, with human labels and appropriate model selection, LLM evaluators can match or exceed some human-judgment aspects.",
            "paper_reference": "See §3 (REASONEval design and implementation), §3.4 (Fine-tuning on PRM800K), §4.1 (meta-evaluation setup), §4.2 (results: Table 1), §4.3 (OOD generalization: Table 2), and Appendix D (thresholds/prompts).",
            "uuid": "e9947.1",
            "source_info": {
                "paper_title": "Evaluating Mathematical Reasoning Beyond Accuracy",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Human annotation (PRM800K, MR-MATH, MR-GSM8K)",
            "name_full": "Human step-level annotations used as ground truth (PRM800K; undergrad labels for MR-MATH; human labels in MR-GSM8K)",
            "brief_description": "Human annotators (trained/higher-education with math background) labeled step-level correctness and redundancy to provide gold-standard supervision and meta-evaluation ground truth; used both to train evaluators (PRM800K) and to test evaluators (MR-MATH, MR-GSM8K).",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Mathematical reasoning / step-level labeling",
            "llm_judge_model": null,
            "llm_judge_setup": null,
            "human_evaluation_setup": "PRM800K: ≈800k step-level labels across ≈75k solutions (human annotated) used to train REASONEVAL; MR-MATH: undergraduates labeled 83 samples with incorrect steps and 76 without (solutions still reach correct final answer) to test step-level error localization; MR-GSM8K: human-labeled step correctness for GSM8K and reversed problems (≈1.4k samples each) used for OOD tests.",
            "agreement_metric": "Human labels serve as ground truth; automatic evaluators are compared to these labels using macro F1 and AUC; used to compute false positive rate estimates when applying automated judges.",
            "losses_identified": "Humans provide higher-quality, nuanced judgments (especially on ambiguous redundancy), but are costly and slow. The paper emphasizes that models and automated methods can degrade on subtle step-level judgments and ambiguity handling when human labels are unavailable or when training labels are noisy.",
            "examples_of_loss": "Comparisons show that evaluators trained on noisy synthetic labels (e.g., Math-shepherd) perform worse against human labels; many automatic methods (including prompting LLMs) have lower step-level F1/AUC compared to REASONEVAL trained on human PRM800K, illustrating the importance of human labeling for fine-grained judgment.",
            "counterexamples_or_caveats": "When high-quality human labels are used for training, LLM evaluators can approach or exceed prompting baselines at many metrics; nevertheless, human labeling remains the gold standard for ambiguity-rich judgments such as redundancy.",
            "paper_reference": "See §3.4 (PRM800K description), §4.1 (construction of MR-MATH and MR-GSM8K with human labels), §4.2 (analysis of training data effects), and Table 3.",
            "uuid": "e9947.2",
            "source_info": {
                "paper_title": "Evaluating Mathematical Reasoning Beyond Accuracy",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MRGSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation",
            "rating": 2
        },
        {
            "paper_title": "LLMs cannot find reasoning errors, but can correct them!",
            "rating": 2
        },
        {
            "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
            "rating": 1
        },
        {
            "paper_title": "Let's Verify Step by Step",
            "rating": 1
        }
    ],
    "cost": 0.013108,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating Mathematical Reasoning Beyond Accuracy</h1>
<p>Shijie Xia ${ }^{1,2,5}$, Xuefeng $\mathbf{L i}^{1,2,5}$, Yixin $\mathbf{L i u}^{4}$, Tongshuang $\mathbf{W u}^{3 <em>}$, Pengfei $\mathbf{L i u}^{1,2,5 </em>}$<br>${ }^{1}$ Shanghai Jiao Tong University, ${ }^{2}$ Shanghai Artificial Intelligence Laboratory<br>${ }^{3}$ Carnegie Mellon University, ${ }^{4}$ Yale University, ${ }^{5}$ Generative AI Research Lab (GAIR) {shijiexia, xuefengli, pengfei}@sjtu.edu.cn, yixin.liu@yale.edu, sherryw@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>The leaderboard of Large Language Models (LLMs) in mathematical tasks has been continuously updated. However, the majority of evaluations focus solely on the final results, neglecting the quality of the intermediate steps. This oversight can mask underlying problems, such as logical errors or unnecessary steps in the reasoning process. To measure reasoning beyond final-answer accuracy, we introduce REASONEVAL, a new methodology for evaluating the quality of reasoning steps. REASONEVAL employs validity and redundancy to characterize the reasoning quality, as well as accompanying LLMs to assess them automatically. We explore different design options for the LLM-based evaluators and empirically demonstrate that REASONEVAL, when instantiated with base models possessing strong mathematical knowledge and trained with high-quality labeled data, consistently outperforms baseline methods in the meta-evaluation datasets. We also highlight the strong generalization capabilities of REASONEVAL. By utilizing REASONEVAL to evaluate LLMs specialized in math, we find that an increase in final-answer accuracy does not necessarily guarantee an improvement in the overall quality of the reasoning steps for challenging mathematical problems. Additionally, we observe that REASONEVAL can play a significant role in data selection. We open-source the best-performing model, meta-evaluation script, and all evaluation results to facilitate future research.</p>
<p>Code — https://github.com/GAIR-NLP/ReasonEval</p>
<h2>1 Introduction</h2>
<p>Mathematical reasoning, a core cognitive skill, is crucial for resolving complex problems and making informed decisions (Hendrycks et al. 2021; Lewkowycz et al. 2022), playing a significant role in large language models (LLMs) research (Azerbayev et al. 2023; Lu et al. 2023). Given its significance, reliably evaluating mathematical reasoning in LLMs becomes crucial. Current methodologies to evaluate mathematical reasoning in LLMs focus primarily on the final result (Luo et al. 2023; Chern et al. 2023; Yu et al. 2023), neglecting the intricacies of the reasoning process. For example, the OpenLLM leaderboard, ${ }^{1}$ a relatively well-recognized</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>benchmark for LLMs, uses overall accuracy to assess models' mathematical reasoning. Despite being effective to some degree, such evaluation practice could mask underlying issues such as logical errors or unnecessary steps that compromise accuracy and efficiency of reasoning steps. In this work, we argue that a desirable evaluation criterion for mathematical reasoning encompasses not only the accuracy of the final answer but also the correctness and efficiency of each step in the reasoning process. Moreover, it is imperative that the evaluation metrics be open-source and replicable to ensure transparency and reliability.</p>
<p>Our design philosophy stems from the fact that a correct final answer does not guarantee a flawless reasoning process (Lewkowycz et al. 2022; Uesato et al. 2022), and excessive or irrelevant reasoning steps can lead to potential errors as well as increased computational costs (Zhang et al. 2023). Once these issues go unnoticed, they can cause problems in many application scenarios. For example, in K12 mathematics education, incorrect or redundant solution steps provided by LLMs could mislead students. There have been some recent works related to the above evaluation principle. Specifically, Uesato et al. (2022); Lightman et al. (2023); Wang et al. (2023) present process reward models for mathematical reasoning, ${ }^{2}$ which focus on their utility as verifiers (Cobbe et al. 2021) to boost the accuracy (i.e., by generating many candidate solutions and selecting the one ranked highest by the verifier), leaving its underlying potential to identify reasoning errors ${ }^{3}$ and redundancy. Clinciu, Eshghi, and Hastie (2021); Golovneva et al. (2022) rely on embedding-based methods to measure the quality of general reasoning explanation, which has limitations in handling diverse reasoning steps from various models.</p>
<p>In response to these challenges, we propose ReAsonEval, a suite comprising a new evaluation methodology with defined metrics for assessing mathematical reasoning quality and corresponding LLM-based evaluators for automated calculation. As illustrated in Figure 1, REASONEVAL emphasizes the validity (i.e., the step contains no mistakes</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Given a solution to a math problem, REASONEVAL scores each step and identifies the potential error location, serving as an extension to verify the final answer only.</p>
<p>in calculation and logic) of each reasoning step, and evaluates the <em>redundancy</em> (i.e., the step lacks utility in solving the problem but is still valid) of steps to ensure efficiency. We explore different design options for the LLM-based evaluators and show training such a high-quality evaluator for mathematical reasoning is not a straightforward task, which requires an LLM with strong mathematical knowledge as well as high-quality training data. We empirically demonstrate that REASONEVAL using the optimal design can consistently outperform baseline methods by a large margin in the meta-evaluation datasets (§4.2). Moreover, we highlight the strong generalization ability of REASONEVAL (§4.3).</p>
<p>Furthermore, we show the utility of REASONEVAL through the lens of two preliminary applications, as detailed in §5: (1) evaluating different LLMs trained for mathematical reasoning; and (2) selecting high-quality data to train such LLMs. We have the following main findings: (1) We find that an improvement in the result accuracy is not sufficient to ensure an enhancement in the overall quality of reasoning steps in challenging mathematical problems; (2) We find that the model scale, the base model, and the training methods have significantly influenced the quality of reasoning steps; (3) We find that REASONEVAL can select high-quality training data to improve the efficiency of solving problems and the quality of solutions. These findings pave the way for future work to design methods that take into account the process of problem-solving.</p>
<p>Overall, our main contributions are as follows:</p>
<p>(1) We recognize the potential gap between what we are evaluating and what we are desiring in mathematical reasoning, prioritize validity and redundancy aspect to address the misalignments and inefficiencies that can arise in mathematical reasoning processes (§3). (2) We design a meta-evaluation testbed to assess the reliability (§4) and utility (§5) of mentioned metrics, guiding us to a superior metric design, solidifying the foundation for future exploration and significantly lowering trial and error costs. (3) We open-source our best-performing metric, meta-evaluation script and all evaluation results to facilitate future research.</p>
<h2>2 Preliminaries</h2>
<h3>2.1 Problem Formulation</h3>
<p>Given a mathematical problem <em>q</em>, solution steps <strong>ĥ</strong> = {<em>ĥ</em>₁, ..., <em>ĥ</em>ₚ} and an answer <em>â</em> generated by LLMs, the goal is to evaluate how well the generated solution and answer are. Usually, the ground truth answer <em>a</em> is available but the reference solution step <strong>h</strong> = {<em>h</em>₁, ..., <em>h</em>⋯} is not always provided.</p>
<h3>2.2 Existing Evaluation Methodology</h3>
<p><strong>Answer-based Matching</strong> Most of the existing works (Luo et al. 2023; Chern et al. 2023; Yu et al. 2023) measure the mathematical reasoning quality by directly comparing the fi-</p>
<p>nal answer (i.e., $\hat{a}$ and $a$ ) and calculating the overall accuracy on a given dataset.</p>
<p>Reference-based Scoring Instead of only using the final result as a scoring criterion, some works <em>Sawada et al. (2023)</em> try to measure the reasoning quality by comparing the similarity between generated and reference solution steps (i.e., $\hat{\mathbf{h}}$ and $\hat{\mathbf{h}}$ ). Although datasets like MATH <em>Hendrycks et al. (2021)</em> and GSM8K <em>Cobbe et al. (2021)</em> provide the ground truth solutions, the existence of diverse reasoning paths leading to the same answer $a$ renders reliance on any single one of them as a reference unreliable.</p>
<p>Prompting-based Method This method directly asks LLMs with a well-defined prompt to generate a judgment for a given generated solution and answer <em>Hao et al. (2024)</em>. This approach usually requires a very powerful LLM, often GPT-4, which may not be practical for iterative model development due to cost and transparency concerns.</p>
<p>The aforementioned methods either focus solely on the final results (e.g., $a$ ) or are limited by the use of reference solutions and proprietary LLMs, and most of them only concentrate on evaluating "correctness", neglecting other aspects such as the redundancy of solution steps. This has inspired us to propose a set of evaluation criteria that better aligns with the reasoning models in the era of LLMs.</p>
<h2>3 REASONEval: Metrics and Implementations</h2>
<h3>3.1 Design Principle</h3>
<p>We argue that for tasks involving multi-step reasoning, measuring the quality of reasoning should not solely depend on the correctness of the final result. It should also consider (1) the accuracy of each reasoning step; (2) the efficiency of the reasoning process. To this end, we design evaluators that assess reasoning steps regarding validity and redundancy in a step-by-step format, checking whether each step advances well towards solving the problem. Precisely, validity denotes the step contains no mistakes in calculation and logic, and redundancy describes the step lacks utility in solving the problem but is still valid.</p>
<h3>3.2 Scoring Scheme</h3>
<p>Following this principle, we formulate such a metric design process as a classification task. We classify each step into three classes: positive, neutral, and negative. The positive label indicates that the step is correct and contributes to solving the question, the neutral label represents that the step is correct but does not make any progress, and the negative label signifies an incorrect step. Given a question $q$ and the solution steps $\hat{\mathbf{h}}={\hat{h}<em N="N">{1}, \ldots, \hat{h}</em>}$, each step will be assigned the probability of the three classes as follows:</p>
<p>$$
\begin{gathered}
\left{p^{1}, \ldots, p^{N}\right}=\operatorname{REASONEval}\left(q, \hat{h}<em N="N">{1}, \ldots, \hat{h}</em>\right) \
p^{i}=\left(p_{\text {positive }}^{i}, p_{\text {neutral }}^{i}, p_{\text {negative }}^{i}\right)
\end{gathered}
$$</p>
<p>The validity score, concerning only the correctness of the reasoning steps, is defined as:</p>
<p>$$
S_{\text {validity }}^{i}=p_{\text {positive }}^{i}+p_{\text {neutral }}^{i}
$$</p>
<p>The redundancy score, concerning the utility of the steps given that they are valid, is defined as:</p>
<p>$$
S_{\text {redundancy }}^{i}=p_{\text {neutral }}^{i}
$$</p>
<p>We can aggregate the step-level scores to get the solutionlevel scores. We use the 'min' and 'max' operations:</p>
<p>$$
\begin{aligned}
S_{\text {validity }}^{\text {all }} &amp; =\min \left(S_{\text {validity }}^{1}, \ldots, S_{\text {validity }}^{N}\right) \
S_{\text {redundancy }}^{\text {all }} &amp; =\max \left(S_{\text {redundancy }}^{1}, \ldots, S_{\text {redundancy }}^{N}\right)
\end{aligned}
$$</p>
<p>We describe the detailed justification of our scoring scheme in Appendix A.</p>
<h3>3.3 Model Architecture</h3>
<p>LLM Backbone Our defined evaluation method (Eq.1) can be implemented by directly prompting existing LLMs or fine-tuning LLMs using supervised dataset. To make a comprehensive study, in this work we instantiate REASONEVAL by different types of LLMs, which vary in base model types (e.g., Llama2 <em>Touvron et al. (2023)</em> and Mistral <em>Jiang et al. (2023)</em>), model sizes (e.g., 7B, 34B), and post-training strategies (e.g., continued pretraining <em>Azerbayev et al. (2023)</em> and fine-tuning).</p>
<p>Task-specific Layer Our evaluator's architecture is identical to the base model, except that the linear layer for nexttoken prediction is replaced with a linear layer for outputting the possibilities of each class. After normalization with the softmax layer, we get the possibility for each class.</p>
<h3>3.4 Fine-tuning</h3>
<p>The above task formulation allows us to utilize the existing dataset, PRM800K <em>Lightman et al. (2023)</em>, as training data in REASONEVAL. PRM800K contains about 800,000 step-level labels over 75,000 solutions. It is collected by employing humans to label the step-by-step solutions generated by GPT- 4 to MATH problems. The label categories for the reasoning steps are identical to those mentioned in $\S 3.2$. In the training phase, the loss function only includes the last token of each step, as it contains the full information about the step. We also take the last token of each step for prediction at test time. We provide the details on training and splitting solutions in Appendix B.</p>
<h2>4 Meta Evaluation</h2>
<h3>4.1 Meta-evaluation Setup</h3>
<p>Meta-evaluation ${ }^{4}$ Datasets Inspired by <em>Zeng et al. (2024)</em>, which propose a benchmark named Meta-Reasoning-GSM8K (MR-GSM8K), we construct a meta-evaluation dataset MR-MATH to better assess how well different evaluators can detect errors in reasoning steps. It is constructed as follows: (1) To collect the first type of errors affecting the correctness of steps, we recruit undergraduates who have a solid mathematical background to label solutions generated by Abel <em>Chern et al. (2023)</em> and WizardMath <em>Luo et al. (2023)</em>. We collect 83 samples with incorrect steps and 76 without, pinpointing</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MR-MATH-invalid</th>
<th style="text-align: center;">MR-MATH-redundant</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Solution-level</td>
<td style="text-align: center;">Step-level</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1 Score</td>
<td style="text-align: center;">AUC</td>
</tr>
<tr>
<td style="text-align: center;">Embedding-based Methods</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ROSCOE-SA</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">57.5</td>
</tr>
<tr>
<td style="text-align: center;">ROSCOE-SS</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">49.6</td>
</tr>
<tr>
<td style="text-align: center;">Prompting-based Methods</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Step-level Evaluators</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Math-shepherd-Mistral-7b</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">77.3</td>
</tr>
<tr>
<td style="text-align: center;">REASONEVAL $_{\text {Llama2-7B }}$</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">79.5</td>
</tr>
<tr>
<td style="text-align: center;">REASONEVAL $_{\text {WizardMath-7B-V1.0 }}$</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">81.9</td>
</tr>
<tr>
<td style="text-align: center;">REASONEVAL $_{\text {Mistral-7B }}$</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">85.1</td>
</tr>
<tr>
<td style="text-align: center;">REASONEVAL $_{\text {Llemma-7B }}$</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">84.3</td>
</tr>
<tr>
<td style="text-align: center;">REASONEVAL $_{\text {Abel-7B-002 }}$</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">86.2</td>
</tr>
<tr>
<td style="text-align: center;">REASONEVAL $_{\text {WizardMath-7B-V1.1 }}$</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">87.5</td>
</tr>
<tr>
<td style="text-align: center;">REASONEVAL $_{\text {Llemma-34B }}$</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">90.8</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of methods for automatic evaluation of reasoning steps in MR-MATH. For any methods that require setting a threshold, we report the Area Under the Curve (AUC) metric.
the first error location in the former. All the solutions reach the correct final answers, meaning the evaluators need to judge correctness based on the process rather than the outcome. Additionally, since the style of solutions differs from the training set of PRM800K (See examples of solutions in Appendix C), it tests the generalization of REASONEval to some degree. (2) For the second type of errors affecting the efficiency of problem solving process, as they are more rarer than the first one, we sample solutions from the test set of PRM800K directly, containing 150 samples with redundant steps and 150 samples without.</p>
<p>Evaluators We compare three methods to evaluate reasoning steps automatically: embedding-based methods, prompting-based methods and step-level evaluators. For the embedding-based methods, we choose ROSCOE (Golovneva et al. 2022), a SOTA method among them. Specifically, we select the semantic alignment metrics (ROSCOE-SA) and the semantic similarity metrics (ROSCOE-SS), which do not require references solution steps. For the prompting-based methods, we select GPT-3.5-turbo and GPT-4, two mainstream generation models. We use the prompt suggested by Zeng et al. (2024) for the invalid errors and the modified version for the redundant errors. For the step-level evaluators, we select 7 base models for REASONEval: Llama-2-7B, Mistral-7B, Llemma-7B, Llemma-34B, WizardMath-7B-V1.0 (Luo et al. 2023), WizardMath-7B-V1.1, Abel-7B002 (Chern et al. 2023). The llemma series is continuing pertaining on math-based corpus. The Abel and WizardMath series are fine-tuning for solving mathematical problems. We also select the open-source SOTA process reward model Math-shepherd-Mistral-7B (Wang et al. 2023) to compare.</p>
<p>The settings for these methods are in Appendix D.
Meta-evaluation Metrics We test two abilities: judging whether the solution contains errors and locating the error step. In the first task the ground truth is labels for solutions (solution-level), and in the second tasks the ground truth is labels for steps (step-level). Since both are classification tasks, we present the macro F1 score as a performance metric. Additionally, for any methods that require setting a threshold, we report the Area Under the Curve (AUC) metric, which is a more balanced evaluation of performance across different threshold settings.</p>
<h3>4.2 Results and Analysis</h3>
<p>Overall Results We present our main results in Table 1. REASONEval outperforms baseline methods across all error types and label levels. It is noteworthy that the identification of errors at the step level is generally more challenging than at the solution level for all methods. This suggests a higher complexity in pinpointing errors at the individual step level. Furthermore, identifying redundant errors is more intricate than invalid errors due to the inherent ambiguity involved in the former.</p>
<p>Analysis on Base Models We investigate the correlation between the base LLMs’ mathematical reasoning capabilities and the performance of REASONEVAL fine-tuned from them. In Figure 2, for the invalid errors, an increase in AUC shows a positive correlation with the base models’ ability to solve MATH problems, indicating that enhancing mathematical problem-solving abilities is beneficial for identifying invalid errors. It is noted that the Llemma-34B outperforms all 7B models, although its Pass@1 is not the highest. It highlights</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MR-GSM8K-original</th>
<th style="text-align: center;">MR-GSM8K-reversed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Solution-level</td>
<td style="text-align: center;">Step-level</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OOD</td>
<td style="text-align: center;">F1 Score</td>
<td style="text-align: center;">AUC</td>
</tr>
<tr>
<td style="text-align: center;">Embedding-based Methods</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ROSCOE-SA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">54.4</td>
</tr>
<tr>
<td style="text-align: center;">ROSCOE-SS</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">60.1</td>
</tr>
<tr>
<td style="text-align: center;">Prompting-based Methods</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Step-level Evaluators</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Math-shepherd-Mistral-7b</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">93.9</td>
</tr>
<tr>
<td style="text-align: center;">REASONEVAL Mistral-7B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">79.8</td>
</tr>
<tr>
<td style="text-align: center;">REASONEVAL Wizard Math-7B-V1.1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">90.7</td>
</tr>
<tr>
<td style="text-align: center;">REASONEVAL Llemma-34B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">88.1</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of methods for automatic evaluation of reasoning steps in MR-GSM8K. "OOD" represents that its training data contains the labeled solutions for problems of the dataset. The results of prompting-based methods are from Zeng et al. (2024).
the importance of model scale and the continued pretraining over math-related corpus. For the redundant errors, the distinction across different base models is small, and it does not show the correlation as the invalid errors. This may be due to developers for these models prioritizing the correctness of solutions over their efficiency, resulting in similar performances in this aspect across various LLMs.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Correlation between Pass@1 for the base model on MATH and AUC for the solution-level labels.</p>
<p>Analysis on Training Data In Table 1, the Mistral7B trained on Math-Shepherd falls behind that
trained on PRM800K. We summarize the information about Math-shepherd and PRM800K in Table 3. For Math-shepherd, although having 6x training data, there is more noise in the step-level labels generated with the unsupervised method, thus harming its precision in identifying errors. And the only two classes of labels also make it limited in evaluating redundancy. Nonetheless, the data synthesis method is promising. We leave it for future work to optimize this method to reduce noise.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">#S</th>
<th style="text-align: center;">#C</th>
<th style="text-align: center;">Problem <br> Source</th>
<th style="text-align: center;">Human <br> Ann.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Math-shepherd</td>
<td style="text-align: center;">440 K</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">MATH, <br> GSM8K</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
</tr>
<tr>
<td style="text-align: center;">PRM800K</td>
<td style="text-align: center;">75 K</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison between Math-shepherd and PRM800K. "#S" represents the number of labeled solutions. "#C" represents the number of label categories for the reasoning steps. For Math-shepherd, the categories are "correct" and "incorrect". "Human Ann." indicates whether the labels for the reasoning steps are generated by human annotations.</p>
<h3>4.3 Out-Of-Distribution Generalization</h3>
<p>Setup To assess the out-of-distribution generalization of REASONEval, we evaluate its performance on MR-GSM8K (Zeng et al. 2024). We select two distinct types of questions from the dataset. The first type of question is from the original GSM8K. The second type, termed reversed reasoning (Yu et al. 2023), involves concealing one of the inputs and requiring the computation of the missing input using the provided original answer. Both problem sets cover the full</p>
<p>test set of GSM8K, comprising approximately 1.4K samples each. The step-by-step solutions for these questions are sampled from MetaMath-7B (Yu et al. 2023) and include human annotations labeling the correctness of each step. Since the training data of REASONEVAL does not include the labeled solutions for these types of problems, this evaluation serves as a robust test of its generalization ability. All settings are identical to those described in §4.1.</p>
<p>Results We present the results in Table 2. We observe that both REASONEVAL_{Lemma-34B} and REASONEVAL_{WizardMath-7B-V1.1} achieve superior performance at the step level and approach the performance of GPT-4 at the solution level, demonstrating strong generalization capabilities. Additionally, REASONEVAL is more robust to the question types compared to other methods. For Math-shepherd-Mistral-7B, it performs best in solution-level labels but lags behind in step-level labels, suggesting that the noise in step labels from Math-Shepherd negatively impacts its ability to accurately locate error steps.</p>
<h2>5 Utilizing REASONEVAL for Evaluating and Improving Reasoning Quality</h2>
<h3>5.1 Evaluating Reasoning Quality of LLMs Specialized in Math</h3>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Box-and-whisker plots of $S_{\text{validity}}$ (upper) and $S_{\text{redundancy}}$ (lower). The boundaries of the whiskers are based on the 1.5 interquartile range.</p>
<p>Setup The models selected for measurement are two mainstream LLMs specialized in math: Abel (Chern et al. 2023) and WizardMath (Luo et al. 2023), with different scales. We also report the results of LLaMA-2 (Touvron et al. 2023) naive fine-tuned on PRM800K (approximately 6K solutions that reach the correct final answers) for comparison. We evaluate the performance of the LLMs on MATH and sample 100 solutions for each problem to reduce evaluation noise and sampling randomness. Specifically, the sampling temperature is set to 0.6, and the top-p value is set to 0.95. The maximum sample length is set to 2048 tokens. All solutions are scored by REASONEVAL_{Lemma-34B}.</p>
<p>Our analysis includes two aspects: (1) False positive rate: it refers to the proportion of solutions that have the correct final answers but contain incorrect steps among all solutions with correct final answers. (2) The redundancy of solutions.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Acc. (%)</th>
<th>FPR (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA2-13B-PRM800K</td>
<td>7.4</td>
<td>40.6</td>
</tr>
<tr>
<td>LLaMA2-70B-PRM800K</td>
<td>14.7</td>
<td>21.8</td>
</tr>
<tr>
<td>Abel7B-001</td>
<td>12.4</td>
<td>31.8</td>
</tr>
<tr>
<td>Abel13B</td>
<td>15.2</td>
<td>29.8 (29.2)</td>
</tr>
<tr>
<td>Abel70B</td>
<td>25.7</td>
<td>20.0</td>
</tr>
<tr>
<td>Abel7B-002</td>
<td>30.0</td>
<td>22.8</td>
</tr>
<tr>
<td>WizardMath7B-V1.0</td>
<td>8.9</td>
<td>34.4</td>
</tr>
<tr>
<td>WizardMath13B</td>
<td>10.9</td>
<td>31.3 (28.3)</td>
</tr>
<tr>
<td>WizardMath70B</td>
<td>18.7</td>
<td>16.5</td>
</tr>
<tr>
<td>WizardMath7B-V1.1</td>
<td>31.0</td>
<td>16.7</td>
</tr>
</tbody>
</table>
<p>Table 4: We estimate the false positive rate (FPR) with REASONEVAL_{Lemma-34B}. We also check the false positive rate of Abel13B and WizardMath13B by human (denoted by ), sampling one solution for each problem. "Acc." represents the overall accuracy.</p>
<p>False Positive Rate We set the threshold to 0.25 for the validity scores and calculate the false positive rate. We describe the details of choosing the threshold value in Appendix E. We present the main results in Table 4. We also include the results in Figure 4 with the information of the tested models. We highlight three key takeaways:</p>
<p>Increased final-answer accuracy does not inherently ensure a corresponding reduction in the false positive rate. In Figure 4, it is clear that the false positive rate drops and stays at a level of about 20% as the final-answer accuracy rises. When comparing WizardMath7B-V1.1 with WizardMath70B, although the former exhibits a much higher accuracy (31.0% vs. 18.7%), its false positive rate is almost the same (16.7% vs. 16.5%). It indicates there exists a bottleneck for the quality of reasoning steps to advance.</p>
<p>The model size and base model affect the false positive rate significantly. When comparing LLaMA13B-PRM800K with LLaMA70B-PRM800K, although using the same training data, the distinction in false positive rates is large (40.6% vs. 21.8%). It indicates the importance of the model scale. The base model of Mistral (Jiang et al. 2023) also achieves a lower false positive rate than LLaMA2 (22.8% vs. 31.8%, 16.7% vs. 34.4%). Overall, the scaling law still applies to this field.</p>
<p>RLHF helps lower the false positive rate when the LLMs are relatively strong. There's no distinction in false positive rates between SFT and SFT plus RLHF when the model is relatively weak, like 7B and the 13B model of LLaMA-2 (31.8% vs. 34.4%, 29.8% vs. 31.3%). As the model size becomes larger, SFT plus RLHF performs better (16.5% vs. 20.0%, 16.7% vs. 22.8%). This indicates that ef-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />Figure 4: Correlation between the final-answer accuracy and the false positive rate.</p>
<p>fective supervision by RLHF requires a strong mathematical ability of the model itself.</p>
<p>Redundancy of Solutions We analyze the redundancy in reasoning steps. Our main results are in Figure 3. It is noteworthy that the redundancy scores of Abel family are usually higher. This is because the solutions from Abel often involve restating the problem first, which is considered a meaningless action towards solving the problem by our evaluator. Moreover, the redundancy scores for solutions that fail to reach the correct answers are higher. This suggests that when a model is unsure about how to solve a problem, it tends to make more attempts that lack meaningful progression.</p>
<h3>5.2 Improving Reasoning Quality by Selecting Training Data</h3>
<p>In this part, we explore the potential of REASONEVAL to select high-quality training data for SFT.</p>
<p>Setup The MMIQC (Liu and Yao 2024) dataset consists of a mixture of processed web data and synthetic questionresponse pairs used to enhance the reasoning capabilities of LLMs. We randomly sample 10K unique responses generated by GPT-3.5-turbo on MATH from the dataset. We then filter it using REASONEVAL_{WizardMath-7B-V1.1}, specifically removing samples with validity scores below 0.5 or redundancy scores above 0.15. We also combine these two conditions. We SFT the mistral-7b in different subsets with 3 epochs and observe the performance on MATH. To compare, we randomly sample the same amount of training data three times and report the average performance.</p>
<p>Results We make the following observations from Table 5: (1) In terms of accuracy, all groups filtered by REASONEVAL_{WizardMath-7B-V1.1} outperform the random group and achieve performance close to that of the full dataset. (2) The average number of tokens for the solutions decreases in groups filtered by REASONEVAL_{WizardMath-7B-V1.1}, indicating its advantage in increasing problem-solving efficiency. (3) The group that combines the two filtering conditions significantly improves the quality of reasoning steps with only about half the training data.</p>
<table>
<thead>
<tr>
<th>Filter</th>
<th>#D</th>
<th>Acc.</th>
<th>Val.</th>
<th>Red.</th>
<th>#Token</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td>100%</td>
<td>22.2</td>
<td>65.2</td>
<td>27.4</td>
<td>723.4</td>
</tr>
<tr>
<td>val.</td>
<td>76.7%</td>
<td>22.0</td>
<td>65.9</td>
<td>26.4</td>
<td>699.9</td>
</tr>
<tr>
<td>random</td>
<td>76.7%</td>
<td>20.1</td>
<td>62.5</td>
<td>27.4</td>
<td>765.6</td>
</tr>
<tr>
<td>red.</td>
<td>71.9%</td>
<td>21.8</td>
<td>65.6</td>
<td>22.1</td>
<td>681.5</td>
</tr>
<tr>
<td>random</td>
<td>71.9%</td>
<td>20.3</td>
<td>62.3</td>
<td>28.0</td>
<td>746.1</td>
</tr>
<tr>
<td>red. &amp; val.</td>
<td>56.7%</td>
<td>22.0</td>
<td>67.8</td>
<td>22.5</td>
<td>701.2</td>
</tr>
<tr>
<td>random</td>
<td>56.7%</td>
<td>20.0</td>
<td>62.1</td>
<td>27.6</td>
<td>739.5</td>
</tr>
</tbody>
</table>
<p>Table 5: “#D” represents the percentage of training data from the full set. “Acc.” represents the overall accuracy. “Val.” and “Red.” represent $S_{\text{validity}}$ and $S_{\text{redundancy}}$ for the solutions with correct results. “#Token” represents the average number of tokens for the solutions.</p>
<h2>6 Related Work</h2>
<p>Evaluating reasoning steps automatically The technique used to assess reasoning steps automatically can be broadly divided into three groups: (1) Embedding-based methods: Golovneva et al. (2022) propose several metrics and uses the embedding-based calculation among hypothesis steps, reference steps, and problems to represent them; (2) Parsing-based methods: this approach aims to parse steps into structured forms, like ‘subject-verb-object’ frames (Prasad et al. 2023) or symbolic proofs (Saparov and He 2022). However, it presents challenges for complex datasets such as MATH due to the intricate logic involved; (3) Prompting-based methods: due to the generalization of LLMs, given tailored prompts to SOTA LLMs, they can check the solutions and find the potential errors (Tyen et al. 2023; Zeng et al. 2024).</p>
<p>Process reward model (PRM) Process reward models are mainly used in reinforcement learning to give feedback to LLMs to align with human logic in mathematics. Lightman et al. (2023) and Uesato et al. (2022) both evaluate the performance of PRM as a verifier against the outcome reward model. Ma et al. (2023) combine a check-and-generation idea with PRM to generate a more accurate response. Wang et al. (2023) propose a way to automatically construct process-wise supervision data. These studies focus on leveraging PRM to enhance accuracy. In contrast, our research explores the potential of PRM to develop new evaluation methods in mathematics, moving beyond mere accuracy improvements.</p>
<h2>7 Conclusion</h2>
<p>In this work, we develop REASONEVAL, a new metric to evaluate the quality of reasoning steps from the perspectives of correctness and efficiency. We construct a meta-evaluation testbed and show REASONEVAL using the optimal design can consistently outperform baseline methods by a large margin. Additionally, we empirically demonstrate the strong</p>
<p>generalization ability of REASONEVAL. With REASONEVAL, we find an inconsistency between final-answer accuracy and the quality of reasoning steps. We also show its efficacy in selecting training data.</p>
<h2>References</h2>
<p>Azerbayev, Z.; Schoelkopf, H.; Paster, K.; Dos Santos, M.; McAleer, S. M.; Jiang, A. Q.; Deng, J.; Biderman, S.; and Welleck, S. 2023. Llemma: An Open Language Model for Mathematics. In The Twelfth International Conference on Learning Representations.
Chern, E.; Zou, H.; Li, X.; Hu, J.; Feng, K.; Li, J.; and Liu, P. 2023. Generative AI for Math: Abel. https://github.com/ GAIR-NLP/abel.
Clinciu, M.-A.; Eshghi, A.; and Hastie, H. 2021. A Study of Automatic Metrics for the Evaluation of Natural Language Explanations. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, 2376-2387. Online: Association for Computational Linguistics.
Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; et al. 2021. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168.
Dai, J.; Pan, X.; Sun, R.; Ji, J.; Xu, X.; Liu, M.; Wang, Y.; and Yang, Y. 2023. Safe RLHF: Safe Reinforcement Learning from Human Feedback. In The Twelfth International Conference on Learning Representations.
Golovneva, O.; Chen, M. P.; Poff, S.; Corredor, M.; Zettlemoyer, L.; Fazel-Zarandi, M.; and Celikyilmaz, A. 2022. ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. In The Eleventh International Conference on Learning Representations.
Hao, S.; Gu, Y.; Luo, H.; Liu, T.; Shao, X.; Wang, X.; Xie, S.; Ma, H.; Samavedhi, A.; Gao, Q.; Wang, Z.; and Hu, Z. 2024. LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models. arXiv:2404.05221.
Hendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart, S.; Tang, E.; Song, D.; and Steinhardt, J. 2021. Measuring Mathematical Problem Solving With the MATH Dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).
Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D. S.; Casas, D. d. l.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; et al. 2023. Mistral 7B. ArXiv preprint, abs/2310.06825.
Lewkowycz, A.; Andreassen, A.; Dohan, D.; Dyer, E.; Michalewski, H.; Ramasesh, V.; Slone, A.; Anil, C.; Schlag, I.; Gutman-Solo, T.; et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35: 3843-3857.
Lightman, H.; Kosaraju, V.; Burda, Y.; Edwards, H.; Baker, B.; Lee, T.; Leike, J.; Schulman, J.; Sutskever, I.; and Cobbe, K. 2023. Let's Verify Step by Step. In The Twelfth International Conference on Learning Representations.</p>
<p>Liu, H.; and Yao, A. C.-C. 2024. Augmenting math word problems via iterative question composing. ArXiv preprint, abs/2401.09003.
Loshchilov, I.; and Hutter, F. 2019. Decoupled Weight Decay Regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.
Lu, P.; Qiu, L.; Yu, W.; Welleck, S.; and Chang, K.-W. 2023. A Survey of Deep Learning for Mathematical Reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1460514631.</p>
<p>Luo, H.; Sun, Q.; Xu, C.; Zhao, P.; Lou, J.; Tao, C.; Geng, X.; Lin, Q.; Chen, S.; and Zhang, D. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. ArXiv preprint, abs/2308.09583.
Ma, Q.; Zhou, H.; Liu, T.; Yuan, J.; Liu, P.; You, Y.; and Yang, H. 2023. Let's reward step by step: Step-Level reward model as the Navigators for Reasoning. ArXiv preprint, abs/2310.10080.
OpenAI, R. 2023. GPT-4 technical report. arXiv, 230308774.</p>
<p>Prasad, A.; Saha, S.; Zhou, X.; and Bansal, M. 2023. ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 1006610086.</p>
<p>Saparov, A.; and He, H. 2022. Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought. In The Eleventh International Conference on Learning Representations.
Sawada, T.; Paleka, D.; Havrilla, A.; Tadepalli, P.; Vidas, P.; Kranias, A.; Nay, J. J.; Gupta, K.; and Komatsuzaki, A. 2023. Arb: Advanced reasoning benchmark for large language models. ArXiv preprint, abs/2307.13692.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288.
Tyen, G.; Mansoor, H.; Chen, P.; Mak, T.; and Cărbune, V. 2023. LLMs cannot find reasoning errors, but can correct them! ArXiv preprint, abs/2311.08516.
Uesato, J.; Kushman, N.; Kumar, R.; Song, F.; Siegel, N.; Wang, L.; Creswell, A.; Irving, G.; and Higgins, I. 2022. Solving math word problems with process-and outcome-based feedback. ArXiv preprint, abs/2211.14275.
Wang, P.; Li, L.; Shao, Z.; Xu, R.; Dai, D.; Li, Y.; Chen, D.; Wu, Y.; and Sui, Z. 2023. Math-Shepherd: A Label-Free Step-by-Step Verifier for LLMs in Mathematical Reasoning. ArXiv preprint, abs/2312.08935.
Yu, L.; Jiang, W.; Shi, H.; Jincheng, Y.; Liu, Z.; Zhang, Y.; Kwok, J.; Li, Z.; Weller, A.; and Liu, W. 2023. MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. In The Twelfth International Conference on Learning Representations.</p>
<p>Zeng, Z.; Chen, P.; Liu, S.; Jiang, H.; and Jia, J. 2024. MRGSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation. arXiv:2312.17080.
Zhang, M.; Wang, Z.; Yang, Z.; Feng, W.; and Lan, A. 2023. Interpretable Math Word Problem Solution Generation via Step-by-step Planning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 6858-6877.</p>
<h2>A Ablation Studies on the Scoring Scheme</h2>
<p>Validity Scores The validity score can be defined using two different methods as follows:</p>
<p>$$
\begin{aligned}
&amp; S_{\text {validity }}=p_{\text {positive }}+p_{\text {neutral }} \
&amp; S_{\text {validity }}^{\prime}=p_{\text {positive }}
\end{aligned}
$$</p>
<p>We compare the performance of the two scoring schemes in Table 6. The scoring scheme Eq. 1 consistently outperforms Eq.2.</p>
<p>Aggregation Methods We explore three different methods for aggregating step-level scores: min/max, the arithmetic mean (AM), and the geometric mean (GM). We compare the performance in Table 7. For the validity scores, the min operation consistently outperforms both the AM and GM by a large margin. Conversely, for the redundancy scores, the AM and GM slightly outperform the max operation. This discrepancy may be due to the fact that the redundancy score estimations are less accurate compared to validity scores for ReAsonEval, and averaging operations help to reduce noise. Given the trade-offs and considering the potential future advancements in model capabilities, we ultimately choose the min/max operation, as also taken by Golovneva et al. (2022) and Prasad et al. (2023).</p>
<h2>B Details for ReASONEval</h2>
<p>Training Details The ReASONEval model is trained on NVIDIA A100 GPUs (4 GPUs for the 7B models, 16 GPUs for the 34B models) using the supervised fine-tuning (SFT) framework provided by Dai et al. (2023). Specifically, the model is trained in 1 epoch. We set the batch size to 64 and maximum sequence length to 2048. We use a peak learning rate 1e-6 with 8 warmup steps and cosine learning rate decay to 0 . We use AdamW (Loshchilov and Hutter 2019) as our optimizer with $\beta_{1}=0.9, \beta_{2}=0.95$ and weight decay of 0.1 .</p>
<p>Splitting Solutions For users of ReASONEval, we suggest splitting solutions into steps using delimiters such as " $\backslash n$ " or period marks. In our experiments with the Abel and WizardMath series, solutions naturally contained step markers (e.g., "Step 1:"), allowing for direct step-wise splitting. For the LLaMA-PRM800K series, we split solutions using double newlines (" $\backslash n \backslash n$ ").</p>
<h2>C Examples of Solutions Generated by Different LLMs</h2>
<p>We present examples of solutions generated by different LLMs in Figure 5, including LLaMA-PRM800K ${ }^{5}$, Abel and WizardMath.</p>
<h2>D Settings for Evaluators</h2>
<p>The settings for evaluators are as follows: (1) Embeddingbased methods: for ROSCOE we use the embedding layer "roscoe-512-roberta-base". We select Repetition-token matrix for ROSCOE-SA and the Repetition-step matrix for ROSCOE-SS. The threshold is set to 0.025 . (2) Promptingbased methods: we present the prompts in Figure 6. (3) Steplevel evaluators: for the validity scores, the threshold is set to 0.5 , and for redundancy scores, it is set to 0.15 . Both are applied to the solution-level labels and step-level labels. For math-shepherd we set the threshold to 0.5 for all errors.</p>
<h2>E Threshold Selection for False Positive Rate Estimation</h2>
<p>First, we compare the validity scores of reasoning steps from different models in Figure 3. For solutions with the correct answers but wrong steps, the validity scores should behave similarly to solutions with the incorrect answers. So we can use it to estimate the threshold and calculate the false positive rate. A rough estimation for the threshold is between 0.15 and 0.35 . To further ensure a relatively accurate estimation, we check the false positive rate of Abel13B and WizardMath13B by humans, sampling one solution for each problem. Combining above information, the threshold is set to 0.25 .</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Convert the point $(0,3)$ in rectangular coordinates to polar coordinates. Enter your answer in the form $(r, \theta)$, where $r&gt;0$ and $0 \leq \theta&lt;2 \pi$.</h1>
<p>To convert from rectangular to polar coordinates, I need to use the formulas $r=\sqrt{x^{2}+y^{2}}$ and $\theta=\tan ^{-1}(y / x)$, where $\theta$ is measured from the positive $x$-axis.
In this case, $x=0$ and $y=3$, so I can plug them into the formulas and simplify.
For $r$, I get $r=\sqrt{0^{2}+3^{2}}=\sqrt{9}=3$. For $\theta$, I get $\theta=\tan ^{-1}(3 / 0)=\tan ^{-1}(\infty)=\pi / 2$.
However, this value of $\theta$ is not in the range $[0,2 \pi)$, so I need to adjust it by subtracting $2 \pi$.
Therefore, the polar coordinates of the point are $(3, \pi / 2-2 \pi)=(3,-\pi / 2)$.
Answer $(3,5 \pi / 2)$</p>
<h2>LLaMA-PRM800K</h2>
<ol>
<li>The problem asks us to convert the point $(0,3)$ in rectangular coordinates to polar coordinates. We need to express the answer in the form $(r, \theta)$, where $r&gt;0$ and $0 \leq \theta&lt;2 \pi$.</li>
<li>To convert rectangular coordinates to polar coordinates, we need to find the magnitude $r$ and the angle $\theta$.</li>
<li>The magnitude $r$ is given by the formula $r=\sqrt{x^{2}+y^{2}}$, where $x$ and $y$ are the coordinates of the point. In this case, $x=0$ and $y=3$, so $r=\sqrt{0^{2}+3^{2}}=3$.</li>
<li>The angle $\theta$ is the angle between the positive $x$-axis and the line connecting the origin and the point. By observation, we can see that the angle is $\frac{5 \pi}{3}$.</li>
<li>[Final solution] Therefore, the polar coordinates of the point $(0,3)$ are $\left(3, \frac{5 \pi}{3}\right)$.</li>
</ol>
<h2>Abel</h2>
<p>Step 1: Determine the distance from the origin (r)
Since the point $(0,3)$ is on the positive y-axis, the distance from the origin to the point is simply the y-coordinate, which is 3 . So, $r=3$.</p>
<p>Step 2: Determine the angle $\theta$
We need to find the angle formed by the positive x-axis and the line connecting the origin to the point $(0,3)$. Since the point is on the positive y-axis, the angle is 0 degrees (or 0 radians).</p>
<p>Step 3: Write the polar coordinates
Now that we have the distance (r) and the angle ( $\overline{0} 3 \mathrm{~b} 8$ ), we can write the polar coordinates as $(r, \theta)=(3,0)$.</p>
<p>So, the polar coordinates of the point $(0,3)$ in rectangular coordinates are $(3,0)$.
The answer is: $(3,0)$.</p>
<h2>WizardMath</h2>
<p>Figure 5: Examples of solutions generated by different LLMs</p>
<p>|  | Solution-level |  | Step-level |  |
|  | $S_{1}$-AUC | $S_{2}$-AUC | $S_{1}$-AUC | $S_{2}$-AUC |
| REASONEVAL Llam $_{2-7 B}$ | 79.5 | 74.0 | 80.0 | 67.1 |
| REASONEVAL WizardMath-7B-V1.0 | 81.9 | 80.3 | 83.9 | 77.5 |
| REASONEVAL Mistral-7B | 85.1 | 76.5 | 85.7 | 73.5 |
| REASONEVAL Llemma-7B | 84.3 | 82.9 | 90.5 | 85.9 |
| REASONEVAL Abel-7B-002 | 86.2 | 83.5 | 90.5 | 81.3 |
| REASONEVAL WizardMath-7B-V1.1 | 87.5 | 84.3 | 89.5 | 83.1 |
| REASONEVAL Llemma-34B | 90.8 | 89.7 | 92.8 | 89.6 |</p>
<p>Table 6: Performance of different scoring schemes on MR-MATH-invalid. The $S_{1}$ represents the scoring scheme implemented by Eq.1, and $S_{2}$ represents the scoring scheme implemented by Eq.2.</p>
<p>|  | MR-MATH-invalid |  |  | MR-MATH-redundant |  |  |
|  | MIN | AM | GM | MAX | AM | GM |
| --- | --- | --- | --- | --- | --- | --- |
| REASONEVAL Llam $_{2-7 B}$ | 80.0 | 74.0 | 73.5 | 62.8 | 65.5 | 66.6 |
| REASONEVAL WizardMath-7B-V1.0 | 83.9 | 80.3 | 78.3 | 65.6 | 66.2 | 66.2 |
| REASONEVAL Mistral-7B | 85.7 | 76.5 | 80.2 | 63.4 | 64.4 | 64.5 |
| REASONEVAL Llemma-7B | 90.5 | 82.9 | 82.5 | 63.0 | 63.5 | 63.2 |
| REASONEVAL Abel-7B-002 | 90.5 | 83.5 | 85.6 | 63.6 | 64.7 | 65.3 |
| REASONEVAL WizardMath-7B-V1.1 | 89.5 | 84.3 | 87.9 | 64.8 | 65.4 | 65.4 |
| REASONEVAL Llemma-34B | 92.8 | 89.7 | 90.3 | 62.7 | 64.2 | 64.2 |</p>
<p>Table 7: Performance of different aggregation methods on MR-MATH. The "AM" represents the arithmetic mean of the step scores, and "GM" represents the geometric mean of the step scores.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">The Prompt for identifying invalid errors</th>
<th style="text-align: center;">The Prompt for identifying redundant errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question: {question}</td>
<td style="text-align: center;">Question: {question}</td>
</tr>
<tr>
<td style="text-align: center;">Student Solution: {solution}</td>
<td style="text-align: center;">Student Solution: {solution}</td>
</tr>
<tr>
<td style="text-align: center;">Your task involves three parts: <br> 1. <strong>Step-by-step Evaluation:</strong> Go through the student solution carefully and identify key errors and potential misunderstandings that led to the incorrect solution. <br> 2. <strong>Final Judgement:</strong> Provide an overall judgement on the correctness of the student's solution. <br> 3. <strong>First Error Step:</strong> If the solution is incorrect, generate the step number where the first error occurs, otherwise generate N/A here</td>
<td style="text-align: center;">Your task involves three parts: <br> 1. <strong>Step-by-step Evaluation:</strong> Carefully review the student's solution. Identify any neutral steps that, while reasonable, do not offer new insight, advance the solution, or suggest a next step. <br> 2. <strong>Final Judgment:</strong> Indicate whether the solution contains or does not contain any neutral steps. <br> 3. <strong>Neutral Steps Identified</strong>: If neutral steps are present, list their numbers in list format; otherwise, insert N/A.</td>
</tr>
<tr>
<td style="text-align: center;">Here's the format I want: <br> Step-by-step Evaluation: [Provide a step by step examination of the student solution and identify key errors and misunderstandings here.] <br> Final Judgement: [Insert only <strong>correct</strong> or <strong>wrong</strong> here] <br> First Error Step: [Insert either N/A or the step number where the first error occurs]</td>
<td style="text-align: center;">Here's the format I want: <br> Step-by-step Evaluation: [Provide a step-by-step examination of the student solution and identify the neutral steps.] Final Judgment: [Insert only the word <strong>Present</strong> if neutral steps are identified, or <strong>Absent</strong> if not.] Neutral Steps Identified: [Insert either N/A or the step numbers in list format.]</td>
</tr>
<tr>
<td style="text-align: center;">Please follow this format without any additional introductory or concluding statements.</td>
<td style="text-align: center;">Please follow this format without any additional introductory or concluding statements.</td>
</tr>
</tbody>
</table>
<p>Figure 6: Prompts used to identify invalid and redundant errors, modified from the prompt provided by Zeng et al. (2024).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ It represents the LLaMA-2 naive fine-tuned on PRM800K (approximately 6 K solutions that reach the correct final answers), reflecting the language style of PRM800K.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ Uesato et al. (2022) and Lightman et al. (2023) are closesource.
${ }^{3}$ Identifying reasoning errors and ranking reasoning steps are different tasks. As also highlighted by Tyen et al. (2023), even state-of-the-art (SOTA) models like GPT-4 (OpenAI 2023) still struggle to detect errors in reasoning tasks.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
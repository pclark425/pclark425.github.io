<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1108 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1108</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1108</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-10196346</p>
                <p><strong>Paper Title:</strong> <a href="https://web.archive.org/web/20170922090236/https:/hal.inria.fr/hal-01251060/document" target="_blank">Diversity-Driven Selection of Exploration Strategies in Multi-Armed Bandits</a></p>
                <p><strong>Paper Abstract:</strong> We consider a scenario where an agent has multiple available strategies to explore an unknown environment. For each new interaction with the environment, the agent must select which exploration strategy to use. We provide a new strategy-agnostic method that treat the situation as a Multi-Armed Bandits problem where the reward signal is the diversity of effects that each strategy produces. We test the method empirically on a simulated planar robotic arm, and establish that the method is both able discriminate between strategies of dissimilar quality, even when the differences are tenuous, and that the resulting performance is competitive with the best fixed mixture of strategies.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1108.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1108.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADAPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ADAPT (Diversity-driven Adaptive Strategy Selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive strategy-selection algorithm that frames choice among multiple exploration strategies as a Multi-Armed Bandit problem where the reward is the online increase in sensory diversity (coverage); it selects strategies proportionally to recent diversity while retaining a fixed-probability random re-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ADAPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A strategy-selection agent that (1) computes an online diversity reward per strategy based on the incremental coverage in sensory space produced by that strategy's effects, (2) averages recent per-sample diversity over a time window w to score each strategy, (3) selects strategies probabilistically proportional to these diversity scores, and (4) enforces constant re-evaluation by selecting uniformly at random with probability α. It also overestimates initial diversity for each strategy by adding k fictitious non-overlapping points to avoid first-mover bias. ADAPT is implemented as a simple MAB-style controller (no internal model of strategies) that orchestrates lower-level exploration strategies (e.g., RMB, RGB).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Multi-Armed Bandit framed selection using diversity-as-reward; proportional allocation to recent diversity with fixed-probability random exploration (ε-like), i.e., diversity-driven bandit selection rather than UCB/Thompson.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>On each interaction ADAPT: (a) computes div_τ(y, E) = increase in covered sensory area when a new effect y is added (coverage via union of radius-τ balls); (b) for each strategy s_j maintains the recent averaged diversity score div_{τ,w}(s_j,E) over the last w effects produced by that strategy; (c) with probability α chooses a strategy uniformly at random (to ensure re-evaluation of low-scoring strategies), otherwise samples a strategy with probability proportional to its div_{τ,w} score; (d) initially overestimates strategy scores by adding k fictitious non-overlapping points for the first selections (k=1 in experiments) so each strategy gets an initial inflated score for a short period.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated planar 20-joint robotic arm (end-effector reaching task / black-box f : M → S)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown deterministic black-box mapping f: M→S, continuous motor/action space (angles), continuous low-dimensional sensory feedback (2D end-effector position), redundancy (many motor commands map to same effect) that is heterogeneously distributed across sensory space; only final end-effector position is observed per query (no temporal/postural history used). Not stochastic in the experiments; nonstationarity of strategy usefulness arises from exploration process (interdependence) rather than environment noise.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Motor space M: m=20 joint angles (each in [-150°,150°]) → high-dimensional continuous action space; sensory space S: 2D end-effector coordinates (subset of R^2). Each experiment run uses n = 5000 interactions (timesteps); coverage measured with disk radius τ (e.g. τ=0.02 in figures). The mapping is highly redundant near the origin and lower redundancy near workspace boundary, making uniform motor sampling ineffective for boundary coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative and relative metrics reported: ADAPT matches or closely approaches the best fixed-mixture baseline in cases where one strategy is clearly superior (e.g., when RGB with good inverse-model perturbation d=0.05 outperforms RMB, or when RGB with tiny perturbation d=0.001 is much worse than RMB); when strategies are very similar (d≈0.5) ADAPT is more conservative and attains somewhat worse peak coverage than the single best fixed-mixture but still outperforms random. Measured behaviors: in d=0.5 case ADAPT shifts to using goal-babbling ~80% of the time by timestep 4000, with dominance appearing after ~1500 timesteps. Experiments repeated 25 times; per-run interactions = 5000 timesteps. Exact numerical coverage values are presented in figures but not reported as explicit scalar numbers in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines: fixed-ratio mixtures between Random Motor Babbling (RMB) and Random Goal Babbling (RGB) across p∈[0,1], and pure strategies (RMB or RGB). ADAPT's coverage performance is comparable to the best fixed-mixture when one strategy is clearly better, and slightly worse than the best fixed-mixture when strategies behave very similarly; random selection performs worse. No absolute scalar aggregate (e.g., AUC) numbers are provided in-text; performance comparisons are shown graphically.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirical sample scale: 5000 interactions per run; notable adaptation dynamics observed within the first few thousand steps (e.g., ADAPT shows strategy domination changes by ~1500–4000 steps depending on scenario). No explicit sample-complexity bounds or numerical sample-efficiency metrics (like samples-to-reach X% coverage) are provided—only empirical curves over 5000 timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Trade-off handled by (1) proportional allocation to recent diversity scores (exploitation of strategies currently producing more novel coverage) and (2) a fixed-probability random choice α to ensure ongoing exploration and re-evaluation of low-scoring strategies; additionally uses a short time-window w for scoring so that the algorithm reacts quickly to changes (reducing stale exploitation). Initial overestimation (k fictitious points) encourages early sampling of each strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against: (a) Fixed-ratio mixtures of RMB and RGB (varying probability p of choosing RMB), (b) pure RMB and pure RGB baselines, and (c) random selection (implicit). The paper also situates ADAPT relative to literature on adversarial/nonstationary MABs and EXP4 but does not implement UCB/EXP4 baselines in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) Framing strategy selection as a MAB with diversity-as-reward allows an agent-agnostic adaptive controller to discriminate between exploration strategies and select those that increase sensory coverage. 2) ADAPT successfully identifies and favors the better strategy across different RGB perturbation settings (d parameter): when RGB is poor (d=0.001) ADAPT favors RMB early and converges to mixed usage; when RGB is good (d=0.05) ADAPT increasingly favors RGB; when strategies are similar (d=0.5) ADAPT still detects small advantages but is conservative. 3) ADAPT's performance is empirically competitive with the best fixed mixture, showing the method is effective without designing problem-specific rewards. 4) The diversity measure (coverage via union of τ-balls) is simple but sufficient to produce useful selection signals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported limitations: (a) Experiments are on a deliberately simple simulated environment (20-joint planar arm) and a simplistic inverse model—generalization to more complex, noisy, or partially observable environments remains to be demonstrated. (b) The diversity measure is crude and may not capture task-relevant novelty in other domains. (c) Scalability with number of strategies is unclear: more strategies require more sampling or reduce evaluation accuracy. (d) ADAPT is conservative when strategies are similar, sometimes achieving suboptimal mixtures compared to the best fixed choice. (e) No formal theoretical guarantees (e.g., regret bounds) are provided for the specific proportional-selection scheme used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_code_url</strong></td>
                            <td>http://fabien.benureau.com/code/icdlepirob2015.html</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Intrinsically motivated goal exploration for active motor learning in robots: A case study <em>(Rating: 2)</em></li>
                <li>The strategic student approach for lifelong exploration and learning <em>(Rating: 2)</em></li>
                <li>Learning exploration strategies in model-based reinforcement learning <em>(Rating: 1)</em></li>
                <li>The nonstochastic multiarmed bandit problem <em>(Rating: 2)</em></li>
                <li>Online goal babbling for rapid bootstrapping of inverse models in high dimensions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1108",
    "paper_id": "paper-10196346",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "ADAPT",
            "name_full": "ADAPT (Diversity-driven Adaptive Strategy Selection)",
            "brief_description": "An adaptive strategy-selection algorithm that frames choice among multiple exploration strategies as a Multi-Armed Bandit problem where the reward is the online increase in sensory diversity (coverage); it selects strategies proportionally to recent diversity while retaining a fixed-probability random re-evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ADAPT",
            "agent_description": "A strategy-selection agent that (1) computes an online diversity reward per strategy based on the incremental coverage in sensory space produced by that strategy's effects, (2) averages recent per-sample diversity over a time window w to score each strategy, (3) selects strategies probabilistically proportional to these diversity scores, and (4) enforces constant re-evaluation by selecting uniformly at random with probability α. It also overestimates initial diversity for each strategy by adding k fictitious non-overlapping points to avoid first-mover bias. ADAPT is implemented as a simple MAB-style controller (no internal model of strategies) that orchestrates lower-level exploration strategies (e.g., RMB, RGB).",
            "adaptive_design_method": "Multi-Armed Bandit framed selection using diversity-as-reward; proportional allocation to recent diversity with fixed-probability random exploration (ε-like), i.e., diversity-driven bandit selection rather than UCB/Thompson.",
            "adaptation_strategy_description": "On each interaction ADAPT: (a) computes div_τ(y, E) = increase in covered sensory area when a new effect y is added (coverage via union of radius-τ balls); (b) for each strategy s_j maintains the recent averaged diversity score div_{τ,w}(s_j,E) over the last w effects produced by that strategy; (c) with probability α chooses a strategy uniformly at random (to ensure re-evaluation of low-scoring strategies), otherwise samples a strategy with probability proportional to its div_{τ,w} score; (d) initially overestimates strategy scores by adding k fictitious non-overlapping points for the first selections (k=1 in experiments) so each strategy gets an initial inflated score for a short period.",
            "environment_name": "Simulated planar 20-joint robotic arm (end-effector reaching task / black-box f : M → S)",
            "environment_characteristics": "Unknown deterministic black-box mapping f: M→S, continuous motor/action space (angles), continuous low-dimensional sensory feedback (2D end-effector position), redundancy (many motor commands map to same effect) that is heterogeneously distributed across sensory space; only final end-effector position is observed per query (no temporal/postural history used). Not stochastic in the experiments; nonstationarity of strategy usefulness arises from exploration process (interdependence) rather than environment noise.",
            "environment_complexity": "Motor space M: m=20 joint angles (each in [-150°,150°]) → high-dimensional continuous action space; sensory space S: 2D end-effector coordinates (subset of R^2). Each experiment run uses n = 5000 interactions (timesteps); coverage measured with disk radius τ (e.g. τ=0.02 in figures). The mapping is highly redundant near the origin and lower redundancy near workspace boundary, making uniform motor sampling ineffective for boundary coverage.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative and relative metrics reported: ADAPT matches or closely approaches the best fixed-mixture baseline in cases where one strategy is clearly superior (e.g., when RGB with good inverse-model perturbation d=0.05 outperforms RMB, or when RGB with tiny perturbation d=0.001 is much worse than RMB); when strategies are very similar (d≈0.5) ADAPT is more conservative and attains somewhat worse peak coverage than the single best fixed-mixture but still outperforms random. Measured behaviors: in d=0.5 case ADAPT shifts to using goal-babbling ~80% of the time by timestep 4000, with dominance appearing after ~1500 timesteps. Experiments repeated 25 times; per-run interactions = 5000 timesteps. Exact numerical coverage values are presented in figures but not reported as explicit scalar numbers in text.",
            "performance_without_adaptation": "Baselines: fixed-ratio mixtures between Random Motor Babbling (RMB) and Random Goal Babbling (RGB) across p∈[0,1], and pure strategies (RMB or RGB). ADAPT's coverage performance is comparable to the best fixed-mixture when one strategy is clearly better, and slightly worse than the best fixed-mixture when strategies behave very similarly; random selection performs worse. No absolute scalar aggregate (e.g., AUC) numbers are provided in-text; performance comparisons are shown graphically.",
            "sample_efficiency": "Empirical sample scale: 5000 interactions per run; notable adaptation dynamics observed within the first few thousand steps (e.g., ADAPT shows strategy domination changes by ~1500–4000 steps depending on scenario). No explicit sample-complexity bounds or numerical sample-efficiency metrics (like samples-to-reach X% coverage) are provided—only empirical curves over 5000 timesteps.",
            "exploration_exploitation_tradeoff": "Trade-off handled by (1) proportional allocation to recent diversity scores (exploitation of strategies currently producing more novel coverage) and (2) a fixed-probability random choice α to ensure ongoing exploration and re-evaluation of low-scoring strategies; additionally uses a short time-window w for scoring so that the algorithm reacts quickly to changes (reducing stale exploitation). Initial overestimation (k fictitious points) encourages early sampling of each strategy.",
            "comparison_methods": "Compared against: (a) Fixed-ratio mixtures of RMB and RGB (varying probability p of choosing RMB), (b) pure RMB and pure RGB baselines, and (c) random selection (implicit). The paper also situates ADAPT relative to literature on adversarial/nonstationary MABs and EXP4 but does not implement UCB/EXP4 baselines in experiments.",
            "key_results": "1) Framing strategy selection as a MAB with diversity-as-reward allows an agent-agnostic adaptive controller to discriminate between exploration strategies and select those that increase sensory coverage. 2) ADAPT successfully identifies and favors the better strategy across different RGB perturbation settings (d parameter): when RGB is poor (d=0.001) ADAPT favors RMB early and converges to mixed usage; when RGB is good (d=0.05) ADAPT increasingly favors RGB; when strategies are similar (d=0.5) ADAPT still detects small advantages but is conservative. 3) ADAPT's performance is empirically competitive with the best fixed mixture, showing the method is effective without designing problem-specific rewards. 4) The diversity measure (coverage via union of τ-balls) is simple but sufficient to produce useful selection signals.",
            "limitations_or_failures": "Reported limitations: (a) Experiments are on a deliberately simple simulated environment (20-joint planar arm) and a simplistic inverse model—generalization to more complex, noisy, or partially observable environments remains to be demonstrated. (b) The diversity measure is crude and may not capture task-relevant novelty in other domains. (c) Scalability with number of strategies is unclear: more strategies require more sampling or reduce evaluation accuracy. (d) ADAPT is conservative when strategies are similar, sometimes achieving suboptimal mixtures compared to the best fixed choice. (e) No formal theoretical guarantees (e.g., regret bounds) are provided for the specific proportional-selection scheme used.",
            "source_code_url": "http://fabien.benureau.com/code/icdlepirob2015.html",
            "uuid": "e1108.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Intrinsically motivated goal exploration for active motor learning in robots: A case study",
            "rating": 2,
            "sanitized_title": "intrinsically_motivated_goal_exploration_for_active_motor_learning_in_robots_a_case_study"
        },
        {
            "paper_title": "The strategic student approach for lifelong exploration and learning",
            "rating": 2,
            "sanitized_title": "the_strategic_student_approach_for_lifelong_exploration_and_learning"
        },
        {
            "paper_title": "Learning exploration strategies in model-based reinforcement learning",
            "rating": 1,
            "sanitized_title": "learning_exploration_strategies_in_modelbased_reinforcement_learning"
        },
        {
            "paper_title": "The nonstochastic multiarmed bandit problem",
            "rating": 2,
            "sanitized_title": "the_nonstochastic_multiarmed_bandit_problem"
        },
        {
            "paper_title": "Online goal babbling for rapid bootstrapping of inverse models in high dimensions",
            "rating": 2,
            "sanitized_title": "online_goal_babbling_for_rapid_bootstrapping_of_inverse_models_in_high_dimensions"
        }
    ],
    "cost": 0.010065999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Diversity-driven selection of exploration strategies in multi-armed bandits
Aug 2015. 2015</p>
<p>Fabien Benureau 
Pierre-Yves Oudeyer 
Diversity-driven selection of exploration strategies in multi-armed bandits</p>
<p>Proceedings of IEEE Inter-national Conference on Development and Learning and Epigenetic Robotics
IEEE Inter-national Conference on Development and Learning and Epigenetic RoboticsProvidence, United StatesAug 2015. 201510.1109/DE-Submitted on 5 Jan 2016HAL Id: hal-01251060 https://hal.inria.fr/hal-01251060
We consider a scenario where an agent has multiple available strategies to explore an unknown environment. For each new interaction with the environment, the agent must select which exploration strategy to use. We provide a new strategy-agnostic method that treat the situation as a Multi-Armed Bandits problem where the reward signal is the diversity of effects that each strategy produces. We test the method empirically on a simulated planar robotic arm, and establish that the method is both able discriminate between strategies of dissimilar quality, even when the differences are tenuous, and that the resulting performance is competitive with the best fixed mixture of strategies.</p>
<p>I. MOTIVATION</p>
<p>We are given a black-box that takes inputs and produces outputs. We know the values the inputs can take, but we do not know which inputs produce which outputs. We do not even know which outputs can be produced. We are given the opportunity to sample the black-box a limited number of times. In this context, we propose to investigate the following question: how much diversity of outputs can be produced with the limited access we have?</p>
<p>This question defines an exploration problem. Here, the objective is to discover what outputs the black-box is capable to deliver. To answer such a problem is to provide an exploration strategy, i.e. a method that selects which inputs to experiment with on the black box, in order to produce a diversity of outputs.</p>
<p>In this paper, we interest ourselves with a scenario where we have multiple exploration strategies available, whose internal operational details are not specified, and we must select, for each available interaction with the environment-i.e., the black box-, which exploration strategy to use to generate the inputs to execute.</p>
<p>Stated differently, we have several exploration black-boxes and one environmental black-box, and we want to know which exploration black-box to use on the environmental black-box at each interaction, so as to maximize the diversity of the effects produced by the environmental black-box.</p>
<p>Two salient points are present in our problem statement. We consider exploration problems rather than learning ones. And we establish an objective of diversity, not one of control or of prediction or of fitness or of reward optimization. We briefly motivate these two stances in the following sections.</p>
<p>A. Diversity and Exploration</p>
<p>Behavioural diversity is a factor of individual robustness when facing an evolving environment. It ensures that the next time the environment changes some of the behaviours will remain relevant. At the population level, behavioural diversity provides variability even in the absence of genetic diversity.</p>
<p>This point was recently heeded by the evolutionary robotics community, which was facing, amongst others, two specific challenges: early convergence, when the evolutionary process would get trapped in local minima because of a deceptive fitness function, and bootstrapping problems where the first generation fails to produce rewarding behaviour, hence stalling the evolutionary process. The then solution, staging the fitness function [14,20,37]-a method similar to reward shaping in reinforcement learning [10,26]-, was deemed impractical because requiring problem-specific fitness functions.</p>
<p>The solution came from replacing or modifying the fitness function to encourage behavioural diversity in the population of candidate solutions [8,9,15,23,27,36], a method proposed first in the classical evolutionary algorithm domain [13,33].</p>
<p>In infants, actively fostering diversity in our interaction with the environment through exploratory behaviour is pivotal: it allows to discover and investigate new phenomena and affordances before they are detected as such. For Eleanor Gibson [12], babies are not endowed with the abilities to perceive affordances, but must spend their first years discovering affordances in their environment. For instance, children do not already know that mirror are special objects proposing unique and salient interactions. Instead they must discover their affordance through an unrealed exploration of their environment. This point is important: studying exploratory behaviours on their own-rather than in the context of a learning problemcan shed light on how problems are discovered in the environment in the first place, before they are acknowledged as learning activities.</p>
<p>One could argue that, after noticing the mirror particular nature, the exploratory behaviour of the child in front of the mirror is in fact highly structured, and follows the child-asa-scientist paradigm [16,17,35]. But as Cook points out, more ecological explanations are also available: "selective exploration of confounded evidence is advantageous even if children explore randomly (with no understanding of how to isolate variables)" [7, p. 352]. Therefore the mere production of behavioural diversity is a useful tool in broad and specific exploration.</p>
<p>One other reason to investigate exploration independently from learning: exploration can happen without learning. For instance, a robot randomly producing movements does not exhibit learning, yet exhibit an exploratory behaviour. Similarly, a robot following mindlessly the left wall of a maze explores the maze, and does it successfully to boot. And many vacuum robots available today explore their environment without learning them. In all those examples, exploration is present because the behaviour creates access to (new) information about the environment. That the information is not remembered or exploited is not an exploration issue, it is a learning one.</p>
<p>B. More Than One Exploration Strategy</p>
<p>Different environments lend themselves to different exploration strategies. In simple environments, doing random actions will be as effective as any other strategy. In more complex contexts, more elaborate strategies are needed.</p>
<p>The field of computational intrinsic motivation has developed an abundance of different motivational drives such as novelty, surprise, prediction error, predictive information or competence progress (see [2,30] for reviews). Each of these drives express preferences over what is interesting in the world, and define specific exploration strategies.</p>
<p>Moreover, exploration, for a robot, may be possible through different means: asking for social guidance, observing a peer, or opting for self-exploration. Each of those venues may not be always available, and some, e.g. social guidance, may only be available for infrequent use.</p>
<p>This suggests that robots should be endowed with different exploration strategies to tackle complex environments. Furthermore, we argue that one should resist hiding the choice these strategies represent under a larger, monolithic, opaque exploration strategy. Indeed such a strategy would need to handle simultaneously how, what and possibly when to explore, three aspects which may need to be specifically mediated by other components of behaviour.</p>
<p>Therefore, agents having multiple available exploration strategies are justified. In this article, we propose a strategyagnostic method to select which strategy to choose in function of the empirical behaviour of each of them.</p>
<p>II. PROBLEM</p>
<p>A. Environment</p>
<p>An environment, is formally defined as a function f from M to S. M is the motor space, a bounded hyperrectangle of R m , and represents a parameterization of the movements the robot can execute. S is the sensory space; it is a subset of R s . Effects and goals 1 (desired effects) are elements of S.</p>
<p>A task is defined as a pair (f, n) with f : M → S the environment and n the maximum number of samples of f allowed, i.e. the number of inputs the exploration strategy can try on the environment.</p>
<p>B. Exploration</p>
<p>An exploration strategy evaluates the function f , n times, providing a sequence of elements of S, x 0 , x 1 , ..., x n−1 . Each x i is evaluated as y i = f (x i ), and y i is observed by the exploration strategy before x i+1 is chosen.</p>
<p>In order to evaluate the exploration strategy, we use an exploration measure C, that takes the behavioural trace of the agent as input, i.e., the actions executed and effects produced:
{(x i , y i )} 0≤i&lt;n .
A common objective of the experimenter is to evaluate if the agent has obtained knowledge of all the possibilities of the environment. A good proxy for this is to evaluate the set of effects the agent was able to produce during the exploration. In other words, how well the image of f , f (M )-the reachable space-was sampled.</p>
<p>Since we do not assume that the agents have knowledge of the exploration problem they are examined under, or that they have knowledge of the exploration measures that are used to evaluate their behaviour, and since agents may explore the environment for their own purposes, and self-evaluate their behaviour according to their own metrics, the choice of an exploration measure is necessarily arbitrary. This consideration is not present for instance in reinforcement learning, where the cumulative reward defines an objective motivation for the agent, and an objective evaluation for the experimenter. In an exploration context, it is the responsibility of the experimenter to justify the interest and relevance of the selected exploration measure.</p>
<p>In this work, we select a diversity measure to evaluate the exploration. The importance of diversity for the development of humans and animals was argued above. And behavioural diversity has proven itself empirically in the field of evolutionary robotics. Absent an objective environmental reward for the agent's behaviour, and absent an assumption that the agent possesses specific learning abilities, encouraging diversity in behaviour is relevant in multiple ways. First, it does not put tight constraints on the form the behaviour of the agent. Second, it prepares the agent for future problems: an agent with a diverse behavioural repertoire is likely to also have high amounts of diverse knowledge and skills.</p>
<p>The diversity measure concerns itself only with the sensory part of the behaviour: {y i } 0≤i<n . It is defined as a coverage measure. Given τ > 0, the diversity of the exploration C({y i } 0≤i&lt;n ) is defined as the volume (more precisely the Lebesgue measure) of the union of the n hyperballs of R s with y 0 , y 1 , ..., y n−1 as a centres, and radius τ .
C τ ({y i } 0≤i&lt;n = n i=0 B(y i , τ )
with B(y i , τ ) the hyperball of radius τ and centre y i .</p>
<p>In evolutionary robotics, other measures of diversity such as sparseness [23] or entropy [8] have been used.</p>
<p>III. ILLUSTRATING THE PROBLEM</p>
<p>In this section, we illustrate the problem on a specific example, that will serve as the experimental setup for the method, exposed in the next section.</p>
<p>We consider an idealized robotic arm on a two-dimensional plane, made up of an open chain of 20-joints linked by segments of 1/20th of a meter each, so that the total length of the arm is one meter. The angles of the joints are restricted to values between -150 and 150 degrees. The angles of the joints are the inputs: they uniquely define the posture of the arm, and therefore, the position of the end-effector, which corresponds to the environmental feedback. Let's remark that only the final position of the end-effector, corresponding to the angle inputed in absolute value, is returned by the environment (i.e. there is no posture dependence between two consecutive samples).</p>
<p>A. A Tale of Two Exploration Strategies</p>
<p>Despite the simplicity of the arm setup, it is not a trivial problem, and this is exacerbated since we cannot assume any knowledge about the arm.</p>
<p>The most simple strategy, random motor babbling (RMB), samples the motor space randomly. Here the RMB strategy ( Figure 1) is inefficient: indeed, the redundancy 2 of the arm is heterogeneously distributed in the sensory space (the endeffector position space). In particular, the redundancy is high near the origin, and order of magnitude lower on the edge of the reachable space. Because the RMB strategy is precisely an 2 Considering a subset of the sensory space B, the redundancy of B is defined as the volume (more generally, the Lebesgue measure) of the set of motor commands whose effect belong to B, i.e. {x|f (x) ∈ B} with f the environment feedback function (see section II-A)). [24] provides an algorithm to quantify the redundancy of rigid, multijoint robotic arms, but the computation is only tractable for a small number of joints. estimator of the heterogenity of the redundancy, it rarely ever explores the edges of the reachable space.</p>
<p>A goal babbling strategy is (usually) better suited for exploring the arm setup. We will consider a random goal babbling (RGB) strategy [4,32], that picks a goal at random in the square [−1, 1] × [−1, 1], and translate it to a tentative motor command that tries to put the end-effector as close as possible of the goal.</p>
<p>To translate a goal into a motor command, we need an inverse model. As in this paper, we are only interested in relative performance: we choose a simple inverse model. Our inverse model, when given a goal, finds the nearest effect available in the observed data, retrieves the motor command that produced it, applies a small perturbation to it, and returns the perturbed command for execution of the exploration strategy. The magnitude of the perturbation is parametrized by the perturbation coefficient d: the perturbation is randomly chosen between ±d times the legal joint range. For instance (here 300 • ), if d = 0.05, the motor command is perturbed by a random value chosen in ±15 • on each joint.</p>
<p>Choosing d appropriately is not trivial. In Figure 2, the RGB exploration of three different values of d is shown. The d = 0.05 case results in a good exploration. But d = 0.001 creates degenerated clusters: the perturbation is too low to create enough sensory variability. A contrario, d = 0.5 creates too much variability, and is only marginally better than the RMB exploration of Figure 1.</p>
<p>Let's imagine now that we are given two strategies to explore the arm setup. One is the RMB strategy, and the other is a RGB strategy, with unknown d. We don't assume any knowledge of either strategy. How can we dynamically decide, for each interaction with the black-box, which exploration strategy to choose to maximize the coverage of the exploration over the reachable space?</p>
<p>B. Inverse Model</p>
<p>Given a goal, the inverse model finds the nearest neighbour in the observed effects and applies a small perturbation on its corresponding motor command.</p>
<p>Formally, M is an closed hyperrectangle of R m , and as such it is the Cartesian product of m closed intervals:
M = m−1 m=0 [a i , b i ]
Given an motor command x = {x 0 , x 1 , ..., x m−1 } in M , a perturbation of x is defined by:
PERTURB d (x) = {random(max(a j , x j − d(b j − a j )), min(x j + d(b j − a j ), b j ))} 0≤j&lt;m
with the function random(a, b) drawing a random value in the interval [a, b] according to a uniform distribution. d is the perturbation parameter, and the only parameter of the inverse model, that we can now express in Algorithm 1.</p>
<p>IV. METHOD</p>
<p>A. Effect Diversity</p>
<p>Choosing which strategy to employ at each step of the exploration faces three main challenges: 1) Interdependence: an exploration strategy effectiveness may depend on another strategy; goal babbling relies
Algorithm 1: INVERSE d (y g , E) Data: • d ∈ [0, 1], a perturbation ratio. • E = {(x t , y t )} 0≤t&lt;N ∈ (M × S) N , past observations.
• y g ∈ S, a goal. Result:
• x ∈ M a motor command. Find (x i , y i ) in E so that y i is the the nearest neighbour of y g in {y t } 0≤t&lt;N . x = PERTURB d (x i )
on motor babbling to bootstrap the exploration. Given the inverse model currently used, this is even more true, as goal babbling's performance depends heavily the sensorimotor attractors in which it expands, and thus on the location of the observations produced early in exploration by motor babbling. 2) Dynamical Value: the usefulness of a strategy may change rapidly. Motor babbling is useful in the beginning of the exploration, but its usefulness drops quickly. 3) Agnosticity: since an exploration strategy might be arbitrarily complex, and possibly involve, in turn, other exploration strategy, an adaptive strategy should not rely on knowledge of the internal workings of the strategies amongst which it must choose.</p>
<p>Interdependence does not have to be handled directly, but it implies that even strategies that did poorly in the past must be re-evaluated regularly as the exploration progresses. The dynamical nature of the contribution of each strategy means that performance data becomes obsolete quickly: evaluations should be done over short-term time windows. Agnosticity implies the contributions of the strategies have to be evaluated only from the observations the strategies produce. We introduce a measure that matches those constraints now.</p>
<p>A strategy that produces effects over areas that have already been explored is of little use for exploration. We introduce an online diversity measure that evaluates, each time a strategy is used, how much diversity is created, with regards to already observed effects.</p>
<p>In order to do that, we rely on the diversity measure introduced in section II, based on the union of disks centred on observed effects. Although we reuse the coverage measure here out of convenience, the two measures do not have to have any relationship with one another. The measure is adapted to evaluate a single effect: the diversity of a new observed effect is the increase in diversity, i.e., the increase in the covered area.</p>
<p>Definition 1: Given a set of effects E = {y 0 , y 1 , ..., y n−1 }, and a coverage threshold τ in R + , the diversity of a new effect y n relative to E is defined as:
div τ (y n , E) = C τ (E ∪ {y n }) − C τ (E)
The diversity of a strategy, in turn, is the averaged diversity of the effects it produced, over a given time window.</p>
<p>Definition 2: Given a set of strategies s 0 , s 1 , ..., s q−1 , and a set of observed effects E = {y 0 , y 1 , ..., y n }, we have for a given strategy s j a subsequence y j 0 , y j 1 , ..., y j nj of the effects produced by motor commands emanating from the strategy. Given a time window w in N + , we define the diversity of strategy s j as: n j ).
div τ,w (s j , E) =        1 w w i=0 div τ (y j nj −i , E) if n j &gt; 0 0 otherwise with w = min(w,</p>
<p>B. Multi-Armed Bandits</p>
<p>As expressed above, the problem we tackle shares similarities with the Multi-Armed Bandit problem (MAB) [31]. The exploration strategies are the bandits, amongst which the agent must choose to create diversity. However, the feedback received is a sensory feedback from the environment, which cannot be used as is in the MAB setting.</p>
<p>Using the diversity measure of a strategy introduced above, we can now evaluate the contribution of each strategy to the exploration. We now have a classic MAB problem: we choose between a finite number of different strategies with different diversity scores, and after choosing one we receive a feedback signal from the chosen strategy from which we compute an updated score.</p>
<p>The classic MAB problem considers only bandits that are independent from one another (choosing one does not affect the value of the others), and stationary (the distribution of rewards of the bandit does not change). A variation of the problem, the adversarial (also called non-stochastic or nonstationary) MAB, removes the stationary and interdependence assumptions: an adversary is free to choose arbitrary rewards for each bandit at each timestep.</p>
<p>In practice, a significant portion of the published literature on the adversarial MAB problem only removes the stationary assumption. In other words, the problem takes place in the oblivious opponent model: the actions of the adversary, i.e. the rewards for each bandit at each timestep, are decided before the game starts. This is the case in [38] and [1], who investigate rewards that can arbitrarily change. [11] presents abruptly changing environments, where all bandits' reward distributions change at specified timesteps. [5,[156][157][158][159][160][161][162][163][164][165][166][167][168][169] provides a treatment of the nonoblivious case.</p>
<p>Recently, [25] introduced the Strategic Student Problem that tries to capture the issues involved when learning multiple tasks at the same time. A student has to learn multiple topics (maths, chemistry, history, etc.), and has limited resources (time) to do so. How should he allocate his study time between topics in order to maximize its mean grade at the end of the semester? A possibility is to consider the problem as a MAB problem where the bandits are learning tasks. Interestingly, the works of [4] on goal babbling can be understood in this perspective: each region of the goal space is a different topic, whose improvement is empirically measured through competence progress during learning, and the exploration strategy must decide how to distribute its action given those learning feedback signals.</p>
<p>The strategic student problem also considers another related problem: a student has one topic to learn, but several possible learning strategies. Which one should he choose? Is a mixture of several strategies better than employing the best one all the time? This is the problem of learning how to learn [34]. [3] explored such a problem and showed that a dynamically selected mixture of three active learning strategies outperformed any pure strategy. [21] demonstrated that empirically evaluating and selecting among different small state space representations specific to a task during learning was effective and avoided a large task space when learning was unfeasible. The work of [29] investigates robots dynamically choosing between asking a teacher for a demonstration or doing self-exploration on their own. [19] proposes a method where a robot can self-assess, and has a frustration drive. When frustrated, the robot can opt to choose social help to improve its performance. In the context of reinforcement learning, [18] develops an algorithm that can evaluate dynamically which exploration strategy brings the most rewards. These exploration strategies are driven by extrinsic and intrinsic motivations: maximizing rewards, reducing variance, seeking novelty, seeking unexplored states (a binary novelty), and seeking or avoiding particular features of the state representation. [6] uses the framework of the Strategic Student Problem to create a tutoring system that actively personalizes the sequence of activities to each student, by tracking their performance and identifying which exercises and modalities make the student progress faster. The works of [3], [29] and [18] are singular because they combine deciding how to learn, and deciding what to learn, using a hierarchical approach. The learning strategy is selected first (how), and then it chooses what input to sample (what).</p>
<p>Learning performance typically exhibits diminishing returns, and [25] shows that, in the strict case, this allows to express the mean performance across tasks as a submodular function [22]. [28] has proven that with non-decreasing submodular function, the greedy strategy is guaranteed to be no worse than 1 − 1 e ≈ 0.63 times the optimal solution for maximizing the function. Of course, not all set of learning tasks exhibit a submodular structure. Still, it suggests that a good-enough performance might be obtained through simpleenough algorithm in practice. [25] and [18] advocate the use of the EXP4 algorithm [1] rather than a greedy algorithm, as a more robust approach.</p>
<p>Compared to these works, our approach distinguishes itself on two fronts: first, we are selecting exploration strategies to improve exploration, rather than exploration or learning strategies to improve learning. The resulting strategy is another exploration strategy. Second, we are using diversity to transform the feature vector of the sensory feedback into a scalar that can be adequately interpreted as a reward. To our knowledge, this is the first work to do that in the context of a Multi-Armed Bandit problem.</p>
<p>C. Adaptive Strategy</p>
<p>The ADAPT algorithm chooses strategies proportionally to their diversity. To allow for constant re-evaluation of the strategies, even those with low diversity, the algorithm chooses a strategy at random α percent of the time, with α &gt; 0. Algorithm 2 formally describes this.</p>
<p>Additionally, in order to foster initial experimentation with each strategy, the diversity measure is overestimated at the beginning of the exploration. For a given strategy s j , instead of considering the set E j = {y 0 , y 1 , ..., y nj }, we consider the set E = {y −k , y −k+1 , ..., y 0 , ..., y n }, with k in N + . The set {y −k , y −k+1 , ..., y −1 } is composed of fictitious points only available to the selecting strategy, that generate hyperballs that do not overlap with the observed effects. That way, the diversity of the strategy is overestimated during the w + k first times it is selected. This also avoids having the first strategy selected unfairly preferred because it created the first observation, thus receiving the diversity of a full hyperball volume. We will use k = 1 in all strategies.</p>
<p>V. RESULTS Figure 3, the results of the strategy are displayed. In all three learner configurations, the ADAPT algorithm identifies and uses the correct strategies. When d = 0.001, the goal babbling strategy is inefficient in the beginning, and motor babbling is overwhelmingly used. Motor babbling diversity declines continually during the exploration, and in the later stage, is comparable to goal babbling. As a result, after 4000 timesteps, the two strategies are used roughly equally.</p>
<p>When d = 0.05, goal babbling and motor babbling produce the same diversity at the beginning, but goal babbling declines more slowly than motor babbling. As a result, goal babbling is used more and more as the exploration progresses, as it should be.</p>
<p>When d = 0.5, motor and goal babbling behave similarlyif d had been equal to 1.0, they would be the same strategy. During the early phase of the exploration, the ADAPT algorithm does not distinguish between the two strategies. But in the later phase, goal babbling is able to provide an edge, however small, that is detectable by the ADAPT algorithm. Goal babbling usage dominates after 1500 timesteps, and is used 80% of the time after 4000 timesteps. While the algorithm works qualitatively, it remains to be seen if this translates quantitatively. Figure 4 compares the error of the ADAPT algorithms with fixed-ratio strategies, where the motor babbling strategies is chosen with probability p, and the goal babbling with probability 1 − p.</p>
<p>When goal babbling is much worse than motor babbling (d = 0.001) or when it is much better (d = 0.05), the ADAPT algorithm manages performance on par with the best fixed mixture of strategies. When goal and motor strategy behave similarly, the adapt strategy is more conservative than the best case. This stems from the early stage of the exploration, when the motor babbling and goal babbling strategies are both effective, and hence both significantly used.</p>
<p>VI. DISCUSSION</p>
<p>The ADAPT algorithm we proposed, and the corresponding adaptive strategy we implemented demonstrate how a choice of multiple exploration strategies can be exploited to explore an unknown environment. The diversity measure is, in many ways, rather crude, but it shows that discriminating between exploration strategies is definitely possible, and, advantageous. The general idea behind this work is not particularly new.</p>
<p>Its application to a diversity measure is, however. In fact, since exploration, as explained, does not make the typical assumption about the agents capabilities-it does not assume the agent is capable (or willing) to make predictions, nor to exert (or demonstrate) control over the environment-the method we presented extends the applicability of the Multi-Armed Bandits to situations where learning or reward signals are not present. And it does so without requiring to design a problem-specific reward function.</p>
<p>Our work could be criticized for the simplicity of the environment that is used, and that's a valid point. Yet, we chose to present this method on a simple setup here to avoid the reader having to suspend his intuition, or suspect interference from the robot complex dynamics into the results. The extreme simplicity of our inverse model is also a deliberate choice in this regard. We are currently preparing experiments on a real robot actuated through dynamical motor primitives to reproduce the results in a more complex scenario.</p>
<p>From the experiments we conducted, it is unclear how the ADAPT algorithm will scale with the number of strategies. As more strategies are available, either more time will have to be devoted to exploratory sampling of bad strategies, or strategies will be less accurately evaluated overall. This is the classic exploration/exploitation trade-off.</p>
<p>ACKNOWLEDGMENTS</p>
<p>This work was partially funded by the ANR MACSi and the ERC Starting Grant EXPLORERS 240 007. Computing  The ADAPT algorithm performs well when strategies behave distinctly, and better than random with similar strategies. Each graph displays the performances of fixed mixtures of the two strategies, with the performance of the adaptive strategy added as a dotted line (its standard deviations in displayed in light colour as well). Experiments were repeated 25 times. Note that not all the y-axis of the graphs begin at zero. hours for running simulations were graciously provided by the MCIA Avakas cluster.</p>
<p>SOURCE CODE</p>
<p>The complete source code behind the experiments and the figures is open source and available at: fabien.benureau.com/code/icdlepirob2015.html</p>
<p>Fig. 1 .
1Random motor babbling is not an efficient exploration strategy with a high number of joints.</p>
<p>Fig. 2 .
2Random goal babbling can a very efficient strategy-if the inverse model is well chosen. Each exploration is done over 5000 timesteps. In each case, the last five postures of the exploration are displayed.</p>
<p>Algorithm 2 :
2ADAPT(w, τ ) Input:• s 0 , s 1 , ..., s q−1 , strategies.• E = {y 0 , y 1 , ..., y n−1 }, a set of effects. • τ , coverage threshold. • w, time window.• α, ratio of random choice. Result:• s j , chosen strategy if RANDOM() &lt; α then choose a random strategy. else choose a strategy s j proportionally to its diversity diversity τ,w (s j , E).</p>
<p>Fig. 3 .
3T he ADAPT algorithm correctly selects the best strategy in all three contexts. For each learner, three graphs are shown: the spread graph with the coverage area (τ = 0.02), the diversity graph giving the diversity measure of each strategy in function of the timesteps, and the usage graph, showing how the strategies are effectively used. For the usage graph, the data at time t shows the percentage of use averaged over the surrounding 100 timesteps (50 before, 50 after).</p>
<p>Fig. 4 .
4Fig. 4.
We assume that S is known by the exploration strategy, but nothing prevents S to be set equal to R s</p>
<p>The nonstochastic multiarmed bandit problem. Peter Auer, Nicol Cesa-Bianchi, Yoav Freund, Robert E Schapire, SIAM J. Comput. 321Peter Auer, Nicol Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. SIAM J. Comput., 32(1):48-77, jan 2002.</p>
<p>Gianluca Baldassarre, Marco Mirolli, Intrinsically Motivated Learning in Natural and Artificial Systems. Springer Science + Business MediaGianluca Baldassarre and Marco Mirolli, editors. Intrinsically Motivated Learning in Natural and Artificial Systems. Springer Science + Business Media, 2013.</p>
<p>Online choice of active learning algorithms. Yoram Baram, Ran El-Yaniv, Kobi Luz, J. Mach. Learn. Res. 5Yoram Baram, Ran El-Yaniv, and Kobi Luz. Online choice of active learning algorithms. J. Mach. Learn. Res., 5:255-291, dec 2004.</p>
<p>Intrinsically motivated goal exploration for active motor learning in robots: A case study. Adrien Baranes, Pierre-Yves Oudeyer, IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE. Adrien Baranes and Pierre-Yves Oudeyer. Intrinsically motivated goal exploration for active motor learning in robots: A case study. In 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, oct 2010.</p>
<p>Nicolo Cesa, - Bianchi, Gabor Lugosi, Prediction, Learning, and Games. Cambridge University PressNicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.</p>
<p>Multiarmed bandits for intelligent tutoring systems. Benjamin Clement, Didier Roy, Pierre-Yves Oudeyer, Manuel Lopes, submittedBenjamin Clement, Didier Roy, Pierre-Yves Oudeyer, and Manuel Lopes. Multi- armed bandits for intelligent tutoring systems. (submitted), 2015.</p>
<p>Where science starts: Spontaneous experiments in preschoolers' exploratory play. Claire Cook, Noah D Goodman, Laura E Schulz, Cognition. 1203Claire Cook, Noah D. Goodman, and Laura E. Schulz. Where science starts: Spontaneous experiments in preschoolers' exploratory play. Cognition, 120(3):341- 349, sep 2011.</p>
<p>Open-ended evolutionary robotics: An information theoretic approach. Pierre Delarboulas, Marc Schoenauer, Michle Sebag, Parallel Problem Solving from Nature, PPSN XI. Springer Science + Business MediaPierre Delarboulas, Marc Schoenauer, and Michle Sebag. Open-ended evolutionary robotics: An information theoretic approach. In Parallel Problem Solving from Nature, PPSN XI, pages 334-343. Springer Science + Business Media, 2010.</p>
<p>Beyond black-box optimization: a review of selective pressures for evolutionary robotics. Stephane Doncieux, Jean-Baptiste Mouret, Evolutionary Intelligence. 72Stephane Doncieux and Jean-Baptiste Mouret. Beyond black-box optimization: a review of selective pressures for evolutionary robotics. Evolutionary Intelligence, 7(2):71-93, jul 2014.</p>
<p>Robot shaping: developing autonomous agents through learning. Marco Dorigo, Marco Colombetti, Artificial Intelligence. 712Marco Dorigo and Marco Colombetti. Robot shaping: developing autonomous agents through learning. Artificial Intelligence, 71(2):321-370, dec 1994.</p>
<p>On upper-confidence bound policies for nonstationary bandit problems. 24 pages. Aurlien Garivier, Eric Moulines, Aurlien Garivier and Eric Moulines. On upper-confidence bound policies for non- stationary bandit problems. 24 pages, May 2008.</p>
<p>Exploratory behavior in the development of perceiving, acting, and the acquiring of knowledge. Eleanor J Gibson, Annu. Rev. Psychol. 391Eleanor J. Gibson. Exploratory behavior in the development of perceiving, acting, and the acquiring of knowledge. Annu. Rev. Psychol., 39(1):1-42, jan 1988.</p>
<p>Simple genetic algorithms and the minimal, deceptive problem. D E Goldberg, Morgan KaufmannD. E. Goldberg. Simple genetic algorithms and the minimal, deceptive problem, pages 74-88. Morgan Kaufmann, 1987.</p>
<p>Incremental evolution of complex general behavior. Faustino Gomez, R Miikkulainen, Adaptive Behavior. 53-4Faustino Gomez and R. Miikkulainen. Incremental evolution of complex general behavior. Adaptive Behavior, 5(3-4):317-342, jan 1997.</p>
<p>Sustaining diversity using behavioral information distance. Faustino J Gomez, Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation, GECCO '09. the 11th Annual Conference on Genetic and Evolutionary Computation, GECCO '09New York, NY, USAACMFaustino J. Gomez. Sustaining diversity using behavioral information distance. In Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation, GECCO '09, pages 113-120, New York, NY, USA, 2009. ACM.</p>
<p>Scientific thinking in young children: Theoretical advances, empirical research, and policy implications. A Gopnik, Science. 3376102A. Gopnik. Scientific thinking in young children: Theoretical advances, empirical research, and policy implications. Science, 337(6102):1623-1627, sep 2012.</p>
<p>Stretching to learn: Ambiguous evidence and variability in preschoolers exploratory play. Hyowon Gweon, L Schulz, Hyowon Gweon and L. Schulz. Stretching to learn: Ambiguous evidence and variability in preschoolers exploratory play. 2008.</p>
<p>Learning exploration strategies in model-based reinforcement learning. Todd Hester, Manuel Lopes, Peter Stone, Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS '13. the 2013 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS '13Richland, SCInternational Foundation for Autonomous Agents and Multiagent SystemsTodd Hester, Manuel Lopes, and Peter Stone. Learning exploration strategies in model-based reinforcement learning. In Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS '13, pages 1069-1076, Richland, SC, 2013. International Foundation for Autonomous Agents and Multiagent Systems.</p>
<p>From self-assessment to frustration, a small step toward autonomy in robotic navigation. Adrien Jauffret, Nicolas Cuperlier, Philippe Tarroux, Philippe Gaussier, Front. Neurorobot. 7Adrien Jauffret, Nicolas Cuperlier, Philippe Tarroux, and Philippe Gaussier. From self-assessment to frustration, a small step toward autonomy in robotic navigation. Front. Neurorobot., 7, 2013.</p>
<p>Evolution and development of neural controllers for locomotion, gradient-following, and obstacle-avoidance in artificial insects. J Kodjabachian, J A Meyer, IEEE Trans. Neural Netw. 95J. Kodjabachian and J. A. Meyer. Evolution and development of neural controllers for locomotion, gradient-following, and obstacle-avoidance in artificial insects. IEEE Trans. Neural Netw., 9(5):796-812, 1998.</p>
<p>Sensorimotor abstraction selection for efficient, autonomous robot skill acquisition. George Konidaris, Andrew Barto, 7th IEEE International Conference on Development and Learning. Institute of Electrical &amp; Electronics Engineers. IEEEGeorge Konidaris and Andrew Barto. Sensorimotor abstraction selection for efficient, autonomous robot skill acquisition. In 2008 7th IEEE International Conference on Development and Learning. Institute of Electrical &amp; Electronics Engineers (IEEE), aug 2008.</p>
<p>Submodular function maximization. Andreas Krause, Daniel Golovin, Practical Approaches to Hard Problems. Lucas Bordeaux, Youssef Hamadi, Pushmeet Kohli, and Robert MateescuCambridge University PressAndreas Krause and Daniel Golovin. Submodular function maximization. In Lucas Bordeaux, Youssef Hamadi, Pushmeet Kohli, and Robert Mateescu, editors, Practical Approaches to Hard Problems, pages 71-104. Cambridge University Press, 2014.</p>
<p>Exploiting open-endedness to solve problems through the search for novelty. Joel Lehman, Kenneth O Stanley, Proc. of the Eleventh Intl. Conf. on Artificial Life (ALIFE XI). of the Eleventh Intl. Conf. on Artificial Life (ALIFE XI)Cambridge, MAMIT PressJoel Lehman and Kenneth O. Stanley. Exploiting open-endedness to solve problems through the search for novelty. In Proc. of the Eleventh Intl. Conf. on Artificial Life (ALIFE XI), Cambridge, MA, 2008. MIT Press.</p>
<p>On the quantification of robot redundancy. J Lenarcic, Proc. ICRA 1999. IEEE. ICRA 1999. IEEEJ. Lenarcic. On the quantification of robot redundancy. In Proc. ICRA 1999. IEEE, 1999.</p>
<p>The strategic student approach for lifelong exploration and learning. Manuel Lopes, Pierre-Yves Oudeyer, Proc. ICDL-Epirob. ICDL-EpirobIEEEManuel Lopes and Pierre-Yves Oudeyer. The strategic student approach for life- long exploration and learning. In Proc. ICDL-Epirob 2012. IEEE, nov 2012.</p>
<p>Reward functions for accelerated learning. J Maja, Mataric, Machine Learning: Proceedings of the Eleventh international conference. Maja J Mataric. Reward functions for accelerated learning. In Machine Learning: Proceedings of the Eleventh international conference, pages 181-189, 1994.</p>
<p>Encouraging behavioral diversity in evolutionary robotics: An empirical study. J B Mouret, S Doncieux, Evolutionary Computation. 201J. B. Mouret and S. Doncieux. Encouraging behavioral diversity in evolutionary robotics: An empirical study. Evolutionary Computation, 20(1):91-133, mar 2012.</p>
<p>An analysis of approximations for maximizing submodular set functions. G L Nemhauser, L A Wolsey, M L Fisher, Mathematical Programming. 141G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functions. Mathematical Programming, 14(1):265- 294, dec 1978.</p>
<p>Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner. Mai Sao, Pierre-Yves Nguyen, Oudeyer, Paladyn3Sao Mai Nguyen and Pierre-Yves Oudeyer. Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner. Paladyn, 3(3):136-146, sep 2012.</p>
<p>Intrinsic motivation systems for autonomous mental development. Pierre-Yves Oudeyer, F Kaplan, V V Hafner, IEEE Transactions on Evolutionary Computation. 112Pierre-Yves Oudeyer, F. Kaplan, and V.V. Hafner. Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computa- tion, 11(2):265-286, April 2007.</p>
<p>Some aspects of the sequential design of experiments. Herbert Robbins, Bull. Amer. Math. Soc. 5851952Herbert Robbins. Some aspects of the sequential design of experiments. Bull. Amer. Math. Soc., 58(5):527-535, 09 1952.</p>
<p>Online goal babbling for rapid bootstrapping of inverse models in high dimensions. M Rolf, J J Steil, M Gienger, Proc. ICDL 2011. ICDL 20112M. Rolf, J.J. Steil, and M. Gienger. Online goal babbling for rapid bootstrapping of inverse models in high dimensions. In Proc. ICDL 2011, volume 2, pages 1-8, Aug 2011.</p>
<p>Fitness sharing and niching methods revisited. B Sareni, L Krahenbuhl, IEEE Transactions on Evolutionary Computation. 23B. Sareni and L. Krahenbuhl. Fitness sharing and niching methods revisited. IEEE Transactions on Evolutionary Computation, 2(3):97-106, 1998.</p>
<p>On learning how to learn learning strategies. Jrgen Schmidhuber, FKI-198-94Fakultt fr Informatik, Technische Universitt MnchenTechnical ReportJrgen Schmidhuber. On learning how to learn learning strategies. Technical report, Technical Report FKI-198-94, Fakultt fr Informatik, Technische Universitt Mnchen, 1994.</p>
<p>Serious fun: Preschoolers engage in more exploratory play when evidence is confounded. Laura E Schulz, Elizabeth Baraff Bonawitz, Developmental Psychology. 434Laura E. Schulz and Elizabeth Baraff Bonawitz. Serious fun: Preschoolers engage in more exploratory play when evidence is confounded. Developmental Psychology, 43(4):1045-1050, 2007.</p>
<p>Discovering several robot behaviors through speciation. Leonardo Trujillo, Gustavo Olague, Evelyne Lutton, Francisco Fernndez De Vega, Applications of Evolutionary Computing. Springer Science + Business MediaLeonardo Trujillo, Gustavo Olague, Evelyne Lutton, and Francisco Fernndez de Vega. Discovering several robot behaviors through speciation. In Applications of Evolutionary Computing, pages 164-174. Springer Science + Business Media, 2008.</p>
<p>Incremental robot shaping. Joseba Urzelai, Dario Floreano, Marco Dorigo, Marco Colombetti, Connection Science. 103-4Joseba Urzelai, Dario Floreano, Marco Dorigo, and Marco Colombetti. Incremental robot shaping. Connection Science, 10(3-4):341-360, sep 1998.</p>
<p>Restless bandits: Activity allocation in a changing world. P Whittle, Journal of Applied Probability. 25287P. Whittle. Restless bandits: Activity allocation in a changing world. Journal of Applied Probability, 25:287, 1988.</p>            </div>
        </div>

    </div>
</body>
</html>
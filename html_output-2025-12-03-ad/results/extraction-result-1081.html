<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1081 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1081</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1081</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-1418c9da011db25fa95a32989d5a578bc3bc4601</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1418c9da011db25fa95a32989d5a578bc3bc4601" target="_blank">Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning</a></p>
                <p><strong>Paper Venue:</strong> Journal of machine learning research</p>
                <p><strong>Paper TL;DR:</strong> It is illustrated the computational efficiency of IMGEPs as these robotic experiments use a simple memory-based low-level policy representations and search algorithm, enabling the whole system to learn online and incrementally on a Raspberry Pi 3.</p>
                <p><strong>Paper Abstract:</strong> Intrinsically motivated spontaneous exploration is a key enabler of autonomous lifelong learning in human children. It allows them to discover and acquire large repertoires of skills through self-generation, self-selection, self-ordering and self-experimentation of learning goals. We present the unsupervised multi-goal reinforcement learning formal framework as well as an algorithmic approach called intrinsically motivated goal exploration processes (IMGEP) to enable similar properties of autonomous learning in machines. The IMGEP algorithmic architecture relies on several principles: 1) self-generation of goals as parameterized reinforcement learning problems; 2) selection of goals based on intrinsic rewards; 3) exploration with parameterized time-bounded policies and fast incremental goal-parameterized policy search; 4) systematic reuse of information acquired when targeting a goal for improving other goals. We present a particularly efficient form of IMGEP that uses a modular representation of goal spaces as well as intrinsic rewards based on learning progress. We show how IMGEPs automatically generate a learning curriculum within an experimental setup where a real humanoid robot can explore multiple spaces of goals with several hundred continuous dimensions. While no particular target goal is provided to the system beforehand, this curriculum allows the discovery of skills of increasing complexity, that act as stepping stone for learning more complex skills (like nested tool use). We show that learning several spaces of diverse problems can be more efficient for learning complex skills than only trying to directly learn these complex skills. We illustrate the computational efficiency of IMGEPs as these robotic experiments use a simple memory-based low-level policy representations and search algorithm, enabling the whole system to learn online and incrementally on a Raspberry Pi 3.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1081.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1081.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMB-2D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Model Babbling (2D Simulated Tool-Use)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsically-motivated, modular, population-based goal exploration agent using learning-progress based goal-space selection and stepping-stone preserving mutations, evaluated in a 2D simulated robotic arm environment with tools and distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Active Model Babbling (AMB)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Population-based goal-parameterized learning: a database of low-level policies (RBF parameterizations) with an exploration meta-policy (nearest-neighbor, memory-based) and a slower exploitation meta-policy; intrinsic rewards computed from competence (learning progress) are used to select modular goal spaces; mutations use Stepping-Stone Preserving Mutation (SSPMutation).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (2D robotic arm)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>2D Simulated Tool-Use Environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 2D episodic environment with a 3-joint robotic arm + gripper (4 DOF control vector in [-1,1]^4) that can grasp two sticks (magnetic and Velcro) to reach and move toys; contains 15 objects total (including 10 uncontrollable distractors), moving distractors (cat and dog doing random walks), and static unreachable objects; episodes are 50 steps; outcomes encoded either as end-states (31-D) or object trajectories (155-D) depending on experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of objects (15 total, 10 uncontrollable distractors), motor DoFs (4), policy parameter dimensionality (20 RBF parameters), episode length (50 steps), sensory outcome dimension (31 for end-states, 155 for trajectories); presence of tool-use dependencies (hand -> tool -> toy) and stochastic moving distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low-to-medium (geometric 2D physics, small motor dimensionality but multi-object/tool-use stepping-stone structure and distractors)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Presence/number of distractors (cat and dog random movers, 4 static distractors), stochastic movements of distractors, resets between episodes; experiments also vary whether outcomes are full trajectories or end-states (i.e. sensory representation variation).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (multiple uncontrollable/distracting objects and stochastic distractor motion present in standard setup)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration measured as percentage of discretized goal-space cells reached (discretization per object end-position: 100x100 bins); also per-goal-space intrinsic learning-progress curves and proportions of iterations that moved target objects when exploring a given space.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1 percentiles (25,50,75) for AMB: Magnet Tool exploration: 57%, 61%, 65% of discretized cells reached; Magnet Toy exploration: 0%, 3.0%, 16% (showing toy is harder to reach even with AMB). Random baseline was near 0. (These numbers are percent of reached discretized cells in each object's 2D end-position space.)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports that complexity arises from the stepping-stone structure (hand -> tool -> toy) and that environment variation from distractors/stochastic movers reduces naive exploration success; the modular, intrinsic-learning-progress driven curriculum (AMB) helps prioritize intermediate-complexity goal spaces (tools) before harder goals (toys), thereby mitigating the negative effect of distractors. The paper does not present a formal analytic trade-off curve but provides empirical comparisons showing AMB discovers tools and toys far more efficiently than random or single-goal approaches in the presence of distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Intrinsic-curriculum learning via modular goal spaces and learning-progress (competence derivative) based goal selection; two asynchronous loops: fast incremental exploration (memory-based NN + SSPMutation) and slower batch exploitation updates.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>No explicit cross-environment generalization tests; evaluation focuses on within-environment exploration coverage and competence across sampled goals.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Experiments report long runs (example: 100k iterations reported for 2D environment exploration maps). Results (percentiles) reported after these runs (100k iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In the 2D simulated tool-use domain AMB self-organizes an automatic curriculum: it first focuses on hand movements, then discovers and explores tools, then discovers toys; AMB yields substantial exploration coverage of the magnet tool (median 61% cells) while random or single-goal baselines achieve near-zero coverage; stepping-stone preserving mutations and modular goal spaces are critical to efficiently discover tool-mediated outcomes in the presence of distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1081.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1081.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMB-MMC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Model Babbling (Minecraft Mountain Cart)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AMB instantiated in a procedurally-designed multi-stage 3D Minecraft Mountain Cart environment where navigation uncovers tools (pickaxe) and enables later tasks (break blocks, move cart).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Active Model Babbling (AMB)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same population-based, modular IMGEP architecture: exploration meta-policy (nearest-neighbor memory), goal-space selection via estimated learning progress across object-centered goal modules, and SSPMutation; motor policies implemented as neural-network parameter vectors (mutated, not backprop-trained).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>virtual agent in simulated 3D (Minecraft) environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Minecraft Mountain Cart (MMC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 3D episodic arena where the agent navigates (2 continuous motion commands + binary 'use tool') across an environment with water hazards and two tools (shovel and pickaxe). The pickaxe, if found and used appropriately, can break diamond blocks that block access to a cart that can be pushed along a track; cart movement requires sustained interaction and support for inertia; outcomes include end positions of agent, shovel, pickaxe, cart and states of diamond blocks; several distractors present (3 uncontrollable distractors among objects). Episodes are 40 steps (10 seconds).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number and coupling of tasks (navigation + discover tools + break blocks + move cart), outcome dimensionality (agent pos 2D, tool pos 2D each, cart 1D, block states 5D binary, 3 distractors 2D each), motor/action space (2 continuous + binary use), episodic horizon (40 steps), multi-stage dependencies (nav -> find tool -> break blocks -> move cart).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high (multi-stage sequential dependencies with sparse/deceptive rewards for later tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Stochasticity of navigation (water holes cause failure), variable initial positions, procedural aspects of cart slopes and block configurations, inclusion of distractor objects; multiple seeds/runs reported (20–42 depending on condition).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (tasks and layout fixed per experiment but stochastic dynamics and distractors introduce variation across episodes and seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration measured as number of reached discrete cells in discretized 2D spaces (e.g., 15x30 bins for agent and tool spaces), and number of distinct outcomes for cart; competence tests: fraction of uniformly sampled test goals reached (Euclidean distance threshold 0.05 in normalized [-1,1] space) for pickaxe and cart spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1 percentiles (AMB): Agent Pos. exploration: 55,58,61; Shovel: 32,34,37; Pickaxe: 41,45,48; Blocks: 73,84,93; Cart: 56,360,886 (cart reported as counts of different outcomes reached — large variance). Competence test (Table 2) AMB percentiles: Pickaxe goals: 41,45,49; Cart goals: 8,11,18 (25/50/75 percentiles over seeds). Random policy had near-zero discovery rates (e.g., pickaxe ~0.04% chance under random).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Empirical results show that sequential/structural complexity (multi-stage dependencies) makes random exploration extremely unlikely to discover late-stage tasks (e.g., cart); modular goal sampling with learning progress (AMB) greatly increases discovery probabilities (agent-space exploration yields ~10% pickaxe discovery vs 0.04% for random; pickaxe exploration yields ~1% block breaks, block exploration ~8% cart discovery). The paper frames this as intrinsic curriculum emergence rather than a formal trade-off curve between environment complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Intrinsic automatic curriculum learning via learning-progress based modular goal selection; exploration via mutated neural policies and stepping-stone preserving mutation; occasional exploitation rollouts used to estimate progress (20% of iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>No cross-environment generalization reported; competence tests used many uniformly sampled goals within same environment to evaluate learned controller coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Experiments report runs of up to 40k episodes for MMC (exploration maps shown after 40k episodes); discovery of cart occurred after ~14k–26k episodes in example seeds, showing the timescales for multi-stage discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AMB discovers multi-stage tool-use sequences significantly more often than random or flat baselines; learning-progress based selection directs exploration to intermediate stepping-stone goal spaces (agent -> pickaxe -> blocks -> cart) enabling discovery of later tasks that random or single-space exploration never finds; competence tests show AMB significantly better than Random and RMB at pickaxe competence (p<0.01), while Fixed Curriculum (FC) performs comparably to AMB on some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1081.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1081.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMB-Robotic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Model Babbling (Robotic Tool-Use, Poppy Torso)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AMB applied to a real humanoid-arm robotic setup (Poppy Torso) interacting with joysticks as tools to control a toy robot and a ball that drive light and sound — a high-dimensional, noisy, physical embodiment testbed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Active Model Babbling (AMB)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Population-based IMGEP with RBF motor policies (20 parameters), memory-based exploration meta-policy, learning-progress based modular goal selection, and SSPMutation; outcomes encoded as trajectories of 15 objects (total sensory space ~310 dimensions).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical robot (humanoid arm) interacting with physical objects and sensors</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Robotic Tool-Use Environment (Poppy Torso fronting joysticks, Ergo toy, ball)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Real-world robotic setup with a Poppy Torso agent controlling a 4-joint left arm; two analog joysticks can be reached and used as tools; a Poppy Ergo toy controlled by right joystick can push a ball that controls lights and sounds; many distractor and static objects present; outcomes are object trajectories (hand 3D, joysticks 2D, Ergo 2D, ball 2D, light intensity 1D, sound pitch 1D); total sensory outcome dimension ~310; real-world noise and complex contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Physical embodiment complexity: motor DoFs (4 joints), motor policy parameters (20 RBF params), sensory outcome dimensionality (~310), number of objects (15, 8 uncontrollable distractors), contact dynamics, sensor noise, and real-world stochasticity; episodes of 30 steps in robotic environment.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (high-dimensional sensory space, real-world noise, complex contact and tool-use dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Real-world stochastic variation (sensor and actuator noise), non-deterministic dynamic interactions (grasping, contact), distractor objects with independent motion, varying contexts between rollouts (some objects not reset), and multiple independent seeds/runs (parallel copies used to gather data).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration coverage measured as percentage/count of discretized cells reached per object's end-position or trajectory discretization (1D/2D/3D bins as appropriate), and proportions of iterations where exploring a given space moved other objects (stepping-stone statistics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1 percentiles (AMB): Hand: 22%, 23%, 24% (discretized coverage); Left Joystick: 20%, 22%, 26%; Right Joystick: 16%, 18%, 22%; Ergo (toy): 1.5%, 1.7%, 1.8%; Ball: 0.9%, 1.1%, 1.2%; Light: 2.0%, 3.6%, 4.9%; Sound: 1.7%, 2.8%, 3.6% (25/50/75 percentiles across seeds). These reflect modest but non-zero discovery and control of tool-mediated outcomes despite high noise/complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Empirical findings indicate that real-world variation (noise, contact dynamics) increases difficulty of later-stage discoveries; modular trajectory-based goal spaces produce large goal spaces where some modules (hand) remain a persistent source of learning progress, while harder object spaces show decreases in learning progress as subspaces of learnable trajectories are smaller. The paper presents this as an interplay (not a formal trade-off) where modular intrinsic progress-based selection helps cope with high complexity and variation but may keep focusing on large homogeneous spaces (hand) unless stepping-stone structure yields progress on harder spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Modular intrinsic-curriculum learning with learning-progress driven goal-space selection, stepping-stone preserving mutations to protect early successful subtasks (e.g., grasp), memory-based exploration meta-policy, and asynchronous exploitation/updating.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>No explicit transfer/generalization to new physical setups reported; evaluation focuses on within-setup exploration coverage and stepping-stone discovery probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Example exploration maps shown for first 10k iterations; runs aggregated across seeds (various seeds: AMB had 23 runs in robotic environment); successful tool-mediated outcomes are rare but measurable within tens of thousands of interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AMB is able to discover joystick-based tool-use in a noisy physical setup where random exploration is insufficient; SSPMutation is important to preserve grasping sub-behaviors so the agent explores after tool acquisition; modular goal spaces and learning-progress based selection produce a self-organized curriculum (hand -> left joystick -> right joystick -> Ergo/ball -> light/sound) even under high real-world variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1081.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1081.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMGEP-Comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparative findings across IMGEP variants and baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison of modular, learning-progress driven IMGEP (AMB) versus Random Model Babbling (RMB), Flat Random Goal Babbling (FRGB), Single Goal Space (SGS), Fixed Curriculum (FC), and Random motor policies across three environments, highlighting trade-offs between complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AMB vs RMB vs FRGB vs SGS vs FC vs Random</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multiple instantiations/ablations of the IMGEP architecture: AMB uses modular goal spaces + LP-based goal-space sampling + SSPMutation; RMB uses modular goal spaces + random goal-space selection; FRGB uses a single flat outcome space and FullMutation; SGS targets a single goal space; FC uses a hand-designed fixed curriculum; Random picks random motor commands.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>mixture of simulated and physical embodied agents (see environment-specific entries)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Evaluated across 2D Simulated, Minecraft Mountain Cart, and Robotic Tool-Use environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>See environment-specific entries; collectively these include low-dimensional simulated geometric physics, a multi-stage Minecraft navigation/tool-use domain, and a high-dimensional noisy real robot testbed with distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Varies per environment (see environment-specific entries); comparisons explicitly manipulate: goal-space modularity (modular vs flat), goal-space selection strategy (LP-based vs random vs fixed ordering), mutation operator (SSPMutation vs FullMutation), presence/absence of distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies (experiments span low-to-high complexity environments)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Presence/absence of distractors, stochastic movers, sensory representation variants (trajectory vs end-state), multiple seeds/runs; RMB tested both with and without distractors (RMB performs better when no distractors), FC is hand-designed curriculum controlling exposure order.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>varied across experiments (low when no distractors, high when distractors and real-world noise present)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration coverage percentiles (Table 1), competence percentiles (Table 2), frequencies/probabilities of discovering objects as a function of currently explored goal-space (stepping-stone statistics). Statistical tests (Welch's t-test) for significance between conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Across environments AMB consistently outperforms simple baselines in exploration coverage for target/useful objects: e.g., 2D Magnet Tool AMB median 61% vs RMB median 36% and FRGB 11%; Minecraft pickaxe exploration AMB median 45% vs RMB 35% and FRGB 15%; Robotic left joystick AMB median 22% vs RMB 18% and FRGB ~0.1%. RMB can perform well when all goal-spaces are relevant and no distractors present, but AMB is more robust to distractors and complex stepping-stone structures. FC (hand-designed curriculum) sometimes matches AMB performance.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Empirical trade-offs shown: when environment variation includes distractors and high-dimensional/noisy dynamics, modularity + LP-based selection (AMB) yields superior discovery and sample efficiency; when variation is low and all goal spaces are relevant, simpler random modular exploration (RMB) can approach AMB. The paper frames these as empirical relationships rather than strict theoretical trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Comparison includes intrinsic curriculum (AMB), random modular exploration (RMB), flat random goal babbling (FRGB), single goal focus (SGS), hand-designed fixed curriculum (FC), and purely random motor policy exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>No explicit generalization-to-new-environment experiments; comparisons evaluate relative ability to discover multi-stage tasks and to build repertoires within the same environments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported as number of iterations to discovery in representative seeds: e.g., in MMC cart discovered after ~14k–26k episodes in example AMB runs; 2D runs shown after 100k iterations; robotic examples plotted over first 10k iterations in visualizations. AMB tends to require fewer interactions to discover later-stage tasks compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Key trade-offs: modular goal structure and LP-based sampling enable emergent curricula that prioritize intermediate stepping-stone goals, substantially improving discovery of complex tool-use outcomes under distractors and noise; SSPMutation preserves earlier successful subtasks (crucial in tool-use) and improves exploration efficiency; in environments without distractors or where all goal-spaces are equally relevant, simpler strategies (RMB) can perform competitively, indicating an interaction between environment variation and the necessity of intrinsic curriculum mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Intrinsic Motivation Systems for Autonomous Mental Development <em>(Rating: 2)</em></li>
                <li>Intrinsically Motivated Goal Exploration for Active Learning of Multiple Skills <em>(Rating: 2)</em></li>
                <li>Curiosity-driven Exploration by Self-supervised Prediction <em>(Rating: 1)</em></li>
                <li>Go-Explore: a New Approach for Hard-Exploration Problems <em>(Rating: 2)</em></li>
                <li>MAP-Elites: a Multi-dimensional Archive of Phenotypic Elites <em>(Rating: 2)</em></li>
                <li>Modeling Infant Intrinsic Motivation and Curiosity-based Learning <em>(Rating: 1)</em></li>
                <li>Quality-Diversity Algorithms: A Survey <em>(Rating: 1)</em></li>
                <li>Population-Based Incremental Learning and Goal-Parameterised Policies (Baranes and Oudeyer, 2013) <em>(Rating: 2)</em></li>
                <li>Péré et al. (2018) - Goal-conditioned generative models for exploration <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1081",
    "paper_id": "paper-1418c9da011db25fa95a32989d5a578bc3bc4601",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "AMB-2D",
            "name_full": "Active Model Babbling (2D Simulated Tool-Use)",
            "brief_description": "An intrinsically-motivated, modular, population-based goal exploration agent using learning-progress based goal-space selection and stepping-stone preserving mutations, evaluated in a 2D simulated robotic arm environment with tools and distractors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Active Model Babbling (AMB)",
            "agent_description": "Population-based goal-parameterized learning: a database of low-level policies (RBF parameterizations) with an exploration meta-policy (nearest-neighbor, memory-based) and a slower exploitation meta-policy; intrinsic rewards computed from competence (learning progress) are used to select modular goal spaces; mutations use Stepping-Stone Preserving Mutation (SSPMutation).",
            "agent_type": "simulated agent (2D robotic arm)",
            "environment_name": "2D Simulated Tool-Use Environment",
            "environment_description": "A 2D episodic environment with a 3-joint robotic arm + gripper (4 DOF control vector in [-1,1]^4) that can grasp two sticks (magnetic and Velcro) to reach and move toys; contains 15 objects total (including 10 uncontrollable distractors), moving distractors (cat and dog doing random walks), and static unreachable objects; episodes are 50 steps; outcomes encoded either as end-states (31-D) or object trajectories (155-D) depending on experiments.",
            "complexity_measure": "Number of objects (15 total, 10 uncontrollable distractors), motor DoFs (4), policy parameter dimensionality (20 RBF parameters), episode length (50 steps), sensory outcome dimension (31 for end-states, 155 for trajectories); presence of tool-use dependencies (hand -&gt; tool -&gt; toy) and stochastic moving distractors.",
            "complexity_level": "low-to-medium (geometric 2D physics, small motor dimensionality but multi-object/tool-use stepping-stone structure and distractors)",
            "variation_measure": "Presence/number of distractors (cat and dog random movers, 4 static distractors), stochastic movements of distractors, resets between episodes; experiments also vary whether outcomes are full trajectories or end-states (i.e. sensory representation variation).",
            "variation_level": "medium (multiple uncontrollable/distracting objects and stochastic distractor motion present in standard setup)",
            "performance_metric": "Exploration measured as percentage of discretized goal-space cells reached (discretization per object end-position: 100x100 bins); also per-goal-space intrinsic learning-progress curves and proportions of iterations that moved target objects when exploring a given space.",
            "performance_value": "Table 1 percentiles (25,50,75) for AMB: Magnet Tool exploration: 57%, 61%, 65% of discretized cells reached; Magnet Toy exploration: 0%, 3.0%, 16% (showing toy is harder to reach even with AMB). Random baseline was near 0. (These numbers are percent of reached discretized cells in each object's 2D end-position space.)",
            "complexity_variation_relationship": "Paper reports that complexity arises from the stepping-stone structure (hand -&gt; tool -&gt; toy) and that environment variation from distractors/stochastic movers reduces naive exploration success; the modular, intrinsic-learning-progress driven curriculum (AMB) helps prioritize intermediate-complexity goal spaces (tools) before harder goals (toys), thereby mitigating the negative effect of distractors. The paper does not present a formal analytic trade-off curve but provides empirical comparisons showing AMB discovers tools and toys far more efficiently than random or single-goal approaches in the presence of distractors.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Intrinsic-curriculum learning via modular goal spaces and learning-progress (competence derivative) based goal selection; two asynchronous loops: fast incremental exploration (memory-based NN + SSPMutation) and slower batch exploitation updates.",
            "generalization_tested": false,
            "generalization_results": "No explicit cross-environment generalization tests; evaluation focuses on within-environment exploration coverage and competence across sampled goals.",
            "sample_efficiency": "Experiments report long runs (example: 100k iterations reported for 2D environment exploration maps). Results (percentiles) reported after these runs (100k iterations).",
            "key_findings": "In the 2D simulated tool-use domain AMB self-organizes an automatic curriculum: it first focuses on hand movements, then discovers and explores tools, then discovers toys; AMB yields substantial exploration coverage of the magnet tool (median 61% cells) while random or single-goal baselines achieve near-zero coverage; stepping-stone preserving mutations and modular goal spaces are critical to efficiently discover tool-mediated outcomes in the presence of distractors.",
            "uuid": "e1081.0",
            "source_info": {
                "paper_title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "AMB-MMC",
            "name_full": "Active Model Babbling (Minecraft Mountain Cart)",
            "brief_description": "AMB instantiated in a procedurally-designed multi-stage 3D Minecraft Mountain Cart environment where navigation uncovers tools (pickaxe) and enables later tasks (break blocks, move cart).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Active Model Babbling (AMB)",
            "agent_description": "Same population-based, modular IMGEP architecture: exploration meta-policy (nearest-neighbor memory), goal-space selection via estimated learning progress across object-centered goal modules, and SSPMutation; motor policies implemented as neural-network parameter vectors (mutated, not backprop-trained).",
            "agent_type": "virtual agent in simulated 3D (Minecraft) environment",
            "environment_name": "Minecraft Mountain Cart (MMC)",
            "environment_description": "A 3D episodic arena where the agent navigates (2 continuous motion commands + binary 'use tool') across an environment with water hazards and two tools (shovel and pickaxe). The pickaxe, if found and used appropriately, can break diamond blocks that block access to a cart that can be pushed along a track; cart movement requires sustained interaction and support for inertia; outcomes include end positions of agent, shovel, pickaxe, cart and states of diamond blocks; several distractors present (3 uncontrollable distractors among objects). Episodes are 40 steps (10 seconds).",
            "complexity_measure": "Number and coupling of tasks (navigation + discover tools + break blocks + move cart), outcome dimensionality (agent pos 2D, tool pos 2D each, cart 1D, block states 5D binary, 3 distractors 2D each), motor/action space (2 continuous + binary use), episodic horizon (40 steps), multi-stage dependencies (nav -&gt; find tool -&gt; break blocks -&gt; move cart).",
            "complexity_level": "medium-to-high (multi-stage sequential dependencies with sparse/deceptive rewards for later tasks)",
            "variation_measure": "Stochasticity of navigation (water holes cause failure), variable initial positions, procedural aspects of cart slopes and block configurations, inclusion of distractor objects; multiple seeds/runs reported (20–42 depending on condition).",
            "variation_level": "medium (tasks and layout fixed per experiment but stochastic dynamics and distractors introduce variation across episodes and seeds)",
            "performance_metric": "Exploration measured as number of reached discrete cells in discretized 2D spaces (e.g., 15x30 bins for agent and tool spaces), and number of distinct outcomes for cart; competence tests: fraction of uniformly sampled test goals reached (Euclidean distance threshold 0.05 in normalized [-1,1] space) for pickaxe and cart spaces.",
            "performance_value": "Table 1 percentiles (AMB): Agent Pos. exploration: 55,58,61; Shovel: 32,34,37; Pickaxe: 41,45,48; Blocks: 73,84,93; Cart: 56,360,886 (cart reported as counts of different outcomes reached — large variance). Competence test (Table 2) AMB percentiles: Pickaxe goals: 41,45,49; Cart goals: 8,11,18 (25/50/75 percentiles over seeds). Random policy had near-zero discovery rates (e.g., pickaxe ~0.04% chance under random).",
            "complexity_variation_relationship": "Empirical results show that sequential/structural complexity (multi-stage dependencies) makes random exploration extremely unlikely to discover late-stage tasks (e.g., cart); modular goal sampling with learning progress (AMB) greatly increases discovery probabilities (agent-space exploration yields ~10% pickaxe discovery vs 0.04% for random; pickaxe exploration yields ~1% block breaks, block exploration ~8% cart discovery). The paper frames this as intrinsic curriculum emergence rather than a formal trade-off curve between environment complexity and variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Intrinsic automatic curriculum learning via learning-progress based modular goal selection; exploration via mutated neural policies and stepping-stone preserving mutation; occasional exploitation rollouts used to estimate progress (20% of iterations).",
            "generalization_tested": false,
            "generalization_results": "No cross-environment generalization reported; competence tests used many uniformly sampled goals within same environment to evaluate learned controller coverage.",
            "sample_efficiency": "Experiments report runs of up to 40k episodes for MMC (exploration maps shown after 40k episodes); discovery of cart occurred after ~14k–26k episodes in example seeds, showing the timescales for multi-stage discoveries.",
            "key_findings": "AMB discovers multi-stage tool-use sequences significantly more often than random or flat baselines; learning-progress based selection directs exploration to intermediate stepping-stone goal spaces (agent -&gt; pickaxe -&gt; blocks -&gt; cart) enabling discovery of later tasks that random or single-space exploration never finds; competence tests show AMB significantly better than Random and RMB at pickaxe competence (p&lt;0.01), while Fixed Curriculum (FC) performs comparably to AMB on some metrics.",
            "uuid": "e1081.1",
            "source_info": {
                "paper_title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "AMB-Robotic",
            "name_full": "Active Model Babbling (Robotic Tool-Use, Poppy Torso)",
            "brief_description": "AMB applied to a real humanoid-arm robotic setup (Poppy Torso) interacting with joysticks as tools to control a toy robot and a ball that drive light and sound — a high-dimensional, noisy, physical embodiment testbed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Active Model Babbling (AMB)",
            "agent_description": "Population-based IMGEP with RBF motor policies (20 parameters), memory-based exploration meta-policy, learning-progress based modular goal selection, and SSPMutation; outcomes encoded as trajectories of 15 objects (total sensory space ~310 dimensions).",
            "agent_type": "physical robot (humanoid arm) interacting with physical objects and sensors",
            "environment_name": "Robotic Tool-Use Environment (Poppy Torso fronting joysticks, Ergo toy, ball)",
            "environment_description": "Real-world robotic setup with a Poppy Torso agent controlling a 4-joint left arm; two analog joysticks can be reached and used as tools; a Poppy Ergo toy controlled by right joystick can push a ball that controls lights and sounds; many distractor and static objects present; outcomes are object trajectories (hand 3D, joysticks 2D, Ergo 2D, ball 2D, light intensity 1D, sound pitch 1D); total sensory outcome dimension ~310; real-world noise and complex contact dynamics.",
            "complexity_measure": "Physical embodiment complexity: motor DoFs (4 joints), motor policy parameters (20 RBF params), sensory outcome dimensionality (~310), number of objects (15, 8 uncontrollable distractors), contact dynamics, sensor noise, and real-world stochasticity; episodes of 30 steps in robotic environment.",
            "complexity_level": "high (high-dimensional sensory space, real-world noise, complex contact and tool-use dynamics)",
            "variation_measure": "Real-world stochastic variation (sensor and actuator noise), non-deterministic dynamic interactions (grasping, contact), distractor objects with independent motion, varying contexts between rollouts (some objects not reset), and multiple independent seeds/runs (parallel copies used to gather data).",
            "variation_level": "high",
            "performance_metric": "Exploration coverage measured as percentage/count of discretized cells reached per object's end-position or trajectory discretization (1D/2D/3D bins as appropriate), and proportions of iterations where exploring a given space moved other objects (stepping-stone statistics).",
            "performance_value": "Table 1 percentiles (AMB): Hand: 22%, 23%, 24% (discretized coverage); Left Joystick: 20%, 22%, 26%; Right Joystick: 16%, 18%, 22%; Ergo (toy): 1.5%, 1.7%, 1.8%; Ball: 0.9%, 1.1%, 1.2%; Light: 2.0%, 3.6%, 4.9%; Sound: 1.7%, 2.8%, 3.6% (25/50/75 percentiles across seeds). These reflect modest but non-zero discovery and control of tool-mediated outcomes despite high noise/complexity.",
            "complexity_variation_relationship": "Empirical findings indicate that real-world variation (noise, contact dynamics) increases difficulty of later-stage discoveries; modular trajectory-based goal spaces produce large goal spaces where some modules (hand) remain a persistent source of learning progress, while harder object spaces show decreases in learning progress as subspaces of learnable trajectories are smaller. The paper presents this as an interplay (not a formal trade-off) where modular intrinsic progress-based selection helps cope with high complexity and variation but may keep focusing on large homogeneous spaces (hand) unless stepping-stone structure yields progress on harder spaces.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Modular intrinsic-curriculum learning with learning-progress driven goal-space selection, stepping-stone preserving mutations to protect early successful subtasks (e.g., grasp), memory-based exploration meta-policy, and asynchronous exploitation/updating.",
            "generalization_tested": false,
            "generalization_results": "No explicit transfer/generalization to new physical setups reported; evaluation focuses on within-setup exploration coverage and stepping-stone discovery probabilities.",
            "sample_efficiency": "Example exploration maps shown for first 10k iterations; runs aggregated across seeds (various seeds: AMB had 23 runs in robotic environment); successful tool-mediated outcomes are rare but measurable within tens of thousands of interactions.",
            "key_findings": "AMB is able to discover joystick-based tool-use in a noisy physical setup where random exploration is insufficient; SSPMutation is important to preserve grasping sub-behaviors so the agent explores after tool acquisition; modular goal spaces and learning-progress based selection produce a self-organized curriculum (hand -&gt; left joystick -&gt; right joystick -&gt; Ergo/ball -&gt; light/sound) even under high real-world variation.",
            "uuid": "e1081.2",
            "source_info": {
                "paper_title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "IMGEP-Comparisons",
            "name_full": "Comparative findings across IMGEP variants and baselines",
            "brief_description": "Empirical comparison of modular, learning-progress driven IMGEP (AMB) versus Random Model Babbling (RMB), Flat Random Goal Babbling (FRGB), Single Goal Space (SGS), Fixed Curriculum (FC), and Random motor policies across three environments, highlighting trade-offs between complexity and variation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "AMB vs RMB vs FRGB vs SGS vs FC vs Random",
            "agent_description": "Multiple instantiations/ablations of the IMGEP architecture: AMB uses modular goal spaces + LP-based goal-space sampling + SSPMutation; RMB uses modular goal spaces + random goal-space selection; FRGB uses a single flat outcome space and FullMutation; SGS targets a single goal space; FC uses a hand-designed fixed curriculum; Random picks random motor commands.",
            "agent_type": "mixture of simulated and physical embodied agents (see environment-specific entries)",
            "environment_name": "Evaluated across 2D Simulated, Minecraft Mountain Cart, and Robotic Tool-Use environments",
            "environment_description": "See environment-specific entries; collectively these include low-dimensional simulated geometric physics, a multi-stage Minecraft navigation/tool-use domain, and a high-dimensional noisy real robot testbed with distractors.",
            "complexity_measure": "Varies per environment (see environment-specific entries); comparisons explicitly manipulate: goal-space modularity (modular vs flat), goal-space selection strategy (LP-based vs random vs fixed ordering), mutation operator (SSPMutation vs FullMutation), presence/absence of distractors.",
            "complexity_level": "varies (experiments span low-to-high complexity environments)",
            "variation_measure": "Presence/absence of distractors, stochastic movers, sensory representation variants (trajectory vs end-state), multiple seeds/runs; RMB tested both with and without distractors (RMB performs better when no distractors), FC is hand-designed curriculum controlling exposure order.",
            "variation_level": "varied across experiments (low when no distractors, high when distractors and real-world noise present)",
            "performance_metric": "Exploration coverage percentiles (Table 1), competence percentiles (Table 2), frequencies/probabilities of discovering objects as a function of currently explored goal-space (stepping-stone statistics). Statistical tests (Welch's t-test) for significance between conditions.",
            "performance_value": "Across environments AMB consistently outperforms simple baselines in exploration coverage for target/useful objects: e.g., 2D Magnet Tool AMB median 61% vs RMB median 36% and FRGB 11%; Minecraft pickaxe exploration AMB median 45% vs RMB 35% and FRGB 15%; Robotic left joystick AMB median 22% vs RMB 18% and FRGB ~0.1%. RMB can perform well when all goal-spaces are relevant and no distractors present, but AMB is more robust to distractors and complex stepping-stone structures. FC (hand-designed curriculum) sometimes matches AMB performance.",
            "complexity_variation_relationship": "Empirical trade-offs shown: when environment variation includes distractors and high-dimensional/noisy dynamics, modularity + LP-based selection (AMB) yields superior discovery and sample efficiency; when variation is low and all goal spaces are relevant, simpler random modular exploration (RMB) can approach AMB. The paper frames these as empirical relationships rather than strict theoretical trade-offs.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Comparison includes intrinsic curriculum (AMB), random modular exploration (RMB), flat random goal babbling (FRGB), single goal focus (SGS), hand-designed fixed curriculum (FC), and purely random motor policy exploration.",
            "generalization_tested": false,
            "generalization_results": "No explicit generalization-to-new-environment experiments; comparisons evaluate relative ability to discover multi-stage tasks and to build repertoires within the same environments.",
            "sample_efficiency": "Reported as number of iterations to discovery in representative seeds: e.g., in MMC cart discovered after ~14k–26k episodes in example AMB runs; 2D runs shown after 100k iterations; robotic examples plotted over first 10k iterations in visualizations. AMB tends to require fewer interactions to discover later-stage tasks compared to baselines.",
            "key_findings": "Key trade-offs: modular goal structure and LP-based sampling enable emergent curricula that prioritize intermediate stepping-stone goals, substantially improving discovery of complex tool-use outcomes under distractors and noise; SSPMutation preserves earlier successful subtasks (crucial in tool-use) and improves exploration efficiency; in environments without distractors or where all goal-spaces are equally relevant, simpler strategies (RMB) can perform competitively, indicating an interaction between environment variation and the necessity of intrinsic curriculum mechanisms.",
            "uuid": "e1081.3",
            "source_info": {
                "paper_title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
                "publication_date_yy_mm": "2017-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Intrinsic Motivation Systems for Autonomous Mental Development",
            "rating": 2
        },
        {
            "paper_title": "Intrinsically Motivated Goal Exploration for Active Learning of Multiple Skills",
            "rating": 2
        },
        {
            "paper_title": "Curiosity-driven Exploration by Self-supervised Prediction",
            "rating": 1
        },
        {
            "paper_title": "Go-Explore: a New Approach for Hard-Exploration Problems",
            "rating": 2
        },
        {
            "paper_title": "MAP-Elites: a Multi-dimensional Archive of Phenotypic Elites",
            "rating": 2
        },
        {
            "paper_title": "Modeling Infant Intrinsic Motivation and Curiosity-based Learning",
            "rating": 1
        },
        {
            "paper_title": "Quality-Diversity Algorithms: A Survey",
            "rating": 1
        },
        {
            "paper_title": "Population-Based Incremental Learning and Goal-Parameterised Policies (Baranes and Oudeyer, 2013)",
            "rating": 2
        },
        {
            "paper_title": "Péré et al. (2018) - Goal-conditioned generative models for exploration",
            "rating": 2
        }
    ],
    "cost": 0.017793,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning</h1>
<p>Sébastien Forestier<br>Inria Bordeaux Sud-Ouest<br>200 avenue de la Vieille Tour, 33405 Talence, France<br>Rémy Portelas<br>Inria Bordeaux Sud-Ouest<br>200 avenue de la Vieille Tour, 33405 Talence, France<br>Yoan Mollard<br>Inria Bordeaux Sud-Ouest<br>200 avenue de la Vieille Tour, 33405 Talence, France<br>Pierre-Yves Oudeyer<br>Inria Bordeaux Sud-Ouest<br>200 avenue de la Vieille Tour, 33405 Talence, France</p>
<p>Editor: George Konidaris</p>
<h4>Abstract</h4>
<p>Intrinsically motivated spontaneous exploration is a key enabler of autonomous developmental learning in human children. It enables the discovery of skill repertoires through autotelic learning, i.e. the self-generation, self-selection, self-ordering and self-experimentation of learning goals. We present an algorithmic approach called Intrinsically Motivated Goal Exploration Processes (IMGEP) to enable similar properties of autonomous learning in machines. The IMGEP architecture relies on several principles: 1) self-generation of goals, generalized as parameterized fitness functions; 2) selection of goals based on intrinsic rewards; 3) exploration with incremental goal-parameterized policy search and exploitation with a batch learning algorithm; 4) systematic reuse of information acquired when targeting a goal for improving towards other goals. We present a particularly efficient form of IMGEP, called AMB, that uses a population-based policy and an object-centered spatiotemporal modularity. We provide several implementations of this architecture and demonstrate their ability to automatically generate a learning curriculum within several experimental setups. One of these experiments includes a real humanoid robot exploring multiple spaces of goals with several hundred continuous dimensions and with distractors. While no particular target goal is provided to these autotelic agents, this curriculum allows the discovery of diverse skills that act as stepping stones for learning more complex skills, e.g. nested tool use.</p>
<p>Keywords: Developmental learning, developmental AI, open-ended learning, intrinsic motivations, autotelic agents, population-based IMGEP, goal exploration, curiosity-driven learning, modularity, robotics, automatic curriculum learning.</p>
<h2>1. Introduction</h2>
<p>An extraordinary property of natural intelligence in humans is their capacity for lifelong autonomous learning. During cognitive development, processes of autonomous learning in infants have several properties that are fundamentally different from many current machine learning systems. Among them is the capability to spontaneously explore their environments, driven by an intrinsic motivation to discover and learn new tasks and problems that they imagine and select by themselves (Berlyne, 1966; Gopnik et al., 1999). Crucially, there is no engineer externally imposing one target goal that</p>
<p>they should explore, hand providing a curriculum for learning, nor providing a ready-to-use database of training examples. Rather, children are autotelic learners: they self-select their objectives within a large, potentially open-ended, space of goals they can imagine, and they collect training data by physically practicing these goals. In particular, they explore goals in an organized manner, attributing to them values of interestingness that evolve with time, and allowing them to self-define a learning curriculum that is called a developmental trajectory in developmental sciences (Thelen and Smith, 1996). This self-generated learning curriculum prevents infants from spending too much time on goals that are either too easy or too difficult, and allows them to focus on goals of the right level of complexity at the right time. Within this developmental learning process, the new learned solutions are often stepping stones for discovering how to solve other goals of increasing complexity. Thus, while they are not explicitly guided by a final target goal, these mechanisms allow infants to discover highly complex skills. For instance, biped locomotion or tool use would be extremely difficult to learn by focusing only on these targets from the start as the rewards for those goals are typically rare or deceptive.</p>
<p>An essential component of such organized spontaneous exploration is the intrinsic motivation system, also called curiosity-driven exploration system (Kidd and Hayden, 2015; Oudeyer and Smith, 2016; Gottlieb and Oudeyer, 2018). In the last two decades, a series of computational and robotic models of intrinsically motivated exploration and learning in infants have been developed (Oudeyer and Kaplan, 2007; Baldassarre and Mirolli, 2013; Bazhydai et al., 2020), opening new theoretical perspectives in neuroscience and psychology (Gottlieb et al., 2013). Two key ideas have allowed to simulate and predict important properties of infant spontaneous exploration, ranging from vocal development (Moulin-Frier et al., 2014; Forestier and Oudeyer, 2017), to object affordance and tool learning (Forestier and Oudeyer, 2016a,c). The first key idea is that infants might select experiments that maximize an intrinsic reward based on empirical learning progress (Oudeyer et al., 2007). This mechanism would generate automatically developmental trajectories (e.g. learning curricula) where progressively more complex tasks are practiced, learned and used as stepping stones for more complex skills. The second key idea is that beyond selecting actions or states based on the predictive learning progress they provide, a more powerful way to organize intrinsically motivated exploration is to select goals, i.e. self-generated fitness functions, based on a measure of competence progress, i.e. a measure of progress in learning to produce diverse and controlled behavioral features (Baranes and Oudeyer, 2010b, 2013). Here, the intrinsic reward is the empirical improvement towards solving self-selected goals (Oudeyer and Kaplan, 2007; Forestier and Oudeyer, 2016a), happening through lower-level policy search mechanisms that generate physical actions. The efficiency of such goal exploration processes relies on a form of hindsight learning that leverages the fact that the data collected when targeting a goal can be informative to find better solutions to other goals (for example, a learner trying to achieve the goal of pushing an object on the right but actually pushing it on the left fails to progress on this goal, but learns as a side effect how to push it on the left). These general ideas have been instantiated and studied in the context of population-based learning architectures (e.g. Baranes and Oudeyer (2013); Péré et al. (2018)), as well as more recently in goal-conditioned reinforcement learning architectures (e.g. Colas et al. (2019); Nair et al. (2018); Choi et al. (2021)), used to build autotelic agents (Colas et al., 2022).</p>
<p>Beyond neuroscience and psychology, we believe these models open new perspectives in artificial intelligence, contributing to the foundations of the new field of developmental artificial intelligence (Eppe and Oudeyer, 2021). In particular, algorithmic architectures for intrinsically motivated goal exploration were shown to allow the efficient acquisition of repertoires of high-dimensional motor skills with automated curriculum learning in several robotics experiments (Baranes and Oudeyer, 2013; Forestier and Oudeyer, 2016a). This includes for example learning omnidirectional locomotion or learning multiple ways to manipulate complex soft objects (Rolf et al., 2010; Baranes and Oudeyer, 2013).</p>
<p>In this article, we make several contributions:</p>
<ul>
<li>We present a formalization of Intrinsically Motivated Goal Exploration Processes (IMGEP), that is both more compact and more general than these previous models. In particular, it considers a generalized definition of the concept of goals, construed as abstract parameterized fitness functions that can express arbitrary objectives over full behavioural trajectories and include constraints. This enables to express a diversity of goal exploration algorithms in the same framework, including Quality-Diversity algorithms that were not previously formalized as goal exploration algorithms.</li>
<li>We present a new population-based IMGEP algorithmic architecture, called AMB, implementing two forms of object-centered modularity and using learning-progress to sample associated modular goal spaces. First, we introduce spatial modularity: each object of the environment is associated to a goal space. Second, we introduce temporal modularity: the temporal structure of objects' movement is leveraged for more efficient leveraging of discovered stepping-stones in goal exploration, through a stepping-stone preserving mutation operator (SSPMutation). We also present various instantiations.</li>
<li>We present a systematic experimental study of this new IMGEP algorithm in diverse environments providing opportunities for discovering complex skills like tool use, as well as including complex distractors: a 2D simulated environment, a Minecraft environment, and a real humanoid robotic setup. We compare several variants of IMGEP algorithms, including ablations, in terms of sample efficiency to discover a diversity of behavioral features. We also compare IMGEPs algorithms with algorithms exploring only one target object: we show that letting agents self-organize exploration of diverse objects is vastly more efficient for discovering how to control the target object than channeling the agent to explore only this object. We also compare the exploration resulting from the self-organized learning curriculum of intrinsically motivated agents with the exploration following a curriculum designed by hand with expert knowledge of the task, showing a similar exploration efficiency.</li>
</ul>
<h1>2. Intrinsically Motivated Goal Exploration Processes</h1>
<p>We define a framework for the intrinsically motivated exploration of multiple goals, where the data collected when exploring a goal give some information to help reach other goals. This framework considers that when the agent performed an experiment, it can compute the fitness of that experiment for achieving any goal, not only the one it was trying to reach. Importantly, it does not assume that all goals are achievable, nor that they are of a particular form, enabling to express complex objectives that do not simply depend on the observation of the end policy state but might depend on several aspects of entire behavioral trajectories (see Box on features, goals and goal spaces). Also, the agent autonomously builds its goals but does not know initially which goals are achievable or not, which are easy and which are difficult, nor if certain goals need to be explored so that other goals become achievable.</p>
<h3>2.1 Notations and Assumptions</h3>
<p>Let's consider an agent that executes continuous actions $a \in \mathcal{A}$ in continuous states $s \in \mathcal{S}$ of an environment $E$. We consider policies producing time-bounded rollouts through the dynamics $\delta_{E}\left(\boldsymbol{s}<em _boldsymbol_t="\boldsymbol{t">{\boldsymbol{t}+\boldsymbol{1}} \mid \boldsymbol{s}</em><em _boldsymbol_t="\boldsymbol{t">{0}: \boldsymbol{t}}, \boldsymbol{a}</em><em t__0="t_{0">{0}: \boldsymbol{t}}\right)$ of the environment, and we denote the corresponding behavioral trajectories $\tau=\left{s</em>$.}}, a_{t_{0}}, \cdots, s_{t_{e n d}}, a_{t_{e n d}}\right} \in \mathbb{T</p>
<p>We assume that the agent is able to construct a goal space $\mathcal{G}$ parameterized by $g$, representing fitness functions $f_{g}$ giving the fitness $f_{g}(\tau)$ of an experimentation $\tau$ to reach a goal $g$ (see Box on</p>
<h1>Generalized Goals and Goal Spaces</h1>
<p>In the general case, the agent has algorithmic tools to construct any goal as any function $f_{g}$ (parameterized by vector $g$ ), taking as input a state-action trajectory $\tau$, and returning the fitness of $\tau$ for achieving the goal. As a shorthand, we call the goal $g$ (vector of parameters), but the full representation of the goal consists in the combination of the parameters $g$ and the function or program computing the fitness function ${ }^{a}$.</p>
<p>Formally, given a behavioural trajectory $\tau=\left{s_{t_{0}}, a_{t_{0}}, \cdots, s_{t_{s n d}}, a_{t_{s n d}}\right}$, we assume the agent can compute a set of behavioural features $\varphi_{1}(\tau), \ldots, \varphi_{n}(\tau)$, also called outcomes, over the full trajectory. Those features are vectors that encode any static or dynamic property of the environment or the agent itself. Examples include the average speed of an object, an encoding of an object's trajectory or relation to other objects, the frequency of a movement, or the result of a test checking whether a movement or an object fulfills some properties specified in a sub-part of vector $g$. The features may be given to the agent, or learned, for instance using generative models (see Péré et al. (2018); Nair et al. (2018); Laversanne-Finot et al. (2018); Reinke et al. (2020)).</p>
<p>We also assume that computational tools, in the form of mathematical operators ${ }^{b}$, are available to the agent for constructing goals, i.e. for constructing fitness functions $f_{g}$ using these features. Examples include:</p>
<ul>
<li>$f_{g}(\tau)=\varphi_{g}(\tau)$ : the goal is to produce a trajectory $\tau$ that maximizes feature $\varphi_{g}(\tau)$, where the goal parameter vector $g$ is a simple one dimensional index of the target behavioural feature. Sampling the space of goals thus amounts to sampling which feature to maximize, e.g. maximize agent's speed or number of objects collected.</li>
<li>$f_{g}(\tau)=-\left|\varphi_{\psi_{1}(g)}(\tau)-\psi_{2}(g)\right|$ : the goal $g$ is to produce a trajectory $\tau$ so that its features $\varphi_{\psi_{1}(g)}(\tau)$ are as close as possible to the target vector $\psi_{2}(g)$, using a measure $|\cdot|$, e.g. move the hand or a ball to a particular 3D position (Baranes and Oudeyer, 2013; Péré et al., 2018), or produce a sound with a particular target spectrum (Moulin-Frier et al., 2014).</li>
<li>$f_{g}(\tau)=\varphi_{i}(\tau)$ if $\varphi_{j}(\tau) \in \psi_{3}(g)$ else 0 : the goal $g$ is to produce a trajectory $\tau$ which maximizes feature $i$ while keeping feature vector $j$ inside a local region specified by $\psi_{3}(g)$, e.g. maximize agent's speed while displaying a certain type of pattern of leg movement. When goal sampling is made in this form of function space, IMGEPs correspond to quality-diversity algorithms (Pugh et al., 2016) such as MAP-Elite (Cully et al., 2015).</li>
<li>$f_{g}(\tau)=f_{g_{1}}(\tau)$ if $f_{g_{2}}(\tau)&lt;f_{g_{3}}(\tau)$ else $f_{g_{4}}(\tau)$ : goals can be combined to form more complex constrained optimization problems, e.g. move the ball to follow a target while not getting too close to the walls and holes and minimizing the energy spent.</li>
</ul>
<p>A goal space is a set of goals (fitness functions) parameterized by a vector $g$. Diverse forms of structured parameterization can be used, as shown by the examples above, and corresponding to diverse types of goals. In the experiments of this paper, we define several goal spaces, each with a parameterization representing the target trajectory of positions or sound or light of an object in the environment. Each object $k$ and modality $m$ defines a goal space $\mathcal{G}^{k, m}$ containing goals $g$ of the form $f_{g}^{k, m}(\tau)=-\left|\varphi_{\psi_{1}^{k, m}(g)}(\tau)-\psi_{2}^{k, m}(g)\right|$ where $\psi_{2}^{k, m}(g)$ denotes the target trajectory of object $k$ in modality $m$ (positions, sound or light), and $\varphi_{\psi_{1}^{k, m}(g)}(\tau)$ denotes the features of trajectory $\tau$ of the same object and modality.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Architecture 1 Intrinsically Motivated Goal Exploration Process (IMGEP)</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Require</span><span class="o">:</span><span class="w"> </span><span class="n">Action</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">A</span><span class="o">}\),</span><span class="w"> </span><span class="n">State</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">S</span><span class="o">}\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">knowledge</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}=\</span><span class="n">varnothing</span><span class="o">\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">policies</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\)</span>
<span class="w">    </span><span class="n">Launch</span><span class="w"> </span><span class="n">asynchronously</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">loops</span><span class="o">:</span>
<span class="w">    </span><span class="n">loop</span><span class="w"> </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">Exploration</span><span class="w"> </span><span class="n">loop</span>
<span class="w">        </span><span class="n">Choose</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Execute</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">roll</span><span class="o">-</span><span class="n">out</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\),</span><span class="w"> </span><span class="n">observe</span><span class="w"> </span><span class="n">trajectory</span><span class="w"> </span><span class="o">\(\</span><span class="n">tau</span><span class="o">\)</span>
<span class="w">        </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">From</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="o">\(</span><span class="n">f_</span><span class="o">{</span><span class="n">g</span><span class="o">^{\</span><span class="n">prime</span><span class="o">}}(\</span><span class="n">tau</span><span class="o">)\)</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">computed</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">estimate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">fitness</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">experiment</span><span class="w"> </span><span class="o">\(\</span><span class="n">tau</span><span class="o">\)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">achieving</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">^{\</span><span class="n">prime</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="k">in</span><span class="w"> </span><span class="o">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\)</span>
<span class="w">        </span><span class="n">Compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">fitness</span><span class="w"> </span><span class="o">\(</span><span class="n">f</span><span class="o">=</span><span class="n">f_</span><span class="o">{</span><span class="n">g</span><span class="o">}(\</span><span class="n">tau</span><span class="o">)\)</span><span class="w"> </span><span class="n">associated</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Compute</span><span class="w"> </span><span class="kd">intrinsic</span><span class="w"> </span><span class="n">reward</span><span class="w"> </span><span class="o">\(</span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}=</span><span class="n">I</span><span class="w"> </span><span class="n">R</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="n">f</span><span class="o">)\)</span><span class="w"> </span><span class="n">associated</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">exploration</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\((\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">tau</span><span class="o">,</span><span class="w"> </span><span class="n">f</span><span class="o">)</span><span class="w"> </span><span class="o">\</span><span class="n">quad</span><span class="w"> </span><span class="o">\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">e</span><span class="o">.</span><span class="na">g</span><span class="o">.</span><span class="w"> </span><span class="n">fast</span><span class="w"> </span><span class="n">incremental</span><span class="w"> </span><span class="n">algo</span><span class="o">.</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">tau</span><span class="o">,</span><span class="w"> </span><span class="n">f</span><span class="o">,</span><span class="w"> </span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">knowledge</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(</span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">tau</span><span class="o">,</span><span class="w"> </span><span class="n">f</span><span class="o">,</span><span class="w"> </span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span>
<span class="w">    </span><span class="n">loop</span><span class="w"> </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">Exploitation</span><span class="w"> </span><span class="n">loop</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="n">quad</span><span class="w"> </span><span class="o">\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">e</span><span class="o">.</span><span class="na">g</span><span class="o">.</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">deep</span><span class="w"> </span><span class="n">NN</span><span class="o">,</span><span class="w"> </span><span class="n">SVMs</span><span class="o">,</span><span class="w"> </span><span class="n">GMMs</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}\)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span>
</code></pre></div>

<p>features, goals and goal spaces). Also, we assume that given a trajectory $\tau$, the agent can compute $f_{g}(\tau)$ for any $g \in \mathcal{G}$.</p>
<p>Given $\mathcal{S}, \mathcal{A}$ and $\mathcal{G}$, the agent explores the environment by sampling goals in $\mathcal{G}$ and searching for good solutions to those goals, and learns a goal-parameterized policy $\Pi\left(\boldsymbol{a}<em _boldsymbol_t="\boldsymbol{t">{\boldsymbol{t}+\mathbf{1}} \mid \boldsymbol{g}, \boldsymbol{s}</em><em _boldsymbol_t="\boldsymbol{t">{0}: \boldsymbol{t}+\mathbf{1}}, \boldsymbol{a}</em>\right)$ to reach any goal from any state.}_{0}: \boldsymbol{t}</p>
<p>We can then evaluate the agent's exploration and learning efficiency either by observing its behavior and estimating the diversity of its skills and the reached stepping-stones, or by "opening" agent's internal models and policies to analyze their properties.</p>
<h3>2.2 Algorithmic Architecture</h3>
<p>We present Intrinsically Motivated Goal Exploration Processes (IMGEP) as an algorithmic architecture that can be instantiated into many particular algorithms sharing several general principles (see pseudo-code in Architecture 1):</p>
<ul>
<li>The agent autonomously builds and samples goals as fitness functions, possibly using intrinsic rewards,</li>
<li>Two processes are running in parallel: 1) an exploration loop samples goals and searches for good solutions to those goals with the exploration policy; 2) an exploitation loop uses the data collected during exploration to improve the goal-parameterized policy over the goal space(s),</li>
<li>The data acquired when exploring solutions for a particular goal is reused to extract potential solutions to other goals.</li>
</ul>
<h3>2.3 Goal Exploration</h3>
<p>In the exploration loop, the agent samples a goal $g$, executes its exploration policy $\Pi_{\epsilon}$, and observes the resulting trajectory $\tau$. This new experiment $\tau$ can be used to:</p>
<ul>
<li>compute the fitness associated to goal $g$,</li>
<li>compute an intrinsic reward evaluating the interest of the choice of $g$,</li>
<li>update the goal policy (sampling strategy) using this intrinsic reward,</li>
<li>update the exploration policy $\Pi_{e}$ with a fast incremental learning algorithm,</li>
<li>update the learning database $\mathcal{E}$.</li>
</ul>
<p>Then, asynchronously, this learning database $\mathcal{E}$ can be used to learn a target policy $\Pi$ with a slower or more computationally demanding algorithm, but on the other end resulting in a more accurate policy. The goal space may also be updated based on this data.</p>
<h1>2.4 Intrinsic Rewards</h1>
<p>In goal exploration, a goal $g \in \mathcal{G}$ is chosen at each iteration. $\mathcal{G}$ may be infinite, continuous and of high-dimensionality, making the choice of goal important and non-obvious. Indeed, even if the fitness function $f_{g^{\prime}}(\tau)$ may give information about the fitness of a trajectory $\tau$ to achieve many goals $g^{\prime} \in \mathcal{G}$, the policy leading to $\tau$ has been chosen with the goal $g$ to solve in mind, thus it may not give as much information about other goals than the execution of another policy chosen when targeting other goals.</p>
<p>Intrinsic rewards provide a mean for the agent to self-estimate the expected interest of exploring particular goals for learning how to achieve all goals in $\mathcal{G}$. An intrinsic reward signal $r_{i}$ is associated to a chosen goal $g$, and based on a heuristic (denoted $I R$ ) such as outcome novelty, progress in reducing outcome prediction error, or progress in competence to solve problems (Oudeyer and Kaplan, 2007).</p>
<p>In the experiments of this paper, we use intrinsic rewards based on measuring the competence progress towards the self-generated goals, which has been shown to be particularly efficient for learning repertoires of high-dimensional robotics skills (Baranes and Oudeyer, 2013). Figure 1 shows a schematic representation of possible learning curves and the exploration preference of an agent with intrinsic rewards based on learning progress.</p>
<h2>3. Modular Population-Based IMAGEP Architecture</h2>
<p>In the previous section, we defined the most general IMAGEP architecture without specifying the implementation of its components such as goals and policies. We define here a particular IMAGEP architecture, used in the experiments of this paper, where the goal-parameterized policy $\Pi$ is based on a population of solutions, the goal space $\mathcal{G}$ is constructed in a modular manner from a set of objects and the exploration mutations is temporally modular through taking into account the movement of those objects. This particular architecture is called Modular Population-Based IMAGEP, and we detail its ingredients in the following sections. Its pseudo-code is provided in Architecture 2. Figure 2 summarizes the different components of Active Model Babbling (AMB), our implementation of the Modular Population-Based IMAGEP architecture, using learning progress for goal sampling and stepping-stone preserving mutations.</p>
<h3>3.1 Population-Based Meta-Policies $\Pi$ and $\Pi_{e}$</h3>
<p>In this version of the IMAGEP framework, the goal-parameterized policy $\Pi$ is based on a population of low-level policies. We consider that the starting state $s_{t_{0}}$ of a trajectory is characterized by a feature vector $c$ called context. The policy $\Pi$ is built from a set of low-level policies $\pi_{\theta}$ parameterized by $\theta \in \Theta$, and from a meta-policy $\Pi(\boldsymbol{\theta} \mid \boldsymbol{g}, \boldsymbol{c})$ which, given a goal and context, chooses the best policy $\pi_{\theta}$ to achieve the goal $g$. The policies $\pi_{\theta}\left(\boldsymbol{a}<em _boldsymbol_t="\boldsymbol{t">{\boldsymbol{t}+\mathbf{1}} \mid \boldsymbol{s}</em><em _boldsymbol_t="\boldsymbol{t">{0} ; \boldsymbol{t}+\mathbf{1}}, \boldsymbol{a}</em>\right)$ can be implemented for instance by stochastic black-box generators or small neural networks (see experimental section).}_{0} ; \boldsymbol{t}</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Schematic representation of possible learning curves for different goals and the associated exploration preference for an agent with intrinsic rewards based on learning progress. Left: schematic learning curves associated to 5 imaginary goals: the y axis represents the competence of the agent to achieve the goal ( 1 is perfect, 0 is chance level), and the x axis is training time on a goal. The blue, orange and green curves represent achievable goals, for which agent's competence increases with training, at different rates, and saturates after a long training time. The purple curve represents a goal on which the agent always has the same competence, with no progress. The red curve is the learning curve on an unreachable goal, e.g. moving an uncontrollable object. Right: exploration preference of an agent with a learning progress heuristic (competence derivative) to explore the 5 goals defined by the learning curves. At the beginning of exploration, the agent makes the most progress on goal blue so it prefers to train on this one, and then its preference will shift towards goals orange and green. The agent is making no progress on goal purple so will not choose to explore it, and goal red has a noisy but low estimated learning progress.</p>
<p>During the goal exploration loop, the main objective consists in collecting data that covers well the space of goals: finding $\theta$ parameters that yield good solutions to as many goals as possible. The exploration meta-policy $\Pi_{\epsilon}(\boldsymbol{\theta} \mid \boldsymbol{g}, \boldsymbol{c})$ is learned and used to output a distribution of policies $\pi_{\theta}$ that are interesting to execute to gather information for solving in context $c$ the self-generated goal $g$ and goals similar to $g$. To achieve the objective of collecting interesting data, the exploration meta-policy $\Pi_{\epsilon}$ must have fast and incremental updates. As the aim is to maximize the coverage of the space of goals, being very precise when targeting goals is less crucial than the capacity to update the meta-policy quickly and incrementally. In our experiments, the exploration meta-policy $\Pi_{\epsilon}(\boldsymbol{\theta} \mid \mathbf{g}, \mathbf{c})$ is implemented as a fast memory-based nearest neighbor search with a kd-tree (Bentley, 1975). More precisely, we record for each policy $\pi_{\theta}$ in our database the associated context $c$ it was used in and the outcome $o_{\tau}$ it produced. Then, given a new context $c^{\text {new }}$ and a goal $g$ to attain, which correspond to an (object-specific) outcome $o$ to produce, $\Pi_{\epsilon}$ selects for interaction the policy $\pi_{\theta^{\prime}}$ whose associated context and (object-specific) outcome is closest to $c^{\text {new }}$ and $g$ (using a nearest neighbor search in the concatenated context-outcome space). Note that $c$ is not used for goal selection, such that a single learning progress-based motivation level can be inferred per goal space.</p>
<p>On the contrary, the purpose of the target meta-policy $\Pi$ is to be used in exploitation mode: later on, it can be asked to solve as precisely as possible some goals $g$ with maximum fitness. As the training of this meta-policy can be done asynchronously from data collected by the goal exploration loop, this allows the use of slower training algorithms, possibly batch, that might generalize better, e.g. using Gaussian mixture models, support vector regression or (deep) neural networks. These differences justify the fact that IMGEP uses in general two different representations and learning algorithms for $\Pi_{\epsilon}$ and $\Pi$. This two-level learning scheme has similarities with the Complementary</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Summary of AMB, our modular population-based IMGEP implementation. At each iteration, the agent observes the current context $c$ and chooses a goal space (an object) to explore based on intrinsic rewards (the learning progress to move each object) with $\Gamma$. Then a particular goal $g$ for the chosen object is sampled with $\gamma_{k}$, for instance to push the left joystick to the right. The agent chooses the best policy parameters $\theta$ to reach this goal, with the exploration meta-policy $\Pi_{\epsilon}$, and using an internal model of the world implementing object-centered modularity in goals and mutations. The agent executes policy $\pi_{\theta}$, observes the trajectory $\tau$ and compute the outcome $o_{\tau}$ encoding the movement of each object. Finally, each component is updated with the result of this experiment.</p>
<p>Learning Systems Theory used to account for the organization of learning in mammalian brains (Kumaran et al., 2016). To keep our experiments tractable and focus on studying intrinsically motivated exploration, we consider a simple target meta-policy $\Pi$, corresponding to our exploration meta-policy without mutations mechanisms (section 3.3).</p>
<h1>3.2 Object-Centered Modular Goal Construction</h1>
<p>In the IMGEP architecture, the agent builds and samples goals autonomously. Here, we consider the particular case where the agent builds several goal spaces that correspond to moving each object in the environment. Each goal space represents features of the movement of an object in the environment, such as its end position in $\tau$ or its full trajectory.</p>
<p>We define the outcome $o_{\tau} \in \mathcal{O}$ of an experiment $\tau$ as the features of the movement of all objects, so that $\mathcal{O}=\prod_{k} \mathcal{O}^{k}$ where $o_{\tau}^{k} \in \mathcal{O}^{k}$ are the features of object $k$. Those features come from a perceptual system that may be given to the agent or learned by the agent. From feature space $\mathcal{O}^{k}$, the agent can autonomously generate a corresponding goal space $\mathcal{G}^{k}$ that contains fitness functions of the form $f_{g}(\tau)=-\left|g-o_{\tau}^{k}\right|<em k="k">{k}$. The norm $|\cdot|</em>$.}$ is a distance in the space $O^{k}$, which can be normalized to be able to compare the fitness of goals across goal spaces. The goal space is thus modular, composed of several object-related subspaces: $\mathcal{G}=\bigcup_{k} \mathcal{G}^{k</p>
<h1>Architecture 2 Modular Population-Based IMGEP</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Require</span><span class="o">:</span><span class="w"> </span><span class="n">Action</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">A</span><span class="o">}\),</span><span class="w"> </span><span class="n">State</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">S</span><span class="o">}\),</span><span class="w"> </span><span class="n">Context</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">C</span><span class="o">}\),</span><span class="w"> </span><span class="n">Outcome</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">O</span><span class="o">}\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">knowledge</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}=\</span><span class="n">varnothing</span><span class="o">\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\),</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">policies</span><span class="w"> </span><span class="o">\(\</span><span class="n">gamma_</span><span class="o">{</span><span class="n">k</span><span class="o">}\)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">meta</span><span class="o">-</span><span class="n">policies</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\)</span>
<span class="w">    </span><span class="n">Launch</span><span class="w"> </span><span class="n">asynchronously</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">loops</span><span class="o">:</span>
<span class="w">    </span><span class="n">loop</span><span class="w"> </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">Exploration</span><span class="w"> </span><span class="n">loop</span>
<span class="w">        </span><span class="n">Observe</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">\(</span><span class="n">c</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Choose</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}^{</span><span class="n">k</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Choose</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}^{</span><span class="n">k</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">gamma_</span><span class="o">{</span><span class="n">k</span><span class="o">}\)</span>
<span class="w">        </span><span class="n">Choose</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="o">\(\</span><span class="n">theta</span><span class="o">\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">explore</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">\(</span><span class="n">c</span><span class="o">\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\)</span>
<span class="w">        </span><span class="n">Execute</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">roll</span><span class="o">-</span><span class="n">out</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">\(\</span><span class="n">pi_</span><span class="o">{\</span><span class="n">theta</span><span class="o">}\),</span><span class="w"> </span><span class="n">observe</span><span class="w"> </span><span class="n">trajectory</span><span class="w"> </span><span class="o">\(\</span><span class="n">tau</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Compute</span><span class="w"> </span><span class="n">outcome</span><span class="w"> </span><span class="o">\(</span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">}\)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">trajectory</span><span class="w"> </span><span class="o">\(\</span><span class="n">tau</span><span class="o">\)</span>
<span class="w">        </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">From</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="n">on</span><span class="o">,</span><span class="w"> </span><span class="o">\(</span><span class="n">f_</span><span class="o">{</span><span class="n">g</span><span class="o">^{\</span><span class="n">prime</span><span class="o">}}(\</span><span class="n">tau</span><span class="o">)\)</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">computed</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">estimate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">fitness</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">experiment</span><span class="w"> </span><span class="o">\(\</span><span class="n">tau</span><span class="o">\)</span><span class="w"> </span><span class="k">for</span>
<span class="w">            </span><span class="n">achieving</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">^{\</span><span class="n">prime</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="k">in</span><span class="w"> </span><span class="o">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\)</span>
<span class="w">        </span><span class="n">Compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">fitness</span><span class="w"> </span><span class="o">\(</span><span class="n">f</span><span class="o">=</span><span class="n">f_</span><span class="o">{</span><span class="n">g</span><span class="o">}(\</span><span class="n">tau</span><span class="o">)\)</span><span class="w"> </span><span class="n">associated</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Compute</span><span class="w"> </span><span class="kd">intrinsic</span><span class="w"> </span><span class="n">reward</span><span class="w"> </span><span class="o">\(</span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}=</span><span class="n">I</span><span class="w"> </span><span class="n">R</span><span class="o">\</span><span class="n">left</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">c</span><span class="o">,</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">theta</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">},</span><span class="w"> </span><span class="n">f</span><span class="o">\</span><span class="n">right</span><span class="o">)\)</span><span class="w"> </span><span class="n">associated</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">\(</span><span class="n">c</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">exploration</span><span class="w"> </span><span class="n">meta</span><span class="o">-</span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">c</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">theta</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">tau</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">}\</span><span class="n">right</span><span class="o">)</span><span class="w"> </span><span class="o">\</span><span class="n">quad</span><span class="w"> </span><span class="o">\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">e</span><span class="o">.</span><span class="na">g</span><span class="o">.</span><span class="w"> </span><span class="n">fast</span><span class="w"> </span><span class="n">incr</span><span class="o">.</span><span class="w"> </span><span class="n">algo</span><span class="o">.</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">gamma_</span><span class="o">{</span><span class="n">k</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">c</span><span class="o">,</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">},</span><span class="w"> </span><span class="n">f</span><span class="o">,</span><span class="w"> </span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">c</span><span class="o">,</span><span class="w"> </span><span class="n">k</span><span class="o">,</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">},</span><span class="w"> </span><span class="n">f</span><span class="o">,</span><span class="w"> </span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">knowledge</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(</span><span class="n">c</span><span class="o">,</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">theta</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">tau</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">},</span><span class="w"> </span><span class="n">f</span><span class="o">,</span><span class="w"> </span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span>
<span class="w">    </span><span class="n">loop</span><span class="w"> </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">Exploitation</span><span class="w"> </span><span class="n">loop</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">meta</span><span class="o">-</span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="n">quad</span><span class="w"> </span><span class="o">\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">e</span><span class="o">.</span><span class="na">g</span><span class="o">.</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">DNN</span><span class="o">,</span><span class="w"> </span><span class="n">SVM</span><span class="o">,</span><span class="w"> </span><span class="n">GMM</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span>
</code></pre></div>

<p>With this setting, goal sampling is hierarchical in the sense that the agent first chooses a goal space $\mathcal{G}^{k}$ to explore with a goal space policy $\Gamma$ and then a particular goal $g \in \mathcal{G}^{k}$ with the corresponding goal policy $\gamma_{k}$. Those two levels of choice can make use of self-computed intrinsic rewards $r_{i}$ (see Sec. 2.4).</p>
<p>Given an outcome $o_{\tau}$, the fitness $f_{g}(\tau)$ can thus be computed by the agent for all goals $g \in \mathcal{G}$ and at any time after the experiment $\tau$. For instance, if while exploring the goal of moving object A to the left, object B moved to the right, that outcome can be taken into account later when the goal is to move object B.</p>
<h3>3.3 Object-Centered Temporal Modularity: Stepping-Stone Preserving Mutations</h3>
<p>When targeting a new goal $g$, the internal model (a memory-based nearest neighbor search in our experiments) infers the best policy $\pi_{\theta}$ to reach the goal $g$. The exploration meta-policy $\Pi_{\epsilon}$ then performs a mutation on $\theta$ in order to explore new policies. This mutation step can be seen as analogous to existing approaches injecting noise in parametric RL policies to foster exploration (Fortunato et al., 2018; Plappert et al., 2018). The mutation operator could just add a random noise on the parameters $\theta$, however, those parameters do not all have the same influence on the execution of the policy, in particular with respect to time. In our implementations, the parameters are sequenced in time, with some parameters influencing more the beginning of the policy roll-out and some more the end of the trajectory. However, in the context of tool use, the reaching or grasping of a tool is necessary for executing a subsequent action on an object. A random mutation of policy parameters, irrespective of the moment when the tool is grasped, can result in an action where the agent do not grasp the tool and thus cannot explore the corresponding object.</p>
<p>The Stepping-Stone Preserving Mutation operator (SSPMutation) analyzes the trajectory of the target object while the previous motor policy $\pi_{\theta}$ was run, to find the moment when the object</p>
<p>started to move. The operator does not change the variables of $\theta$ concerning the movement before the object moved and mutates the variables of $\theta$ concerning the movement after the object moved (see an example mutation in Fig. 15). When the goal of the agent is to move the tool and it already succeeded to move the tool in the past with policy $\pi_{\theta}$, then the application of this mutation operator changes the behavior of the agent only when the tool start to move, which makes the agent explore with the tool once grasped and avoid missing the tool. Similarly, when the goal of the agent is to move a toy controlled by a tool, the mutation changes the behavior only when the toy starts to move, which makes the agent grasp the tool and reach the toy before exploring new actions, so that the agent do not miss the tool nor the toy. The idea of this stepping-stone preserving operator is similar to the Go-Explore approach (Ecoffet et al., 2021).</p>
<h1>3.4 Active Model Babbling (AMB) and Random Model Babbling (RMB)</h1>
<p>The Modular Population-Based IMAGEP architecture gives a high-level description of the learning agent with a population-based policy and an object-centered modularity in goals and mutations. Each component of this architecture may be instantiated in various ways. For instance, in the main loop of Architecture 2, many aspects are not constrained such as how the goal is chosen, how the parameters $\theta$ are computed, how the policies $\pi_{\theta}$ are implemented, how the intrinsic rewards are defined.</p>
<p>An important contribution of the present work is to design and showcase the effectiveness of one particular instantiation of modular population-based IMGEPs, called Active Model Babbling (AMB, see fig. 2), which uses a learning-progress based mechanism for sampling goal spaces (see section 4.2.3 for examples of implementations) as well as the stepping-stone preserving mutations mechanism described in section 3.3 (see section 4.2.2 for implementation details).</p>
<p>In addition to AMB, we also present an alternative variant, called Random Model Babbling (RMB), which also uses the stepping-stone preserving mutations, but implements a simpler goal space policy: instead of using learning progress estimates, goal spaces are selected randomly throughout training. Although apparently simplistic, this approach can perform surprisingly well, especially when all goal spaces are relevant, i.e. when there are no distractors (see figure 20).</p>
<p>In Section 4.2, we detail implementations of these types of Modular Population-Based IMAGEP architectures, as well as other baselines.</p>
<h2>4. Experiments</h2>
<p>In this section, we evaluate the Modular Population-Based IMAGEP architecture by designing several algorithmic implementations and several environments suitable for curriculum learning, where the exploration of a task brings information to later solve other tasks. In particular, we study environments where agents discover objects that can be used as tools to move other objects. A good exploration of a tool and of its functioning will yield a better exploration of the objects on which this tool can act. Those tasks provide the opportunity for an intrinsically motivated agent to build on the skills it has learned to explore and learn new skills on its own.</p>
<p>Here, we first describe three tool-use learning environments and we detail our implementations of IMAGEP and of several control conditions. Then, we study the behavior of the different agents in the different environments depending on the learning architecture. We investigate in particular the benefits of a modular representation of the sensory feedback with goals based on objects, and how the exploration mutations can take into account the movement of the goal object. We further examine how and in which conditions the intrinsic motivation component of the IMAGEP architecture improves the learning of skills that can be reused, such as using a tool to move an object.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: 2D Simulated Tool-Use Environment. A simulated robotic arm with a gripper can grab sticks and move toys. The gripper has to close near the handle of a stick to grab it. One magnetic toy and one Velcro toy are reachable with their corresponding stick. Other toys cannot be moved (static or too far away). The cat and the dog are distractors: they move randomly, independently of the arm.</p>
<h1>4.1 Tool-Use Environments</h1>
<p>We design three tool-use environments. The first one is a 2 D simulated robotic arm with 3 joints and a gripper that can grab sticks and move toys. It is a simple environment with no physics and only 2D geometric shapes so very fast to execute. The second environment is a Minecraft scene where an agent is able to move, grab and use tools such as a pickaxe to break blocks. The third one is a real robotic setup with a Torso robot moving its arm that can reach joysticks controlling a toy robot. This setup has complex high-dimensional motor and sensory spaces with noise both in the robot physical arm and in the interaction between objects such as its hand and the joysticks. It is a high-dimensional and noisy environment with a similar stepping-stone structure as the robotic environments but with a completely different sensorimotor setup. The code of the different environments and experiments is available on GitHub ${ }^{1}$.</p>
<h3>4.1.1 2D Simulated Tool-Use Environment</h3>
<p>In the 2D Simulated Environment (see Fig. 3), the learning agent controls a robotic arm with a gripper, that can grab one of two sticks, one with a magnet at the end and one with Velcro, that can themselves be used to move several magnets or Velcro toys. Some other objects cannot be moved, they are called static distractors, and finally a simulated cat and dog are randomly moving in the scene, they are random distractors.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The 2D robotic arm has 3 joints that can rotate from $-\pi$ rad to $\pi$ rad. The length of the 3 segments of the arm are $0.5,0.3$ and 0.2 so the length of the arm is 1 unit. The starting position of the arm is vertical with joints at position 0 rad and its base is fixed at position $(0,0)$. The gripper $g r$ has 2 possible positions: open $(g r \geqslant 0)$ and closed $(g r&lt;0)$. The robotic arm has 4 degrees of freedom represented by a vector in $[-1,1]^{4}$.</p>
<p>Two sticks of length 0.5 can be grasped by the handle side (orange side) in order to catch an out-of-reach object. The magnetic stick can catch magnetic objects (in blue), and the other stick has a Velcro tape to catch Velcro objects (in green). If the gripper closes near the handle of one stick, this stick is considered grasped and follows the gripper's position and the orientation of the arm's last segment until the gripper opens. If the other side of a stick reaches a matching object (magnetic or Velcro), the object then follows the stick. There are three magnetic objects and three Velcro objects, but only one of each type is reachable with its stick. A simulated cat and dog are following a random walk, they have no interaction with the arm nor with other object. Finally, four static black squares have also no interaction with other objects. The arm, tools and other objects are reset to their initial state at the end of each iteration of 50 steps.</p>
<p>The agent receives a sensory feedback representing the result of its actions. This feedback (or outcome) is either composed of the position of each object at 5 time points during the 50 steps trajectory, or just the end state of each object, depending on the experiments. First, the hand is represented by its $X$ and $Y$ position and the aperture of the gripper ( 1 or -1 ). The sticks are represented by the $X$ and $Y$ position of their tip. Similarly, each other object is represented by their $X$ and $Y$ positions. Each of the 15 objects defines a sensory space $S_{i}$ ( 10 of those objects are uncontrollable distractors). The total sensory space $S$ has either 155 dimensions if trajectories are represented, or 31 dimensions if only the end state of each object is represented.</p>
<h1>4.1.2 Minecraft Mountain Cart</h1>
<p>The Minecraft Mountain Cart (MMC) extends the famous Mountain Car control benchmark in a 3D environment with a multi-goal setting (see Fig. 4).</p>
<p>In this episodic task, the agent starts on the left of the rectangular arena and is given ten seconds ( 40 steps) to act on the environment using 2 continuous commands: move and strafe, both using values in $[-1 ; 1] . m o v e(1)$ moves the agent forward at full speed, move(-0.1) moves the agent slowly backward, etc. Similarly strafe(1) moves the agent left at full speed and strafe(-0.1) moves it slowly to the right. Additionally, a third binary action allows the agent to use the currently handled tool.</p>
<p>The first challenge of this environment is to learn how to navigate within the arena's boundaries without falling in water holes (from which the agent cannot get out). Proper navigation might lead the agent to discover one of the two tools of the environment: a shovel and a pickaxe. The former is of no use but the latter enables to break diamond blocks located further ahead in the arena. A last possible interaction is for the agent to get close enough to the cart to move it along its railroad. If given enough speed, the cart is able to climb the left or right slope. The height and width of these slopes were made in such a way that an agent simply hitting the cart at full speed will not provide enough inertia for the cart to climb the slope. The agent must at least partially support the cart along the track to propel it fast enough to fully climb the slope.</p>
<p>The outcome of an episode is a vector composed of the end position of the agent (2D), shovel (2D), pickaxe (2D), cart (1D) and 3 distractors (2D each) positions along with a binary vector (5D) encoding the 5 diamond blocks' states ( 3 objects out of 8 are uncontrollable distractors).</p>
<p>This environment is interesting to study modular IMGEP approaches since it is composed of a set of linked tasks of increasing complexity. Exploring how to navigate will help to discover the tools and, eventually, will allow to break blocks and move the cart.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Minecraft Mountain Cart Environment. If the agent manages to avoid falling into water holes it may retrieve and use a pickaxe to break diamond blocks and access the cart. A shovel is also located in the arena and serves as a controllable distractor.</p>
<h1>4.1.3 Robotic Tool-Use Environment</h1>
<p>In order to benchmark different learning algorithms in a realistic robotic environment with highdimensional action and outcome spaces, we designed a real robotic setup composed of a humanoid arm in front of joysticks that can be used as tools to act on other objects (see Fig. 5). We recorded a video of an early version of the experimental setup ${ }^{2}$.</p>
<p>A Poppy Torso robot (the learning agent) is mounted in front of two joysticks and explores with its left arm. A Poppy Ergo robot (seen as a robotic toy) is controlled by the right joystick and can push a ball that controls some lights and sounds. Poppy is a robust and accessible open-source 3D printed robotic platform (Lapeyre et al., 2014).</p>
<p>The left arm has 4 joints, with a hook at the tip of the arm. A trajectory of the arm is here generated by radial basis functions with 5 parameters on each of the 4 degrees of freedom ( 20 parameters in total).</p>
<p>Two analogical joysticks (Ultrastick 360) can be reached by the left arm and moved in any direction. The right joystick controls the Poppy Ergo robotic toy, and the left joystick do not control any object. The Poppy Ergo robot has 6 motors, and moves with hardwired synergies that allow control of rotational speed and radial extension.</p>
<p>A tennis ball is freely moving in the blue arena which is slightly sloped so that the ball comes close to the center at the end of a movement. The speed of the ball controls (above a threshold) the intensity of the light of a LED circle around the arena. Finally, when the ball touches the border of the arena, a sound is produced and varied in pitch depending on ball position.</p>
<p>Several other objects are included in the environment, with which the agent cannot interact. Two Ergo robots (2D objects) are moving randomly, independently of the agent. Six objects are static: the right hand (3D) of the robot that is disabled in this experiment, the camera recording the ball</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Robotic Tool-Use Environment. Left: a Poppy Torso robot (the learning agent) is mounted in front of two joysticks that can be used as tools to act on other objects: a Poppy Ergo robotic toy and a ball that can produce light and sound. Right: 6 copies of this setup are running in parallel to gather more data. Several Ergo robots are placed between robots: they act as distractors that move randomly, independently of the agents.
trajectory (3D), the blue circular arena (2D), an out-of-reach yellow toy (2D), the red button also out-of-reach (2D) and the lamp (2D). All distractor objects are reset after each roll-out.</p>
<p>The context $c$ of this environment represents the current configuration of objects in the scene. In practice, since only the Ergo and ball are not reset after each roll-out, this amounts to measuring the rotation angle of the Ergo and of the ball around the center of the arena.</p>
<p>The agent is given a perceptual system providing sensory feedback that represents the trajectories of all objects in the scene. First, the 3D trajectory of the hand is computed through a forward model of the arm as its $x, y$ and $z$ position. The 2D states of each joystick and of the Ergo are read by sensors, and the position of the ball retrieved through the camera. The states of the 1D intensity of the light and the 1D pitch of the sound are computed from the ball position and speed. Each of the 15 objects defines a sensory space $S_{i}$ representing its trajectory ( 8 of those objects are uncontrollable distractors). The total sensory space $S$ has 310 dimensions.</p>
<h1>4.2 Implementation of the Modular Population-Based IMAGEP Architecture</h1>
<p>In the following subsections, we detail our implementations of the algorithmic parts of the modular population-based IMAGEP architecture (see architecture 2).</p>
<h3>4.2.1 Motor Policy $\pi_{\theta}$</h3>
<p>In the 2D Simulated environment and the Robotic environment, we implement the motor policies with Radial Basis Functions (RBF). We define 5 Gaussian basis functions with the same shape ( $\sigma=5$ for a 50 steps trajectory in the 2 D environment and $\sigma=3$ for 30 steps in the Robotic environment) and with equally spaced centers (see Fig. 6). The movement of each joint is the result of a weighted sum of the product of 5 parameters and the 5 basis. The total vector $\theta$ has 20 parameters, in both the 2D Simulated and the Robotic environment. In the 2D environment, the fourth joint is a gripper that is considered open if its angle is positive and closed otherwise.</p>
<p>In the Minecraft Mountain Cart environment, trajectories are sampled in a closed-loop fashion using neural networks. The observation vector has the same structure as the outcome vector: it provides the current positions of all objects normalized in $[-1 ; 1]$ (18D). Each neural network is composed of one hidden layer of 64 Relu units and a 3D output with tanh activation functions. The</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Implementation of motor policies $\pi_{\theta}$ through Radial Basis Functions. (a) 5 Gaussian bases with different centers but same shape. (b) the movement of each joint is the result of a weighted sum of the product of 5 parameters and the 5 basis. The total vector $\theta$ has 20 parameters, in both the 2D Simulated and the Robotic environment.</p>
<p>1411 policy parameters are initialized using the initialization scheme of He et al. (2015). Note that in our experiments, neural networks are not trained through backpropagation: as for RBF policies, their parameters are mutated. Our mutation-based approach is close to neuroevolution approaches (Stanley et al., 2019).</p>
<h1>4.2.2 Stepping-Stone Preserving Mutations</h1>
<p>The Stepping-Stone Preserving Mutation operator (SSPMutation) does not change the variables of $\theta$ concerning the movement before the object moved and modifies the variables of $\theta$ concerning the movement after the object moved. SSPMutation adds a Gaussian noise around those values of $\theta$ in the 2D simulated environment $(\sigma=0.05)$ and in Minecraft Mountain Cart $(\sigma=0.3)$, or adds the Gaussian noise around the previous motor positions (in the robotic environment with joysticks). In the experimental section we compare it to the FullMutation operator that adds a Gaussian noise to $\theta$ irrespective of the moment when the target object moved.</p>
<h3>4.2.3 Goal Space Policy $\Gamma$</h3>
<p>The agent estimates its learning progress globally in each goal space (or for each model learned). At each iteration, the context $c$ is observed, a goal space $k$ is chosen by $\Gamma$ and a random goal $g$ is sampled by $\gamma_{k}$ in $\mathcal{G}^{k}$ (corresponding to a fitness function $f_{g}$ ). Then, in $80 \%$ of the iterations, the agent uses $\Pi_{c}(\boldsymbol{\theta} \mid g, c)$ to generate with exploration a policy $\theta$ and does not update its progress estimation. In the other $20 \%$, it uses $\Pi$, without exploration, to generate $\theta$ and updates its learning progress estimation in $\mathcal{G}^{k}$, with the estimated progress in reaching $g$. To estimate the learning progress $r_{i}$ made to reach the current goal $g$, the agent compares the outcome $o_{\tau}$ with the outcome $o_{\tau}^{\prime}$ obtained for the previous context and goal $\left(g^{\prime}, c^{\prime}\right)$ most similar (Euclidean distance) to $(g, c)$ : $r_{i}=f_{g}(\tau)-f_{g}\left(\tau^{\prime}\right)$. Finally, $\Gamma$ implements a non-stationary bandit algorithm to sample goal spaces. The bandit keeps track of a running average $r_{i}^{k}$ of the intrinsic rewards $r_{i}$ associated to the current goal space $\mathcal{G}^{k}$. With probability $20 \%$, it samples a random space $\mathcal{G}^{k}$, and with probability $80 \%$, the probability to sample $\mathcal{G}^{k}$ is proportional to $r_{i}^{k}$ in the 2D Simulated and Minecraft environments, or $\exp \left(\frac{r_{i}^{k}}{\sum_{k} r_{i}^{k}}\right)$ if $r_{i}^{k}&gt;0$ and 0 otherwise, in the Robotic environment.</p>
<h1>4.2.4 Control Conditions</h1>
<p>We design several control conditions.In the Random Model Babbling (RMB) condition, the choice of goal space is random: $\Gamma(\mathbf{k} \mid \mathbf{c})$, and $\gamma_{k}(\mathbf{g} \mid \mathbf{c})$ for each $k$ are always uniform distributions. Agents in the Single Goal Space (SGS) condition always choose the same goal space, of high interest to the engineer: the magnet toy in the 2D Simulated environment, and the ball in the robotic environment. The Fixed Curriculum (FC) condition defines $\Gamma$ as a curriculum sequence engineered by hand: the agents explore objects in a sequence from the easiest to discover to the most complex object while ignoring distractors. The conditions SGS and FC are thus extrinsically motivated controls. We define the Flat Random Goal Babbling (FRGB) condition with a single outcome/goal space containing all the variables of all objects, to compare modular and non-modular representations of the environment. The agents in this condition choose random goals in this space, and use the FullMutation operator. Finally, agents in the Random condition always choose random motor policies $\theta$.</p>
<h3>4.3 Results</h3>
<p>In this section we show the results of several experiments with the three environments and the different learning conditions. We first study in details the Active Model Babbling (AMB) learning algorithm, a modular implementation of the IMGEP architecture. Then, in order to understand the contribution of the different components of this learning algorithm, we compare it to several controls (or ablations): without a modular representation of goals, without the goal sampling based on learning progress, or without the stepping-stone preserving mutation operator. In those experiments, goals are sampled in spaces representing the sensory feedback from the environment. We thus compare several possible encodings of the feedback: with the trajectory of each object or with only the end point of the trajectories. We included distractors that cannot be controlled by the learning agent in the three tool-use environments. We also test the learning conditions with and without distractors to evaluate their robustness to distractors.</p>
<h3>4.3.1 Exploration Measure and Summary Results</h3>
<p>Table 1 shows a summary of the exploration results at the end of the runs, in all conditions in all spaces of all environments, We give the 25,50 and 75 percentiles of the exploration results of all seeds. Exploration measures the percentage of reached cells in a discretization of each goal space. The best condition in each space is highlighted in bold, based on Welch's t-tests (with threshold $p&lt;0.05$ ): if several conditions are not significantly different, they are all highlighted. In the 2 D Simulated environment, there are 100 seeds for each condition, and the exploration measures the number of cells reached in a discretization of the 2D space of the end position of each object with 100 bins on each dimension. In the Minecraft environment, there are 20 runs for conditions Random, SGS, FRGB, FC and 42 for AMB and RMB. The exploration metric for the agent, pickaxe and shovel spaces is the number of reached cells in a discretization of the 2D space in 450 bins ( 15 on the x axis, 30 on the y axis). The same measure is used for the block space, which is discrete with 32 possible combinations. For the cart space we measure exploration as the number of different outcomes reached. In the Robotic environment, there are 6 runs with different seeds for condition SGS, 8 for FRGB, 16 for RMB, 23 for AMB, 12 for FC and 6 for Random, and the exploration also measures the number of cells reached in a discretization of the space of the end position of each object with 1000 bins in 1D, 100 bins on each dimension in 2D, and 20 bins in 3D.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Examples of exploration map of one IMGEP agent in each environment. (a) in the 2D Simulated Environment, we plot the position of the reachable magnet toy at the end of each iteration with a blue point, and the Velcro toy in green. (b) in Minecraft Mountain Cart we plot the end position of the agent, the agent with pickaxe, the agent with shovel, and the cart. (c) in the Robotic environment, the position of the ball is plotted when it moved in the arena.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Env, Space</th>
<th style="text-align: center;">Condition</th>
<th style="text-align: center;">Rdm</th>
<th style="text-align: center;">SGS</th>
<th style="text-align: center;">Flat</th>
<th style="text-align: center;">RMB</th>
<th style="text-align: center;">AMB</th>
<th style="text-align: center;">FC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2D Simulated Environment</td>
<td style="text-align: center;">Magnet Tool</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">$8.0,11,13$</td>
<td style="text-align: center;">$33,36,39$</td>
<td style="text-align: center;">57,61,65</td>
<td style="text-align: center;">61,67,70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Magnet Toy</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">$0,0,5.0$</td>
<td style="text-align: center;">0,3.0,16</td>
<td style="text-align: center;">0,3.0,19</td>
</tr>
<tr>
<td style="text-align: center;">Minecraft <br> Mountain <br> Cart</td>
<td style="text-align: center;">Agent Pos.</td>
<td style="text-align: center;">28,29,30</td>
<td style="text-align: center;">29,29,30</td>
<td style="text-align: center;">$34,36,40$</td>
<td style="text-align: center;">$48,50,54$</td>
<td style="text-align: center;">$55,58,61$</td>
<td style="text-align: center;">59,63,67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Shovel</td>
<td style="text-align: center;">$5,5,6$</td>
<td style="text-align: center;">$5,6,7$</td>
<td style="text-align: center;">$8,11,13$</td>
<td style="text-align: center;">$25,27,30$</td>
<td style="text-align: center;">$32,34,37$</td>
<td style="text-align: center;">34,37,42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pickaxe</td>
<td style="text-align: center;">$6,6,7$</td>
<td style="text-align: center;">$6,7,8$</td>
<td style="text-align: center;">$11,15,19$</td>
<td style="text-align: center;">$33,35,39$</td>
<td style="text-align: center;">$41,45,48$</td>
<td style="text-align: center;">43,51,61</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Blocks</td>
<td style="text-align: center;">$3,3,3$</td>
<td style="text-align: center;">$3,3,3$</td>
<td style="text-align: center;">$3,11,19$</td>
<td style="text-align: center;">$69,77,84$</td>
<td style="text-align: center;">73,84,93</td>
<td style="text-align: center;">100,100,100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cart</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,1</td>
<td style="text-align: center;">$5,162,409$</td>
<td style="text-align: center;">$56,360,886$</td>
<td style="text-align: center;">386,787,1207</td>
</tr>
<tr>
<td style="text-align: center;">Robotic <br> Environment</td>
<td style="text-align: center;">Hand</td>
<td style="text-align: center;">24,24,25</td>
<td style="text-align: center;">18,19,20</td>
<td style="text-align: center;">20,21,22</td>
<td style="text-align: center;">22,24,25</td>
<td style="text-align: center;">22,23,24</td>
<td style="text-align: center;">21,22,23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L. Joystick</td>
<td style="text-align: center;">$4.2,4.7,5.9$</td>
<td style="text-align: center;">$1.9,3.3,4.6$</td>
<td style="text-align: center;">$0.1,0.1,0.3$</td>
<td style="text-align: center;">$15,18,19$</td>
<td style="text-align: center;">20,22,26</td>
<td style="text-align: center;">23,26,29</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R. Joystick</td>
<td style="text-align: center;">$0.6,0.9,1.0$</td>
<td style="text-align: center;">$0.3,0.4,0.5$</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">$10,11,13$</td>
<td style="text-align: center;">16,18,22</td>
<td style="text-align: center;">$15,17,18$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ergo</td>
<td style="text-align: center;">$0.2,0.3,0.4$</td>
<td style="text-align: center;">$0.1,0.1,0.2$</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">$1.2,1.5,1.7$</td>
<td style="text-align: center;">$1.5,1.7,1.8$</td>
<td style="text-align: center;">$1.7,1.7,1.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ball</td>
<td style="text-align: center;">$0,0,0.1$</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">$0.8,1.0,1.0$</td>
<td style="text-align: center;">$0.9,1.1,1.2$</td>
<td style="text-align: center;">$0.9,0.9,1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Light</td>
<td style="text-align: center;">$0.1,0.1,0.1$</td>
<td style="text-align: center;">$0.1,0.2,0.2$</td>
<td style="text-align: center;">$0.1,0.1,0.1$</td>
<td style="text-align: center;">$0.8,1.8,3.0$</td>
<td style="text-align: center;">2.0,3.6,4.9</td>
<td style="text-align: center;">$1.8,2.2,3.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sound</td>
<td style="text-align: center;">$0.1,0.1,0.1$</td>
<td style="text-align: center;">$0.1,0.1,0.1$</td>
<td style="text-align: center;">$0.1,0.1,0.1$</td>
<td style="text-align: center;">$0.8,1.1,2.6$</td>
<td style="text-align: center;">$1.7,2.8,3.6$</td>
<td style="text-align: center;">$1.2,1.6,2.3$</td>
</tr>
</tbody>
</table>
<p>Table 1: Exploration results in all environments and conditions.</p>
<h1>4.3.2 Intrinsically Motivated Goal Exploration</h1>
<p>Here we study in detail the Active Model Babbling (AMB) learning algorithm. AMB agents encode the sensory feedback about objects with a modular representation: each object is associated with one independent learning module. At each iteration, they first select an object to explore, then a particular goal to reach for this object. They execute a motor policy to reach this goal, and observe</p>
<p>the outcome. The selection of the object to explore is based on a self-estimation of the learning progress made to move each object according to chosen goals. The AMB algorithm is thus a modular implementation of the IMGEP architecture.</p>
<p>Exploration Maps - We first plot examples of exploration results as cumulative exploration maps, one per environment. Those maps show all the positions where one AMB agent succeeded to move objects.</p>
<p>Fig. 7(a) shows the position of the reachable toys of the 2 D simulated environment at the end of each iteration in one trial of intrinsically motivated goal exploration. The reachable area for those two toys is the inside the circle of radius 1.5 and center 0 . We can see that in 100 k iterations, the agent succeeded to transport the toys in many places in this area. The experiments with other seeds are very similar. Fig. 7(b) shows an exploration map of a typical run in Minecraft Mountain Cart after 40 k iterations. As you can see the agent successfully managed to (1) navigate within the arena boundaries, (2) move the pickaxe and shovel, (3) use the pickaxe to break blocks and (4) move the cart located behind these blocks. An example in the robotic environment is shown in Fig. 7(c) where we plot the position of the ball when it moved in the first $10 k$ iterations of the exploration of one agent.</p>
<p>Overall, they show that IMGEP agents discovered how to use the different tools in each environment within the time limit: the sticks to grab the toys in the 2 D simulated environment, the pickaxe to mine blocks to reach the cart in Minecraft Mountain Cart, the joysticks to move the toy and push the ball in the robotic experiment.</p>
<p>Discoveries - In order to understand the tool-use structure of the exploration problem in each environment, we can look in more details how agents succeeded to move objects while exploring other objects. Indeed, to the agents starting to explore, tools are objects like any other object (e.g. the hand, the stick and the ball have the same status). However, if a tool needs to be used to move another object, then this tool will be discovered before that object, so the exploration of this tool is a stepping-stone giving more chances to discover novelty with that object than the exploration of any other object. To quantify these dependencies between objects in our tool-use environments, we show the proportion of movements where an object of interest has been moved depending on the currently explored object.</p>
<p>Concerning the 2D simulated environment, Fig. 8 shows the proportion of the iterations with a goal in a given space that allowed to move (a) the magnet tool, (b) the magnet toy, in 10 runs with different seeds. First, random movements of the arm have almost zero chances to reach the magnet tool or toy. Exploring movements of the hand however have about $1.5 \%$ chances to move the magnet tool, but still almost zero chances to reach the toy. Exploring the magnet tool makes this tool move in about $93 \%$ of the iterations, and makes the toy move in about $0.1 \%$ of movements. Finally, exploring the toy makes the tool and the toy move with a high probability as soon as the toy was discovered. Those results illustrate the stepping-stone structure of this environment, where each object must be well explored in order to discover the next step in complexity (Hand $\rightarrow$ Tool $\rightarrow$ Toy).</p>
<p>In Minecraft Mountain Cart (see Fig. 8(c,d,e)), random exploration with neural networks in this environment is extremely challenging. An agent following random policies has $0.04 \%$ chances to discover the pickaxe, $0.00025 \%$ chances to break a single block and it never managed to move the cart (over 800k episodes). IMGEP agents reach better performances by leveraging the sequential nature of the environment: when exploring the agent space there is around $10 \%$ chances to discover the pickaxe, and exploring the pickaxe space has around $1 \%$ chances to break blocks. Finally, exploring the block space has about $8 \%$ chances to lead an agent to discover the cart.</p>
<p>In the Robotic environment, a similar stepping-stone exploration structure is displayed (see Fig. $8(\mathrm{f}, \mathrm{g}, \mathrm{h})$ ): in order to discover the left joystick, the robots needs to do random movements with its arm, which have about $2.9 \%$ chances to makes the left joystick move, or explore its hand ( $0.2 \%$ chance). To discover the right joystick, the agent has to explore the left joystick, which gives a probability of $3.3 \%$ to reach the right one. To discover the Ergo (the white robotic toy in the center</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Stepping-stone structure of the three environments. In the 2D Simulated environment, we show the proportion of iterations that allowed to (a) move the magnet tool, (b) move the magnet toy, depending on the currently explored goal space (or random movements), for 10 IMGEP agents. The fastest way to discover the tool is to explore the hand and to discover the toy is to explore the tool. In the Minecraft Mountain Cart environment, we show the proportion of iterations that allowed to (c) move the pickaxe, (d) mine diamond blocks, and (e) move the cart, depending on the currently explored goal space (or random movements), for 10 agents with different seeds. Exploring the agent space helps discover the pickaxe, exploring the pickaxe helps discover the blocks, and exploring the blocks helps discover the cart. In the Robotic environment, we show the proportion of iterations that allowed to (f) reach the left joystick, (g) reach the right joystick, and (h) move the Ergo robot, depending on the currently explored goal space (or random movements), averaged for 11 IMGEP agents with different seeds. Exploring random movements or the Hand space helps discover the left joystick, exploring the left joystick helps discover the right one, which helps discover the Ergo toy.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Example of learned skills in the Minecraft Mountain Cart. (a) One AMB agent's trajectory for a single cart goal. (b) Five final cart positions reached by an AMB agent when tasked to reach five different targets. This agent successfully learned to push the cart along the track.
of the blue arena), the exploration of the right joystick gives $23 \%$ chances to move it, whereas the exploration of the Hand, the left joystick or random movements has a very low probability to make it move.</p>
<h1>4.3.3 Learned Skills</h1>
<p>In Minecraft Mountain Cart we performed post-training tests of competence in addition of exploration measures. Using modular approaches allows to easily test competence on specific objects of the environment. Fig. 9(b) shows an example in the cart space for an AMB agent. This agent successfully learned to move the cart close to the 5 queried locations.</p>
<p>For each of the RMB, AMB and FC runs we performed a statistical analysis of competence in the cart and pickaxe spaces using 1000 and 800 uniformly generated goals, respectively. We were also able to test SGS trials for cart competence as this condition has the cart as goal space. A goal is considered reached if the Euclidean distance between the outcome and the goal is lower than 0.05 in the normalized space (in range $[-1,1]$ ) for each object. Since the pickaxe goal space is loosely defined as a rectangular area around the environment's arena, many goals are not reachable. Results are shown in Table 2. SGS agents never managed to move the cart for any of the given goals. AMB appears to be significantly better than RMB on the pickaxe space ( $p&lt;0.01$ on Welch's t-tests). However it is not in the cart space $(p=0.09)$, which might be due to the stochasticity of the environment. FC is not significantly better than AMB on the cart and pickaxe spaces.</p>
<p>Intrinsic Rewards based on Learning Progress - The IMAGEP agents self-evaluate their learning progress to control each object. When they choose a goal for an object, they monitor what is the actual movement given to the object and compare it to the goal. If the distance between the goals</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Pickaxe goals</th>
<th style="text-align: center;">Cart goals</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FC</td>
<td style="text-align: center;">$39,49,55$</td>
<td style="text-align: center;">$12,17,25$</td>
</tr>
<tr>
<td style="text-align: left;">AMB</td>
<td style="text-align: center;">$41,45,49$</td>
<td style="text-align: center;">$8,11,18$</td>
</tr>
<tr>
<td style="text-align: left;">RMB</td>
<td style="text-align: center;">$37,40,43$</td>
<td style="text-align: center;">$6,9,15$</td>
</tr>
<tr>
<td style="text-align: left;">SGS</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$0,0,0$</td>
</tr>
</tbody>
</table>
<p>Table 2: Competence results in Minecraft Mountain Cart. We give the 25, 50 and 75 percentiles of the competence results of all seeds.
and the actual reached movements decrease over time on average, this tells the agents it is making progress to control this object. This signal is used as an intrinsic reward signal that the agent will seek to maximize by choosing to explore objects that yield a high learning progress. We can analyze this signal to understand at which point the agent perceived progress to control each object and how its exploration behavior changed over time.</p>
<p>Fig. 10 (top) shows the intrinsic rewards of two agents (different seeds) to explore each object in the 2 D simulated environment, computed by the agents as the average of intrinsic rewards based on learning progress to move each object. We can see that the intrinsic reward of the hand increases first as it is the easiest object to move. Then, when the sticks are discovered, the agents start to make progress to move them in many directions. Similarly, while exploring the sticks, they discover the reachable toys, so they start making progress in moving those toys. However, the static objects can't be moved so their learning progress is strictly zero, and the objects moving randomly independently of the agent (cat and dog) have a very low progress.</p>
<p>Fig. 10 (middle) shows the intrinsic reward of two agents in the Minecraft Mountain Cart environment. Both agents first explore the simpler agent space and then quickly improves on the shovel and pickaxe spaces. Exploring the pickaxe space leads to discover how to progress in the block space. Finally, after some progress in the block space, the cart is discovered after 14 k episodes for the first agent (left figure) and 26 k episodes for the other (right figure). The 3 distracting flowers have an interest strictly equal to zero in both runs.</p>
<p>Fig. 10 (bottom) shows the intrinsic reward of two agents in the Robotic environment. The first interesting object is the robot's hand, followed by the left joystick and then the right joystick. The left joystick is the easiest to reach and move so it gets interesting before the right one in most runs, but then they have similar learning progress curves. However, the right joystick can be used as a tool to control other objects, so that one will be touched more often. Then, the agent can discover the Ergo and Ball while exploring the joysticks. Finally, some agents also discover that the ball can be used to make light or sound. Here also, the progress of static objects is zero and the one of random objects is low. Note that, unlike in the 2D simulation and in the Minecraft environment, the intrinsic reward for exploring the robot's hand remains high. This phenomenon is most likely due to the use of full trajectories as goals in the Robotic environment, which creates large goal spaces (compared to when using end-positions as goals). Additionally, the hand space is quite homogeneous: there are many trajectories that are equally learnable, so the hand space remains for a very long time a source of learning progress. For more difficult objects, the space is also large, but the subspace of learnable trajectories is much smaller, thus the decrease in LP can be seen.</p>
<p>Overall, the evolution of those interests show that evaluating the learning progress to move objects allows agents to self-organize a learning curriculum focusing on the objects currently yielding the most progress and to discover stepping stones one after the other.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>Early version of the experimental setup: https://youtu.be/NOLAwD4ZTW0</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
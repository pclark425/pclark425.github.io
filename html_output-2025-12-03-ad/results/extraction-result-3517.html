<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3517 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3517</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3517</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-7c950f94f8b209a4260ac34a7df36495cb7ef1b6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7c950f94f8b209a4260ac34a7df36495cb7ef1b6" target="_blank">Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation</a></p>
                <p><strong>Paper Venue:</strong> AACL</p>
                <p><strong>Paper TL;DR:</strong> A natural language inference test suite is introduced to enable probing the capabilities of NLP methods, with the aim of understanding sub-clausal negation.</p>
                <p><strong>Paper Abstract:</strong> Negation is poorly captured by current language models, although the extent of this problem is not widely understood. We introduce a natural language inference (NLI) test suite to enable probing the capabilities of NLP methods, with the aim of understanding sub-clausal negation. The test suite contains premiseâ€“hypothesis pairs where the premise contains sub-clausal negation and the hypothesis is constructed by making minimal modifications to the premise in order to reflect different possible interpretations. Aside from adopting standard NLI labels, our test suite is systematically constructed under a rigorous linguistic framework. It includes annotation of negation types and constructions grounded in linguistic theory, as well as the operations used to construct hypotheses. This facilitates fine-grained analysis of model performance. We conduct experiments using pre-trained language models to demonstrate that our test suite is more challenging than existing benchmarks focused on negation, and show how our annotation supports a deeper understanding of the current NLI capabilities in terms of negation and quantification.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3517.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3517.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-MNLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (fine-tuned on MNLI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa base model fine-tuned on the MNLI NLI dataset, used as the primary baseline to evaluate handling of sub-clausal negation in the NaN-NLI test suite.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A robustly optimized BERT pretraining approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (base), fine-tuned on MNLI</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based masked-language-model architecture (RoBERTa) in its base variant, fine-tuned for three-way NLI on the MNLI dataset. The paper uses the HuggingFace transformers implementation and standard fine-tuning hyperparameters (batch size 16, lr 3e-5, 3 epochs for MNLI).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>NaN-NLI (sub-clausal negation NLI test suite)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A linguistically-motivated NLI test-suite targeting sub-clausal and non-verbal negation constructions; premise-hypothesis pairs are minimally edited to probe fine-grained negation and quantification reasoning. Also evaluated on NaN-Quant (quantification subset).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard fine-tuning on MNLI (no special negation pretraining or masking).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Standard (macro F1 by class): Contradiction 0.664, Entailment 0.648, Neutral 0.207; All (macro F1) 0.580. Binary (Entailment vs Not-Entailment) All F1 0.670. Strict accuracy (all hypotheses for a premise must be correct): 0.250 (12/48).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>On an easier negation subset (MNLI-neg), RoBERTa-MNLI All F1 = 0.862 (Table 5), showing much higher performance on prior benchmarks compared to NaN-NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Not applicable (this is the baseline model for NaN-NLI). Compared to performance on MNLI-neg (All F1 0.862), performance on NaN-NLI drops sharply (All F1 0.580), indicating NaN-NLI is substantially harder.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Strong over-prediction of Entailment on NaN-NLI (confusion matrices), very poor Neutral classification (F1 0.207), extremely low Strict accuracy (0.25). Fails on sub-clausal/synthetic negation and on cases involving quantification and comparative quantifiers; biased by lexical overlap heuristics from MNLI training data.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reports breakdowns showing RoBERTa's much higher scores on prior negation subsets (MNLI-neg) versus NaN-NLI; error analysis (Tables 7-9, 13) shows high error rates for not+quantifier constructions (ER 0.559) and comparative quantifier changes (ER 0.650). BERTScore analysis (Table 4) shows NaN-NLI premise/hypothesis pairs are lexically much more similar, reducing utility of lexical cues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3517.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3517.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-MNLI-NegNLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (fine-tuned on MNLI then fine-tuned on NegNLI subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa base model first fine-tuned on MNLI and then additionally fine-tuned on the NegNLI subset (negation-augmented NLI) to improve handling of negation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (base) fine-tuned on MNLI, then NegNLI</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same RoBERTa base model as above, but further fine-tuned on a negation-focused NLI dataset (NegNLI subset) to adapt to negation phenomena; NegNLI fine-tuning hyperparameters: batch size 16, lr 2e-5, 5 epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>NaN-NLI (sub-clausal negation NLI test suite) and NaN-Quant subset</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above: a controlled NLI test-suite probing sub-clausal negation and quantification bounds; NaN-Quant is the subset involving quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Additional fine-tuning on NegNLI (negation-focused NLI subset).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On NaN-NLI: Standard per-class F1: Contradiction 0.692, Entailment 0.684, Neutral 0.366; All (macro F1) 0.629. Binary All F1 0.721. Strict accuracy 0.292 (14/48). On NaN-Quant (quantification subset): All F1 0.486; Contradiction 0.477, Entailment 0.600, Neutral 0.379 (Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to RoBERTa-MNLI (no NegNLI fine-tuning) on NaN-NLI All F1 0.580, RoBERTa-MNLI-NegNLI All F1 = 0.629 (improvement +0.049). Also, on prior MNLI-neg benchmark RoBERTa-MNLI All F1 = 0.862 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Fine-tuning on NegNLI improved overall NaN-NLI macro F1 from 0.580 to 0.629 (+0.049), with the largest relative gains in Neutral classification (from F1 0.207 to 0.366). Strict accuracy improved marginally (0.25 -> 0.292).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although improved, the model still fails frequently: Neutral remains the hardest class (still low F1 0.366), Strict accuracy remains very low (0.292). The model still struggles with sub-clausal/synthetic negation and quantification bounds (NaN-Quant All F1 0.486). Error modes: not+quantifier constructions, comparative quantifier changes, and degree/affixial negation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper provides per-construction error rates (Table 8) and per-operation error rates (Table 9), showing remaining high error rates after NegNLI fine-tuning: e.g., comparative quantifier change ER 0.650, indefinite quantifier change ER 0.486. The authors attribute some gains to exposing models to more negation during pretraining/fine-tuning but show that many complex sub-clausal patterns remain unresolved.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3517.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3517.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CueNB-MNLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CueNB (negation-focused pretraining) fine-tuned on MNLI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CueNB is a RoBERTa-variant pretrained with additional negation data augmentation and a negation cue masking strategy, then fine-tuned on MNLI to evaluate whether negation-focused pretraining improves NLI on sub-clausal negation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving negation detection with negation-focused pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CueNB (negation-focused variant of RoBERTa), fine-tuned on MNLI</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa variant (named CueNB) pretrained with extra negation examples (negation-focused pretraining) and a strategy that masks negation cues during pretraining to improve negation representations; then fine-tuned on MNLI for NLI evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>NaN-NLI (sub-clausal negation NLI test suite)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same NaN-NLI test suite targeting fine-grained sub-clausal negation and quantification reasoning in NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Negation-focused pretraining: additional data augmentation with negation types and negation-cue masking prior to standard MNLI fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On NaN-NLI (CueNB-MNLI): Standard per-class F1: Contradiction 0.678, Entailment 0.678, Neutral 0.250; All (macro F1) 0.605. Binary All F1 0.718. Strict accuracy 0.250 (12/48).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to RoBERTa-MNLI baseline on NaN-NLI All F1 0.580, CueNB-MNLI All F1 = 0.605 (improvement +0.025). On prior NegNLI/MNLI-neg benchmarks, RoBERTa-MNLI had higher performance (see Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Negation-focused pretraining (CueNB) yields modest gains over vanilla RoBERTa when both are fine-tuned on MNLI: All F1 up from 0.580 to 0.605 (+0.025). Gains are modest and Neutral remains low.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Despite negation-focused pretraining, CueNB still under-performs on many sub-clausal negation cases, especially quantifier-related constructions and comparative quantifier changes. Neutral classification remains poor (F1 0.250) and Strict accuracy remains low (0.250).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Comparison between CueNB-MNLI and CueNB-MNLI-NegNLI shows additional fine-tuning on NegNLI further helps Neutral detection (CueNB-MNLI-NegNLI Neutral F1 0.395), indicating pretraining plus targeted fine-tuning both matter; detailed error rates by construction/operation provided in Tables 8/9.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3517.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3517.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CueNB-MNLI-NegNLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CueNB (negation-focused pretraining) fine-tuned on MNLI then NegNLI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CueNB model (negation-focused pretraining) further fine-tuned on a negation-focused NLI dataset (NegNLI) to combine pretraining and targeted fine-tuning for improved handling of negation in NaN-NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving negation detection with negation-focused pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CueNB (negation-focused RoBERTa variant), fine-tuned on MNLI then NegNLI</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same as CueNB-MNLI but with additional fine-tuning on the NegNLI subset (hyperparameters: batch size 16, lr 2e-5, 5 epochs for NegNLI), aiming to improve recognition of negation patterns via both pretraining and targeted supervised adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>NaN-NLI (sub-clausal negation NLI test suite)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above: controlled NLI suite to probe sub-clausal negation and quantification reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Negation-focused pretraining + fine-tuning on MNLI and then on NegNLI (negation-specific NLI examples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On NaN-NLI: Standard per-class F1: Contradiction 0.651, Entailment 0.694, Neutral 0.395; All (macro F1) 0.624. Binary All F1 0.741. Strict accuracy 0.271 (13/48).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to RoBERTa-MNLI (no negation pretraining) All F1 0.580, CueNB-MNLI-NegNLI All F1 = 0.624 (improvement +0.044). Compared to RoBERTa-MNLI-NegNLI All F1 0.629, performance is comparable (slightly lower overall but better Neutral F1 in some runs).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Negation-focused pretraining combined with NegNLI fine-tuning led to modest overall improvements over the vanilla baseline; largest gains are in binary (Not-Entailment) and Neutral detection compared to models without negation-focused pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Even with negation-focused pretraining and NegNLI fine-tuning, models still fail many NaN-NLI cases: Neutral remains hardest (though improved to F1 0.395), strict accuracy still under 0.28, and many quantifier/relative-bound cases remain unresolved.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Analysis shows pretraining on negation cues plus NegNLI fine-tuning helps Neutral detection more than baseline pretraining alone; detailed per-construction error breakdown (Tables 8-9) highlights which constructions remain most problematic (not+quantifier, comparative quantifier changes, degree expressions).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3517.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3517.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NaN-NLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NaN-NLI: The NaN-NLI Test Suite for Sub-clausal Negation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linguistically-grounded NLI test suite introduced in this paper that focuses on sub-clausal and non-verbal negation, with richly annotated premise-hypothesis pairs and controlled minimal edits to probe negation scope and quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NaN-NLI (dataset / benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Test-suite style dataset with 258 premise-hypothesis pairs (on average 5 hypotheses per premise), annotated for negation type, construction, and operations; includes a quantification subset (NaN-Quant). Designed to be lexically similar premise/hypothesis pairs to force reasoning about negation rather than rely on lexical cues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>NaN-NLI (strict NLI evaluation incl. Strict setting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Three evaluation settings: Standard (3-way NLI), Binary (Entailment vs Not-Entailment), and Strict (all hypotheses for a premise must be labeled correctly). Focuses on strict logical interpretation of negation and quantifier bounds rather than lexical signals.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>N/A (benchmark). Used to evaluate models, including interventions such as negation-focused pretraining (CueNB) and NegNLI fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Across tested models, performance is substantially lower than on prior negation benchmarks: e.g., RoBERTa-MNLI All F1 0.580; RoBERTa-MNLI-NegNLI All F1 0.629; CueNB-MNLI All F1 0.605; CueNB-MNLI-NegNLI All F1 0.624. Strict accuracies are very low (0.25-0.292). NaN-Quant subset yields further drops (RoBERTa-MNLI-NegNLI All F1 0.486).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>N/A (benchmark). Used to reveal that previous benchmarks overestimate model capability; models fine-tuned on negation data improved but still far from solving the tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dataset is an evaluation set only and has unbalanced label distribution (fewer Neutral samples), and is focused on general English (not domain-specific). However, it reveals clear model failures: inability to handle sub-clausal/synthetic negation, quantifier bounds, and many minimal edits leading to different logical relations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The paper provides detailed analyses: BERTScore lexical-similarity comparisons (Table 4) showing high premise-hypothesis similarity, per-construction error rates (Table 8), per-operation error rates (Table 9), and per-negation-type performance (Table 13). These analyses identify quantifier-related constructions and synthetic negation as the main failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>An analysis of natural language inference benchmarks through the lens of negation <em>(Rating: 2)</em></li>
                <li>Improving negation detection with negation-focused pre-training <em>(Rating: 2)</em></li>
                <li>Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly <em>(Rating: 2)</em></li>
                <li>What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models <em>(Rating: 2)</em></li>
                <li>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3517",
    "paper_id": "paper-7c950f94f8b209a4260ac34a7df36495cb7ef1b6",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "RoBERTa-MNLI",
            "name_full": "RoBERTa (fine-tuned on MNLI)",
            "brief_description": "RoBERTa base model fine-tuned on the MNLI NLI dataset, used as the primary baseline to evaluate handling of sub-clausal negation in the NaN-NLI test suite.",
            "citation_title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa (base), fine-tuned on MNLI",
            "model_description": "Transformer-based masked-language-model architecture (RoBERTa) in its base variant, fine-tuned for three-way NLI on the MNLI dataset. The paper uses the HuggingFace transformers implementation and standard fine-tuning hyperparameters (batch size 16, lr 3e-5, 3 epochs for MNLI).",
            "model_size": null,
            "reasoning_task_name": "NaN-NLI (sub-clausal negation NLI test suite)",
            "reasoning_task_description": "A linguistically-motivated NLI test-suite targeting sub-clausal and non-verbal negation constructions; premise-hypothesis pairs are minimally edited to probe fine-grained negation and quantification reasoning. Also evaluated on NaN-Quant (quantification subset).",
            "method_or_intervention": "Standard fine-tuning on MNLI (no special negation pretraining or masking).",
            "performance": "Standard (macro F1 by class): Contradiction 0.664, Entailment 0.648, Neutral 0.207; All (macro F1) 0.580. Binary (Entailment vs Not-Entailment) All F1 0.670. Strict accuracy (all hypotheses for a premise must be correct): 0.250 (12/48).",
            "baseline_performance": "On an easier negation subset (MNLI-neg), RoBERTa-MNLI All F1 = 0.862 (Table 5), showing much higher performance on prior benchmarks compared to NaN-NLI.",
            "improvement_over_baseline": "Not applicable (this is the baseline model for NaN-NLI). Compared to performance on MNLI-neg (All F1 0.862), performance on NaN-NLI drops sharply (All F1 0.580), indicating NaN-NLI is substantially harder.",
            "limitations_or_failures": "Strong over-prediction of Entailment on NaN-NLI (confusion matrices), very poor Neutral classification (F1 0.207), extremely low Strict accuracy (0.25). Fails on sub-clausal/synthetic negation and on cases involving quantification and comparative quantifiers; biased by lexical overlap heuristics from MNLI training data.",
            "ablation_or_analysis": "Paper reports breakdowns showing RoBERTa's much higher scores on prior negation subsets (MNLI-neg) versus NaN-NLI; error analysis (Tables 7-9, 13) shows high error rates for not+quantifier constructions (ER 0.559) and comparative quantifier changes (ER 0.650). BERTScore analysis (Table 4) shows NaN-NLI premise/hypothesis pairs are lexically much more similar, reducing utility of lexical cues.",
            "uuid": "e3517.0",
            "source_info": {
                "paper_title": "Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "RoBERTa-MNLI-NegNLI",
            "name_full": "RoBERTa (fine-tuned on MNLI then fine-tuned on NegNLI subset)",
            "brief_description": "RoBERTa base model first fine-tuned on MNLI and then additionally fine-tuned on the NegNLI subset (negation-augmented NLI) to improve handling of negation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa (base) fine-tuned on MNLI, then NegNLI",
            "model_description": "Same RoBERTa base model as above, but further fine-tuned on a negation-focused NLI dataset (NegNLI subset) to adapt to negation phenomena; NegNLI fine-tuning hyperparameters: batch size 16, lr 2e-5, 5 epochs.",
            "model_size": null,
            "reasoning_task_name": "NaN-NLI (sub-clausal negation NLI test suite) and NaN-Quant subset",
            "reasoning_task_description": "As above: a controlled NLI test-suite probing sub-clausal negation and quantification bounds; NaN-Quant is the subset involving quantification.",
            "method_or_intervention": "Additional fine-tuning on NegNLI (negation-focused NLI subset).",
            "performance": "On NaN-NLI: Standard per-class F1: Contradiction 0.692, Entailment 0.684, Neutral 0.366; All (macro F1) 0.629. Binary All F1 0.721. Strict accuracy 0.292 (14/48). On NaN-Quant (quantification subset): All F1 0.486; Contradiction 0.477, Entailment 0.600, Neutral 0.379 (Table 10).",
            "baseline_performance": "Compared to RoBERTa-MNLI (no NegNLI fine-tuning) on NaN-NLI All F1 0.580, RoBERTa-MNLI-NegNLI All F1 = 0.629 (improvement +0.049). Also, on prior MNLI-neg benchmark RoBERTa-MNLI All F1 = 0.862 (Table 5).",
            "improvement_over_baseline": "Fine-tuning on NegNLI improved overall NaN-NLI macro F1 from 0.580 to 0.629 (+0.049), with the largest relative gains in Neutral classification (from F1 0.207 to 0.366). Strict accuracy improved marginally (0.25 -&gt; 0.292).",
            "limitations_or_failures": "Although improved, the model still fails frequently: Neutral remains the hardest class (still low F1 0.366), Strict accuracy remains very low (0.292). The model still struggles with sub-clausal/synthetic negation and quantification bounds (NaN-Quant All F1 0.486). Error modes: not+quantifier constructions, comparative quantifier changes, and degree/affixial negation.",
            "ablation_or_analysis": "Paper provides per-construction error rates (Table 8) and per-operation error rates (Table 9), showing remaining high error rates after NegNLI fine-tuning: e.g., comparative quantifier change ER 0.650, indefinite quantifier change ER 0.486. The authors attribute some gains to exposing models to more negation during pretraining/fine-tuning but show that many complex sub-clausal patterns remain unresolved.",
            "uuid": "e3517.1",
            "source_info": {
                "paper_title": "Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "CueNB-MNLI",
            "name_full": "CueNB (negation-focused pretraining) fine-tuned on MNLI",
            "brief_description": "CueNB is a RoBERTa-variant pretrained with additional negation data augmentation and a negation cue masking strategy, then fine-tuned on MNLI to evaluate whether negation-focused pretraining improves NLI on sub-clausal negation.",
            "citation_title": "Improving negation detection with negation-focused pre-training",
            "mention_or_use": "use",
            "model_name": "CueNB (negation-focused variant of RoBERTa), fine-tuned on MNLI",
            "model_description": "RoBERTa variant (named CueNB) pretrained with extra negation examples (negation-focused pretraining) and a strategy that masks negation cues during pretraining to improve negation representations; then fine-tuned on MNLI for NLI evaluation.",
            "model_size": null,
            "reasoning_task_name": "NaN-NLI (sub-clausal negation NLI test suite)",
            "reasoning_task_description": "Same NaN-NLI test suite targeting fine-grained sub-clausal negation and quantification reasoning in NLI.",
            "method_or_intervention": "Negation-focused pretraining: additional data augmentation with negation types and negation-cue masking prior to standard MNLI fine-tuning.",
            "performance": "On NaN-NLI (CueNB-MNLI): Standard per-class F1: Contradiction 0.678, Entailment 0.678, Neutral 0.250; All (macro F1) 0.605. Binary All F1 0.718. Strict accuracy 0.250 (12/48).",
            "baseline_performance": "Compared to RoBERTa-MNLI baseline on NaN-NLI All F1 0.580, CueNB-MNLI All F1 = 0.605 (improvement +0.025). On prior NegNLI/MNLI-neg benchmarks, RoBERTa-MNLI had higher performance (see Table 5).",
            "improvement_over_baseline": "Negation-focused pretraining (CueNB) yields modest gains over vanilla RoBERTa when both are fine-tuned on MNLI: All F1 up from 0.580 to 0.605 (+0.025). Gains are modest and Neutral remains low.",
            "limitations_or_failures": "Despite negation-focused pretraining, CueNB still under-performs on many sub-clausal negation cases, especially quantifier-related constructions and comparative quantifier changes. Neutral classification remains poor (F1 0.250) and Strict accuracy remains low (0.250).",
            "ablation_or_analysis": "Comparison between CueNB-MNLI and CueNB-MNLI-NegNLI shows additional fine-tuning on NegNLI further helps Neutral detection (CueNB-MNLI-NegNLI Neutral F1 0.395), indicating pretraining plus targeted fine-tuning both matter; detailed error rates by construction/operation provided in Tables 8/9.",
            "uuid": "e3517.2",
            "source_info": {
                "paper_title": "Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "CueNB-MNLI-NegNLI",
            "name_full": "CueNB (negation-focused pretraining) fine-tuned on MNLI then NegNLI",
            "brief_description": "CueNB model (negation-focused pretraining) further fine-tuned on a negation-focused NLI dataset (NegNLI) to combine pretraining and targeted fine-tuning for improved handling of negation in NaN-NLI.",
            "citation_title": "Improving negation detection with negation-focused pre-training",
            "mention_or_use": "use",
            "model_name": "CueNB (negation-focused RoBERTa variant), fine-tuned on MNLI then NegNLI",
            "model_description": "Same as CueNB-MNLI but with additional fine-tuning on the NegNLI subset (hyperparameters: batch size 16, lr 2e-5, 5 epochs for NegNLI), aiming to improve recognition of negation patterns via both pretraining and targeted supervised adaptation.",
            "model_size": null,
            "reasoning_task_name": "NaN-NLI (sub-clausal negation NLI test suite)",
            "reasoning_task_description": "As above: controlled NLI suite to probe sub-clausal negation and quantification reasoning.",
            "method_or_intervention": "Negation-focused pretraining + fine-tuning on MNLI and then on NegNLI (negation-specific NLI examples).",
            "performance": "On NaN-NLI: Standard per-class F1: Contradiction 0.651, Entailment 0.694, Neutral 0.395; All (macro F1) 0.624. Binary All F1 0.741. Strict accuracy 0.271 (13/48).",
            "baseline_performance": "Compared to RoBERTa-MNLI (no negation pretraining) All F1 0.580, CueNB-MNLI-NegNLI All F1 = 0.624 (improvement +0.044). Compared to RoBERTa-MNLI-NegNLI All F1 0.629, performance is comparable (slightly lower overall but better Neutral F1 in some runs).",
            "improvement_over_baseline": "Negation-focused pretraining combined with NegNLI fine-tuning led to modest overall improvements over the vanilla baseline; largest gains are in binary (Not-Entailment) and Neutral detection compared to models without negation-focused pretraining.",
            "limitations_or_failures": "Even with negation-focused pretraining and NegNLI fine-tuning, models still fail many NaN-NLI cases: Neutral remains hardest (though improved to F1 0.395), strict accuracy still under 0.28, and many quantifier/relative-bound cases remain unresolved.",
            "ablation_or_analysis": "Analysis shows pretraining on negation cues plus NegNLI fine-tuning helps Neutral detection more than baseline pretraining alone; detailed per-construction error breakdown (Tables 8-9) highlights which constructions remain most problematic (not+quantifier, comparative quantifier changes, degree expressions).",
            "uuid": "e3517.3",
            "source_info": {
                "paper_title": "Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "NaN-NLI",
            "name_full": "NaN-NLI: The NaN-NLI Test Suite for Sub-clausal Negation",
            "brief_description": "A linguistically-grounded NLI test suite introduced in this paper that focuses on sub-clausal and non-verbal negation, with richly annotated premise-hypothesis pairs and controlled minimal edits to probe negation scope and quantification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NaN-NLI (dataset / benchmark)",
            "model_description": "Test-suite style dataset with 258 premise-hypothesis pairs (on average 5 hypotheses per premise), annotated for negation type, construction, and operations; includes a quantification subset (NaN-Quant). Designed to be lexically similar premise/hypothesis pairs to force reasoning about negation rather than rely on lexical cues.",
            "model_size": null,
            "reasoning_task_name": "NaN-NLI (strict NLI evaluation incl. Strict setting)",
            "reasoning_task_description": "Three evaluation settings: Standard (3-way NLI), Binary (Entailment vs Not-Entailment), and Strict (all hypotheses for a premise must be labeled correctly). Focuses on strict logical interpretation of negation and quantifier bounds rather than lexical signals.",
            "method_or_intervention": "N/A (benchmark). Used to evaluate models, including interventions such as negation-focused pretraining (CueNB) and NegNLI fine-tuning.",
            "performance": "Across tested models, performance is substantially lower than on prior negation benchmarks: e.g., RoBERTa-MNLI All F1 0.580; RoBERTa-MNLI-NegNLI All F1 0.629; CueNB-MNLI All F1 0.605; CueNB-MNLI-NegNLI All F1 0.624. Strict accuracies are very low (0.25-0.292). NaN-Quant subset yields further drops (RoBERTa-MNLI-NegNLI All F1 0.486).",
            "baseline_performance": null,
            "improvement_over_baseline": "N/A (benchmark). Used to reveal that previous benchmarks overestimate model capability; models fine-tuned on negation data improved but still far from solving the tasks.",
            "limitations_or_failures": "Dataset is an evaluation set only and has unbalanced label distribution (fewer Neutral samples), and is focused on general English (not domain-specific). However, it reveals clear model failures: inability to handle sub-clausal/synthetic negation, quantifier bounds, and many minimal edits leading to different logical relations.",
            "ablation_or_analysis": "The paper provides detailed analyses: BERTScore lexical-similarity comparisons (Table 4) showing high premise-hypothesis similarity, per-construction error rates (Table 8), per-operation error rates (Table 9), and per-negation-type performance (Table 13). These analyses identify quantifier-related constructions and synthetic negation as the main failure modes.",
            "uuid": "e3517.4",
            "source_info": {
                "paper_title": "Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "An analysis of natural language inference benchmarks through the lens of negation",
            "rating": 2
        },
        {
            "paper_title": "Improving negation detection with negation-focused pre-training",
            "rating": 2
        },
        {
            "paper_title": "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
            "rating": 2
        },
        {
            "paper_title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
            "rating": 2
        },
        {
            "paper_title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "rating": 1
        }
    ],
    "cost": 0.015570999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation</h1>
<p>Hung Thinh Truong ${ }^{1, <em>}$ Yulia Otmakhova ${ }^{1, </em>}$ Timothy Baldwin ${ }^{1,3}$<br>Trevor Cohn ${ }^{1}$<br>Jey Han Lau ${ }^{1}$ Karin Verspoor ${ }^{2,1}$<br>${ }^{1}$ The University of Melbourne ${ }^{2}$ RMIT University ${ }^{3}$ MBZUAI<br>{hungthinht, yotmakhova}@student.unimelb.edu.au tb@1dwin.net<br>trevor.cohn@unimelb.edu.au jeyhan.lau@gmail.com karin.verspoor@rmit.edu.au</p>
<h4>Abstract</h4>
<p>Negation is poorly captured by current language models, although the extent of this problem is not widely understood. We introduce a natural language inference (NLI) test suite to enable probing the capabilities of NLP methods, with the aim of understanding sub-clausal negation. The test suite contains premisehypothesis pairs where the premise contains sub-clausal negation and the hypothesis is constructed by making minimal modifications to the premise in order to reflect different possible interpretations. Aside from adopting standard NLI labels, our test suite is systematically constructed under a rigorous linguistic framework. It includes annotation of negation types and constructions grounded in linguistic theory, as well as the operations used to construct hypotheses. This facilitates fine-grained analysis of model performance. We conduct experiments using pre-trained language models to demonstrate that our test suite is more challenging than existing benchmarks focused on negation, and show how our annotation supports a deeper understanding of the current NLI capabilities in terms of negation and quantification.</p>
<h2>1 Introduction</h2>
<p>Negation is an important linguistic phenomenon which denotes non-existence, denial, or contradiction, and is core to language understanding. NLP work on negation has mostly focused on detecting instances of negation (Peng et al., 2018; Khandelwal and Sawant, 2020; Truong et al., 2022), and the effect of negation on downstream or probing tasks (Kassner and SchÃ¼tze, 2020; Ettinger, 2020; Hossain et al., 2020). A consistent finding in recent work on pre-trained language models (PLMs) is that they struggle to correctly handle negation, but also that existing NLP benchmarks are deficient in terms of their relative occurrence and variability</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>of negation (Barnes et al., 2021; Tang et al., 2021; Hossain et al., 2022).</p>
<p>In this work, we address the problem of evaluating the ability of models to handle negation in the English language using a systematic, linguisticallybased approach. Specifically, we adopt the typology proposed by Pullum and Huddleston (2002) whereby negation is classified based on both form (verbal and non-verbal; analytic and synthetic) and meaning (clausal and sub-clausal; ordinary and meta-linguistic). Based on this typology, we observe that most negation instances occurring in existing benchmarks are analytic, verbal, and clausal, which is arguably more straightforward to handle than non-verbal, synthetic, and sub-clausal negation. For instance, the dataset proposed by Hossain et al. (2020) is constructed by adding the syntactic negation cue not to the main verb of the premise and/or the hypothesis of MNLI (Williams et al., 2018) training examples, resulting almost exclusively in verbal, analytic, and clausal negation.</p>
<p>Motivated by this, we construct a new evaluation dataset with a focus on sub-clausal negation, where it is non-trivial to determine the correct negation scope. For instance, the negation in They saw not one but three dolphins only scopes over the modifier one, and thus carries a positive meaning (They saw three dolphins). We choose NLI as the probing task based on the intuition that a complete grasp of negation is required to make correct inference judgements. Moreover, we adopt the test suite framework (Lehmann et al., 1996) instead of naturally-occurring text corpora, to elicit a full range of linguistic constructions that denote sub-clausal negation. This facilitates systematic evaluation of model performance along controlled dimensions. We collect examples for each construction from Pullum and Huddleston (2002) to use as premises, and then construct corresponding hypotheses by introducing minimum changes to premises which highlight their possible interpreta-</p>
<p>tions. We manually annotate the constructed pairs in terms of negation types, negation constructions, and the operations used to construct the hypotheses.</p>
<p>In summary, our main contributions are:</p>
<ol>
<li>We introduce the "NaN-NLI" test suite for probing the capabilities of NLP models to capture sub-clausal negation. ${ }^{1}$ In addition to standard NLI labels, it contains various linguistic annotations related to negation, to facilitate fine-grained analysis of different constructional and semantic sub-types of negation;</li>
<li>We conduct extensive experiments to confirm that our test suite is more difficult than existing negation-focused NLI benchmarks, and show how our annotations can be used to guide error analysis and interpretation of model performance; and</li>
<li>We present a subset of our test suite (NaNQuant) with samples involving not only negation but also quantification, and show that quantification is an especially challenging phenomenon that requires future exploration.</li>
</ol>
<h2>2 Related Work</h2>
<p>To investigate the abilities of PLMs to assign the correct interpretation to negation, many probing tasks have been proposed. For instance, Kassner and SchÃ¼tze (2020); Ettinger (2020) formulated a cloze-style fill-in-the-blank task where BERT is asked to predict words for two near-identical but contrasting sentences (e.g. A bird can $\qquad$ vs. $A$ bird cannot ___ ). Hossain et al. (2020) constructed an NLI dataset where negations essential to correctly judge the label for a premise-hypothesis pair were manually added to existing NLI benchmarks. Hartmann et al. (2021) constructed a multilingual dataset with minimal pairs of NLI examples to analyze model behavior in the presence/absence of negation. Most recently, Hossain et al. (2022) conducted a comprehensive analysis of the effect of negation on a wide range of NLU tasks in the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) benchmarks. These papers expose various limitations of both current benchmarks and PLMs in the face of negation. However, they all focus on verbal and clausal negation, which are more</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>straightforward to process, whereas our dataset targets non-verbal and sub-clausal negation, where it is more difficult to determine the correct scope.</p>
<p>The idea of using a test suite to measure the performance of NLP models was introduced by Lehmann et al. (1996), where the authors propose general guidelines for test suite construction. Adopting this methodology for a domain-specific task, Cohen et al. (2010) constructed a dataset for benchmarking ontology concept recognition systems. Most recently, Ribeiro et al. (2020) proposed a task-agnostic testing methodology which closely follows the idea of behavioral testing from software engineering to comprehensively test the linguistic capabilities of NLP models. The main advantages of test suites over datasets made up of naturallyoccurring examples are: (1) control over the precise composition of the data: we can undertake a targeted evaluation of specific criteria (e.g. linguistic features); (2) systematicity: a test suite has specific structure, with samples classified into welldefined categories; and (3) control of redundancy: we can remove samples with similar properties or over-sample rare occurrences.</p>
<h2>3 A Test Suite for Non-verbal Negation</h2>
<h3>3.1 Negation typology</h3>
<p>According to Pullum and Huddleston (2002), negation can be classified according to four main aspects:</p>
<ul>
<li>Verbal vs. non-verbal: verbal negation is when the negation marker is associated with the verb, while non-verbal negation is associated with an adjunct or object.</li>
<li>Analytic vs. synthetic: when the negation marker's only syntactic function is to mark negation (e.g. not), it represents analytic negation, whereas in synthetic negation the marker can have other syntactic functions (e.g. a compound negator nothing can also be a subject or an object).</li>
<li>
<p>Clausal vs. sub-clausal: Clausal negation negates the entire clause it is contained in, whereas the scope of sub-clausal negation is strictly less than the entire clause. For instance, in Not for the first time, she felt utterly betrayed, only the phrase Not for the first time is negated.</p>
</li>
<li>
<p>Ordinary vs. meta-linguistic: metalinguistic negation acts as a correction to how the negative meaning is understood. For instance, in The house is not big, it is huge, the negation is understood as a correction, since huge is a more correct way of describing the size of the house.</p>
</li>
</ul>
<p>The first two categories relate to the syntax of negation itself while the last two relate to semantics. In this work, we focus on sub-clausal negation as the correct negation scope can be challenging to determine, which can lead to misunderstanding of the negated instance. Although meta-linguistic negation can also cause difficulties with interpretation, as this class is rare in practice, we did not include them in our test suite.</p>
<h3>3.2 Test suite construction process</h3>
<h3>3.2.1 Selecting premises</h3>
<p>We manually collect sentences from Pullum and Huddleston (2002) to use as premises. Most samples are special constructions of non-verbal negation where they denote sub-clausal negation. Below we describe the main types of these constructions.</p>
<p>Not + quantifiers: not combines with a quantifier and scopes only over that quantifier.</p>
<p>Not all: not is used to deny the larger amount, and imply a normal value. Possible quantifiers include not all, not every, not many, not much, not often.</p>
<p>Not one, not two: not one is used to denote a complete non-existence of something, and has the same meaning as nothing or no one. When combining with a numbers larger than one (usually in phrases of time and distance), not can convey the meaning of as little as, as in not two years ago.</p>
<p>Not a little: This construction negates the lower bound of the quantification and asserts the upper bound, denoting a fairly large amount. For instance, not a little confusion is equivalent to much confusion.</p>
<p>Not + focus particles (even/only): Not even generally marks clausal negation while not only marks sub-clausal negation as it carries positive meaning. For instance, Not even Ed approved of the plan implies that Ed did not approve the plan, whereas in Not only Ed approved of the plan, Ed did in fact approve the plan.</p>
<p>Not + degree expressions: Expressions such as not very, not quite mark sub-clausal negation
by reducing the degree of adjectives, adverbs, or determiners (e.g. not very confident).</p>
<p>Not + affixially-negated adjectives/adverbs: When accompanied by a gradable adjective, the construction not un- has the meaning of negating the lower end of the scale for that adjective. For example, not unattractive suggests the appearance ranks higher than intermediate.</p>
<p>Not in coordination: Not can appear in a coordinative construction and typically scopes over only one of the coordinating parts, thus marking sub-clausal negation. In They are now leaving not on Friday but on Saturday, not scopes only over Friday and denies They are leaving on Friday.</p>
<p>Not with PPs: Not can modify prepositional phrases (PPs) to denote sub-clausal negation. In Not for the first time, she felt utterly betrayed, not only negates the PP for the first time, and the sentence has positive polarity in that she did feel utterly betrayed.</p>
<p>Not in verbless subordinate clauses: Not can scope only over a verbless subordinate clause (e.g. We need someone not afraid of taking risks.).</p>
<p>Not in implicit propositions with that: The construction not that has the function of denying something that is natural or expected in the context (e.g. There are spare blankets in here, not that you'll have any need of them.).</p>
<p>Absolute and approximate negators: Absolute negators (no, never) denote absolute non-existence but can also denote sub-clausal negation when they are part of a prepositional phrase. In They were friends in no time, only the PP in no time is negated. Approximate negators (rarely, seldom) denote a quantification that is close to zero. They imply positive meaning and thus denote sub-clausal negation.</p>
<h3>3.2.2 Constructing premise-hypothesis pairs</h3>
<p>When constructing hypothesis sentences for premises, we aimed to keep lexical changes to a minimum. This was especially so in the case of neutral hypotheses: though it is trivial to create any number of neutral hypotheses by changing semantically important parts of a sentence to other lexical items thus making it impossible to determine the truth value, intuitively, it would make the sentence embedding of the hypothesis quite different from that of the premise and thus easier for models to classify correctly. We also strove to make hypotheses linguistically diverse by introducing various changes to functional words rather than relying only on deletion and addition of not as was done</p>
<p>previously. Overall, we used 10 operations, with more than half the hypotheses including two or more changes. They are listed in Table 1 together with representative examples and their frequency counts across all sentences.</p>
<p>As outlined above, when creating hypotheses, we employed a much wider variety of linguistic operations than previous datasets, including movement of a negation marker across constituent boundaries, changing its type or scope, and substitution of indefinite pronouns. Thus we expect our dataset to be both richer and more difficult from the point of view of NLU. On average, for each of the selected premises, we created 5 hypotheses.</p>
<h3>3.2.3 Annotating the inference relationship within premise-hypothesis pairs</h3>
<p>Following Giampiccolo et al. (2007), we adopt a three-way classification of inference relationships between the premise $(p)$ and the hypothesis $(q)$ based on the following truth values:</p>
<ul>
<li>Entailment: if $p$ is True, $q$ must be True.</li>
<li>Contradiction: if $p$ is True, $q$ must be False.</li>
<li>Neutral: if $p$ is True, $q$ can be both True and False, and the available context does not allow us to make a specific judgement.</li>
</ul>
<p>Two annotators (the main authors of the paper, one of whom holds a graduate degree in linguistics) labeled all constructed pairs with these categories; disagreements were resolved via discussion. The inter-annotator agreement prior to adjudication was 0.86 in terms of Cohen's $\kappa$ (Cohen, 1960), which corresponds to near-perfect agreement (Artstein and Poesio, 2008). We employed the following linguistic tests to distinguish between entailed and neutral pairs (Kroeger, 2018; Anderson, 2018):</p>
<ul>
<li>It should be impossible to deny $q$ while asserting $p$, that is, to connect $p$ and $p$ using such expressions as but it is not the fact that ...</li>
<li>It should be unnatural to express doubt about $q$ while asserting $p$, that is, to connect them using such expressions as but I am not sure whether ...</li>
<li>It should be highly redundant to assert $q$ after stating $p$, that is, to connect them with such phrases as In fact ...</li>
</ul>
<p>If $q$ fails at least one of these tests, it is considered to be neutral to the premise; we regard a hypothesis to be entailed only if it passes all three tests. A contradiction was defined to be a statement which is the opposite of what is entailed by a premise. For example, given the premise $p=$ She didn't promise to help him, the constructed hypotheses can be annotated in the following way:</p>
<ul>
<li>Entailment: She didn't promise him help (fails all three tests).</li>
<li>Contradiction: She promised to help him (direct opposite of $p$ ).</li>
<li>Neutral: She promised not to help him (it can be be denied, asserted, and tentatively asserted).</li>
</ul>
<h3>3.2.4 Annotating premise-hypothesis pairs in terms of negation types, patterns, and introduced changes</h3>
<p>Finally, the annotators were asked to annotate each sample with respect to the following:</p>
<ul>
<li>Negation types in both the premise and hypothesis, as described in Section 3.1 (verbal vs. non-verbal, analytic vs. synthetic, clausal vs. sub-clausal).</li>
<li>Negation constructions in the premises, as described in Section 3.2.1. For some constructions, we also specify their sub-types using their representative expressions as names. For example, for not +quantifier, we annotate three sub-types which have distinct meanings: not many, not one, and not two.</li>
<li>Operations used to construct hypotheses, as outlined in Table 1.</li>
</ul>
<p>The initial inter-annotator agreement scores (Cohen's $\kappa$ ) were $0.99,0.88$, and 0.83 , for negation types, negation constructions, and operations respectively, which is close to near perfect as the categories are well-defined in Pullum and Huddleston (2002). All disagreements were then resolved via discussion. We include such detailed linguistic annotation in the test suite to facilitate error analysis and identifying the most problematic cases.</p>
<h3>3.2.5 Test suite statistics and comparison with existing negation benchmarks</h3>
<p>The statistics of the resulting dataset - named "NaN-NLI" - in terms of label distribution and the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Operation type</th>
<th style="text-align: left;">Example</th>
<th style="text-align: left;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Indefinite quantifier change <br> (many, rarely)</td>
<td style="text-align: left;">She rarely goes out these days. $\Rightarrow$ She never goes out these days.</td>
<td style="text-align: left;">74</td>
</tr>
<tr>
<td style="text-align: left;">Numerical quantifier change <br> (one, twenty)</td>
<td style="text-align: left;">Not for the first time, she felt utterly betrayed. $\Rightarrow$ She felt utterly betrayed <br> for the second time.</td>
<td style="text-align: left;">27</td>
</tr>
<tr>
<td style="text-align: left;">Negator addition or deletion</td>
<td style="text-align: left;">Not even Ed approved of the plan. $\Rightarrow$ Even Ed approved of the plan.</td>
<td style="text-align: left;">130</td>
</tr>
<tr>
<td style="text-align: left;">Negator position change</td>
<td style="text-align: left;">He was here not ten minutes ago. $\Rightarrow$ He was not here ten minutes ago.</td>
<td style="text-align: left;">101</td>
</tr>
<tr>
<td style="text-align: left;">Negator token change</td>
<td style="text-align: left;">Such mistakes are not common. $\Rightarrow$ Such mistakes are uncommon.</td>
<td style="text-align: left;">6</td>
</tr>
<tr>
<td style="text-align: left;">Clause or sub-clause deletion</td>
<td style="text-align: left;">Not often do we see her lose her cool like that. $\Rightarrow$ We do not see her often.</td>
<td style="text-align: left;">36</td>
</tr>
<tr>
<td style="text-align: left;">Comparative quantifier change <br> (more, less)</td>
<td style="text-align: left;">They had found not one mistake. $\Rightarrow$ They had found less than one mistake.</td>
<td style="text-align: left;">20</td>
</tr>
<tr>
<td style="text-align: left;">Focus particle change (even, <br> only)</td>
<td style="text-align: left;">Not even Ed approved of the plan. $\Rightarrow$ Not only Ed approved of the plan.</td>
<td style="text-align: left;">16</td>
</tr>
<tr>
<td style="text-align: left;">Lexical change</td>
<td style="text-align: left;">We had a not very amicable discussion. $\Rightarrow$ We did not have discussion.</td>
<td style="text-align: left;">13</td>
</tr>
<tr>
<td style="text-align: left;">Syntactic change</td>
<td style="text-align: left;">Not an accomplished dancer, he moved rather clumsily. $\Rightarrow$ He moved rather <br> clumsily because he was not an accomplished dancer.</td>
<td style="text-align: left;">4</td>
</tr>
</tbody>
</table>
<p>Table 1: Types, examples, and counts of operations used to construct hypotheses</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Premise</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Hypothesis</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Instances</td>
<td style="text-align: right;">Verbal/ <br> Non-V</td>
<td style="text-align: right;">Ana/Syn</td>
<td style="text-align: right;">Clausal/ <br> Sub-C</td>
<td style="text-align: right;">Verbal/ <br> Non-V</td>
<td style="text-align: right;">Ana/Syn</td>
<td style="text-align: right;">Clausal/ <br> Sub-C</td>
</tr>
<tr>
<td style="text-align: left;">C</td>
<td style="text-align: right;">$117(45.3 \%)$</td>
<td style="text-align: right;">$5.2 / 94.9$</td>
<td style="text-align: right;">$87.2 / 20.5$</td>
<td style="text-align: right;">$0.9 / 99.2$</td>
<td style="text-align: right;">$46.2 / 27.4$</td>
<td style="text-align: right;">$52.1 / 18.8$</td>
<td style="text-align: right;">$46.2 / 28.2$</td>
</tr>
<tr>
<td style="text-align: left;">E</td>
<td style="text-align: right;">$97(37.6 \%)$</td>
<td style="text-align: right;">$0.2 / 99.9$</td>
<td style="text-align: right;">$84.5 / 20.6$</td>
<td style="text-align: right;">$5.2 / 94.9$</td>
<td style="text-align: right;">$53.6 / 20.5$</td>
<td style="text-align: right;">$60.8 / 11.3$</td>
<td style="text-align: right;">$52.6 / 21.7$</td>
</tr>
<tr>
<td style="text-align: left;">N</td>
<td style="text-align: right;">$44(17.1 \%)$</td>
<td style="text-align: right;">$6.8 / 93.2$</td>
<td style="text-align: right;">$100.0 / 18.2$</td>
<td style="text-align: right;">$6.8 / 93.2$</td>
<td style="text-align: right;">$43.2 / 20.5$</td>
<td style="text-align: right;">$61.4 / 2.3$</td>
<td style="text-align: right;">$43.2 / 20.5$</td>
</tr>
<tr>
<td style="text-align: left;">ALL</td>
<td style="text-align: right;">258</td>
<td style="text-align: right;">$3.5 / 96.5$</td>
<td style="text-align: right;">$88.4 / 20.2$</td>
<td style="text-align: right;">$3.5 / 96.5$</td>
<td style="text-align: right;">$48.5 / 23.6$</td>
<td style="text-align: right;">$57.0 / 13.2$</td>
<td style="text-align: right;">$48.1 / 24.4$</td>
</tr>
</tbody>
</table>
<p>Table 2: Distribution of class labels for premises-hypothesis pairs and percentage of each types of negation in premises and hypotheses. $C, E, N$ denote Contradiction, Entailment, and Neutral, respectively.
types of negation used in premises and hypotheses is presented in Table 2. Following Hossain et al. (2020), we do not enforce a uniform distribution for the Entailment, Contradiction, and Neutral classes but rather focus on constructing fluent and natural continuations which are as close to the premise as possible. Similarly, when constructing hypotheses, it was impossible to adhere to a particular type of negation or even to preserve it in all cases. Thus, while premises mostly have subclausal non-verbal negation expressed by synthetic means, the hypotheses exhibit a wider variety of patterns. It should be noted that though we report the distribution of particular negation patterns as a percentage of sentences, the values for categories do not sum to $100 \%$ as some sentences contain more than one instance of negation. Lastly, Table 3 shows the distribution of operations for each of NLI labels. In general, we find the distribution to be quite similar for the most common categories, which allows us to claim that we are not creating major artifacts during annotation.</p>
<p>To estimate the difficulty of our benchmark relative to existing benchmarks, we use BERTScore (Zhang et al., 2019) to compare the average similarity between the premise and hypothesis for the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Operation type</th>
<th style="text-align: right;">C</th>
<th style="text-align: right;">E</th>
<th style="text-align: right;">N</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Indefinite quantifier change</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">21</td>
<td style="text-align: right;">10</td>
</tr>
<tr>
<td style="text-align: left;">Numerical quantifier change</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">14</td>
</tr>
<tr>
<td style="text-align: left;">Comparative quantifier change</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Negator addition or deletion</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">27</td>
<td style="text-align: right;">33</td>
</tr>
<tr>
<td style="text-align: left;">Negator position change</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">22</td>
</tr>
<tr>
<td style="text-align: left;">Negator token change</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">Clause or sub-clause deletion</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Focus particle change</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">Lexical change</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: left;">Syntactic change</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
<p>Table 3: Distribution of operation types in each class $(\%)$
three classes. For comparison, we use a subset of the MNLI dataset (Williams et al., 2018) containing only sentences with negation, as extracted by Hossain et al. (2020) ("MNLI-neg" hereafter), and the MNLI subset of the NegNLI dataset proposed by Hossain et al. (2020) ("NegNLI" hereafter). The average similarity scores are presented in Table 4; for the Contradiction and Neutral classes, in brackets we report the absolute difference over the score for the Entailment class to show how difficult it is to differentiate them. It can be seen that in our test suite, hypotheses are substantially more similar to premises than is the case for other datasets; and it</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">MNLI- <br> neg</th>
<th style="text-align: left;">NegNLI</th>
<th style="text-align: left;">NaN- <br> NLI</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Contradiction</td>
<td style="text-align: left;">0.88</td>
<td style="text-align: left;">0.92</td>
<td style="text-align: left;">$\mathbf{0 . 9 6}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$(0.02)$</td>
<td style="text-align: left;">$(0.00)$</td>
<td style="text-align: left;">$(0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">Entailment</td>
<td style="text-align: left;">0.91</td>
<td style="text-align: left;">0.92</td>
<td style="text-align: left;">$\mathbf{0 . 9 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Neutral</td>
<td style="text-align: left;">0.89</td>
<td style="text-align: left;">0.90</td>
<td style="text-align: left;">$\mathbf{0 . 9 5}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$(0.01)$</td>
<td style="text-align: left;">$(0.02)$</td>
<td style="text-align: left;">$(0.01)$</td>
</tr>
</tbody>
</table>
<p>Table 4: Average similarity (in terms of BERTScore) between premises and hypotheses for Entailment, Contradiction and Neutral classes.
is much harder to separate classes based on lexical similarity alone, with the difference between Entailment and Contradiction classes being negligible, and the difference with Neutral being smaller than for other datasets.</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental settings</h3>
<p>For evaluation, we consider the three settings of:</p>
<ul>
<li>Standard: a three-way classification task with three labels: Entailment, Contradiction, and Neutral.</li>
<li>Binary: a binary classification task with two labels: Entailment, and Not Entailment, where we consider all Contradiction and Neutral pairs to be Not Entailment.</li>
<li>Strict: We only consider as correct those samples where all hypotheses for a given premise are assigned the correct label (Entailment, Contradiction, or Neutral).</li>
</ul>
<p>We report $F_{1}$-score for the Standard and Binary settings, and Accuracy for the Strict setting. Methods investigated include RoBERTa (Liu et al., 2019) and its CueNB (Truong et al., 2022) variant pretrained with additional negation data augmentation and a negation cue masking strategy. We finetune each model on MNLI (Williams et al., 2018) (denoted "-MNLI"), and the MNLI subset of the NegNLI dataset (Hossain et al., 2020) (denoted "-NegNLI").</p>
<h3>4.2 Main results</h3>
<p>For the first experiment, we measure the performance of a baseline RoBERTa model fine-tuned over MNLI on our test suite, in addition to other existing negation-focused NLI datasets. As shown in Table 5, the results for our evaluation set are substantially lower compared to existing NLI datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">MNLI- <br> neg</th>
<th style="text-align: left;">NegNLI</th>
<th style="text-align: left;">NaN- <br> NLI</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Contradiction</td>
<td style="text-align: left;">0.917</td>
<td style="text-align: left;">0.718</td>
<td style="text-align: left;">$\underline{0.664}$</td>
</tr>
<tr>
<td style="text-align: left;">Entailment</td>
<td style="text-align: left;">0.834</td>
<td style="text-align: left;">0.656</td>
<td style="text-align: left;">$\underline{0.648}$</td>
</tr>
<tr>
<td style="text-align: left;">Neutral</td>
<td style="text-align: left;">0.780</td>
<td style="text-align: left;">0.651</td>
<td style="text-align: left;">$\underline{0.207}$</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: left;">0.862</td>
<td style="text-align: left;">0.676</td>
<td style="text-align: left;">$\underline{0.580}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Results $\left(F_{1}\right)$ of RoBERTa-MNLI on existing negation-focused NLI benchmarks. The lowest result for each row is underlined.</p>
<p>This shows that our dataset contains many challenging instances of negation. The differences are especially stark for the Neutral class, confirming our intuition that making the sentences in a pair as similar as possible would make them more difficult for the model.</p>
<p>Figure 1 provides the confusion matrices of the baseline RoBERTa-MNLI on existing benchmarks. In NaN-NLI, most errors are from over-predicting Entailment. This again shows that the sentences in our pairs are very similar lexically, and also reconfirms the known bias in MNLI that lexical overlap is a strong cue for entailment (McCoy et al., 2019). On the other hand, for MNLI-neg and NegNLI, the performance for the Contradiction class is the highest. This again reveals a bias in MNLI training data, in that if there is negation in either the premise or hypothesis, the labels are more likely to be Contradiction (Gururangan et al., 2018).</p>
<p>Table 6 reports the detailed results for each class across different evaluation settings. Overall, we observe a common trend in that CueNB outperforms the baseline RoBERTa when fine-tuned on the MNLI dataset. This can be explained by the fact that CueNB is pre-trained using more text containing negations, especially non-verbal and synthetic negations (e.g. no one, nobody), resulting in better representations for those negation cues. Finetuning on the NegNLI dataset further improves performance, with both RoBERTa-MNLI-NegNLI and CueNB-MNLI-NegNLI having comparable performance but RoBERTa performing better for the Contradiction class while CueNB is more accurate for the Neutral class. For the Strict setting, we observe very low results for all models with RoBERTa-MNLI-NegNLI outperforming its CueNB counterpart by one premise. This underlines the difficulty of our test suite, and shows that current methods struggle with sub-clausal negation.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Confusion matrices of RoBERTa-MNLI on different negation-focused NLI benchmarks. $C, E, N$ denote the Contradiction, Entailment, and Neutral class respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RoBERTa-MNLI</th>
<th style="text-align: center;">RoBERTa-MNLI- <br> NegNLI</th>
<th style="text-align: center;">CueNB-MNLI</th>
<th style="text-align: center;">CueNB-MNLI- <br> NegNLI</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">St</td>
<td style="text-align: center;">Contradiction</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.651</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Entailment</td>
<td style="text-align: center;">0.648</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.694</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Neutral</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.395</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.624</td>
</tr>
<tr>
<td style="text-align: center;">Binary</td>
<td style="text-align: center;">Entailment</td>
<td style="text-align: center;">0.648</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.694</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Not Entailment</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.744</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.769</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.741</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Strict</td>
<td style="text-align: center;">$0.250(12 / 48)$</td>
<td style="text-align: center;">0.292 (14/48)</td>
<td style="text-align: center;">$0.250(12 / 48)$</td>
<td style="text-align: center;">$0.271(13 / 48)$</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on our proposed NaN-NLI test suite</p>
<h2>5 Discussion</h2>
<p>We further investigate the results of the best performing model RoBERTa-MNLI-NegNLI in detail to explore potential patterns in the model's predictions on our test suite.</p>
<h3>5.1 What types of negation are hard?</h3>
<p>First, we break down the results by the type of negation used in the premise or hypothesis. There is a substantial difference in performance between samples with analytic and synthetic negation, the latter being more difficult to classify (see Appendix B for details). Considering that in previous datasets negation was expressed primarily by analytic means, we can conclude that the abundance of synthetic negation patterns in our dataset also contributes to its difficulty. In terms of the relation between negation types and inference labels assigned by the models, one significant ${ }^{2}$ pattern we notice is that when there is no negation in the hypothesis, models assign Entailment more often. Moreover, there is a significant ${ }^{2}$ preference to assign Neutral label when there are analytic negations in the premise</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>compared to synthetic negation. We argue that this is due to the fact that Neutral is the majority class in NegNLI training data.</p>
<p>We further investigate the results based on negation constructions (Section 3.2.1) and operations types (Section 3.2.2). Here, we report error rate, which is the ratio of wrongly predicted samples over all samples in the same construction/modification category. As for linguistic constructions, we find that the most difficult constructions relate to negation in the context of a quantifier, which we further investigate in Section 5.2. Following that, graded adjectives/adverbs, absolute and approximate negators, and degree expressions are among the more challenging construction types for the model to handle. On the other hand, the model deals with coordinations, implicit propositions, and verbless clauses well, with close to zero errors. Following a similar trend, making changes to the quantifiers (either indefinite or comparative) generally confuses the model. We find substantially high error rates for the remaining types of operation except for syntactic change, showing that the model is robust to changing the order of clauses and phrases. Table 7 shows some examples of P-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Premise</th>
<th style="text-align: center;">Hypothesis</th>
<th style="text-align: center;">Gold</th>
<th style="text-align: center;">Predict</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Not even then did he lose patience.</td>
<td style="text-align: center;">Even then, he did not lose patience.</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">He did not lose patience even then.</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Not only then did he lose patience. Only then did he lose patience.</td>
<td style="text-align: center;">C C</td>
<td style="text-align: center;">E E</td>
</tr>
<tr>
<td style="text-align: center;">I found his story not wholly convincing.</td>
<td style="text-align: center;">I did not find his story wholly convincing.</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I found his story wholly not convincing.</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I found his story wholly convincing.</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I did not find his story wholly not convincing</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;">Not one, not two, but three of them made the mistake.</td>
<td style="text-align: center;">More than three of them made the mistake.</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">More than two of them made the mistake.</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">More than one of them made the mistake.</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">One of them did not make the mistake.</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Two of them did not make the mistake.</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Less than two of them made the mistake.</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Less than three of them made the mistake.</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Less than four of them made the mistake.</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;">He was here not ten minutes ago.</td>
<td style="text-align: center;">He was here less than ten minutes ago.</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">He was not here less than ten minutes ago.</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">He was here more than ten minutes ago.</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">He was not here more than ten minutes ago.</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">He was not here ten minutes ago.</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">He was here one minute ago.</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">He was here twenty minutes ago.</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
</tbody>
</table>
<p>Table 7: Selected samples along with the predictions of RoBERTa-MNLI-NegNLI. Highlighting is used to indicate prediction errors.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Construction type</th>
<th style="text-align: left;">ER</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">not + quantifier</td>
<td style="text-align: left;">$\mathbf{0 . 5 5 9}$</td>
</tr>
<tr>
<td style="text-align: left;">not + focus particle</td>
<td style="text-align: left;">0.261</td>
</tr>
<tr>
<td style="text-align: left;">not + degree expression</td>
<td style="text-align: left;">0.300</td>
</tr>
<tr>
<td style="text-align: left;">not + affixially-negated adjective/adverb</td>
<td style="text-align: left;">0.423</td>
</tr>
<tr>
<td style="text-align: left;">not + PP</td>
<td style="text-align: left;">0.067</td>
</tr>
<tr>
<td style="text-align: left;">Absolute and approximate negator</td>
<td style="text-align: left;">0.333</td>
</tr>
<tr>
<td style="text-align: left;">not in verbless clause</td>
<td style="text-align: left;">0.077</td>
</tr>
<tr>
<td style="text-align: left;">not in coordination</td>
<td style="text-align: left;">0.000</td>
</tr>
<tr>
<td style="text-align: left;">not in implicit proposition</td>
<td style="text-align: left;">0.000</td>
</tr>
</tbody>
</table>
<p>Table 8: Error rates (ER) of negation constructions</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Operation type</th>
<th style="text-align: left;">ER</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Indefinite quantifier change</td>
<td style="text-align: left;">0.486</td>
</tr>
<tr>
<td style="text-align: left;">Numerical quantifier change</td>
<td style="text-align: left;">0.333</td>
</tr>
<tr>
<td style="text-align: left;">Comparative quantifier change</td>
<td style="text-align: left;">$\mathbf{0 . 6 5 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Negator addition or deletion</td>
<td style="text-align: left;">0.364</td>
</tr>
<tr>
<td style="text-align: left;">Negator position change</td>
<td style="text-align: left;">0.327</td>
</tr>
<tr>
<td style="text-align: left;">Negator token change</td>
<td style="text-align: left;">0.333</td>
</tr>
<tr>
<td style="text-align: left;">Clause or sub-clause deletion</td>
<td style="text-align: left;">0.333</td>
</tr>
<tr>
<td style="text-align: left;">Focus particle change</td>
<td style="text-align: left;">0.375</td>
</tr>
<tr>
<td style="text-align: left;">Lexical change</td>
<td style="text-align: left;">0.308</td>
</tr>
<tr>
<td style="text-align: left;">Syntactic change</td>
<td style="text-align: left;">0.000</td>
</tr>
</tbody>
</table>
<p>Table 9: Error rates (ER) across operation types</p>
<p>H pairs, together with their correct and predicted labels.</p>
<h3>5.2 Using NaN-NLI as a test suite for determining the bounds of quantification</h3>
<p>In over half of the samples in our test suite (133), negation interplays with quantification in terms of upper and lower bounds. In the easiest case, if a premise negates a proposition for all members of a set (None of them supported her), a contradictory hypothesis would assert that same proposition for any number of members of the set (One of them supported her). However, it can be hard even for humans to determine if an expression involving quantification is True or False with regard to another proposition, as it can involve not only indefinite (any, some, none, many) and numeric (one, two, twenty) quantifiers, but also comparative quantifiers (more, less), gradable adjectives (attractive $\rightarrow$ non unattractive $\rightarrow$ not attractive $\rightarrow$ unattractive), or adverbs of frequency (never, seldom, not often, sometimes, etc). As negation makes this task even harder, we maintain that our test set can be a valuable resource for testing the sensitivity of models to changing of quantification bounds.</p>
<p>As can be seen from Table 10, the performance of the model drops even further on the quantification subset, showing that quantification adds to the difficulty of classification. Interestingly, though, it slightly increases for the Neutral class while plum-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">NaN-NLI</th>
<th style="text-align: left;">NaN-Quant</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Contradiction</td>
<td style="text-align: left;">0.692</td>
<td style="text-align: left;">$\underline{0.477}$</td>
</tr>
<tr>
<td style="text-align: left;">Entailment</td>
<td style="text-align: left;">0.684</td>
<td style="text-align: left;">$\underline{0.600}$</td>
</tr>
<tr>
<td style="text-align: left;">Neutral</td>
<td style="text-align: left;">$\underline{0.366}$</td>
<td style="text-align: left;">0.379</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: left;">0.629</td>
<td style="text-align: left;">$\underline{0.486}$</td>
</tr>
</tbody>
</table>
<p>Table 10: Results $\left(F_{1}\right)$ on the whole NaN-NLI dataset vs. its quantification subset (NaN-Quant). The lowest result for each row is underlined.
meting for the easiest class of Contradiction. We notice that often it is due to inability of the model to detect the lower or upper bound of proposition, that is, where it ceases to hold. For example, here the model correctly predicts Entailment as more than two is still within the quantification bounds:</p>
<p>Not one, not two, but three of them made the mistake. $\Rightarrow$ More than two of them made the mistake.</p>
<p>However, when we increment the number past the bound of two, the hypothesis becomes contradictory, but the model fails to detect that and still predicts Entailment, possibly because three is also present in the premise:</p>
<p>Not one, not two, but three of them made the mistake. $\Rightarrow$ More than three of them made the mistake.</p>
<p>In a similar way, such phrases as not two years ago implicate a lower bound of the proposition, implying that it is False for numbers smaller than two, but the model's prediction of Neutral instead of Contradiction does not reflect that:</p>
<p>Not two years ago this company was ranked in the top ten. $\Rightarrow$ One year ago this company was ranked in the top ten.</p>
<h3>5.3 Does gender affect negation?</h3>
<p>We manually augment the test suite with simple heuristics to investigate whether gender has an effect on negation. In particular, when the sentences pairs contain a gender-specific pronouns or names, we would generate an equivalent set of sentences pairs with alternate gender pronouns or names (e.g. he $\rightarrow$ she, Ed $\rightarrow$ Sally). In general, we notice no difference between the original and the genderaltered samples, showing that gender bias does not affect the types of negations in our test suite.</p>
<h3>5.4 Limitations</h3>
<p>The most prominent limitation of our test suite is unbalanced classes distribution, especially for the Neutral class. As discussed in Section 3.2.2, the fact that we try to construct the hypotheses by making minimum edits to the premises would make it very hard to construct meaningful Neutral samples. However, we argue that this is acceptable for the evaluation set, as it does not cause bias in training models.</p>
<p>Additionally, our test suite samples are mostly in the general English domain. As shown in previous work (Khandelwal and Sawant, 2020; Truong et al., 2022), the ways that negation is represented varies substantially across domains, and there may be other potentially challenging patterns of negation in other domains or in specific text types (e.g. in clinical notes), as well as other languages (JimÃ©nezZafra et al., 2021). These directions we leave for subsequent work.</p>
<h2>6 Conclusion</h2>
<p>In this work, we proposed a new test suite, dubbed NaN-NLI, for probing the performance of NLP models on data containing sub-clausal negation. In addition to standard NLI labels, we also annotated the test suite using a systematic linguistic framework. NaN-NLI facilitates extensive analysis of negation instances based on their negation and construction type. Extensive experiments show that our test suite is significantly harder for existing models than existing benchmarks, and reveal the limited capabilities of pretrained language models in dealing with this type of negation. Detailed analysis of the results reveals a class of negations that are particularly challenging, namely those involving quantifiers, showing that our test suite can also be used as a resource to evaluate the upper and lower bounds of quantification.</p>
<h2>Acknowledgement</h2>
<p>The authors would like to thank the anonymous reviewers for their constructive reviews. This research was undertaken using the LIEF HPCGPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200. This research was conducted by the Australian Research Council Training Centre in Cognitive Computing for Medical Technologies (project number ICI70200030) and funded by the Australian Government.</p>
<h2>References</h2>
<p>Catherine Anderson. 2018. Essentials of Linguistics. McMaster University.</p>
<p>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational linguistics, 34(4):555-596.</p>
<p>Jeremy Barnes, Erik Velldal, and Lilja Ã˜vrelid. 2021. Improving sentiment analysis with multi-task learning of negation. Natural Language Engineering, 27(2):249-269.</p>
<p>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37-46.
K. Bretonnel Cohen, Christophe Roeder, William A. Baumgartner Jr., Lawrence E. Hunter, and Karin Verspoor. 2010. Test suite design for biomedical ontology concept recognition systems. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10), Valletta, Malta. European Language Resources Association (ELRA).</p>
<p>Allyson Ettinger. 2020. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Transactions of the Association for Computational Linguistics, 8:34-48.</p>
<p>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1-9, Prague. Association for Computational Linguistics.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Mareike Hartmann, Miryam de Lhoneux, Daniel Hershcovich, Yova Kementchedjhieva, Lukas Nielsen, Chen Qiu, and Anders SÃ¸gaard. 2021. A multilingual benchmark for probing negation-awareness with minimal pairs. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 244-257, Online. Association for Computational Linguistics.</p>
<p>Md Mosharaf Hossain, Dhivya Chinnappa, and Eduardo Blanco. 2022. An analysis of negation in natural language understanding corpora. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 716-723, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Md Mosharaf Hossain, Venelin Kovatchev, Pranoy Dutta, Tiffany Kao, Elizabeth Wei, and Eduardo Blanco. 2020. An analysis of natural language inference benchmarks through the lens of negation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9106-9118, Online. Association for Computational Linguistics.</p>
<p>Salud MarÃ­a JimÃ©nez-Zafra, Noa P Cruz-DÃ­az, Maite Taboada, and MarÃ­a Teresa MartÃ­n-Valdivia. 2021. Negation detection for sentiment analysis: A case study in spanish. Natural Language Engineering, 27(2):225-248.</p>
<p>Nora Kassner and Hinrich SchÃ¼tze. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811-7818, Online. Association for Computational Linguistics.</p>
<p>Aditya Khandelwal and Suraj Sawant. 2020. NegBERT: A transfer learning approach for negation detection and scope resolution. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 5739-5748, Marseille, France. European Language Resources Association.</p>
<p>Paul Kroeger. 2018. Analyzing Meaning: An Introduction to Semantics and Pragmatics. Language Science Press.</p>
<p>Sabine Lehmann, Stephan Oepen, Sylvie Regnier-Prost, Klaus Netter, Veronika Lux, Judith Klein, Kirsten Falkedal, Frederik Fouvry, Dominique Estival, Eva Dauphin, Herve Compagnion, Judith Baur, Lorna Balkan, and Doug Arnold. 1996. TSNLP - test suites for natural language processing. In COLING 1996 Volume 2: The 16th International Conference on Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448, Florence, Italy. Association for Computational Linguistics.</p>
<p>Yifan Peng, Xiaosong Wang, Le Lu, Mohammadhadi Bagheri, Ronald Summers, and Zhiyong Lu. 2018. NegBio: a high-performance tool for negation and uncertainty detection in radiology reports. AMIA Summits on Translational Science Proceedings, 2018:188.</p>
<p>Geoffrey K. Pullum and Rodney Huddleston. 2002. Negation, chapter 9. Cambridge University Press.</p>
<p>Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 49024912, Online. Association for Computational Linguistics.</p>
<p>Gongbo Tang, Philipp RÃ¶nchen, Rico Sennrich, and Joakim Nivre. 2021. Revisiting negation in neural machine translation. Transactions of the Association for Computational Linguistics, 9:740-755.</p>
<p>Thinh Truong, Timothy Baldwin, Trevor Cohn, and Karin Verspoor. 2022. Improving negation detection with negation-focused pre-training. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4188-4193, Seattle, United States. Association for Computational Linguistics.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. BERTScore: Evaluating text generation with BERT. In International Conference on Learning Representations.</p>
<h2>A Implementation Details</h2>
<p>All models are implemented using the transformers package from HuggingFace (Wolf et al., 2020). We use the base variant of RoBERTa. For fine-tuning on NegNLI, we split the dataset into training/validation sets with a $85: 15$ ratio.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyper-parameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">lr</td>
<td style="text-align: center;">$3 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: center;">epochs</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
</tbody>
</table>
<p>Table 11: Hyper-parameters for fine-tuning on MNLI</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyper-parameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">lr</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: center;">epochs</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
</tbody>
</table>
<p>Table 12: Hyper-parameters for fine-tuning on NegNLI</p>
<h2>B Results by Negation Types</h2>
<p>In Table 13 we show the performance of one of the models (RoBERTa-MNLI-NegNLI) for samples with a particular type of negation used in the premise or hypothesis. It should be noted that since in the premises negation was almost exclusively non-verbal and sub-clausal, the results for some categories (Premise - Verbal, Premise - Clausal) are not meaningful.</p>
<h2>C Prediction Examples</h2>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Negation type</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">$F_{1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Premise</td>
<td style="text-align: center;">Verbal</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Non-Verbal</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Analytic</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Synthetic</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Clausal</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sub-clausal</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;">Hypothesis</td>
<td style="text-align: center;">Verbal</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Non-Verbal</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.57</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Analytic</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Synthetic</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Clausal</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.57</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sub-clausal</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.57</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.58</td>
</tr>
</tbody>
</table>
<p>Table 13: Macro-averaged results for RoBERTa-MNLI-NegNLI by negation type</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ As determined by the $\chi^{2}$ test with $p$-value $&lt;0.05$&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7059 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7059</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7059</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-273186279</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.05229v2.pdf" target="_blank">GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs'capabilities and limitations in mathematical reasoning.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7059.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7059.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM-Symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM‑Symbolic benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controllable, template-based variant of GSM8K that generates many instantiations of grade‑school math problems (variable names and numeric values changed, clauses added/removed) to evaluate robustness and true logical reasoning of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GSM-Symbolic (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Symbolic template generation and controlled instance sampling of GSM8K questions to produce distributions of test instances with varied names, numbers and clause counts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>benchmark / dataset</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>generated from GSM8K templates; numeric ranges chosen near GSM8K distributions</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>evaluation framework (uses Chain‑of‑Thought prompting for model evaluation in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-Symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Template-driven variants of GSM8K producing many instantiations to test sensitivity to superficial changes (names), numeric changes (values), number of clauses (difficulty), and added irrelevant clauses (No‑Op).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>grade‑school math word problems / arithmetic reasoning / discrete multi‑step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>8‑shot Chain‑of‑Thought accuracy (%) with empirical distribution (mean ± std)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported per model; distributions across 50 datasets of 100 examples each, e.g., models show drops of several percentage points vs. GSM8K and large variance (stds reported per model in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to single fixed GSM8K test accuracy, GSM‑Symbolic yields lower mean accuracies and non‑negligible variance across instantiations (many models: GSM8K accuracy lies to the right of GSM‑Symbolic distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Viewing performance as a distribution reveals fragility: small numeric changes or added clauses reduce accuracy and increase variance; many models show statistically significant drops on GSM‑Symbolic versus the original GSM8K subset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Focuses on grade‑school arithmetic style reasoning; does not test higher‑order theorem proving; potential residual influence of arithmetic difficulty is analyzed and argued to be minimal for chosen ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7059.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7059.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM-NoOp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM‑NoOp benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GSM‑Symbolic variant that injects seemingly relevant but operationally irrelevant clauses ('No‑Op' statements) into problems to test whether models can ignore irrelevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GSM-NoOp (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset created by adding No‑Op clauses to GSM‑Symbolic templates to probe distractibility and the ability to identify relevance in reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>benchmark / dataset</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>derived from GSM‑Symbolic templates; No‑Op clauses seeded to resemble plausible operations</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>evaluation (8‑shot CoT prompting used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-NoOp</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Tasks are grade‑school math problems augmented with extra statements that are irrelevant to the computation; designed to reveal whether models blindly convert textual statements into operations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>distractor robustness / relevance detection in multi‑step arithmetic problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>8‑shot Chain‑of‑Thought accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Severe drops reported across models; e.g., Phi‑3‑mini drops >65% (from ~83.7% on GSM8K to ~18.0% on NoOp), other models show tens of percentage‑point declines (Table 1 reports model‑wise NoOp means and stds).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Large negative deltas vs. GSM8K and GSM‑Symbolic; few‑shot examples containing No‑Ops do not recover the majority of the loss for most models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Models frequently treat No‑Op statements as operations (e.g., subtracting donated items, applying discount multiplicatively) causing catastrophic performance declines; in‑context examples with the same No‑Op do not reliably fix the issue.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Designed for grade‑school math style problems; does not prove inability for all forms of reasoning, but demonstrates a strong failure mode in current LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7059.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7059.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT (8‑shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain‑of‑Thought prompting (8‑shot, greedy decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An in‑context prompting method that elicits multi‑step natural language reasoning traces from LLMs by providing 8 worked examples (CoT) before the target question; used as the default evaluation technique in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain‑of‑Thought (8‑shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting technique rather than a model; provides exemplars with intermediate steps to encourage multistep reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>prompting technique</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>8 exemplar GSM8K CoT shots used in main experiments; variants used for some ablations (GSM‑Symbolic shots, GSM‑NoOp shots)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>chain‑of‑thought prompting; experiments use greedy decoding except where API forced others</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K / GSM‑Symbolic / GSM‑NoOp</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used to evaluate multi‑step arithmetic reasoning under CoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi‑step arithmetic reasoning; demonstration of intermediate reasoning chain</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) under 8‑shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Used across experiments; authors report that varying the number of shots had little effect in preliminaries, and that CoT with 8 shots is the standard evaluation (results shown in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT is the common baseline for their evaluations; providing CoT exemplars did not prevent catastrophic drops on GSM‑NoOp, and changing the source of shots (GSM‑Symbolic vs NoOp) had limited recovery for many models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT prompting elicits reasoning traces but does not guarantee formal logical reasoning; CoT does not reliably solve distractor (No‑Op) failures for most models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not convert pattern‑matching into formal reasoning; reliance on exemplar selection and possible data contamination can bias outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7059.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7059.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT‑4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT‑4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state‑of‑the‑art closed‑source multimodal language model from OpenAI evaluated in experiments; demonstrates strong arithmetic reasoning but also shows fragility to No‑Op distractors and increased clause counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT‑4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's advanced transformer‑based model family (GPT‑4 series); closed model, high performance on reasoning/knowledge tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (closed‑source)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not disclosed in paper (general large‑scale web and curated data per OpenAI), evaluated with GSM8K and GSM‑Symbolic; no special formal reasoning datasets reported</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>evaluated with 8‑shot Chain‑of‑Thought prompting (greedy decoding where available)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Grade‑school arithmetic benchmark and the paper's symbolic/distractor variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi‑step arithmetic word problems (grade‑school level); robustness to variant instantiations and distractors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>8‑shot CoT accuracy (%) (mean ± std across 50 sets of 100 examples)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K(full): 95.2%; GSM8K(100): 95.0%; GSM‑Symbolic‑M1: 94.4% (±1.62); GSM‑Symbolic: 94.9% (±1.87); GSM‑P1: 93.9% (±2.59); GSM‑P2: 88.0% (±3.43); GSM‑NoOp: 63.1% (±4.53).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Small drop from GSM8K→GSM‑Symbolic overall vs. other open models; very large drop on GSM‑NoOp (~32 percentage points vs. GSM‑Symbolic / ~32 vs GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High absolute accuracy but still sensitive to added irrelevant clauses and increased clause counts; shows less variance than many open models but still exhibits catastrophic No‑Op failures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Despite strong baseline performance, vulnerable to distractor clauses and degradation as clause count increases; authors note possible data contamination advantages for some closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7059.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7059.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT‑4o‑mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT‑4o‑mini (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller GPT‑4o variant evaluated with CoT prompting; high performance but still exhibits significant accuracy loss on No‑Op and harder variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT‑4o‑mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller member of the GPT‑4o family; transformer‑based closed model optimized for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (closed‑source, compact variant)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not specified; evaluated with GSM8K and GSM‑Symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>8‑shot Chain‑of‑Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>same grade‑school math and symbolic/distractor variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>8‑shot CoT accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K(full): 94.2%; GSM8K(100): 95.0%; GSM‑Symbolic‑M1: 92.5% (±1.63); GSM‑Symbolic: 91.7% (±2.02); GSM‑P1: 81.1% (±3.05); GSM‑P2: 72.4% (±4.57); GSM‑NoOp: 54.1% (±3.85).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Strong baseline but larger relative drops on P1/P2 and NoOp than top closed models; NoOp drop ~40 points vs GSM8K(100).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller closed model retains high accuracy on canonical GSM8K but is considerably affected by distractors (GSM‑NoOp) and increased clauses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Substantial sensitivity to irrelevant clauses and numeric perturbations; decoding mode on APIs may be uncontrolled which can affect reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7059.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7059.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1‑preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o1‑preview (OpenAI newer series)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI 'o1' series model evaluated as a strong closed baseline; robust across many GSM‑Symbolic difficulty levels but still experiences steep drops on No‑Op.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1‑preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High‑performing transformer family member from OpenAI; reported as robust across difficulty variants in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (closed‑source)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not specified; typical large pretraining and instruction tuning per provider</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>8‑shot Chain‑of‑Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>grade‑school math and symbolic/distractor variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi‑step arithmetic reasoning; distractor robustness</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>8‑shot CoT accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K(full): 94.9%; GSM8K(100): 96.0%; GSM‑Symbolic‑M1: 93.6% (±1.68); GSM‑Symbolic: 92.7% (±1.82); GSM‑P1: 95.4% (±1.72); GSM‑P2: 94.0% (±2.38); GSM‑NoOp: 77.4% (±3.84).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Relatively small drops across Symbolic variants compared to many open models; still notable decline on NoOp (~18–19 points vs GSM‑Symbolic).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>o1‑preview displays robust mean performance and smaller variance across difficulty changes, but is not immune to No‑Op distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Although more robust than many open models, performance still degrades with added irrelevant clauses and increased clause counts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7059.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7059.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma2‑9b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma2‑9B (open model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open‑model (Gemma2 family) of practical size showing high GSM8K accuracy but substantial drops and variance under GSM‑Symbolic and especially No‑Op variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2‑9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open transformer language model (Gemma2 series) evaluated across multiple symbolic variants; near‑state‑of‑the‑art among open models in arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not specified in paper; general pretraining plus instruction tuning per Gemma2 release</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>8‑shot Chain‑of‑Thought prompting; greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>grade‑school arithmetic and symbolic/distractor variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi‑step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>8‑shot CoT accuracy (%) (mean ± std)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K(full): 85.3%; GSM8K(100): 87.0%; GSM‑Symbolic‑M1: 71.2% (±2.81); GSM‑Symbolic: 79.1% (±2.99); GSM‑P1: 44.0% (±5.69); GSM‑P2: 41.8% (±6.00); GSM‑NoOp: 22.3% (±5.11).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Large decreases from GSM8K→Symbolic variants and catastrophic failure on NoOp (≈63 point drop from GSM8K(full) to NoOp).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Strong on canonical GSM8K but demonstrates fragility: sensitive to numeric changes, clause additions, and severely distracted by No‑Op statements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High variance across instantiations; failure on No‑Op indicates pattern‑matching rather than formal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7059.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7059.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma2‑27b‑it</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma2‑27B‑instruct‑tuned</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger Gemma2 open model (27B, instruction tuned) that achieves high accuracy on GSM8K and better robustness to symbolic variants than smaller open models, but still drops on distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2‑27B‑it</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>27B parameter transformer (Gemma2 family) with instruction tuning; evaluated across GSM variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>27B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not specified; instruction tuning reported for Gemma2‑it variants</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>8‑shot Chain‑of‑Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>grade‑school arithmetic and symbolic/distractor variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi‑step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>8‑shot CoT accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K(full): 89.7%; GSM8K(100): 92.0%; GSM‑Symbolic‑M1: 90.2% (±1.86); GSM‑Symbolic: 88.3% (±2.56); GSM‑P1: 80.7% (±4.07); GSM‑P2: 63.4% (±4.14); GSM‑NoOp: 30.0% (±3.39).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Smaller mean drops on Symbolic relative to smaller open models but large collapse on NoOp and P2 difficulty (≈59.7 points from GSM8K(100) to NoOp).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Scaling/instruction tuning improves robustness to some variants, but distractor No‑Op clauses and increased clause counts still produce large accuracy reductions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Even large instruction‑tuned models fail to perform robust formal reasoning; No‑Op remains a critical failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7059.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7059.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi‑3‑mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi‑3‑mini‑128k‑instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Phi family model evaluated in the paper; exhibits high GSM8K accuracy but experiences a catastrophic >65% drop on GSM‑NoOp, illustrating extreme distractibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi‑3‑mini‑128k‑instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer‑based open model (Phi family) with instruction tuning and extended context; evaluated with CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not specified; Phi family pretraining and instruction tuning (per cited Phi report)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>8‑shot Chain‑of‑Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>grade‑school arithmetic and symbolic/distractor variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi‑step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>8‑shot CoT accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K(full): 83.7%; GSM8K(100): 85.0%; GSM‑Symbolic‑M1: 85.9% (±2.44); GSM‑Symbolic: 80.7% (±2.94); GSM‑P1: 63.4% (±5.63); GSM‑P2: 37.5% (±5.76); GSM‑NoOp: 18.0% (±3.83).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Extremely large relative collapse on NoOp (~67.0 percentage points from GSM8K(full) to NoOp), illustrating the paper's reported 'up to 65% drop' statement.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Phi‑3‑mini demonstrates that some modern open models can be catastrophically distracted by irrelevant clauses; scaling alone (within family) not sufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Susceptible to No‑Op distractors and increased complexity (P2); fine‑tuning on easier tasks (GSM‑P1) did not transfer to P2 per ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7059.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7059.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3‑8b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B Llama3 model evaluated; exhibits mixed behavior on symbolic variants (notably variance across instantiations) and performance degradation with added clauses and No‑Ops.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3‑8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open transformer family model (Llama3) at 8B parameters, instruction‑tuned variants included in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not specified in paper; Llama3 family pretraining/instruction tuning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>8‑shot Chain‑of‑Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>grade‑school arithmetic and symbolic/distractor variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>arithmetic word problems and robustness to variations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>8‑shot CoT accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K(full): 55.8%; GSM8K(100): 61.0%; GSM‑Symbolic‑M1: 79.5% (±3.62); GSM‑Symbolic: 74.6% (±2.94); GSM‑P1: 53.8% (±4.54); GSM‑P2: 12.3% (±3.43); GSM‑NoOp: 18.6% (±3.86).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Shows peculiar discrepancies between GSM8K(full) and Symbolic subsets (authors note variability across models and instances); large drop on P2 and NoOp.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Llama3‑8B is sensitive to numeric/structural changes; variance across instances is a consistent phenomenon across models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High sensitivity to increased clause complexity and irrelevant clauses; possible data set alignment effects influence absolute numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems. <em>(Rating: 2)</em></li>
                <li>Chain of thought empowers transformers to solve inherently serial problems. <em>(Rating: 2)</em></li>
                <li>A peek into token bias: Large language models are not yet genuine reasoners. <em>(Rating: 2)</em></li>
                <li>Large language models can be easily distracted by irrelevant context. <em>(Rating: 2)</em></li>
                <li>GSM‑Plus: A comprehensive benchmark for evaluating the robustness of LLMs as mathematical problem solvers. <em>(Rating: 2)</em></li>
                <li>On limitations of the transformer architecture. <em>(Rating: 1)</em></li>
                <li>When can transformers reason with abstract symbols? <em>(Rating: 1)</em></li>
                <li>Can large language models reason and plan? <em>(Rating: 1)</em></li>
                <li>One-layer transformer provably learns one-nearest neighbor in context. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7059",
    "paper_id": "paper-273186279",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "GSM-Symbolic",
            "name_full": "GSM‑Symbolic benchmark",
            "brief_description": "A controllable, template-based variant of GSM8K that generates many instantiations of grade‑school math problems (variable names and numeric values changed, clauses added/removed) to evaluate robustness and true logical reasoning of LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GSM-Symbolic (benchmark)",
            "model_description": "Symbolic template generation and controlled instance sampling of GSM8K questions to produce distributions of test instances with varied names, numbers and clause counts.",
            "model_size": null,
            "architecture_type": "benchmark / dataset",
            "training_data": "generated from GSM8K templates; numeric ranges chosen near GSM8K distributions",
            "reasoning_method": "evaluation framework (uses Chain‑of‑Thought prompting for model evaluation in experiments)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM-Symbolic",
            "benchmark_description": "Template-driven variants of GSM8K producing many instantiations to test sensitivity to superficial changes (names), numeric changes (values), number of clauses (difficulty), and added irrelevant clauses (No‑Op).",
            "task_type": "grade‑school math word problems / arithmetic reasoning / discrete multi‑step reasoning",
            "performance_metric": "8‑shot Chain‑of‑Thought accuracy (%) with empirical distribution (mean ± std)",
            "performance_value": "Reported per model; distributions across 50 datasets of 100 examples each, e.g., models show drops of several percentage points vs. GSM8K and large variance (stds reported per model in Table 1).",
            "comparison_with_baseline": "Compared to single fixed GSM8K test accuracy, GSM‑Symbolic yields lower mean accuracies and non‑negligible variance across instantiations (many models: GSM8K accuracy lies to the right of GSM‑Symbolic distribution).",
            "key_findings": "Viewing performance as a distribution reveals fragility: small numeric changes or added clauses reduce accuracy and increase variance; many models show statistically significant drops on GSM‑Symbolic versus the original GSM8K subset.",
            "limitations": "Focuses on grade‑school arithmetic style reasoning; does not test higher‑order theorem proving; potential residual influence of arithmetic difficulty is analyzed and argued to be minimal for chosen ranges.",
            "uuid": "e7059.0",
            "source_info": {
                "paper_title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GSM-NoOp",
            "name_full": "GSM‑NoOp benchmark",
            "brief_description": "A GSM‑Symbolic variant that injects seemingly relevant but operationally irrelevant clauses ('No‑Op' statements) into problems to test whether models can ignore irrelevant information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GSM-NoOp (benchmark)",
            "model_description": "Dataset created by adding No‑Op clauses to GSM‑Symbolic templates to probe distractibility and the ability to identify relevance in reasoning chains.",
            "model_size": null,
            "architecture_type": "benchmark / dataset",
            "training_data": "derived from GSM‑Symbolic templates; No‑Op clauses seeded to resemble plausible operations",
            "reasoning_method": "evaluation (8‑shot CoT prompting used in experiments)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM-NoOp",
            "benchmark_description": "Tasks are grade‑school math problems augmented with extra statements that are irrelevant to the computation; designed to reveal whether models blindly convert textual statements into operations.",
            "task_type": "distractor robustness / relevance detection in multi‑step arithmetic problems",
            "performance_metric": "8‑shot Chain‑of‑Thought accuracy (%)",
            "performance_value": "Severe drops reported across models; e.g., Phi‑3‑mini drops &gt;65% (from ~83.7% on GSM8K to ~18.0% on NoOp), other models show tens of percentage‑point declines (Table 1 reports model‑wise NoOp means and stds).",
            "comparison_with_baseline": "Large negative deltas vs. GSM8K and GSM‑Symbolic; few‑shot examples containing No‑Ops do not recover the majority of the loss for most models.",
            "key_findings": "Models frequently treat No‑Op statements as operations (e.g., subtracting donated items, applying discount multiplicatively) causing catastrophic performance declines; in‑context examples with the same No‑Op do not reliably fix the issue.",
            "limitations": "Designed for grade‑school math style problems; does not prove inability for all forms of reasoning, but demonstrates a strong failure mode in current LLMs.",
            "uuid": "e7059.1",
            "source_info": {
                "paper_title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CoT (8‑shot)",
            "name_full": "Chain‑of‑Thought prompting (8‑shot, greedy decoding)",
            "brief_description": "An in‑context prompting method that elicits multi‑step natural language reasoning traces from LLMs by providing 8 worked examples (CoT) before the target question; used as the default evaluation technique in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chain‑of‑Thought (8‑shot)",
            "model_description": "Prompting technique rather than a model; provides exemplars with intermediate steps to encourage multistep reasoning outputs.",
            "model_size": null,
            "architecture_type": "prompting technique",
            "training_data": "8 exemplar GSM8K CoT shots used in main experiments; variants used for some ablations (GSM‑Symbolic shots, GSM‑NoOp shots)",
            "reasoning_method": "chain‑of‑thought prompting; experiments use greedy decoding except where API forced others",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM8K / GSM‑Symbolic / GSM‑NoOp",
            "benchmark_description": "Used to evaluate multi‑step arithmetic reasoning under CoT prompting",
            "task_type": "multi‑step arithmetic reasoning; demonstration of intermediate reasoning chain",
            "performance_metric": "accuracy (%) under 8‑shot CoT",
            "performance_value": "Used across experiments; authors report that varying the number of shots had little effect in preliminaries, and that CoT with 8 shots is the standard evaluation (results shown in Table 1).",
            "comparison_with_baseline": "CoT is the common baseline for their evaluations; providing CoT exemplars did not prevent catastrophic drops on GSM‑NoOp, and changing the source of shots (GSM‑Symbolic vs NoOp) had limited recovery for many models.",
            "key_findings": "CoT prompting elicits reasoning traces but does not guarantee formal logical reasoning; CoT does not reliably solve distractor (No‑Op) failures for most models.",
            "limitations": "Does not convert pattern‑matching into formal reasoning; reliance on exemplar selection and possible data contamination can bias outcomes.",
            "uuid": "e7059.2",
            "source_info": {
                "paper_title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT‑4o",
            "name_full": "GPT‑4o (OpenAI)",
            "brief_description": "A state‑of‑the‑art closed‑source multimodal language model from OpenAI evaluated in experiments; demonstrates strong arithmetic reasoning but also shows fragility to No‑Op distractors and increased clause counts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT‑4o",
            "model_description": "OpenAI's advanced transformer‑based model family (GPT‑4 series); closed model, high performance on reasoning/knowledge tasks.",
            "model_size": null,
            "architecture_type": "transformer (closed‑source)",
            "training_data": "not disclosed in paper (general large‑scale web and curated data per OpenAI), evaluated with GSM8K and GSM‑Symbolic; no special formal reasoning datasets reported",
            "reasoning_method": "evaluated with 8‑shot Chain‑of‑Thought prompting (greedy decoding where available)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp",
            "benchmark_description": "Grade‑school arithmetic benchmark and the paper's symbolic/distractor variants",
            "task_type": "multi‑step arithmetic word problems (grade‑school level); robustness to variant instantiations and distractors",
            "performance_metric": "8‑shot CoT accuracy (%) (mean ± std across 50 sets of 100 examples)",
            "performance_value": "GSM8K(full): 95.2%; GSM8K(100): 95.0%; GSM‑Symbolic‑M1: 94.4% (±1.62); GSM‑Symbolic: 94.9% (±1.87); GSM‑P1: 93.9% (±2.59); GSM‑P2: 88.0% (±3.43); GSM‑NoOp: 63.1% (±4.53).",
            "comparison_with_baseline": "Small drop from GSM8K→GSM‑Symbolic overall vs. other open models; very large drop on GSM‑NoOp (~32 percentage points vs. GSM‑Symbolic / ~32 vs GSM8K).",
            "key_findings": "High absolute accuracy but still sensitive to added irrelevant clauses and increased clause counts; shows less variance than many open models but still exhibits catastrophic No‑Op failures.",
            "limitations": "Despite strong baseline performance, vulnerable to distractor clauses and degradation as clause count increases; authors note possible data contamination advantages for some closed models.",
            "uuid": "e7059.3",
            "source_info": {
                "paper_title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT‑4o‑mini",
            "name_full": "GPT‑4o‑mini (OpenAI)",
            "brief_description": "A smaller GPT‑4o variant evaluated with CoT prompting; high performance but still exhibits significant accuracy loss on No‑Op and harder variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT‑4o‑mini",
            "model_description": "Smaller member of the GPT‑4o family; transformer‑based closed model optimized for efficiency.",
            "model_size": null,
            "architecture_type": "transformer (closed‑source, compact variant)",
            "training_data": "not specified; evaluated with GSM8K and GSM‑Symbolic",
            "reasoning_method": "8‑shot Chain‑of‑Thought prompting",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp",
            "benchmark_description": "same grade‑school math and symbolic/distractor variants",
            "task_type": "arithmetic word problems",
            "performance_metric": "8‑shot CoT accuracy (%)",
            "performance_value": "GSM8K(full): 94.2%; GSM8K(100): 95.0%; GSM‑Symbolic‑M1: 92.5% (±1.63); GSM‑Symbolic: 91.7% (±2.02); GSM‑P1: 81.1% (±3.05); GSM‑P2: 72.4% (±4.57); GSM‑NoOp: 54.1% (±3.85).",
            "comparison_with_baseline": "Strong baseline but larger relative drops on P1/P2 and NoOp than top closed models; NoOp drop ~40 points vs GSM8K(100).",
            "key_findings": "Smaller closed model retains high accuracy on canonical GSM8K but is considerably affected by distractors (GSM‑NoOp) and increased clauses.",
            "limitations": "Substantial sensitivity to irrelevant clauses and numeric perturbations; decoding mode on APIs may be uncontrolled which can affect reproducibility.",
            "uuid": "e7059.4",
            "source_info": {
                "paper_title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "o1‑preview",
            "name_full": "o1‑preview (OpenAI newer series)",
            "brief_description": "OpenAI 'o1' series model evaluated as a strong closed baseline; robust across many GSM‑Symbolic difficulty levels but still experiences steep drops on No‑Op.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "o1‑preview",
            "model_description": "High‑performing transformer family member from OpenAI; reported as robust across difficulty variants in the paper.",
            "model_size": null,
            "architecture_type": "transformer (closed‑source)",
            "training_data": "not specified; typical large pretraining and instruction tuning per provider",
            "reasoning_method": "8‑shot Chain‑of‑Thought prompting",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp",
            "benchmark_description": "grade‑school math and symbolic/distractor variants",
            "task_type": "multi‑step arithmetic reasoning; distractor robustness",
            "performance_metric": "8‑shot CoT accuracy (%)",
            "performance_value": "GSM8K(full): 94.9%; GSM8K(100): 96.0%; GSM‑Symbolic‑M1: 93.6% (±1.68); GSM‑Symbolic: 92.7% (±1.82); GSM‑P1: 95.4% (±1.72); GSM‑P2: 94.0% (±2.38); GSM‑NoOp: 77.4% (±3.84).",
            "comparison_with_baseline": "Relatively small drops across Symbolic variants compared to many open models; still notable decline on NoOp (~18–19 points vs GSM‑Symbolic).",
            "key_findings": "o1‑preview displays robust mean performance and smaller variance across difficulty changes, but is not immune to No‑Op distractors.",
            "limitations": "Although more robust than many open models, performance still degrades with added irrelevant clauses and increased clause counts.",
            "uuid": "e7059.5",
            "source_info": {
                "paper_title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Gemma2‑9b",
            "name_full": "Gemma2‑9B (open model)",
            "brief_description": "An open‑model (Gemma2 family) of practical size showing high GSM8K accuracy but substantial drops and variance under GSM‑Symbolic and especially No‑Op variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma2‑9B",
            "model_description": "Open transformer language model (Gemma2 series) evaluated across multiple symbolic variants; near‑state‑of‑the‑art among open models in arithmetic tasks.",
            "model_size": "9B",
            "architecture_type": "transformer",
            "training_data": "not specified in paper; general pretraining plus instruction tuning per Gemma2 release",
            "reasoning_method": "8‑shot Chain‑of‑Thought prompting; greedy decoding",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp",
            "benchmark_description": "grade‑school arithmetic and symbolic/distractor variants",
            "task_type": "multi‑step arithmetic word problems",
            "performance_metric": "8‑shot CoT accuracy (%) (mean ± std)",
            "performance_value": "GSM8K(full): 85.3%; GSM8K(100): 87.0%; GSM‑Symbolic‑M1: 71.2% (±2.81); GSM‑Symbolic: 79.1% (±2.99); GSM‑P1: 44.0% (±5.69); GSM‑P2: 41.8% (±6.00); GSM‑NoOp: 22.3% (±5.11).",
            "comparison_with_baseline": "Large decreases from GSM8K→Symbolic variants and catastrophic failure on NoOp (≈63 point drop from GSM8K(full) to NoOp).",
            "key_findings": "Strong on canonical GSM8K but demonstrates fragility: sensitive to numeric changes, clause additions, and severely distracted by No‑Op statements.",
            "limitations": "High variance across instantiations; failure on No‑Op indicates pattern‑matching rather than formal reasoning.",
            "uuid": "e7059.6",
            "source_info": {
                "paper_title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Gemma2‑27b‑it",
            "name_full": "Gemma2‑27B‑instruct‑tuned",
            "brief_description": "A larger Gemma2 open model (27B, instruction tuned) that achieves high accuracy on GSM8K and better robustness to symbolic variants than smaller open models, but still drops on distractors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma2‑27B‑it",
            "model_description": "27B parameter transformer (Gemma2 family) with instruction tuning; evaluated across GSM variants.",
            "model_size": "27B",
            "architecture_type": "transformer",
            "training_data": "not specified; instruction tuning reported for Gemma2‑it variants",
            "reasoning_method": "8‑shot Chain‑of‑Thought prompting",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp",
            "benchmark_description": "grade‑school arithmetic and symbolic/distractor variants",
            "task_type": "multi‑step arithmetic word problems",
            "performance_metric": "8‑shot CoT accuracy (%)",
            "performance_value": "GSM8K(full): 89.7%; GSM8K(100): 92.0%; GSM‑Symbolic‑M1: 90.2% (±1.86); GSM‑Symbolic: 88.3% (±2.56); GSM‑P1: 80.7% (±4.07); GSM‑P2: 63.4% (±4.14); GSM‑NoOp: 30.0% (±3.39).",
            "comparison_with_baseline": "Smaller mean drops on Symbolic relative to smaller open models but large collapse on NoOp and P2 difficulty (≈59.7 points from GSM8K(100) to NoOp).",
            "key_findings": "Scaling/instruction tuning improves robustness to some variants, but distractor No‑Op clauses and increased clause counts still produce large accuracy reductions.",
            "limitations": "Even large instruction‑tuned models fail to perform robust formal reasoning; No‑Op remains a critical failure mode.",
            "uuid": "e7059.7",
            "source_info": {
                "paper_title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Phi‑3‑mini",
            "name_full": "Phi‑3‑mini‑128k‑instruct",
            "brief_description": "A Phi family model evaluated in the paper; exhibits high GSM8K accuracy but experiences a catastrophic &gt;65% drop on GSM‑NoOp, illustrating extreme distractibility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Phi‑3‑mini‑128k‑instruct",
            "model_description": "Transformer‑based open model (Phi family) with instruction tuning and extended context; evaluated with CoT prompting.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": "not specified; Phi family pretraining and instruction tuning (per cited Phi report)",
            "reasoning_method": "8‑shot Chain‑of‑Thought prompting",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp",
            "benchmark_description": "grade‑school arithmetic and symbolic/distractor variants",
            "task_type": "multi‑step arithmetic word problems",
            "performance_metric": "8‑shot CoT accuracy (%)",
            "performance_value": "GSM8K(full): 83.7%; GSM8K(100): 85.0%; GSM‑Symbolic‑M1: 85.9% (±2.44); GSM‑Symbolic: 80.7% (±2.94); GSM‑P1: 63.4% (±5.63); GSM‑P2: 37.5% (±5.76); GSM‑NoOp: 18.0% (±3.83).",
            "comparison_with_baseline": "Extremely large relative collapse on NoOp (~67.0 percentage points from GSM8K(full) to NoOp), illustrating the paper's reported 'up to 65% drop' statement.",
            "key_findings": "Phi‑3‑mini demonstrates that some modern open models can be catastrophically distracted by irrelevant clauses; scaling alone (within family) not sufficient.",
            "limitations": "Susceptible to No‑Op distractors and increased complexity (P2); fine‑tuning on easier tasks (GSM‑P1) did not transfer to P2 per ablation.",
            "uuid": "e7059.8",
            "source_info": {
                "paper_title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Llama3‑8b",
            "name_full": "Llama 3 8B",
            "brief_description": "An 8B Llama3 model evaluated; exhibits mixed behavior on symbolic variants (notably variance across instantiations) and performance degradation with added clauses and No‑Ops.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3‑8B",
            "model_description": "Open transformer family model (Llama3) at 8B parameters, instruction‑tuned variants included in evaluations.",
            "model_size": "8B",
            "architecture_type": "transformer",
            "training_data": "not specified in paper; Llama3 family pretraining/instruction tuning",
            "reasoning_method": "8‑shot Chain‑of‑Thought prompting",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM8K, GSM‑Symbolic, GSM‑P1, GSM‑P2, GSM‑NoOp",
            "benchmark_description": "grade‑school arithmetic and symbolic/distractor variants",
            "task_type": "arithmetic word problems and robustness to variations",
            "performance_metric": "8‑shot CoT accuracy (%)",
            "performance_value": "GSM8K(full): 55.8%; GSM8K(100): 61.0%; GSM‑Symbolic‑M1: 79.5% (±3.62); GSM‑Symbolic: 74.6% (±2.94); GSM‑P1: 53.8% (±4.54); GSM‑P2: 12.3% (±3.43); GSM‑NoOp: 18.6% (±3.86).",
            "comparison_with_baseline": "Shows peculiar discrepancies between GSM8K(full) and Symbolic subsets (authors note variability across models and instances); large drop on P2 and NoOp.",
            "key_findings": "Llama3‑8B is sensitive to numeric/structural changes; variance across instances is a consistent phenomenon across models.",
            "limitations": "High sensitivity to increased clause complexity and irrelevant clauses; possible data set alignment effects influence absolute numbers.",
            "uuid": "e7059.9",
            "source_info": {
                "paper_title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought empowers transformers to solve inherently serial problems.",
            "rating": 2
        },
        {
            "paper_title": "A peek into token bias: Large language models are not yet genuine reasoners.",
            "rating": 2
        },
        {
            "paper_title": "Large language models can be easily distracted by irrelevant context.",
            "rating": 2
        },
        {
            "paper_title": "GSM‑Plus: A comprehensive benchmark for evaluating the robustness of LLMs as mathematical problem solvers.",
            "rating": 2
        },
        {
            "paper_title": "On limitations of the transformer architecture.",
            "rating": 1
        },
        {
            "paper_title": "When can transformers reason with abstract symbols?",
            "rating": 1
        },
        {
            "paper_title": "Can large language models reason and plan?",
            "rating": 1
        },
        {
            "paper_title": "One-layer transformer provably learns one-nearest neighbor in context.",
            "rating": 1
        }
    ],
    "cost": 0.020527499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GSM-SYMBOLIC: UNDERSTANDING THE LIMITA-TIONS OF MATHEMATICAL REASONING IN LARGE LANGUAGE MODELS
27 Aug 2025</p>
<p>Iman Mirzadeh imirzadeh@apple.com 
Washington State University</p>
<p>Keivan Alizadeh 
Washington State University</p>
<p>Hooman Shahrokhi 
Washington State University</p>
<p>Oncel Tuzel 
Washington State University</p>
<p>Samy Bengio 
Washington State University</p>
<p>Mehrdad Farajtabar farajtabar@apple.com 
Washington State University</p>
<p>Apple 
Washington State University</p>
<p>GSM-SYMBOLIC: UNDERSTANDING THE LIMITA-TIONS OF MATHEMATICAL REASONING IN LARGE LANGUAGE MODELS
27 Aug 202589D16148FFE5C7A51B9E7BCB0F53AE06arXiv:2410.05229v2[cs.LG]
Recent advancements in Large Language Models (LLMs) have sparked interest in their mathematical reasoning capabilities.While performance on the widely popular GSM8K benchmark has improved, questions remain about whether reported evaluation metrics are reliable, and reasoning abilities of LLMs have advanced.To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions.GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question.Specifically, the performance of models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark.Furthermore, we investigate the fragility of mathematical reasoning in these models and demonstrate that their performance significantly deteriorates as the number of clauses in a question increases.We hypothesize that this decline is due to the fact that current LLMs are not capable of genuine logical reasoning; instead, they attempt to replicate the reasoning steps observed in their training data.When we add a single clause that appears relevant to the question, we observe significant performance drops (up to 65%) across all state-of-the-art models, even though the added clause does not contribute to the reasoning chain needed to reach the final answer.Overall, our work provides a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning. 1</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including natural language processing, question answering, and creative tasks (Gunter et al., 2024;OpenAI, 2023;Dubey et al., 2024;Anil et al., 2023;Abdin et al., 2024;Rivière et al., 2024).Their potential to perform complex reasoning tasks, particularly in coding and mathematics, has garnered significant attention from researchers and practitioners.</p>
<p>However, the question of whether current LLMs are genuinely capable of true logical reasoning remains an important research focus.While some studies highlight impressive capabilities, a closer examination reveals substantial limitations.Literature suggests that the reasoning process in LLMs is probabilistic pattern-matching rather than formal reasoning (Jiang et al., 2024).Although LLMs can match more abstract reasoning patterns, they fall short of true logical reasoning.Small changes in input tokens can drastically alter model outputs, indicating a strong token bias and suggesting that these models are highly sensitive and fragile (Jiang et al., 2024;Shi et al., 2023).Additionally, in tasks requiring the correct selection of multiple tokens, the probability of arriving at an accurate answer decreases exponentially with the number of tokens or steps involved, underscoring their inherent unreliability in complex reasoning scenarios (Schaeffer et al., 2023).</p>
<p>GSM8K</p>
<p>When Sophie watches her nephew, she gets out a variety of toys for him.The bag of building blocks has 31 blocks in it.The bin of stuffed animals has 8 stuffed animals inside.The tower of stacking rings has 9 multicolored rings on it.Sophie recently bought a tube of bouncy balls, bringing her total number of toys for her nephew up to 62. How many bouncy balls came in the tube?</p>
<p>Let T be the number of bouncy balls in the tube.After buying the tube of balls, Sophie has 31+8+9+ T = 48 + T =62 toys for her nephew.Thus, T =62-48 = &lt;&lt;62-48=14&gt;&gt;14 bouncy balls came in the tube.</p>
<p>GSM Symbolic Template</p>
<p>When {name} watches her {family}, she gets out a variety of toys for him.The bag of building blocks has {x} blocks in it.The bin of stuffed animals has {y} stuffed animals inside.The tower of stacking rings has {z} multicolored rings on it.{name}recently bought a tube of bouncy balls, bringing her total number of toys she bought for her {family} up to {total}.How many bouncy balls came in the tube?#variables: -name = sample(names) -family = sample(["nephew", "cousin", "brother"]) -x = range(5, 100) -y = range(5, 100) -z = range(5, 100) -total = range(100, 500) -ans = range (85,200) #conditions: -x + y + z + ans == total Let T be the number of bouncy balls in the tube.After buying the tube of balls, {name} has {x} + {y} + {z} + T = { x + y + z } + T = {total} toys for her {family}.</p>
<p>Thus, T = {total} -{ x + y + z } = &lt;&lt;{total}-{ x + y + z }={ans}&gt;&gt;{ans} bouncy balls came in the tube.</p>
<p>Figure 1: Illustration of the GSM-Symbolic template creation process.This dataset serves as a tool to investigate the presumed reasoning capabilities of LLMs, enabling the design of controllable mathematical reasoning evaluations with more reliable metrics.Our results reveal that all state-ofthe-art LLMs exhibit significant performance variations, suggesting the fragility or lack of reasoning.</p>
<p>Mathematical reasoning is a crucial cognitive skill that supports problem-solving in numerous scientific and practical applications.Consequently, the ability of large language models (LLMs) to effectively perform mathematical reasoning tasks is key to advancing artificial intelligence and its realworld applications.The GSM8K (Grade School Math 8K) dataset Cobbe et al. (2021) has emerged as a popular benchmark for evaluating the mathematical reasoning capabilities of LLMs.While it includes simple math questions with detailed solutions, making it suitable for techniques like Chainof-Thought (CoT) prompting, it provides only a single metric on a fixed set of questions.This limitation restricts comprehensive insights into the models' mathematical reasoning.Moreover, the popularity and prevalence of GSM8K can increase the risk of inadvertent data contamination.Finally, the static nature of GSM8K does not allow for controllable experiments to understand model limitations, such as behavior under varied conditions or changes in question aspects and difficulty levels.</p>
<p>To address these limitations, a more versatile and adaptive evaluation framework is needed-one that can generate diverse question variants and adjust complexity levels to better explore the robustness and reasoning abilities of LLMs.This would facilitate a deeper understanding of the strengths and weaknesses of these models in mathematical reasoning tasks.We make the following contributions:</p>
<p>• We introduce GSM-Symbolic, an enhanced benchmark that generates diverse variants of GSM8K questions using symbolic templates (Sec.3), as shown in Fig. 1.This enables a more nuanced and reliable evaluation of LLMs' performance across various setups, moving beyond single-point accuracy metrics.Our large-scale study on 25 state-of-the-art open and closed models provides significant insights into LLMs' behavior in mathematical reasoning tasks.• We question the reliability of currently reported results on GSM8K and demonstrate that the performance of LLMs can be viewed as a distribution with unwarranted variance across different instantiations of the same question.We show that the performance of all models drops on GSM-Symbolic (Sec.4.1), hinting at potential data contamination.• We show that LLMs exhibit more robustness to changes in superficial elements like proper names but are very sensitive to changes in numerical values (Sec.4.2).We show that performance degradation and variance increase as the number of clauses increases, indicating that LLMs' reasoning capabilities struggle with increased complexity (Sec.4.3).</p>
<p>• Finally, we further question the reasoning abilities of LLMs and introduce the GSM-NoOp dataset.By adding seemingly relevant but ultimately irrelevant information to problems, we demonstrate substantial performance drops (up to 65%) across all state-of-the-art models (Sec. 4.4).This reveals a critical flaw in the models' ability to discern relevant information for problem-solving, likely because their reasoning is not formal not formal in the conventional sense and is mostly based on pattern matching.We show that even when provided with multiple examples of the same question or examples containing similar irrelevant information, LLMs struggle to overcome the challenges posed by GSM-NoOp.This suggests deeper issues in their reasoning processes that cannot be alleviated by in-context shots and needs further investigation.</p>
<p>Overall, our work provides a comprehensive understanding of the limitations of LLMs in mathematical reasoning.Our results emphasize the need for more reliable evaluation methodologies and further research into the reasoning capabilities of large language models.</p>
<p>BACKGROUND: REASONING &amp; LANGUAGE MODELS</p>
<p>Logical reasoning is a critical trait of intelligent systems, and building intelligent systems capable of such reasoning has long been an important goal in artificial intelligence (Newell &amp; Simon, 1956;Marcus, 2003;Brachman &amp; Levesque, 2004;Legg &amp; Hutter, 2007;Pearl, 2014;Chollet, 2019).</p>
<p>Recent advancements in Large Language Models (LLMs) have demonstrated significant potential across various domains; however, their reasoning abilities remain uncertain and inconsistent.Many works have investigated whether LLMs are truly capable of reasoning by examining how these models solve tasks that require logical reasoning.</p>
<p>We define logical reasoning as the process by which an agent (a human or a machine) employs logical steps to achieve a "novel" goal.The emphasis on "novelty" is crucial because it helps distinguish genuine reasoning from mere memorization of solutions or responses, or mimicking the logical steps previously encountered (Gignac &amp; Szodorai, 2024).This distinction aligns with ideas akin to the Chinese Room Argument (Cole, 2024), underscoring the difference between true comprehension and pattern-matching.Furthermore, this definition connects to several established definitions in AI literature.For instance, the "length-generalization" refers to the ability to apply known logical steps on larger inputs.Moreover, in order to successfully apply such logical steps, an agent often requires other skills such "decomposing" a problem into smaller sub-problems, and "composing" logical steps from these sub-problems in order to solve another problem.</p>
<p>One interesting direction focuses on modeling the computation performed by transformers.For example, parallels have been drawn between components such as attention and feed-forward modules and simple computational primitives (Weiss et al., 2021;Zhou et al., 2024).Delétang et al. (2023) demonstrated that transformers fail to generalize on non-regular tasks and showed that structured memory (e.g., memory tape) is necessary for handling complex tasks.This is related to the effectiveness of Chain-of-Thought (CoT) prompting (Wei et al., 2022) and using scratchpads for LLMs as additional memory for intermediate computations.Moreover, Zubic et al. (2025) show theoretically and empirically that Structured State Space Models (SSMs) and Transformers face fundamental limitations in performing function composition and complex reasoning tasks.Overall, current results suggest that while the transformer architecture has limitations and lacks the required expressiveness for solving problems across several complexity classes, these limitations can be alleviated with additional memory (e.g., scratchpads) (Liu et al., 2024).However, this still requires generating vast amounts of tokens to solve a problem (Peng et al., 2024;OpenAI, 2024).While these works provide insights into the theoretical computational complexity of transformers, in practice, it remains unclear whether these LLMs can perform formal logical reasoning to solve tasks.</p>
<p>There is a considerable body of work suggesting that the reasoning process in LLMs is fragile and not formal (Kambhampati, 2024;Valmeekam et al., 2022;2024;Nezhurina et al., 2024;McCoy et al., 2023;Zhang et al., 2023), even though it appears that these models understand symbols and can work with them to some limited degree (Boix-Adserà et al., 2024).Instead, LLMs likely perform a form of probabilistic pattern-matching and searching to find closest seen data during training without proper understanding of concepts.While this process goes beyond naive memorization of words and the models are capable of searching and matching more abstract reasoning steps, it still falls short of true formal reasoning.For instance, Jiang et al. (2024) show, with statistical guarantees, that most LLMs still struggle with logical reasoning due to strong token bias, where the reasoning output of the model changes when a single token of input changes.This aligns with our results, which indicate that the performance of models on different instances of the same mathematical question can vary greatly from one instance to another.Li et al. (2024b) prove that a single transformer layer learns a one-nearest neighbor, which could explain why the reasoning of models is highly sensitive to input tokens.Schaeffer et al. (2023) argue that when a task requires emitting multiple tokens correctly, the probability of answering correctly decreases exponentially with the number of tokens.Dziri et al. (2023) represent reasoning tasks as computation graphs and find that full computation subgraphs appear much more frequently in training data for correct predictions than incorrect ones.Razeghi et al. (2022)</p>
<p>GSM-SYMBOLIC</p>
<p>The GSM8K dataset (Cobbe et al., 2021) includes over 8000 grade school math questions and answers, divided into 7473 training and 1319 test examples.As shown in Fig. 1, the questions are relatively simple, requiring knowledge of only the four main arithmetic operations.However, since GSM8K is a single, popular test set, there is a risk of data contamination, and performance may change significantly with minor modifications to the questions.These limitations have led to efforts to generate new datasets and variants.iGSM (Ye et al., 2024) is a math dataset created through a synthetic pipeline that captures parameter dependencies in a hierarchical and graph structure.GSM-IC (Shi et al., 2023) shows that irrelevant context can impair LLM performance, focusing on prompting techniques.Our work, however, suggests a more fundamental issue: LLMs struggle even when given multiple shots of the same question, indicating deeper challenges in problemsolving that cannot be resolved with few-shot prompting or fine-tuning on unseen distractions or variations of the same or different difficulty levels.GSM-Plus (Li et al., 2024a) introduces variants of GSM8K questions but lacks symbolic templates and has a fixed size and difficulty.GSM1K (Zhang et al., 2024) mirrors the style and complexity of GSM8K to identify systematic overfitting in existing models, but it is not publicly available for researchers.</p>
<p>While the mentioned benchmarks offer a single performance metric on a fixed number of questions, we argue that viewing LLM performance as a distribution across various problem instances provides deeper insights.The design of GSM-Symbolic enables the generation of numerous instances and allows for finer control over question difficulty.We believe our paper contributes to this direction by offering a reliable evaluation framework that underscores the importance of generating multiple instances to assess LLMs' mathematical capabilities and their robustness to diverse problem difficulties and augmentations.</p>
<p>TEMPLATE GENERATION</p>
<p>Given a specific example from the test set of GSM8K, we create parsable templates as shown in Fig. 1 (right).The annotation process involves identifying variables, their domains, and necessary conditions to ensure the correctness of both the question and the answer.For instance, since the questions are grade-school level, a common condition is divisibility to ensure the answer is a whole number.We use common proper names (e.g., persons, foods, currencies) to streamline creation.After creating the templates, we apply several automated checks to ensure the annotation process is correct.For example, we verify that none of the original variable values appear in the template.</p>
<p>We also check that the original values satisfy all conditions and that the final answer matches the original question's answer.Once data are generated, 10 random samples per template are reviewed manually.As a final automated check, after evaluating all models, we verify that at least two models answer each question correctly; otherwise, the question is reviewed manually again.</p>
<p>When constructing symbolic templates, we deliberately select numerical ranges that closely align with those in the original GSM8Ktest set.This decision reflects our focus on assessing logical reasoning capabilities rather than arithmetic skills.Our analysis in Appendix A.6 confirms that the expanded ranges remain within boundaries where models maintain their arithmetic accuracy.</p>
<p>EXPERIMENTAL SETUP</p>
<p>While we provide further details on our experimental setup and evaluation in the Appendix, we briefly review the important aspects here:</p>
<p>Models.Throughout this work, we report on more than 20 open models of various sizes, ranging from 2B to 27B.Additionally, we include state-of-the-art closed models such as GPT-4o-mini, GPT-4o, o1-mini, and o1-preview.To conserve space, we present results for a few selected models in each experiment, but the full results for all models are available in Tab. 1 of the Appendix A.2.</p>
<p>Evaluation Setup Overall, for this work, we conducted nearly 500 total evaluations on various setups.To this end, we maintained a manageable dataset size by using 100 templates and generating 50 samples per template, resulting in 5000 total examples for each benchmark.Therefore, we have 50 datasets of 100 examples each, where each example is a mutation of one of the original 100 examples from GSM8K.Unless stated otherwise, we follow a common evaluation setup on GSM8K and other math benchmarks that includes Chain-of-Thought (CoT) prompting with 8-shots with greedy decoding.However, we note that in our preliminary experiments, the number of shots did not significantly change the performance and conclusions.We provide our prompt template in Fig. 9.</p>
<p>EXPERIMENTS &amp; RESULTS</p>
<p>In this section, we present our main results and postpone complementary findings to the Appendix.We begin our experiments by addressing an important question regarding the reliability of current reported metrics on GSM8K.By studying the distribution of performance on GSM-Symbolic, we demonstrate notable performance variation.More importantly, we observe that the performance of models drops on GSM-Symbolic (Sec.4.1).</p>
<p>Next, we investigate the fragility of reasoning in LLMs by comparing performance distributions when only proper names are changed versus when values and numbers are altered.Our findings indicate that the original GSM8K performance of models is much closer to the performance distribution when only names are changed.However, performance drops more significantly when values are changed, with this trend continuing as both changes are applied simultaneously (Sec.4.2).We then examine the impact of question difficulty, as indicated by the number of clauses added to or removed from the questions.Our results show that as the number of clauses increases, average performance drops, and the variance in performance increases consistently across all models (Sec.4.3).</p>
<p>Finally, in Sec.4.4, we tackle a more fundamental question: whether the models truly understand the mathematical concepts.We show that, likely due to potential pattern matching and the fact that the training distribution of models included only necessary information for solving questions, adding seemingly relevant clauses to the question that do not impact the reasoning process required to solve it significantly drops the performance of all models.</p>
<p>HOW RELIABLE ARE THE CURRENT GSM8K RESULTS?</p>
<p>As our first experiment, we evaluate the performance of several state-of-the-art models on GSM-Symbolic.The number of samples and difficulty can be adjusted by modifying variable domains, as we will see in subsequent sections.Fig. 2 shows the empirical distribution of the performance of models on GSM-Symbolic computed on these 50 datasets.As shown, all models exhibit a non-negligible variance across different sets.For instance, for the Gemma2-9B, the gap between the worst performance and the best performance is more than 12%, while for Phi-3.5-mini,this gap Gemma2-2b-it -6.2</p>
<p>Gemma2-9b -6.2</p>
<p>Gemma2-9b-it -6.2</p>
<p>Mistral-7b-it-v0.3 -6.1</p>
<p>Mathstral-7b-v0.1 -4.8</p>
<p>Phi-3-medium -4.8</p>
<p>Phi-3-small -3.9</p>
<p>Gemma2b</p>
<p>-3.9</p>
<p>Gemma2b-it -3.7</p>
<p>Gemma-7b-it -3.4</p>
<p>Mistral-7b-v0.1
-3.0 Phi-3-mini -2.8 Phi-3.5-mini-it -2.4 GPT-4o-mini -2.2 o1-preview -1.4 Gemma2-27b-it -1.3 Llama3-8b-it -0.7 Mistral-7b-v0.3 -0.6 o1-mini -0.3 GPT-4o</p>
<p>Models</p>
<p>Figure 3: The performance of all state-of-the-art models on GSM-Symbolic drops compared to GSM8K.Later, we investigate the factors that impact the performance drops in more depth.</p>
<p>is around 15%.It is interesting that this variation even exists, as the only differences across different instances of each question are the changes in names and values, while the overall reasoning steps needed to solve a question remain the same.</p>
<p>Another noteworthy observation is that the performance (represented by the dashed line in Fig. 2) on the original questions from the 100 examples of GSM8K used as templates is often more than one standard deviation away from the center of the GSM-Symbolic performance distribution, frequently on the right side of the distribution (this holds for 21 out of 25 models).One explanation for this could be data contamination, where some of the test examples from GSM8K inadvertently ended up in the training set of these models, leading to an optimistic bias in performance.Fig. 3 shows the performance drop from GSM8K to GSM-Symbolic for several models.We can see that for models such as Gemma2-9B, Phi-3, Phi-3.5, and Mathstral-7B, the dashed line in Fig. 2 lies on the right side, and the drop in performance is higher than for models such as Llama3-8b and GPT-4o, where the performance on GSM8K is close to the center of the GSM-Symbolic distribution and the drop in performance is negligible.In Appendix A.3, we present further results to support this claim for other models such as Phi-2 and Mistral-7B.These results lead us to investigate the fragility of the reasoning abilities of LLMs in the next section.</p>
<p>HOW FRAGILE IS MATHEMATICAL REASONING IN LARGE LANGUAGE MODELS?</p>
<p>In the previous sub-section, we observed high performance variation across different sets generated from the same templates, along with a performance degradation compared to the original GSM8K accuracy.This suggests that the perceived reasoning process of language models may not be formal and is hence susceptible to changes.One explanation is that these models attempt to perform a kind of in-distribution pattern-matching, aligning given questions and solution steps with similar ones seen in the training data.As no formal reasoning is involved in this process, it could lead to high variance across different instances of the same question.In this sub-section and the next one, we investigate these observations further and we show that several factors contribute to the performance variation of the models.First, we investigate the impact of the type of change to understand the difference between changing names (e.g., person names, places, foods, currencies, etc.) versus changing numbers (i.e., the values of variables).</p>
<p>Figure 4 demonstrates that while performance variation persists, the variance is lower when changing names compared to numbers.Notably, the original GSM8K accuracy of models is now much closer to the center of the changed proper names distribution, in contrast to changed numbers or both.Furthermore, a gradual shift in the means of distributions from right to left, along with an increase in variance, is evident across almost all models.It is both striking and concerning that such performance variance exists when only changing proper names, as this level of variability would not be expected from a grade-school student with genuine mathematical understanding.</p>
<p>From the results in this section, we observe that by increasing the difficulty of changes (from names to numbers), the performance drops and the variance increases, overall suggesting that the reasoning capabilities of state-of-the-art LLMs are fragile for the aforementioned reasons.Assuming that LLMs are not performing formal reasoning, how important is the question difficulty on the distribution of performance?In the next section, we study this question further.</p>
<p>HOW DOES QUESTION DIFFICULTY AFFECT PERFORMANCE DISTRIBUTION?</p>
<p>The results in the previous subsection motivate us to study the impact of question difficulty on the mean and variance of the performance distribution.To this end, we generate several new templates from the GSM-Symb, as illustrated in Fig. 5. First, by removing one clause, we obtain GSM-Symbolic-Minus-1 or GSM-M1 for short.Similarly, we can add one or two clauses Published as a conference paper at ICLR 2025 Different Levels of GSM-Symbolic Difficulty GSM-Symbolic-M1: To make a call from a phone booth, you must pay $0.6 for each minute of your call.After 10 minutes, that price drops to $0.5 per minute.How much would a 60-minute call cost?GSM-Symbolic: To make a call from a phone booth, you must pay $0.6 for each minute of your call.After 10 minutes, that price drops to $0.5 per minute.How much would a 60-minute call cost?GSM-Symbolic-P1: To make a call from a hotel room phone, you must pay $0.6 for each minute of your call.After 10 minutes, that price drops to $0.5 per minute.After 25 minutes from the start of the call, the price drops even more to $0.3 per minute.How much would a 60-minute call cost?GSM-Symbolic-P2: To make a call from a hotel room phone, you must pay $0.6 for each minute of your call.After 10 minutes, the price drops to $0.5 per minute.After 25 minutes from the start of the call, the price drops even more to $0.3 per minute.If your total bill is more than $10, you get a 25% discount.How much would a 60-minute call cost?  to the questions to increase the difficulty, resulting in GSM-Symbolic-Plus-1 (GSM-P1) and GSM-Symbolic-Plus-2 (GSM-P2), respectively2 .</p>
<p>As shown in Fig. 6, the trend of the evolution of the performance distribution is very consistent across all models: as the difficulty increases, the performance decreases and the variance increases.</p>
<p>Note that overall, the rate of accuracy drop also increases as the difficulty increases.This is in line with the hypothesis that models are not performing formal reasoning, as the number of required reasoning steps increases linearly, but the rate of drop seems to be faster.Moreover, considering the pattern-matching hypothesis, the increase in variance suggests that searching and pattern-matching become significantly harder for models as the difficulty increases.</p>
<p>GSM-NoOp</p>
<p>Oliver picks 44 kiwis on Friday.Then he picks 58 kiwis on Saturday.On Sunday, he picks double the number of kiwis he did on Friday, but five of them were a bit smaller than average.How many kiwis does Oliver have?o1-mini: Oliver picks kiwis over three days as follows:</p>
<p>-Friday: 44 kiwis -Saturday: 58 kiwis -Sunday: Double the number he picked on Friday, which is 2 × 44 = 88 kiwis However, on Sunday, 5 of these kiwis were smaller than average.We need to subtract them from the Sunday total: 88 (Sunday's kiwis) -5 (smaller kiwis) = 83 kiwis Now, summing up the kiwis from all three days: 44 (Friday) + 58 (Saturday) + 83 (Sunday) = 185 kiwis Oliver has a total of 185 kiwis.</p>
<p>Llama3-8B: Let's break it down step by step: Oliver picks 44 kiwis on Friday.He picks 58 kiwis on Saturday.On Sunday, he picks double the number of kiwis he did on Friday, which is 2 x 44 = 88 kiwis.</p>
<p>Five of the kiwis picked on Sunday are a bit smaller than average, so we subtract 5 from the total number of kiwis picked on Sunday: 88 -5 = 83 kiwis.Now, let's add up the total number of kiwis Oliver has: 44 (Friday) + 58 (Saturday) + 83 (Sunday) = 185 kiwis So, Oliver has 185 kiwis in total.We add seemingly relevant statements to the questions that are, in fact, irrelevant to the reasoning and conclusion.However, the majority of models fail to ignore these statements and blindly convert them into operations, leading to mistakes.</p>
<p>CAN LLMS REALLY UNDERSTAND MATHEMATICAL CONCEPTS?</p>
<p>In the previous sections, we studied the impact of type of change and difficulty on the performance distribution.In this section, we demonstrate that models are susceptible to catastrophic performance drops on instances not part of the training distribution, potentially due to their reliance on in-distribution pattern-matching.We introduce GSM-NoOp, a dataset designed to challenge the reasoning capabilities of language models.To create the templates, we add seemingly relevant but ultimately inconsequential statements to GSM-Symbolic templates.Since these statements carry no operational significance, we refer to them as "No-Op".</p>
<p>Fig. 7 illustrates an example from GSM-NoOp.An interesting observation is that models tend to blindly subtract the number of smaller fruits, potentially because their training datasets included similar examples that required conversion to subtraction operations.In the Appendix, we include additional failure cases from GSM-NoOp.Overall, we find that models tend to convert statements to operations without truly understanding their meaning.For instance, a common case we observe is that models interpret statements about "discount" as "multiplication", regardless of the context.This raises the question of whether these models have truly understood the mathematical concepts well enough.Consequently, as shown in Fig. 8a, there is a catastrophic performance decline across all tested models, with the Phi-3-mini model experiencing over a 65% drop, and even stronger models such as o1-preview showing significant declines.</p>
<p>To better understand this performance drop, we conducted another experiment.While our previous evaluations on GSM-P2 used the original 8-shots of GSM8K, here we explore two new scenarios where we change the source of the 8-shots.We report the results in Figures 8b and 8c.</p>
<p>NoOp-Symb (Using GSM-Symbolic shots of the same question): During evaluation, we include 8 different shots of the same question coming from GSM-Symbolic.Hence, each shot provides the required reasoning steps.The target question from GSM-NoOp then presents yet another variation of the same question that is different only in values and the added clause that is inconsequential.This setup should simplify the task by making it clear that the extra information in the target question is irrelevant.However, as shown in Fig. 8b, the performance remains within the standard deviation, even with 8 shots of the same question providing the reasoning chain.Interestingly, Fig. 8c shows that some models can perform significantly better, even though they don't perform nearly as well on GSM8K and GSM-Symbolic.We believe this is a very notable observation.</p>
<p>NoOp-NoOp (Using GSM-NoOp shots of different questions): Here, we provide 8 shots chosen randomly from different questions of GSM-NoOp in the context.These questions share the common As previously demonstrated, performance on GSM-Symbolic is very close to that on GSM8K.However, on GSM-NoOp, the significant drop in performance cannot be recovered, even when using the exact same question's variation as shots (NoOp-Symb) or when using different questions with different GSM-NoOpthat contain No-Op operations (NoOp-NoOp) as shots.(c) Notably, some models that perform significantly worse than those in (b) on GSM8K and GSM-Symbolic show much better performance on NoOp-Symb.</p>
<p>fact that the correct answer should ignore the No-Op statement.We observe that for the Llama-3-8B model, the performance remains the same compared to the original No-Op model, while for the Phi-3 model, performance slightly decreases.</p>
<p>CONCLUSION</p>
<p>In this work, we have investigated the reasoning capabilities of large language models (LLMs) and the limitations of current evaluations on GSM8K.We introduced GSM-Symbolic, a novel benchmark with multiple variants designed to provide deeper insights into the mathematical reasoning abilities of LLMs.Our extensive study reveals significant performance variability across different instantiations of the same question, challenging the reliability of current GSM8K results that rely on single-point accuracy metrics.We found that while LLMs exhibit some robustness to changes in proper names, they are more sensitive to variations in numerical values.We have also observed the performance of LLMs deteriorating as question complexity increases.</p>
<p>The introduction of GSM-NoOp exposes a critical flaw in LLMs' ability to genuinely understand mathematical concepts and discern relevant information for problem-solving.Adding seemingly relevant but ultimately inconsequential information to the logical reasoning of the problem led to substantial performance drops across models.Importantly, we demonstrate that LLMs struggle even when provided with multiple examples of the same question or examples containing similar irrelevant information.This suggests deeper issues in their reasoning processes that cannot be easily mitigated through few-shot learning or fine-tuning.</p>
<p>Ultimately, our work underscores significant limitations in LLMs' ability to perform genuine mathematical reasoning.The LLMs' high performance variance on different instances of the same question, their significant drop in performance with a slight increase in difficulty, and their sensitivity to inconsequential information indicate that their reasoning is fragile and may be more akin to sophisticated pattern matching rather than true logical reasoning.We believe further research is needed to develop AI systems capable of formal reasoning, moving beyond probabilistic pattern matching to achieve more robust and generalizable problem-solving skills.This remains a critical challenge for the field as we strive to create systems with human-like cognitive abilities or general intelligence.</p>
<p>A APPENDIX</p>
<p>In this appendix, we provide additional details to the main text, including:</p>
<p>• A.1: Detailed experimental setups, including the prompt template.</p>
<p>• A.2: Full results on GSM8K, GSM-Symbolic, and their variants.</p>
<p>• A.3: Additional results for the distributional performance of several models, similar to the results from Sec. 4.1 in the main text.</p>
<p>• A.4: Additional results for Sec.4.3, where we studied the impact of question difficulty.We show that fine-tuning on easier tasks does not necessarily improve performance on more difficult tasks.</p>
<p>• A.5: A more comprehensive discussion and analysis of performance for OpenAI o1-mini and o1-preview models.</p>
<p>A.1 DETAILED EXPERIMENTAL SETUP</p>
<p>In this work, all reported evaluations results use 8-shots with chain-of-thought prompting.We use the following prompt format:  Except for the last experiment in Sec.4.4, we use the original 8 shots from GSM8K.In addition, we allow the models to generate until either their context size limit is reached, they generate one of the end-of-response tokens such as '</s>' or '&lt;|endoftext|&gt;', or they finish answering the current question and move on to generating the next question, indicated by another 'Q:' generation.</p>
<p>Finally, we note that in all experiments we use greedy decoding to generate responses from models, with one exception: currently, the available APIs for "o1-mini" and "o1-preview" models do not allow controlling the decoding strategy, and it seems that at the time of writing, these models do not perform greedy decoding, as responses to the same prompt change.</p>
<p>A.2 FULL RESULTS</p>
<p>In Tab. 1, we present the comprehensive performance results of various models, including Gemma (Mesnard et al., 2024), Gemma2 (Rivière et al., 2024), Phi (Abdin et al., 2024), Mistral (Jiang et al., 2023), Llama3 (Dubey et al., 2024), GPT-4o (OpenAI, 2023), and the o1 (OpenAI, 2024) series, on GSM8K and its different variants, GSM-Symbolic.</p>
<p>We report two sets of results for GSM8K: the first column indicates the accuracy on the full test set of GSM8K (comprising 1,319 examples), while the second column shows the accuracy on a subset of 100 questions from the GSM8K test set, which we randomly selected to generate GSM-Symbolic templates.It is noteworthy that the performance levels across both sets are very similar, with no significant differences observed.</p>
<p>A.3 ADDITIONAL RESULTS ON GSM-SYMBOLIC PERFORMANCE DISTRIBUTIONS</p>
<p>In section 4.1, we have presented results for several models in Fig. 2. Here, we provide additional results showing the performance on GSM-Symbolic for other models also have high variance.</p>
<p>Moreover, these models correspond to highest drop.</p>
<p>Another important question regarding our results is measuring the statistical significance3 .In this work, we calculate statistical significance using the one-sample t-test with two-sided alternative hypothesis to determine whether the 50 different performance results on GSM-Symbolic differ from the original 100 questions from GSM8K (i.e., the null hypothesis)4 .As we can see in Fig. 10, for an overwhelming majority of models (except Llama3-8B and GPT-4o), the results are statistically significant.</p>
<p>Thus far, we have assessed the probability of obtaining the 50 GSM-Symbolic scores under the assumption that the population mean equals the accuracy on the original 100 questions from GSM8K.However, Ivanova et al. ( 2025) argue that a one-sample t-test is inappropriate because the evaluation involves two datasets, and that a two-sample z-test would therefore be more suitable.</p>
<p>Overall, we do not believe that extensive statistical testing adds significant value the main focus of this paper.We discuss our position further in Sec.B. Nonetheless, in the interest of transparency, we clearly state our assumptions and present alternative viewpoints.Thus, if one disagrees with our assumptions and accepts the assumptions of Ivanova et al. ( 2025), our results would likewise be statistically non-significant, as those authors have shown.</p>
<p>A.4 ABLATION: DOES FINE-TUNING ON EASIER TASKS HELP WITH MORE DIFFICULT TASKS?</p>
<p>In Sec.4.3, we observed that the performance on GSM-P2 is significantly lower than the performance on GSM-P1.We also argued that it is unlikely that additional fine-tuning or including shots from GSM-P1 would be beneficial.Here, in Fig. 11a, we show that including shots from GSM-P1 does not improve performance compared to the results where shots come solely from GSM8K.</p>
<p>Moreover, in Fig. 11b, we demonstrate that fine-tuning Phi-3.5 on GSM-P1 slightly improves performance on GSM-P1 while decreasing performance on GSM-P2.We have used a set of 50 templates from GSM-P1, separate from the test templates, and generated 10000 examples for finetuning training set.</p>
<p>Overall, while this direction warrants further research, current results suggest that scaling training data will not be helpful in improving the reasoning capabilities of language models.</p>
<p>A.5 RESULTS ON O1-PREVIEW AND O1-MINI</p>
<p>The recently released o1-preview and o1-mini models (OpenAI, 2024) have demonstrated strong performance on various reasoning and knowledge-based benchmarks.As observed in Tab. 1, the mean of their performance distribution is significantly higher than that of other open models.</p>
<p>In Fig. 12 (top), we illustrate that both models exhibit non-negligible performance variation.When the difficulty level is altered, o1-mini follows a similar pattern to other open models: as the difficulty increases, performance decreases and variance increases.</p>
<p>The o1-preview model demonstrates robust performance across all levels of difficulty, as indicated by the closeness of all distributions.However, it is important to note that both o1-preview and o1-mini experience a significant performance drop on GSM-NoOp .In Fig. 13, we illustrate that</p>
<p>GSM-NoOp</p>
<p>Liam wants to buy some school supplies.He buys 24 erasers that now cost $6.75 each, 10 notebooks that now cost $11.0 each, and a ream of bond paper that now costs $19. How much should Liam pay now, assuming that due to inflation, prices were 10% cheaper last year?</p>
<p>o1-preview: To solve the problem, we need to calculate how much Liam should pay, considering that last year's prices were 10% cheaper due to inflation.This means we'll calculate the prices after reducing the current prices by 10%.</p>
<p>Step 1: Calculate last year's prices by reducing the current prices by 10%.</p>
<p>-Eraser price last year: $6.75 × 0.90 = $6.075-Notebook price last year: $11.00 × 0.90 = $9.90 -Bond paper price last year: $19.00 × 0.90 = $17.10</p>
<p>Step 2: Calculate the total cost using last year's prices.</p>
<p>-Total cost for erasers: 24 erasers × $6.075 = $145.80-Total cost for notebooks: 10 notebooks × $9.90 = $99.00-Total cost for bond paper: 1 ream × $17.10 = $17.10</p>
<p>Step 3: Sum up all the costs to get the total amount Liam should pay.$145.80 + $99.00 + $17.10 = $261.90</p>
<p>Answer:$261.90</p>
<p>Figure 13: Sample response from o1-preview on an example from GSM-NoOp: the model blindly applies the inflation rate, even though the inflation amount is irrelevant as the question clearly indicates the given prices are for "now" and not last year.</p>
<p>A.6 ABLATION: THE IMPACT OF ARITHMETIC ACCURACY An important question regarding the design of GSM-Symbolic is: when designing templates, "how should the numerical range of the variables be chosen?"and "how much of the performance drop can be attributed to arithmetic mistakes?". Figure 15: Statistics of the number of digits in the final answer for questions in GSM8K-test, GSM-Symbolic, GSM-P1, and GSM-P2.Compared to GSM8K-test, the GSM-Symbolic set shows a slight reduction in the number of questions with 1-digit or 2-digit final answers, and an increase in 3-digit answers.However, overall, the range of numbers does not increase significantly, with a significant majority of the final answers containing fewer than 5 digits.Phi-3-medium-128k-instruct Overall, when creating the symbolic templates, we have chosen numerical ranges close to their original values in the GSM8K test set.The rationale behind this choice is that the arithmetic ability of the models is much less important than their logical reasoning capabilities.However, we face a challenge: we need to generate many instances per question.Given that not all values in the range satisfy the conditions and hence won't be selected, we need to slightly increase the numeric range of numbers to ensure enough instances are generated from each template.</p>
<p>However, we show that this adjustment does not push the numerical values into a range where models have low arithmetic accuracy.As observed in Fig. 15:</p>
<p>• Compared to GSM8K, GSM-Symbolic has more 3-digit final answers and fewer 1-digit and 2-digit answers.However, as shown in Fig. 17, modern LLMs such as Gemma2-9B have no difficulty with up to 3-digit addition and multiplication.• Even though GSM-P1 and GSM-P2 have a very similar digit distribution to GSM-Symbolic and are mostly within the range that modern LLMs can perform accurate arithmetic (Fig. 17), the performance drop on these benchmarks is very significant (Fig. 17c).</p>
<p>Additionally, Fig. 16 provides statistics on the different operations involved in calculating intermediate answers across various datasets.Here, "intermediate" refers to the result of each intermediate operation throughout the solution process.For example, if a solution step involves 12 × 6 = 72, we categorize this as a two-digit multiplication.We observe that, overall, the distribution of the digit lengths in intermediate answers in Fig. 16, is similar to that in the final answers in Fig. 15.</p>
<p>Moreover, in Fig. 17, we demonstrate that modern Language Learning Models (LLMs) such as Gemma2-9B and Phi3-Medium are capable of nearly perfect addition and subtraction up to a length of 4, as well as achieving very high accuracy in multiplication and division.To this end, for each digit length of the answer, we ask the model in a zero-shot manner what the result of the operation would be.For example, for addition, we use the prompt "What is x plus y?" and for multiplication, we ask "What is x times y?", and so on.To reduce the computational burden, we do not check arithmetic accuracy on every possible combination of numbers.However, we ensure that for every digit length and every operation, we have at least 1,000 test cases.</p>
<p>Finally, in Table 2 we report the arithmetic accuracy results of Gemma2-9b-it on a more realistic setup where calculations are extracted from the generated responses.To this end, we extracted parts of the generated responses where the equal sign (=) was generated along with numerical values, and then evaluated whether the left-hand side of the equation had the same numerical value as the righthand side.As shown in the table, we observe very high arithmetic accuracy, similar to the previous results in this section.Overall, we believe it is unlikely that arithmetic difficulty can account for the significant performance drop on benchmarks such as GSM-P2, which has a very similar distribution to GSM-Symbolic and falls well within the range of fewer than 5 digits that models like Gemma2-9B can handle accurately.</p>
<p>B DISCUSSION ON STATISTICAL ANALYSIS</p>
<p>After the initial publication of our paper, we received various feedback regarding the statistical analysis of our results.Most notably, Ivanova et al. ( 2025) provide a detailed statistical analysis based on our results and argue that several of our observations are not statistically significant.First and foremost, we would like to thank the authors for taking the time to analyze our work.We firmly believe in the importance of the peer review process.</p>
<p>Figure 2
2
Figure 2: 8-shot CoT performance across 50 sets generated from GSM-Symbolic templates.All state-of-the-art models exhibit notable variance in accuracy.It is interesting that for the majority of the models, the performance on GSM8K (represented by dashed line) falls on the right side of the distribution, which statistically speaking, should have a very low likelihood.</p>
<p>Figure 4 :
4
Figure4: How sensitive are LLMs when we change only names, only proper numbers, or both names and numbers?Overall, models have noticeable performance variation even if we only change names, but even more when we change numbers or combine these changes.</p>
<p>Figure 5 :
5
Figure 5: Modifying the difficulty level of GSM-Symbolic by modifying the number of clauses.</p>
<p>Figure 6 :
6
Figure6: The impact of increasing the number of clauses on performance: As the difficulty increases from GSM-M1→ GSM-Symb→ GSM-P1→ GSM-P2, the distribution of performance shifts to the left (i.e., accuracy decreases), and the variance increases.</p>
<p>Figure 7 :
7
Figure 7: An example from the GSM-NoOp dataset:We add seemingly relevant statements to the questions that are, in fact, irrelevant to the reasoning and conclusion.However, the majority of models fail to ignore these statements and blindly convert them into operations, leading to mistakes.</p>
<p>Figure 8 :
8
Figure8: (a) The performance of models drops significantly on GSM-NoOp, with more recent models experiencing a greater decline than older ones.(b) As previously demonstrated, performance on GSM-Symbolic is very close to that on GSM8K.However, on GSM-NoOp, the significant drop in performance cannot be recovered, even when using the exact same question's variation as shots (NoOp-Symb) or when using different questions with different GSM-NoOpthat contain No-Op operations (NoOp-NoOp) as shots.(c) Notably, some models that perform significantly worse than those in (b) on GSM8K and GSM-Symbolic show much better performance on NoOp-Symb.</p>
<p>Figure 9 :
9
Figure 9: The prompt format used for evaluations.</p>
<p>Figure 10 :
10
Figure10: Additional results on performance variation on GSM-Symbolic.</p>
<p>Figure 11 :Figure 12 :
1112
Figure11: Using in-context shots or finetuning on GSM-P1 does not improve performance on GSM-P2: (a) Compared to the case where 8 shots come from GSM8K, when we include shots from GSM-P1the performance on GSM-P2 does not improve.(b) Finetuning on GSM-P1 can improve performance on GSM-P1 but not on GSM-P2.</p>
<p>Figure 16 :
16
Figure 16: Statistics on the Number of Digits in Intermediate answers: Compared to GSM8K, GSM-Symbolic and its variants involve operations that result in 3-digit numbers.However, as shown in Fig. 17, modern LLMs are well capable of performing arithmetic within this range.</p>
<p>Figure 17 :
17
Figure17: Arithmetic accuracy of Gemma2-9B and Phi3-Medium: (a) and (d) The addition accuracy of the models remains nearly perfect for calculations involving up to 4 digits.Note that, as shown in Fig.15, the majority of the final answers in GSM-Symbolic and its variants are below 5 digits.(b) and (e) Although GSM-P1 and GSM-P2 have a similar digit-length distribution to GSM-Symbolic, as illustrated in Fig.15, the performance drop is significant.</p>
<p>Table 1 :
1
Full 8-shot results of all models on GSM8Kand different variants of GSM-Symbolic.
ModelGSM8K (Full)GSM8K (100)Symbolic-M1SymbolicSymbolic-P1 Symbolic-P2 Symbolic-NoOpGemma2b12.111.024.5 (± 3.85)8.2 (± 2.21)3.6 (± 2.13)1.5 (± 1.63)4.7 (± 1.99)Gemma2b-it12.111.016.2 (± 3.28)8.2 (± 2.21)1.5 (± 1.49)1.5 (± 1.63)4.1 (± 2.48)Gemma-7b53.850.034.1 (± 4.41) 25.6 (± 3.25) 26.0 (± 5.30) 3.1 (± 1.92)8.7 (± 2.71)Gemma-7b-it29.333.034.1 (± 4.41) 25.6 (± 3.25) 6.0 (± 3.38)3.1 (± 1.92)8.7 (± 2.71)Gemma2-2b47.546.057.2 (± 3.40) 40.1 (± 3.04) 19.5 (± 3.89) 1.3 (± 1.37)8.8 (± 4.12)Gemma2-2b-it47.546.057.2 (± 3.40) 40.1 (± 3.04) 19.5 (± 3.89) 4.5 (± 1.94)15.7 (± 3.97)Gemma2-9b85.387.071.2 (± 2.81) 79.1 (± 2.99) 44.0 (± 5.69) 41.8 (± 6.00)22.3 (± 5.11)Gemma2-9b-it85.387.084.4 (± 2.36) 79.1 (± 2.99) 68.1 (± 4.77) 41.8 (± 6.00)22.3 (± 5.11)Gemma2-27b-it89.792.090.2 (± 1.86) 88.3 (± 2.56) 80.7 (± 4.07) 63.4 (± 4.14)30.0 (± 3.39)Phi-256.053.053.0 (± 3.10) 41.4 (± 3.56) 23.3 (± 4.07) 8.9 (± 3.33)11.2 (± 3.51)Phi-3-mini-128k-instruct83.785.085.9 (± 2.44) 80.7 (± 2.94) 63.4 (± 5.63) 37.5 (± 5.76)18.0 (± 3.83)Phi-3-small-128k-instruct88.589.086.4 (± 1.95) 83.7 (± 2.65) 72.0 (± 3.65) 50.7 (± 4.99)24.5 (± 4.81)Phi-3-medium-128k-instruct87.389.089.6 (± 1.65) 82.5 (± 2.86) 75.8 (± 3.89) 53.1 (± 4.80)29.4 (± 4.18)Phi-3.5-mini-instruct84.988.087.6 (± 1.98) 82.1 (± 3.38) 64.8 (± 5.43) 44.8 (± 6.32)22.4 (± 4.03)Mistral-7b-v0.144.548.055.4 (± 3.18) 41.1 (± 3.36) 17.4 (± 4.82) 5.5 (± 2.55)16.2 (± 4.43)Mistral-7b-instruct-v0.139.742.044.9 (± 4.29) 30.5 (± 3.47) 13.1 (± 3.51) 4.0 (± 2.24)10.1 (± 3.42)Mistral-7b-v0.340.644.054.0 (± 2.95) 40.0 (± 4.43) 15.6 (± 4.02) 3.9 (± 2.31)16.7 (± 4.26)Mistral-7b-instruct-v0.356.256.062.3 (± 2.68) 50.0 (± 3.49) 24.5 (± 4.34) 10.8 (± 3.60)15.9 (± 4.44)Mathstral-7b-v0.180.180.082.9 (± 2.87) 74.0 (± 3.49) 57.4 (± 5.20) 35.5 (± 5.07)20.4 (± 3.58)Llama3-8b55.861.079.5 (± 3.62) 74.6 (± 2.94) 53.8 (± 4.54) 12.3 (± 3.43)18.6 (± 3.86)Llama3-8b-instruct76.074.079.5 (± 3.62) 74.6 (± 2.94) 53.8 (± 4.54) 28.3 (± 4.37)18.6 (± 3.86)GPT-4o-mini94.295.092.5 (± 1.63) 91.7 (± 2.02) 81.1 (± 3.05) 72.4 (± 4.57)54.1 (± 3.85)GPT-4o95.295.094.4 (± 1.62) 94.9 (± 1.87) 93.9 (± 2.59) 88.0 (± 3.43)63.1 (± 4.53)o1-mini95.193.094.9 (± 1.49) 94.5 (± 1.58) 94.3 (± 2.57) 89.1 (± 3.56)66.0 (± 4.60)o1-preview94.996.093.6 (± 1.68) 92.7 (± 1.82) 95.4 (± 1.72) 94.0 (± 2.38)77.4 (± 3.84)35404550GSM Symbolic Accuracy (%) -(8s CoT)</p>
<p>Table 2 :
2
Ablation: Arithmetic accuracy of Gemma2-9b-it on different GSM benchmarks.
Benchmark1-digit 2-digits 3-digits 4-digits 5-digits AllGSM8K99.899.199.396.895.998.9GSM-Symbolic 99.799.297.697.299.198.3GSM-P197.598.698.796.299.597.4GSM-P296.298.697.596.499.297.1
Note that adding or removing a clause does not necessarily correspond to increasing or decreasing the number of required reasoning steps by exactly one. However, our main focus in this section is to understand the evolution of the performance distribution rather than the precise performance numbers.
This is a very complicated topic, as the notion of statistical significance depends on many assumptions about the properties of data. For instance, one could view the overall performance of models as a Bernoulli trial, with model accuracy representing the probability of success. However, this requires an i.i.d. assumption on the questions and the model accuracy for each question, which may not necessarily hold and needs further investigation.
In general, the t-test assumes normality, which may not necessarily hold for evaluation results. However, using a standard normality test based on the skewness and kurtosis of samples, we have verified that all distributions in Fig.10pass the normality test (p-value of the normality test = 0.1).
ACKNOWLEDGMENTSThe authors would like to thank Max Horton, Fartash Faghri, Moin Nabi, Devi Krishna, and Arsalan Farooq for the valuable feedback and support.Published as a conference paper at ICLR 2025GSM-NoOpA loaf of sourdough at the cafe costs $9.Muffins cost $3 each.If we purchase 10 loaves of sourdough and 10 muffins, how much more do the sourdough loaves cost compared to the muffins, if we plan to donate 3 loaves of sourdough and 2 muffins from this purchase?o1-preview: To solve this problem, we need to compare the total cost of the sourdough loaves and muffins after considering the donated items.Step 1: Calculate the number of items kept after donations.-Sourdough loaves kept: 10 loaves − 3 donated = 7 loaves -Muffins kept: 10 muffins − 2 donated = 8 muffinsStep 2: Calculate the total cost of the items kept.-Total cost of sourdough loaves: 7 loaves × $9 = $63 -Total cost of muffins: 8 muffins × $3 = $24Step 3: Find the difference in total cost.-Difference: $63 − $24 = $39 Answer:$39 o1-mini: Let's break down the costs step by step to determine how much more the sourdough loaves cost compared to the muffins after accounting for the donations.Figure14: Sample response from o1-preview and o1-mini on an example from GSM-NoOp: while the donation amount is irrelevant to the price difference, the models subtract the amount we donate.However, we note that we hold a fundamentally different philosophical view of reasoning thanIvanova et al. (2025).For instance, we do not believe the performance of an ideal reasoner-one that has truly understood a question-should follow a Bernoulli distribution.Rather, when focusing one single question, we argue that a degenerate distribution would be a more appropriate model.Although LLMs are probabilistic learners and do not follow such idealized behavior, this distinction is crucial to our analysis.To be more specific, let us denote P (o = true) as the probability of a model answering question o from the GSM8K dataset correctly, and P (x = true) as the probability of correctly answering question x from GSM-Symbolic-Names, presented in Sec.4.2, where the names have been changed but the variables remain the same-thus eliminating the influence of arithmetic mistakes.We argue that treating P (x = true) as a Bernoulli trial with a model-specific success probability p m is reasonable but not particularly useful for our purposes, as it ignores the model's response to the original question.Instead, we are interested in the conditional probability distributions P (x = true | o = true) and P (x = true | o = false), where the probability of answering x correctly is conditioned on whether the model correctly answers the original question o.Under this setup, our expectation is that for a "true reasoner," P (x = true | o = true) should follow a Bernoulli distribution with p = 1, and P (x = true | o = false) should follow a Bernoulli distribution with p = 0, since superficial changes (such as renaming variables) should not affect the outcome.Accordingly, the distribution of the total number of correct answers-conditioned on the model's responses to the original questions-would have an expected value of exactly 100 • p m and a standard deviation of 0. 5 . 6However, Figure18shows that LLMs such as gemma2-9b do not exhibit this behavior (other models follow a similar pattern).Overall, it is up to the reader to decide whether our stated assumptions are valid, or whether the statistical assumptions proposed by Ivanova et al. (2025) offer a better foundation for interpretation.Regardless, we acknowledge that we should have articulated our assumptions more clearly in the original version.5 One intuition behind our argument is the following: if a grade-school student can fully answer grade-school math questions but cannot answer high-school ones, then a test composed of 10 grade-school and 10 highschool questions would yield a performance of 50%.One could claim that the student's performance follows a Bernoulli trial with p = 0.5 (with expected score 10 and variance np(1 − p) = 5), but this interpretation is not meaningful.A more realistic assumption is that the student always answers grade-school questions correctly and always fails high-school ones.In both cases, we do not expect any variance 6 The arguments involving a degenerate distribution might seem extreme, as they assume the absence of non-reasoning factors (e.g., arithmetic mistakes, overlooking information during reasoning).In practice, this assumption does not always hold-humans also exhibit non-zero variance.However, we believe this variance is extremely small.It is worth noting that we are discussing simple grade-school questions involving basic arithmetic operations.For humans, especially when using a scratchpad, the likelihood of making mistakes on such calculations is very low.
. Sam Ade Marah I Abdin, Ammar Jacobs, Jyoti Ahmad Awan, Ahmed Aneja, Hany Awadallah, Nguyen Awadalla, Amit Bach, Arash Bahree, Harkirat S Bakhtiari, Alon Behl, Misha Benhaim, Johan Bilenko, Sébastien Bjorck, Martin Bubeck, Caio Cai, Teodoro César, Weizhu Mendes, Vishrav Chen, Parul Chaudhary, Allie Chopra, Gustavo Del Giorno, Matthew De Rosa, Ronen Dixon, Dan Eldan, Amit Iter, Abhishek Garg, Suriya Goswami, Emman Gunasekar, Junheng Haider, Russell J Hao, Jamie Hewett, Mojan Huynh, Xin Javaheripi, Piero Jin, Nikos Kauffmann, Dongwoo Karampatziakis, Mahoud Kim, Lev Khademi, James R Kurilenko, Yin Lee, Yuanzhi Tat Lee, Chen Li, Weishung Liang, Eric Liu, Zeqi Lin, Piyush Lin, Arindam Madan, Hardik Mitra, Anh Modi, Brandon Nguyen, Barun Norick, Daniel Patra, Thomas Perez-Becker, Reid Portet, Heyang Pryzant, Marko Qin, Corby Radmilac, Sambudha Rosset, Olatunji Roy, Olli Ruwase, Amin Saarikivi, Adil Saied, Michael Salim, Shital Santacroce, Ning Shah, Hiteshi Shang, Xia Sharma, Masahiro Song, Xin Tanaka, Rachel Wang, Guanhua Ward, Philipp Wang, Michael Witte, Can Wyatt, Jiahang Xu, Sonali Xu, Fan Yadav, Ziyi Yang, Donghan Yang, Chengruidong Yu, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhang, Zhou, 10.48550/arXiv.2404.14219Phi-3 technical report: A highly capable language model locally on your phone. CoRR, abs/2404.14219, 2024</p>
<p>Gemini: A family of highly capable multimodal models. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Lakshman Tamara Von Glehn, Mehran Yagati, Lucas Kazemi, Misha Gonzalez, Jakub Khalman, Sygnowski, 10.48550/arXiv.2312.118052023</p>
<p>When can transformers reason with abstract symbols?. Enric Boix-Adserà, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, Joshua M Susskind, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaMay 7-11, 2024. 2024OpenReview.net</p>
<p>Knowledge representation and reasoning. Ronald Brachman, Hector Levesque, arXiv:1911.01547François Chollet. On the measure of intelligence. Elsevier2004. 2019arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168The Stanford Encyclopedia of Philosophy. Edward N Zalta, Uri Nodelman, 2021. 2024Metaphysics Research Lab, Stanford UniversityarXiv preprintDavid Cole. The Chinese Room Argument. Winter 2024 edition</p>
<p>Neural networks and the chomsky hierarchy. Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Kevin Li, Elliot Wenliang, Chris Catt, Marcus Cundy, Shane Hutter, Joel Legg, Pedro A Veness, Ortega, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023OpenReview.net</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Bethany Baptiste Rozière, Binh Biron, Bobbie Tang, Charlotte Chern, Chaya Caucheteux, Chloe Nayak, Chris Bi, Chris Marra, Christian Mcconnell, Christophe Keller, Chunyang Touret, Corinne Wu, Cristian Canton Wong, Cyrus Ferrer, Damien Nikolaidis, Daniel Allonsius, Danielle Song, Danny Pintz, David Livshits, Dhruv Esiobu, Dhruv Choudhary, Diego Mahajan, Diego Garcia-Olano, Hailey Perino, Hannah Nguyen, Hu Korevaar, Hugo Xu, Touvron, 10.48550/arXiv.2407.21783Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy. Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, 2024</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jiang, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Sean Sanyal, Xiang Welleck, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, 2023</p>
<p>Defining intelligence: Bridging the gap between human and artificial perspectives. Gilles E Gignac, Eva T Szodorai, Intelligence. 2024</p>
<p>Apple intelligence foundation language models. Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey P Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, 10.48550/arXiv.2407.21075Xin Wang, Xin Zheng, and Walker Cheng2024</p>
<p>Pengfei Hong, Navonil Majumder, Deepanway Ghosal, arXiv:2401.09395Somak Aditya, Rada Mihalcea, and Soujanya Poria. Evaluating llms' mathematical and coding competency through ontology-guided interventions. 2024arXiv preprint</p>
<p>Towards more rigorous evaluations of language models. Ilija Desi R Ivanova, Momchil Ilievski, Konstantinov, ICLR Blogposts 2025. 2025</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Florian Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 10.48550/arXiv.2310.06825Mistral 7b. 2023</p>
<p>A peek into token bias: Large language models are not yet genuine reasoners. Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J Su, Camillo J Taylor, Dan Roth, 10.48550/arXiv.2406.110502024</p>
<p>Can large language models reason and plan?. Subbarao Kambhampati, Annals of the New York Academy of Sciences. 15342682499612024</p>
<p>Universal intelligence: A definition of machine intelligence. Minds and machines. Shane Legg, Marcus Hutter, 200717</p>
<p>Gsm-plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 11-16, 2024. 2024a1ACL 2024</p>
<p>One-layer transformer provably learns one-nearest neighbor in context. Zihao Li, Yuan Cao, Cheng Gao, Yihan He, Han Liu, Jason M Klusowski, Jianqing Fan, Mengdi Wang, 2024b</p>
<p>Chain of thought empowers transformers to solve inherently serial problems. Zhiyuan Liu, Hong Liu, Denny Zhou, Tengyu Ma, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaMay 7-11, 2024. 2024OpenReview.net</p>
<p>Embers of autoregression: Understanding large language models through the problem they are trained to solve. ; R Thomas Gary F Marcus, Shunyu Mccoy, Dan Yao, Matthew Friedman, Thomas L Hardy, Griffiths, 2003. 2023MIT pressThe algebraic mind: Integrating connectionism and cognitive science</p>
<p>Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Cristian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, 10.48550/arXiv.2403.08295Open models based on gemini research and technology. 2024</p>
<p>The logic theory machine-a complex information processing system. Allen Newell, Herbert Simon, IRE Transactions on information theory. 231956</p>
<p>Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models. Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, Jenia Jitsev, arXiv:2406.020612024arXiv preprint</p>
<p>10.48550/arXiv.2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Learning to reason with large language models. Openai, 2024</p>
<p>Probabilistic reasoning in intelligent systems: networks of plausible inference. Judea Pearl, 2014Elsevier</p>
<p>On limitations of the transformer architecture. Binghui Peng, Srini Narayanan, Christos H Papadimitriou, 10.48550/arXiv.2402.081642024</p>
<p>Yasaman Razeghi, Adam Roberts, Colin Raffel, Ariel Herbert, - Voss, arXiv:2202.08904Impact of pretraining term frequencies on few-shot reasoning. 2022arXiv preprint</p>
<p>. Morgane Rivière, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, A Christopher, Danila Choquette-Choo, David Sinopalnikov, Dimple Weinberger, Dominika Vijaykumar, Dustin Rogozinska, Elisa Herbison, Emma Bandy, Eric Wang, Erica Noland, Evan Moreira, Evgenii Senter, Francesco Eltyshev, Visin, Gary Gabriel Rasskin, Glenn Wei, Gus Cameron, Hadi Martins, Hanna Hashemi, Harleen Klimczak-Plucinska, Harsh Batra, Ivan Dhand, Jacinda Nardini, Jack Mein, James Zhou, Jeff Svensson, Jetha Stanway, Jin Peng Chan, Joana Zhou, Carrasqueira, 10.48550/arXiv.2408.00118Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjösund, Lauren Usui, Laurent SifreLena Heuermann, Leticia Lagoand Lilly Mc-Nealus. Gemma 2: Improving open language models at a practical size. CoRR, abs/2408.00118, 2024</p>
<p>Are emergent abilities of large language models a mirage?. Rylan Schaeffer, Brando Miranda, Sanmi Koyejo, 2023</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, Denny Zhou, International Conference on Machine Learning, ICML 2023. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, Honolulu, Hawaii, USAPMLRJuly 2023. 2023202of Proceedings of Machine Learning Research</p>
<p>Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. Saurabh Srivastava, M B Annarose, P Anto, Ajay Menon, Adwaith Sukumar, T Samod, Alan Philipose, Stevin Prince, Sooraj Thomas, 2024</p>
<p>A causal framework to quantify the robustness of mathematical reasoning with language models. Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schoelkopf, Mrinmaya Sachan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Large language models still can't plan (A benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo Hernandez, Sarath Sreedharan, Subbarao Kambhampati, 10.48550/arXiv.2206.104982022</p>
<p>Llms still can't plan; can lrms? a preliminary evaluation of openai's o1 on planbench. Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati, 2024272770270</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, arXiv:2201.119032022arXiv preprint</p>
<p>Thinking like transformers. Gail Weiss, Yoav Goldberg, Eran Yahav, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila, Tong Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139of Proceedings of Machine Learning Research</p>
<p>Physics of language models: Part 2.1, grade-school math and the hidden reasoning process. Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan Allen-Zhu, arXiv:2407.203112024arXiv preprint</p>
<p>On the paradox of learning to reason from data. Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den Broeck, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23. Edith Elkind, the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-2382023Main Track</p>
<p>A careful examination of large language model performance on grade school arithmetic. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue, 10.48550/arXiv.2405.003322024</p>
<p>What algorithms can transformers learn? A study in length generalization. Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M Susskind, Samy Bengio, Preetum Nakkiran, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaMay 7-11, 2024. 2024OpenReview.net</p>
<p>Limits of deep learning: Sequence modeling through the lens of complexity theory. Nikola Zubic, Federico Soldà, Aurelio Sulser, Davide Scaramuzza, The Thirteenth International Conference on Learning Representations. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
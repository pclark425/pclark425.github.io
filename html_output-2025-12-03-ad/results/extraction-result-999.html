<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-999 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-999</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-999</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-6bf04318d6e57463f7823b9770c5a6c19a7b47e9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6bf04318d6e57463f7823b9770c5a6c19a7b47e9" target="_blank">Fishr: Invariant Gradient Variances for Out-of-distribution Generalization</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces a new regularization - named Fishr - that enforces domain invariance in the space of the gradients of the loss: specifically, the domain-level variances of gradients are matched across training domains.</p>
                <p><strong>Paper Abstract:</strong> Learning robust models that generalize well under changes in the data distribution is critical for real-world applications. To this end, there has been a growing surge of interest to learn simultaneously from multiple training domains - while enforcing different types of invariance across those domains. Yet, all existing approaches fail to show systematic benefits under controlled evaluation protocols. In this paper, we introduce a new regularization - named Fishr - that enforces domain invariance in the space of the gradients of the loss: specifically, the domain-level variances of gradients are matched across training domains. Our approach is based on the close relations between the gradient covariance, the Fisher Information and the Hessian of the loss: in particular, we show that Fishr eventually aligns the domain-level loss landscapes locally around the final weights. Extensive experiments demonstrate the effectiveness of Fishr for out-of-distribution generalization. Notably, Fishr improves the state of the art on the DomainBed benchmark and performs consistently better than Empirical Risk Minimization. Our code is available at https://github.com/alexrame/fishr.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e999.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e999.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fishr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A regularization method that enforces invariance across training domains by matching domain-level gradient variances (per-parameter) to align local loss landscapes (Hessians and risks) and reduce spurious, domain-specific solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fishr (gradient-variance matching)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute per-sample gradients g_e^i, estimate domain-level gradient variance vectors v_e = Var(G_e) (often restricted to classifier weights), maintain an EMA of v_e for stability, and add a penalty L_Fishr = (1/|E|) sum_e ||v_e - v_mean||^2 to the ERM loss; implemented efficiently with BackPACK for per-sample gradients and applied typically on the final linear classifier to save memory. The regularizer encourages similar gradient statistics across domains, which (via relations between gradient covariance, empirical Fisher, and Hessian diagonal) aligns domain-level Hessians and risks, reducing an inconsistency score and improving OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Colored MNIST (synthetic) and DomainBed benchmark (multi-domain image datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Colored MNIST is a controlled synthetic dataset with a single spurious color-label correlation (non-interactive, single-step synthetic challenge). DomainBed is a benchmark suite of multiple multi-domain image datasets (VLCS, PACS, OfficeHome, TerraIncognita, DomainNet, etc.) used for evaluation; neither is an open-ended interactive virtual lab—both are static datasets used to evaluate domain shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Regularization via matching domain-level per-parameter gradient variances (diagonal of gradient covariance) across domains; uses EMA for stable estimates and applies penalty to downweight domain-specific gradient variance components.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations between covariates and labels (correlation shifts), domain-specific shortcuts/irrelevant variables, diversity shifts across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects spurious domain-specific signals by measuring discrepancies in domain-level gradient variances, differences in empirical risks across domains, and misalignment of Hessian diagonals (Frobenius norms / cosine similarity measures reported).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Penalizes mismatch in gradient-variance components across domains (L2 penalty with coefficient λ), which effectively reduces the influence of directions with domain-dependent gradient variance; EMA (γ) smooths estimates before penalization.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutes spurious explanations by showing that minimizing the Fishr penalty reduces domain inconsistency score: domain risks and Hessian diagonals become aligned and test accuracy on shifted domains increases (empirical refutation via synthetic reversal experiments and benchmark averages).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Colored MNIST (proof of concept): Fishr_θ test accuracy 71.2% (Fishr_ω 69.5%, Fishr_φ 73.8% in some settings); DomainBed (Test-domain model selection): average accuracy 70.8% across benchmarks (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline ERM: DomainBed average 68.7% (Test-domain); on Colored MNIST ERM performs poorly on correlation shift (57.8% average in DomainBed setup or 14.0% under some IRM-specific hyperparameter setups). Fishr improves over ERM in these reported averages.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Colored MNIST: 1 (color); linear toy: 3 spurious features (experiments reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Matching per-sample gradient variances across domains aligns domain-level Hessian diagonals and empirical risks, reduces a domain inconsistency score, and yields systematic improvement over ERM across multiple domain-shift benchmarks; the method is scalable (applied on classifier weights), uses EMA for stable variance estimates, and empirically outperforms gradient-mean matching and several prior baselines on both synthetic and real multi-domain datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fishr: Invariant Gradient Variances for Out-of-distribution Generalization', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e999.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e999.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fish</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fish: Gradient Matching for Domain Generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gradient-based domain generalization baseline that matches domain-level gradient means (expected gradients) to bias optimization toward shared minima across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gradient matching for domain generalization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fish (gradient-mean matching)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Align domain-level expected gradients g_e = E_{(x,y)~D_e} ∇_θ ℓ(f_θ(x),y); concretely increases the dot product between domain gradient means or minimizes ||g_A - g_B||^2 to encourage shared descent directions across domains. Implementations use meta-learning-style optimization or regularization over gradient means.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Colored MNIST (synthetic), DomainBed benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static synthetic (Colored MNIST) and static multi-domain image benchmarks (DomainBed); not interactive. Used as a baseline to evaluate gradient-mean approaches versus variance-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Matches domain-level gradient means to favor minima shared by domains, thereby discouraging domain-specific solutions driven by spurious features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations and domain-specific shortcuts that manifest as differing expected gradients across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit detection via differences in domain-level mean gradients (g_A vs g_B); no explicit per-sample disagreement diagnostics retained.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Encourages updates in directions common across domains (via dot-product maximization or mean-gradient alignment), which downweights domain-specific mean gradient directions.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>No explicit refutation procedure; success measured empirically by improved test accuracy on shifted domains or by convergence to shared minima.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>DomainBed average reported 69.1% (Table 4) when used as a baseline in the paper; on the linear toy it achieves high test accuracy (93%) when it finds invariant solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>ERM baseline in DomainBed average 68.7%; Fish (mean-based) slightly improves over ERM in some benchmarks but underperforms Fishr in general.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>In linear toy example: 3 spurious features; in Colored MNIST: 1 spurious color feature.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gradient-mean alignment helps find shared minima and can eliminate some spurious solutions in controlled linear settings, but it discards per-sample variance/disagreement information and is less effective than variance-matching (Fishr) on realistic domain-shift benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fishr: Invariant Gradient Variances for Out-of-distribution Generalization', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e999.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e999.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AND-mask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AND-mask: Learning Explanations that are Hard to Vary</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gradient-masking approach that updates parameters only on components where domain gradients agree in sign, aiming to find explanations that are consistent across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning explanations that are hard to vary</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AND-mask (gradient masking by sign agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>At each update, compute per-domain gradients and construct a mask that keeps parameter components where gradients from all domains point in the same direction (sign agreement); only update those components, preventing updates in directions with conflicting domain gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>DomainBed benchmark (evaluated on multiple static multi-domain datasets) and synthetic examples in original work</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static multi-domain classification benchmarks used to evaluate invariance-finding methods; not an interactive environment.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Hard masking: zero-out parameter updates for gradient components that disagree across domains, effectively preventing learning of domain-specific (spurious) directions.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Domain-specific shortcuts and spurious correlations that produce conflicting gradient directions across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects spurious/conflicting signals by sign-disagreement of per-domain gradient components.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Zeroes (completely downweights) inconsistent gradient components via a binary mask.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By construction, prevents model from moving along inconsistent directions; empirical refutation is via downstream performance on shifted domains.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>In the paper's DomainBed comparison, AND-mask average accuracy was 67.5% (Table 4), below Fishr and near or below ERM on multiple datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared to ERM (68.7% average in Table 4), AND-mask did not consistently improve and performs worse on some real datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AND-mask can align Hessians by preventing conflicting updates, but its hard-masking introduces 'dead zones' where parameters cannot recover (ignores gradient magnitudes), which empirically leads to poorer performance on complex real-world benchmarks compared to softer variance-based regularization (Fishr).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fishr: Invariant Gradient Variances for Out-of-distribution Generalization', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e999.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e999.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAND-mask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAND-mask: An Enhanced Gradient Masking Strategy for the Discovery of Invariances in Domain Generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An improved gradient-masking variant that addresses some limitations of AND-mask to discover invariances, e.g., by smoothing or enhancing mask decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sand-mask: An enhanced gradient masking strategy for the discovery of invariances in domain generalization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SAND-mask (enhanced gradient masking)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A variant of gradient-masking that refines the binary masking strategy to mitigate practical issues (e.g., dead zones), typically via softened masks or aggregation heuristics to allow more stable optimization while still suppressing inconsistent gradient components across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>DomainBed benchmark (multi-domain image datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static benchmark datasets; not interactive or experimental-design environments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Gradient masking with enhancements over AND-mask to reduce practical failure modes (details per original SAND-mask paper).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Domain-specific spurious correlations producing inconsistent gradient components.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Sign-disagreement and improved heuristics derived from per-domain gradient comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Softened or enhanced masking to reduce influence of inconsistent gradient components without fully zeroing them.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Empirical assessment via improved invariant performance relative to AND-mask and ERM in some settings (paper reports comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Reported in the paper's DomainBed table as average 67.2% (Table 4), similar to AND-mask and generally below Fishr.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared to ERM (68.7%), SAND-mask did not consistently outperform ERM across the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gradient masking strategies can be improved but still suffer from limitations (e.g., optimization issues, sensitivity to hyperparameters); in this paper SAND-mask did not match the robustness of Fishr on the evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fishr: Invariant Gradient Variances for Out-of-distribution Generalization', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e999.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e999.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>V-REx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>V-REx: Variance Risk Extrapolation (Risk variance penalization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A penalty-based method that encourages similar empirical risks across training domains by penalizing variance of per-domain risks, motivated to promote invariance and OOD robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Out-of-distribution generalization via risk extrapolation (rex)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>V-REx (risk-variance penalty)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute per-domain empirical risks R_e(θ) and add a regularization term that penalizes the variance (or squared differences) across these risks (e.g., penalize |R_A - R_B|^2 for two domains or Var_e[R_e] more generally), encouraging models whose risks are similar on all training environments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Colored MNIST (synthetic) and DomainBed benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static synthetic and real multi-domain image benchmarks; used to evaluate risk-based invariance penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Downweights domain-specific explanations by penalizing differences in empirical risks across domains, encouraging reliance on features that yield similar performance in all domains.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Correlation shifts and domain-dependent spurious correlations that change domain risks if relied upon.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detection is implicit: differences in per-domain empirical risks indicate domain-specific solutions; the variance penalty uses these signals.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Penalty (λ) on risk-variance reduces emphasis on domain-exploitative features by increasing loss when per-domain risks diverge.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation occurs when the model equalizes risks across domains, empirically tested by comparing per-domain risks and downstream OOD performance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>In the paper's DomainBed results V-REx average accuracy was 68.2% (Table 4); on Colored MNIST in the IRM setup V-REx achieved 67.2% test accuracy (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>ERM baseline in DomainBed average 68.7% (in some benchmarks V-REx slightly worse or similar), showing mixed results depending on model selection protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Matching or penalizing risk variance is effective at enforcing label-aware invariance (unlike purely feature-based methods) and can reduce reliance on spurious correlations, but in this paper Fishr (feature-adaptive variance matching via gradients) is argued to be a more fine-grained variant that also aligns Hessians.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fishr: Invariant Gradient Variances for Out-of-distribution Generalization', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e999.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e999.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Risk Minimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal-inspired approach that seeks predictors that are simultaneously optimal (invariant) across multiple environments, with the goal of learning causal (stable) mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant risk minimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>IRM (Invariant predictor constraint)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Formulate invariance as an optimization constraint: find a representation Φ and classifier w such that w ∘ Φ is simultaneously optimal for all environments (often enforced via a penalty that encourages ∇_{w|w=1.0} R_e(w ∘ Φ) = 0). This aims to find predictors whose optimality does not depend on environment-specific correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Colored MNIST (synthetic) and DomainBed benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static synthetic and real multi-domain datasets used to evaluate invariant-objective methods; not interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Constrained/penalized optimization to enforce predictor invariance across domains, thereby discouraging reliance on domain-specific (spurious) features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations and domain-specific shortcuts that cause predictors to differ across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detection by checking whether a single predictor achieves optimality across environments (if not, the predictor is considered non-invariant).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Enforces invariance which implicitly downweights domain-specific components by constraining classifier optimality across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Attempts to eliminate spurious predictors by discarding solutions that are not simultaneously optimal across domains; empirical refutation is via improved OOD performance if invariance is achieved.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>In the DomainBed comparisons IRM average accuracy reported 66.9% (Table 4) and it performed better than ERM on some synthetic setups but showed limitations (sensitivity to hyperparameters; known pitfalls in non-linear settings). On Colored MNIST IRM (in the IRM experimental setup) achieves 65.6% test acc (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>ERM can outperform IRM in some settings (e.g., non-noisy fully predictive invariant features), reflecting IRM's practical limitations as discussed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IRM provides a causal objective to avoid spurious domain-specific predictors, but it has known practical pitfalls (sensitivity to non-linearity, hyperparameters, and model selection) and frequently fails to outperform ERM under restricted, fair model-selection regimes; Fishr provides an alternative gradient-based invariant criterion that empirically outperforms IRM on the DomainBed benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fishr: Invariant Gradient Variances for Out-of-distribution Generalization', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e999.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e999.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep CORAL: Correlation Alignment for Deep Domain Adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A feature-based domain adaptation method that aligns second-order statistics (feature covariances) across domains to learn domain-invariant features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep coral: Correlation alignment for deep domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CORAL (feature covariance alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute feature covariance matrices Cov(Z_e) per domain and add a penalty ||Cov(Z_A) - Cov(Z_B)||_F^2 (or general multi-domain variant) to align feature second-order statistics across domains, encouraging domain-invariant embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>DomainBed benchmark and other domain-adaptation/domain-generalization datasets (static)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static datasets (real images) used to evaluate feature-alignment methods; not interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Feature-covariance alignment (matching feature second-order statistics across domains), which addresses distributional feature shifts but is label-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Diversity shifts and some covariate distribution shifts; does not explicitly address label-dependent spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>No explicit detection of spurious label-based signals; relies on mismatched feature covariances to identify domain shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Align features so domain-specific feature statistics are reduced; does not directly downweight label-dependent spurious features.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Not designed to refute label-dependent spurious correlations; empirical failure on Colored MNIST demonstrates this limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>CORAL average accuracy 69.2% in DomainBed Table 4; performs well on several diversity-shift datasets but fails to address correlation shifts (e.g., Colored MNIST where it cannot detect label-dependent shortcuts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared to ERM (68.7% avg), CORAL is slightly better on average in DomainBed but not robust to label/correlation shifts (fails on Colored MNIST).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Feature-covariance matching helps for some diversity/covariate shifts but is label-agnostic and can fail catastrophically on correlation shifts where spurious features correlate with labels; gradient-based, label-aware methods (V-REx, Fishr) are preferable for label-dependent spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fishr: Invariant Gradient Variances for Out-of-distribution Generalization', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Invariant risk minimization <em>(Rating: 2)</em></li>
                <li>Learning explanations that are hard to vary <em>(Rating: 2)</em></li>
                <li>Gradient matching for domain generalization <em>(Rating: 2)</em></li>
                <li>Out-of-distribution generalization via risk extrapolation (rex) <em>(Rating: 2)</em></li>
                <li>Deep coral: Correlation alignment for deep domain adaptation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-999",
    "paper_id": "paper-6bf04318d6e57463f7823b9770c5a6c19a7b47e9",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "Fishr",
            "name_full": "Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization",
            "brief_description": "A regularization method that enforces invariance across training domains by matching domain-level gradient variances (per-parameter) to align local loss landscapes (Hessians and risks) and reduce spurious, domain-specific solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Fishr (gradient-variance matching)",
            "method_description": "Compute per-sample gradients g_e^i, estimate domain-level gradient variance vectors v_e = Var(G_e) (often restricted to classifier weights), maintain an EMA of v_e for stability, and add a penalty L_Fishr = (1/|E|) sum_e ||v_e - v_mean||^2 to the ERM loss; implemented efficiently with BackPACK for per-sample gradients and applied typically on the final linear classifier to save memory. The regularizer encourages similar gradient statistics across domains, which (via relations between gradient covariance, empirical Fisher, and Hessian diagonal) aligns domain-level Hessians and risks, reducing an inconsistency score and improving OOD generalization.",
            "environment_name": "Colored MNIST (synthetic) and DomainBed benchmark (multi-domain image datasets)",
            "environment_description": "Colored MNIST is a controlled synthetic dataset with a single spurious color-label correlation (non-interactive, single-step synthetic challenge). DomainBed is a benchmark suite of multiple multi-domain image datasets (VLCS, PACS, OfficeHome, TerraIncognita, DomainNet, etc.) used for evaluation; neither is an open-ended interactive virtual lab—both are static datasets used to evaluate domain shifts.",
            "handles_distractors": true,
            "distractor_handling_technique": "Regularization via matching domain-level per-parameter gradient variances (diagonal of gradient covariance) across domains; uses EMA for stable estimates and applies penalty to downweight domain-specific gradient variance components.",
            "spurious_signal_types": "Spurious correlations between covariates and labels (correlation shifts), domain-specific shortcuts/irrelevant variables, diversity shifts across domains.",
            "detection_method": "Detects spurious domain-specific signals by measuring discrepancies in domain-level gradient variances, differences in empirical risks across domains, and misalignment of Hessian diagonals (Frobenius norms / cosine similarity measures reported).",
            "downweighting_method": "Penalizes mismatch in gradient-variance components across domains (L2 penalty with coefficient λ), which effectively reduces the influence of directions with domain-dependent gradient variance; EMA (γ) smooths estimates before penalization.",
            "refutation_method": "Refutes spurious explanations by showing that minimizing the Fishr penalty reduces domain inconsistency score: domain risks and Hessian diagonals become aligned and test accuracy on shifted domains increases (empirical refutation via synthetic reversal experiments and benchmark averages).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Colored MNIST (proof of concept): Fishr_θ test accuracy 71.2% (Fishr_ω 69.5%, Fishr_φ 73.8% in some settings); DomainBed (Test-domain model selection): average accuracy 70.8% across benchmarks (Table 4).",
            "performance_without_robustness": "Baseline ERM: DomainBed average 68.7% (Test-domain); on Colored MNIST ERM performs poorly on correlation shift (57.8% average in DomainBed setup or 14.0% under some IRM-specific hyperparameter setups). Fishr improves over ERM in these reported averages.",
            "has_ablation_study": true,
            "number_of_distractors": "Colored MNIST: 1 (color); linear toy: 3 spurious features (experiments reported).",
            "key_findings": "Matching per-sample gradient variances across domains aligns domain-level Hessian diagonals and empirical risks, reduces a domain inconsistency score, and yields systematic improvement over ERM across multiple domain-shift benchmarks; the method is scalable (applied on classifier weights), uses EMA for stable variance estimates, and empirically outperforms gradient-mean matching and several prior baselines on both synthetic and real multi-domain datasets.",
            "uuid": "e999.0",
            "source_info": {
                "paper_title": "Fishr: Invariant Gradient Variances for Out-of-distribution Generalization",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Fish",
            "name_full": "Fish: Gradient Matching for Domain Generalization",
            "brief_description": "A gradient-based domain generalization baseline that matches domain-level gradient means (expected gradients) to bias optimization toward shared minima across domains.",
            "citation_title": "Gradient matching for domain generalization",
            "mention_or_use": "use",
            "method_name": "Fish (gradient-mean matching)",
            "method_description": "Align domain-level expected gradients g_e = E_{(x,y)~D_e} ∇_θ ℓ(f_θ(x),y); concretely increases the dot product between domain gradient means or minimizes ||g_A - g_B||^2 to encourage shared descent directions across domains. Implementations use meta-learning-style optimization or regularization over gradient means.",
            "environment_name": "Colored MNIST (synthetic), DomainBed benchmark",
            "environment_description": "Static synthetic (Colored MNIST) and static multi-domain image benchmarks (DomainBed); not interactive. Used as a baseline to evaluate gradient-mean approaches versus variance-based approaches.",
            "handles_distractors": true,
            "distractor_handling_technique": "Matches domain-level gradient means to favor minima shared by domains, thereby discouraging domain-specific solutions driven by spurious features.",
            "spurious_signal_types": "Spurious correlations and domain-specific shortcuts that manifest as differing expected gradients across domains.",
            "detection_method": "Implicit detection via differences in domain-level mean gradients (g_A vs g_B); no explicit per-sample disagreement diagnostics retained.",
            "downweighting_method": "Encourages updates in directions common across domains (via dot-product maximization or mean-gradient alignment), which downweights domain-specific mean gradient directions.",
            "refutation_method": "No explicit refutation procedure; success measured empirically by improved test accuracy on shifted domains or by convergence to shared minima.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "DomainBed average reported 69.1% (Table 4) when used as a baseline in the paper; on the linear toy it achieves high test accuracy (93%) when it finds invariant solutions.",
            "performance_without_robustness": "ERM baseline in DomainBed average 68.7%; Fish (mean-based) slightly improves over ERM in some benchmarks but underperforms Fishr in general.",
            "has_ablation_study": false,
            "number_of_distractors": "In linear toy example: 3 spurious features; in Colored MNIST: 1 spurious color feature.",
            "key_findings": "Gradient-mean alignment helps find shared minima and can eliminate some spurious solutions in controlled linear settings, but it discards per-sample variance/disagreement information and is less effective than variance-matching (Fishr) on realistic domain-shift benchmarks.",
            "uuid": "e999.1",
            "source_info": {
                "paper_title": "Fishr: Invariant Gradient Variances for Out-of-distribution Generalization",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "AND-mask",
            "name_full": "AND-mask: Learning Explanations that are Hard to Vary",
            "brief_description": "A gradient-masking approach that updates parameters only on components where domain gradients agree in sign, aiming to find explanations that are consistent across domains.",
            "citation_title": "Learning explanations that are hard to vary",
            "mention_or_use": "use",
            "method_name": "AND-mask (gradient masking by sign agreement)",
            "method_description": "At each update, compute per-domain gradients and construct a mask that keeps parameter components where gradients from all domains point in the same direction (sign agreement); only update those components, preventing updates in directions with conflicting domain gradients.",
            "environment_name": "DomainBed benchmark (evaluated on multiple static multi-domain datasets) and synthetic examples in original work",
            "environment_description": "Static multi-domain classification benchmarks used to evaluate invariance-finding methods; not an interactive environment.",
            "handles_distractors": true,
            "distractor_handling_technique": "Hard masking: zero-out parameter updates for gradient components that disagree across domains, effectively preventing learning of domain-specific (spurious) directions.",
            "spurious_signal_types": "Domain-specific shortcuts and spurious correlations that produce conflicting gradient directions across domains.",
            "detection_method": "Detects spurious/conflicting signals by sign-disagreement of per-domain gradient components.",
            "downweighting_method": "Zeroes (completely downweights) inconsistent gradient components via a binary mask.",
            "refutation_method": "By construction, prevents model from moving along inconsistent directions; empirical refutation is via downstream performance on shifted domains.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "In the paper's DomainBed comparison, AND-mask average accuracy was 67.5% (Table 4), below Fishr and near or below ERM on multiple datasets.",
            "performance_without_robustness": "Compared to ERM (68.7% average in Table 4), AND-mask did not consistently improve and performs worse on some real datasets.",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "AND-mask can align Hessians by preventing conflicting updates, but its hard-masking introduces 'dead zones' where parameters cannot recover (ignores gradient magnitudes), which empirically leads to poorer performance on complex real-world benchmarks compared to softer variance-based regularization (Fishr).",
            "uuid": "e999.2",
            "source_info": {
                "paper_title": "Fishr: Invariant Gradient Variances for Out-of-distribution Generalization",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "SAND-mask",
            "name_full": "SAND-mask: An Enhanced Gradient Masking Strategy for the Discovery of Invariances in Domain Generalization",
            "brief_description": "An improved gradient-masking variant that addresses some limitations of AND-mask to discover invariances, e.g., by smoothing or enhancing mask decisions.",
            "citation_title": "Sand-mask: An enhanced gradient masking strategy for the discovery of invariances in domain generalization",
            "mention_or_use": "use",
            "method_name": "SAND-mask (enhanced gradient masking)",
            "method_description": "A variant of gradient-masking that refines the binary masking strategy to mitigate practical issues (e.g., dead zones), typically via softened masks or aggregation heuristics to allow more stable optimization while still suppressing inconsistent gradient components across domains.",
            "environment_name": "DomainBed benchmark (multi-domain image datasets)",
            "environment_description": "Static benchmark datasets; not interactive or experimental-design environments.",
            "handles_distractors": true,
            "distractor_handling_technique": "Gradient masking with enhancements over AND-mask to reduce practical failure modes (details per original SAND-mask paper).",
            "spurious_signal_types": "Domain-specific spurious correlations producing inconsistent gradient components.",
            "detection_method": "Sign-disagreement and improved heuristics derived from per-domain gradient comparisons.",
            "downweighting_method": "Softened or enhanced masking to reduce influence of inconsistent gradient components without fully zeroing them.",
            "refutation_method": "Empirical assessment via improved invariant performance relative to AND-mask and ERM in some settings (paper reports comparisons).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Reported in the paper's DomainBed table as average 67.2% (Table 4), similar to AND-mask and generally below Fishr.",
            "performance_without_robustness": "Compared to ERM (68.7%), SAND-mask did not consistently outperform ERM across the benchmark.",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Gradient masking strategies can be improved but still suffer from limitations (e.g., optimization issues, sensitivity to hyperparameters); in this paper SAND-mask did not match the robustness of Fishr on the evaluated benchmarks.",
            "uuid": "e999.3",
            "source_info": {
                "paper_title": "Fishr: Invariant Gradient Variances for Out-of-distribution Generalization",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "V-REx",
            "name_full": "V-REx: Variance Risk Extrapolation (Risk variance penalization)",
            "brief_description": "A penalty-based method that encourages similar empirical risks across training domains by penalizing variance of per-domain risks, motivated to promote invariance and OOD robustness.",
            "citation_title": "Out-of-distribution generalization via risk extrapolation (rex)",
            "mention_or_use": "use",
            "method_name": "V-REx (risk-variance penalty)",
            "method_description": "Compute per-domain empirical risks R_e(θ) and add a regularization term that penalizes the variance (or squared differences) across these risks (e.g., penalize |R_A - R_B|^2 for two domains or Var_e[R_e] more generally), encouraging models whose risks are similar on all training environments.",
            "environment_name": "Colored MNIST (synthetic) and DomainBed benchmark",
            "environment_description": "Static synthetic and real multi-domain image benchmarks; used to evaluate risk-based invariance penalties.",
            "handles_distractors": true,
            "distractor_handling_technique": "Downweights domain-specific explanations by penalizing differences in empirical risks across domains, encouraging reliance on features that yield similar performance in all domains.",
            "spurious_signal_types": "Correlation shifts and domain-dependent spurious correlations that change domain risks if relied upon.",
            "detection_method": "Detection is implicit: differences in per-domain empirical risks indicate domain-specific solutions; the variance penalty uses these signals.",
            "downweighting_method": "Penalty (λ) on risk-variance reduces emphasis on domain-exploitative features by increasing loss when per-domain risks diverge.",
            "refutation_method": "Refutation occurs when the model equalizes risks across domains, empirically tested by comparing per-domain risks and downstream OOD performance.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "In the paper's DomainBed results V-REx average accuracy was 68.2% (Table 4); on Colored MNIST in the IRM setup V-REx achieved 67.2% test accuracy (Table 3).",
            "performance_without_robustness": "ERM baseline in DomainBed average 68.7% (in some benchmarks V-REx slightly worse or similar), showing mixed results depending on model selection protocol.",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Matching or penalizing risk variance is effective at enforcing label-aware invariance (unlike purely feature-based methods) and can reduce reliance on spurious correlations, but in this paper Fishr (feature-adaptive variance matching via gradients) is argued to be a more fine-grained variant that also aligns Hessians.",
            "uuid": "e999.4",
            "source_info": {
                "paper_title": "Fishr: Invariant Gradient Variances for Out-of-distribution Generalization",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "IRM",
            "name_full": "Invariant Risk Minimization",
            "brief_description": "A causal-inspired approach that seeks predictors that are simultaneously optimal (invariant) across multiple environments, with the goal of learning causal (stable) mechanisms.",
            "citation_title": "Invariant risk minimization",
            "mention_or_use": "use",
            "method_name": "IRM (Invariant predictor constraint)",
            "method_description": "Formulate invariance as an optimization constraint: find a representation Φ and classifier w such that w ∘ Φ is simultaneously optimal for all environments (often enforced via a penalty that encourages ∇_{w|w=1.0} R_e(w ∘ Φ) = 0). This aims to find predictors whose optimality does not depend on environment-specific correlations.",
            "environment_name": "Colored MNIST (synthetic) and DomainBed benchmark",
            "environment_description": "Static synthetic and real multi-domain datasets used to evaluate invariant-objective methods; not interactive.",
            "handles_distractors": true,
            "distractor_handling_technique": "Constrained/penalized optimization to enforce predictor invariance across domains, thereby discouraging reliance on domain-specific (spurious) features.",
            "spurious_signal_types": "Spurious correlations and domain-specific shortcuts that cause predictors to differ across environments.",
            "detection_method": "Detection by checking whether a single predictor achieves optimality across environments (if not, the predictor is considered non-invariant).",
            "downweighting_method": "Enforces invariance which implicitly downweights domain-specific components by constraining classifier optimality across environments.",
            "refutation_method": "Attempts to eliminate spurious predictors by discarding solutions that are not simultaneously optimal across domains; empirical refutation is via improved OOD performance if invariance is achieved.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "In the DomainBed comparisons IRM average accuracy reported 66.9% (Table 4) and it performed better than ERM on some synthetic setups but showed limitations (sensitivity to hyperparameters; known pitfalls in non-linear settings). On Colored MNIST IRM (in the IRM experimental setup) achieves 65.6% test acc (Table 3).",
            "performance_without_robustness": "ERM can outperform IRM in some settings (e.g., non-noisy fully predictive invariant features), reflecting IRM's practical limitations as discussed in the paper.",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "IRM provides a causal objective to avoid spurious domain-specific predictors, but it has known practical pitfalls (sensitivity to non-linearity, hyperparameters, and model selection) and frequently fails to outperform ERM under restricted, fair model-selection regimes; Fishr provides an alternative gradient-based invariant criterion that empirically outperforms IRM on the DomainBed benchmark.",
            "uuid": "e999.5",
            "source_info": {
                "paper_title": "Fishr: Invariant Gradient Variances for Out-of-distribution Generalization",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "CORAL",
            "name_full": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation",
            "brief_description": "A feature-based domain adaptation method that aligns second-order statistics (feature covariances) across domains to learn domain-invariant features.",
            "citation_title": "Deep coral: Correlation alignment for deep domain adaptation",
            "mention_or_use": "use",
            "method_name": "CORAL (feature covariance alignment)",
            "method_description": "Compute feature covariance matrices Cov(Z_e) per domain and add a penalty ||Cov(Z_A) - Cov(Z_B)||_F^2 (or general multi-domain variant) to align feature second-order statistics across domains, encouraging domain-invariant embeddings.",
            "environment_name": "DomainBed benchmark and other domain-adaptation/domain-generalization datasets (static)",
            "environment_description": "Static datasets (real images) used to evaluate feature-alignment methods; not interactive.",
            "handles_distractors": false,
            "distractor_handling_technique": "Feature-covariance alignment (matching feature second-order statistics across domains), which addresses distributional feature shifts but is label-agnostic.",
            "spurious_signal_types": "Diversity shifts and some covariate distribution shifts; does not explicitly address label-dependent spurious correlations.",
            "detection_method": "No explicit detection of spurious label-based signals; relies on mismatched feature covariances to identify domain shifts.",
            "downweighting_method": "Align features so domain-specific feature statistics are reduced; does not directly downweight label-dependent spurious features.",
            "refutation_method": "Not designed to refute label-dependent spurious correlations; empirical failure on Colored MNIST demonstrates this limitation.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "CORAL average accuracy 69.2% in DomainBed Table 4; performs well on several diversity-shift datasets but fails to address correlation shifts (e.g., Colored MNIST where it cannot detect label-dependent shortcuts).",
            "performance_without_robustness": "Compared to ERM (68.7% avg), CORAL is slightly better on average in DomainBed but not robust to label/correlation shifts (fails on Colored MNIST).",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Feature-covariance matching helps for some diversity/covariate shifts but is label-agnostic and can fail catastrophically on correlation shifts where spurious features correlate with labels; gradient-based, label-aware methods (V-REx, Fishr) are preferable for label-dependent spurious signals.",
            "uuid": "e999.6",
            "source_info": {
                "paper_title": "Fishr: Invariant Gradient Variances for Out-of-distribution Generalization",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Invariant risk minimization",
            "rating": 2
        },
        {
            "paper_title": "Learning explanations that are hard to vary",
            "rating": 2
        },
        {
            "paper_title": "Gradient matching for domain generalization",
            "rating": 2
        },
        {
            "paper_title": "Out-of-distribution generalization via risk extrapolation (rex)",
            "rating": 2
        },
        {
            "paper_title": "Deep coral: Correlation alignment for deep domain adaptation",
            "rating": 1
        }
    ],
    "cost": 0.02524075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization</h1>
<p>Alexandre Ramé ${ }^{1}$ Corentin Dancette ${ }^{1}$ Matthieu Cord ${ }^{12}$</p>
<h4>Abstract</h4>
<p>Learning robust models that generalize well under changes in the data distribution is critical for realworld applications. To this end, there has been a growing surge of interest to learn simultaneously from multiple training domains - while enforcing different types of invariance across those domains. Yet, all existing approaches fail to show systematic benefits under controlled evaluation protocols. In this paper, we introduce a new regularization - named Fishr - that enforces domain invariance in the space of the gradients of the loss: specifically, the domain-level variances of gradients are matched across training domains. Our approach is based on the close relations between the gradient covariance, the Fisher Information and the Hessian of the loss: in particular, we show that Fishr eventually aligns the domain-level loss landscapes locally around the final weights. Extensive experiments demonstrate the effectiveness of Fishr for out-of-distribution generalization. Notably, Fishr improves the state of the art on the DomainBed benchmark and performs consistently better than Empirical Risk Minimization. Our code is available at https: //github.com/alexrame/fishr.</p>
<h2>1. Introduction</h2>
<p>The success of deep neural networks in supervised learning (Krizhevsky et al., 2012) relies on the crucial assumption that the train and test data distributions are identical. In particular, the tendency of networks to rely on simple features (Valle-Perez et al., 2019; Geirhos et al., 2020) is generally a desirable behavior reflecting Occam's razor. However, in case of distribution shift, this simplicity bias deteriorates performance when more complex features are needed (Tenenbaum, 2018; Shah et al., 2020). For example, in the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Fishr principle. Fishr considers the individual (per-sample) gradients of the loss in the network weights $\theta$. Specifically, Fishr matches the domain-level gradient variances of the distributions across the two training domains: $A\left(\left{\boldsymbol{g}<em i="1">{A}^{i}\right}</em>}^{n_{A}}\right.$ in orange) and $B\left(\left{\boldsymbol{g<em i="1">{B}^{i}\right}</em>\right.$ in blue). We will show how this regularization during the learning of $\theta$ improves the out-of-distribution generalization properties by aligning the domain-level loss landscapes at convergence.
recent fight against Covid-19, most of the deep learning methods developed to detect coronavirus from chest scans were shown useless for clinical use (DeGrave et al., 2021; Roberts et al., 2021): indeed, networks exploited simple bias in the training datasets such as patients' age or body position rather than 'truly' analyzing medical pathologies.}^{n_{B}</p>
<p>To better generalize under distribution shifts, most works (Blanchard et al., 2011; Muandet et al., 2013) assume that the training data is divided into different training domains in which there is a constant underlying causal mechanism (Peters et al., 2016). To remove the domain-dependent explanations, different invariance criteria across those training domains have been proposed. Ganin et al. (2016); Sun et al. (2016); Sun \&amp; Saenko (2016) enforce similar feature distributions, others (Arjovsky et al., 2019; Krueger et al., 2021) force the classifier to be simultaneously optimal across all domains. Yet, despite the popularity of this research topic, none of these methods perform significantly better than the classical Empirical Risk Minimization (ERM) when applied with controlled model selection and restricted hyperparameter search (Gulrajani \&amp; Lopez-Paz, 2021; Ye et al., 2021).</p>
<p>These failures motivate the need for new ideas.
To foster the emergence of a shared mechanism with consistent generalization properties, our intuition is that learning should progress consistently and similarly across domains. Besides, the learning procedure of deep neural networks is dictated by the distribution of the gradients with respect to the network weights (Yin et al., 2018; Sankararaman et al., 2020) - usually backpropagated in the network during gradient descent. Additionally, individual gradients are expressive representations of the input (Fort et al., 2019; Charpiat et al., 2019). Thus, we seek distributional invariance across domains in the gradient space: domain-level gradients should be similar, not only in average direction, but most importantly in statistics such as variance and disagreements.</p>
<p>In this paper, we propose the Fishr regularization for out-of-distribution generalization in classification for computer vision - summarized in Fig. 1. We match the domainlevel gradient variances, i.e., the second moment of the gradient distributions. In contrast, previous gradient-based works such as Fish (Shi et al., 2021) only match the domainlevel gradients means, i.e., the first moment.</p>
<p>Our strategy is also motivated by the close relations between the gradient variance, the Fisher Information (Fisher, 1922) and the Hessian. This explains the name of our work, Fishr, using gradients as in Fish and related to the Fisher Matrix. Notably, we will study how Fishr forces the model to have similar domain-level Hessians and promotes consistent explanations - by generalizing the inconsistency formalism introduced in Parascandolo et al. (2021).</p>
<p>To reduce the computational cost, we justify an approximation that tackles the gradients only in the classifier, easily implemented with BackPACK (Dangel et al., 2020).</p>
<p>We summarize our contributions as follows:</p>
<ul>
<li>We introduce Fishr, a scalable regularization that brings closer the domain-level gradient variances.</li>
<li>We theoretically justify that Fishr matches domainlevel risks and Hessians, and consequently, reduces inconsistencies across domains.</li>
</ul>
<p>Empirically, we first validate that Fishr tackles distribution shifts on the synthetic Colored MNIST (Arjovsky et al., 2019). Then, we show that Fishr performs best on the DomainBed benchmark (Gulrajani \&amp; Lopez-Paz, 2021) when compared with state-of-the-art counterparts. Critically, Fishr is the only method to perform systematically better than ERM on all real datasets - PACS, VLCS, OfficeHome, TerraIncognita and DomainNet.</p>
<h2>2. Context and Related Work</h2>
<p>We first describe our task and provide the notations used along our paper. Then we remind some important related works to understand how our Fishr stands in a rich literature.</p>
<p>Problem definition and notations. We study out-ofdistribution (OOD) generalization for classification. Our model is a deep neural network (DNN) $f_{\theta}$ (parametrized by $\theta$ ) made of a deep features extractor $\Phi_{\phi}$ on which we plug a dense linear classifier $w_{\omega}: f_{\theta}=w_{\omega} \circ \Phi_{\phi}$ and $\theta=(\phi, \omega)$. In training, we have access to different domains $\mathcal{E}$ : for each domain $e \in \mathcal{E}$, the dataset $\mathcal{D}<em e="e">{e}=\left{\left(\boldsymbol{x}</em>}^{i}, \boldsymbol{y<em i="1">{e}^{i}\right)\right}</em>}^{n_{e}}$ contains $n_{e}$ i.i.d. (input, labels) samples drawn from a domaindependent probability distribution. Combined together, the datasets $\left{\mathcal{D<em _in="\in" _mathcal_E="\mathcal{E" e="e">{e}\right}</em>|}}$ are of size $n=\sum_{e \in \mathcal{E}} n_{e}$. Our goal is to learn weights $\theta$ so that $f_{\theta}$ predicts well on a new test domain, unseen in training. As described in Koh et al. (2020) and Ye et al. (2021), most common distribution shifts are diversity shifts - where the training and test distributions comprise data from related but distinct domains, for instance pictures and drawings of the same objects - or correlation shifts - where the distribution of the covariates at test time differs from the one during training. To generalize well despite these distribution shifts, $f_{\theta}$ should ideally capture an invariant mechanism across training domains. Following standard notations, $|\boldsymbol{M<em 2="2">{F}^{2}$ denotes the Frobenius norm of matrix $\boldsymbol{M}$; $|\boldsymbol{v}|</em>$ is a column vector with all elements equal to 1 .
The standard Empirical Risk Minimization (ERM) (Vapnik, 1999) framework simply minimizes the average empirical risk over all training domains, i.e., $\frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathcal{R}}^{2}$ denotes the euclidean norm of vector $\boldsymbol{v} ; \mathbf{1<em e="e">{e}(\theta)$ where $\mathcal{R}</em>}(\theta)=\frac{1}{n_{e}} \sum_{i=1}^{n_{e}} \ell\left(f_{\theta}\left(\boldsymbol{x<em e="e">{e}^{i}\right), \boldsymbol{y}</em>\right)$ and $\ell$ is the negative log-likelihood loss. Many approaches try to exploit some external source of knowledge (Xie et al., 2021), in particular the domain information. As a side note, these partitions may be inferred if not provided (Creager et al., 2021). Some works explore data augmentations to mix samples from different domains (Wang et al., 2020; Wu et al., 2020), some re-weight the training samples to favor underrepresented groups (Sagawa et al., 2020a;b; Zhang et al., 2021) and others include domain-dependent weights (Ding \&amp; Fu, 2017; Mancini et al., 2018). Yet, most recent works promote invariance via a regularization criterion and only differ by the choice of the statistics to be matched across training domains. They can be categorized into three groups: these methods enforce agreement either (1) in features (2) in predictors or (3) in gradients.}^{i</p>
<p>First, some approaches aim at extracting domain-invariant features and were extensively studied for unsupervised domain adaptation. The features are usually aligned with adversarial methods (Ganin et al., 2016; Gong et al., 2016; Li</p>
<p>et al., 2018b;c) or with kernel methods (Muandet et al., 2013; Long et al., 2014). Yet, the simple covariance matching in CORAL (Sun et al., 2016; Sun \&amp; Saenko, 2016) performs best on various tasks for OOD generalization (Gulrajani \&amp; Lopez-Paz, 2021). With $\boldsymbol{Z}<em _phi="\phi">{e}^{i j}$ the $j$ th dimension of the features extracted by $\Phi</em>}$ for the $i$ th example $\boldsymbol{x<em A="A">{e}^{i}$ of domain $e \in \mathcal{E}={A, B}$, CORAL minimizes $\left|\operatorname{Cov}\left(\boldsymbol{Z}</em>}\right)-\operatorname{Cov}\left(\boldsymbol{Z<em F="F">{B}\right)\right|</em>}^{2}$ where $\operatorname{Cov}\left(\boldsymbol{Z<em e="e">{e}\right)=$ $\frac{1}{n</em>}-1}\left(\boldsymbol{Z<em e="e">{e}^{\top} \boldsymbol{Z}</em>}-\frac{1}{n_{e}}\left(\mathbf{1}^{\top} \boldsymbol{Z<em e="e">{e}\right)^{\top}\left(\mathbf{1}^{\top} \boldsymbol{Z}</em>}\right)\right)$ is the feature covariance matrix. CORAL is more powerful than mere feature matching $\left|\frac{1}{n_{A}} \mathbf{1}^{\top} \boldsymbol{Z<em B="B">{A}-\frac{1}{n</em>}} \mathbf{1}^{\top} \boldsymbol{Z<em 2="2">{B}\right|</em>$ as in Deep Domain Confusion (DDC) (Tzeng et al., 2014). Yet, Johansson et al. (2019) and Zhao et al. (2019) show that these approaches are insufficient to guarantee good generalization.}^{2</p>
<p>Motivated by arguments from causality (Pearl, 2009) and the idea that statistical dependencies are epiphenomena of an underlying structure, Invariant Risk Minimization (IRM) (Arjovsky et al., 2019) explains that the predictor should be invariant (Peters et al., 2016; Rojas-Carulla et al., 2018), i.e., simultaneously optimal across all domains. Yet, recent works point out pitfalls of IRM (Guo et al., 2021; Kamath et al., 2021; Ahuja et al., 2019), that does not provably work with non-linear data (Rosenfeld et al., 2021) and could not improve over ERM when hyperparameter selection is restricted (Koh et al., 2020; Gulrajani \&amp; Lopez-Paz, 2021). Among many suggested improvements (Chang et al., 2020; Idnani \&amp; Kao, 2020; Teney et al., 2020; Ahmed et al., 2021), Risk Extrapolation (V-REx) (Krueger et al., 2021) argues that training risks from different domains should be similar and thus penalizes $\left|\mathcal{R}<em B="B">{A}-\mathcal{R}</em>={A, B}$.}\right|^{2}$ when $\mathcal{E</p>
<p>A third and most recent line of work promotes agreements between gradients with respect to network weights. Gradient agreements help batches from different tasks to cooperate, and have been previously employed for multitasks (Du et al., 2018; Yu et al., 2020), continual (Lopez-Paz \&amp; Ranzato, 2017), meta (Finn et al., 2017; Zhang et al., 2020) and reinforcement (Zhang et al., 2019) learning. In OOD generalization, Koyama \&amp; Yamaguchi (2020); Parascandolo et al. (2021); Shi et al. (2021) try to find minimas in the loss landscape that are shared across domains. Specifically, these works tackle the domain-level expected gradients:</p>
<p>$$
\boldsymbol{g}<em _left_boldsymbol_x="\left(\boldsymbol{x">{e}=\mathbb{E}</em><em e="e">{e}, \boldsymbol{y}</em>}\right) \sim \mathcal{D<em _theta="\theta">{e}} \nabla</em>} \ell\left(f_{\theta}\left(\boldsymbol{x<em e="e">{e}\right), \boldsymbol{y}</em>\right)
$$</p>
<p>When $\mathcal{E}={A, B}$, IGA (Koyama \&amp; Yamaguchi, 2020) minimizes $\left|\boldsymbol{g}<em B="B">{A}-\boldsymbol{g}</em>\right|<em A="A">{2}^{2}$; Fish (Shi et al., 2021) increases $\boldsymbol{g}</em>} \cdot \boldsymbol{g<em A="A">{B}$; AND-mask (Parascandolo et al., 2021) and others (Mansilla et al., 2021; Shahtalebi et al., 2021) update weights only when $\boldsymbol{g}</em>$ point to the same direction.}$ and $\boldsymbol{g}_{B</p>
<p>Along with the increased computation cost, the main limitation of previous gradient-based methods is the per-domain batch averaging of gradients: this removes more granular
statistics, in particular the information from pairwise interactions between gradients from samples in a same domain. In opposition, our new regularization for OOD generalization keeps extra information from individual gradients and matches across domains the domain-level gradient variances. In a nutshell, Fishr is similar to the covariance-based CORAL (Sun et al., 2016; Sun \&amp; Saenko, 2016) but in the gradient space rather than in the feature space.</p>
<h2>3. Fishr</h2>
<h3>3.1. Gradient variance matching</h3>
<p>The individual gradient $\boldsymbol{g}<em _theta="\theta">{e}^{i}=\nabla</em>} \ell\left(f_{\theta}\left(\boldsymbol{x<em e="e">{e}^{i}\right), \boldsymbol{y}</em>}^{i}\right)$ is the firstorder derivative for the $i$-th data example $\left(\boldsymbol{x<em e="e">{e}^{i}, \boldsymbol{y}</em>}^{i}\right)$ from domain $e \in \mathcal{E}$ with respect to the weights $\theta$. Previous methods have matched the gradient means $\boldsymbol{g<em e="e">{e}=\frac{1}{n</em>}} \sum_{i=1}^{n_{e}} \boldsymbol{g<em e="e">{e}^{i}$ for each domain $e \in \mathcal{E}$. These gradient means capture the average learning direction but can not capture gradient disagreements (Sankararaman et al., 2020; Yin et al., 2018). With $\boldsymbol{G}</em>}=\left[\boldsymbol{g<em i="1">{e}^{i}\right]</em> \times|\theta|$, we compute the domain-level gradient variance vectors of size $|\theta|$ :}^{n_{e}}$ of size $n_{e</p>
<p>$$
\boldsymbol{v}<em e="e">{e}=\operatorname{Var}\left(\boldsymbol{G}</em>}\right)=\frac{1}{n_{e}-1} \sum_{i=1}^{n_{e}}\left(\boldsymbol{g<em e="e">{e}^{i}-\boldsymbol{g}</em>
$$}\right)^{2</p>
<p>where the square indicates an element-wise product. To reduce the distribution shifts in the network $f_{\theta}$ across domains, we bring the domain-level gradient variances $\left{\boldsymbol{v}<em _in="\in" _mathcal_E="\mathcal{E" e="e">{e}\right}</em>$ closer. Hence, our Fishr regularization is:}</p>
<p>$$
\mathcal{L}<em _in="\in" _mathcal_E="\mathcal{E" e="e">{\text {Fishr }}(\theta)=\frac{1}{|\mathcal{E}|} \sum</em>}}\left|\boldsymbol{v<em 2="2">{e}-\boldsymbol{v}\right|</em>
$$}^{2</p>
<p>the square of the Euclidean distance between the gradient variance from the different domains $e \in \mathcal{E}$ and the mean gradient variance $\boldsymbol{v}=\frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \boldsymbol{v}_{e}$. Balanced with a hyperparameter coefficient $\lambda&gt;0$, this Fishr penalty complements the original ERM objective, i.e., the empirical training risks:</p>
<p>$$
\mathcal{L}(\theta)=\frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathcal{R}<em _Fishr="{Fishr" _text="\text">{e}(\theta)+\lambda \mathcal{L}</em>(\theta)
$$}</p>
<p>Remark 3.1. Gradients $\boldsymbol{g}_{e}^{i}$ can be computed on all network weights $\theta$. Yet, to reduce the memory and training costs, they will often be computed only on a subset of $\theta$, e.g., only on classification weights $\omega$. This approximation is discussed in Section 4.2.2 and Appendix D.3.2.</p>
<h3>3.2. Theoretical analysis</h3>
<p>We theoretically motivate our Fishr regularization by leveraging the domain inconsistency score introduced in ANDmask (Parascandolo et al., 2021). We first derive a generalized upper bound for this score. Then, we show that Fishr minimizes this upper bound by matching simultaneously domain-level risks and Hessians.</p>
<h3>3.2.1. INCONSISTENCY FORMALISM</h3>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Loss landscapes around inconsistent weights $\theta^{<em>}$ at convergence. $N_{A, \theta^{</em>}}^{0.2}$ contains weights $\theta$ for which $\mathcal{R}<em B="B">{A}(\theta)$ is low ( $\leq 0.2$ ) but $\mathcal{R}</em>}(\theta)$ is high ( $\geq 0.9$ ). This inconsistency is due to conflicting domain-level loss landscapes, specifically gaps between domain-level risks and curvatures at $\theta^{*}$. This is visible in the disagreements across the variances of gradients $\left{\boldsymbol{g<em i="1">{A}^{i}\right}</em>}^{n_{A}}$ and $\left{\boldsymbol{g<em i="1">{B}^{i}\right}</em>$.}^{n_{B}</p>
<p>Parascandolo et al. (2021) argues that "patchwork solutions sewing together different strategies" for different domains may not generalize well: good weights should be optimal on all domains and "hard to vary" (Deutsch, 2011). They formalize this insight with an inconsistency score:</p>
<p>$$
\mathcal{I}^{\epsilon}\left(\theta^{<em>}\right)=\max <em>{(A, B) \in \mathcal{E}^{2}} \max </em>{\theta \in N_{A, \theta^{</em>}}^{e}}\left|\mathcal{R}<em A="A">{B}(\theta)-\mathcal{R}</em>\right)\right|
$$}\left(\theta^{*</p>
<p>where $\theta \in N_{A, \theta^{<em>}}^{e}$ if there exists a path in the weights space between $\theta$ and $\theta^{</em>}$ where the risk $\mathcal{R}<em A="A">{A}$ remains in an $\epsilon&gt;0$ interval around $\mathcal{R}</em>\left(\theta^{<em>}\right) . \mathcal{I}$ increases with conflicting geometries in the loss landscapes around $\theta^{</em>}$ as in Fig. 2: i.e., when another 'close' solution $\theta$ is equivalent to the current solution $\theta^{*}$ in a domain $A$ but yields different risks in $B$.</p>
<p>For $e \in \mathcal{E}$, the second-order Taylor expansion of $\mathcal{R}<em _theta="\theta">{e}$ around $\theta^{<em>}=0$ (with a change of variable) gives:
$\mathcal{R}<em e="e">{e}(\theta)=\mathcal{R}</em>\left(\theta^{</em>}\right)+\theta^{\top} \nabla</em>\left(\theta^{} \mathcal{R}_{e<em>}\right)+\frac{1}{2} \theta^{\top} H_{e} \theta+\mathcal{O}\left(|\theta|<em e="e">{2}^{2}\right)$,
where the Hessian $\boldsymbol{H}</em>\left(\theta^{}=\nabla_{\theta}^{2} \mathcal{R}_{e</em>}\right)$ approximates the local curvature of the loss landscape. Moreover, we assume simultaneous convergence, i.e., $\theta^{<em>}$ is a local minima across all domains: $\nabla_{\theta} \mathcal{R}_{e}\left(\theta^{</em>}\right)=0$. Thus, locally around $\theta^{*}$ :</p>
<p>$$
\begin{aligned}
&amp; \max <em>{\theta \in N</em>{A, \theta^{<em>}}^{e}}\left|\mathcal{R}<em A="A">{B}(\theta)-\mathcal{R}</em>\left(\theta^{</em>}\right)\right| \
&amp; \approx \max <em A="A">{\left|\mathcal{R}</em>}(\theta)-\mathcal{R<em _frac_1="\frac{1">{A}\left(\theta^{<em>}\right)\right| \leq \epsilon}\left|\mathcal{R}<em A="A">{B}(\theta)-\mathcal{R}</em>\left(\theta^{</em>}\right)\right| \
&amp; \approx \max </em>}{2}\left|\theta^{\top} H_{A} \theta\right| \leq \epsilon}\left|\mathcal{R<em B="B">{B}\left(\theta^{<em>}\right)+\frac{1}{2} \theta^{\top} H_{B} \theta-\mathcal{R}_{A}\left(\theta^{</em>}\right)\right| \
&amp; \lesssim\left|\mathcal{R}</em>\left(\theta^{<em>}\right)-\mathcal{R}_{A}\left(\theta^{</em>}\right)\right|+\max <em A="A">{\frac{1}{2}\left|\theta^{\top} H</em> \theta\right|
\end{aligned}
$$} \theta\right| \leq \epsilon} \frac{1}{2}\left|\theta^{\top} H_{B</p>
<p>where we deduced the last line from the triangle inequality. Appendix A. 1 formally demonstrates following equality.</p>
<p>Proposition 1. Under the quadratic bowl Assumption A. 1 with positive definite Hessians, for small $\epsilon$ (see Eq. 11):</p>
<p>$$
\begin{aligned}
\mathcal{I}^{\epsilon}\left(\theta^{<em>}\right)=\max <em B="B">{(A, B) \in \mathcal{E}^{2}} &amp; \left(\mathcal{R}</em>\left(\theta^{</em>}\right)-\mathcal{R}<em _frac_1="\frac{1">{A}\left(\theta^{*}\right)\right. \
&amp; \left.+\max </em> \theta\right)
\end{aligned}
$$}{2} \theta^{\top} H_{A} \theta \leq \epsilon} \frac{1}{2} \theta^{\top} H_{B</p>
<p>The Hessian being positive definite is a standard hypothesis, notably used in Parascandolo et al. (2021), that is empirically reasonable (Sagun et al., 2018): "in only very few steps ... large negative eigenvalues disappear" (Ghorbani et al., 2019).</p>
<p>The first term in the RHS of Proposition 1 is the difference between domain-level risks, whose square is the criterion minimized in V-REx (Krueger et al., 2021). We will prove and show that Fishr forces this term to be small in Section 3.2.2. In contrast, Parascandolo et al. (2021) made the strong assumption: $\mathcal{R}_{A}\left(\theta^{<em>}\right)=\mathcal{R}_{B}\left(\theta^{</em>}\right)=0$.</p>
<p>While Parascandolo et al. (2021) ignored this first term, we follow their diagonal approximation of the Hessians to analyze the second term. In that case, $\boldsymbol{H}<em 1="1">{e}=$ $\operatorname{diag}\left(\lambda</em>&gt;0$. Then:}^{e}, \cdots, \lambda_{h}^{e}\right)$ with $\forall i \in{1, \ldots, h}, \lambda_{i}^{e</p>
<p>$$
\begin{aligned}
\max <em A="A">{\frac{1}{2} \theta^{\top} H</em> \theta &amp; =\max } \theta \leq \epsilon} \frac{1}{2} \theta^{\top} H_{B<em 2="2">{|\hat{\theta}|</em>}^{2} \leq \epsilon} \sum_{i} \hat{\theta<em i="i">{i}^{2} \lambda</em> \
&amp; =\epsilon \times \max }^{B} / \lambda_{i}^{A<em i="i">{i} \lambda</em>
\end{aligned}
$$}^{B} / \lambda_{i}^{A</p>
<p>This is large when exists $i$ such that $\lambda_{i}^{A}$ is small but $\lambda_{i}^{B}$ is large: indeed, a small weight perturbation in the direction of the associated eigenvector would change the loss slightly in the domain $A$ but drastically in domain $B$. Thus, this second term decreases when $\boldsymbol{H}<em B="B">{A}$ and $\boldsymbol{H}</em>}$ have similar eigenvalues. This result holds when Hessians are co-diagonalizable. In conclusion, this explains why forcing $\boldsymbol{H<em B="B">{A}=\boldsymbol{H}</em>$ reduces inconsistencies in the loss landscape and thus improves generalization. AND-mask matches Hessians by zeroing out gradients with inconsistent directions across domains; however, this masking strategy introduces dead zones (Shahtalebi et al., 2021) in weights where the model could get stuck, ignores gradient magnitudes and empirically performs poorly with real datasets from DomainBed. As shown in Section 3.2.3, Fishr proposes a new method to align domain-level Hessians leveraging the close relations between the gradient variance, the Fisher Information and the Hessian.</p>
<h3>3.2.2. FISHR MATCHES THE DOMAIN-LEVEL RISKS</h3>
<p>Gradients take into account the label $Y$, which appears as an argument for the loss $\ell$. Hence, gradient-based approaches are 'label-aware' by design. In contrast, feature-based methods were shown to fail in case of label shifts, because they do not consider $Y$ (Johansson et al., 2019; Zhao et al., 2019).
The fact that the label and the loss appear in the formula of the gradients has another important consequence: matching gradient distributions also matches training risks, as motivated in V-REx (Krueger et al., 2021). We confirm this insight in Table 2: matching gradient variances with Fishr induces $\left|\mathcal{R}<em B="B">{A}-\mathcal{R}</em>={A, B}$.
Intuitively, gradient amplitudes are directly weighted by the loss values: multiplying the loss by a constant will also multiply the gradients by the same constant. Thus roughly, if the domain-level empirical training risks are different, then the domain-level gradient norms should also differ.
Theoretically, we prove in Appendix A. 2 that Fishr regularization component with reference to the classification bias is exactly the difference between domain-level mean squared errors. We recover the objective from V-REx (Krueger et al., 2021), with a different loss (squared error instead of negative log likelihood). More generally, we show in this Appendix that Fishr in the classifier $w_{\omega}$ acts as a feature-adaptive version of V-REx: the components in Fishr adaptively force the risks to be similar across domains.}\right|^{2} \rightarrow 0$ when $\mathcal{E</p>
<h3>3.2.3. FISHR MATCHES THE DOMAIN-LEVEL HESSIANS</h3>
<p>The Hessian matrix $\boldsymbol{H}=\sum_{i=1}^{n} \nabla_{\theta}^{2} \ell\left(f_{\theta}\left(\boldsymbol{x}^{i}\right), \boldsymbol{y}^{i}\right)$ is of key importance in deep learning. Yet, $\boldsymbol{H}$ cannot be computed efficiently in general. Recent methods (Izmailov et al., 2018; Parascandolo et al., 2021; Foret et al., 2021) tackled the Hessian indirectly by modifying the learning procedure. In contrast, we use the fact that the diagonal of $\boldsymbol{H}$ is approximated by the gradient variance $\operatorname{Var}(\boldsymbol{G})$; this is confirmed in Table 1. This result is derived below from 3 individual and standard approximation steps.</p>
<p>Table 1: Cosine similarity between Hessian diagonals and gradient variances $\cos \left(\operatorname{Diag}\left(\boldsymbol{H}<em e="e">{e}\right), \operatorname{Var}\left(\boldsymbol{G}</em>\right)\right)$, for an ERM at convergence on Colored MNIST with the two training domains $e \in{90 \%, 80 \%}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$e=90 \%$</th>
<th style="text-align: center;">$e=80 \%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">On classifier weights $w$</td>
<td style="text-align: center;">0.9999980</td>
<td style="text-align: center;">0.9999905</td>
</tr>
<tr>
<td style="text-align: left;">On all network weights $\theta$</td>
<td style="text-align: center;">0.9971040</td>
<td style="text-align: center;">0.9962264</td>
</tr>
</tbody>
</table>
<p>The Hessian and the 'true' Fisher Information Matrix (FIM). The 'true' FIM $\boldsymbol{F}=$ $\sum_{i=1}^{n} \mathbb{E}<em _theta="\theta">{\hat{\boldsymbol{y}} \sim P</em>$
with theoretically probably bounded errors under mild assumptions (Schraudolph, 2002).}\left(\cdot \mid \boldsymbol{x}^{i}\right)}\left[\nabla_{\theta} \log p_{\theta}\left(\hat{\boldsymbol{y}} \mid \boldsymbol{x}^{i}\right) \nabla_{\theta} \log p_{\theta}\left(\hat{\boldsymbol{y}} \mid \boldsymbol{x}^{i}\right)^{\top}\right]$ (Fisher, 1922; C.R., 1945) approximates the Hessian $\boldsymbol{H</p>
<p>The 'true' FIM and the 'empirical' FIM. Yet, $\boldsymbol{F}$ remains costly as it demands one backpropagation per class. That's why most empirical works (e.g., in compression (Frantar et al., 2021; Liu et al., 2021) and optimization (Dangel et al., 2021)) approximate the 'true' FIM $\boldsymbol{F}$ with the 'empirical' FIM $\hat{\boldsymbol{F}}=\boldsymbol{G}<em e="e">{e}^{\top} \boldsymbol{G}</em>)$. This was discussed in Li et al. (2020) and further highlighted even at early stages of training (before overfitting) in the Fig. 1 and the Appendix S3 of Singh \&amp; Alistarh (2020).}=$ $\sum_{i=1}^{n} \nabla_{\theta} \log p_{\theta}\left(\boldsymbol{y}^{i} \mid \boldsymbol{x}^{i}\right) \nabla_{\theta} \log p_{\theta}\left(\boldsymbol{y}^{i} \mid \boldsymbol{x}^{i}\right)^{\top}$ (Martens, 2014) where $p_{\theta}(\cdot \mid \boldsymbol{x})$ is the density predicted by $f_{\theta}$ on input $\boldsymbol{x}$. While $\boldsymbol{F}$ uses the model distribution $P_{\theta}(\cdot \mid X), \hat{\boldsymbol{F}}$ uses the data distribution $P(Y \mid X)$. Despite this key difference, $\hat{\boldsymbol{F}}$ and $\boldsymbol{F}$ were shown to share the same structure and to be similar up to a scalar factor (Thomas et al., 2020). They also have analogous properties: $\operatorname{Tr}(\hat{\boldsymbol{F}}) \approx \operatorname{Tr}(\boldsymbol{F</p>
<p>The 'empirical' FIM and the gradient covariance. Critically, $\hat{\boldsymbol{F}}$ is nothing else than the unnormalized uncentered covariance matrix when $\ell$ is the negative loglikelihood. Thus, the gradient covariance matrix $\boldsymbol{C}=$ $\frac{1}{n-1}\left(\boldsymbol{G}^{\top} \boldsymbol{G}-\frac{1}{n}\left(\mathbf{1}^{\top} \boldsymbol{G}\right)^{\top}\left(\mathbf{1}^{\top} \boldsymbol{G}\right)\right)$ of size $|\theta| \times|\theta|$ and $\hat{\boldsymbol{F}}$ are equivalent (up to the multiplicative constant $n$ ) at any first-order stationary point: $\boldsymbol{C} \gtrsim \hat{\boldsymbol{F}}$. Overall, this suggests that $\boldsymbol{C}$ and $\boldsymbol{H}$ are closely related (Jastrzebski et al., 2018);</p>
<p>Table 2: Invariance analysis at convergence on Colored MNIST across the two training domains $\mathcal{E}={90 \%, 80 \%}$. Compared to ERM, Fishr matches the gradient variance $\left(\operatorname{Diag}\left(\boldsymbol{C}<em 80="80" _="\%">{90 \%}\right) \approx \operatorname{Diag}\left(\boldsymbol{C}</em>}\right)\right)$ in all network weights $\theta$. Most importantly, this enforces invariance in domainlevel risks $\left(\mathcal{R<em 80="80" _="\%">{90 \%} \approx \mathcal{R}</em>}\right)$ and in domain-level Hessians $\left(\operatorname{Diag}\left(\boldsymbol{H<em 80="80" _="\%">{90 \%}\right) \approx \operatorname{Diag}\left(\boldsymbol{H}</em>\right)\right)$. The gradient variance, computable efficiently with a unique backpropagation, serves as a proxy for the Hessian. Details and more experiments in Section 4.1 (notably Fig. 3) and in Appendix C.2.1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ERM</th>
<th style="text-align: center;">Fishr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\left|\operatorname{Var}\left(\boldsymbol{G}<em 80="80" _="\%">{90 \%}\right)-\operatorname{Var}\left(\boldsymbol{G}</em>$}\right)\right|_{F}^{2</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">$4.1 \times 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">$\left|\mathcal{R}<em 80="80" _="\%">{90 \%}-\mathcal{R}</em>$}\right|^{2</td>
<td style="text-align: center;">$1.0 \times 10^{-2}$</td>
<td style="text-align: center;">$3.8 \times 10^{-6}$</td>
</tr>
<tr>
<td style="text-align: center;">$\left|\operatorname{Diag}\left(\boldsymbol{H}<em 80="80" _="\%">{90 \%}-\boldsymbol{H}</em>$}\right)\right|_{F}^{2</td>
<td style="text-align: center;">$2.9 \times 10^{-1}$</td>
<td style="text-align: center;">$2.7 \times 10^{-4}$</td>
</tr>
</tbody>
</table>
<p>Consequences for Fishr. Critically, Fishr considers the gradient variance $\operatorname{Var}(\boldsymbol{G})$, i.e., the diagonal components of $\boldsymbol{C}$. In our multi-domain framework, we define the domain-level matrices with the subscript $e$. Table 2 empirically confirms that matching $\left{\operatorname{Diag}\left(\boldsymbol{C}<em _in="\in" _mathcal_E="\mathcal{E" e="e">{e}\right)\right}</em>}}$ - i.e., $\left{\operatorname{Var}\left(\boldsymbol{G<em _in="\in" _mathcal_E="\mathcal{E" e="e">{e}\right)\right}</em>$ - with Fishr forces the domain-level Hes-}</p>
<p>sians $\left{\operatorname{Diag}\left(\boldsymbol{H}<em _mathrm_e="\mathrm{e">{\mathrm{e}}\right)\right}</em>} \in \mathcal{E}}$ to be aligned at convergence (on the diagonal for computational reasons). Tackling the second moment of the first-order derivatives enables to regularize the second-order derivatives. Moreover, Appendix C.2.4 shows that matching the diagonals of $\left{\boldsymbol{C<em _mathrm_e="\mathrm{e">{\mathrm{e}}\right}</em>} \in \mathcal{E}}$ or $\left{\tilde{\boldsymbol{F}<em _mathrm_e="\mathrm{e">{\mathrm{e}}\right}</em>$ — i.e., centering or not the variances - perform similarly.} \in \mathcal{E}</p>
<p>Remark 3.2. Limitation of our approximation. We acknowledge that approximating the 'true' FIM $\boldsymbol{F}$ by the 'empirical' FIM $\tilde{\boldsymbol{F}}$ is not fully justified theoretically (Martens, 2014; Kunstner et al., 2019). Indeed, this approximation is valid only under strong assumptions, in particular $\chi^{2}$ convergence of predictions $P_{\theta}(\cdot \mid X)$ towards labels $P(Y \mid X)$ as detailed in Proposition 1 from Thomas et al. (2020). In this paper, we trade off theoretical guarantees for efficiency.</p>
<p>Remark 3.3. Diagonal approximation. The empirical similarities between $\boldsymbol{C}$ and $\boldsymbol{H}$ motivate using gradient variance rather than gradient covariance, which scales down the number of targeted components from $|\theta|^{2}$ to $|\theta|$. Indeed, diagonally approximating the Hessian is common: e.g., for OOD generalization (Parascandolo et al., 2021), optimization (LeCun et al., 2012; Kingma \&amp; Ba, 2014), continual learning (Kirkpatrick et al., 2017) and pruning (LeCun et al., 1990; Theis et al., 2018). This is based on the empirical evidence (Becker \&amp; Le Cun, 1988) that Hessians are diagonally dominant at the end of training. Our diagonal approximation is also motivated by the critical importance of $\operatorname{Tr}(\boldsymbol{C})$ (Jastrzebski et al., 2021; Faghri et al., 2020) to analyze the generalization properties of DNNs. We confirm empirically in Appendix C.2.3 that considering the off-diagonal parts of $\boldsymbol{C}$ performs no better that just matching the diagonals.</p>
<p>Conclusion. Fishr efficiently matches (1) domain-level empirical risks and (2) domain-level Hessians across the training domains, using gradient variances as a proxy. This will align domain-level loss landscapes, reduce domain inconsistencies and increase domain generalization. In particular, the domain-level Hessian matching illustrates that Fishr is more than just a generalization of gradient-mean approaches such as Fish (Shi et al., 2021).</p>
<p>Finally, we refer the readers to Appendix A. 3 where we leverage the Neural Tangent Kernel (NTK) (Jacot et al., 2018) theory to further motivate the gradient variance matching during the optimization process - and not only at convergence. In brief, as $\boldsymbol{F}$ and the NTK matrices share the same non-zero eigenvalues, similar $\left{\boldsymbol{C}<em _mathrm_e="\mathrm{e">{\mathrm{e}}\right}</em>$ during training reduce the simplicity bias by preventing the learning of different domain-dependent shortcuts at different training speeds: this favors a shared mechanism that predicts the same thing for the same reasons across domains.} \in \mathcal{E}</p>
<h2>4. Experiments</h2>
<p>We prove Fishr effectiveness on Colored MNIST (Arjovsky et al., 2019) and then on the DomainBed benchmark (Gulrajani \&amp; Lopez-Paz, 2021). To facilitate reproducibility, the code is available at https://github.com/ alexrame/fishr. Moreover, we show in Appendix B that Fishr is effective in the linear setting.</p>
<h3>4.1. Proof of concept on Colored MNIST</h3>
<p>The task in Colored MNIST (Arjovsky et al., 2019) is to predict whether the digit is below or above 5. Moreover, the labels are flipped with 25\% probability (except in Appendix C.2.2). Critically, the digits' colors spuriously correlate with the labels: the correlation strength varies across the two training domains $\mathcal{E}={90 \%, 80 \%}$. To test whether the model has learned to ignore the color, this correlation is reversed at test time. In brief, a biased model that only considers the color would have $10 \%$ test accuracy whereas an oracle model that perfectly predicts the shape would have $75 \%$. As previously done in V-REx (Krueger et al., 2021), we strictly follow the IRM implementation and just replace the IRM penalty by our Fishr penalty. This means that we use the exact same MLP and hyperparameters, notably the same two-stage scheduling selected in IRM for the regularization strength $\lambda$, that is low until epoch 190 and then jumps to a large value, which was optimized via a gridsearch for IRM. More experimental details are provided in Appendix C.1.</p>
<p>Table 3 reports the accuracy averaged over 10 runs with standard deviation. Fishr ${ }<em _omega="\omega">{\theta}$ (i.e., applying Fishr on all weights $\theta$ ) obtains the best trade-off between train and test accuracies; notably in test, it reaches $71.2 \%$, or $70.2 \%$ when digits are grayscale. Moreover, computing the gradients only in the classifier $w</em>$ performs almost as well ( $69.5 \%$ in test for Fishr $<em _phi="\phi">{\omega}$ ) while reducing drastically the computational cost. Finally, Fishr ${ }</em>$ only in the features extractor $\phi$ works best in test, though it has lower train accuracy. This last experiment shows that we can reduce domain shifts without</p>
<p>Table 3: Colored MNIST results. All methods use hyperparameters optimized for IRM.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Train acc.</th>
<th style="text-align: center;">Test acc.</th>
<th style="text-align: center;">Gray test acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ERM</td>
<td style="text-align: center;">$86.4 \pm 0.2$</td>
<td style="text-align: center;">$14.0 \pm 0.7$</td>
<td style="text-align: center;">$71.0 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">IRM</td>
<td style="text-align: center;">$71.0 \pm 0.5$</td>
<td style="text-align: center;">$65.6 \pm 1.8$</td>
<td style="text-align: center;">$66.1 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">V-REx</td>
<td style="text-align: center;">$71.7 \pm 1.5$</td>
<td style="text-align: center;">$67.2 \pm 1.5$</td>
<td style="text-align: center;">$68.6 \pm 2.2$</td>
</tr>
<tr>
<td style="text-align: center;">Fishr $_{\theta}$</td>
<td style="text-align: center;">$69.6 \pm 0.9$</td>
<td style="text-align: center;">$71.2 \pm 1.1$</td>
<td style="text-align: center;">$70.2 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">Fishr $_{\omega}$</td>
<td style="text-align: center;">$71.0 \pm 0.9$</td>
<td style="text-align: center;">$69.5 \pm 1.0$</td>
<td style="text-align: center;">$70.2 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: center;">Fishr $_{\phi}$</td>
<td style="text-align: center;">$65.6 \pm 1.3$</td>
<td style="text-align: center;">$73.8 \pm 1.0$</td>
<td style="text-align: center;">$70.0 \pm 0.9$</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Colored MNIST dynamics. At epoch 190, λ strongly steps up: then, the Fishrθ regularization matches the domain-level gradient variances (red) across domains E = {90%, 80%}, and consequently, the training empirical risks (dotted pink) and Hessians (purple). This reduces train accuracy (orange) but increases test accuracy (blue) as the network learns to predict the digit's shape. As shown in Fig. 7, training dynamics are different for ERM.</p>
<p>explicitly forcing the predictors to be simultaneously optimal. These results highlight the effectiveness of gradient variance matching — even with standard hyperparameters — at different layers of the network.</p>
<p>The main advantage of this synthetic dataset is the possibility of empirically validating some theoretical insights. For example, the training dynamics in Fig. 3 show that the domain-level empirical risks get closer once the Fishrθ gradient variance matching loss is activated after step 190 (|R90% − R80%| → 0), even though predicting accurately on the domain 90% is easier than on the domain 80%. This confirms insights from Section 3.2.2. Similarly, we observe that Fishr matches Hessians across the two training domains. This is confirmed by further experiments in Appendix C.2, and validates insights from Section 3.2.3. Overall, Fishr regularization reduces train accuracy, but sharply increases test accuracy. Yet, the main drawback of Colored MNIST is its insufficiency to ensure generalization for real-world datasets. Overall, it should be considered as a proof-of-concept.</p>
<h3>4.2. DomainBed benchmark</h3>
<h4>4.2.1. DATASETS AND PROCEDURE</h4>
<p>We conduct extensive experiments on the DomainBed benchmark (Gulrajani &amp; Lopez-Paz, 2021). In addition to the synthetic Colored MNIST (Arjovsky et al., 2019) and Rotated MNIST (Ghifary et al., 2015), the multi-domain image classification datasets are the real VLCS (Fang et al., 2013), PACS (Li et al., 2017), OfficeHome (Venkateswara et al., 2017), TerraIncognita (Beery et al., 2018) and DomainNet (Peng et al., 2019). To limit access to test domain, the framework enforces that all methods are trained with only 20 different configurations of hyperparameters and</p>
<p>Algorithm 1 Training procedure for Fishr on DomainBed.</p>
<p>Input: DNN fθ, observations D<sup>e</sup> = {(x<sup>i</sup><sub>e</sub>, y<sup>i</sup><sub>e</sub>)}<sup>n<sup>e</sup><sub>w</sub></sup> for domains e ∈ E, regularization weight λ, warmup iteration iwarmup, exponential moving average γ and batch size b<sup>s</sup>
Initialize: moving averages: ∀e ∈ E, v<sup>mean</sup><sub>e</sub> ← 0
for iter from 1 to #iters do
{# Step 1: standard ERM procedure}
for e ∈ E do
Randomly select batch: { (x<sup>i</sup><sub>e</sub>, y<sup>i</sup><sub>e</sub>)}<sup>i∈B</sup> of size b<sup>s</sup>
Compute predictions: ∀i ∈ B, y<sup>i</sup><sub>e</sub> ← fθ(x<sup>i</sup><sub>e</sub>)
Compute empirical risks: R<sup>e</sup>(θ) ← ∑<sub>i∈B</sub> ℓ(y<sup>i</sup><sub>e</sub>, y<sup>i</sup><sub>e</sub>)
end for
L(θ) = ∑<sub>e∈E</sub> R<sup>e</sup>(θ)
{# Step 2: gradient variances in classifier}
for e ∈ E do
Compute individual gradients in w<sup>ω</sup> with Back-PACK: ∀i ∈ B, y<sup>i</sup><sub>e</sub> ← ∇<sup>ω</sup> ℓ(y<sup>i</sup><sub>e</sub>, y<sup>i</sup><sub>e</sub>)
Compute domain gradient variances v<sup>e</sup> (Eq. 2)
Update v<sup>mean</sup><sub>e</sub> = v<sup>ω</sup> ← γv<sup>mean</sup><sub>e</sub> + (1 − γ)v<sup>iter</sup><sub>e</sub>
end for
if iter ≥ iwarmup then
L(θ) += λL<sup>Fishr</sup>(θ) (Eq. 3)
end if
{# Step 3: gradient descent in the whole network}
Backpropagate gradients ∇<sup>θ</sup>L(θ) in the network fθ with standard PyTorch
end for</p>
<p>for the same number of steps. Results are averaged over three trials. This experimental setup is further described in Appendix D.1. By imposing the datasets, the training procedure and controlling the hyperparameter search, DomainBed is arguably the fairer open-source benchmark to rigorously compare the different strategies for OOD generalization.</p>
<h4>4.2.2. IMPLEMENTATION DETAILS</h4>
<p>We systematically apply Fishr only in the classifier w<sup>ω</sup> in DomainBed. Indeed, keeping individual gradients in memory for φ from a ResNet-50 was impossible for computational reasons. Fishrθ and Fishr<sup>ω</sup> performed similarly in previous Section 4.1. This is partly because the gradients in ω still depend on Φ<sup>φ</sup>. Additionally, as highlighted in Appendix D.3.2, this relaxation may improve results for real-world datasets. Indeed, while Colored MNIST is a correlation shift challenge, the other datasets mostly demonstrate diversity shifts where "each domain represents a certain spectrum of diversity in data" (Ye et al., 2021). Then, as the pixels distribution are quite different across domains, low-level layers may need to adapt to these domain-dependent peculiarities. Moreover, if we used all weights θ = (φ, ω) to compute gradient variances, the invariance in w<sup>ω</sup> may be</p>
<p>Table 4: DomainBed benchmark. We format first, second and worse than ERM results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Algorithm</th>
<th style="text-align: center;">Accuracy ( $\uparrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Ranking ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CMNIST</td>
<td style="text-align: center;">RMNIST</td>
<td style="text-align: center;">VLCS</td>
<td style="text-align: center;">PACS</td>
<td style="text-align: center;">OfficeHome</td>
<td style="text-align: center;">TerraInc</td>
<td style="text-align: center;">DomainNet</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Arith. <br> mean</td>
<td style="text-align: center;">Geom. <br> mean</td>
<td style="text-align: center;">Median</td>
</tr>
<tr>
<td style="text-align: center;">ERM</td>
<td style="text-align: center;">57.8 $\pm 0.2$</td>
<td style="text-align: center;">97.8 $\pm 0.1$</td>
<td style="text-align: center;">77.6 $\pm 0.3$</td>
<td style="text-align: center;">86.7 $\pm 0.3$</td>
<td style="text-align: center;">66.4 $\pm 0.5$</td>
<td style="text-align: center;">53.0 $\pm 0.3$</td>
<td style="text-align: center;">41.3 $\pm 0.1$</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">IRM</td>
<td style="text-align: center;">67.7 $\pm 1.2$</td>
<td style="text-align: center;">97.5 $\pm 0.2$</td>
<td style="text-align: center;">76.9 $\pm 0.6$</td>
<td style="text-align: center;">84.5 $\pm 1.1$</td>
<td style="text-align: center;">63.0 $\pm 2.7$</td>
<td style="text-align: center;">50.5 $\pm 0.7$</td>
<td style="text-align: center;">28.0 $\pm 5.1$</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">GroupDRO</td>
<td style="text-align: center;">61.1 $\pm 0.9$</td>
<td style="text-align: center;">97.9 $\pm 0.1$</td>
<td style="text-align: center;">77.4 $\pm 0.5$</td>
<td style="text-align: center;">87.1 $\pm 0.1$</td>
<td style="text-align: center;">66.2 $\pm 0.6$</td>
<td style="text-align: center;">52.4 $\pm 0.1$</td>
<td style="text-align: center;">33.4 $\pm 0.3$</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Mixup</td>
<td style="text-align: center;">58.4 $\pm 0.2$</td>
<td style="text-align: center;">98.0 $\pm 0.1$</td>
<td style="text-align: center;">78.1 $\pm 0.3$</td>
<td style="text-align: center;">86.8 $\pm 0.3$</td>
<td style="text-align: center;">68.0 $\pm 0.2$</td>
<td style="text-align: center;">54.4 $\pm 0.3$</td>
<td style="text-align: center;">39.6 $\pm 0.1$</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">MLDG</td>
<td style="text-align: center;">58.2 $\pm 0.4$</td>
<td style="text-align: center;">97.8 $\pm 0.1$</td>
<td style="text-align: center;">77.5 $\pm 0.1$</td>
<td style="text-align: center;">86.8 $\pm 0.4$</td>
<td style="text-align: center;">66.6 $\pm 0.3$</td>
<td style="text-align: center;">52.0 $\pm 0.1$</td>
<td style="text-align: center;">41.6 $\pm 0.1$</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: center;">CORAL</td>
<td style="text-align: center;">58.6 $\pm 0.5$</td>
<td style="text-align: center;">98.0 $\pm 0.0$</td>
<td style="text-align: center;">77.7 $\pm 0.2$</td>
<td style="text-align: center;">87.1 $\pm 0.5$</td>
<td style="text-align: center;">68.4 $\pm 0.2$</td>
<td style="text-align: center;">52.8 $\pm 0.2$</td>
<td style="text-align: center;">41.8 $\pm 0.1$</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">MMD</td>
<td style="text-align: center;">63.3 $\pm 1.3$</td>
<td style="text-align: center;">98.0 $\pm 0.1$</td>
<td style="text-align: center;">77.9 $\pm 0.1$</td>
<td style="text-align: center;">87.2 $\pm 0.1$</td>
<td style="text-align: center;">66.2 $\pm 0.3$</td>
<td style="text-align: center;">52.0 $\pm 0.4$</td>
<td style="text-align: center;">23.5 $\pm 9.4$</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">DANN</td>
<td style="text-align: center;">57.0 $\pm 1.0$</td>
<td style="text-align: center;">97.9 $\pm 0.1$</td>
<td style="text-align: center;">79.7 $\pm 0.5$</td>
<td style="text-align: center;">85.2 $\pm 0.2$</td>
<td style="text-align: center;">65.3 $\pm 0.8$</td>
<td style="text-align: center;">50.6 $\pm 0.4$</td>
<td style="text-align: center;">38.3 $\pm 0.1$</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: center;">CDANN</td>
<td style="text-align: center;">59.5 $\pm 2.0$</td>
<td style="text-align: center;">97.9 $\pm 0.0$</td>
<td style="text-align: center;">79.9 $\pm 0.2$</td>
<td style="text-align: center;">85.8 $\pm 0.8$</td>
<td style="text-align: center;">65.3 $\pm 0.5$</td>
<td style="text-align: center;">50.8 $\pm 0.6$</td>
<td style="text-align: center;">38.5 $\pm 0.2$</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">MTL</td>
<td style="text-align: center;">57.6 $\pm 0.3$</td>
<td style="text-align: center;">97.9 $\pm 0.1$</td>
<td style="text-align: center;">77.7 $\pm 0.5$</td>
<td style="text-align: center;">86.7 $\pm 0.2$</td>
<td style="text-align: center;">66.5 $\pm 0.4$</td>
<td style="text-align: center;">52.2 $\pm 0.4$</td>
<td style="text-align: center;">40.8 $\pm 0.1$</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">SagNet</td>
<td style="text-align: center;">58.2 $\pm 0.3$</td>
<td style="text-align: center;">97.9 $\pm 0.0$</td>
<td style="text-align: center;">77.6 $\pm 0.1$</td>
<td style="text-align: center;">86.4 $\pm 0.4$</td>
<td style="text-align: center;">67.5 $\pm 0.2$</td>
<td style="text-align: center;">52.5 $\pm 0.4$</td>
<td style="text-align: center;">40.8 $\pm 0.2$</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">ARM</td>
<td style="text-align: center;">63.2 $\pm 0.7$</td>
<td style="text-align: center;">98.1 $\pm 0.1$</td>
<td style="text-align: center;">77.8 $\pm 0.3$</td>
<td style="text-align: center;">85.8 $\pm 0.2$</td>
<td style="text-align: center;">64.8 $\pm 0.4$</td>
<td style="text-align: center;">51.2 $\pm 0.5$</td>
<td style="text-align: center;">36.0 $\pm 0.2$</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">V-REx</td>
<td style="text-align: center;">67.0 $\pm 1.3$</td>
<td style="text-align: center;">97.9 $\pm 0.1$</td>
<td style="text-align: center;">78.1 $\pm 0.2$</td>
<td style="text-align: center;">87.2 $\pm 0.6$</td>
<td style="text-align: center;">65.7 $\pm 0.3$</td>
<td style="text-align: center;">51.4 $\pm 0.5$</td>
<td style="text-align: center;">30.1 $\pm 3.7$</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">RSC</td>
<td style="text-align: center;">58.5 $\pm 0.5$</td>
<td style="text-align: center;">97.6 $\pm 0.1$</td>
<td style="text-align: center;">77.8 $\pm 0.6$</td>
<td style="text-align: center;">86.2 $\pm 0.5$</td>
<td style="text-align: center;">66.5 $\pm 0.6$</td>
<td style="text-align: center;">52.1 $\pm 0.2$</td>
<td style="text-align: center;">38.9 $\pm 0.6$</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: center;">AND-mask</td>
<td style="text-align: center;">58.6 $\pm 0.4$</td>
<td style="text-align: center;">97.5 $\pm 0.0$</td>
<td style="text-align: center;">76.4 $\pm 0.4$</td>
<td style="text-align: center;">86.4 $\pm 0.4$</td>
<td style="text-align: center;">66.1 $\pm 0.2$</td>
<td style="text-align: center;">49.8 $\pm 0.4$</td>
<td style="text-align: center;">37.9 $\pm 0.6$</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">SAND-mask</td>
<td style="text-align: center;">62.3 $\pm 1.0$</td>
<td style="text-align: center;">97.4 $\pm 0.1$</td>
<td style="text-align: center;">76.2 $\pm 0.5$</td>
<td style="text-align: center;">85.9 $\pm 0.4$</td>
<td style="text-align: center;">65.9 $\pm 0.5$</td>
<td style="text-align: center;">50.2 $\pm 0.1$</td>
<td style="text-align: center;">32.2 $\pm 0.6$</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: center;">Fish</td>
<td style="text-align: center;">61.8 $\pm 0.8$</td>
<td style="text-align: center;">97.9 $\pm 0.1$</td>
<td style="text-align: center;">77.8 $\pm 0.6$</td>
<td style="text-align: center;">85.8 $\pm 0.6$</td>
<td style="text-align: center;">66.0 $\pm 2.9$</td>
<td style="text-align: center;">50.8 $\pm 0.4$</td>
<td style="text-align: center;">43.4 $\pm 0.3$</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">Fishr</td>
<td style="text-align: center;">68.8 $\pm 1.4$</td>
<td style="text-align: center;">97.8 $\pm 0.1$</td>
<td style="text-align: center;">78.2 $\pm 0.2$</td>
<td style="text-align: center;">86.9 $\pm 0.2$</td>
<td style="text-align: center;">68.2 $\pm 0.2$</td>
<td style="text-align: center;">53.6 $\pm 0.4$</td>
<td style="text-align: center;">41.8 $\pm 0.2$</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
<p>overshadowed by $\Phi_{\phi}$ due to $|\omega| \ll|\phi|$. Finally, it's worth noting that this last-layer approximation is consistent with the IRM condition (Arjovsky et al., 2019) and is common for unsupervised domain adaptation (Ganin et al., 2016).</p>
<p>Fishr relies on three hyperparameters. First, the $\lambda$ coefficient controls the regularization strength: with $\lambda=0$ we recover ERM while a high $\lambda$ may cause underfitting. We show that Fishr is robust to the choice of the sampling distribution for hyperparameter $\lambda$ in Appendix D.3.3. Second the warmup iteration defines the step at which we activate the regularization. This warmup strategy is taken from previous works such as IRM (Arjovsky et al., 2019), V-REx (Krueger et al., 2021) or Spectral Decoupling (Pezeshki et al., 2021). Before that step, the DNN is trained with ERM to learn predictive features. After that step, the Fishr regularization encourages the DNN to have invariant gradient variances. Lastly, the domain-level gradient variances are more accurate when estimated over more data points. Rather than increasing the batch size, we follow Le Roux et al. (2011) and leverage an exponential moving average for computing stable gradient variances. Therefore our third hyperparameter is the coefficient $\gamma$ controlling the update speed: at step $t$, we match $\bar{v}<em e="e">{e}^{t}=\gamma \bar{v}</em>}^{t-1}+(1-\gamma) \boldsymbol{v<em e="e">{e}^{t}$ rather than of $\boldsymbol{v}</em>| \times|\omega|)$. We study by ablation the importance of this warmup strategy and this $\gamma$ in Appendices D.3.1 and D.3.2.}^{t}$ from Eq. 2. The closer $\gamma$ is to 1 , the smoother the variance is along training. $\bar{v}_{e}^{t-1}$ from previous step $t-1$ is 'detached' from the computational graph. Similar strategies have already been used for OOD generalization (Nam et al., 2020; Blanchard et al., 2021). The memory overhead is $(|\mathcal{E</p>
<p>Fishr is simple to implement (see the Algorithm 1) using the</p>
<p>BackPACK (Dangel et al., 2020) package. While PyTorch (Paszke et al., 2019) can compute efficiently batch gradients, BackPACK optimizes the computation of individual gradients, sample per sample, at almost no time overhead. Thus, Fishr is also at low computational costs. For example, on PACS ( 7 classes and $|\omega|=14,343$ ) with a ResNet-50 and batch size 32, Fishr induces an overhead in memory of $+0.2 \%$ and in training time of $+2.7 \%$ (with a Tesla V100) compared to ERM; on the larger-scale DomainNet (345 classes and $|\omega|=706,905$ ), the overhead is $+7.0 \%$ in memory and $+6.5 \%$ in training time. As a side note, keeping the full covariance of size $|\omega|^{2} \approx 5 \times 10^{8}$ on DomainNet would not have been possible. In contrast, Fish (Shi et al., 2021) leverages a meta-learning algorithm that is impractical as $|\mathcal{E}|$ times longer to train than ERM.</p>
<h3>4.2.3. ReSults</h3>
<p>Table 4 summarizes the results on DomainBed using the 'Test-domain' model selection: the validation set (to select the best hyperparameters) follows the same distribution as the test domain. Appendix D. 2 reports results with the 'Training-domain' model selection while results are detailed per dataset in Appendix D.4.</p>
<p>ERM was carefully tuned in DomainBed and thus remains a strong baseline. Moreover, all previous methods are far from the best score on at least one dataset. Invariant predictors (IRM, V-REx) and gradient masking (AND-mask) approaches perform poorly on real datasets. Additionally, CORAL not only performs worse than ERM on TerraIncognita, but most importantly fails to detect correlation shifts on Colored MNIST: this is because feature-based approaches</p>
<p>do not take into account the label, as previously stated in Section 3.2.2.</p>
<p>Contrarily, Fishr is the only method to efficiently tackle correlation and diversity shifts, as defined in (Ye et al., 2021). Indeed, not only Fishr outperforms ERM on Colored MNIST ( $68.8 \%$ vs. $57.8 \%$ ), but Fishr also systematically performs better than ERM on all real datasets: the differences are over standard errors on VLCS ( $78.2 \%$ vs. $77.6 \%$ ), OfficeHome ( $68.2 \%$ vs. $66.4 \%$ ) and on the larger-scale DomainNet ( $41.8 \%$ vs. $41.3 \%$ ). Appendix D.3.2 shows that Fishr performs even better when combined with gradient-mean matching. In summary, Fishr consistently beats ERM (despite the restricted hyperparameter search): this is the main point to validate the effectiveness of our method.</p>
<p>Additionally, Fishr performs best after averaging: Firshr reaches $70.8 \%$ vs. $69.2 \%$ for the second best CORAL. When ignoring the Colored MNIST task, averaging over the 6 other datasets leads to a similar ranking: 1.Fishr(avg=71.1), 2.CORAL(71.0), 3.Mixup(70.8) and 4.ERM(70.5). This arguably partial metric is confirmed by the more robust ranking information; Fishr's median ranking of second reflects that Fishr is consistently among the best methods. Overall, Fishr is the state-of-the-art approach, not only in average accuracy, but most importantly in average ranking.</p>
<h2>5. Conclusion</h2>
<p>In this paper, we addressed the task of out-of-distribution generalization for classification in computer vision. We derive a new and simple regularization - Fishr - that matches the gradient variances across domains as a proxy for matching domain-level risks and Hessians. We prove that this reduces inconsistencies across domains. Fishr reaches state-of-the-art performances on DomainBed when samples from the test domain are available for model selection. Our experiments - reproducible with our open-source implementation - suggest that Fishr would consistently improve a deep classifier for real-world usages when dealing with data from multiple domains. We hope to pave the way towards new gradient-based regularization to improve the generalization abilities of deep neural networks.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work was granted access to the HPC resources of IDRIS under the allocation A0100612449 made by GENCI. We acknowledge the financial support by the ANR agency in the chair VISA-DEEP (ANR-20-CHIA-0022-01).</p>
<h2>References</h2>
<p>Ahmed, F., Bengio, Y., van Seijen, H., and Courville, A. Systematic generalisation with group invariant predictions. In $I C L R, 2021 .(\mathrm{p} .3)$.</p>
<p>Ahuja, K., Caballero, E., Zhang, D., Bengio, Y., Mitliagkas, I., and Rish, I. Invariance principle meets information bottleneck for out-of-distribution generalization. In NeurIPS, 2019. (pp. 3, 18).</p>
<p>Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. Invariant risk minimization. arXiv preprint, 2019. (pp. 1, $2,3,6,7,8,16,19,20,23)$.</p>
<p>Becker, S. and Le Cun, Y. Improving the convergence of back-propagation learning with second order methods. In Connectionist models summer school, 1988. (p. 6).</p>
<p>Beery, S., Van Horn, G., and Perona, P. Recognition in terra incognita. In ECCV, 2018. (pp. 7, 20).</p>
<p>Blanchard, G., Lee, G., and Scott, C. Generalizing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011. (p. 1).</p>
<p>Blanchard, G., Deshmukh, A. A., Dogan, U., Lee, G., and Scott, C. Domain generalization by marginal transfer learning. JMLR, 2021. (pp. 8, 19, 21).</p>
<p>Cha, J., Chun, S., Lee, K., Cho, H.-C., Park, S., Lee, Y., and Park, S. SWAD: Domain generalization by seeking flat minima. In NeurIPS, 2021. (p. 20).</p>
<p>Chang, S., Zhang, Y., Yu, M., and Jaakkola, T. Invariant rationalization. In ICML, 2020. (p. 3).</p>
<p>Charpiat, G., Girard, N., Felardos, L., and Tarabalka, Y. Input similarity from the neural network perspective. In NeurIPS, 2019. (p. 2).
C.R., R. Information and accuracy attainable in the estimation of statistical parameters. In Bulletin of the Calcutta Mathematical Society, 1945. (p. 5).</p>
<p>Creager, E., Jacobsen, J.-H., and Zemel, R. Environment inference for invariant learning. In ICML, 2021. (p. 2).</p>
<p>D'Amour, A., Heller, K., Moldovan, D., Adlam, B., Alipanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein, J., Hoffman, M. D., et al. Underspecification presents challenges for credibility in modern machine learning. $J M L R, 2020 .(\mathrm{p} .21)$.</p>
<p>Dangel, F., Kunstner, F., and Hennig, P. BackPACK: Packing more into backprop. In $I C L R, 2020$. (pp. 2, 8, 17).</p>
<p>Dangel, F., Tatzel, L., and Hennig, P. Vivit: Curvature access through the generalized gauss-newton's low-rank structure. arXiv preprint, 2021. (p. 5).</p>
<p>DeGrave, A. J., Janizek, J. D., and Lee, S.-I. Ai for radiographic covid-19 detection selects shortcuts over signal. Nature Machine Intelligence, 2021. (p. 1).</p>
<p>Deutsch, D. The beginning of infinity: Explanations that transform the world. Penguin UK, 2011. (p. 4).</p>
<p>Ding, Z. and Fu, Y. Deep domain generalization with structured low-rank constraint. In TIP, 2017. (p. 2).</p>
<p>Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. Sharp minima can generalize for deep nets. In ICML, 2017. (p. 17).</p>
<p>Du, Y., Czarnecki, W. M., Jayakumar, S. M., Farajtabar, M., Pascanu, R., and Lakshminarayanan, B. Adapting auxiliary losses using gradient similarity. arXiv preprint, 2018. (p. 3).</p>
<p>Faghri, F., Duvenaud, D., Fleet, D. J., and Ba, J. A study of gradient variance in deep learning. arXiv preprint, 2020. (p. 6).</p>
<p>Fang, C., Xu, Y., and Rockmore, D. N. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. (pp. 7, 20).</p>
<p>Finn, C., Abbeel, P., and Levine, S. Model-agnostic metalearning for fast adaptation of deep networks. In ICML, 2017. (p. 3).</p>
<p>Fisher, R. A. On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society of London., 1922. (pp. 2, 5).</p>
<p>Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B. Sharpness-aware minimization for efficiently improving generalization. In $I C L R, 2021$. (p. 5).</p>
<p>Fort, S., Nowak, P. K., Jastrzebski, S., and Narayanan, S. Stiffness: A new perspective on generalization in neural networks. arXiv preprint, 2019. (p. 2).</p>
<p>Frantar, E., Kurtic, E., and Alistarh, D. Efficient matrixfree approximations of second-order information, with applications to pruning and optimization. arXiv preprint, 2021. (p. 5).</p>
<p>Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V. Domain-adversarial training of neural networks. $J M L R$, 2016. (pp. 1, 2, 8, 19).</p>
<p>Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2020. (p. 1).</p>
<p>Ghifary, M., Kleijn, W. B., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multitask autoencoders. In ICCV, 2015. (pp. 7, 16, 20).</p>
<p>Ghorbani, B., Krishnan, S., and Xiao, Y. An investigation into neural net optimization via hessian eigenvalue density. In ICML, 2019. (p. 4).</p>
<p>Gong, M., Zhang, K., Liu, T., Tao, D., Glymour, C., and Schölkopf, B. Domain adaptation with conditional transferable components. In ICML, 2016. (p. 2).</p>
<p>Gulrajani, I. and Lopez-Paz, D. In search of lost domain generalization. In $I C L R, 2021$. (pp. 1, 2, 3, 6, 7, 19).</p>
<p>Guo, R., Zhang, P., Liu, H., and Kiciman, E. Out-ofdistribution prediction with invariant risk minimization: The limitation and an effective fix. arXiv preprint, 2021. (p. 3).</p>
<p>Gur-Ari, G., Roberts, D. A., and Dyer, E. Gradient descent happens in a tiny subspace. arXiv preprint, 2018. (p. 16).</p>
<p>Heskes, T. On "natural" learning and pruning in multilayered perceptrons. Neural Computation, 2000. (p. 18).</p>
<p>Huang, Z., Wang, H., Xing, E. P., and Huang, D. Selfchallenging improves cross-domain generalization. In ECCV, 2020. (p. 20).</p>
<p>Idnani, D. and Kao, J. C. Learning robust representations with score invariant learning. In ICML UDL Workshop, 2020. (p. 3).</p>
<p>Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A. Averaging weights leads to wider optima and better generalization. In UAI, 2018. (p. 5).</p>
<p>Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In NeurIPS, 2018. (pp. 6, 15).</p>
<p>Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Storkey, A., and Bengio, Y. Three factors influencing minima in SGD. In ICANN, 2018. (pp. 5, 14).</p>
<p>Jastrzebski, S., Arpit, D., Astrand, O., Kerg, G. B., Wang, H., Xiong, C., Socher, R., Cho, K., and Geras, K. J. Catastrophic fisher explosion: Early phase fisher matrix impacts generalization. In ICML, 2021. (p. 6).</p>
<p>Johansson, F. D., Sontag, D., and Ranganath, R. Support and invertibility in domain-invariant representations. In AISTATS, 2019. (pp. 3, 5).</p>
<p>Kamath, P., Tangella, A., Sutherland, D., and Srebro, N. Does invariant risk minimization capture invariance? In AISTATS, 2021. (p. 3).</p>
<p>Karakida, R., Akaho, S., and Amari, S.-i. Pathological spectra of the fisher information metric and its variants in deep neural networks. arXiv preprint, 2019. (p. 16).</p>
<p>Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint, 2014. (pp. 6, 17, 19).</p>
<p>Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. In PNAS, 2017. (p. 6).</p>
<p>Koh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R. L., Gao, I., et al. Wilds: A benchmark of in-the-wild distribution shifts. arXiv preprint, 2020. (pp. 2, 3).</p>
<p>Kopitkov, D. and Indelman, V. Neural spectrum alignment: Empirical study. arXiv preprint, 2019. (p. 16).</p>
<p>Koyama, M. and Yamaguchi, S. Out-of-distribution generalization with maximal invariant predictor. arXiv preprint, 2020. (pp. 3, 19, 20, 22).</p>
<p>Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In NeurIPS, 2012. (p. 1).</p>
<p>Krueger, D., Caballero, E., Jacobsen, J.-H., Zhang, A., Binas, J., Zhang, D., Priol, R. L., and Courville, A. Out-ofdistribution generalization via risk extrapolation (rex). In ICML, 2021. (pp. 1, 3, 4, 5, 6, 8, 15, 20, 21).</p>
<p>Kunstner, F., Hennig, P., and Balles, L. Limitations of the empirical fisher approximation for natural gradient descent. In NeurIPS, 2019. (p. 6).</p>
<p>Le Roux, N., Bengio, Y., and Fitzgibbon, A. Improving first and second-order methods by modeling uncertainty. Optimization for Machine Learning, 2011. (pp. 8, 21, 24).</p>
<p>LeCun, Y., Denker, J., Solla, S., Howard, R., and Jackel, L. Optimal brain damage. In NeurIPS, 1990. (p. 6).</p>
<p>LeCun, Y., Cortes, C., and Burges, C. Mnist handwritten digit database, 2010. (pp. 16, 20).</p>
<p>LeCun, Y., Bottou, L., Orr, G. B., and Müller, K.-R. Efficient backprop. In Neural Networks. 2012. (p. 6).</p>
<p>Li, D., Yang, Y., Song, Y.-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In ICCV, 2017. (pp. 7, 20).</p>
<p>Li, D., Yang, Y., Song, Y.-Z., and Hospedales, T. Learning to generalize: Meta-learning for domain generalization. In AAAI, 2018a. (p. 19).</p>
<p>Li, H., Pan, S. J., Wang, S., and Kot, A. C. Domain generalization with adversarial feature learning. In CVPR, 2018b. (pp. 2, 19).</p>
<p>Li, X., Gu, Q., Zhou, Y., Chen, T., and Banerjee, A. Hessian based analysis of sgd for deep nets: Dynamics and generalization. In SIAM, 2020. (pp. 5, 17).</p>
<p>Li, Y., Gong, M., Tian, X., Liu, T., and Tao, D. Domain generalization via conditional invariant representations. In AAAI, 2018c. (pp. 3, 19).</p>
<p>Liu, L., Zhang, S., Kuang, Z., Zhou, A., Xue, J.-H., Wang, X., Chen, Y., Yang, W., Liao, Q., and Zhang, W. Group fisher pruning for practical network compression. In ICML, 2021. (p. 5).</p>
<p>Long, M., Wang, J., Ding, G., Sun, J., and Yu, P. S. Transfer joint matching for unsupervised domain adaptation. In CVPR, 2014. (p. 3).</p>
<p>Lopez-Paz, D. and Ranzato, M. A. Gradient episodic memory for continual learning. In NeurIPS, 2017. (p. 3).</p>
<p>Maddox, W. J., Tang, S., Moreno, P. G., Wilson, A. G., and Damianou, A. On transfer learning via linearized neural networks. In NeurIPS workshop, 2019. (p. 16).</p>
<p>Mancini, M., Bulo, S. R., Caputo, B., and Ricci, E. Best sources forward: domain generalization through sourcespecific nets. In ICIP, 2018. (p. 2).</p>
<p>Mansilla, L., Echeveste, R., Milone, D. H., and Ferrante, E. Domain generalization via gradient surgery. In ICCV, 2021. (p. 3).</p>
<p>Martens, J. New insights and perspectives on the natural gradient method. arXiv preprint, 2014. (pp. 5, 6).</p>
<p>Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In ICML, 2015. (p. 18).</p>
<p>Muandet, K., Balduzzi, D., and Schölkopf, B. Domain generalization via invariant feature representation. In ICML, 2013. (pp. 1, 3).</p>
<p>Nam, H., Lee, H., Park, J., Yoon, W., and Yoo, D. Reducing domain gap by reducing style bias. In CVPR, 2021. (p. 20).</p>
<p>Nam, J., Cha, H., Ahn, S., Lee, J., and Shin, J. Learning from failure: De-biasing classifier from biased classifier. In NeurIPS, 2020. (pp. 8, 21).</p>
<p>Parascandolo, G., Neitz, A., Orvieto, A., Gresele, L., and Schölkopf, B. Learning explanations that are hard to vary. In $I C L R$, 2021. (pp. 2, 3, 4, 5, 6, 20, 24).</p>
<p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. (p. 8).</p>
<p>Pearl, J. Causality. Cambridge university press, 2009. (p. 3).</p>
<p>Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B. Moment matching for multi-source domain adaptation. In ICCV, 2019. (pp. 7, 20).</p>
<p>Peters, J., Bühlmann, P., and Meinshausen, N. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society, 2016. (pp. 1, 3).</p>
<p>Pezeshki, M., Kaba, S.-O., Bengio, Y., Courville, A., Precup, D., and Lajoie, G. Gradient starvation: A learning proclivity in neural networks. In NeurIPS, 2021. (pp. 8, 16).</p>
<p>Rame, A., Kirchmeyer, M., Rahier, T., Rakotomamonjy, A., Gallinari, P., and Cord, M. Diverse weight averaging for out-of-distribution generalization. arXiv preprint, 2022. (p. 20).</p>
<p>Roberts, M., Driggs, D., Thorpe, M., Gilbey, J., Yeung, M., Ursprung, S., Aviles-Rivero, A. I., Etmann, C., McCague, C., Beer, L., et al. Common pitfalls and recommendations for using machine learning to detect and prognosticate for covid-19 using chest radiographs and ct scans. Nature Machine Intelligence, 2021. (p. 1).</p>
<p>Rojas-Carulla, M., Schölkopf, B., Turner, R., and Peters, J. Invariant models for causal transfer learning. $J M L R$, 2018. (p. 3).</p>
<p>Rosenfeld, E., Ravikumar, P. K., and Risteski, A. The risks of invariant risk minimization. In $I C L R, 2021$. (p. 3).</p>
<p>Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks. In $I C L R, 2020$ a. (pp. 2, 19).</p>
<p>Sagawa, S., Raghunathan, A., Koh, P. W., and Liang, P. An investigation of why overparameterization exacerbates spurious correlations. In ICML, 2020b. (p. 2).</p>
<p>Sagun, L., Evci, U., Guney, V. U., Dauphin, Y., and Bottou, L. Empirical analysis of the hessian of over-parametrized neural networks, 2018. (p. 4).</p>
<p>Sankararaman, K. A., De, S., Xu, Z., Huang, W. R., and Goldstein, T. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In ICML, 2020. (pp. 2, 3).</p>
<p>Schaul, T., Zhang, S., and LeCun, Y. No more pesky learning rates. In ICML, 2013. (p. 14).</p>
<p>Schraudolph, N. N. Fast curvature matrix-vector products for second-order gradient descent. In Neural computation, 2002. (p. 5).</p>
<p>Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netrapalli, P. The pitfalls of simplicity bias in neural networks. In NeurIPS, 2020. (p. 1).</p>
<p>Shahtalebi, S., Gagnon-Audet, J.-C., Laleh, T., Faramarzi, M., Ahuja, K., and Rish, I. Sand-mask: An enhanced gradient masking strategy for the discovery of invariances in domain generalization. In ICML UDL Workshop, 2021. (pp. 3, 4, 19, 20, 24).</p>
<p>Shi, Y., Seely, J., Torr, P. H., Siddharth, N., Hannun, A., Usunier, N., and Synnaeve, G. Gradient matching for domain generalization. arXiv preprint, 2021. (pp. 2, 3, 6, $8,16,20,22,24)$.</p>
<p>Singh, S. P. and Alistarh, D. Woodfisher: Efficient secondorder approximation for neural network compression. In NeurIPS, 2020. (pp. 5, 17).</p>
<p>Sun, B. and Saenko, K. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016. (pp. 1, 3, 19).</p>
<p>Sun, B., Feng, J., and Saenko, K. Return of frustratingly easy domain adaptation. In AAAI, 2016. (pp. 1, 3).</p>
<p>Tenenbaum, J. Building machines that learn and think like people. In AAMAS, 2018. (p. 1).</p>
<p>Teney, D., Abbasnejad, E., and van den Hengel, A. Unshuffling data for improved generalization. arXiv preprint, 2020. (p. 3).</p>
<p>Teney, D., Abbasnejad, E., Lucey, S., and van den Hengel, A. Evading the simplicity bias: Training a diverse set of models discovers solutions with superior OOD generalization. arXiv preprint, 2021. (p. 21).</p>
<p>Theis, L., Korshunova, I., Tejani, A., and Huszár, F. Faster gaze prediction with dense networks and fisher pruning. arXiv preprint, 2018. (p. 6).</p>
<p>Thomas, V., Pedregosa, F., van Merriënboer, B., Manzagol, P.-A., Bengio, Y., and Roux, N. L. On the interplay between noise and curvature and its effect on optimization and generalization. In AISTATS, 2020. (pp. 5, 6, 17).</p>
<p>Turner, R., Eriksson, D., McCourt, M., Kiili, J., Laaksonen, E., Xu, Z., and Guyon, I. Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020. In NeurIPS, 2021. (p. 24).</p>
<p>Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., and Darrell, T. Deep domain confusion: Maximizing for domain invariance. In CoRR, 2014. (p. 3).</p>
<p>Valle-Perez, G., Camargo, C. Q., and Louis, A. A. Deep learning generalizes because the parameter-function map is biased towards simple functions. In ICLR, 2019. (p. 1).</p>
<p>Vapnik, V. N. An overview of statistical learning theory. In TNN, 1999. (pp. 2, 19).</p>
<p>Venkateswara, H., Eusebio, J., Chakraborty, S., and Panchanathan, S. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. (pp. 7, 20).</p>
<p>Wald, Y., Feder, A., Greenfeld, D., and Shalit, U. On calibration and out-of-domain generalization. In NeurIPS, 2021. (p. 21).</p>
<p>Wang, Y., Li, H., and Kot, A. C. Heterogeneous domain generalization via domain mixup. In ICASSP, 2020. (p. 2).</p>
<p>Wu, Y., Inkpen, D., and El-Roby, A. Dual mixup regularized learning for adversarial domain adaptation. In ECCV, 2020. (p. 2).</p>
<p>Xie, S. M., Kumar, A., Jones, R., Khani, F., Ma, T., and Liang, P. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In ICLR, 2021. (p. 2).</p>
<p>Yan, S., Song, H., Li, N., Zou, L., and Ren, L. Improve unsupervised domain adaptation with mixup training. arXiv preprint, 2020. (p. 19).</p>
<p>Yang, G. and Salman, H. A fine-grained spectral perspective on neural networks. arXiv preprint, 2019. (p. 16).</p>
<p>Ye, N., Li, K., Hong, L., Bai, H., Chen, Y., Zhou, F., and Li, Z. Ood-bench: Benchmarking and understanding out-ofdistribution generalization datasets and algorithms. arXiv preprint, 2021. (pp. 1, 2, 7, 9).</p>
<p>Yin, D., Pananjady, A., Lam, M., Papailiopoulos, D., Ramchandran, K., and Bartlett, P. Gradient diversity: a key ingredient for scalable distributed learning. In AISTATS, 2018. (pp. 2, 3).</p>
<p>Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. Gradient surgery for multi-task learning. In NeurIPS, 2020. (p. 3).</p>
<p>Zhang, M., Marklund, H., Dhawan, N., Gupta, A., Levine, S., and Finn, C. Adaptive risk minimization: A metalearning approach for tackling group distribution shift. arXiv preprint, 2020. (pp. 3, 20, 21).</p>
<p>Zhang, X., Cui, P., Xu, R., Zhou, L., He, Y., and Shen, Z. Deep stable learning for out-of-distribution generalization. In CVPR, 2021. (pp. 2, 21).</p>
<p>Zhang, Y., Yu, W., and Turk, G. Learning novel policies for tasks. In ICML, 2019. (p. 3).</p>
<p>Zhao, H., Des Combes, R. T., Zhang, K., and Gordon, G. On learning invariant representations for domain adaptation. In ICML, 2019. (pp. 3, 5).</p>
<p>These Appendices complement the main paper.</p>
<ol>
<li>We first detail some theoretical points. Appendix A. 1 demonstrates our Proposition 1. Appendix A. 2 shows that Fishr acts as a feature-adaptive V-REx. Appendix A. 3 motivates Fishr with intuitions from the Neural Tangent Kernel theory.</li>
<li>Appendix B proves the effectiveness of our approach for a linear toy dataset.</li>
<li>Appendix C enriches the Colored MNIST experiment in the IRM setup. In detail, we first describe the experimental setup in Appendix C.1. We then validate in Appendix C. 2 some insights provided in the main paper; in particular, Appendix C.2.3 motivates the diagonal approximation of the gradient covariance.</li>
<li>Appendix D enriches the DomainBed experiments. After a description of the benchmark protocols in Appendix D.1, Appendix D. 2 discusses the model selection strategy. Then Appendix D. 3 provides additional experiments to analyze key components of Fishr. Specifically, D.3.1 analyzes the exponential moving average; D.3.2 compares gradient mean versus gradient variance matching and also motivates ignoring the gradients in the features extractor; D.3.3 discusses the methodology to select hyperparameter distributions. Finally, Appendix D. 4 provides the per-dataset results.</li>
</ol>
<h1>A. Additional Theoretical Analysis</h1>
<h2>A.1. Demonstration of Proposition 1 from Section 3.2.1</h2>
<p>Assumption A.1. We make the quadratic bowl assumption around the local minima $\theta^{*}$ on all domains : $\forall e \in \mathcal{E}$,</p>
<p>$$
\mathcal{R}<em e="e">{e}(\theta)=\mathcal{R}</em>\left(\theta^{<em>}\right)+\frac{1}{2}\left(\theta-\theta^{</em>}\right)^{\top} H_{e}\left(\theta-\theta^{*}\right)
$$</p>
<p>where $H_{e}$ is positive definite of eigenvalues $\lambda_{1}^{e} \geq \cdots \geq \lambda_{h}^{e}&gt;0$.
Remark A.2. Assumption A. 1 is milder on $N_{e, \theta^{<em>}}^{\epsilon}$ for low $\epsilon$. Indeed, when $\epsilon \rightarrow 0$, then $\max <em>{\theta \in N</em>{e, \theta^{</em>}}^{\epsilon}}\left|\theta-\theta^{<em>}\right|_{2}^{2} \rightarrow 0$ and the quadratic approximation coincides with the second-order Taylor expansion around $\theta^{</em>}$. Moreover, this approximation is common in optimization (Schaul et al., 2013; Jastrzebski et al., 2018).</p>
<p>Proposition 2. (Reformulation of Proposition 1, illustrated in Fig. 4). Let $\epsilon&gt;0$, weights $\theta^{<em>}$. $\forall(A, B) \in \mathcal{E}^{2}$, with $N_{A, \theta^{</em>}}^{\epsilon}$ the largest path-connected region of weights space where the risk $\mathcal{R}<em A="A">{A}$ remains in an $\epsilon$ interval around $\mathcal{R}</em>\right)$, we note:}\left(\theta^{*</p>
<p>$$
\begin{aligned}
\mathcal{I}^{\epsilon}(A, B) &amp; =\max <em>{\theta \in N</em>{A, \theta^{<em>}}^{\epsilon}}\left|\mathcal{R}<em A="A">{B}(\theta)-\mathcal{R}</em>\left(\theta^{</em>}\right)\right| \
R(A, B) &amp; =\mathcal{R}<em _frac_1="\frac{1">{B}\left(\theta^{<em>}\right)-\mathcal{R}_{A}\left(\theta^{</em>}\right) \
H^{\epsilon}(A, B) &amp; =\max </em>\left(\theta-\theta^{}{2<em>}\right)^{\top} H_{A}\left(\theta-\theta^{</em>}\right) \leq \epsilon} \frac{1}{2}\left(\theta-\theta^{<em>}\right)^{\top} H_{B}\left(\theta-\theta^{</em>}\right)
\end{aligned}
$$</p>
<p>If $\forall(A, B) \in \mathcal{E}^{2}$ such as $R(A, B)&lt;0$, we have:</p>
<p>$$
\epsilon \leq-R(A, B) \times \frac{\lambda_{h}^{A}}{\lambda_{1}^{B}}
$$</p>
<p>then under previous Assumption A.1,</p>
<p>$$
\max <em B_="B)" _A_="(A," _in="\in" _mathcal_E="\mathcal{E">{(A, B) \in \mathcal{E}^{2}} \mathcal{I}^{\epsilon}(A, B)=\max </em>(A, B)\right)
$$}^{2}}\left(R(A, B)+H^{\epsilon</p>
<p>Proof We first prove that, under quadratic Assumption A.1, $\forall A \in \mathcal{E}, N_{A, \theta^{<em>}}^{\epsilon}=\left{\theta \mid\left|\mathcal{R}<em A="A">{A}(\theta)-\mathcal{R}</em>\left(\theta^{</em>}\right)\right| \leq \epsilon\right}$. Indeed, the former is always included in the latter by definition. Reciprocally, be given $\theta$ in the latter, $\left{\lambda \theta^{<em>}+(1-\lambda) \theta \mid \lambda \in[0,1]\right}$ linearly connects $\theta^{</em>}$ to $\theta$ in parameter space with the risk $\mathcal{R}<em A="A">{A}$ remaining in an $\epsilon$ interval around $\mathcal{R}</em>\left(\theta^{<em>}\right)$ because $\forall \mu \in[0,1]$ we have $\left|\mathcal{R}_{A}\left(\mu \theta^{</em>}+(1-\mu) \theta\right)-\mathcal{R}_{A}\left(\theta^{<em>}\right)\right|=(1-\mu)^{2}\left|\mathcal{R}<em A="A">{A}(\theta)-\mathcal{R}</em>\left(\theta^{</em>}\right)\right| \leq(1-\mu)^{2} \epsilon \leq \epsilon$.
Therefore $\forall(A, B) \in \mathcal{E}^{2}$ :</p>
<p>$$
\mathcal{I}^{\epsilon}(A, B)=\max <em A="A">{\left|\mathcal{R}</em>}(\theta)-\mathcal{R<em _frac_1="\frac{1">{A}\left(\theta^{<em>}\right)\right| \leq \epsilon}\left|\mathcal{R}<em A="A">{B}(\theta)-\mathcal{R}</em>\left(\theta^{</em>}\right)\right|=\max </em>\left(\theta-\theta^{}{2<em>}\right)^{\top} H_{A}\left(\theta-\theta^{</em>}\right) \leq \epsilon}\left|R(A, B)+\frac{1}{2}\left(\theta-\theta^{<em>}\right)^{\top} H_{B}\left(\theta-\theta^{</em>}\right)\right|
$$</p>
<p>As the Hessians are positive, $H^{\epsilon}(A, B)&gt;0$. We now need to split the analysis based on the sign of $R(A, B)$.</p>
<p>Case $R(A, B) \geq 0$ Both $R(A, B)$ and $H^{\epsilon}(A, B)$ are non-negative. Removing the absolute value from the RHS of Eq. 13 gives: $\mathcal{I}^{\epsilon}(A, B)=R(A, B)+H^{\epsilon}(A, B)$. Taking the maximum over $(A, B) \in \mathcal{E}^{2}$ where $R(A, B) \geq 0$ gives:</p>
<p>$$
\begin{aligned}
&amp; \max <em B_="B)" _A_="(A," _in="\in" _mathcal_E="\mathcal{E">{(A, B) \in \mathcal{E}^{2} \mid R(A, B) \geq 0} \mathcal{I}^{\epsilon}(A, B) \
&amp; =\max </em>(A, B)\right)
\end{aligned}
$$}^{2} \mid R(A, B) \geq 0}\left(R(A, B)+H^{\epsilon</p>
<p>Case $R(A, B)&lt;0$ Leveraging $\lambda_{1}^{B}$ the largest eigenvalue from $H_{B}$ and $\lambda_{h}^{A}$ the lowest eigenvalue from $H_{A}$, we upper bound:</p>
<p>$$
H^{\epsilon}(A, B) \leq \max <em 1="1">{\frac{\lambda</em>\left|\theta-\theta^{}^{A}}{2<em>} \right|<em 1="1">{2}^{2} \leq \epsilon} \frac{\lambda</em>\left|\theta-\theta^{}^{B}}{2</em>}\right|<em 1="1">{2}^{2}=\epsilon \times \frac{\lambda</em>
$$}^{B}}{\lambda_{h}^{A}</p>
<p>Then Eq. 11 gives $H^{\epsilon}(A, B)&lt;-R(A, B)$. Thus the number inside the absolute value from the RHS of Eq. 13 is negative. This leads to: $\mathcal{I}^{\epsilon}(A, B)=-R(A, B)-H^{\epsilon}(A, B)&lt;-R(A, B)=R(B, A)&lt;$ $\mathcal{I}^{\epsilon}(B, A)$. Thus the max over $\mathcal{E}^{2}$ of function $(A, B) \rightarrow \mathcal{I}^{\epsilon}(A, B)$ can not be achieved for $(A, B)$ with $R(A, B)&lt;0$. We obtain:</p>
<p>$$
\max <em B_="B)" _A_="(A," _in="\in" _mathcal_E="\mathcal{E">{(A, B) \in \mathcal{E}^{2}} \mathcal{I}^{\epsilon}(A, B)=\max </em>(A, B)
$$}^{2} \mid R(A, B) \geq 0} \mathcal{I}^{\epsilon</p>
<p>Similarly, $R(A, B)+H^{\epsilon}(A, B) \leq 0&lt;\mathcal{R}(B, A)+H^{\epsilon}(B, A)$. Thus the max over $\mathcal{E}^{2}$ of function $(A, B) \rightarrow$ $\left(R(A, B)+H^{\epsilon}(A, B)\right)$ can not be achieved for $(A, B)$ with $R(A, B)&lt;0$. We obtain:</p>
<p>$$
\max <em B_="B)" _A_="(A," _in="\in" _mathcal_E="\mathcal{E">{(A, B) \in \mathcal{E}^{2}}\left(R(A, B)+H^{\epsilon}(A, B)\right)=\max </em>(A, B)\right)
$$}^{2} \mid R(A, B) \geq 0}\left(R(A, B)+H^{\epsilon</p>
<p>Conclusion Combining Eq. 14, Eq. 16 and Eq. 17, we conclude the proof.</p>
<h1>A.2. Fishr as a feature-adaptive version of V-REx</h1>
<p>We delve into the theoretical analysis of the Fishr regularization in the classifier $w_{\omega}$, that leverages $p$ features extracted from $\phi$. We note $z_{e}^{i} \in \mathbb{R}^{p}$ the features for the $i$-th example from the domain $e, \hat{y}<em e="e">{e}^{i} \in[0,1]$ the predictions after sigmoid and $y</em>\right}}^{i} \in{0,1}$ the one-hot encoded target. The linear layer $W$ is parametrized by weights $\left{w_{k<em b="b">{k=1}^{p}$ and bias $b$.
The gradient of the loss for this sample with respect to the bias $b$ is $\nabla</em>} \ell\left(y_{e}^{i}, \hat{y<em e="e">{e}^{i}\right)=\left(\hat{y}</em>}^{i}-y_{e}^{i}\right)$. Thus, the uncentered gradient variance in $b$ for domain $e$ is: $\boldsymbol{v<em e="e">{e}^{b}=\frac{1}{n</em>}} \sum_{i=1}^{n_{e}}\left(\hat{y<em e="e">{e}^{i}-y</em>$, which is exactly the mean squared error (MSE) between predictions and targets in domain $e$. Thus, matching gradient variances in $b$ will match risks across domains. This is the objective from V-REx (Krueger et al., 2021), where the squared error has replaced the negative log likelihood.
We can also look at the gradients with respect to the weight $w_{k}: \nabla_{w_{k}} \ell\left(y_{e}^{i}, \hat{y}}^{i}\right)^{2<em e="e">{e}^{i}\right)=\left(\hat{y}</em>}^{i}-y_{e}^{i}\right) z_{e}^{i}[k]$. Thus, the uncentered gradient variance in $w_{k}$ for domain $e$ is: $\boldsymbol{v<em k="k">{e}^{w</em>}}=\frac{1}{n_{e}} \sum_{i=1}^{n_{e}}\left(\left(\hat{y<em e="e">{e}^{i}-y</em> \in{0,1}\right)$; in that case, Fishr matches domain-level risks on groups of samples having a shared feature.
More exactly in Fishr, we match centered gradient variances, equivalent to the uncentered variance gradient matching at convergence under the assumption $\boldsymbol{g}_{e} \approx 0$. Experiments in Table 5 and in Appendix C.2.4 confirm that centering or not the variances perform similarly.}^{i}\right) z_{e}^{i}[k]\right)^{2}$. This is the squared error, weighted for each sample $\left(z_{e}^{i}, y_{e}^{i}\right)$ by the square of the $k$-th feature $z_{e}^{i}[k]$ : matching gradient variances directly matches these weighted squared errors, with $k$ different weighting schemes, that depend on the features distribution. This describes Fishr as a feature-adaptive version of V-REx (Krueger et al., 2021). An intuitive example is when features are binary $\left(z_{e}^{i</p>
<h2>A.3. Neural Tangent Kernel perspective</h2>
<p>In this Section we motivate the matching of gradient covariances with new arguments from the Neural Tangent Kernel (NTK) (Jacot et al., 2018) theory. As a reminder, the NTK $\boldsymbol{K} \in \mathbb{R}^{n \times n}$ is the gramian matrix with entries $\boldsymbol{K}[i, j]=$</p>
<p>$\nabla_{\theta} f_{\theta}\left(x^{i}\right)^{\top} \cdot \nabla_{\theta} f_{\theta}\left(x^{j}\right)$ that measure the gradients similarity at two different input points $x^{i}$ and $x^{j}$. This kernel dictates the training dynamics of the DNN and remains fixed in the infinite width limit. Most importantly, as stated in Yang \&amp; Salman (2019), "the simplicity bias of a wide neural network can be read off quickly from the spectrum of $\boldsymbol{K}$ : if the largest eigenvalue [ $\lambda^{\max }$ ] of $\boldsymbol{K}$ accounts for most of $\operatorname{Tr}(\boldsymbol{K})$, then a typical random network looks like a function from the top eigenspace of $\boldsymbol{K}$ ": this holds for ReLu networks. In summary, gradient descent mostly happens in a tiny subspace (Gur-Ari et al., 2018) whose directions are defined by the main eigenvectors from $\boldsymbol{K}$. Moreover, the learning speed is dictated by $\lambda^{\max }$, which can be used to estimate a condition for a learning rate $\eta$ to converge: $\eta&lt;2 / \lambda^{\max }$ (Karakida et al., 2019).
In a multi-domain framework, having similar spectral decompositions across $\left{\boldsymbol{K}<em _in="\in" _mathcal_E="\mathcal{E" e="e">{e}\right}</em>$ during the optimization process would improve OOD generalization for two reasons:}</p>
<ol>
<li>Having similar top eigenvectors across $\left{\boldsymbol{K}<em _in="\in" _mathcal_E="\mathcal{E" e="e">{e}\right}</em>$ would delete detrimental domain-dependent shortcuts and favor the learning of a common mechanism. Indeed, truly informative features should remain consistent across domains.}</li>
<li>Having similar top eigenvalues across $\left{\boldsymbol{K}<em _in="\in" _mathcal_E="\mathcal{E" e="e">{e}\right}</em>$ would improve the optimization schema for simultaneous training at the same speed. Indeed, it would facilitate the finding of a learning rate for simultaneous convergence on all domains. It's worth noting that if we quickly overfit on a first domain using spurious explanations, invariances will then be hard to learn due to the gradient starvation phenomena (Pezeshki et al., 2021).}</li>
</ol>
<p>Directly matching $\boldsymbol{K}<em e="e">{e}$ would require assuming that each domain coincides and contains the same samples; for example, with different pose angles (Ghifary et al., 2015). To avoid such a strong assumption, we leverage the fact that the 'true' Fisher Information Matrix $\boldsymbol{F}$ and the NTK $\boldsymbol{K}$ share the same non-zero eigenvalues since $\boldsymbol{F}$ is dual to $\boldsymbol{K}$ (see Appendix C. 1 in Maddox et al. (2019), notably for classification tasks). Moreover, their eigenvectors are strongly related (see Appendix C in Kopitkov \&amp; Indelman (2019)). Thus, having similar $\left{\boldsymbol{F}</em>\right}<em e="e">{e \in \mathcal{E}}$ encourages $\left{\boldsymbol{K}</em>$ (see Section 3.2.3), this further motivates the need to match gradient variances during the SGD trajectory - and not only at convergence as in Section 3.2.}\right}_{e \in \mathcal{E}}$ to have similar spectral decomposition. Based on the close relations between $\boldsymbol{C}$ and $\boldsymbol{F</p>
<h1>B. Experiments on a Linear Example</h1>
<p>We experimentally prove that Fishr is effective in the linear setting. To do so, we consider the binary classification dataset introduced in the Section 3.2 from Fish (Shi et al., 2021). Each example is composed of 4 static features ( $f_{1}, f_{2}, f_{3}, f_{4}$ ). While $f_{1}$ is invariant across the two train domains and the test domain, the three other features are spurious: their correlations with the label vary in each domain. The model is a linear logistic regression, with trainable weights $W$ and bias $b$. As $f_{2}$ and $f_{3}$ have higher correlations with the label than $f_{1}$ in training, ERM relies mostly on $f_{2}$ and $f_{3}$. This is indicated in the first line of Table 5 by the large values (3.3) for weights associated to $f_{2}$ and $f_{3}$; this induces low test accuracy (57\%). On the contrary, Fishr forces the linear model to rely mostly on the invariant feature $f_{1}$, as indicated by the lower values (1.2) for weights associated to $f_{2}$ and $f_{3}$; in accuracy, Fishr performs similarly in test and train (93\%).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Matched statistics</th>
<th style="text-align: center;">Train acc.</th>
<th style="text-align: center;">Test acc.</th>
<th style="text-align: center;">$W$</th>
<th style="text-align: center;">$b$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ERM</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$97 \%$</td>
<td style="text-align: center;">$57 \%$</td>
<td style="text-align: center;">$[2.8,3.3,3.3,0.0]$</td>
<td style="text-align: center;">-2.7</td>
</tr>
<tr>
<td style="text-align: center;">Fish</td>
<td style="text-align: center;">Gradient means</td>
<td style="text-align: center;">$93 \%$</td>
<td style="text-align: center;">$93 \%$</td>
<td style="text-align: center;">$[0.4,0.2,0.2,0.0]$</td>
<td style="text-align: center;">-0.4</td>
</tr>
<tr>
<td style="text-align: center;">Fishr</td>
<td style="text-align: center;">Centered gradient variances</td>
<td style="text-align: center;">$93 \%$</td>
<td style="text-align: center;">$93 \%$</td>
<td style="text-align: center;">$[2.0,1.2,1.2,0.0]$</td>
<td style="text-align: center;">-0.6</td>
</tr>
<tr>
<td style="text-align: center;">Fishr</td>
<td style="text-align: center;">Uncentered gradient variances</td>
<td style="text-align: center;">$93 \%$</td>
<td style="text-align: center;">$93 \%$</td>
<td style="text-align: center;">$[1.9,0.9,0.9,0.0]$</td>
<td style="text-align: center;">-0.6</td>
</tr>
</tbody>
</table>
<p>Table 5: Performances comparison on the linear dataset from (Shi et al., 2021)</p>
<h2>C. Colored MNIST in the IRM Setup</h2>
<h2>C.1. Description of the Colored MNIST experiment</h2>
<p>Colored MNIST is a binary digit classification dataset introduced in IRM (Arjovsky et al., 2019). Compared to the traditional MNIST (LeCun et al., 2010), it has 2 main differences. First, 0-4 and 5-9 digits are each collapsed into a single class, with a $25 \%$ chance of label flipping. Second, digits are either colored red or green, with a strong correlation between label and color in training. However, this correlation is reversed at test time. Specifically, in training, the model has access to two domains $\mathcal{E}={90 \%, 80 \%}$ : in the first domain, green digits have a $90 \%$ chance of being in 5-9; in the second, this chance</p>
<p>goes down to $80 \%$. In test, green digits have a $10 \%$ chance of being in 5-9. Due to this modification in correlation, a model should ideally ignore the color information and only rely on the digits' shape: this would obtain a $75 \%$ test accuracy.</p>
<p>In the experimental setup from IRM, the network is a 3 layers MLP with ReLu activation, optimized with Adam (Kingma \&amp; Ba, 2014). IRM selected the following hyperparameters by random search over 50 trials: hidden dimension of 390, $l_{2}$ regularizer weight of 0.00110794568 , learning rate of 0.0004898536566546834 , penalty anneal iters (or warmup iter) of 190, penalty weight $(\lambda)$ of 91257.18613115903 , 501 epochs and batch size 25,000 (half of the dataset size). We strictly keep the same hyperparameters values in our proof of concept in Section 4.1. The code is almost unchanged from https://github.com/facebookresearch/InvariantRiskMinimization.</p>
<h1>C.2. Empirical validation of some key insights</h1>
<h2>C.2.1. Hessian matching</h2>
<p>Based on empirical works (Li et al., 2020; Singh \&amp; Alistarh, 2020; Thomas et al., 2020), we argue in Section 3.2.3 that gradient covariance $\boldsymbol{C}$ can be used as a proxy to regularize the Hessian $\boldsymbol{H}$ - even though the proper approximation bounds are out of scope of this paper. This was empirically validated at convergence in Table 2 and during training in Fig. 3. We leveraged the DiagHessian method from BackPACK to compute Hessian diagonals, in all network weights $\theta$. Notably, Hessians are impractical in a training objective as computing "Hessian is an order of magnitude more computationally intensive" (see Fig. 9 in Dangel et al. (2020)). This Appendix further analyzes the Hessian trajectory during training.</p>
<p>Fig. 5 illustrates the dynamics for Fish $\mathrm{r}_{\theta}$ : following the scheduling previously described in Appendix C.1, $\lambda$ jumping to a high value at epoch 190 activates the regularization. After this epoch, the domain-level Hessians are not only close in Frobenius distance, but also have similar norms and directions. On the contrary, when using only ERM in Fig. 6, the distance between domain-level Hessians keeps increasing with the number of epochs. As a side note, flatter loss landscapes in ERM — as reflected by the Hessian norms in orange — do not correlate with improved generalization (Dinh et al., 2017).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Hessian dynamics on Colored MNIST with Fishr: at epoch 190, $\lambda$ steps up. Then domain-level Hessians are matched across domains (purple). More precisely, they take similar directions - high cosine similarity (red) — and similar norms (blue). The Hessians' norms (orange) remain quite high thus the loss landscapes are rather sharp.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Hessian dynamics on Colored MNIST with ERM: $\lambda=0$ along training. The Frobenius distance between domain-level Hessians (purple) keeps increasing: so does the distance between their norms (blue). Their cosine similarity (red) steadily decreases. The loss landscapes are flat at convergence (low Hessian norms in orange).</p>
<p>This is also visible in Fig. 7, which is equivalent to Fig. 3, but for ERM (without the Fishr regularization). The distance between domain-level gradient variances (red) keeps increasing across domains $\mathcal{E}={90 \%, 80 \%}$ : so does the distance across Hessians (purple). The distance across risks (pink) decreases, but slower than with Fishr regularization. Overall, the network still predicts the digit's color while only slightly using the digit's shape. That's why the test accuracy (blue) remains low.</p>
<h3>C.2.2. Colored MNIST without label flipping</h3>
<p>To further validate that Fishr can tackle distribution shifts, we investigate Colored MNIST but without the 25\% label flipping. In Table 6 , the label is then fully predictable from the digit shape. Using hyperparameters defined previously in Appendix C.1, we recover that IRM ( $82.2 \%$ ) fails when the invariant feature is fully predictive (Ahuja et al., 2019): indeed, it performs worse than ERM ( $91.8 \%$ ). In contrast, V-REx and Fishr perform better ( $95.3 \%$ ): in conclusion, Fishr works even without label noise.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Colored MNIST dynamics with ERM.</p>
<p>Table 6: Colored MNIST experiments without label flipping.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Train acc.</th>
<th style="text-align: center;">Test acc.</th>
<th style="text-align: center;">Gray test acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ERM</td>
<td style="text-align: center;">$99.0 \pm 0.0$</td>
<td style="text-align: center;">$91.8 \pm 0.2$</td>
<td style="text-align: center;">$95.0 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">IRM</td>
<td style="text-align: center;">$96.4 \pm 0.2$</td>
<td style="text-align: center;">$82.2 \pm 0.1$</td>
<td style="text-align: center;">$92.6 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">V-REx</td>
<td style="text-align: center;">$97.1 \pm 0.2$</td>
<td style="text-align: center;">$95.3 \pm 0.4$</td>
<td style="text-align: center;">$94.1 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">Fishr $_{\theta}$</td>
<td style="text-align: center;">$97.9 \pm 0.2$</td>
<td style="text-align: center;">$93.6 \pm 0.4$</td>
<td style="text-align: center;">$94.8 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">Fishr $_{\omega}$</td>
<td style="text-align: center;">$97.0 \pm 0.2$</td>
<td style="text-align: center;">$95.3 \pm 0.4$</td>
<td style="text-align: center;">$94.1 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">Fishr $_{\phi}$</td>
<td style="text-align: center;">$97.9 \pm 0.1$</td>
<td style="text-align: center;">$93.5 \pm 0.3$</td>
<td style="text-align: center;">$94.8 \pm 0.4$</td>
</tr>
</tbody>
</table>
<h1>C.2.3. GRADIENT VARIANCE OR COVARIANCE ?</h1>
<p>We have justified ignoring the off-diagonal parts of the covariance to reduce the memory overhead. For the sake of completeness, the second line in Table 7 shows results with the full covariance matrix. This experiment is possible only when considering gradient in the classifier $w_{\omega}$ for memory reasons. Overall, results are similar (or slightly worse) as when using only the diagonal: the slight difference may be explained by the approaches' different suitability to the hyperparameters (that were optimized for IRM). In conclusion, this preliminary experiment suggests that targeting the diagonal components is the most critical. We hope future works will further investigate this diagonal approximation or provide new methods to reduce the computational costs, such as K-FAC approximations (Heskes, 2000; Martens \&amp; Grosse, 2015).</p>
<p>Table 7: Colored MNIST experiments with different statistics matched. All hyperparameters were optimized for IRM.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">25\% label flipping</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">No label flipping</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Gradients in</td>
<td style="text-align: center;">Name</td>
<td style="text-align: center;">Matched statistics</td>
<td style="text-align: center;">Train acc.</td>
<td style="text-align: center;">Test acc.</td>
<td style="text-align: center;">Gray test acc.</td>
<td style="text-align: center;">Train acc.</td>
<td style="text-align: center;">Test acc.</td>
<td style="text-align: center;">Gray test acc.</td>
</tr>
<tr>
<td style="text-align: center;">$\omega$</td>
<td style="text-align: center;">Centered variance ( $\times$ Fish $r_{\omega}$ )</td>
<td style="text-align: center;">$\operatorname{Var}\left(G_{c}\right)$</td>
<td style="text-align: center;">$71.0 \pm 0.9$</td>
<td style="text-align: center;">$69.5 \pm 1.0$</td>
<td style="text-align: center;">$70.2 \pm 1.1$</td>
<td style="text-align: center;">$97.0 \pm 0.2$</td>
<td style="text-align: center;">$95.3 \pm 0.4$</td>
<td style="text-align: center;">$94.1 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Centered covariance</td>
<td style="text-align: center;">$C_{c}$</td>
<td style="text-align: center;">$70.7 \pm 1.0$</td>
<td style="text-align: center;">$69.1 \pm 1.1$</td>
<td style="text-align: center;">$69.9 \pm 1.1$</td>
<td style="text-align: center;">$97.0 \pm 0.2$</td>
<td style="text-align: center;">$95.3 \pm 0.4$</td>
<td style="text-align: center;">$94.0 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Uncentered variance</td>
<td style="text-align: center;">$\operatorname{Disg}\left(\frac{1}{c} \hat{\boldsymbol{F}}_{c}\right)$</td>
<td style="text-align: center;">$71.3 \pm 0.9$</td>
<td style="text-align: center;">$69.5 \pm 1.0$</td>
<td style="text-align: center;">$70.3 \pm 1.0$</td>
<td style="text-align: center;">$97.0 \pm 0.2$</td>
<td style="text-align: center;">$95.3 \pm 0.4$</td>
<td style="text-align: center;">$94.1 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">$\theta$</td>
<td style="text-align: center;">Centered variance ( $\times$ Fish $r_{\theta}$ )</td>
<td style="text-align: center;">$\operatorname{Var}\left(G_{c}\right)$</td>
<td style="text-align: center;">$69.6 \pm 0.9$</td>
<td style="text-align: center;">$71.2 \pm 1.1$</td>
<td style="text-align: center;">$70.2 \pm 0.7$</td>
<td style="text-align: center;">$97.9 \pm 0.1$</td>
<td style="text-align: center;">$93.5 \pm 0.3$</td>
<td style="text-align: center;">$94.7 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Centered covariance</td>
<td style="text-align: center;">$C_{c}$</td>
<td style="text-align: center;">Not</td>
<td style="text-align: center;">possible</td>
<td style="text-align: center;">for</td>
<td style="text-align: center;">computational</td>
<td style="text-align: center;">(memory)</td>
<td style="text-align: center;">reasons</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Uncentered variance</td>
<td style="text-align: center;">$\operatorname{Disg}\left(\frac{1}{c} \hat{\boldsymbol{F}}_{c}\right)$</td>
<td style="text-align: center;">$71.0 \pm 0.8$</td>
<td style="text-align: center;">$70.0 \pm 1.1$</td>
<td style="text-align: center;">$70.1 \pm 0.9$</td>
<td style="text-align: center;">$97.9 \pm 0.0$</td>
<td style="text-align: center;">$93.5 \pm 0.3$</td>
<td style="text-align: center;">$94.8 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">$\phi$</td>
<td style="text-align: center;">Centered variance ( $\times$ Fish $r_{\phi}$ )</td>
<td style="text-align: center;">$\operatorname{Var}\left(G_{c}\right)$</td>
<td style="text-align: center;">$65.6 \pm 1.3$</td>
<td style="text-align: center;">$73.8 \pm 1.0$</td>
<td style="text-align: center;">$70.0 \pm 0.9$</td>
<td style="text-align: center;">$97.9 \pm 0.1$</td>
<td style="text-align: center;">$93.5 \pm 0.3$</td>
<td style="text-align: center;">$94.8 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Centered covariance</td>
<td style="text-align: center;">$C_{c}$</td>
<td style="text-align: center;">Not</td>
<td style="text-align: center;">possible</td>
<td style="text-align: center;">for</td>
<td style="text-align: center;">computational</td>
<td style="text-align: center;">(memory)</td>
<td style="text-align: center;">reasons</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Uncentered variance</td>
<td style="text-align: center;">$\operatorname{Disg}\left(\frac{1}{c} \hat{\boldsymbol{F}}_{c}\right)$</td>
<td style="text-align: center;">$71.5 \pm 0.8$</td>
<td style="text-align: center;">$69.1 \pm 1.1$</td>
<td style="text-align: center;">$70.0 \pm 1.0$</td>
<td style="text-align: center;">$97.9 \pm 0.1$</td>
<td style="text-align: center;">$93.5 \pm 0.3$</td>
<td style="text-align: center;">$94.8 \pm 0.4$</td>
</tr>
</tbody>
</table>
<h2>C.2.4. CENTERED OR UNCENTERED VARIANCE ?</h2>
<p>In Section 3.2.3, we argue that the gradient centered covariance $\boldsymbol{C}$ and the empirical Fisher Information Matrix (or uncentered covariance) $\hat{\boldsymbol{F}}$ are highly related and equivalent when the DNN is at convergence and the gradient means are zero. So, we could have tackled the diagonals of the domain-level $\left{\hat{\boldsymbol{F}}<em _in="\in" _mathcal_E="\mathcal{E" e="e">{e}\right}</em>$ across domains, i.e., without centering the variances. Empirically, comparing the first and third lines in Table 7 shows that centering or not the variance are almost equivalent. This holds true when applying Fishr on all weights $\theta$ (as lines fourth and six are also very similar). This was empirically confirmed in DomainBed: for example, Fishr with either centered or uncentered variances reach 67.8. Still, it's worth noting that explicitly matching simultaneously the gradient centered variances along with the gradient means performs best in Appendix D.3.2.}</p>
<h1>D. DomainBed</h1>
<h2>D.1. Description of the DomainBed benchmark</h2>
<p>We now further detail our experiments on the DomainBed benchmark. Scores from most baselines are taken from the DomainBed (Gulrajani \&amp; Lopez-Paz, 2021) paper. Scores for AND-mask and SAND-mask are taken from the SAND-mask paper (Shahtalebi et al., 2021). Scores for IGA (Koyama \&amp; Yamaguchi, 2020) are not yet available: yet, for the sake of completeness, we analyze IGA in Appendix D.3.2. Missing scores will be included when available.</p>
<p>The same procedure was applied for all methods: for each domain, a random hyperparameter search of 20 trials over a joint distribution, described in Table 8, is performed. We discuss the choice of these distributions in Appendix D.3.3. The learning rate, the batch size (except for ARM), the weight decay and the dropout distributions are shared across all methods all trained with Adam (Kingma \&amp; Ba, 2014). Specific hyperparameter distributions for concurrent methods can be found in the original work of Gulrajani \&amp; Lopez-Paz (2021). The data from each domain is split into $80 \%$ (used as training and testing) and $20 \%$ (used as validation for hyperparameter selection) splits. This random process is repeated with 3 different seeds: the reported numbers are the means and the standard errors over these 3 seeds.</p>
<p>Table 8: Hyperparameters, their default values and distributions for random search.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Condition</th>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Default value</th>
<th style="text-align: left;">Random distribution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VLCS / PACS /</td>
<td style="text-align: left;">learning rate</td>
<td style="text-align: left;">0.00005</td>
<td style="text-align: left;">$10^{\text {Uniform }(-5,-3.5)}$</td>
</tr>
<tr>
<td style="text-align: left;">OfficeHome /</td>
<td style="text-align: left;">batch size</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">$2^{\text {Uniform }(3,5.5)}$ if not DomainNet else $2^{\text {Uniform }(3,5)}$</td>
</tr>
<tr>
<td style="text-align: left;">TerraIncognita /</td>
<td style="text-align: left;">weight decay</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">$10^{\text {Uniform }(-6,-2)}$</td>
</tr>
<tr>
<td style="text-align: left;">DomainNet</td>
<td style="text-align: left;">dropout</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">RandomChoice $([0,0.1,0.5])$</td>
</tr>
<tr>
<td style="text-align: left;">Rotated MNIST /</td>
<td style="text-align: left;">learning rate</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">$10^{\text {Uniform }(-4.5,-3.5)}$</td>
</tr>
<tr>
<td style="text-align: left;">Colored MNIST</td>
<td style="text-align: left;">batch size</td>
<td style="text-align: left;">64</td>
<td style="text-align: left;">$2^{\text {Uniform }(3,9)}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">weight decay</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: left;">steps</td>
<td style="text-align: left;">5000</td>
<td style="text-align: left;">5000</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">regularization strength $\lambda$</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">$10^{\text {Uniform }(1,4)}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ema $\gamma$</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">Uniform $(0.9,0.99)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">warmup iterations</td>
<td style="text-align: left;">1500</td>
<td style="text-align: left;">Uniform $(0,5000)$</td>
</tr>
</tbody>
</table>
<p>We clarify a subtle point (omitted in the Algorithm 1) concerning the hyperparameter $\gamma$ that controls: $\bar{v}<em c="c">{c}^{t}=\gamma \bar{v}</em>}^{t-1}+(1-\gamma) v_{c}^{t}$ at step $t$. We remind that $\bar{v<em c="c">{c}^{t-1}$ from previous step $t-1$ is 'detached' from the computational graph. Thus when $\mathcal{L}$ from Eq. 4 is differentiated during SGD, the gradients going through $\boldsymbol{v}</em>$. Finally, with this $(1-\gamma)$ correction, the gradients' strength backpropagated in the network is independent of $\gamma$.}^{t}$ are multiplied by $(1-\gamma)$. To compensate this and decorrelate the impact of $\gamma$ and of $\lambda$ (that controls the regularization strength), we match $\frac{1}{1-\gamma} \bar{v}_{c}^{t</p>
<p>Here we list all concurrent approaches.</p>
<ul>
<li>ERM: Empirical Risk Minimization (Vapnik, 1999)</li>
<li>IRM: Invariant Risk Minimization (Arjovsky et al., 2019)</li>
<li>GroupDRO: Group Distributionally Robust Optimization (Sagawa et al., 2020a)</li>
<li>Mixup: Interdomain Mixup (Yan et al., 2020)</li>
<li>MLDG: Meta Learning Domain Generalization (Li et al., 2018a)</li>
<li>CORAL: Deep CORAL (Sun \&amp; Saenko, 2016)</li>
<li>MMD: Maximum Mean Discrepancy (Li et al., 2018b)</li>
<li>DANN: Domain Adversarial Neural Network (Ganin et al., 2016)</li>
<li>CDANN: Conditional Domain Adversarial Neural Network (Li et al., 2018c)</li>
<li>
<p>MTL: Marginal Transfer Learning (Blanchard et al., 2021)</p>
</li>
<li>
<p>SagNet: Style Agnostic Networks (Nam et al., 2021)</p>
</li>
<li>ARM: Adaptive Risk Minimization (Zhang et al., 2020)</li>
<li>V-REx: Variance Risk Extrapolation (Krueger et al., 2021)</li>
<li>RSC: Representation Self-Challenging (Huang et al., 2020)</li>
<li>AND-mask: Learning Explanations that are Hard to Vary (Parascandolo et al., 2021)</li>
<li>SAND-mask: An Enhanced Gradient Masking Strategy for the Discovery of Invariances in Domain Generalization (Shahtalebi et al., 2021)</li>
<li>IGA: Out-of-distribution generalization with maximal invariant predictor (Koyama \&amp; Yamaguchi, 2020)</li>
<li>Fish: Gradient Matching for Domain Generalization (Shi et al., 2021)</li>
</ul>
<p>We omitted the recent weight averaging approaches (Cha et al., 2021; Rame et al., 2022) whose contribution is complementary to others, that uses a custom hyperparameter search and does not report scores with the 'Test-domain' model selection.</p>
<p>DomainBed includes seven multi-domain computer vision classification datasets:</p>
<ol>
<li>Colored MNIST (Arjovsky et al., 2019) is a variant of the MNIST handwritten digit classification dataset (LeCun et al., 2010). As described previously in Appendix C.1, domain $d \in{90 \%, 80 \%, 10 \%}$ contains a disjoint set of digits colored: the correlation strengths between color and label vary across domains. The dataset contains 70,000 examples of dimension $(2,28,28)$ and 2 classes. Most importantly, the network, the hyperparameters, the image shapes, etc. are not the same as in the IRM setup from Section 4.1.</li>
<li>Rotated MNIST (Ghifary et al., 2015) is a variant of MNIST where domain $d \in{0,15,30,45,60,75}$ contains digits rotated by $d$ degrees, with 70,000 examples of dimension $(1,28,28)$ and 10 classes.</li>
<li>VLCS (Fang et al., 2013) includes photographic domains $d \in{$ Caltech101, LabelMe, SUN09, VOC2007}, with 10,729 examples of dimension $(3,224,224)$ and 5 classes.</li>
<li>PACS (Li et al., 2017) includes domains $d \in{$ art, cartoons, photos, sketches $}$, with 9,991 examples of dimension $(3,224,224)$ and 7 classes.</li>
<li>OfficeHome (Venkateswara et al., 2017) includes domains $d \in{$ art, clipart, product, real $}$, with 15,588 examples of dimension $(3,224,224)$ and 65 classes.</li>
<li>TerraIncognita (Beery et al., 2018) contains photographs of wild animals taken by camera traps at locations $d \in{$ L100, L38, L43, L46}, with 24,788 examples of dimension $(3,224,224)$ and 10 classes.</li>
<li>DomainNet (Peng et al., 2019) has six domains $d \in{$ clipart, infograph, painting, quickdraw, real, sketch $}$, with 586,575 examples of size $(3,224,224)$ and 345 classes.</li>
</ol>
<p>The convolutional neural network architecture used for the MNIST experiments is the one introduced in DomainBed: note that this is not the same MLP (described in Appendix C.1) as in our proof of concept in Section 4.1. All real datasets leverage a 'ResNet-50' pretrained on ImageNet, with a dropout layer before the newly added dense layer and fine-tuned with frozen batch normalization layers.</p>
<h1>D.2. 'Training-domain' model selection</h1>
<p>In the main paper, we focus on the 'Test-domain' model selection, where the validation set follows the same distribution as the test domain. This is important to adapt the degree of model invariance according to the test domain. For Fishr, if the domain-dependant correlations are useful in test, the selected $\lambda$ would be small and Fishr would behave like ERM; in contrast, if the domain-dependant correlations are detrimental in test, the selected $\lambda$ would be large, and Fishr would improve over ERM by enforcing invariance.</p>
<p>Table 9: DomainBed with 'Training-domain' model selection. We format first, second and worse than ERM results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Algorithm</th>
<th style="text-align: center;">Accuracy ( $\uparrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Ranking ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CMNIST</td>
<td style="text-align: center;">RMNIST</td>
<td style="text-align: center;">VLCS</td>
<td style="text-align: center;">PACS</td>
<td style="text-align: center;">OfficeHome</td>
<td style="text-align: center;">TerraInc</td>
<td style="text-align: center;">DomainNet</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Arith. mean</td>
<td style="text-align: center;">Geom. mean</td>
<td style="text-align: center;">Median</td>
</tr>
<tr>
<td style="text-align: center;">ERM</td>
<td style="text-align: center;">51.5 $\pm$ a 1</td>
<td style="text-align: center;">98.0 $\pm$ a 0</td>
<td style="text-align: center;">77.5 $\pm$ a 4</td>
<td style="text-align: center;">85.5 $\pm$ a 2</td>
<td style="text-align: center;">66.5 $\pm$ a 3</td>
<td style="text-align: center;">46.1 $\pm$ 1 a</td>
<td style="text-align: center;">40.9 $\pm$ a 1</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">IRM</td>
<td style="text-align: center;">52.0 $\pm$ a 1</td>
<td style="text-align: center;">97.7 $\pm$ a 1</td>
<td style="text-align: center;">78.5 $\pm$ a 5</td>
<td style="text-align: center;">83.5 $\pm$ a 4</td>
<td style="text-align: center;">64.3 $\pm$ 22</td>
<td style="text-align: center;">47.6 $\pm$ a 4</td>
<td style="text-align: center;">33.9 $\pm$ 2 a</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">GroupDRO</td>
<td style="text-align: center;">52.1 $\pm$ a 0</td>
<td style="text-align: center;">98.0 $\pm$ a 0</td>
<td style="text-align: center;">76.7 $\pm$ a 0</td>
<td style="text-align: center;">84.4 $\pm$ a 4</td>
<td style="text-align: center;">66.0 $\pm$ a 5</td>
<td style="text-align: center;">43.2 $\pm$ 11</td>
<td style="text-align: center;">33.3 $\pm$ a 2</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">Mixup</td>
<td style="text-align: center;">52.1 $\pm$ a 2</td>
<td style="text-align: center;">98.0 $\pm$ a 1</td>
<td style="text-align: center;">77.4 $\pm$ a 0</td>
<td style="text-align: center;">84.6 $\pm$ a 4</td>
<td style="text-align: center;">68.1 $\pm$ a 3</td>
<td style="text-align: center;">47.9 $\pm$ a 4</td>
<td style="text-align: center;">39.2 $\pm$ a 1</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">MLDG</td>
<td style="text-align: center;">51.5 $\pm$ a 1</td>
<td style="text-align: center;">97.9 $\pm$ a 0</td>
<td style="text-align: center;">77.2 $\pm$ a 4</td>
<td style="text-align: center;">84.9 $\pm$ 10</td>
<td style="text-align: center;">66.8 $\pm$ a 6</td>
<td style="text-align: center;">47.7 $\pm$ a 9</td>
<td style="text-align: center;">41.2 $\pm$ a 1</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">CORAL</td>
<td style="text-align: center;">51.5 $\pm$ a 1</td>
<td style="text-align: center;">98.0 $\pm$ a 1</td>
<td style="text-align: center;">78.8 $\pm$ a 0</td>
<td style="text-align: center;">86.2 $\pm$ a 3</td>
<td style="text-align: center;">68.7 $\pm$ a 3</td>
<td style="text-align: center;">47.6 $\pm$ 10</td>
<td style="text-align: center;">41.5 $\pm$ a 1</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">MMD</td>
<td style="text-align: center;">51.5 $\pm$ a 2</td>
<td style="text-align: center;">97.9 $\pm$ a 0</td>
<td style="text-align: center;">77.5 $\pm$ a 9</td>
<td style="text-align: center;">84.6 $\pm$ a 5</td>
<td style="text-align: center;">66.3 $\pm$ a 1</td>
<td style="text-align: center;">42.2 $\pm$ 1a</td>
<td style="text-align: center;">25.4 $\pm$ a 5</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">DANN</td>
<td style="text-align: center;">51.5 $\pm$ a 3</td>
<td style="text-align: center;">97.8 $\pm$ a 1</td>
<td style="text-align: center;">78.6 $\pm$ a 4</td>
<td style="text-align: center;">83.6 $\pm$ a 4</td>
<td style="text-align: center;">65.9 $\pm$ a 6</td>
<td style="text-align: center;">46.7 $\pm$ a 5</td>
<td style="text-align: center;">38.3 $\pm$ a 1</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">CDANN</td>
<td style="text-align: center;">51.7 $\pm$ a 1</td>
<td style="text-align: center;">97.9 $\pm$ a 1</td>
<td style="text-align: center;">77.5 $\pm$ a 1</td>
<td style="text-align: center;">82.6 $\pm$ a 9</td>
<td style="text-align: center;">65.8 $\pm$ 13</td>
<td style="text-align: center;">45.8 $\pm$ 1a</td>
<td style="text-align: center;">38.3 $\pm$ a 3</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">MTL</td>
<td style="text-align: center;">51.4 $\pm$ a 1</td>
<td style="text-align: center;">97.9 $\pm$ a 0</td>
<td style="text-align: center;">77.2 $\pm$ a 4</td>
<td style="text-align: center;">84.6 $\pm$ a 5</td>
<td style="text-align: center;">66.4 $\pm$ a 5</td>
<td style="text-align: center;">45.6 $\pm$ 12</td>
<td style="text-align: center;">40.6 $\pm$ a 1</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">SagNet</td>
<td style="text-align: center;">51.7 $\pm$ a 0</td>
<td style="text-align: center;">98.0 $\pm$ a 0</td>
<td style="text-align: center;">77.8 $\pm$ a 5</td>
<td style="text-align: center;">86.3 $\pm$ a 2</td>
<td style="text-align: center;">68.1 $\pm$ a 1</td>
<td style="text-align: center;">48.6 $\pm$ 10</td>
<td style="text-align: center;">40.3 $\pm$ a 1</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">ARM</td>
<td style="text-align: center;">56.2 $\pm$ a 1</td>
<td style="text-align: center;">98.2 $\pm$ a 1</td>
<td style="text-align: center;">77.6 $\pm$ a 3</td>
<td style="text-align: center;">85.1 $\pm$ a 4</td>
<td style="text-align: center;">64.8 $\pm$ a 3</td>
<td style="text-align: center;">45.5 $\pm$ a 3</td>
<td style="text-align: center;">35.5 $\pm$ a 2</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: center;">V-REx</td>
<td style="text-align: center;">51.8 $\pm$ a 1</td>
<td style="text-align: center;">97.9 $\pm$ a 1</td>
<td style="text-align: center;">78.3 $\pm$ a 2</td>
<td style="text-align: center;">84.9 $\pm$ a 6</td>
<td style="text-align: center;">66.4 $\pm$ a 6</td>
<td style="text-align: center;">46.4 $\pm$ a 6</td>
<td style="text-align: center;">33.6 $\pm$ 29</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">RSC</td>
<td style="text-align: center;">51.7 $\pm$ a 2</td>
<td style="text-align: center;">97.6 $\pm$ a 1</td>
<td style="text-align: center;">77.1 $\pm$ a 5</td>
<td style="text-align: center;">85.2 $\pm$ a 9</td>
<td style="text-align: center;">65.5 $\pm$ a 9</td>
<td style="text-align: center;">46.6 $\pm$ 10</td>
<td style="text-align: center;">38.9 $\pm$ a 5</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: center;">AND-mask</td>
<td style="text-align: center;">51.3 $\pm$ a 2</td>
<td style="text-align: center;">97.6 $\pm$ a 1</td>
<td style="text-align: center;">78.1 $\pm$ a 9</td>
<td style="text-align: center;">84.4 $\pm$ a 9</td>
<td style="text-align: center;">65.6 $\pm$ a 4</td>
<td style="text-align: center;">44.6 $\pm$ a 3</td>
<td style="text-align: center;">37.2 $\pm$ a 6</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: center;">SAND-mask</td>
<td style="text-align: center;">51.8 $\pm$ a 2</td>
<td style="text-align: center;">97.4 $\pm$ a 1</td>
<td style="text-align: center;">77.4 $\pm$ a 2</td>
<td style="text-align: center;">84.6 $\pm$ a 9</td>
<td style="text-align: center;">65.8 $\pm$ a 4</td>
<td style="text-align: center;">42.9 $\pm$ 17</td>
<td style="text-align: center;">32.1 $\pm$ a 6</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: center;">Fish</td>
<td style="text-align: center;">51.6 $\pm$ a 1</td>
<td style="text-align: center;">98.0 $\pm$ a 0</td>
<td style="text-align: center;">77.8 $\pm$ a 3</td>
<td style="text-align: center;">85.5 $\pm$ a 3</td>
<td style="text-align: center;">68.6 $\pm$ a 4</td>
<td style="text-align: center;">45.1 $\pm$ 13</td>
<td style="text-align: center;">42.7 $\pm$ a 2</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Fishr</td>
<td style="text-align: center;">52.0 $\pm$ a 2</td>
<td style="text-align: center;">97.8 $\pm$ a 0</td>
<td style="text-align: center;">77.8 $\pm$ a 1</td>
<td style="text-align: center;">85.5 $\pm$ a 4</td>
<td style="text-align: center;">67.8 $\pm$ a 1</td>
<td style="text-align: center;">47.4 $\pm$ 1a</td>
<td style="text-align: center;">41.7 $\pm$ a 0</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">5</td>
</tr>
</tbody>
</table>
<p>In Table 9, we use the 'Training-domain' model selection: the validation set is formed by randomly collecting $20 \%$ of each training domain. Fishr performs better than ERM on all real datasets (over standard errors for OfficeHome and DomainNet), except for PACS where the two reach $85.5 \%$. In average, Fishr ( $67.1 \%$ ) finishes third and is above most methods such as V-REx ( $65.6 \%$ ). Fishr median ranking is fifth, with a mean ranking of 5.6. These additional results were not included in the main paper due to space constraints and also because this 'Training-domain' model selection has three clear limitations.</p>
<p>First, learning causal mechanisms can be useless in this 'Training-domain' setup. Indeed, when the correlations are more predictive in training than the causal features, the variant model may be selected over the invariant one. This explains the poor results for all methods in 'Training-domain' Colored MNIST, where the color information is more predictive than the shape information in training. The best model on this task is ARM (Zhang et al., 2020) that uses test time adaptation - thus in a sense uses information from the test-domain - and whose contribution is mostly complementary to ours.</p>
<p>Second, the 'Training-domain' setup suffers from underspecification: "predictors with equivalently strong held-out performance in the training domain [...] can behave very differently" in test (D'Amour et al., 2020). This underspecification favors low regularization thus low values of $\lambda$. To select the model with the best generalization properties, future benchmarks may consider the training calibration (Wald et al., 2021) rather than merely selecting the model with the best training accuracy.</p>
<p>Third, the 'Test-domain' model selection is more realistic for real applications. Indeed, one user would easily label some samples to validate the efficiency of its algorithm. It's not realistic to believe that the users would simply deploy their new algorithm without at least checking that the performances are correct. We recall that the 'Test-domain' setup in DomainBed benchmark is quite restricting, allowing only one evaluation per choice of hyperparameters, without early-stopping.</p>
<p>That's why Teney et al. (2021) even states that "OOD performance cannot, by definition, be performed with a validation set from the same distribution as the training data". Both opinions being reasonable and arguable, we included 'Trainingdomain' results for the sake of completeness, where Fishr remains stronger than ERM. Yet, our state-of-the-art results on the 'Test-domain' setup from Table 4 alone are sufficient to prove the usefulness of our approach for real-world applications.</p>
<h1>D.3. Fishr component analysis on DomainBed</h1>
<h2>D.3.1. FOCUS ON THE EXPONENTIAL MOVING AVERAGE</h2>
<p>Following Le Roux et al. (2011), we use an exponential moving average (ema) parameterized by $\gamma$ for computing gradient variances in DomainBed: the closer $\gamma$ is to 1 , the longer a batch will impact the variance from later steps. We now further analyze the impact of this strategy, which is not specific to Fishr and was used previously in other works (Nam et al., 2020; Blanchard et al., 2021; Zhang et al., 2021) for OOD generalization. Notably, this ema strategy could be applied to better estimate domain-level empirical risks in V-REx (Krueger et al., 2021). For a fair comparison, we introduce a new approach — V-REx with ema - that penalizes $\left|\mathcal{R}<em B="B">{A}^{t}-\hat{\mathcal{R}}</em>}^{t}\right|^{2}$ at step $t$ where $\hat{\mathcal{R}<em e="e">{e}^{t}=\gamma \mathcal{R}</em>={A, B}$.}^{t-1}+(1-\gamma) \mathcal{R}_{e}^{t}$ when $\mathcal{E</p>
<p>Thus, we compare V-REx and Fishr, with $\gamma=0(\boldsymbol{\mathcal { K }})$ or with $\gamma \sim \operatorname{Uniform}(0.9,0.99)(\checkmark$, as described in Table 8). On the</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Sorbonne Université, CNRS, LIP6, Paris, France ${ }^{2}$ Valeo.ai. Correspondence to: Alexandre Ramé &lt;alexandre.rame@sorbonneuniversite.fr $&gt;$.</p>
<p>Proceedings of the $39^{\text {th }}$ International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
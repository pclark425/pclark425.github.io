<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9943 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9943</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9943</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-50ee11e0d057defe6c449099e01b12cf3cb74446</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/50ee11e0d057defe6c449099e01b12cf3cb74446" target="_blank">Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is found that LLM-judges have powerful implicit biases, prioritizing style over factuality and safety, and the supervised fine-tuning stage of post-training, and not the PO stage, has the greatest impact on alignment, with data scaling and prompt diversity as the driving factors.</p>
                <p><strong>Paper Abstract:</strong> The release of ChatGPT in November 2022 sparked an explosion of interest in post-training and an avalanche of new preference optimization (PO) methods. These methods claim superior alignment by virtue of better correspondence with human pairwise preferences, often measured by LLM-judges. In this work, we attempt to answer the following question -- do LLM-judge preferences translate to progress on other, more concrete metrics for alignment, and if not, why not? We define a concrete metric for alignment, and introduce SOS-Bench (Substance Outweighs Style Benchmark), which is to the best of our knowledge the largest standardized, reproducible LLM meta-benchmark to date. We find that (1) LLM-judge preferences do not correlate with concrete measures of safety, world knowledge, and instruction following; (2) LLM-judges have powerful implicit biases, prioritizing style over factuality and safety; and (3) the supervised fine-tuning (SFT) stage of post-training, and not the PO stage, has the greatest impact on alignment, with data scaling and prompt diversity as the driving factors. Our codebase and complete results can be found at https://github.com/penfever/sos-bench.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9943.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9943.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Style-over-Substance Bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-judge Prioritization of Style over Factuality and Safety</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds that LLMs used as judges strongly prioritize stylistic attributes (tone, verbosity, politeness, didactic style) when producing overall preferences, often at the expense of factual correctness and safety considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Alignment benchmarking / open-ended assistant responses</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Arena-Hard-Auto judges (originally GPT-4-1106-preview; in experiments substituted GPT-4o-mini and a panel including gpt-3.5-turbo-1106, gpt-4o-mini-2024-07-18, gpt-4o-2024-08-06, claude-3-5-sonnet-20241022)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Arena-Hard-Auto style pairwise preference pipeline: judge generates its own response, then compares two model outputs using an overall preference plus fine-grained criteria (completeness, conciseness, style, safety, correctness); chain-of-thought prompting used for consistent judgments; scores aggregated via Bradley-Terry.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Not conducted as part of new experiments in this paper; the paper cites prior work (Li et al., 2024c and others) establishing that Arena-Hard style judges correlate with human pairwise preferences, but detailed human-annotator procedures are not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Correlation between fine-grained factor scores and overall judge score (Pearson's R). Style has nearly perfect correlation with overall score: Pearson's R ≈ 0.999 (Table 2, Figure 2).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>The principal loss is a shift in what the metric measures: LLM-judge evaluations become dominated by style, so improvements measured by LLM-judges may reflect stylistic reward-hacking rather than real gains in factuality, safety, or substantive correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>An 8B fine-tuned model ranked above GPT-4 because it produced verbose, blandly polite, didactic responses — a stylistic pattern that LLM-judges favored. Figure 2 and Table 2 show style nearly perfectly predicts overall score across judges.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The authors acknowledge prior evidence that Arena-Hard and similar LLM-judge pipelines correlate with human pairwise preferences (cited work), so LLM-judges are not wholly unaligned with humans; however the authors stress that these correlations do not imply LLM-judges measure the same substantive qualities as static, ground-truth benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Abstract; Section 3 (LLM-Judge benchmarks); Section 4 (Figure 2, Table 2); Discussion/Recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9943.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9943.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Implicit Reweighting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit Reweighting of Explicit Judging Criteria by LLM-Judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When given explicit multi-criteria judging instructions, LLM-judges do not weight those criteria equally; instead they implicitly upweight some (style, completeness) and downweight others (conciseness, sometimes correctness/safety), yielding overall preferences that diverge from an intended balanced metricset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Alignment benchmarking / multi-criteria evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Same Arena-Hard-Auto judges as above; panel used in ablations included gpt-3.5-turbo-1106, gpt-4o-mini-2024-07-18, gpt-4o-2024-08-06, claude-3-5-sonnet-20241022</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Judges asked to give overall preference plus individual ratings on five explicit criteria (completeness, conciseness, style, safety, correctness); then correlations between criteria ratings and overall score computed.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No parallel human multi-criteria crowd experiment reported in this paper; authors compare LLM-judge behavior to what might be expected from humans conceptually and cite prior human preference results.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman and Pearson correlations between each fine-grained factor and the overall score (Table 2). Style: Spearman=1, Pearson≈0.999; Completeness Pearson≈0.963; Correctness Pearson≈0.881; Safety Pearson≈0.727; Conciseness Pearson≈0.114.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of intended, transparent weighting of criteria — LLM-judges introduce implicit, strong inductive biases that re-prioritize the evaluation objective (favoring stylistic features) and thereby obscure which model capabilities truly improved.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Across a panel of judges the style factor predicted overall score almost perfectly (Table 2), indicating that even when correctness and safety are requested as explicit criteria their influence on overall judgment is substantially reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Reweighting patterns were consistent across the judges tested (low standard deviation in Table 2), but the authors caution that changing judge templates, baseline choices, or judge models could change the implicit weighting—i.e., this finding is robust for their studied configurations but not claimed universal.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 4.1; Figure 2; Table 2</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9943.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9943.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stylistic Penalty vs Factual Leniency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disproportionate Penalization of Stylistic Violations and Relative Leniency Towards Factual Errors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic interventions show LLM-judges penalize stylistic deviations (e.g., sarcasm, extreme concision) far more heavily than they penalize factual errors, producing counterintuitive loss patterns in judged scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Alignment benchmarking / sensitivity to response perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o-mini used as the transforming model and judge in many ablations; original Arena-Hard protocol uses GPT-4 variants as judges.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Authors altered model outputs systematically (made responses bland, introduced factual errors, compressed to concise answers, or rewrote with sarcastic tone) and recomputed LLM-judge scores with the pipeline unchanged; judged loss reported as percent change.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No direct human re-evaluation for these transformed responses reported in this paper; transformations were created automatically (GPT-4o-mini) and then judged by LLM-judges.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Loss reported as percent reduction in Arena-Hard score relative to base (Table 3): Bland = 8% loss, Wrong (factual error) = 13% loss, Concise = 63% loss, Sarcastic = 96% loss.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as judges can mis-prioritize severity: minor stylistic changes that humans might consider acceptable (e.g., a sarcastic but factually correct tone, or concise correct answers) are heavily penalized, while substantive factual errors may only produce modest penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Sarcastic rewrite of responses (non-offensive) produced ~96% loss (Table 3), while introducing salient factual errors produced only ~13% loss — indicating judges weight stylistic violations far more heavily than correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Authors note these interventions were generated by GPT-4o-mini (not human annotators) and that they explored a limited set of violation types; they caution further work is needed to generalize findings and that different judge templates or judge models could yield different sensitivities.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 4.2; Table 3; Appendix references mentioned in Section 4.2</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9943.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9943.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Weak Tracking of Static Benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Poor Correlation Between LLM-Judge Preference Benchmarks and Static, Ground-Truth Benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-judge preference benchmarks (e.g., Arena-Hard, Alpaca Eval, MT-Bench) correlate well with each other and with human pairwise preferences to some degree, but they show weak correlation with a variety of established static benchmarks that measure objective factors (world knowledge, instruction following, safety).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Benchmark evaluation across world knowledge, instruction following, safety; meta-benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Arena-Hard-Auto pipeline judges (GPT-4 variants), other preference benchmark judges referenced (MT-Bench, Alpaca Eval).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise preference benchmarks aggregated via Bradley-Terry on relatively small test sets (e.g., Arena-Hard ~500 questions); judges compare model outputs pairwise and produce win-rates/Likert scores.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human pairwise preferences are referenced as prior validation for some LLM-judge pipelines (Li et al., 2024c), but detailed human evaluation protocols are not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson's R reported for benchmark pairs (Table 1): LiveBench vs HELM = 0.94; Arena-Hard vs ChatBot Arena = 0.81; HELM vs Arena-Hard = 0.40; LiveBench vs Arena-Hard = 0.51 (indicating weak-to-moderate correlations between preference benchmarks and static benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM-judge-driven improvements can be misleading: methods that score well under LLM-judges do not necessarily show commensurate gains on objective tasks measuring world knowledge, instruction following, or safety, so using LLM-judges as primary progress measures can hide regressions in substantive capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Meta-analysis (SOS-BENCH) shows LLM-judge scores are poor predictors of performance on the large, ground-truth meta-benchmark of world knowledge/instruction-following/safety; Arena-Hard behaves as an outlier compared to SOS-BENCH trends (Section 6, Table 5, Figure 3 commentary).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The paper concedes that preference benchmarks correlate with human pairwise preferences (cited prior work), and that different components in the LLM-judge pipeline (choice of judge, template, baseline) can substantially affect outcomes, so mismatch is not absolute or inevitable.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 3 (Table 1); Section 6 results and discussion about Arena-Hard being an outlier vs SOS-BENCH</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9943.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9943.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pipeline/Operational Losses</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Operational and Structural Losses from Replacing Human Evaluation with LLM-Judges (Opacity, Lack of Ground Truth, Small Coverage, Confounds)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The LLM-judge evaluation pipeline replaces explicit, deterministic, reference-based metrics with opaque, model-based comparison judgments, introducing several losses: loss of explainability, loss of verifiable ground truth, limited domain coverage (small test sets), and susceptibility to judging-template confounds and unstated implicit biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Benchmark methodology / meta-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>General description of LLM judges (e.g., GPT-4 family, GPT-4o-mini, Claude) as used in Arena-Hard-Auto and similar benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLM-judges operate in a pairwise comparison setup, require specification of both metric and judging criteria (which may be underspecified), and have higher per-sample unit cost leading to smaller, broader test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Contrasted implicitly: traditional static benchmarks are reference-based, deterministic, and allow objective aggregation over many questions in narrow domains; human evaluations (when used) provide ground-truth references or calibrated subjective judgments across many items (detailed protocols vary and are not repro'd here).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Qualitative: the paper enumerates structural distinctions and confounds rather than a single numeric metric; empirical consequences are shown in correlations, factor reweighting, and intervention losses (Tables 1-3, Figures 1-3).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Losses include: (1) interpretability—judgments are opaque and hard to ablate; (2) lack of verifiable ground-truth labels—comparison-based scores can mask substantive errors; (3) lower coverage—small, expensive testsets reduce domain-specific diagnostic power; (4) susceptibility to implicit judge biases and judging-template design choices; (5) reproducibility and high cost concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Figure 1 diagrams these pipeline-level confounds; authors note that Arena-Hard uses only ~500 questions and that the judge template and baseline choice are among the most impactful factors (Section 3). The recommendation section argues these operational issues render current LLM-judge benchmarks unsuitable as primary alignment metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Authors note that some of these limitations (cost and small testset size) may alleviate as LLMs become cheaper, and that LLM-judges do provide a useful proxy for average human pairwise preferences in some contexts; they recommend standardized judging templates and explicit biases to mitigate these losses.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Figure 1 and associated text in Section 3; Section 7 (Recommendations); Section 9 (Impact / Limitations)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>LLMs instead of human judges? a large scale empirical study across 20 nlp evaluation tasks <em>(Rating: 2)</em></li>
                <li>LLM evaluators recognize and favor their own generations <em>(Rating: 2)</em></li>
                <li>Length-controlled alpacaeval: A simple way to debias automatic evaluators <em>(Rating: 2)</em></li>
                <li>Humans or llms as the judge? a study on judgement biases <em>(Rating: 2)</em></li>
                <li>Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9943",
    "paper_id": "paper-50ee11e0d057defe6c449099e01b12cf3cb74446",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Style-over-Substance Bias",
            "name_full": "LLM-judge Prioritization of Style over Factuality and Safety",
            "brief_description": "The paper finds that LLMs used as judges strongly prioritize stylistic attributes (tone, verbosity, politeness, didactic style) when producing overall preferences, often at the expense of factual correctness and safety considerations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Alignment benchmarking / open-ended assistant responses",
            "llm_judge_model": "Arena-Hard-Auto judges (originally GPT-4-1106-preview; in experiments substituted GPT-4o-mini and a panel including gpt-3.5-turbo-1106, gpt-4o-mini-2024-07-18, gpt-4o-2024-08-06, claude-3-5-sonnet-20241022)",
            "llm_judge_setup": "Arena-Hard-Auto style pairwise preference pipeline: judge generates its own response, then compares two model outputs using an overall preference plus fine-grained criteria (completeness, conciseness, style, safety, correctness); chain-of-thought prompting used for consistent judgments; scores aggregated via Bradley-Terry.",
            "human_evaluation_setup": "Not conducted as part of new experiments in this paper; the paper cites prior work (Li et al., 2024c and others) establishing that Arena-Hard style judges correlate with human pairwise preferences, but detailed human-annotator procedures are not reported here.",
            "agreement_metric": "Correlation between fine-grained factor scores and overall judge score (Pearson's R). Style has nearly perfect correlation with overall score: Pearson's R ≈ 0.999 (Table 2, Figure 2).",
            "losses_identified": "The principal loss is a shift in what the metric measures: LLM-judge evaluations become dominated by style, so improvements measured by LLM-judges may reflect stylistic reward-hacking rather than real gains in factuality, safety, or substantive correctness.",
            "examples_of_loss": "An 8B fine-tuned model ranked above GPT-4 because it produced verbose, blandly polite, didactic responses — a stylistic pattern that LLM-judges favored. Figure 2 and Table 2 show style nearly perfectly predicts overall score across judges.",
            "counterexamples_or_caveats": "The authors acknowledge prior evidence that Arena-Hard and similar LLM-judge pipelines correlate with human pairwise preferences (cited work), so LLM-judges are not wholly unaligned with humans; however the authors stress that these correlations do not imply LLM-judges measure the same substantive qualities as static, ground-truth benchmarks.",
            "paper_reference": "Abstract; Section 3 (LLM-Judge benchmarks); Section 4 (Figure 2, Table 2); Discussion/Recommendations",
            "uuid": "e9943.0",
            "source_info": {
                "paper_title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Implicit Reweighting",
            "name_full": "Implicit Reweighting of Explicit Judging Criteria by LLM-Judges",
            "brief_description": "When given explicit multi-criteria judging instructions, LLM-judges do not weight those criteria equally; instead they implicitly upweight some (style, completeness) and downweight others (conciseness, sometimes correctness/safety), yielding overall preferences that diverge from an intended balanced metricset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Alignment benchmarking / multi-criteria evaluation",
            "llm_judge_model": "Same Arena-Hard-Auto judges as above; panel used in ablations included gpt-3.5-turbo-1106, gpt-4o-mini-2024-07-18, gpt-4o-2024-08-06, claude-3-5-sonnet-20241022",
            "llm_judge_setup": "Judges asked to give overall preference plus individual ratings on five explicit criteria (completeness, conciseness, style, safety, correctness); then correlations between criteria ratings and overall score computed.",
            "human_evaluation_setup": "No parallel human multi-criteria crowd experiment reported in this paper; authors compare LLM-judge behavior to what might be expected from humans conceptually and cite prior human preference results.",
            "agreement_metric": "Spearman and Pearson correlations between each fine-grained factor and the overall score (Table 2). Style: Spearman=1, Pearson≈0.999; Completeness Pearson≈0.963; Correctness Pearson≈0.881; Safety Pearson≈0.727; Conciseness Pearson≈0.114.",
            "losses_identified": "Loss of intended, transparent weighting of criteria — LLM-judges introduce implicit, strong inductive biases that re-prioritize the evaluation objective (favoring stylistic features) and thereby obscure which model capabilities truly improved.",
            "examples_of_loss": "Across a panel of judges the style factor predicted overall score almost perfectly (Table 2), indicating that even when correctness and safety are requested as explicit criteria their influence on overall judgment is substantially reduced.",
            "counterexamples_or_caveats": "Reweighting patterns were consistent across the judges tested (low standard deviation in Table 2), but the authors caution that changing judge templates, baseline choices, or judge models could change the implicit weighting—i.e., this finding is robust for their studied configurations but not claimed universal.",
            "paper_reference": "Section 4.1; Figure 2; Table 2",
            "uuid": "e9943.1",
            "source_info": {
                "paper_title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Stylistic Penalty vs Factual Leniency",
            "name_full": "Disproportionate Penalization of Stylistic Violations and Relative Leniency Towards Factual Errors",
            "brief_description": "Systematic interventions show LLM-judges penalize stylistic deviations (e.g., sarcasm, extreme concision) far more heavily than they penalize factual errors, producing counterintuitive loss patterns in judged scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Alignment benchmarking / sensitivity to response perturbations",
            "llm_judge_model": "GPT-4o-mini used as the transforming model and judge in many ablations; original Arena-Hard protocol uses GPT-4 variants as judges.",
            "llm_judge_setup": "Authors altered model outputs systematically (made responses bland, introduced factual errors, compressed to concise answers, or rewrote with sarcastic tone) and recomputed LLM-judge scores with the pipeline unchanged; judged loss reported as percent change.",
            "human_evaluation_setup": "No direct human re-evaluation for these transformed responses reported in this paper; transformations were created automatically (GPT-4o-mini) and then judged by LLM-judges.",
            "agreement_metric": "Loss reported as percent reduction in Arena-Hard score relative to base (Table 3): Bland = 8% loss, Wrong (factual error) = 13% loss, Concise = 63% loss, Sarcastic = 96% loss.",
            "losses_identified": "Using LLMs as judges can mis-prioritize severity: minor stylistic changes that humans might consider acceptable (e.g., a sarcastic but factually correct tone, or concise correct answers) are heavily penalized, while substantive factual errors may only produce modest penalties.",
            "examples_of_loss": "Sarcastic rewrite of responses (non-offensive) produced ~96% loss (Table 3), while introducing salient factual errors produced only ~13% loss — indicating judges weight stylistic violations far more heavily than correctness.",
            "counterexamples_or_caveats": "Authors note these interventions were generated by GPT-4o-mini (not human annotators) and that they explored a limited set of violation types; they caution further work is needed to generalize findings and that different judge templates or judge models could yield different sensitivities.",
            "paper_reference": "Section 4.2; Table 3; Appendix references mentioned in Section 4.2",
            "uuid": "e9943.2",
            "source_info": {
                "paper_title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Weak Tracking of Static Benchmarks",
            "name_full": "Poor Correlation Between LLM-Judge Preference Benchmarks and Static, Ground-Truth Benchmarks",
            "brief_description": "LLM-judge preference benchmarks (e.g., Arena-Hard, Alpaca Eval, MT-Bench) correlate well with each other and with human pairwise preferences to some degree, but they show weak correlation with a variety of established static benchmarks that measure objective factors (world knowledge, instruction following, safety).",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Benchmark evaluation across world knowledge, instruction following, safety; meta-benchmarking",
            "llm_judge_model": "Arena-Hard-Auto pipeline judges (GPT-4 variants), other preference benchmark judges referenced (MT-Bench, Alpaca Eval).",
            "llm_judge_setup": "Pairwise preference benchmarks aggregated via Bradley-Terry on relatively small test sets (e.g., Arena-Hard ~500 questions); judges compare model outputs pairwise and produce win-rates/Likert scores.",
            "human_evaluation_setup": "Human pairwise preferences are referenced as prior validation for some LLM-judge pipelines (Li et al., 2024c), but detailed human evaluation protocols are not reproduced here.",
            "agreement_metric": "Pearson's R reported for benchmark pairs (Table 1): LiveBench vs HELM = 0.94; Arena-Hard vs ChatBot Arena = 0.81; HELM vs Arena-Hard = 0.40; LiveBench vs Arena-Hard = 0.51 (indicating weak-to-moderate correlations between preference benchmarks and static benchmarks).",
            "losses_identified": "LLM-judge-driven improvements can be misleading: methods that score well under LLM-judges do not necessarily show commensurate gains on objective tasks measuring world knowledge, instruction following, or safety, so using LLM-judges as primary progress measures can hide regressions in substantive capabilities.",
            "examples_of_loss": "Meta-analysis (SOS-BENCH) shows LLM-judge scores are poor predictors of performance on the large, ground-truth meta-benchmark of world knowledge/instruction-following/safety; Arena-Hard behaves as an outlier compared to SOS-BENCH trends (Section 6, Table 5, Figure 3 commentary).",
            "counterexamples_or_caveats": "The paper concedes that preference benchmarks correlate with human pairwise preferences (cited prior work), and that different components in the LLM-judge pipeline (choice of judge, template, baseline) can substantially affect outcomes, so mismatch is not absolute or inevitable.",
            "paper_reference": "Section 3 (Table 1); Section 6 results and discussion about Arena-Hard being an outlier vs SOS-BENCH",
            "uuid": "e9943.3",
            "source_info": {
                "paper_title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Pipeline/Operational Losses",
            "name_full": "Operational and Structural Losses from Replacing Human Evaluation with LLM-Judges (Opacity, Lack of Ground Truth, Small Coverage, Confounds)",
            "brief_description": "The LLM-judge evaluation pipeline replaces explicit, deterministic, reference-based metrics with opaque, model-based comparison judgments, introducing several losses: loss of explainability, loss of verifiable ground truth, limited domain coverage (small test sets), and susceptibility to judging-template confounds and unstated implicit biases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Benchmark methodology / meta-evaluation",
            "llm_judge_model": "General description of LLM judges (e.g., GPT-4 family, GPT-4o-mini, Claude) as used in Arena-Hard-Auto and similar benchmarks.",
            "llm_judge_setup": "LLM-judges operate in a pairwise comparison setup, require specification of both metric and judging criteria (which may be underspecified), and have higher per-sample unit cost leading to smaller, broader test sets.",
            "human_evaluation_setup": "Contrasted implicitly: traditional static benchmarks are reference-based, deterministic, and allow objective aggregation over many questions in narrow domains; human evaluations (when used) provide ground-truth references or calibrated subjective judgments across many items (detailed protocols vary and are not repro'd here).",
            "agreement_metric": "Qualitative: the paper enumerates structural distinctions and confounds rather than a single numeric metric; empirical consequences are shown in correlations, factor reweighting, and intervention losses (Tables 1-3, Figures 1-3).",
            "losses_identified": "Losses include: (1) interpretability—judgments are opaque and hard to ablate; (2) lack of verifiable ground-truth labels—comparison-based scores can mask substantive errors; (3) lower coverage—small, expensive testsets reduce domain-specific diagnostic power; (4) susceptibility to implicit judge biases and judging-template design choices; (5) reproducibility and high cost concerns.",
            "examples_of_loss": "Figure 1 diagrams these pipeline-level confounds; authors note that Arena-Hard uses only ~500 questions and that the judge template and baseline choice are among the most impactful factors (Section 3). The recommendation section argues these operational issues render current LLM-judge benchmarks unsuitable as primary alignment metrics.",
            "counterexamples_or_caveats": "Authors note that some of these limitations (cost and small testset size) may alleviate as LLMs become cheaper, and that LLM-judges do provide a useful proxy for average human pairwise preferences in some contexts; they recommend standardized judging templates and explicit biases to mitigate these losses.",
            "paper_reference": "Figure 1 and associated text in Section 3; Section 7 (Recommendations); Section 9 (Impact / Limitations)",
            "uuid": "e9943.4",
            "source_info": {
                "paper_title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "LLMs instead of human judges? a large scale empirical study across 20 nlp evaluation tasks",
            "rating": 2
        },
        {
            "paper_title": "LLM evaluators recognize and favor their own generations",
            "rating": 2
        },
        {
            "paper_title": "Length-controlled alpacaeval: A simple way to debias automatic evaluators",
            "rating": 2
        },
        {
            "paper_title": "Humans or llms as the judge? a study on judgement biases",
            "rating": 2
        },
        {
            "paper_title": "Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates",
            "rating": 1
        }
    ],
    "cost": 0.01630575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking</h1>
<p>Benjamin Feuer ${ }^{1,2}$, Micah Goldblum ${ }^{3}$, Teresa Datta ${ }^{1}$, Sanjana Nambiar ${ }^{2}$, Raz Besaleli, Samuel<br>Dooley, Max Cembalest, John P. Dickerson ${ }^{1}$<br>${ }^{1}$ Arthur AI, ${ }^{2}$ NYU, ${ }^{3}$ Columbia University</p>
<h4>Abstract</h4>
<p>The release of ChatGPT in November 2022 sparked an explosion of interest in post-training and an avalanche of new preference optimization (PO) methods. These methods claim superior alignment by virtue of better correspondence with human pairwise preferences, often measured by LLM-judges. In this work, we attempt to answer the following question - do LLM-judge preferences translate to progress on other, more concrete metrics for alignment, and if not, why not? We define a concrete metric for alignment, and introduce SOS-BENCH (Substance Outweighs Style Benchmark), the largest standardized, reproducible LLM meta-benchmark to date. We find that (1) LLM-judge preferences do not correlate with concrete measures of safety, world knowledge, and instruction following; (2) LLM-judges have powerful implicit biases, prioritizing style over factuality and safety; and (3) the supervised fine-tuning (SFT) stage of posttraining has a large impact on alignment, with data scaling and prompt diversity as the driving factors. Our codebase and complete results can be found at https://github.com/penfever/sos-bench.</p>
<h2>1 INTRODUCTION</h2>
<p>The release of ChatGPT in November 2022 sparked an explosion of interest in post-training and an avalanche of new preference optimization (PO) methods and data curation strategies for supervised fine tuning (SFT). Many of these recent works evaluate primarily or exclusively using LLM-judge preference benchmarks such as MT-Bench, Alpaca Eval, and Arena-Hard-Auto (Dubois et al., 2024; Li et al., 2024c; Zheng et al., 2023). Such benchmarks employ LLM-judges that are intended to automate evaluation and align with human preferences.</p>
<p>These are becoming standard in part because they are seen as robust predictors of a wide range of desirable downstream model behaviors. In abstracts of recent works which solely or primarily report PO benchmark scores, authors claim that their methods improve instruction following, reduce harm, improve reasoning, boost response accuracy, and result in higher language generation quality (Meng et al., 2024; Wang et al., 2024a; Wu et al., 2024a; Hong et al., 2024). Implicit in these claims rests an assumption that the Bradley-Terry model, which converges to a local minimum over undifferentiated, pairwise preferences, will necessarily and naturally converge on these desiderata as well. ${ }^{1}$</p>
<p>Is this actually the case? Do LLM-judge preferences translate to progress on other, more concrete metrics for alignment, and if not, why not?</p>
<p>Our contributions are as follows -</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>We establish a common framework for systematically evaluating LLM-judge pipelines and for analyzing potential confounds at each stage of these pipelines.</li>
<li>We find that LLM-judges for alignment benchmarking have powerful implicit biases. Specifically, they prioritize stylistic preferences over other important considerations, like factuality and safety.</li>
<li>As a complement to LLM-judge benchmarks, we introduce SOS-Bench, a new alignment benchmark with ground truth, designed to gauge progress on alignment with helpful, honest, harmless (HHH) principles (Askell et al., 2021).</li>
<li>We conduct (to the best of our knowledge) the largest controlled meta-analysis of publicly available post-training methods to date, and show that data scaling in the SFT stage as well as prompt diversity are the most important predictors of improved alignment.</li>
</ul>
<p>We conclude with some practical recommendations for the research community.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The LLM-judge pipeline introduces new potential confounds in evaluation, compared to standard benchmarks. We diagram the LLM-judge pipeline for alignment benchmarking and observe that it is more complex than that of most standard benchmarks; (a) it replaces an explainable, deterministic metric with an opaque LLM-judge. (b) it does not attempt to establish any verifiable ground truth. (c) it contains a relatively small number of questions covering an very wide range of topics, resulting in limited coverage of any particular knowledge domain. (d) it introduces novel confounds in the form of the judging template (explicit bias) and the judge's unstated internal preferences (implicit bias).</p>
<h1>2 Preliminaries</h1>
<p>Post-training. Post-training, popularized by ChatGPT, consists of all parametric updates to the model after the pretraining run is completed. The conceptual goal of post-training is transforming a text completion model into a useful AI assistant. It typically encompasses two stages: supervised instruction fine-tuning (SFT) and preference optimization (PO).
LLM judges. LLM judges are LLMs which are prompted to generate decisions over content; in the case of the benchmarks we consider in this paper, we are particularly interested in preference judges, which attempt to approximate human pairwise preferences over model outputs.
Established benchmarks of model alignment. There are at least two distinct trends in benchmarking model alignment which are commonly used today. The first, older method, is to assemble a large test set of either static or dynamically updated questions with ground truth answers, and evaluate models against them. Stanford's HELM benchmark, HuggingFace's Open LLM Leaderboards, and Abacus AI's LiveBench are examples of this approach (Liang et al., 2023; White et al., 2024).</p>
<p>Model authors will often aggregate such measures into large scale comparisons. In the next section, we describe in detail the second and most commonly used style of benchmarks in the alignment literature, LLM-judge benchmarks.</p>
<h1>3 LLM-JuDGE Preference Benchmarks</h1>
<p>LLM-judge preference benchmarks (hereafter referred to as LLM-judge benchmarks), such as Arena-Hard-Auto, Alpaca Eval, and MT Bench, have become quite popular since their introduction shortly after the initial release of Llama. Today, many papers report solely or primarily on these benchmarks. This abrupt surge in their use invites closer consideration of their properties and design.</p>
<p>The Arena-Hard-Auto Pipeline. For convenience, we summarize the steps in the arena-hard-auto judgment pipeline here. The chosen LLM judge conducts a pairwise comparison against a baseline model (GPT-4-0314 in the original paper), scoring outputs on 5-point Likert scale. The judge generates its own response to the question before judging, and uses chain-of-thought prompting for consistent judgments. The paper implements a two-game setup to avoid position bias, aggregating 1000 judgments per model using Bradley-Terry, resulting in final scores and confidence intervals through bootstrapping. This judgment pipeline has been shown to have strong correlation with the judgments of humans Li et al. (2024c) $95 \%$ confidence intervals are reported as part of the benchmark; these can be as high as $4 \%$ for judgments close to the 50th percentile score. However, we believe that these uncertainty estimates are too conservative - therefore, in Appendix I.3, we independently ablate these choices for a single judge, and find that the choice of baseline and judge template are the most impactful factors. Surprisingly, we find that replacing the carefully filtered Arena-hard questions with questions from popular non-technical subreddits such as AskHistorians has little effect on the ranking, suggesting that LLM judge scores are produced in a manner only modestly dependent on the knowledge domain.</p>
<p>Pairwise preference benchmark scores are inconsistent with static benchmarks, but consistent with each other. In Table 1 we compare the ranking of eight top-performing LLMs on LiveBench, HELM, Arena-Hard-Auto, and ChatBot Arena (White et al., 2024; Liang et al., 2023). We find that LLM-judges of alignment indeed have preferences that closely (albeit imperfectly) correlate with those of humans, but that their correlation with static benchmarks is weak. Similar effects can be observed on the leaderboard of BenchBench, a recent paper which aims to standardize benchmark agreement testing. (Perlitz et al., 2024). When we aggregate using standard benchmarks (HELM, HuggingFace OpenLLM Leaderboard 2, LiveBench and OpenCompass), the highest overall BenchBench score is 2.2, and the highest pairwise preference score is 0.69 . Conversely, if we instead aggregate using preference benchmarks (Alpaca Eval, MT-Bench, Arena-Hard, Chatbot Arena) the highest overall score is 1.8 , and the highest standard benchmark score is 1.4. Such measures, however, cannot tell us which benchmark regime we ought to trust more, or why (Perlitz et al., 2024). In order to understand this point better, we introduce a framework for interpreting and comparing them to one another.</p>
<p>Towards a universal framework for LLM-judge benchmarks. We observe that all pipelines which employ LLMs as surrogates for human pairwise preferences necessarily make use of certain components. In order to facilitate analysis of these systems, we diagram their common architectural flow in Figure 1.</p>
<p>From this, it quickly becomes clear that there are several key structural distinctions to be made between LLM-judge benchmarks and standard NLP benchmarks:</p>
<ul>
<li>Most standard benchmark metrics are model-free; all LLM-judge benchmarks require a judge model. The most commonly used standard benchmarking metrics, such as BLEU, are model-free. While both the choice of metric and the choice of judge can be a potential confound, the opacity of LLMs makes their behavior much more challenging to interpret, compared to deterministic model-free metrics. Despite this, it is not yet standard practice to ablate this choice, or ensemble across judges, most likely for reasons of cost.</li>
<li>Standard benchmark scores are reference-based; LLM-judge benchmarks are comparison-based. The choice of baseline response to which assistants are compared</li>
</ul>
<p>represents another potential confound, as pairwise preferences over texts do not obey the transitive property. ${ }^{2}$</p>
<ul>
<li>Standard benchmarks contain many questions on a narrow subject; LLM-judge benchmarks contain a small number of questions on a wide range of subjects. Because of their high unit costs, LLM-judge benchmarks are smaller than standard NLP benchmarks like GLUE and MMLU; Arena-Hard has a test set of 500 questions. The authors have justified this choice by demonstrating that their benchmarks nevertheless correlate strongly with preference reports on ChatBot Arena. But this does not guarantee that a preference score on a broad range of topics and people will correlate well with individual use cases, as capabilities are vector-valued, not scalar-valued. To the extent that strong correlations are achievable, they would only be so if the judgment criteria were consistent across all tasks and all judges, which would tend to favor stylistic factors.</li>
<li>Standard benchmarks specify a metric, but LLM-judge benchmarks must specify both a metric and judging criteria. This introduces a new potential confound not present in standard benchmarks; the instructions to the judge may be underspecified, unclear, or may simply not reflect best practices in model alignment with human preference.</li>
</ul>
<p>While the first three concerns might be expected to ameliorate over time, as LLMs become less expensive to operate, the fourth concern is foundational - there is no way for a judge to complete a preference-selection task without some degree of inductive bias (Ethayarajh et al., 2024). We observe that such a bias may be explicit (i.e., it may be introduced via the instructions to the judge) or implicit (i.e., representing a fixed value system the judge brings to the task either independently of, or in violation of, the instructions). A reasonable desideratum for an objective LLM-judge benchmark would be to make as many biases as possible explicit, and to curb the use of implicit bias. ${ }^{3}$ In service of this goal, we devote our next section to developing an understanding of implicit bias in LLM judges.</p>
<p>Table 1: Pairwise preference benchmarks do not track established benchmarks. We report Pearson's R as a measure of the strength of correlation between pairs of traditional benchmarks and pairwise preference benchmarks. LiveBench and HELM are strongly correlated, as are Arena-Hard and ChatBot Arena. All other pairs show comparatively weaker correlation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark Pair</th>
<th style="text-align: left;">Correlation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LiveBench, HELM</td>
<td style="text-align: left;">0.94</td>
</tr>
<tr>
<td style="text-align: left;">Arena-Hard, ChatBot Arena</td>
<td style="text-align: left;">0.81</td>
</tr>
<tr>
<td style="text-align: left;">HELM, ChatBot Arena</td>
<td style="text-align: left;">0.68</td>
</tr>
<tr>
<td style="text-align: left;">LiveBench, ChatBot Arena</td>
<td style="text-align: left;">0.65</td>
</tr>
<tr>
<td style="text-align: left;">LiveBench, Arena-Hard</td>
<td style="text-align: left;">0.51</td>
</tr>
<tr>
<td style="text-align: left;">HELM, Arena-Hard</td>
<td style="text-align: left;">0.40</td>
</tr>
</tbody>
</table>
<h1>4 IMPLICIT BIAS IN LLM-JUDGES</h1>
<p>Intuitively, we might expect that given a set of judging criteria and no explicit instructions on how to prioritize them, LLMs would assign equal weight to all criteria. However, we find that this is not the case. Rather, LLMs demonstrate powerful implicit biases between judging criteria. They heavily reweight criteria while determining their final preference, all but ignoring some and emphasizing</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>others. LLMs also exhibit implicit bias within judging criteria, with certain kinds of violations scored far more harshly than others.</p>
<p>Experimental setting. We conduct our experiments on a series of post-trained LLAMA-3-8B base models, LLAMA-3 base without post-training, opt-125m, and several GPT checkpoints (Dubey et al., 2024; Brown et al., 2020; Zhang et al., 2022; Xu et al., 2024b; Meng et al., 2024). As of this writing, all of the checkpoints are available on HuggingFace; we provide names and references for all the post-training methods we consider in Appendix C. Our LLM-judge benchmark is Arena-Hard-Auto, from Li et al. (2024c). We choose to make a case study of this LLM-judge benchmark because it is very popular in the literature and it makes some of the strongest claims to alignment with human preference. Unless otherwise noted, we use the standard settings for Arena-Hard-Auto, which as of this writing uses GPT-4-0314 as a baseline model and GPT-4-1106-preview as a judge. For reasons of cost, in Table 3, we substitute gpt-4o-mini-2024-07-18 for the standard judge. In order to conduct our experiment, we also modify the judge template. The judge template we use can be found in Appendix F.2. Following the authors, we report scores in the form of win-rates over a baseline model, and report pairwise preferences in the form of Likert scores.</p>
<h1>4.1 GIVEN EXPLICIT JUDGMENT CRITERIA, LLM-JUDGES IMPLICITLY REWEIGHT THEM</h1>
<p>In order to conduct these experiments, we alter the judge template to provide the judge with explicit biases, while leaving room for implicit ones as well. In addition to an overall preference, we instruct the judge to state a preference on five fine-grained criteria: completeness, conciseness, style, safety, correctness. Correctness and completeness assess honesty, safety assesses harmlessness, and completeness, style and conciseness assess helpfulness.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Judges implicitly reweight explicit criteria. When asked to render an overall judgment using a set of explicit criteria, models will implicitly weight some of those criteria more than others. We report the LLM's overall judgment as Arena-Hard Score, alongside independent LLM judgments of five key factors in the response. Style is perfectly correlated with the overall score (Pearson's R).</p>
<p>In Figure 2, we show that the style score correlates perfectly with the overall score. By contrast, safety scores for most models are nearly identical, and conciseness is only weakly correlated. Given a set of criteria, LLM-judges implicitly favor completeness and style, and this behavior is strongly</p>
<p>conserved across judges: see Table 2. When seen in this light, the unintuitive ranking of an 8B finetune above GPT-4 makes more sense; the fine-tune has produced verbose, didactic, blandly polite responses to every prompt, a set of behaviors we could collectively term stylistic reward hacking. It is important to note that factors are not necessarily independent; we analyze the degree and direction of cross-correlation using factor analysis in Appendix K.</p>
<h1>4.2 LLM-JUDGES IMPLICITLY WEIGHT CRITERIA VIOLATIONS DIFFERENTLY</h1>
<p>In these experiments, we introduce systematic criteria violations into the model responses for the top performing model (Magpie+DPO) and recompute the model's LLM-judge score, while leaving the rest of the pipeline unaffected. We then report the loss, expressed as the percentage difference between the base score and the new score. We hope to gain some indication of whether the model has understood the explicit instructions for each criteria, and how it will weight violations of those criteria. We provide samples of the altered responses in Appendix G.</p>
<p>For all of our interventions, the transforming model was GPT-4o-mini, and it was given instructions not to change or remove any factual claims in the original response. To create our undiverse intervention, we prompted GPT to transform each response and make it as repetitive as possible, eliminating synonyms and unusual words. The exact prompts we use can be found in Appendix H. To create our wrong intervention, the research team reviewed each response and changed one salient fact in the model response; e.g., if a model asserted that a condition always held, we altered it to say that it never held. For our concise intervention, we prompted GPT to compress each response as much as possible. Finally, for our sarcastic response, we instructed GPT to rewrite the responses in an obnoxious and irritating tone (without writing anything offensive or harmful).
The results, in Table 3, show that, far from treating all violations equally, LLM judges are highly critical of unconventional stylistic changes, such as making the assistant's tone sarcastic, but fairly lenient on major factual errors in the response. It is not clear that these implicit biases accurately reflect our priorities in model alignment; sarcasm, while probably not desirable in most cases, is only a minor violation of helpfulness, whereas incorrect answers strongly violate the honesty principle.</p>
<h2>5 SOS-BENCH</h2>
<p>New, objective measures of progress in alignment can help the research community make progress faster. Fortunately, there exist many useful static benchmarks of various aspects of LLM behavior and performance; by categorizing and unifying these disparate works, we can produce a large-scale meta-benchmark to measure progress on certain key aspects of alignment.</p>
<p>SOS-BENCH (Substance Over Style Benchmark) combines 19 existing world knowledge, instruction following, and safety benchmarks for a holistic view of model performance. For the complete list of benchmarks we use, please refer to Table 8. All of the questions in our benchmark contain ground truth answers, and aggregates are reported using the average of normalized accuracies (by the size of the test set), with $95 \%$ confidence intervals.</p>
<p>Table 2: The reweighting is stable across judges. We query a panel of four judges (gpt-3.5-turbo-1106, gpt-4o-mini-2024-07-18, gpt-4o-2024-08-06, claude-3-5-sonnet-20241022) and find that style predicts overall score almost perfectly for every judge in our panel (we report the average correlation and standard deviation).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Factor</th>
<th style="text-align: left;">Spearman</th>
<th style="text-align: left;">Pearson</th>
<th style="text-align: left;">Std</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Style</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Completeness</td>
<td style="text-align: left;">0.964</td>
<td style="text-align: left;">0.963</td>
<td style="text-align: left;">0.04</td>
</tr>
<tr>
<td style="text-align: left;">Correctness</td>
<td style="text-align: left;">0.906</td>
<td style="text-align: left;">0.881</td>
<td style="text-align: left;">0.079</td>
</tr>
<tr>
<td style="text-align: left;">Safety</td>
<td style="text-align: left;">0.827</td>
<td style="text-align: left;">0.727</td>
<td style="text-align: left;">0.147</td>
</tr>
<tr>
<td style="text-align: left;">Conciseness</td>
<td style="text-align: left;">0.097</td>
<td style="text-align: left;">0.114</td>
<td style="text-align: left;">0.128</td>
</tr>
</tbody>
</table>
<p>Comparing SOS-Bench to existing evaluations. All in all, we test models on 152,380 data points; to the best of our knowledge, this is almost 7 x larger than the largest previous open source LLM benchmark which end users can run themselves, HuggingFace's OpenLLM Leaderboard 2. While individual model releases and technical reports also release meta-benchmark results, they suffer from two failings; they are not always reproducible, and the aggregations of results, which are different for each model release, are vulnerable to cherry picking. By combining scale and standardization, we hope to create a reliable, concrete alignment measure.</p>
<p>Table 3: Judges implicitly disfavor certain violations. When a systematic violation is introduced across all responses, judges weight each violation very differently. This effect occurs without any change to the judge's instructions, making it an implicit bias. Sarcasm and conciseness are heavily penalized, while incorrect and bland responses are only weakly penalized.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Intervention</th>
<th style="text-align: center;">Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base</td>
<td style="text-align: center;">$00 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Bland</td>
<td style="text-align: center;">$08 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Wrong</td>
<td style="text-align: center;">$13 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Concise</td>
<td style="text-align: center;">$63 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Sarcastic</td>
<td style="text-align: center;">$96 \%$</td>
</tr>
</tbody>
</table>
<p>Measuring alignment. We subdivide our benchmark into three factors (world knowledge, instruction following, and safety) and report the results for each. For the sake of comparison, we also report results from Arena-Hard-Auto. For results on individual benchmarks, we refer the reader to https://github.com/penfever/sos-bench.</p>
<p>A concrete measure of alignment. No measure of alignment can cover all of the factors worthy of consideration; prior work has variously emphasized accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency, among other factors (Liang et al., 2023). We choose to focus on a representative subset of tasks, namely, the widely disseminated Helpful, Honest and Harmless (HHH) principles (Askell et al., 2021). These principles, popularized by the AI startup Anthropic, have the virtues of being widely recognized and largely uncontroversial, and therefore make a suitable starting point. Concretely, we propose that if model A is better on objective measurements of all three factors with respect to model B, then model A is better aligned than model B. As objective measurements of HHH principles remain aspirational for the time being, we propose the following conceptual mapping;
Model A is more honest than model B IFF it exhibits statistically superior performance on measures of world knowledge. Although there is more to honesty than world knowledge, it is not possible for a model to tell the truth if it does not know what the truth is. Model A is more helpful than model B IFF it exhibits statistically superior performance on measures of instruction following, because a model that correctly understands instructions is always at least as helpful as a model which fails to understand them, all else equal. Model A is more harmless than model B IFF it exhibits statistically superior performance on measures of safety, such as red-teaming or refusal benchmarks.</p>
<h1>6 ReSULtS</h1>
<p>Data scaling improves alignment in the SFT stage of post-training. Contrary to recent work from Zhou et al. (2023a), in Figure 3 we show that when it comes to the SFT stage of model post-training, the scale of data used during post-training and the diversity of prompts, rather than the curation method or several other potential factors we consider, are the strongest predictors of downstream task performance. The only outlier measure is Arena-Hard (Arena), our LLM-Judge benchmark. This observation is consistent with our hypotheses in Section 3 and Section 4.</p>
<p>Interestingly, although all measures of alignment are positively affected by data scaling, the magnitude is much greater for instruction following (helpfulness). This makes intuitive sense, given that enhancing LLM responses to declarative commands is one of the primary goals in alignment.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: More is more in alignment. In the SFT stage of post-training, the size of the dataset, rather than the method used to curate the data, is the strongest predictor of alignment. We report average normalized accuracy on the y axis, and dataset size (in 1000s) on the X axis. The shaded region represents $95 \%$ confidence intervals.</p>
<p>Table 4: Generalist post-training outperforms specialists, even on specialist datasets. Generalist post-training on large scale data outperforms a wide range of specialist methods in the literature, even on benchmarks in their domain. Dataset sizes are reported in 1000s. The best overall model is in bold, and the best performing specialist is noted in italics.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">DS Name</th>
<th style="text-align: left;">Data Qty (K)</th>
<th style="text-align: left;">Coding-Avg</th>
<th style="text-align: left;">Math-Avg</th>
<th style="text-align: left;">NLP-Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama 3 Instruct</td>
<td style="text-align: left;">3000</td>
<td style="text-align: left;">$18.3 \pm 6.9$</td>
<td style="text-align: left;">$\mathbf{1 3 . 8} \pm 2.1$</td>
<td style="text-align: left;">$39.0 \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: left;">Bagel</td>
<td style="text-align: left;">4000</td>
<td style="text-align: left;">$\mathbf{1 8 . 6} \pm 6.9$</td>
<td style="text-align: left;">$10.7 \pm 1.7$</td>
<td style="text-align: left;">$\mathbf{3 9 . 2} \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: left;">Numina-CoT</td>
<td style="text-align: left;">860</td>
<td style="text-align: left;">$05.3 \pm 3.7$</td>
<td style="text-align: left;">$13.0 \pm 2.0$</td>
<td style="text-align: left;">$29.2 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: left;">Replete Coder</td>
<td style="text-align: left;">2830</td>
<td style="text-align: left;">$05.3 \pm 3.7$</td>
<td style="text-align: left;">$07.3 \pm 1.3$</td>
<td style="text-align: left;">$35.4 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: left;">FLAN</td>
<td style="text-align: left;">1875</td>
<td style="text-align: left;">$04.0 \pm 3.1$</td>
<td style="text-align: left;">$07.5 \pm 1.3$</td>
<td style="text-align: left;">$34.1 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: left;">Tulu-Human</td>
<td style="text-align: left;">106</td>
<td style="text-align: left;">$01.0 \pm 1.3$</td>
<td style="text-align: left;">$03.5 \pm 0.9$</td>
<td style="text-align: left;">$35.2 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: left;">MetaMath</td>
<td style="text-align: left;">400</td>
<td style="text-align: left;">$12.6 \pm 5.6$</td>
<td style="text-align: left;">$03.3 \pm 0.9$</td>
<td style="text-align: left;">$32.2 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: left;">Code Llama 3</td>
<td style="text-align: left;">800</td>
<td style="text-align: left;">$09.3 \pm 5.1$</td>
<td style="text-align: left;">$03.7 \pm 0.9$</td>
<td style="text-align: left;">$27.1 \pm 1.4$</td>
</tr>
</tbody>
</table>
<p>Specialist post-training methods underperform generalist methods. We ablate the importance of prompt diversity in Table 4 by running our benchmark suite on a pool of specialist datasets designed to improve performance on one narrow task. We find that, when starting from a base checkpoint, data scaling without prompt diversity leads to overall poorer model performance compared to generalist scaling, including on specialized benchmarks. We define prompt diversity as the breadth of domain knowledge and the variation in semantic structures among prompts.
Preference optimization trades world knowledge for improved safety and instruction following. We also conduct experiments on the second stage of post training. Somewhat surprisingly, the most significant effect we detect is a degradation in world knowledge; see Table 5. There are improvements in instruction following and safety, but they are of much smaller magnitude than the improvements during SFT. The finding that world knowledge can degrade is consistent with prior</p>
<p>Table 5: Two-stage post-training trades world knowledge for instruction following and safety. The magnitude of the improvements are much smaller than in the SFT stage, and are often not statistically significant. The magnitude of the losses in world knowledge is larger and usually statistically significant. The most significant positive effects of post-training are on LLM-judged benchmarks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">DS Size (K)</th>
<th style="text-align: left;">WK</th>
<th style="text-align: left;">IF</th>
<th style="text-align: left;">SAFETY</th>
<th style="text-align: left;">ARENA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Tulu-SFT-Mix</td>
<td style="text-align: left;">330</td>
<td style="text-align: left;">$25.5 \pm 0.5$</td>
<td style="text-align: left;">$43.8 \pm 3.6$</td>
<td style="text-align: left;">$50.6 \pm 0.3$</td>
<td style="text-align: left;">04.3</td>
</tr>
<tr>
<td style="text-align: left;">Tulu-SFT-Mix + DPO</td>
<td style="text-align: left;">390</td>
<td style="text-align: left;">$24.5 \pm 0.5$</td>
<td style="text-align: left;">$44.5 \pm 3.6$</td>
<td style="text-align: left;">$51.0 \pm 0.3$</td>
<td style="text-align: left;">12.4</td>
</tr>
<tr>
<td style="text-align: left;">2-STAGE DELTA</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">-1.0</td>
<td style="text-align: left;">0.7</td>
<td style="text-align: left;">0.4</td>
<td style="text-align: left;">8.09</td>
</tr>
<tr>
<td style="text-align: left;">Magpie</td>
<td style="text-align: left;">450</td>
<td style="text-align: left;">$23.6 \pm 0.5$</td>
<td style="text-align: left;">$36.6 \pm 3.4$</td>
<td style="text-align: left;">$43.9 \pm 0.3$</td>
<td style="text-align: left;">18.0</td>
</tr>
<tr>
<td style="text-align: left;">Magpie + DPO</td>
<td style="text-align: left;">510</td>
<td style="text-align: left;">$21.8 \pm 0.5$</td>
<td style="text-align: left;">$38.5 \pm 3.4$</td>
<td style="text-align: left;">$48.9 \pm 0.3$</td>
<td style="text-align: left;">40.8</td>
</tr>
<tr>
<td style="text-align: left;">2-STAGE DELTA</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">-1.8</td>
<td style="text-align: left;">1.9</td>
<td style="text-align: left;">5.0</td>
<td style="text-align: left;">22.8</td>
</tr>
<tr>
<td style="text-align: left;">UltraChat</td>
<td style="text-align: left;">200</td>
<td style="text-align: left;">$23.2 \pm 0.5$</td>
<td style="text-align: left;">$41.4 \pm 3.5$</td>
<td style="text-align: left;">$47.9 \pm 0.3$</td>
<td style="text-align: left;">09.2</td>
</tr>
<tr>
<td style="text-align: left;">UltraChat + DPO</td>
<td style="text-align: left;">260</td>
<td style="text-align: left;">$21.4 \pm 0.5$</td>
<td style="text-align: left;">$39.4 \pm 3.5$</td>
<td style="text-align: left;">$48.5 \pm 0.3$</td>
<td style="text-align: left;">13.8</td>
</tr>
<tr>
<td style="text-align: left;">2-STAGE DELTA</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">-1.8</td>
<td style="text-align: left;">-2.0</td>
<td style="text-align: left;">0.6</td>
<td style="text-align: left;">4.61</td>
</tr>
</tbody>
</table>
<p>work from Bai et al. (2022), although they claim that the degradation is limited to small models. We leave the ablation of model size to future work, but note that new methods for preference optimization are often demonstrated only on small models, making this an important research consideration. There are at least two other confounds which could affect this result; the first is the choice of 2 -stage dataset (we used UltraFeedback from Ding et al. (2023) for all experiments), and the choice of DPO as preference optimization algorithm. We leave the ablation of the stage-two dataset to future work, but expect the effect of this choice to be significant if it involves data scaling. We ablate the latter, and find that some other methods, most notably ORPO from Hong et al. (2024), perform better than DPO in this particular experiment; however, no method is Pareto-dominant over the baseline. See Appendix E for extended results. All in all, we conclude that methods research in preference optimization shows marked promise, but will require more robust and careful benchmarking to instill confidence in the results.</p>
<h1>7 RECOMMENDATIONS</h1>
<p>Prominent researchers have urged the alignment community to converge on more precise definitions of the problem they aim to solve, as well as better measures of progress (Carlini, 2024). While there is some value in the ability to closely approximate the pairwise, undifferentiated preferences of an 'average' human, the outsized influence of subjective design decisions such as judge instructions, inherent hackability, high unit cost, small test sets, and non-reproducibility of LLM-judges render them unsuitable for their current primary role in measuring progress in model alignment and posttraining. The model alignment process necessitates trade-offs, and we cannot make those tradeoffs consciously when undocumented implicit biases dominate our key metrics. We call on the research community to develop more targeted benchmarks for specific HHH factors of interest, such as the recent IFEval (Zhou et al., 2023b) for instruction following and FLASK (Ye et al., 2024) for particular skill sets, and recommend that reviewers insist that authors who wish to make broad claims about their method support those claims with general-purpose alignment benchmarks such as SOS-BENCH. We also call on the research community to move beyond the Bradley-Terry model, towards more sophisticated approaches to post-training.</p>
<h2>8 Related Work</h2>
<p>A series of recent works have offered various critiques on the use of LLM-judges (Chen et al., 2024; Bavaresco et al., 2024; Wei et al., 2024; Sorensen et al., 2024). Several known confounds for pairwise benchmarks are well-established in the literature, in particular a bias by both humans and LLMs in favor of longer responses and a preference of LLMs for the style of their own output; efforts have been made to control for length as a confound (Panickssery et al., 2024; Park et al., 2024; Dubois et al., 2024; Wu \&amp; Aji, 2023). There also exists considerable prior literature critiquing</p>
<p>human judgment bias and proposing mechanisms for collective decision making, including social choice theory and prospect theory (Ge et al., 2024; Ethayarajh et al., 2024). Finally, there exists a literature on the limitations of RLHF and pairwise preferences, including findings that universal AI alignment using RLHF is impossible (Singhal et al., 2024; Casper et al., 2023; Lambert \&amp; Calandra, 2024; Mishra, 2023; Lambert et al., 2023). Extending prior work, our research focuses on novel semantic biases, and also introduces the concept of an implicit weighting of factors on the part of LLM judges. Finally, our work draws on all of the aforementioned traditions to produce a metaanalysis on the progress, or lack thereof, of methods for LLM post-training.</p>
<p>Data scaling laws have been across a wide range of machine learning disciplines, including computer vision and natural language understanding (Miller et al., 2021; Hoffmann et al., 2022). The research community has even offered prizes to any important problems which defy this trend (McKenzie et al., 2024). In contrast, some authors contend that scaling laws do not apply in this setting (Zhou et al., 2023a). To the best of our knowledge, ours is the first work to document data scaling effects in post-training.</p>
<p>Similar to some aspects of our work is the excellent contribution of Ye et al. (2024), who propose judging a fine-grained evaluation of LLM skills instead of coarse preferences. We report both fine and coarse preferences, and use the explicit fine-grained preferences to reveal the implicit LLM bias in determining coarse preference.</p>
<h1>9 IMPACT / LIMITATIONS</h1>
<p>This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work; in particular, we wish to briefly highlight certain fairness considerations. By making explicit the criteria we use to judge alignment, it would be easy to unintentionally introduce explicit harms that unfairly impact specific groups. Therefore, we encourage the community to make judging templates narrow whenever possible and expose them to critique. Our proposed concrete measure of alignment assumes that there is a unitary better aligned model, which is itself a controversial assumption.</p>
<p>Because of the high unit cost of LLM-judge benchmarks, we base our empirical LLM-judge analysis solely on results from Arena-Hard-Auto (Li et al., 2024c). In Section 3, we provide evidence that LLM-judge benchmarks correlate strongly with human preference (this is also established in prior work), and therefore can be expected to correlate with one another as well.</p>
<p>Nevertheless, it is possible that varying components in the LLM-judge pipeline would alter its behavior substantially, and therefore, we do not recommend treating our results as concrete evidence that all LLM-judge benchmarks will follow any particular inductive bias.</p>
<p>Also for reasons of cost, we generate most of our systematic violations using GPT-4o-Mini, rather than human annotators. We also note that we explore only a small subset of many possible violations and many possible judgment criteria in this work. We consider this an important area for future research. Another important extension of this work are evaluations in languages other than English.</p>
<p>Our work advocates for the development of explicit inductive bias in LLM-judges. If implemented naively, this could lead to Goodhart effects. It is therefore essential that special research emphasis be placed on the development and standardization of judging templates and metrics, and that these factors be continuously optimized to combat overfitting (Manheim \&amp; Garrabrant, 2019).</p>
<h2>10 REPRODUCIbILITY STATEMENT</h2>
<p>We have tried to ensure that the research described in this paper is reproducible. Our benchmark comparisons in Section 3 and Table 1 can be reproduced using the LiveBench, ChatBot Arena, Arena-Hard-Auto, HELM and BenchBench leaderboards, which are publicly available. The checkpoints used in our experiments in Section 4 will be made available once our work is de-anonymized; our chat templates and experimental details are in the appendix. Our repository and code for Section 5 is already publicly available, and our list of benchmarks is documented in Table 8 as well.</p>
<h1>REFERENCES</h1>
<p>ajibawa-2023. Code llama 3, 2024. URL https://huggingface.co/ajibawa-2023/ Code-Llama-3-8B. Accessed: September 6, 2024.</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment. CoRR, abs/2112.00861, 2021. URL https://arxiv.org/abs/ 2112.00861 .</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022. URL https://arxiv.org/abs/2204.05862.</p>
<p>Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, André F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, and Alberto Testoni. Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks, 2024. URL https://arxiv.org/abs/2406.18403.</p>
<p>Felix Brandt, Vincent Conitzer, Ulle Endriss, Jérôme Lang, and Ariel D Procaccia. Handbook of Computational Social Choice. Cambridge University Press, 2016.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.</p>
<p>Nicholas Carlini. Some lessons from adversarial machine learning, July 2024. URL https: //www.youtube.com/watch?v=umfeF0Dx-r4. Conference Talk.</p>
<p>Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback, 2023. URL https: //arxiv.org/abs/2307.15217.</p>
<p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or llms as the judge? a study on judgement biases, 2024. URL https://arxiv.org/abs/2402. 10669 .</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling InstructionFinetuned Language Models. Journal of Machine Learning Research, 25(70):1-53, 2024. URL http://jmlr.org/papers/v25/23-0870.html.</p>
<p>Cognitive Computations. Dolphin-2.9-llama3-8b. https://huggingface.co/ cognitivecomputations/dolphin-2.9-llama3-8b, 2024. Accessed: 2024-09-07.</p>
<p>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations, 2023. URL https://arxiv.org/abs/2305.14233.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman. Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella</p>
<p>Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators, 2024. URL https://arxiv.org/ abs/2404.04475.</p>
<p>Jon Durbin. Bagel: A bagel, with everything. https://github.com/jondurbin/bagel, 2023.</p>
<p>Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization, 2024. URL https://arxiv.org/abs/ 2402.01306 .</p>
<p>Luise Ge, Daniel Halpern, Evi Micha, Ariel D. Procaccia, Itai Shapira, Yevgeniy Vorobeychik, and Junlin Wu. Axioms for AI alignment from human feedback, 2024. URL https://arxiv. org/abs/2405.14758.</p>
<p>Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li (eds.), Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pp. 4447-4455. PMLR, 02-04 May 2024. URL https://proceedings.mlr.press/v238/gheshlaghi-azar24a.html.</p>
<p>Prannaya Gupta, Le Qi Yau, Hao Han Low, I-Shiang Lee, Hugo Maximus Lim, Yu Xin Teoh, Jia Hng Koh, Dar Win Liew, Rishabh Bhardwaj, Rajat Bhardwaj, and Soujanya Poria. Walledeval: A comprehensive safety evaluation toolkit for large language models, 2024. URL https:// arxiv.org/abs/2408.03837.</p>
<p>Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, and Nouha Dziri. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms, 2024. URL https://arxiv.org/abs/2406.18495.</p>
<p>Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection, 2022. URL https://arxiv.org/abs/2203.09509.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. CoRR, abs/2103.03874, 2021. URL https://arxiv.org/abs/2103.03874.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/ 2203.15556.</p>
<p>Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model, 2024. URL https://arxiv.org/abs/2403.07691.</p>
<p>Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew E. Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hanna Hajishirzi. Camels in a changing climate: Enhancing lm adaptation with tulu 2. ArXiv, abs/2311.10702, 2023. URL https://api.semanticscholar.org/CorpusID:265281298.</p>
<p>Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset, 2023. URL https://arxiv.org/abs/2307.04657.</p>
<p>Nathan Lambert and Roberto Calandra. The alignment ceiling: Objective mismatch in reinforcement learning from human feedback, 2024. URL https://arxiv.org/abs/2311.00168.</p>
<p>Nathan Lambert, Thomas Krendl Gilbert, and Tom Zick. The history and risks of reinforcement learning and human feedback, 2023. URL https://arxiv.org/abs/2310.13595.</p>
<p>Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface. co/AI-MO/NuminaMath-CoT] (https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024.</p>
<p>Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao. Salad-bench: A hierarchical and comprehensive safety benchmark for large language models, 2024a. URL https://arxiv.org/abs/2402.05044.</p>
<p>Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan HelmBurger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort B. Breuer, Samuel Marks, Oam Patel, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Lin, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Ruoyu Wang, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang, and Dan Hendrycks. The wmdp benchmark: Measuring and reducing malicious use with unlearning, 2024b. URL https://arxiv.org/abs/2403.03218.</p>
<p>Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline, 2024c. URL https://arxiv.org/abs/2406.11939.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santarkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models, 2023. URL https://arxiv.org/abs/2211.09110.</p>
<p>David Manheim and Scott Garrabrant. Categorizing variants of goodhart's law, 2019. URL https: //arxiv.org/abs/1803.04585.</p>
<p>Ian R. McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, Derik Kauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong Huang, The Floating Droid, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping Zhou, Najoung Kim, Samuel R. Bowman, and Ethan Perez. Inverse scaling: When bigger isn't better, 2024. URL https://arxiv.org/abs/2306.09479.</p>
<p>Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward, 2024. URL https://arxiv.org/abs/2405.14734.</p>
<p>John Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: On the strong correlation between out-of-distribution and in-distribution generalization, 2021. URL https://arxiv. org/abs/2107.04649.</p>
<p>Abhilash Mishra. Ai alignment and social choice: Fundamental limitations and policy implications, 2023. URL https://arxiv.org/abs/2310.16048.</p>
<p>OpenAccess AI Collective. Axolotl. https://github.com/axolotl-ai-cloud/ axolotl, 2024. Accessed: September 09, 2024.</p>
<p>Arjun Panickssery, Samuel R. Bowman, and Shi Feng. Llm evaluators recognize and favor their own generations, 2024. URL https://arxiv.org/abs/2404.13076.</p>
<p>Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization, 2024. URL https://arxiv.org/abs/2403.19159.</p>
<p>Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R. Bowman. Bbq: A hand-built bias benchmark for question answering, 2022. URL https://arxiv.org/abs/2110.08193.</p>
<p>Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, and Sathwik Tejaswi Madhusudhan. Curry-DPO: Enhancing alignment using curriculum learning \&amp; ranked preferences, 2024. URL https://arxiv.org/abs/2403.07230.</p>
<p>Yotam Perlitz, Ariel Gera, Ofir Arviv, Asaf Yehudai, Elron Bandel, Eyal Shnarch, Michal ShmueliScheuer, and Leshem Choshen. Do these llm benchmarks agree? fixing benchmark evaluation with benchbench, 2024. URL https://arxiv.org/abs/2407.13696.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=HPuSIXJaa9.</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q\&amp;a benchmark, 2023. URL https://arxiv.org/abs/2311.12022.</p>
<p>Replete AI. Replete coder, 2024. URL https://web.archive.org/ web/20240625041503/https://huggingface.co/Replete-AI/ Replete-Coder-Llama3-8B. Accessed: September 6, 2024.</p>
<p>Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: A test suite for identifying exaggerated safety behaviours in large language models, 2024. URL https://arxiv.org/abs/2308.01263.</p>
<p>ShareGPT. Sharegpt, 2023. URL https://sharegpt.com/. Accessed: 2024-09-07.
Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. "do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models, 2024. URL https://arxiv.org/abs/2308.03825.</p>
<p>Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A Long Way to Go: Investigating Length Correlations in RLHF, July 2024. URL http://arxiv.org/abs/2310.03716. arXiv:2310.03716 [cs].</p>
<p>Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. A roadmap to pluralistic alignment, 2024. URL https://arxiv.org/abs/2402. 05070 .</p>
<p>Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, and Sam Toyer. A strongreject for empty jailbreaks, 2024. URL https://arxiv.org/abs/2402.10260.</p>
<p>Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. Musr: Testing the limits of chain-of-thought with multistep soft reasoning, 2024. URL https://arxiv.org/abs/ 2310.16049 .</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán</p>
<p>Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo JaimovitchLópez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michal Swedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023. URL https://arxiv.org/abs/2206.04615.</p>
<p>Teknium. Openhermes-2.5. https://huggingface.co/datasets/teknium/ OpenHermes-2.5, 2023. Accessed: 2024-09-07.</p>
<p>Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023. URL https://arxiv.org/abs/2310.16944.</p>
<p>Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances large language model capabilities, 2024a. URL https://arxiv.org/abs/2406.04692.</p>
<p>Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark, 2024b. URL https://arxiv.org/abs/2406.01574.</p>
<p>Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, and Mei Han. Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates, 2024. URL https://arxiv.org/abs/2408.13006.</p>
<p>Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid ShwartzZiv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. Livebench: A challenging, contamination-free llm benchmark, 2024. URL https://arxiv.org/abs/2406.19314.</p>
<p>Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jiawei Chen, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan He. Towards robust alignment of language models: Distributionally robustifying direct preference optimization, 2024a. URL https://arxiv.org/abs/ 2407.07880 .</p>
<p>Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation biases for large language models, 2023. URL https://arxiv.org/abs/2307.03025.</p>
<p>Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment, 2024b. URL https://arxiv.org/ abs/2405.00675.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=CfXh93NDgH.</p>
<p>Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing, 2024b. URL https://arxiv.org/abs/2406.08464.</p>
<p>Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. Flask: Fine-grained language model evaluation based on alignment skill sets, 2024. URL https://arxiv.org/abs/2307.10928.</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=N8N0hgNDRt.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv. org/abs/2205.01068.</p>
<p>Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=B18u7ZR1bM.</p>
<p>Table 6: Our trained models perform comparably to HF checkpoints. We compare the models we train to checkpoints from other labs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">model</th>
<th style="text-align: center;">average</th>
<th style="text-align: center;">coding</th>
<th style="text-align: center;">data_analysis</th>
<th style="text-align: center;">instruction_following</th>
<th style="text-align: center;">language</th>
<th style="text-align: center;">math</th>
<th style="text-align: center;">reasoning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">tulu2 (MAGPIE)</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: left;">tulu2 (ALLEN-AI)</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: left;">tulu2 (OURS)</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: left;">wizardLM (OURS)</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: left;">wizardLM (MAGPIE)</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: left;">ultrachat (OURS)</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">ultrachat (PRINCETON-NLP)</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">13</td>
</tr>
</tbody>
</table>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/ abs/2306.05685.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openreview.net/forum?id=KBMOKmX2he.</p>
<p>Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023b. URL https: //arxiv.org/abs/2311.07911.</p>
<p>Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, and Chenguang Zhu. Wpo: Enhancing rlhf with weighted preference optimization, 2024. URL https://arxiv.org/abs/2406.11827.</p>
<h1>A Model Training Details</h1>
<p>In order to reduce the environmental cost of this paper, whenever possible, we used publicly available checkpoints on HuggingFace; however, in some cases, checkpoints were unavailable, and we fine-tuned our own.</p>
<p>We fine-tuned all our models using Axolotl (OpenAccess AI Collective, 2024).
Our Llama3-8B models were fine-tuned for 10000 steps or 2 epochs (whichever came first), at a learning rate of 2e-5. Our Mistral-7B models were finetuned for 3 epochs at a learning rate of 5e-6.</p>
<p>All models were trained at sequence lengths of 8192, with an AdamW optimizer, and a cosine LR scheduler. We utilized gradient checkpointing, flash attention and sample packing.</p>
<p>Some of the checkpoints we report on in our meta-analysis were trained by others, and the particular hyperparameters used are not always available. We therefore conduct an ablation study on the expected variance caused by our retraining. We find that our chosen hyperparameter settings produce results quite similar to the pretrained checkpoints we retrieve online; see Table 6.</p>
<h2>B COMPUTE AND RESOURCES</h2>
<p>In this section, we provide approximate upper bound estimates of the compute and API costs required to produce the results featured in the main paper, in A100-hours and USD, respectively. We break down our estimates by section. For an estimate which includes the cost of ablations, experiments featured in the appendix, and failed or incomplete experiments, a conservative estimate would be $2 x$ the costs listed below.</p>
<h2>Compute costs.</h2>
<p>Table 7: Recent work in post-training has relied heavily on LLM judgments. Post-training stage (PT) value 1 refers to SFT datasets and methods, 2 refers to preference optimization methods, 3 refers to other methods. For the Only LLM-Judge and Only PO columns, 1 is positive and 0 is negative. This table covers the main paper only, and does not include appendix experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method Name</th>
<th style="text-align: center;">Year</th>
<th style="text-align: center;">PT</th>
<th style="text-align: center;">Only LLM-Judge</th>
<th style="text-align: center;">Only PO</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FLAN from Chung et al. (2024)</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Zephyr from Tunstall et al. (2023)</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">WizardLM from Xu et al. (2024a)</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Tulu2 from Ivison et al. (2023)</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">1,2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">UltraChat from Ding et al. (2023)</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">LIMA from Zhou et al. (2023a)</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">MetaMath from Yu et al. (2024)</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">DPO from Rafailov et al. (2023)</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">IPO from Gheshlaghi Azar et al. (2024)</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">WPO from Zhou et al. (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Magpie from Xu et al. (2024b)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Curry-DPO from Pattnaik et al. (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">DR-DPO from Wu et al. (2024a)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">SimPO from Meng et al. (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">ORPO from Hong et al. (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">SPPO from Wu et al. (2024b)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">MoA from Wang et al. (2024a)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">WildChat from Zhao et al. (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">KTO from Ethayarajh et al. (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Non-academic works used in this paper</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Bagel from Durbin (2023)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">1,2</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">OpenHermes-2.5 from Teknium (2023)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">1,2</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Dolphin from Cognitive Computations (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">1,2</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">ShareGPT from ShareGPT (2023)</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Replete Coder from Replete AI (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Code Llama 3 from ajibawa-2023 (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Numina Math CoT from LI et al. (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Tulu Human from Ivison et al. (2023)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
</tbody>
</table>
<p>The compute cost for Figure 3 was 250 A100-hours. Table 4, which required more model training, was 850 A100-hours. Table 5 was 225 A100-hours.</p>
<h1>API costs.</h1>
<p>We primarily utilize the OpenAI API, following Li et al. (2024c); The cost to produce Figure 2 was $\$ 40$ USD, as we replaced the GPT-4 judge with a GPT-4o-mini judge. The cost of Table 2 was $\$ 600$ USD. The cost to produce Table 3 was $\$ 25$ USD, for the same reason. The cost to produce Figure 3 was $\$ 750$ USD, as we used the original GPT-4 judge so as to remain consistent with Li et al. (2024c). The cost of Table 5 was $\$ 90$.</p>
<h2>C Method Comparison</h2>
<p>In Table 7 we cite all methods referred to in this work.</p>
<h2>D SOS-BENCH BENCHMARK DATASETS</h2>
<p>In Table 8 we cite all datasets used in our meta-benchmark.</p>
<p>Table 8: List of benchmark datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark Name</th>
<th style="text-align: left;">Test Set Size</th>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Factor</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LiveBench-Coding from White et al. (2024)</td>
<td style="text-align: left;">130</td>
<td style="text-align: left;">Exact Match Acc</td>
<td style="text-align: left;">WK</td>
</tr>
<tr>
<td style="text-align: left;">LiveBench-Data Analysis from White et al. (2024)</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">Exact Match Acc</td>
<td style="text-align: left;">WK</td>
</tr>
<tr>
<td style="text-align: left;">LiveBench-Instruction Following from White et al. (2024)</td>
<td style="text-align: left;">200</td>
<td style="text-align: left;">Exact Match Acc</td>
<td style="text-align: left;">IF</td>
</tr>
<tr>
<td style="text-align: left;">LiveBench-Language from White et al. (2024)</td>
<td style="text-align: left;">140</td>
<td style="text-align: left;">Exact Match Acc</td>
<td style="text-align: left;">WK</td>
</tr>
<tr>
<td style="text-align: left;">LiveBench-Math from White et al. (2024)</td>
<td style="text-align: left;">230</td>
<td style="text-align: left;">Exact Match Acc</td>
<td style="text-align: left;">WK</td>
</tr>
<tr>
<td style="text-align: left;">LiveBench-Reasoning from White et al. (2024)</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">Exact Match Acc</td>
<td style="text-align: left;">WK</td>
</tr>
<tr>
<td style="text-align: left;">IFEval from Zhou et al. (2023b)</td>
<td style="text-align: left;">540</td>
<td style="text-align: left;">Avg of Custom Metrics</td>
<td style="text-align: left;">IF</td>
</tr>
<tr>
<td style="text-align: left;">MATH Lvl 5 from Hendrycks et al. (2021)</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">Exact Match Acc</td>
<td style="text-align: left;">WK</td>
</tr>
<tr>
<td style="text-align: left;">MuSR from Sprague et al. (2024)</td>
<td style="text-align: left;">750</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">WK</td>
</tr>
<tr>
<td style="text-align: left;">GPQA from Rein et al. (2023)</td>
<td style="text-align: left;">1250</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">WK</td>
</tr>
<tr>
<td style="text-align: left;">MMLU-Pro from Wang et al. (2024b)</td>
<td style="text-align: left;">12000</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">WK</td>
</tr>
<tr>
<td style="text-align: left;">BBH from Srivastava et al. (2023)</td>
<td style="text-align: left;">6750</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">WK</td>
</tr>
<tr>
<td style="text-align: left;">BeaverTails from Ji et al. (2023)</td>
<td style="text-align: left;">1400</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
<tr>
<td style="text-align: left;">CDNA from Gupta et al. (2024)</td>
<td style="text-align: left;">2730</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
<tr>
<td style="text-align: left;">DTToxicity from Gupta et al. (2024)</td>
<td style="text-align: left;">4800</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
<tr>
<td style="text-align: left;">JailbreakHub from Shen et al. (2024)</td>
<td style="text-align: left;">15100</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
<tr>
<td style="text-align: left;">BBQ from Parrish et al. (2022)</td>
<td style="text-align: left;">58500</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
<tr>
<td style="text-align: left;">WMDP from Li et al. (2024b)</td>
<td style="text-align: left;">3670</td>
<td style="text-align: left;">Inverse Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
<tr>
<td style="text-align: left;">XSTest from Röttger et al. (2024)</td>
<td style="text-align: left;">450</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
<tr>
<td style="text-align: left;">WildGuardTest from Han et al. (2024)</td>
<td style="text-align: left;">1730</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
<tr>
<td style="text-align: left;">Toxigen from Hartvigsen et al. (2022)</td>
<td style="text-align: left;">9900</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
<tr>
<td style="text-align: left;">StrongREJECT from Souly et al. (2024)</td>
<td style="text-align: left;">310</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
<tr>
<td style="text-align: left;">SGXSTest from Gupta et al. (2024)</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
<tr>
<td style="text-align: left;">SaladBench from Li et al. (2024a)</td>
<td style="text-align: left;">30400</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">Safety</td>
</tr>
</tbody>
</table>
<p>Table 9: Ablating the choice of post-training method shows meaningful differences in alignment. However, no method is Pareto-dominant over the baseline. Unlike other alignment benchmarks, LLM-judged benchmarks have large-magnitude reactions to stage-two post-training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">PO Method</th>
<th style="text-align: center;">IF</th>
<th style="text-align: center;">WK</th>
<th style="text-align: center;">SAFETY</th>
<th style="text-align: center;">ARENA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">None</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">$47.9 \pm 0.27$</td>
<td style="text-align: center;">4.36</td>
</tr>
<tr>
<td style="text-align: left;">ORPO</td>
<td style="text-align: center;">$\mathbf{4 2 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 3 . 6}$</td>
<td style="text-align: center;">$43.1 \pm 0.27$</td>
<td style="text-align: center;">11.5</td>
</tr>
<tr>
<td style="text-align: left;">KTO</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">$47.4 \pm 0.27$</td>
<td style="text-align: center;">10.6</td>
</tr>
<tr>
<td style="text-align: left;">RDPO</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">$47.4 \pm 0.27$</td>
<td style="text-align: center;">13.6</td>
</tr>
<tr>
<td style="text-align: left;">DPO</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">$46.8 \pm 0.27$</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: left;">IPO</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">$\mathbf{4 8 . 5} \pm 0.27$</td>
<td style="text-align: center;">$\mathbf{1 5 . 7}$</td>
</tr>
</tbody>
</table>
<h1>E ADDITIONAL TWO-STAGE POST-TRAINING RESULTS</h1>
<p>We ablate the choice of DPO as a post-training algorithm in Table 9, and find that at least one preference optimization method outperforms the baseline on every factor; however, the effect is very small for all factors except LLM-judge.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ It is worth noting that reference-based labels also can fall short of the idealized ground truth, in particular on open-ended generative tasks such as summarization.
${ }^{3}$ We define inductive bias as a valuable factor in machine learning, which allows the LLM to make predictions on new data based on what it learned. Implicit inductive bias we define, with some subjectivity, as any decision criteria which do not intuitively derive from the instructions provided to the judge. We note that this term is sometimes used in the ML literature to refer to the tendency of optimizers to more frequently visit some minima than others; this is not the sense in which we use the term.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8987 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8987</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8987</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-8cc369961ce9cd04f300ee0a81646556ee0626c3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8cc369961ce9cd04f300ee0a81646556ee0626c3" target="_blank">Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work involves enriching the Stack-LSTM transition-based AMR parser by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs and shows an in-depth study ablating each of the new components of the parser.</p>
                <p><strong>Paper Abstract:</strong> Our work involves enriching the Stack-LSTM transition-based AMR parser (Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs. In addition, we also combined several AMR-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification, named entities and contextualized embeddings. We achieve a highly competitive performance that is comparable to the best published results. We show an in-depth study ablating each of the new components of the parser.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8987.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8987.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transition Action Sequence (Stack-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transition-based action sequence representation used with Stack-LSTMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearization of AMR graphs into a sequence of parser actions (shift/reduce, node/edge creation and labels) used as the target sequence for a transition-based Stack-LSTM parser; training uses oracle action sequences derived from graph-to-token alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Transition action sequence (stack-based linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The AMR graph is represented as an ordered sequence of transition parser actions and associated labels (node/concept insertions, edge attachments, role labels). The parser incrementally consumes the input buffer and applies actions to a stack; the action sequence encodes the construction history of the graph and thus linearizes the graph structure into a sequence suitable for sequence-model training.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) graphs (rooted labeled directed acyclic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Generate oracle action sequences from AMR graphs using word-to-node alignments (hard alignments described below) and a transition system (Stack-LSTM implementation of Ballesteros & Al-Onaizan 2017); label prediction is separated from action prediction (two-step softmax) to reduce action vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (graph prediction from text); the action sequences are the supervised targets used to train the transition-based parser.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline BO (JAMR alignments) Smatch 65.9 (row 0). Reimplementation with label separation and improved alignments achieved Smatch 68.3 (row 2); final best single model using this representation + other improvements reached Smatch up to 75.5 with RL (row 16).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The transition-action linearization is the core representation used by BO and by this paper's reimplementation; compared to earlier BO (which used larger joint action+label space) splitting labels from actions reduced the action set (from ~478 to 10) and improved search and final performance (see rows 0->1/2). The paper also compares to seq2graph/character-based methods in the literature (e.g. Zhang et al. 2019, van Noord & Bos 2017) in Table 1, where seq2graph approaches may achieve comparable or higher Smatch but use different architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Encodes full construction process allowing greedy linear-time parsing with Stack-LSTM; label separation reduces action vocabulary improving learnability; works naturally with transition-based oracles; supports incorporation of attention and contextualized embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on high-quality word-to-node alignments to produce reliable oracle action sequences; oracle upper bound limited (reported oracle upper bound 93.3 F1 and noisy/incomplete alignments lead to training noise).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When alignments are incomplete or erroneous the oracle action sequence is noisy and the parser's training objective tied to the oracle can be suboptimal; BO's oracle-induced upper bound and noisy alignments cause lower recall on nodes not aligned to tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8987.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8987.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hard Alignments (SEM+JAMR + percolation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symmetrized EM (SEM) alignments combined with JAMR rule-based alignments and upward percolation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid hard alignment pipeline that merges probabilistic alignments (SEM) and rule-based JAMR alignments, plus deterministic post-processing to align named-entity/date/quantity and intermediate nodes by percolating child alignments upward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Hard word-to-node alignments (SEM + JAMR merge)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Precomputed discrete alignments that map AMR graph nodes (concepts, name nodes, date/quantity nodes) to positions (tokens) in the sentence. The method first runs a symmetrized EM aligner (Pourdamghani et al. 2014), fills unaligned nodes by percolating child alignments upward using role-priority rules, and finally merges with JAMR alignments for remaining unaligned nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (node-to-word alignments linking graph nodes to sentence tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Produce initial alignments via SEM; for nodes left unaligned, percolate child node alignments upward (choosing child by role precedence); then merge with JAMR rule-based alignments and fill any remaining gaps. These alignments are used to derive oracle action sequences (graph->action sequence).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing training (generation of oracle action sequences for supervised learning); indirectly improves parsing metrics (Smatch and sub-metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using the paper's alignments (vs JAMR-only) improved Smatch: BO (JAMR) Smatch 65.9 (row 0) vs BO+Label with the paper's alignments 68.3 (row 2). The authors state the hard alignments present a clear advantage over JAMR alignments; incremental gains observed across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared directly to JAMR-only alignments: the combined SEM+JAMR+percolation approach yields higher recall (the paper notes BO benefits from a more recall-oriented alignment), resulting in improved Smatch. The paper also contrasts with approaches that learn alignments as latent variables (Lyu & Titov 2018), which the authors reference as an alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Higher recall and coverage (aligns nodes that JAMR left unaligned after postprocessing), produces better oracle sequences leading to improved parser performance; deterministic rules address named entities, dates, quantities.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Still pre-learned/hard and not updated during parser training; may propagate errors from the separate alignment pipeline into oracle sequences; requires multiple systems and postprocessing heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Oracle action sequences remain imperfect (oracle upper bound ~93.3 F1) and some sentences still have incomplete or erroneous oracle sequences; examples with poor alignments still degrade training (motivates RL-based exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8987.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8987.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Soft Alignments via Attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention-based soft alignments (bidirectional LSTM encoder + general Luong attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A soft alignment mechanism over encoder token representations used at each parser time-step to provide a differentiable mapping between parser actions and sentence tokens, complementing hard alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Attention-based soft alignment (encoder-decoder attention)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each parser time-step the parser computes attention weights over bidirectional LSTM encoder token representations (general/Luong attention) producing a context vector which is concatenated into the parser state; this provides soft, learned alignments between the current decoding/action step and input tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (soft alignment between graph construction steps/actions and sentence token sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Add a BiLSTM encoder over sentence tokens; compute attention scores e_i = a_j^T W_a h_i where a_j encodes action history/decoder state; obtain softmax weights alpha and context vector c_j = sum_i alpha_i * h_i; integrate c_j into parser state and predict next action/label.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (improves action prediction during graph construction); used to complement hard alignments when producing action sequences and during decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Attention additions increased performance in experiments: e.g., BO+Label (JAMR) 67.0 (row 1) vs adding attention (row 8) 69.8; the paper states adding attention on top of improved alignments adds about 1 point in Smatch in their ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Combining hard alignments with soft attention has a synergistic effect per authors; attention alone (without their alignment improvements) gives gains but the combined approach outperforms JAMR-only baselines. The attention mechanism is inspired by NMT literature (Bahdanau et al. 2014; Luong et al. 2015).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides differentiable, learnable alignment signals that can compensate for noisy/hard alignments; enables the parser to use global sentence context at each action step; synergizes with precomputed alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds computational overhead (encoder + attention) and complexity; may not fully replace the need for hard alignments for rare/unaligned node types.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not sufficient alone to resolve all alignment-induced oracle noise; must be combined with preprocessed alignments and concept/NER preprocessing for best results per ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8987.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8987.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label Separation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action/Label prediction separation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modelling change that splits the joint prediction of parser action + label into two successive predictions (first action type, then label/concept), drastically reducing the joint action space and improving learning and search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Two-step action+label decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Instead of treating each (action,label) pair as a distinct atomic action, the model predicts the coarse action (one of ~10) and then independently predicts the label or concept in a separate softmax. This reduces the effective action vocabulary and simplifies the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (affects the way graph construction steps are represented as targets)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>During derivation of oracle sequences, produce separate supervision signals: the action sequence (without labels) and the corresponding label sequence; train two output softmaxes instead of a single large softmax over combined actions.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing training (improved learning of action sequences and labels).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BO (JAMR) baseline 65.9 Smatch (row 0). BO + Label (JAMR) 67.0 (row 1). Using label separation with the paper's alignments yields Smatch 68.3 (row 2): demonstrates an improvement due to label separation and reduced action vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Label separation reduces action-space complexity compared to BO's original joint action+label predictions (BO reported 478 actions); this leads to better search guidance and improved parsing performance in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces action-space size (from hundreds to ~10 actions + labels), eases learning and search, leads to measurable Smatch gains.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Introduces dependence on correctly predicting the coarse action before label; may require careful balancing between the two softmax losses.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure cases reported beyond general errors induced by alignment noise and oracle limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8987.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8987.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preprocessed Concept Tagger (BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-based per-token concept prediction used as preprocessed node labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A separate classifier uses contextualized BERT embeddings to predict AMR concept labels for each word (or none), and these predicted concept features are fed as additional inputs to the parser to improve concept identification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Preprocessed token-to-concept labels (BERT tagger)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each input token is labeled by a linear classifier trained on BERT contextual embeddings (mean of last 4 layers, pooled over wordpieces) to predict the AMR concept associated with that token; the predicted concept label is used as an auxiliary feature vector concatenated into the token representation for parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (concept nodes, including PropBank sense tags like want-01 and base lexical concepts)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Train a linear classifier on BERT features using AMR training data (OntoNotes + AMR datasets) to predict concept labels; during parser training/use, provide predicted concept vectors as input features, effectively converting word context into candidate node labels.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (concept identification subtask) and overall graph prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Adding preprocessed concepts increased Smatch: e.g., +Concepts step yields Smatch 70.9 (row 6) compared to earlier stages (row 5 69.8). The Concepts metric itself improved to 84 (row 6/7 onwards).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Preprocessing concepts with a BERT-based tagger adds over a point of Smatch compared to not using preprocessed concepts and is complementary to attention and contextual embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages strong contextual embeddings to improve concept identification, increases concept accuracy and contributes to improved overall Smatch; isolates concept prediction as a simpler per-token classification.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Predicts each label in isolation (no sequence-level modeling of surrounding labels), which may miss dependencies between neighboring concepts; errors in the tagger propagate into parser input.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Isolated per-token predictions ignore structured dependencies among concepts; no explicit failure examples given but sequencing dependencies are a potential limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8987.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8987.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NER Preprocessing & Wikification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preprocessed named-entity labels plus post-processing wikification (dictionary + entity linker)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train a neural NER model to predict AMR named-entity types as input features for the parser, and post-process parser output to add :wiki links using a training-derived dictionary and an external entity linking system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Preprocessed NER labels and post-hoc Wikification</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Named-entity node types are predicted using a neural NER model trained on AMR-extracted entities (jackknifed). Wikipedia links are not predicted by the parser; instead, a dictionary built from training data and an external entity-linking system (Sil et al. 2018) are used in post-processing to assign :wiki values to :name nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (named-entity subgraphs and :wiki annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Train an NER tagger (jackknifed training) to produce NER labels as features; at post-processing lookup :name nodes in a training-built dictionary for Wikipedia links and, if missing, use an external entity-linkerâ€™s output to assign wiki links.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (improves named entity identification and helps produce :wiki annotations in final graphs as post-processing).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>NER preprocessing increased NER metric: e.g., adding NER raised NER metric to 83 (row 5) and NER F1 of the NER models averaged 79.48 on the NER dev set. Wikification metric improvements appear marginal and are handled in post-processing (Wikification column shows varied numbers across rows).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Approach similar to Lyu & Titov (2018) in spirit for wikification, but implemented with a dictionary+entity-linker combination; parser itself does not predict Wikipedia nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Improves named-entity handling in the parser via explicit features; reliable production of :wiki labels via dictionary and entity linking in post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Parser does not natively predict :wiki nodes; wikification depends on dictionary coverage and external linker quality, which may limit recall.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Nodes not found in the dictionary rely on the external linker; when neither provides a link, :wiki is missing. The approach may misassign wiki links if dictionary frequency heuristic fails.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8987.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8987.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Character-based translation (van Noord & Bos 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural semantic parsing by character-based translation: Experiments with abstract meaning representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A character-level sequence-to-sequence translation approach treating AMR graphs as linearized character sequences (character-based translation) to perform semantic parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural semantic parsing by character-based translation: Experiments with abstract meaning representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Character-level linearization / character-based seq2seq</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The AMR is represented as a linear string suitable for character-level sequence-to-sequence translation (treating the AMR serialization as a character sequence), enabling use of character-based neural translation models.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (linearized/serialized as character sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize/linearize AMR graphs into a string representation (character-level) that can be modeled by a seq2seq translator at character granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (mapping sentence to serialized AMR) via seq2seq translation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in the paper's comparison table: van Noord and Bos (2017) Smatch 71.0 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Listed in Table 1 alongside transition-based methods and newer sequence-to-graph approaches; character-based methods provide an alternative linearization with different trade-offs in modeling granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Avoids explicit alignment or transition systems by framing parsing as sequence transduction; character-level modeling can handle open vocabularies and morphological variation.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May require long output sequences; serialization may obscure graph structure and reentrancies, making decoding and validity of graphs harder to enforce.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure cases in this paper beyond general challenges of sequence linearizations (handling reentrancies and graph constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8987.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8987.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent-alignment graph prediction (Lyu & Titov 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR parsing as graph prediction with latent alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that treats word-to-node alignments as latent variables, jointly learning graph prediction and alignments rather than relying on precomputed hard alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AMR parsing as graph prediction with latent alignment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph prediction with latent alignments</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Instead of relying on precomputed discrete alignments, the model treats alignments between sentence tokens and AMR graph nodes as latent and learns them jointly with graph structure prediction, effectively integrating alignment inference into training.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Learn a model that predicts graph structure while marginalizing or inferring latent token-node alignments during training/inference (no explicit precomputed serialization required).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (graph prediction directly from sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1: Lyu and Titov (2018) Smatch 74.4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Contrasts with this paper's approach of combining precomputed hard alignments with attention; authors note latent-alignment approaches as recent alternatives and list Lyu & Titov (2018) in comparison table.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Removes dependency on possibly noisy external alignment pipelines; potentially more coherent joint modelling of alignment and graph prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Latent-variable models can be harder to train and may require complex inference; not the strategy adopted by this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not discussed in this paper; mentioned as alternative approach to alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8987.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8987.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sequence-to-graph transduction (Zhang et al. 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR parsing as sequence-to-graph transduction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that treat AMR parsing as transduction from an input token sequence to a graph structure, often using neural sequence models adapted to output graph structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AMR parsing as sequence-to-graph transduction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Sequence-to-graph transduction / linearization-based seq2graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Approaches that map input sentences to graphs by combining sequence encoders with decoders that either directly predict graph structure or predict a linearized form that is subsequently converted into a graph; these methods often adapt sequence models to handle graph outputs (e.g., pointer mechanisms, special serialization).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (sequence -> graph mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Various (paper referenced as sequence-to-graph): can include linearization schemes, structured decoders, or direct graph generation conditioned on sentence encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (sequence-to-graph mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1: Zhang et al. (2019) Smatch 76.3 (best in table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Reported to achieve strong Smatch (76.3), outperforming many transition-based systems in Table 1; paper contrasts transition-based Stack-LSTM approach with sequence-to-graph transduction approaches in related work and results comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Can directly model graph outputs and leverage powerful sequence encoders/decoders; reported high Smatch performance in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May require careful design to ensure graph well-formedness (reentrancies, variable reuse); different conversion/serialization choices affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper; included for comparison as a high-performing alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Aligning english strings with abstract meaning representation graphs <em>(Rating: 2)</em></li>
                <li>A discriminative graph-based parser for the abstract meaning representation <em>(Rating: 2)</em></li>
                <li>AMR parsing as graph prediction with latent alignment <em>(Rating: 2)</em></li>
                <li>Neural semantic parsing by character-based translation: Experiments with abstract meaning representations <em>(Rating: 2)</em></li>
                <li>AMR parsing as sequence-to-graph transduction <em>(Rating: 2)</em></li>
                <li>Self-critical sequence training for image captioning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8987",
    "paper_id": "paper-8cc369961ce9cd04f300ee0a81646556ee0626c3",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Transition Action Sequence (Stack-LSTM)",
            "name_full": "Transition-based action sequence representation used with Stack-LSTMs",
            "brief_description": "A linearization of AMR graphs into a sequence of parser actions (shift/reduce, node/edge creation and labels) used as the target sequence for a transition-based Stack-LSTM parser; training uses oracle action sequences derived from graph-to-token alignments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Transition action sequence (stack-based linearization)",
            "representation_description": "The AMR graph is represented as an ordered sequence of transition parser actions and associated labels (node/concept insertions, edge attachments, role labels). The parser incrementally consumes the input buffer and applies actions to a stack; the action sequence encodes the construction history of the graph and thus linearizes the graph structure into a sequence suitable for sequence-model training.",
            "graph_type": "Abstract Meaning Representation (AMR) graphs (rooted labeled directed acyclic graphs)",
            "conversion_method": "Generate oracle action sequences from AMR graphs using word-to-node alignments (hard alignments described below) and a transition system (Stack-LSTM implementation of Ballesteros & Al-Onaizan 2017); label prediction is separated from action prediction (two-step softmax) to reduce action vocabulary.",
            "downstream_task": "AMR parsing (graph prediction from text); the action sequences are the supervised targets used to train the transition-based parser.",
            "performance_metrics": "Baseline BO (JAMR alignments) Smatch 65.9 (row 0). Reimplementation with label separation and improved alignments achieved Smatch 68.3 (row 2); final best single model using this representation + other improvements reached Smatch up to 75.5 with RL (row 16).",
            "comparison_to_others": "The transition-action linearization is the core representation used by BO and by this paper's reimplementation; compared to earlier BO (which used larger joint action+label space) splitting labels from actions reduced the action set (from ~478 to 10) and improved search and final performance (see rows 0-&gt;1/2). The paper also compares to seq2graph/character-based methods in the literature (e.g. Zhang et al. 2019, van Noord & Bos 2017) in Table 1, where seq2graph approaches may achieve comparable or higher Smatch but use different architectures.",
            "advantages": "Encodes full construction process allowing greedy linear-time parsing with Stack-LSTM; label separation reduces action vocabulary improving learnability; works naturally with transition-based oracles; supports incorporation of attention and contextualized embeddings.",
            "disadvantages": "Relies on high-quality word-to-node alignments to produce reliable oracle action sequences; oracle upper bound limited (reported oracle upper bound 93.3 F1 and noisy/incomplete alignments lead to training noise).",
            "failure_cases": "When alignments are incomplete or erroneous the oracle action sequence is noisy and the parser's training objective tied to the oracle can be suboptimal; BO's oracle-induced upper bound and noisy alignments cause lower recall on nodes not aligned to tokens.",
            "uuid": "e8987.0",
            "source_info": {
                "paper_title": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Hard Alignments (SEM+JAMR + percolation)",
            "name_full": "Symmetrized EM (SEM) alignments combined with JAMR rule-based alignments and upward percolation",
            "brief_description": "A hybrid hard alignment pipeline that merges probabilistic alignments (SEM) and rule-based JAMR alignments, plus deterministic post-processing to align named-entity/date/quantity and intermediate nodes by percolating child alignments upward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Hard word-to-node alignments (SEM + JAMR merge)",
            "representation_description": "Precomputed discrete alignments that map AMR graph nodes (concepts, name nodes, date/quantity nodes) to positions (tokens) in the sentence. The method first runs a symmetrized EM aligner (Pourdamghani et al. 2014), fills unaligned nodes by percolating child alignments upward using role-priority rules, and finally merges with JAMR alignments for remaining unaligned nodes.",
            "graph_type": "AMR graphs (node-to-word alignments linking graph nodes to sentence tokens)",
            "conversion_method": "Produce initial alignments via SEM; for nodes left unaligned, percolate child node alignments upward (choosing child by role precedence); then merge with JAMR rule-based alignments and fill any remaining gaps. These alignments are used to derive oracle action sequences (graph-&gt;action sequence).",
            "downstream_task": "AMR parsing training (generation of oracle action sequences for supervised learning); indirectly improves parsing metrics (Smatch and sub-metrics).",
            "performance_metrics": "Using the paper's alignments (vs JAMR-only) improved Smatch: BO (JAMR) Smatch 65.9 (row 0) vs BO+Label with the paper's alignments 68.3 (row 2). The authors state the hard alignments present a clear advantage over JAMR alignments; incremental gains observed across experiments.",
            "comparison_to_others": "Compared directly to JAMR-only alignments: the combined SEM+JAMR+percolation approach yields higher recall (the paper notes BO benefits from a more recall-oriented alignment), resulting in improved Smatch. The paper also contrasts with approaches that learn alignments as latent variables (Lyu & Titov 2018), which the authors reference as an alternative.",
            "advantages": "Higher recall and coverage (aligns nodes that JAMR left unaligned after postprocessing), produces better oracle sequences leading to improved parser performance; deterministic rules address named entities, dates, quantities.",
            "disadvantages": "Still pre-learned/hard and not updated during parser training; may propagate errors from the separate alignment pipeline into oracle sequences; requires multiple systems and postprocessing heuristics.",
            "failure_cases": "Oracle action sequences remain imperfect (oracle upper bound ~93.3 F1) and some sentences still have incomplete or erroneous oracle sequences; examples with poor alignments still degrade training (motivates RL-based exploration).",
            "uuid": "e8987.1",
            "source_info": {
                "paper_title": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Soft Alignments via Attention",
            "name_full": "Attention-based soft alignments (bidirectional LSTM encoder + general Luong attention)",
            "brief_description": "A soft alignment mechanism over encoder token representations used at each parser time-step to provide a differentiable mapping between parser actions and sentence tokens, complementing hard alignments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Attention-based soft alignment (encoder-decoder attention)",
            "representation_description": "At each parser time-step the parser computes attention weights over bidirectional LSTM encoder token representations (general/Luong attention) producing a context vector which is concatenated into the parser state; this provides soft, learned alignments between the current decoding/action step and input tokens.",
            "graph_type": "AMR graphs (soft alignment between graph construction steps/actions and sentence token sequence)",
            "conversion_method": "Add a BiLSTM encoder over sentence tokens; compute attention scores e_i = a_j^T W_a h_i where a_j encodes action history/decoder state; obtain softmax weights alpha and context vector c_j = sum_i alpha_i * h_i; integrate c_j into parser state and predict next action/label.",
            "downstream_task": "AMR parsing (improves action prediction during graph construction); used to complement hard alignments when producing action sequences and during decoding.",
            "performance_metrics": "Attention additions increased performance in experiments: e.g., BO+Label (JAMR) 67.0 (row 1) vs adding attention (row 8) 69.8; the paper states adding attention on top of improved alignments adds about 1 point in Smatch in their ablations.",
            "comparison_to_others": "Combining hard alignments with soft attention has a synergistic effect per authors; attention alone (without their alignment improvements) gives gains but the combined approach outperforms JAMR-only baselines. The attention mechanism is inspired by NMT literature (Bahdanau et al. 2014; Luong et al. 2015).",
            "advantages": "Provides differentiable, learnable alignment signals that can compensate for noisy/hard alignments; enables the parser to use global sentence context at each action step; synergizes with precomputed alignments.",
            "disadvantages": "Adds computational overhead (encoder + attention) and complexity; may not fully replace the need for hard alignments for rare/unaligned node types.",
            "failure_cases": "Not sufficient alone to resolve all alignment-induced oracle noise; must be combined with preprocessed alignments and concept/NER preprocessing for best results per ablation.",
            "uuid": "e8987.2",
            "source_info": {
                "paper_title": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Label Separation",
            "name_full": "Action/Label prediction separation",
            "brief_description": "A modelling change that splits the joint prediction of parser action + label into two successive predictions (first action type, then label/concept), drastically reducing the joint action space and improving learning and search.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Two-step action+label decomposition",
            "representation_description": "Instead of treating each (action,label) pair as a distinct atomic action, the model predicts the coarse action (one of ~10) and then independently predicts the label or concept in a separate softmax. This reduces the effective action vocabulary and simplifies the policy.",
            "graph_type": "AMR graphs (affects the way graph construction steps are represented as targets)",
            "conversion_method": "During derivation of oracle sequences, produce separate supervision signals: the action sequence (without labels) and the corresponding label sequence; train two output softmaxes instead of a single large softmax over combined actions.",
            "downstream_task": "AMR parsing training (improved learning of action sequences and labels).",
            "performance_metrics": "BO (JAMR) baseline 65.9 Smatch (row 0). BO + Label (JAMR) 67.0 (row 1). Using label separation with the paper's alignments yields Smatch 68.3 (row 2): demonstrates an improvement due to label separation and reduced action vocabulary.",
            "comparison_to_others": "Label separation reduces action-space complexity compared to BO's original joint action+label predictions (BO reported 478 actions); this leads to better search guidance and improved parsing performance in ablations.",
            "advantages": "Reduces action-space size (from hundreds to ~10 actions + labels), eases learning and search, leads to measurable Smatch gains.",
            "disadvantages": "Introduces dependence on correctly predicting the coarse action before label; may require careful balancing between the two softmax losses.",
            "failure_cases": "No explicit failure cases reported beyond general errors induced by alignment noise and oracle limitations.",
            "uuid": "e8987.3",
            "source_info": {
                "paper_title": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Preprocessed Concept Tagger (BERT)",
            "name_full": "BERT-based per-token concept prediction used as preprocessed node labels",
            "brief_description": "A separate classifier uses contextualized BERT embeddings to predict AMR concept labels for each word (or none), and these predicted concept features are fed as additional inputs to the parser to improve concept identification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Preprocessed token-to-concept labels (BERT tagger)",
            "representation_description": "Each input token is labeled by a linear classifier trained on BERT contextual embeddings (mean of last 4 layers, pooled over wordpieces) to predict the AMR concept associated with that token; the predicted concept label is used as an auxiliary feature vector concatenated into the token representation for parsing.",
            "graph_type": "AMR graphs (concept nodes, including PropBank sense tags like want-01 and base lexical concepts)",
            "conversion_method": "Train a linear classifier on BERT features using AMR training data (OntoNotes + AMR datasets) to predict concept labels; during parser training/use, provide predicted concept vectors as input features, effectively converting word context into candidate node labels.",
            "downstream_task": "AMR parsing (concept identification subtask) and overall graph prediction.",
            "performance_metrics": "Adding preprocessed concepts increased Smatch: e.g., +Concepts step yields Smatch 70.9 (row 6) compared to earlier stages (row 5 69.8). The Concepts metric itself improved to 84 (row 6/7 onwards).",
            "comparison_to_others": "Preprocessing concepts with a BERT-based tagger adds over a point of Smatch compared to not using preprocessed concepts and is complementary to attention and contextual embeddings.",
            "advantages": "Leverages strong contextual embeddings to improve concept identification, increases concept accuracy and contributes to improved overall Smatch; isolates concept prediction as a simpler per-token classification.",
            "disadvantages": "Predicts each label in isolation (no sequence-level modeling of surrounding labels), which may miss dependencies between neighboring concepts; errors in the tagger propagate into parser input.",
            "failure_cases": "Isolated per-token predictions ignore structured dependencies among concepts; no explicit failure examples given but sequencing dependencies are a potential limitation.",
            "uuid": "e8987.4",
            "source_info": {
                "paper_title": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "NER Preprocessing & Wikification",
            "name_full": "Preprocessed named-entity labels plus post-processing wikification (dictionary + entity linker)",
            "brief_description": "Train a neural NER model to predict AMR named-entity types as input features for the parser, and post-process parser output to add :wiki links using a training-derived dictionary and an external entity linking system.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Preprocessed NER labels and post-hoc Wikification",
            "representation_description": "Named-entity node types are predicted using a neural NER model trained on AMR-extracted entities (jackknifed). Wikipedia links are not predicted by the parser; instead, a dictionary built from training data and an external entity-linking system (Sil et al. 2018) are used in post-processing to assign :wiki values to :name nodes.",
            "graph_type": "AMR graphs (named-entity subgraphs and :wiki annotations)",
            "conversion_method": "Train an NER tagger (jackknifed training) to produce NER labels as features; at post-processing lookup :name nodes in a training-built dictionary for Wikipedia links and, if missing, use an external entity-linkerâ€™s output to assign wiki links.",
            "downstream_task": "AMR parsing (improves named entity identification and helps produce :wiki annotations in final graphs as post-processing).",
            "performance_metrics": "NER preprocessing increased NER metric: e.g., adding NER raised NER metric to 83 (row 5) and NER F1 of the NER models averaged 79.48 on the NER dev set. Wikification metric improvements appear marginal and are handled in post-processing (Wikification column shows varied numbers across rows).",
            "comparison_to_others": "Approach similar to Lyu & Titov (2018) in spirit for wikification, but implemented with a dictionary+entity-linker combination; parser itself does not predict Wikipedia nodes.",
            "advantages": "Improves named-entity handling in the parser via explicit features; reliable production of :wiki labels via dictionary and entity linking in post-processing.",
            "disadvantages": "Parser does not natively predict :wiki nodes; wikification depends on dictionary coverage and external linker quality, which may limit recall.",
            "failure_cases": "Nodes not found in the dictionary rely on the external linker; when neither provides a link, :wiki is missing. The approach may misassign wiki links if dictionary frequency heuristic fails.",
            "uuid": "e8987.5",
            "source_info": {
                "paper_title": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Character-based translation (van Noord & Bos 2017)",
            "name_full": "Neural semantic parsing by character-based translation: Experiments with abstract meaning representations",
            "brief_description": "A character-level sequence-to-sequence translation approach treating AMR graphs as linearized character sequences (character-based translation) to perform semantic parsing.",
            "citation_title": "Neural semantic parsing by character-based translation: Experiments with abstract meaning representations",
            "mention_or_use": "mention",
            "representation_name": "Character-level linearization / character-based seq2seq",
            "representation_description": "The AMR is represented as a linear string suitable for character-level sequence-to-sequence translation (treating the AMR serialization as a character sequence), enabling use of character-based neural translation models.",
            "graph_type": "AMR graphs (linearized/serialized as character sequences)",
            "conversion_method": "Serialize/linearize AMR graphs into a string representation (character-level) that can be modeled by a seq2seq translator at character granularity.",
            "downstream_task": "AMR parsing (mapping sentence to serialized AMR) via seq2seq translation.",
            "performance_metrics": "Reported in the paper's comparison table: van Noord and Bos (2017) Smatch 71.0 (Table 1).",
            "comparison_to_others": "Listed in Table 1 alongside transition-based methods and newer sequence-to-graph approaches; character-based methods provide an alternative linearization with different trade-offs in modeling granularity.",
            "advantages": "Avoids explicit alignment or transition systems by framing parsing as sequence transduction; character-level modeling can handle open vocabularies and morphological variation.",
            "disadvantages": "May require long output sequences; serialization may obscure graph structure and reentrancies, making decoding and validity of graphs harder to enforce.",
            "failure_cases": "No explicit failure cases in this paper beyond general challenges of sequence linearizations (handling reentrancies and graph constraints).",
            "uuid": "e8987.6",
            "source_info": {
                "paper_title": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Latent-alignment graph prediction (Lyu & Titov 2018)",
            "name_full": "AMR parsing as graph prediction with latent alignment",
            "brief_description": "An approach that treats word-to-node alignments as latent variables, jointly learning graph prediction and alignments rather than relying on precomputed hard alignments.",
            "citation_title": "AMR parsing as graph prediction with latent alignment",
            "mention_or_use": "mention",
            "representation_name": "Graph prediction with latent alignments",
            "representation_description": "Instead of relying on precomputed discrete alignments, the model treats alignments between sentence tokens and AMR graph nodes as latent and learns them jointly with graph structure prediction, effectively integrating alignment inference into training.",
            "graph_type": "AMR graphs",
            "conversion_method": "Learn a model that predicts graph structure while marginalizing or inferring latent token-node alignments during training/inference (no explicit precomputed serialization required).",
            "downstream_task": "AMR parsing (graph prediction directly from sentences).",
            "performance_metrics": "Reported in Table 1: Lyu and Titov (2018) Smatch 74.4.",
            "comparison_to_others": "Contrasts with this paper's approach of combining precomputed hard alignments with attention; authors note latent-alignment approaches as recent alternatives and list Lyu & Titov (2018) in comparison table.",
            "advantages": "Removes dependency on possibly noisy external alignment pipelines; potentially more coherent joint modelling of alignment and graph prediction.",
            "disadvantages": "Latent-variable models can be harder to train and may require complex inference; not the strategy adopted by this paper.",
            "failure_cases": "Not discussed in this paper; mentioned as alternative approach to alignment.",
            "uuid": "e8987.7",
            "source_info": {
                "paper_title": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Sequence-to-graph transduction (Zhang et al. 2019)",
            "name_full": "AMR parsing as sequence-to-graph transduction",
            "brief_description": "A family of methods that treat AMR parsing as transduction from an input token sequence to a graph structure, often using neural sequence models adapted to output graph structures.",
            "citation_title": "AMR parsing as sequence-to-graph transduction",
            "mention_or_use": "mention",
            "representation_name": "Sequence-to-graph transduction / linearization-based seq2graph",
            "representation_description": "Approaches that map input sentences to graphs by combining sequence encoders with decoders that either directly predict graph structure or predict a linearized form that is subsequently converted into a graph; these methods often adapt sequence models to handle graph outputs (e.g., pointer mechanisms, special serialization).",
            "graph_type": "AMR graphs (sequence -&gt; graph mapping)",
            "conversion_method": "Various (paper referenced as sequence-to-graph): can include linearization schemes, structured decoders, or direct graph generation conditioned on sentence encodings.",
            "downstream_task": "AMR parsing (sequence-to-graph mapping).",
            "performance_metrics": "Reported in Table 1: Zhang et al. (2019) Smatch 76.3 (best in table).",
            "comparison_to_others": "Reported to achieve strong Smatch (76.3), outperforming many transition-based systems in Table 1; paper contrasts transition-based Stack-LSTM approach with sequence-to-graph transduction approaches in related work and results comparison.",
            "advantages": "Can directly model graph outputs and leverage powerful sequence encoders/decoders; reported high Smatch performance in comparisons.",
            "disadvantages": "May require careful design to ensure graph well-formedness (reentrancies, variable reuse); different conversion/serialization choices affect performance.",
            "failure_cases": "Not detailed in this paper; included for comparison as a high-performing alternative.",
            "uuid": "e8987.8",
            "source_info": {
                "paper_title": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Aligning english strings with abstract meaning representation graphs",
            "rating": 2
        },
        {
            "paper_title": "A discriminative graph-based parser for the abstract meaning representation",
            "rating": 2
        },
        {
            "paper_title": "AMR parsing as graph prediction with latent alignment",
            "rating": 2
        },
        {
            "paper_title": "Neural semantic parsing by character-based translation: Experiments with abstract meaning representations",
            "rating": 2
        },
        {
            "paper_title": "AMR parsing as sequence-to-graph transduction",
            "rating": 2
        },
        {
            "paper_title": "Self-critical sequence training for image captioning",
            "rating": 1
        }
    ],
    "cost": 0.018075249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning</h1>
<p>Tahira Naseem ${ }^{\text {A }}$ Abhishek Shah ${ }^{\text {Â® }} \quad$ Hui Wan ${ }^{\text {A }}$<br>Radu Florian ${ }^{\text {a }}$ Salim Roukos ${ }^{\text {a }}$ Miguel Ballesteros ${ }^{\text {A }}$<br>${ }^{A}$ IBM Research, Yorktown Heights, NY, USA<br>${ }^{\text {Â® }}$ IBM Watson, New York, NY, USA<br>tnaseem, hwan, raduf, roukos@us.ibm.com<br>abhishek.shah1, miguel.ballesteros@ibm.com</p>
<h4>Abstract</h4>
<p>Our work involves enriching the Stack-LSTM transition-based AMR parser (Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs. In addition, we also combined several AMR-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification, named entities and contextualized embeddings. We achieve a highly competitive performance that is comparable to the best published results. We show an indepth study ablating each of the new components of the parser.</p>
<h2>1 Introduction</h2>
<p>Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and GÃ³mez-RodrÃ­guez, 2018; Zhang et al., 2019) in recent years.</p>
<p>We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired
from neural machine translation (Bahdanau et al., 2014) and by the recent trends on contextualized representations (Peters et al., 2018; Devlin et al., 2018).</p>
<p>Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward.</p>
<h2>2 Stack-LSTM AMR Parser</h2>
<p>We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM dependency parser by Dyer et al. (2015). This approach allows unbounded lookahead and makes use of greedy inference. BO also learns character-level word representations to capitalize on morphosyntactic regularities (Ballesteros et al., 2015). BO uses recurrent neural networks to represent the stack data structures that underlie many linear-time parsing algorithms. It follows transition-based parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2003, 2008); words are read from a buffer and they are incrementally combined, in a stack, with a set of actions towards producing the final parse. The input is a sentence and the output is a complete AMR graph without</p>
<p>any preprocessing required. ${ }^{1}$ We use Dynet (Neubig et al., 2017) to implement the parser. In what follows, we present several additions to the original BO model that improved the results.</p>
<h3>2.1 Label Separation</h3>
<p>BO's actions are enriched with labels that may correspond to AMR nodes or labels that decorate the arcs of the graph. BO reported a total of 478 actions in the 2014 dataset. We tried splitting the prediction in two separate steps, first the action, then the label or concept. This reduces the number of actions to 10 and helps the model to drive the search better.</p>
<h3>2.2 Hard Alignments and Soft Alignments</h3>
<p>AMR annotations do not provide alignments between the nodes of an AMR graph and the tokens in the corresponding sentence. We need such alignments to generate action sequences with an oracle for training. The parser is then trained to generate these action sequences. The quality of word-to-graph alignments has a direct impact in the accuracy of the parser.</p>
<p>In previous work, both rule-based (Flanigan et al., 2014) and machine learning (Pourdamghani et al., 2014) methods have been used to produce word-to-graph alignments. Once generated, the alignments are often not updated during training (Flanigan et al., 2016; Damonte et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017). More recently, Lyu and Titov (2018) learn these alignments as latent variables.</p>
<p>In this work, we combine pre-learned (hard) alignments with an attention mechanism. As shown in section 4, the combination has a synergistic effect. In the following, we first explain our method for producing hard alignments and then we elaborate on the attention mechanism.</p>
<p>Hard Alignments Generation: In order to produce word-to-graph alignments, we combine the outputs of the symmetrized Expectation Maximization approach (SEM) of Pourdamghani et al. (2014) with those of the rule-based algorithm (JAMR) of Flanigan et al. (2014). Pourdamghani et al. (2014) do not produce alignments for all concepts; for example, named-entity nodes, dateentity nodes and numerical-quantity nodes are left unaligned. We post-process the output to deter-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ministically align these nodes based on the alignments of its children (if any). We then merge the output with JAMR alignments. Overall, the alignment process involves the following steps:</p>
<ol>
<li>Produce initial alignments using SEM ${ }^{2}$.</li>
<li>Fill in the unaligned nodes by upwards percolation of child node alignments. ${ }^{3}$</li>
<li>Use JAMR alignments ${ }^{4}$ for any nodes still unaligned and fill in intermediate nodes again.</li>
</ol>
<p>Soft Alignments via Attention: The parser state is represented by the STACK, BUFFER and a list with the history of actions (which are encoded as LSTMs, the first two being Stack-LSTMs (Dyer et al., 2015)). This forms the vector $\mathbf{s}_{t}$ that represents the state:</p>
<p>$$
\mathbf{s}<em t="t">{t}=\max \left{\mathbf{0}, \mathbf{W}\left[\mathbf{s t}</em>} ; \mathbf{b<em t="t">{t} ; \mathbf{a}</em>\right}
$$}\right]+\mathbf{d</p>
<p>This vector $\mathbf{s}<em i="i">{t}$ is used to predict the best action (and concept to add, if applicable) to take, given the state with a softmax. We complement the state with an attention over the input sentence (Bahdanau et al., 2014). In particular, we use general attention (Luong et al., 2015). In order to do so, we add a bidirectional LSTM encoder to the BO parsing model and we run attention over it in each time step. More formally, the attention weights $\alpha</em>$ :}$ (for position $i$ ) are calculated based on the actions predicted so far (represented as $a_{j}$ ), the encoder representation of the sentence $\left(h_{i}\right)$ and a projection weight matrix $W_{a</p>
<p>$$
\begin{aligned}
&amp; e_{i}=a_{j}^{\top} W_{a} h_{i} \
&amp; \alpha_{i}=\frac{\exp \left(e_{i}\right)}{\sum_{k} \exp \left(e_{k}\right)}
\end{aligned}
$$</p>
<p>A vector representation $\left(c_{j}\right)$ is computed by a weighted sum of the encoded sentence word representations and the $\alpha$ values.</p>
<p>$$
c_{j}=\sum_{i} \alpha_{i} \cdot h_{i}
$$</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Given the sentence representation produced by the attention mechanism $\left(c_{j}\right)$, we complement the parser state as follows:</p>
<p>$$
\begin{aligned}
&amp; g_{j}=\tanh \left(W_{D e e}^{1} d_{j}+W_{A t t}^{1} c_{j}\right) \
&amp; u_{j}=\tanh \left(g_{j}+W_{D e e}^{2} d_{j}+W_{A t t}^{2} c_{j}\right) \
&amp; \mathbf{s}<em t="t">{t}=\max \left{\mathbf{0}, \mathbf{W}\left[\mathbf{s t}</em>} ; \mathbf{b<em t="t">{t} ; \mathbf{a}</em>\right}
\end{aligned}
$$} ; \mathbf{u}_{j}\right]+\mathbf{d</p>
<p>where $d_{j}$ is the concatenation of the output vector of the LSTM with the history of actions LSTM and the output vector of the LSTM that represents the stack. This new vector $\mathbf{s}_{t}$ replaces the one described in (1).</p>
<p>Those familiar with neural machine translation will recognize that we are using the concatenation of the output of the LSTMs that represent the stack and the action history as the decoder is used in the standard sequence to sequence with attention model (Bahdanau et al., 2014).</p>
<h3>2.3 Preprocessed Nodes</h3>
<p>We produce two types of pre-processed nodes: 1) Named Entity labels (NER) and 2) Concept labels (such as want-01, boy etc.). We use NER labels and preprocessed concepts the same way BO and Dyer et al. (2015) used part-of-speech tags - as another vector concatenated to the word representation and learned during training.</p>
<p>Concepts: AMR representation abstracts away from exact lexical forms. In the case of objects, the concepts are usually represented using the uninflected base forms; for events, the OntoNotes PropBank sense number is attached with the base form (such as want-01). We train a linear classifier that uses contextualized BERT embeddings (Devlin et al., 2018) of each word to predict the corresponding concept (which can be none). Each label is predicted in isolation with no regard to the surrounding labels. The tagger is trained using a combination of OntoNotes 5.0 (LDC2013T19) and LDC2017T10 AMR training data.</p>
<p>Named entities: We extracted named entities from the AMR dataset (there are more than 100 entity types in the AMR language) and we trained a neural network NER model (Ni et al., 2017) to predict NER labels for the AMR parser. In the NER model, the target word and its surrounding words and tags are used as features. We jackknifed (90/10) the training data, to train the AMR parser. The ten jackknifed models got an average NER F1 score of 79.48 on the NER dev set.</p>
<h3>2.4 Contextualized Vectors</h3>
<p>Recent work has shown that the use of pre-trained networks improves the performance of downstream tasks. BO uses pre-trained word embeddings by Ling et al. (2015) along with learned character embeddings. In this work, we explore the effect of using contextualized word vectors as pre-trained word embeddings. We experiment with recent context based embedding obtained with BERT (Devlin et al., 2018).</p>
<p>We use average of last 4 layers of BERT Large model with hidden representations of size 1024. We produce the word representation by mean pooling the representations of word piece tokens obtained using BERT. We only use the contextualized word vectors as input to our model, we do not back-propagate through the BERT layers.</p>
<h3>2.5 Wikification</h3>
<p>Given that BO does not produce Wikipedia nodes during prediction, we pre-process the AMR data removing all Wikipedia nodes. In order to produce Wikipedia entries in our AMR graphs, we run a wikification approach as post-processing. We combine the approach of Lyu and Titov (2018) with the entity linking technique of Sil et al. (2018).</p>
<p>First, we produce a dictionary of Wikipedia links for all the named entity nodes that appear with :wiki label in the training data. If a node appears with multiple Wikipedia links, the most frequent one is added to the dictionary. Separately, we also process the target sentence using the entity linking system of Sil et al. (2018). This system identifies the entities as well as links them.</p>
<p>During post processing, every node with :name label is looked up in the dictionary and if found, is assigned the corresponding Wikipedia link. This is very similar to the approach of Lyu and Titov (2018). If the node is not found in the dictionary, and the system of Sil et al. (2018) produces a Wikipedia link, we use that link.</p>
<h3>2.6 Smatch Weighting</h3>
<p>The upper bound for BO's oracle is only 93.3 F1 for the entire development set. We observed that the oracle produces a score close to perfect for most sentences, yet it loses some points in others. During training, we have the gold AMR graph available for every sentence. We compare it to the oracle graph and use the Smatch score as a</p>
<p>weight for the training example. This is a way to down-weight the examples whose oracle actions sequence is incomplete or erroneous. This modification resulted in moderate gains (see row 14 in Table 1) and also lead to the training with exploration experiments described below.</p>
<h2>3 Reinforcement Learning</h2>
<p>BO relies on the oracle action sequences. The training objective is to maximize the likelihood of oracle actions. This strategy has two drawbacks. First, inaccurate/incomplete alignments between the tokens and the graph nodes introduce noise into oracle action sequences.(As mentioned above, the oracle upper bound is only 93.3 F1. With the enhanced alignments, BO reported 89.5 F1 in the LDC2014 development set). Second, even for the perfectly aligned sentences, the oracle action sequence is not the only or the best action sequence that can lead to the gold graph; there could be shorter sequences that are easier to learn. Therefore, strictly binding the training objective to the oracle action sequences can lead to suboptimal performance, as evidenced in (DaumÃ© III and Marcu, 2005; DaumÃ© III et al., 2009; Goldberg and Nivre, 2012, 2013; Ballesteros et al., 2016) among others.</p>
<p>To circumvent these issues, we resort to a Reinforcement Learning (RL) objective where the Smatch score of the predicted graph for a given sentence is used as reward. This alleviates the strong dependency on hard alignment and leaves room to training with exploration of the action space. This line of work is also motivated by Goodman et al. (2016), who used imitation learning to build AMR parses from dependency trees.</p>
<p>We use the self-critical policy gradient training algorithm by Rennie et al. (2017) which is a special case of the REINFORCE algorithm of Williams (1992) with a baseline. This method allows the use of an external evaluation measure as reward (Paulus et al., 2017). In particular, we want to maximize the expected Smatch reward,</p>
<p>$$
L_{R L}=E_{g^{s} \sim p_{\theta}}\left[r\left(g^{s}\right)\right]
$$</p>
<p>where $p_{\theta}$ is the policy specified by the parser parameters $\theta$ and $g^{s}$ is a graph sampled from $p_{\theta}$. The gradient of this objective can be approximated using a single sample from $p_{\theta}$. For each sentence, we produce two graphs using the current model
parameters. A greedy best graph $\hat{g}$ and a graph $g^{s}$ produced by sampling from action space. The gradient of 8 is approximated as in (Rennie et al., 2017),</p>
<p>$$
\nabla_{\theta} L_{R L}=\left(r\left(g^{s}\right)-r(\hat{g})\right) \nabla_{\theta} \log \left(p_{\theta}\left(g^{s}\right)\right)
$$</p>
<p>where $r(g)$ is the Smatch score of graph $g$ with respect to the ground truth. The Smatch of the greedy graph $r(\hat{g})$ serves as a baseline that can reduce the variance in the gradient estimate (Williams, 1992).</p>
<p>With $\epsilon$ probability, we flatten the sampling distribution by calculating the square root of the probabilities. In our experiments, $\epsilon$ is set to 0.05 . We first train our full model with the maximumlikelihood objective of BO that achieves an F-score 72.8 without beam search when evaluated in the development set. The RL training is then initialized with the parameters of this trained model. For RL training, we use a batch-size of 40.</p>
<h2>4 Experiments and Results</h2>
<p>We start by reimplementing $\mathrm{BO}^{5}$ and we train models with the most recent dataset (LDC2017T10) ${ }^{6}$. We include label separation in our reimplementation (Experiments 1..16) which separates the prediction of actions and labels in two different softmax layers. All our experiments use beam 10 for decoding and they are the best (when evaluated in the development set) of 5 different random seeds. Word, input and hidden representations have 100 dimensions (with BERT, input dimensions are 1024), action and label embeddings are of size 20. Our results are presented in Table 1.</p>
<p>We achieve the best results ever reported in some of the metrics. Unlabeled Smatch (16) by 1 point and SRL by 2 points. These two metrics represent the structure and semantic parsing task. For all the remaining metrics, our parser consistently achieves the second best results. Also, our best single model (16) achieves more than 9 Smatch points on top of BO (0). Guo and Lu (2018)'s parser is a reimplementation of BO with a refined search space (which we did not attempt) and we beat their performance by 5 points.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Id</th>
<th style="text-align: center;">Experiment</th>
<th style="text-align: center;">Smatch</th>
<th style="text-align: center;">Unlabeled</th>
<th style="text-align: center;">No WSD</th>
<th style="text-align: center;">Named Entities</th>
<th style="text-align: center;">Wikification</th>
<th style="text-align: center;">Negations</th>
<th style="text-align: center;">Concepts</th>
<th style="text-align: center;">Rentrancies</th>
<th style="text-align: center;">SRL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">BO (JAMR)</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">59</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">BO + Label (JAMR)</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">BO + Label</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">66</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$2+$ POS</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">67</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$3+$ DEP</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">67</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$4+$ NER</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">67</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$5+$ Concepts</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">69</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$6+$ BERT</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">72</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$1+$ Attention</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">$8+$ POS</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$9+$ DEP</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">$10+$ NER</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">$11+$ Concepts</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">$12+$ BERT $^{11}$</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">72</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">$13+$ Smatch</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">72</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">$8+$ BERT</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">71</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$14+$ RL</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zhang et al. (2019)</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lyu and Titov (2018)</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">van Noord and Bos (2017)</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">66</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Guo and Lu (2018)</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">64</td>
</tr>
</tbody>
</table>
<p>Table 1: Results, including comparison with the best systems, in the LDC2017T10 test set (aka AMR 2.0). Results highlighted in bold are the best in each metric. BO is (Ballesteros and Al-Onaizan, 2017) (which did not produce wikification). (JAMR) means that the model uses JAMR alignments, the rest use our alignments. Metrics by Cai and Knight (2013) and Damonte et al. (2016).</p>
<p>The hard alignments proposed in this paper present a clear advantage over the JAMR alignments. BO ignores nodes that are not aligned to tokens in the sentence, and it benefits from a more recall oriented alignment method. Adding attention on top of that adds a point, while preprocessing named entities improve the NER metric. Adding concepts preprocessed with our BERT based tagger adds more than a point. Smatch weighting lead to half a point on top of (14).</p>
<p>BERT contextualized vectors provide more than a point on top of the best model with traditional word embeddings (without attention, the difference is of 2 points). Combining BERT with a model that only sees words (15), we achieve the best results surpassed only by models that also use contextualized vectors and reinforcement learning objective, However, we added Smatch weighting (14) and Reinforcement Learning (16) on top of 13. This was decided based on development data results, where 13 performed better than the BERT only model (15) by about a point.</p>
<p>Finally, training with exploration via reinforcement learning gives further gains of about 2 points and achieves one of the best results ever reported on the task and state of the art in some of the metrics.</p>
<h2>5 Conclusions</h2>
<p>We report modifications in a competitive AMR parser achieving one of the best results in the task.</p>
<p>Our main contribution augments training with Policy Learning by priming samples that are more suitable for the evaluation metric. We perform an in-depth ablation experiment that shows the impact of each of our contributions. Our unlabeled Smatch score (achieving the best graph structure) suggests that a new strategy to predict labels may reach even higher numbers.</p>
<h2>Acknowledgments</h2>
<p>We thank RamÃ³n Astudillo, Avi Sil, Young-Suk Lee, Vittorio Castelli and Todd Ward for useful comments and support.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</p>
<p>Miguel Ballesteros and Yaser Al-Onaizan. 2017. Amr parsing using stack-lstms. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1269-1275. Association for Computational Linguistics.</p>
<p>Miguel Ballesteros, Chris Dyer, and Noah A. Smith. 2015. Improved transition-based parsing by modeling characters instead of words with LSTMs. pages 349-359.</p>
<p>Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and Noah A. Smith. 2016. Training with exploration improves a greedy stack lstm parser. In Proceedings of</p>
<p>the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2005-2010. Association for Computational Linguistics.</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178-186.</p>
<p>Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In $A C L$ (2), pages 748-752.</p>
<p>Wei-Te Chen. 2015. Learning to map dependency parses to abstract meaning representations. In $A C L$.</p>
<p>Wei-Te Chen and Martha Palmer. 2017. Unsupervised amr-dependency parse alignment. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 558-567. Association for Computational Linguistics.</p>
<p>Chenhui Chu and Sadao Kurohashi. 2016. Supervised syntax-based alignment between english sentences and abstract meaning representation graphs. arXiv preprint arXiv:1606.02126.</p>
<p>Marco Damonte, Shay B Cohen, and Giorgio Satta. 2016. An incremental parser for abstract meaning representation. arXiv preprint arXiv:1608.06111.</p>
<p>Hal DaumÃ© III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine Learning, 75:297-325.</p>
<p>Hal DaumÃ© III and Daniel Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction. In Proc. of ICML.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transitionbased dependency parsing with stack long shortterm memory. pages 334-343.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A Smith, and Jaime Carbonell. 2016. Cmu at semeval-2016 task 8: Graph-based amr parsing with infinite ramp loss. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 12021206.</p>
<p>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages $1426-1436$.</p>
<p>William Foland and James H Martin. 2017. Abstract meaning representation parsing using lstm recurrent neural networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages $463-472$.</p>
<p>Yoav Goldberg and Joakim Nivre. 2012. A dynamic oracle for arc-eager dependency parsing. pages 959976 .</p>
<p>Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. Transactions of the Association for Computational Linguistics, 1:403-414.</p>
<p>James Goodman, Andreas Vlachos, and Jason Naradowsky. 2016. Noise reduction and targeted exploration in imitation learning for abstract meaning representation parsing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Zhijiang Guo and Wei Lu. 2018. Better transitionbased amr parsing with a refined search space. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Wang Ling, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015. Two/too simple adaptations of word2vec for syntax problems. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1299-1304.</p>
<p>Yijia Liu, Wanxiang Che, Bo Zheng, Bing Qin, and Ting Liu. 2018. An amr aligner tuned by transitionbased parser. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2422-2430. Association for Computational Linguistics.</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 1412-1421.</p>
<p>Chunchuan Lyu and Ivan Titov. 2018. Amr parsing as graph prediction with latent alignment. arXiv preprint arXiv:1805.05286.</p>
<p>Jonathan May. 2016. Semeval-2016 task 8: Meaning representation parsing. In Proceedings of the 10th international workshop on semantic evaluation (semeval-2016), pages 1063-1073.</p>
<p>Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha</p>
<p>Swayamdipta, and Pengcheng Yin. 2017. Dynet: The dynamic neural network toolkit. arXiv preprint arXiv:1701.03980.</p>
<p>Jian Ni, Georgiana Dinu, and Radu Florian. 2017. Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14701480, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. pages 149-160.</p>
<p>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. 34:513-553.</p>
<p>Rik van Noord and Johan Bos. 2017. Neural semantic parsing by character-based translation: Experiments with abstract meaning representations. CoRR, 1705.09980 .</p>
<p>Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304.</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365.</p>
<p>Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and Kevin Knight. 2014. Aligning english strings with abstract meaning representation graphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages $425-429$.</p>
<p>Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7008-7024.</p>
<p>Avirup Sil, Gourab Kundu, Radu Florian, and Wael Hamza. 2018. Neural cross-lingual entity linking. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Ida Szubert, Adam Lopez, and Nathan Schneider. 2018. A structured syntax-semantics interface for englishamr alignment. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1169-1180. Association for Computational Linguistics.</p>
<p>David Vilares and Carlos GÃ³mez-RodrÃ­guez. 2018. A transition-based algorithm for unrestricted amr parsing. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers).</p>
<p>Chuan Wang and Nianwen Xue. 2017. Getting the most out of amr parsing. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1257-1268.</p>
<p>Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015a. Boosting transition-based AMR parsing with refined actions and auxiliary analyzers. In Proc. of ACL 2015, pages 857-862.</p>
<p>Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015b. A transition-based algorithm for amr parsing. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 366-375, Denver, Colorado. Association for Computational Linguistics.</p>
<p>Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256.</p>
<p>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. pages 195-206.</p>
<p>Sheng Zhang, Xutai Ma, Kevin Duh, and Benjamin Van Durme. 2019. Amr parsing as sequence-to-graph transduction. arXiv preprint arXiv:1905.08704.</p>
<p>Junsheng Zhou, Feiyu Xu, Hans Uszkoreit, Weiguang QU, Ran Li, and Yanhui Gu. 2016. Amr parsing with an incremental joint model. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 680-689, Austin, Texas. Association for Computational Linguistics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ BO reported results on the 2014 dataset.
${ }^{6}$ LDC2016E25 and LDC2017T10 contain the same AMR annotations as of March 2016. LDC2017T10 is the general release while LDC2016E25 was released for Semeval 2016 participants (May, 2016).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ https://isi.edu/ damghani/papers/ Aligner.zip
${ }^{3}$ When multiple child nodes are aligned, role labels are used to select best node for alignment percolation. Node role labels are preferred in the following order - :name (in general), :unit (for quantities), :ARG2 (for have-org-role and rateentities) and then any other labels except :mod.
${ }^{4}$ https://github.com/jflanigan/jamr&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
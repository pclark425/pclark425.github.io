<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6868 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6868</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6868</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-272397850</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.02231v2.pdf" target="_blank">SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration</a></p>
                <p><strong>Paper Abstract:</strong> Here we show that a Large Language Model (LLM) can serve as a foundation model for a Chemical Language Model (CLM) which performs at or above the level of CLMs trained solely on chemical SMILES string data. Using supervised fine-tuning (SFT) and direct preference optimization (DPO) on the open-source Llama LLM, we demonstrate that we can train an LLM to respond to prompts such as generating molecules with properties of interest to drug development. This overall framework allows an LLM to not just be a chatbot client for chemistry and materials tasks, but can be adapted to speak more directly as a CLM which can generate molecules with user-specified properties</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6868.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6868.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SmileyLlama</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmileyLlama (SFT fine-tuned Llama-3.1-8B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemical language model produced by supervised fine-tuning an open-weight Llama-3.1-8B-Instruct on ~2 million ChEMBL SMILES paired with RDKit-calculated property prompts so the model will autoregressively generate SMILES that satisfy user-specified property windows or pattern constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SmileyLlama (fine-tuned Llama-3.1-8B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, supervised fine-tuned chemical language model (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>~2 million SMILES from ChEMBL v28. For each SMILES the authors computed RDKit properties (H-bond donors/acceptors, molecular weight, logP, rotatable bonds, fraction sp3, TPSA, presence/absence of macrocycles, presence/absence of covalent-warhead SMARTS, presence/absence of undesirable SMARTS, BRICS single-pass substructures, chemical formula). Prompts were constructed from these calculated properties and the SMILES used as the completion.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive SMILES completion via supervised fine-tuning (SFT) on prompt→SMILES pairs; sampling with temperature annealing (authors explored T in 0.01–2.0, optimal often 0.6–1.4, T=1.1 maximizing valid/novel/non-redundant output in some tasks). Zero-shot prompting possible after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (regular SMILES strings); BRICS substructure SMILES and SMARTS used in prompts/constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo small-molecule generation for drug discovery: generate drug-like molecules satisfying medicinal-chemistry properties and scaffolds; can be prompted to focus chemical space for lead-like/fragment-like design and downstream predicted target binding.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Property ranges specified in prompts (e.g., ≤k HBD/HBA, MW, logP, rotatable bonds, TPSA, Fsp3); presence/absence of SMARTS (undesirable groups, covalent warheads); ability to request retention of BRICS substructures or chemical formula; duplicate removal for diversity during sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit for property computation, validity checking, SMARTS matching and BRICS decomposition; GuacaMol benchmark suite and Frechet ChemNet Distance for distributional evaluation; training/inference tooling: Axolotl, LoRA low-rank adapters, accelerate, DeepSpeed; TDCommons (TDC) oracles used in related DPO experiments (separate model/step).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEMBL v28 (~2M SMILES) for SFT; additional re-randomized ChEMBL-derived dataset for second SFT; evaluation used GuacaMol benchmarks and Enamine substructure list.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (% valid SMILES), Uniqueness (% unique among generated), Novelty (% not in training set), KL divergence between generated and training property distributions, Fréchet ChemNet Distance (FCD), QED distributions, task-specific success rates (percentage of generated molecules satisfying prompt constraints), TDC oracle scores when used (for binding predictions).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors report substantive improvement after SFT vs base Llama: for a panel of property tasks SmileyLlama achieved high task-success rates (examples quoted in paper: ≤k H-bond donors: 95.7% (1xSFT), 96.0% (2xSFT), 97.1% (DPO); 'exactly k HBD' tasks: ~21.4% (1xSFT) → ~30.4% (DPO); Lipinski rule-of-five success: 65.3% (1xSFT) → 84.9% (DPO) in reported tables). The authors found optimal sampling temperatures typically 0.6–0.8 for SFT tasks and T≈1.1 for maximizing valid/novel/non-redundant outputs in unconstrained generation. Distributional comparisons (Figure S1) show property distributions of generated molecules are similar to ChEMBL for most metrics; QED of generated molecules trended slightly higher than training set. Frequency of undesirable SMARTS was lower in SmileyLlama than in ChEMBL for several categories (e.g., 'unsaturated benzene' 0.55% in ChEMBL vs 0.0% in SmileyLlama in Table S2).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Authors report limits including degraded performance on tasks the model was not trained on (e.g., exact-count HBD or some Lipinski exact constraints), a tendency for reduced diversity / 'crashing diversity' and partial memorization after optimization steps, temperature-dependent tradeoffs between diversity and validity, and the absence of any wet-lab synthesis/biological validation. They also note that SFT acts mainly as a prompting wrapper that elicits latent capability and that multimodal/protein-specific inputs were not used and remain future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6868.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6868.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SmileyLlama-Opt (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmileyLlama-Opt (Direct Preference Optimization of SmileyLlama)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of SmileyLlama further optimized with Direct Preference Optimization (DPO) to bias generation toward higher scores on SMILES-based scoring functions (e.g., QED, and TDCommons predicted binding scores for GSK3B, JNK3, DRD2) by pairing higher-scoring outputs with lower-scoring ones and updating model weights accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SmileyLlama-Opt (DPO-optimized SmileyLlama / Llama-3.1-8B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, supervised fine-tuned + preference-optimized (SFT + DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Initial SFT dataset from ChEMBL v28 (~2M SMILES) plus DPO training data constructed from the model's own generated SMILES: for each DPO iteration the authors sampled 2,000 SMILES per objective (QED, GSK3B, JNK3, DRD2), enforced uniqueness, scored with oracles, paired winners/losers randomly to form preference pairs; paper reports an aggregate DPO dataset of ~76,000 examples used for DPO optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct Preference Optimization: sample many SMILES from the SFT model given a target prompt, score each sample with a scoring function/oracle, form winner-loser pairs (higher-scoring vs lower-scoring), and perform gradient updates to increase probability of winners (authors ran single-epoch DPO loops repeated over ~20 epochs for some tasks). Sampling temperature varied (authors used T=0.8–1.1 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Optimize generated molecules for drug-like metrics (QED) and predicted target binding to protein targets (GSK3B, JNK3, DRD2) as assessed by TDCommons/TDC oracles; also multi-objective generation by combining prompts for several objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Same prompt-specified property ranges and SMARTS constraints as SFT; during DPO the scoring function (RDKit property checks or TDC oracle scores) acts as the optimization objective; authors enforced diversity by discarding duplicate SMILES before scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>TDCommons / TDC oracles for target-affinity prediction (GSK3B, JNK3, DRD2), RDKit for property computation and validity checking, Axolotl and LoRA for training infrastructure, accelerate and DeepSpeed for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEMBL v28 for SFT; generated SMILES scored by RDKit or TDC oracles to form DPO training pairs (authors sampled 2,000 SMILES per target per DPO epoch; reported ~8,000 responses when optimizing four objectives at once and ~76,000 DPO examples overall).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Oracle scores (TDC predicted binding scores, median and percentiles across epochs), uniqueness (unique SMILES/total), validity, novelty (percent not in SFT dataset), task-specific success rates (percentage achieving desired property windows), QED distributions, GuacaMol metrics and FCD/KL for distributional shift.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>DPO substantially improved the ability to generate molecules with higher oracle scores: median TDC oracle scores for target binding rose substantially after ~10 DPO epochs for GSK3B and JNK3, with the fraction of unique generated molecules predicted active rising above 0.5 for those targets. DPO-optimized models also improved property task success rates reported in benchmark tables (examples in paper: improvements from SFT to DPO on multiple prompts; see SmileyLlama → SmileyLlama-Opt numbers such as ≤k HBD 95.7%→97.1%; exact HBD 21.4%→30.4%; Lipinski rule-of-five 65.3%→84.9% in reported tables). Authors also found that models optimized separately for single objectives could be prompted to combine those objectives at inference (implicit multi-objective capability). DPO training (≈76k examples) took ~3 hours 30 minutes on a single 2×A40 node according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>DPO caused a 'crashing diversity' effect (diversity decreased after score plateaus), model memorization of frequently seen molecules during optimization, a tradeoff between temperature (higher T delays diversity collapse but slows convergence), dependence on the fidelity and biases of scoring/oracle models (no experimental binding validation), and limitation to SMILES-based scoring functions (no multimodal protein-structure conditioning).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6868.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6868.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Llama-3.1-8B-Instruct (base model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-weight instruction-tuned Llama model used as the foundation model; authors show it can produce SMILES zero/few-shot but is substantially improved by SFT on chemical property→SMILES pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, instruction-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on general language/instruction datasets (paper does not detail pretraining corpora for Llama), not specialized on chemistry-specific SMILES corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Zero-, one-, few-shot prompting to produce SMILES strings (authors tested zero/one/two/five/ten/twenty shot configurations in GuacaMol benchmark comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES produced via prompting</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Baseline generative capability for SMILES; serves as foundation to be converted to a CLM via SFT and DPO</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>When used zero-shot/few-shot authors provided few-shot examples in prompt; no inherent property constraints unless provided in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Evaluated with RDKit for validity and GuacaMol metrics; used as the pre-fine-tuning base for SmileyLlama.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>No chemical-specific training dataset used for the base model in this paper; evaluation used ChEMBL examples and GuacaMol benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>GuacaMol metrics: validity, uniqueness, novelty, KL divergence, Fréchet ChemNet Distance. The authors report substantially lower values for Llama zero-shot compared to SFT models (example numbers reported in Table S1 for Llama zero-shot T=0.6: Validity ≈0.8867, Uniqueness ≈0.0830, Novelty ≈0.1458 — as reported in the paper's supplemental table).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Llama can generate syntactically correct SMILES in zero-shot, but is far below fine-tuned SmileyLlama in validity/uniqueness/novelty and distributional similarity; increasing number of examples in the prompt increased uniqueness/novelty at cost of validity per authors' analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Without chemistry-specific SFT it produces fewer valid/novel drug-like molecules; few-shot prompting increases diversity but reduces validity; lacks direct property control that SFT/DPO impart.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Direct Preference Optimization: Your Language Model is Secretly a Reward Model <em>(Rating: 2)</em></li>
                <li>Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development <em>(Rating: 2)</em></li>
                <li>Benchmarking Models for de Novo Molecular Design (GuacaMol) <em>(Rating: 2)</em></li>
                <li>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules <em>(Rating: 1)</em></li>
                <li>The Llama 3 Herd of Models <em>(Rating: 1)</em></li>
                <li>Chemical language models enable navigation in sparsely populated chemical space <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6868",
    "paper_id": "paper-272397850",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "SmileyLlama",
            "name_full": "SmileyLlama (SFT fine-tuned Llama-3.1-8B-Instruct)",
            "brief_description": "A chemical language model produced by supervised fine-tuning an open-weight Llama-3.1-8B-Instruct on ~2 million ChEMBL SMILES paired with RDKit-calculated property prompts so the model will autoregressively generate SMILES that satisfy user-specified property windows or pattern constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SmileyLlama (fine-tuned Llama-3.1-8B-Instruct)",
            "model_type": "decoder-only LLM, supervised fine-tuned chemical language model (SFT)",
            "model_size": "8B",
            "training_data_description": "~2 million SMILES from ChEMBL v28. For each SMILES the authors computed RDKit properties (H-bond donors/acceptors, molecular weight, logP, rotatable bonds, fraction sp3, TPSA, presence/absence of macrocycles, presence/absence of covalent-warhead SMARTS, presence/absence of undesirable SMARTS, BRICS single-pass substructures, chemical formula). Prompts were constructed from these calculated properties and the SMILES used as the completion.",
            "generation_method": "Autoregressive SMILES completion via supervised fine-tuning (SFT) on prompt→SMILES pairs; sampling with temperature annealing (authors explored T in 0.01–2.0, optimal often 0.6–1.4, T=1.1 maximizing valid/novel/non-redundant output in some tasks). Zero-shot prompting possible after fine-tuning.",
            "chemical_representation": "SMILES (regular SMILES strings); BRICS substructure SMILES and SMARTS used in prompts/constraints.",
            "target_application": "De novo small-molecule generation for drug discovery: generate drug-like molecules satisfying medicinal-chemistry properties and scaffolds; can be prompted to focus chemical space for lead-like/fragment-like design and downstream predicted target binding.",
            "constraints_used": "Property ranges specified in prompts (e.g., ≤k HBD/HBA, MW, logP, rotatable bonds, TPSA, Fsp3); presence/absence of SMARTS (undesirable groups, covalent warheads); ability to request retention of BRICS substructures or chemical formula; duplicate removal for diversity during sampling.",
            "integration_with_external_tools": "RDKit for property computation, validity checking, SMARTS matching and BRICS decomposition; GuacaMol benchmark suite and Frechet ChemNet Distance for distributional evaluation; training/inference tooling: Axolotl, LoRA low-rank adapters, accelerate, DeepSpeed; TDCommons (TDC) oracles used in related DPO experiments (separate model/step).",
            "dataset_used": "ChEMBL v28 (~2M SMILES) for SFT; additional re-randomized ChEMBL-derived dataset for second SFT; evaluation used GuacaMol benchmarks and Enamine substructure list.",
            "evaluation_metrics": "Validity (% valid SMILES), Uniqueness (% unique among generated), Novelty (% not in training set), KL divergence between generated and training property distributions, Fréchet ChemNet Distance (FCD), QED distributions, task-specific success rates (percentage of generated molecules satisfying prompt constraints), TDC oracle scores when used (for binding predictions).",
            "reported_results": "Authors report substantive improvement after SFT vs base Llama: for a panel of property tasks SmileyLlama achieved high task-success rates (examples quoted in paper: ≤k H-bond donors: 95.7% (1xSFT), 96.0% (2xSFT), 97.1% (DPO); 'exactly k HBD' tasks: ~21.4% (1xSFT) → ~30.4% (DPO); Lipinski rule-of-five success: 65.3% (1xSFT) → 84.9% (DPO) in reported tables). The authors found optimal sampling temperatures typically 0.6–0.8 for SFT tasks and T≈1.1 for maximizing valid/novel/non-redundant outputs in unconstrained generation. Distributional comparisons (Figure S1) show property distributions of generated molecules are similar to ChEMBL for most metrics; QED of generated molecules trended slightly higher than training set. Frequency of undesirable SMARTS was lower in SmileyLlama than in ChEMBL for several categories (e.g., 'unsaturated benzene' 0.55% in ChEMBL vs 0.0% in SmileyLlama in Table S2).",
            "experimental_validation": false,
            "challenges_or_limitations": "Authors report limits including degraded performance on tasks the model was not trained on (e.g., exact-count HBD or some Lipinski exact constraints), a tendency for reduced diversity / 'crashing diversity' and partial memorization after optimization steps, temperature-dependent tradeoffs between diversity and validity, and the absence of any wet-lab synthesis/biological validation. They also note that SFT acts mainly as a prompting wrapper that elicits latent capability and that multimodal/protein-specific inputs were not used and remain future work.",
            "uuid": "e6868.0",
            "source_info": {
                "paper_title": "SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SmileyLlama-Opt (DPO)",
            "name_full": "SmileyLlama-Opt (Direct Preference Optimization of SmileyLlama)",
            "brief_description": "A variant of SmileyLlama further optimized with Direct Preference Optimization (DPO) to bias generation toward higher scores on SMILES-based scoring functions (e.g., QED, and TDCommons predicted binding scores for GSK3B, JNK3, DRD2) by pairing higher-scoring outputs with lower-scoring ones and updating model weights accordingly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SmileyLlama-Opt (DPO-optimized SmileyLlama / Llama-3.1-8B-Instruct)",
            "model_type": "decoder-only LLM, supervised fine-tuned + preference-optimized (SFT + DPO)",
            "model_size": "8B",
            "training_data_description": "Initial SFT dataset from ChEMBL v28 (~2M SMILES) plus DPO training data constructed from the model's own generated SMILES: for each DPO iteration the authors sampled 2,000 SMILES per objective (QED, GSK3B, JNK3, DRD2), enforced uniqueness, scored with oracles, paired winners/losers randomly to form preference pairs; paper reports an aggregate DPO dataset of ~76,000 examples used for DPO optimization.",
            "generation_method": "Direct Preference Optimization: sample many SMILES from the SFT model given a target prompt, score each sample with a scoring function/oracle, form winner-loser pairs (higher-scoring vs lower-scoring), and perform gradient updates to increase probability of winners (authors ran single-epoch DPO loops repeated over ~20 epochs for some tasks). Sampling temperature varied (authors used T=0.8–1.1 in experiments).",
            "chemical_representation": "SMILES strings",
            "target_application": "Optimize generated molecules for drug-like metrics (QED) and predicted target binding to protein targets (GSK3B, JNK3, DRD2) as assessed by TDCommons/TDC oracles; also multi-objective generation by combining prompts for several objectives.",
            "constraints_used": "Same prompt-specified property ranges and SMARTS constraints as SFT; during DPO the scoring function (RDKit property checks or TDC oracle scores) acts as the optimization objective; authors enforced diversity by discarding duplicate SMILES before scoring.",
            "integration_with_external_tools": "TDCommons / TDC oracles for target-affinity prediction (GSK3B, JNK3, DRD2), RDKit for property computation and validity checking, Axolotl and LoRA for training infrastructure, accelerate and DeepSpeed for efficiency.",
            "dataset_used": "ChEMBL v28 for SFT; generated SMILES scored by RDKit or TDC oracles to form DPO training pairs (authors sampled 2,000 SMILES per target per DPO epoch; reported ~8,000 responses when optimizing four objectives at once and ~76,000 DPO examples overall).",
            "evaluation_metrics": "Oracle scores (TDC predicted binding scores, median and percentiles across epochs), uniqueness (unique SMILES/total), validity, novelty (percent not in SFT dataset), task-specific success rates (percentage achieving desired property windows), QED distributions, GuacaMol metrics and FCD/KL for distributional shift.",
            "reported_results": "DPO substantially improved the ability to generate molecules with higher oracle scores: median TDC oracle scores for target binding rose substantially after ~10 DPO epochs for GSK3B and JNK3, with the fraction of unique generated molecules predicted active rising above 0.5 for those targets. DPO-optimized models also improved property task success rates reported in benchmark tables (examples in paper: improvements from SFT to DPO on multiple prompts; see SmileyLlama → SmileyLlama-Opt numbers such as ≤k HBD 95.7%→97.1%; exact HBD 21.4%→30.4%; Lipinski rule-of-five 65.3%→84.9% in reported tables). Authors also found that models optimized separately for single objectives could be prompted to combine those objectives at inference (implicit multi-objective capability). DPO training (≈76k examples) took ~3 hours 30 minutes on a single 2×A40 node according to the paper.",
            "experimental_validation": false,
            "challenges_or_limitations": "DPO caused a 'crashing diversity' effect (diversity decreased after score plateaus), model memorization of frequently seen molecules during optimization, a tradeoff between temperature (higher T delays diversity collapse but slows convergence), dependence on the fidelity and biases of scoring/oracle models (no experimental binding validation), and limitation to SMILES-based scoring functions (no multimodal protein-structure conditioning).",
            "uuid": "e6868.1",
            "source_info": {
                "paper_title": "SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Llama-3.1-8B-Instruct",
            "name_full": "Meta-Llama-3.1-8B-Instruct (base model)",
            "brief_description": "Open-weight instruction-tuned Llama model used as the foundation model; authors show it can produce SMILES zero/few-shot but is substantially improved by SFT on chemical property→SMILES pairs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct",
            "model_type": "decoder-only LLM, instruction-tuned",
            "model_size": "8B",
            "training_data_description": "Pretrained on general language/instruction datasets (paper does not detail pretraining corpora for Llama), not specialized on chemistry-specific SMILES corpora.",
            "generation_method": "Zero-, one-, few-shot prompting to produce SMILES strings (authors tested zero/one/two/five/ten/twenty shot configurations in GuacaMol benchmark comparisons).",
            "chemical_representation": "SMILES produced via prompting",
            "target_application": "Baseline generative capability for SMILES; serves as foundation to be converted to a CLM via SFT and DPO",
            "constraints_used": "When used zero-shot/few-shot authors provided few-shot examples in prompt; no inherent property constraints unless provided in prompt.",
            "integration_with_external_tools": "Evaluated with RDKit for validity and GuacaMol metrics; used as the pre-fine-tuning base for SmileyLlama.",
            "dataset_used": "No chemical-specific training dataset used for the base model in this paper; evaluation used ChEMBL examples and GuacaMol benchmarks.",
            "evaluation_metrics": "GuacaMol metrics: validity, uniqueness, novelty, KL divergence, Fréchet ChemNet Distance. The authors report substantially lower values for Llama zero-shot compared to SFT models (example numbers reported in Table S1 for Llama zero-shot T=0.6: Validity ≈0.8867, Uniqueness ≈0.0830, Novelty ≈0.1458 — as reported in the paper's supplemental table).",
            "reported_results": "Llama can generate syntactically correct SMILES in zero-shot, but is far below fine-tuned SmileyLlama in validity/uniqueness/novelty and distributional similarity; increasing number of examples in the prompt increased uniqueness/novelty at cost of validity per authors' analysis.",
            "experimental_validation": false,
            "challenges_or_limitations": "Without chemistry-specific SFT it produces fewer valid/novel drug-like molecules; few-shot prompting increases diversity but reduces validity; lacks direct property control that SFT/DPO impart.",
            "uuid": "e6868.2",
            "source_info": {
                "paper_title": "SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
            "rating": 2,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        },
        {
            "paper_title": "Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development",
            "rating": 2,
            "sanitized_title": "therapeutics_data_commons_machine_learning_datasets_and_tasks_for_drug_discovery_and_development"
        },
        {
            "paper_title": "Benchmarking Models for de Novo Molecular Design (GuacaMol)",
            "rating": 2,
            "sanitized_title": "benchmarking_models_for_de_novo_molecular_design_guacamol"
        },
        {
            "paper_title": "SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules",
            "rating": 1,
            "sanitized_title": "smiles_a_chemical_language_and_information_system_1_introduction_to_methodology_and_encoding_rules"
        },
        {
            "paper_title": "The Llama 3 Herd of Models",
            "rating": 1,
            "sanitized_title": "the_llama_3_herd_of_models"
        },
        {
            "paper_title": "Chemical language models enable navigation in sparsely populated chemical space",
            "rating": 1,
            "sanitized_title": "chemical_language_models_enable_navigation_in_sparsely_populated_chemical_space"
        }
    ],
    "cost": 0.0176505,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration
9 Sep 2024</p>
<p>Joseph M Cavanagh 
Kenneth S. Pitzer Theory Center
Department of Chemistry
University of California
94720BerkeleyCAUSA</p>
<p>Kenneth S. Pitzer Theory Center
Department of Chemistry
University of California
94720BerkeleyCAUSA</p>
<p>Kunyang Sun 
Kenneth S. Pitzer Theory Center
Department of Chemistry
University of California
94720BerkeleyCAUSA</p>
<p>Kenneth S. Pitzer Theory Center
Department of Chemistry
University of California
94720BerkeleyCAUSA</p>
<p>Andrew Gritsevskiy 
Department of Computer Science
University of Wisconsin-Madison
53706MadisonWI</p>
<p>Department of Computer Science
University of Wisconsin-Madison
53706MadisonWI</p>
<p>Dorian Bagni 
Kenneth S. Pitzer Theory Center
Department of Chemistry
University of California
94720BerkeleyCAUSA</p>
<p>Kenneth S. Pitzer Theory Center
Department of Chemistry
University of California
94720BerkeleyCAUSA</p>
<p>Thomas D Bannister 
Department of Molecular Medicine
The Herbert Wertheim UF Scripps Institute for Biomedical Innovation and Technology
130 Scripps Way33458JupiterFL</p>
<p>Department of Molecular Medicine
The Herbert Wertheim UF Scripps Institute for Biomedical Innovation and Technology
130 Scripps Way33458JupiterFL</p>
<p>Teresa Head-Gordon 
Kenneth S. Pitzer Theory Center
Department of Chemistry
University of California
94720BerkeleyCAUSA</p>
<p>Departments of Bioengineering and Chemical and Biomolecular Engineering
University of California
94720BerkeleyCAUSA</p>
<p>Chemical Sciences Division
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Kenneth S. Pitzer Theory Center
Department of Chemistry
University of California
94720BerkeleyCAUSA</p>
<p>Departments of Bioengineering and Chemical and Biomolecular Engineering
University of California
94720BerkeleyCAUSA</p>
<p>Chemical Sciences Division
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration
9 Sep 20245C11CE9E192DD1907D96C425065365E2arXiv:2409.02231v2[physics.chem-ph]
Here we show that a Large Language Model (LLM) can serve as a foundation model for a Chemical Language Model (CLM) which performs at or above the level of CLMs trained solely on chemical SMILES string data.Using supervised fine-tuning (SFT) and direct preference optimization (DPO) on the open-source Llama LLM, we demonstrate that we can train an LLM to respond to prompts such as generating molecules with properties of interest to drug development.This overall framework allows an LLM to not just be a chatbot client for chemistry and materials tasks, but can be adapted to speak more directly as a CLM which can generate molecules with user-specified properties.</p>
<p>Introduction</p>
<p>Language Models (LMs) are statistical models of probability distributions of units of language, and can be adapted to autoregressively generate text by sampling from these distributions 1 .Although LMs were originally adapted for the domain of natural language, in recent years, chemical language models (CLMs) trained on different string representations of small molecules have emerged as a useful tool for de novo generation of molecules, especially those with potential pharmaceutical applications [2][3][4][5][6][7] .Among the most notable one dimensional representations of molecules are Simplified Molecular-Input Line-Entry System (SMILES) strings 8 and SELF-referencing Embedded Strings (SELFIES) 9 .Early work using CLMs for molecular generation has also explored different combinations of string representations with model architectures, including recurrent neural networks (RNNs) using long short term memory (LSTM) cells 10,11 , generative pre-trained transformers (GPTs) 12,13 and structured state space sequence (S4) models 14,15 .While these models have achieved advances in molecular generation tasks, downstream optimization of molecules, especially for properties useful in medicinal chemistry, requires additional training or different model frameworks 5,6,[16][17][18] .Aside from model architecture development, many recent advancements of LMs have resulted from training scaled-up transformers 19 on massive amounts of data.These Large Language Models (LLMs) such as GPT4 and Llama show capabilities across a surprisingly broad range of natural language processing tasks [20][21][22] .</p>
<p>Roughly speaking, there are four main families of methods for steering the outputs of a pre-trained LLM.The first is through prompt engineering, wherein a pre-trained LLM is given a task description, possibly with examples, in its context window 23 .This does not provide the LLM with any additional knowledge (except for the description and any examples), but rather extracts knowledge obtained through pre-training.The second approach is supervised fine-tuning (SFT), in which the weights of a LLM are further optimized on task-specific data.However, fine-tuning rarely alters the model's underlying capabilities, and is better mechanistically described as a "wrapper" which efficiently elicits existing ability 24 .The third is through scoring the outputs of a LLM and updating the weights such that the language model is more likely to produce higher-reward outputs; approaches include directly using reinforcement learning (REINFORCE) 25 , Reinforcement Learning through Human Feedback (RLHF) 26,27 , and Direct Preference Optimization (DPO) 28 .Fourth and finally, researchers can use interpretability techniques-either by discovering monosemantic features corresponding to human-interpretable concepts, and then modifying these to guide the model towards specific behaviors, or through using activation addition of relevant "steering vectors" [29][30][31][32][33] .These various methods are not mutually exclusive, and can be combined with each other to achieve even better results.In this study, we focus on all but the last of these methods to modify LLMs into CLMs.</p>
<p>Here, we demonstrate the usefulness of SFT and DPO by converting an open-weight LLM, Meta-Llama-3.1-8B-Instruct(which we will also refer to as "Llama" for the remainder of the paper), into a useful chemical language model 22 .Our methods here consist of two steps.First, supervised fine-tuning (SFT) of Llama yields a model, SmileyLlama, with the ability to generate drug-like molecules that have properties specified in a prompt.Second, we use DPO to optimize our model's outputs towards one or multiple SMILES-based scoring functions, yielding a new model, SmileyLlama-Opt.This lets us improve the reliability of our model for generating molecules with certain properties.Separately, we investigate the use of DPO to grant SmileyLlama the ability to explore new, useful regions of chemical space such as the binders of specific proteins, including those not represented in the original dataset.</p>
<p>Methods</p>
<p>Supervised Fine-Tuning</p>
<p>The procedure we used for fine-tuning Llama is simple and extensible.We constructed our fine-tuning dataset with the SMILES strings of approximately 2 million molecules from ChEMBL Dataset v28 34 .For each molecule in our dataset, we randomly picked a number of properties of pharmaceutical interests to calculate using RDKit 35 .These properties include ranges of hydrogen-bond donors, hydrogen-bond acceptors, molecular weight, logP, number of rotatable bonds, the fraction of sp 3 carbons (i.e., the number of sp 3 carbons divided by the total number of carbon atoms in the molecule, termed F sp 3 ), the presence/absence of macrocycles, the presence/absence of covalent warhead-related SMARTS patterns, the presence/absence of at least one undesirable SMARTS pattern, a SMILES string representation of a BRICS single-pass substructure and the specification of a chemical formula.</p>
<p>For numerical properties, we choose ranges that are relevant for medicinal chemistry, recognizing a distinction between chemical diversity space and drug-like chemical space, where molecules must have suitable characteristics of drugs related not just to their shape but also to relevant biological phenomena (e.g., oral absorption, metabolism, distribution).For example, drug design starting molecules are typically smaller in size and lipophilicity than their follow-on analogs and optimized drugs, so a user can specify that a model obeys the Rule of Three for fragment-based drug discovery when generating molecules 36 .As another example, we choose the TPSA ranges based on those which tend to be orally bioavailable or able to pass through the placenta or blood-brain-barrier 37 .If a drug need not meet these criteria (e.g., an injectable drug for a peripheral target) then a user can adjust the TPSA range criterion.</p>
<p>After calculating and picking these properties for each SMILES string, we construct a prompt containing values of these properties, with the "correct" completion being the SMILES string that these properties were calculated from.This trains our model to generate molecules that have properties specified in the prompt.See Figure 1 and Algorithm 1 in the Supporting Information (SI) section 2 for other representations of this method not discussed in the main text below.Further information about the specifics of these properties and the ranges we choose to specify during training can be found in the SI section 1.</p>
<p>Direct Preference Optimization</p>
<p>First, we use DPO to improve our model's ability to robustly generate molecules with properties specified in the prompt.We prompt our fine-tuned model to generate molecules with a given property, such as 3 or fewer H-Bond donors.We sample several SMILES strings and use RDKit to assess whether they have the correct properties.We pair up molecules which correctly followed the prompt with those that don't as winners and losers, respectively, then use a single epoch of DPO to improve the model's results.See Algorithm 2 in SI section 2 for pseudocode of this scoring and pairing procedure.</p>
<p>We also use Direct Preference Optimization to add four new capabilities to SmileyLlama, namely QED and the binding to GSK3β, JNK3, and DRD2 as assessed by machine learning models implemented by TDCommons' oracle [38][39][40][41] .To do this, we prompt the model to create molecules with the property of a 'High QED/GSK3B/JNK3/DRD2', sampling 2000 of these SMILES strings.This gives us 8,000 responses in total, with 2,000 generated for each target.We enforce diversity here, discarding any string which is identical to another.Discarding redundant SMILES strings here has two purposes.First, it gives us the best use of a set number of oracle calls.Second, we found that this helps ameliorate crashing diversity in later epochs due to DPO causing the model to simply memorize the molecules it sees most often, although it doesn't completely solve this issue.We then score these responses with the TDCommons oracle corresponding to the property specified in the prompt and pair each response with a random different response to the same prompt.We identify the 'winner' as the molecule with the greater score and the 'loser' as the molecule with the lesser score.We then run a single epoch of DPO before repeating the process.It should be emphasized that we did not train separate models for each objective-we simply specified which score (QED/GSK3B/JNK3/DRD2) our model is supposed to optimize in the prompt.We report the results for 20 epochs of this procedure.See Algorithm 3 in SI section 2 for pseudocode of the preparation of each iteration of DPO.</p>
<p>Training Details</p>
<p>We performed both SFT and DPO on Llama using the Axolotl Package 22,42 .4][45][46] Additional parameters for our training are a LoRA rank of 32, a LoRA alpha of 16, a LoRA dropout of 5%, and 10 warmup steps.We use the accelerate and deepspeed packages to improve the efficiency of our training  . For ST, we trained for 1 epoch using a micro batch size of 4 on a single 4xA40 node for approximately 1 day.We also construct another dataset to train on from the same list of SMILES strings from ChEMBL, but with re-randomized hints.</p>
<p>We performed both SFT and DPO on Llama-3.1-8B-Instruct to create SmileyLlama and SmileyLlama-Opt as described here.For SFT we used a prompt with a system instruction of You love and excel at generating SMILES strings of drug-like molecules and a user instruction of the form Output a SMILES string for a drug like molecule with the following properties: if properties are specified, or Output a SMILES string for a drug like molecule: when no properties are specified.We structure the prompts used for SFT so that, during inference, the properties of generated molecules can be specified in the prompt.This lets users avoid having to downselect the vast majority of generated molecules for having the correct characteristics.Figure 1 shows the overall workflow for the SFT procedure for SmileyLlama.</p>
<p>Results</p>
<p>Temperature dependence and distribution of molecule properties</p>
<p>We begin our investigation of SmileyLlama by generating molecules adjusting the temperature hyperparameter.We sample 10,000 molecules at various temperatures without properties specified in our prompt.We generate these samples independently from each other and in parallel, rather than letting the model generate 10,000 molecules sequentially.This is both to avoid potential biases introduced by earlier molecules in the context and to speed up inference due to the quadratic cost of an attention head.In Figure 2a, we find that the optimal temperature is between 0.6 and 1.4, depending on the relative importance of diversity and validity.The largest percentage of SMILES strings which were valid, novel, and non-redundant occurred at T=1.1.This is a departure from typical LLMs, where generating many short, unique responses is typically not required and the ideal temperature is typically 0.6-0.8 22.</p>
<p>We also investigate the distribution of properties of generated molecules and compare these to the ChEMBL training set.We find that, for the majority of properties, the two are remarkably similar, as shown in Figure S1.The primary exception here is QED (Figure 2b), where the generated molecules tend to score slightly higher than those of the training set.</p>
<p>Comparison of SmileyLlama with few-shot Llama and other CLMs</p>
<p>To test the generation ability of Llama-FT with other existing methods, we used the GuacaMol suite 79 to benchmark the validity, uniqueness, and novelty of the molecules.Additionally, KL divergence and Frechet Chemnet Distance (FCD) 80   (a) dependence on temperature.We generated 10,000 SMILES strings at temperatures of 0.01 and 0.1 through 2.0 with intervals of 0.1 (b) QED score distributions for 10,000 molecules generated by SmileyLlama at T=1.1 and 10,000 randomly sampled ChEMBL compounds.We used matplotlib 78 to generate the plots in this and all following figures, including those in the SI.</p>
<p>In Table 1, we show that SFT significantly improves SmileyLlama's ability to generate drug-like molecules.</p>
<p>Table 1: GuacaMol benchmarks for the Llama before and after SFT and compared to other CLMs.The model benchmarks include valid chemical molecules, uniqueness and novelty with respect to the training set, and distribution similarity evaluated using KL divergence and Frechet ChemNet distance.The temperatures were chosen as those at which the model performed fairly well, a more complete table (S1) can be found in the SI.All models generated 10,000 samples for this benchmark.</p>
<p>Benchmark</p>
<p>Validity Uniqueness Novelty KL div FCD Llama zero-shot (T=0.We also investigated the ability of Llama-3.1-8B-Instruct to produce molecules with no fine-tuning by providing it with zero, one, and few-shot prompts from the ChEMBL database.We find that even without SFT or examples provided in a prompt, Llama is able to produce accurate SMILES strings, although far from the level of the fine-tuned model or other state-of-the-art CLMs.As shown in Table S1, we find that the more examples we provided in the prompt, the more unique and novel molecules will typically be generated, at the cost of losing validity.In contrast, SmileyLlama, with the user instruction of Output a SMILES string for a drug like molecule:, generated SMILES strings with no examples given in the prompt (zero-shot) which saves on computational cost during inference while providing outputs on par with or better than state-of-the-art CLMs.We also find that while doing a second round of SFT on a second dataset from the same ChEMBL source improves the validity and uniqueness somewhat, it's not anywhere near as consequential as the first and slightly hurts novelty, as shown in Table 1.</p>
<p>Property Specification</p>
<p>We next test SmileyLlama's ability to generate molecules with one of any of the criteria we used for SFT (ranges are given in the SI), with a panel of 380 tasks.One example is a case with a specific number of H-bond donors and acceptors, a task it was never trained on; as a reminder, SmileyLlama was trained on examples corresponding to generation of a molecule from a range of H-bond donors.We also test our model's ability to generate molecules from all 320 substructures in the Enamine database 81 and to generate molecules that follow the Lipinski rule-of-five and rule-of-three 82 .We sampled 1,000 SMILES strings from each prompt, with the exception of the Lipinski rules for which we sampled 5,000 SMILES strings.Optimal performance was typically found at T=0.6 to 0.8.</p>
<p>In Table 2 we show the average percentage of valid and unique SMILES strings generated for each family of specified properties.We note that for the English language, 'unique' has a somewhat ambiguous meaning.The number of unique items can refer to both the number of items, not counting duplicates, or the number of items which do not have a duplicate.We mean the former when we refer to the proportion of valid, unique elements-in python, this proportion would be len(set(valid_items))/len(items) and the percentage would be the proportion times 100%.We find that our SFT model typically does well on some tasks, with the exception of those it wasn't trained on such as exact H-bond donors and Lipinski rules.Fine-tuning on twice the amount of data did not seem to greatly affect the performance on these benchmarks (2xSFT).</p>
<p>Table 2: Percentage of valid, unique generated molecules over a panel of tasks using SmileyLlama.Percentage of valid SMILES strings with duplicates removed out of 1,000 that fit the properties specified in the prompt, for each family of properties.For example, ≤ k H-bond donors refers to the average number of valid, unique SMILES strings corresponding to molecules with ≤ k H-bond donors for k = 3, 4, 5, 7, with 1,000 examples generated for each.The temperatures shown offered the best performance for these models, which can be seen in the more complete table S3.More results are also shown in the SI.1xSFT refers to SmileyLlama, DPO refers to SmileyLlama-Opt and 2xSFT refers to SmileyLlama 2xSFT (as in We further optimized SmileyLlama for this task using DPO.DPO's most popular application has been in improving the responses of LLM-derived ChatBots, such as the instruct-tuned Llama3 models, but it has also found use in improving the outputs of CLMs 83 .Here the relevance of DPO provides a way for us to further optimize the model by pairing desirable responses with undesirable responses.The model's weights are then updated to be more likely to produce the 'winner' of the pairing and less likely to produce the 'loser' of the pairing.This avoids the need to separately train a reward model 28 .We generated our dataset by simply pairing up unsuccessful attempts at generating structures with successful attempts randomly.This gives us the minimum of (successful results, failed results) pairings for each task.We lump these pairings into a single dataset and optimize for 1 epoch using DPO.In this case, there were about 76,000 examples; DPO took about 3 hours and 30 minutes on a single 2xA40 node.The new DPO-optimized model, SmileyLlama-Opt, significantly improved results on the benchmark across the board, albeit with a higher optimal temperature than the optimal temperature for the SFT models as seen in Table 2.</p>
<p>Optimizing for target affinities and implicit multi-objective optimization</p>
<p>Finally we consider DPO for generating unique and valid ligands which bind to specific protein targets.In this case the SmileyLlama model is prompted to generate molecules with a high score for the four objectives from TDCommons: QED and binding to the drug target proteins GSK3B, JNK3, and DRD2.DPO training involved pairing each output smiles string with another, random string, leading to 2000 random pairings per task-epoch, with winners and losers assigned based on the higher score.This paring is directly shown in Algorithm 3 of SI section 2.</p>
<p>As seen in Figure 3a the SmileyLlama model doesn't initially understand the task at hand, so the median scores, aside from QED, start very low but rises substantially after ∼10 epochs such that unique, generated molecules rises above 0.5 in every score, which is the threshold for predicted activity for binding to GSK3B and JNK3. 84As shown in Figure 3b, however, diversity decreases shortly after the scores plateau, but typically remain above 50%.Increasing temperature causes the model to require more epochs for the same improvement, but postpones and attenuates the crashing of diversity, as shown in Figure S2 in the SI section 4. A surprising corollary to this is the finding that SmileyLlama-TDC-DPO can combine its knowledge gained during single-objective optimization to perform well at a task specifying multiple objectives.We find that we can can elicit this by combining the prompts used during direct preference optimization.For instance, in Figure 3c we show the distributions of QED and DRD2 scores of 400 molecules generated by SmileyLlama-TDC-DPO using three different prompts.We find that even though our model was not trained on the generation of molecules with both a high QED and a high DRD2 score, it was able to combine the individuated training to yield predicted molecules with both attributes.</p>
<p>Discussion and Conclusion</p>
<p>There have been other efforts to train LLMs on chemistry-focused data resulting in a model which can, among other tasks, translate between natural and chemical language 85,86 .Some more explicit attempts to elicit molecular generation capabilities from LLMs have used in-context learning and prompt engineering to generate molecules or optimize them for certain characteristics 87 .Reinforcement learning has also been explored as a way to optimize LLMs' generation for targets 88 .There have also been studies of using in-context learning on either a specialized LLM or a general purpose LLM after SFT to generate and optimize molecules for targets with intriguingly successful results 87,[89][90][91] .</p>
<p>These advances show promise in using LLMs for molecular generation in drug discovery, but our study clarifies a few crucial points in this, going forward.First, it is not necessary to pretrain a specialized model on chemistry-specific text to generate molecules from a text description; a much less resource-intensive SFT training run + DPO on prompt-following on a dataset of a few million molecules is enough to do that.Second, DPO provides a resource-efficient way of optimizing the model to produce molecules that score well on a SMILES-based objective with zero examples or in-context learning.Third, training the same model using DPO on multiple individual criteria optimizes the model explicitly for a set of single-objectives, and implicity for a combination of objectives which can be specified in a prompt.This sets the stage for new, far more data-and-compute efficient ways for multi-objective optimization in CLMs.Rather than having to optimize a model's output in 2 n tasks, we could have a model with outputs optimized in n tasks, then combine the prompts used to specify these tasks in many different combinations.This property, if it continues to hold in the limit of more tasks, could help specify something like a single model, trained on a large variety of targets, which can be used as something like a "foundation model" for molecular generation tasks, having enough latent ability to optimize for a wide range of objectives that it can be later fine-tuned for the purposes of excelling at a specific molecular generation task.It would also be useful to be able to specify where, exactly, on the Pareto frontier a model should "aim for" when generating molecules that score well on multiple objectives.Such questions could serve as useful further research directions.</p>
<p>In summary, we have investigated several aspects of the adaptation of LLMs to problems that have been in the domain of CLMs.The property that has been key to all of the special advantages here is one of prompting-that LLMs can be fine-tuned and optimized with DPO to follow different tasks given different prompts.Information about the region of interest in chemical space can be specified on the prompt, letting the model generate structures in this region.Training on a variety of prompts on a large model allows for specialization in multiple tasks, and there is some evidence to suggest that the models can correctly generalize to scenarios with new prompts that leverage the abilities that the model developed when trained on similar prompts.</p>
<p>There are still a wide variety of unexplored possibilities in this space.Using the methods provided above, others may be able to allow an LLM to take in more useful forms of input in its prompt, such as data specific to the protein.A multimodal LLM could take advantage of information for which text cannot represent well.Alternatively, the framework for modifying LLMs to explore specific regions of chemical space introduced in this paper could also be leveraged for molecular design outside of drug discovery.As with many of the fields touched by large language models in the early 2020s, the newly opened frontier of possibility is as vast as it is exciting.</p>
<p>1 Details of properties to fine-tune Overview of selected properties for fine-tuning.When fine-tuning Llama to generate drug-like molecules, we carefully assess various design choices and proceed with the following properties that medicinal chemists would consider when proposing de novo drug molecules.We categorized and summarized all 12 properties into 4 subgroups as follows.</p>
<p>• Physiochemical properties.Absorption, distribution, metabolism, and excretion (ADME) are the crucial criteria to quantify the localization and concentration of drug molecules within the body after administration.As a result, we build on the list of properties proposed in the classical Lipinski's rule of 5 1 with some modern additions 2,3 to generate drug-like molecules that could demonstrate decent ADME.</p>
<ul>
<li>• Structure flexibility features.Binding sites within a targeted biomolecule (most often a protein) display by nature complex 3D geometry, with key potential sites of drug-target interactions (amino acid side chains, as an example) somewhat fixed in space.The protein, however, has a dynamic structure and even the binding pocket undergoes changes in shape.Drug-like molecules need to be sufficiently rigid to make efficient interactions with the target protein, including in most cases a high degree of selectivity over making corresponding interactions with related proteins).Perhaps less intuitive is that drug-like molecules must be flexible enough to maintain those interactions as the protein adapts its conformation.There is a "Goldilocks principle" at play, where too rigid or too flexible are each undesired extremes.Here, we chose the following two properties to account for the flexibility aspect.</li>
</ul>
<p>-Number of Rotatable Bonds (#rot)</p>
<p>-Whether the molecule contains a macrocycle (defined as an 8-membered ring or larger)</p>
<p>• Pattern-based features.In practical drug discovery, there are always some key patterns and/or scaffolds that medicinal chemists would like to hold onto or get rid of.For instance, in the lead optimization phase, retaining the key moiety and desired chemical formula are rather essential.Meanwhile, avoiding chemically unstable groups, PAINS molecules 4 , and molecules that would cause structure alerts could increase the chance of success in development.Therefore, we have the following three properties for fine-tuning.</p>
<p>-Avoidance of undesirable chemical patterns -Retention of specified substructure (between 50 and 250 Da in molecular weight)</p>
<p>-Chemical formula</p>
<p>• Covalent warhead feature.Drugs can be broadly categorized into noncovalent and covalent drugs, depending on whether the drug reacts with its target, for example, an electrophilic group forming a bond with a nucleophilic amino acid side chain.Each type has both advantages and limitations over the other type.While most drugs are non-covalent, either can be desired.To give the model the ability to generate covalent binders, we also curated common covalent warhead-related SMARTS patterns from the Enamine fragment library 5 to indicate whether our generated molecules have the capacity to covalently bind to the target or not.</p>
<p>-Whether the molecule contains common covalent warhead-related SMARTS patterns</p>
<p>Prompting options used in fine-tuning.To incorporate the properties mentioned above into the training, we used several ways of prompting to satisfy the requirement from target uses.For numerical properties, including all physiochemical properties and #rot, we prompted Llama by providing a specific range that the training molecules falls into for that specific category.All the cutoff values used for ranges are either commonly used standards in drug discovery or derived from the training distribution.Besides the range guidance, we added the prompt that tell Llama exactly how many #HBDs and #HBAs are contained in the training data that gives its ability to do more nuanced generation.If a property falls into multiple valid rangesfor instance, 4 H-bond donors is less than or equal to all of 4, 5 and 7-we select one of the ranges at random to place in the prompt (if the properly is picked to be placed into the prompt).It is important that the list of ranges for a property span all possible molecules, otherwise a prompt which omits information biases the model toward producing molecules with values of that property outside of the list of ranges.If we never included information in the prompt about when a molecule had over 7 H-bond donors, but did sometimes include information about the number of H-bond donors when they number under or equal to 7, then omitting the information would give the model a hint that the number of H-bond donors is more likely to be over 7. Doing this during training would bias results during inference.This is the same reason we specify undesirable properties such as the presence of bad SMARTS strings in the prompt sometimes. 1or other categorical properties, we used a combination of RDKit modules, SMILES strings, and SMILES arbitrary target specification (SMARTS) strings to recognize if certain properties or chemical patterns are present in the training input.Substructure SMILES strings generated from BRICS decomposition using RDKit are used for substructure withholding with an option to specify where to grow the molecules with a "" indicator.Unlike the objective of containing the scaffold exactly, chemical pattern avoidance and covalent warhead recognition required matching of more general substructures and/or certain functional groups.Here, we used SMARTS strings as our representation because of its ability of matching chemical patterns.More details about the specific SMARTS patterns used are shown later in this section.</p>
<p>Below is a detailed list of possible components of that could appear in a training prompt.</p>
<p>• N H-bond donors, N =≤ 3, ≤ 4, ≤ 5, ≤ 7, &gt; 7</p>
<p>• N H-bond acceptors, N =≤ 3, ≤ 4, ≤ 5, ≤ 10, ≤ 15, &gt; 15</p>
<p>• N Molecular weight, N =≤ 300, ≤ 400, ≤ 500, ≤ 600, &gt; 600
• N LogP, N =≤ 3, ≤ 4, ≤ 5, ≤ 6, &gt;6
• N Rotatable bonds, N =≤ 7, ≤ 10, &gt; 10</p>
<p>• N Fraction sp 3 , N =&lt; 0.4, &gt; 0.4, &gt; 0.5, &gt; 0.6  6 .In this work, we used the same list of SMARTS patterns as their work to avoid bad patterns, including cyclopentadiene, cyclopentadiene ylidenes, aromaticity-breaking tautomers, antiaromatic system, unstable halogen-heteroatom bonds, unstable fused rings, allenic system, thiazyl linkages, and peroxide bonds.In Table S2, we also present the frequency of sampling undesirable chemical groups in ChEMBL and across different generative models.
• [C^2]1=[C^2]-[C^2]=[C^2]~[C;!d4]~[C;!^2;d2]1 • [C^2]1~[C^2]~[C^2]~[C^2]~[C;!^2;d2]~[N]1 • [#6^2]1~[#6^2]~[#6^3;!d4]~[#6^2]2~[#6^2][ #6^2]~[#6^2]~<a href="~[ * ]">#6^2</a>~[#6^2]~2~[#6^2]~1 • [#6]1(=[ * ])[#6]=[#6][#6]=[#6]1 • [#6]1=[#6][R{2-}]=[R{2-}]1 • [#6^2]1~[#6^2]~[#6^2]~[#6^2]~[#6^1]~[#6^1]~1 • [#7,#8,#16]-[#9,#17,#35,#53] • [r3,r4]@[r5,r6] • [ * ]=[#6,#7,#8]=[ * ] • [#7,#16]=[#16] • [#8]-[#8]
In addition to the patterns mentioned above, we use the following SMARTS patterns to enforce our generated pyrroles to be one of the following correct forms.</p>
<p>•  •
[#8]=<a href="=[#8]">#16</a>(-[#9])-[#6] • [#8]=<a href="-[#6]-[#17]">#6</a>-[#7] • [#7]#[#6]-<a href="=[#6]">#6</a>-[#6] • [#6]#[#6]-<a href="=[#8]">#6</a>-[#7]-[#6]-,:[#6] • [#6]-,:[#6]-[#7]-<a href="=[#8]">#6</a>-<a href="-[#6]">#6</a>-[#17] • [#6]=[#6]-<a href="=[#8]">#6</a>-[#7]-[#6] • [#6]-<a href="-[#8]">#5</a>-[#8] • [#6]-[#6]-[#16]-[#16]-[#6]-[#6]-[#7]-<a href="=[#8]">#6</a>-[#6] • [#6]=[#6]-<a href="=[#8]">#16</a>(=[#8])-[#7] • [#8]=<a href="=[#8]">#16</a>(-[#9])-[#8]-[#6]:[#6] • [#6]-[#6]1-[#6]-[#8]-1 • [#6]-,:[#6]-[#6]-[#7]1-[#6]-[#6]-1-[#6]-[#8]-[#6] (:[#6]:,-[#6]:,-[#6]:,-[#6]):[#6] • [#8]=<a href="-[#7]">#6</a>-[#6]12-[#6]-[#6]-1-[#6]-2 • [#7]-<a href="=[#8]">#6</a>-<a href="-[#9]">#6</a>-[#17] • [#8]=[#6]-[#6]1:[#6]:[#6]:[#6]:[#6]:[#6]:1-<a href="-[#8]">#5</a>-[#8] • [#7]1-[#6]-[#6]-[#6]-1=[#8] • [#6]-,:[#6]-<a href="-[#6]#[#7]">#7</a>-[#6] • [#6]-<a href="-[#17]">#6</a>-<a href="=[#8]">#6</a>-[#7]-[#6] • [#8]=<a href="-[#6]-[#17]">#6</a>-[#7]-[#6] • [#7]-<a href="=[#8]">#6</a>-<a href="-[#6]#[#7]">#6</a>=[#6] • [#6]=[#6]-[#6]-[#7]-,:[#6]-,:[#6] • <a href="=[#8]">#16</a>(=[#8])-[#9] • <a href="=[#8]">#16</a>(=[#8])-[#6]=[#6] • [#6]-,:[#6]:,-[#6] • <a href=":,-[#6]:,-[#6]">#6</a>:,-[#6]:,-[#7]:,-[#6]
3 Supporting Tables    11 , molecular weight, approximate log partition coefficient between octanol and water (ALOGP) 12 , polarizable surface area (PSA) and topological PSA 13 , and the number of structural alerts 14 .</p>
<p>Figure 1 :
1
Figure 1: A visualization of the SFT workflow for Smiley-Llama.Given the Llama-3.1-8B-Instructmodel, we used prompt-response pairs consisting of calculated molecular properties and completed SMILES strings to fine-tune Llama on SMILES strings completions, yielding SmileyLlama.Crucially, we construct the prompt for each example using properties calculated from the correct response (a SMILES string from ChEMBL).</p>
<p>are used to analyze the distributional shifts from the ChEMBL training data.</p>
<p>Figure 2 :
2
Figure 2: The dependence of SmileyLlama on temperature and resulting QED metric from the GuacaMol Benchmark.(a)dependence on temperature.We generated 10,000 SMILES strings at temperatures of 0.01 and 0.1 through 2.0 with intervals of 0.1 (b) QED score distributions for 10,000 molecules generated by SmileyLlama at T=1.1 and 10,000 randomly sampled ChEMBL compounds.We used matplotlib78 to generate the plots in this and all following figures, including those in the SI.</p>
<p>Figure 3 :
3
Figure 3: Results of the DPO of SmileyLlama on TDC scores.(a) Median score and (b) the uniqueness (unique SMILES strings / total SMILES strings) of 2000 molecules generated per epoch.Figure S3 (SI section 4) contains more details about the results of this procedure.(c) Box-and-whisker plots demonstrating SmileyLlama-TDC-DPO adaptation to a multi-objective task (right) without additional training.It is compared to its performance on two single-objective optimization tasks (left and center).More such box-and-whisker plots can be found in Figure S4.</p>
<hr />
<p>Number of Hydrogen Bond Donors (#HBD) Number of Hydrogen Bond Acceptors (#HBA) -Molecular Weight (MW) Log of Partition Coefficient (LogP) -Topological Polar Surface Area (TPSA) Fraction of sp 3 hybridized carbon atoms (F sp 3 )</p>
<p>1 • 1 • 1 • 1 • 1 •
11111
[N^2]1~<a href="=[ * ]">C,N;^2</a>~[C,N;^2]~[C,N;^2]~[C^3][N^2]1~[C,N;^2]~<a href="=[ * ]">C,N;^2</a>~[C,N;^2]~[C;^3][N^2]1~[C,N;^2]~[C,N;^2]~<a href="=[ * ]">C,N;^2</a>~[C;^3]<a href="=[ * ]">C,N;^2</a>1~[N;^2]~[C,N;^2]~[C,N;^2]~[C;^3][C,N;^2]1~[N;^2]~<a href="=[ * ]">C,N;^2</a>~[C,N;^2]~[C;^3][C,N;^2]1~[N;^2]~[C,N;^2]~<a href="=[ * ]">C,N;^2</a>~[C;^3]1SMART patterns used to encode common covalent warhead-related functional groups.</p>
<p>Figure S1 :
S1
Figure S1: Distribution comparisons for 15 different properties of the generated molecules from SmileyLlama (blue) with molecules from the training dataset from ChEMBL (orange).The molecular properties considered are well-recognized chemical features related to the drug-likeliness of a molecule which can be obtained through 2D topological connectivity of the molecule: fraction of sp 3 hybridized carbons and heteroatoms, number of heavy atoms, number of hydrogen bond donors and acceptors, number of aliphatic and aromatic rings and the maximum ring size, number of rotatable bonds, quantitative estimate of drug-likelihood (QED) value11  , molecular weight, approximate log partition coefficient between octanol and water (ALOGP)12  , polarizable surface area (PSA) and topological PSA13  , and the number of structural alerts14  .</p>
<p>Figure S2 :
S2
Figure S2: Plots showing the median TDC oracle scores for the 1000 unique SMILES string outputs generated at various temperatures from repeating the procedure in Optimizing outputs for target affinities using various temperatures for the inference steps used to generate the SMILES strings for the next generations' dataset.Below is the uniqueness of the first 1000 SMILES strings samples at each epoch for jobs at various temperatures.</p>
<p>Figure S3 :
S3
Figure S3: Plots showing the 25, 50th, 75th and 100th percentiles for TDC scores for the DPO optimization where inference was run at T=0.8 (top along with the validity, uniqueness and novelty over time (bottom) for 2000 randomly sampled SMILES strings generated at each epoch.Novelty here is defined as the percentage of SMILES strings not in the SFT dataset from ChEMBL.</p>
<p>Figure S4 :
S4
Figure S4: Box-and-Whiskers plots of SmileyLlama-TDC-DPO optimized for 20 epochs of DPO with samples generated at T=0.8.400 samples were generated for each property specification in the prompt (shown at the top of each plot) and scored according to the 4 oracles (at the bottom of each plot).SmileyLlama-TDC-DPO was only trained using the first four prompts shown; all others are out of the training distribution.</p>
<p>Table 1 )
1Property ≤ k H-bond donors ≤ k H-bond acceptors ≤ k molecular weight ≤ k ClogP Exactly k H-bond donors Enamine Substructures Lipinski rule-of-five Rule-of-three1xSFT, T=0.6 2xSFT,T=0.6 DPO, T=1.1 95.7% 96.0% 97.1% 93.4% 94.4% 97.8% 76.8% 78.5% 97.0% 79.2% 79.8% 96.0% 21.4% 21.7% 30.4% 48.5% 50.3% 70.9% 65.3% 66.7% 84.9% 42.8% 44.0% 92.8%</p>
<p>SMART patterns used to identify bad chemical groups.Li et al. pointed out a list of bad chemical patterns that exists in ChEMBL database, which will negatively affect compound generation
• N TPSA, N =≤ 90, ≤ 140, ≤ 200, &gt; 200• a macrocycle, no macrocycles• has bad SMARTS, lacks bad SMARTS• has covalent warheads, lacks covalent warheads• substructure of <em>a_smiles_string</em>• a chemical formula of <em>formula</em></p>
<p>Common covalent warheads are extracted from the Enamine Covalent Screening and Covalent Fragment Library 7 .The key functional groups include Acrylamides, Chloroacetamides, Cyanoacrylamides, Aldehydes, Sulfonyl fluorides, Fluorosulfates, Butynamides, Boronic acids/esters, Vinyl sulfones, Chloropropanamides, beta-Lactams, Formyl Boronics, Disulfides, Epoxides, Aziridines, Bicyclobutanes, ChloroFluoroAcetamides, Cyanamides, and Active Arylators.The list of SMARTS strings is shown below.</p>
<p>Table S1 :
S1
GuacaMol benchmarks for the Llama before and after fine-tuning and other SOTA methods.The model benchmarks include valid chemical molecules, uniqueness and novelty with respect to the training set, and distribution similarity evaluated using KL divergence and Frechet ChemNet distance.
Benchmark Llama zero-shot (T=0.6) Llama one-shot (T=0.6) Llama two-shot (T=0.6) Llama five-shot (T=0.6) Llama ten-shot (T=0.6) Llama twenty-shot (T=0.6) Llama zero-shot (T=1.1) Llama one-shot (T=1.1) Llama two-shot (T=1.1) Llama five-shot (T=1.1) Llama ten-shot (T=1.1) Llama twenty-shot (T=1.1) SmileyLlama (T=0.6) SmileyLlama 2xSFT (T=0.6) SmileyLlama (T=1.1) SmileyLlama 2xSFT (T=1.1) LSTM 8 GPT 9 S4 10Validity Uniqueness Novelty KL div 0.8867 0.0830 0.1458 0.6874 0.0000 FCD 0.8527 0.5617 0.9548 0.8688 0.0026 0.8492 0.6594 0.9945 0.8699 0.0025 0.7838 0.7502 0.9954 0.8768 0.0036 0.7068 0.8471 0.9943 0.8838 0.0074 0.6197 0.9400 0.9959 0.8992 0.0208 0.7030 0.6455 0.9006 0.8132 0.0054 0.7101 0.9256 0.9925 0.9234 0.0341 0.6572 0.9750 0.9948 0.9168 0.0426 0.5701 0.9944 0.9956 0.9150 0.0716 0.4872 0.9988 0.9718 0.9003 0.1017 0.4050 1.000 0.8086 0.8851 0.1331 0.9968 0.9356 0.9113 0.8941 0.2925 0.9967 0.9279 0.8835 0.8981 0.3542 0.9783 0.9994 0.9713 0.9760 0.8369 0.9831 0.9990 0.9615 0.9849 0.8488 0.9828 0.9992 0.8476 0.9930 0.9006 0.9146 0.9995 0.9776 0.9785 0.8263 0.9712 0.9967 0.9606 0.9942 0.8526</p>
<p>Table S2 :
S2
Frequency of generating or sampling undesirable chemical structures in ChEMBL, SmileyLlama, and other SOTA methods.Here, we randomly sample 10,000 molecules from the ChEMBL dataset, SmileyLlama and our retrained models to compare their statistics of five different categories of undesirable chemical patterns.
ModelChEMBL SmileyLlama LSTM GPTS4Unsaturated Benzene0.55%0.0%0.04% 0.04% 0.03%Unsaturated Naphthalene0.02%0.02%0.02% 0.01% 0.01%Wrong Pyrrole0.29%0.03%0.02% 0.03% 0.04%Cyclopentadiene Ylidene0.96%0.05%0.04% 0.06% 0.05%Benzyne0.01%0.0%0.0%0.0%0.0%</p>
<p>Table S3 :
S3
Percentage of valid SMILES strings with duplicates removed out of 1,000 that fit the properties specified in the prompt, for each family of properties.For example, ≤ k H-bond donors refers to the average number of valid, unique SMILES strings corresponding to molecules with ≤ k H-bond donors for k = 3, 4, 5, 7, with 1,000 examples generated for each.For properties with "in range", the relevant ranges are the same as those used to construct the prompts of theSFTtraining set.2% 86.9% 95.3% 93.3% 93.4% 87.7% Presence of warhead-related SMARTS 97.0% 87.9% 96.8% 96.9% 97.2% 91.4% Absence of warhead-related SMARTS 7.4% 13.6% 23.8% 51.3% 14.0% 16.2% Exactly k H-bond donors 21.4% 19.7% 30.7% 30.4% 21.7% 21.9% Exactly k H-bond acceptors 19.9% 14.4% 27.0% 31.9%20.5% 14.9% Enamine substructures 48.5% 46.1% 45.9% 70.9% 50.3% 50.4% Lipinski rule-of-five 65.3% 55.7% 77.3% 84.9% 66.7% 58.7% Rule-of-three 42.8% 31.9%64.4% 92.8% 44.0% 34.6%
Prompt1xSFT T=0.6 T=1.1 T=0.6 T=1.1 T=0.6 T=1.1 DPO 2xSFT≤ k H-bond donors ≤ k H-bond acceptors ≤ k molecular weight ≤ k ClogP Rotatable bonds in range Fraction sp 3 in range TPSA in range Presence of macrocycle Absence of macrocycle No bad SMARTS95.7% 87.5% 97.6% 97.1% 96.0% 90.5% 93.4% 78.3% 96.9% 97.8% 94.4% 81.3% 76.8% 63.6% 92.1% 97.0% 78.5% 67.4% 79.2% 70.4% 88.6% 96.0% 79.8% 71.4% 91.1% 78.8% 95.4% 96.0% 91.4% 83.7% 79.8% 66.5% 96.0% 96.6% 79.8% 68.2% 93.0% 83.2% 97.9% 97.1% 93.3% 86.5% 56.4% 41.8% 80.6% 80.7% 64.1% 47.6% 95.2% 84.8% 97.3% 95.5% 96.9% 90.3% 92.
4 Supporting Figures</p>
<p>This is not always possible to do with substructures, since some molecules don't have BRICS decompositions.
AcknowledgmentsThis work was supported in part by the National Institute of Allergy and Infectious Disease grant U19-AI171954 for the drug molecule application.We thank the CPIMS program, Office of Science, Office of Basic Energy Sciences, Chemical Sciences Division of the U.S. Department of Energy under Contract DE-AC02-05CH11231 for support of the machine learning.We'd like to thank Riza Özçelik for kindly providing the retraining code for different CLMs for benchmarking, Yingze Wang for providing the list of undesirable SMARTS substructures, and Nicole Kennedy for suggesting properties of molecules useful to medicinal chemists.
Two decades of statistical language modeling: where do we go from here? Proceedings of the IEEE. R Rosenfeld, 200088</p>
<p>MolGAN: An implicit generative model for small molecular graphs. N D Cao, T Kipf, abs/1805.119732018</p>
<p>Generative Models for De Novo Drug Design. X Tong, X Liu, X Tan, X Li, J Jiang, Z Xiong, T Xu, H Jiang, N Qiao, M Zheng, Journal of medicinal chemistry. 2021</p>
<p>Language models can learn complex molecular distributions. D Flam-Shepherd, K Zhu, A Aspuru-Guzik, Nature Communications. 132021</p>
<p>Chemical language models enable navigation in sparsely populated chemical space. M Skinnider, R Stacey, D Wishart, L Foster, Nature Machine Intelligence. 32021</p>
<p>REINVENT 2.0: An AI Tool for De Novo Drug Design. T Blaschke, J Arús-Pous, H Chen, C Margreitter, C Tyrchan, O Engkvist, K Papadopoulos, A Patronov, Journal of chemical information. 2020</p>
<p>Chemical language models for de novo drug design: Challenges and opportunities. Current opinion in structural biology. F Grisoni, 202379102527</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, J. Chem. Inf. Comput. Sci. 281988</p>
<p>M Krenn, F Hase, A Nigam, P Friederich, Aspuru-Guzik, A. Self-referencing embedded strings (SELFIES): A 100. </p>
<p>Long Short-Term Memory. S Hochreiter, J Schmidhuber, Neural Computation. 91997</p>
<p>Generative Recurrent Networks for De Novo Drug Design. A Gupta, A T Müller, B J H Huisman, J A Fuchs, P Schneider, G Schneider, Molecular Informatics. 372017</p>
<p>Improving Language Understanding by Generative Pre-Training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>MolGPT: Molecular Generation Using a Transformer-Decoder Model. V Bagal, R Aggarwal, P K Vinod, U Priyakumar, Journal of chemical information and modeling. 622021</p>
<p>Efficiently Modeling Long Sequences with Structured State Spaces. A Gu, K Goel, C Ré, 2022</p>
<p>Chemical language modeling with structured state space sequence models. R Özçelik, S De Ruiter, E Criscuolo, F Grisoni, Nature Communications. 1561762024</p>
<p>cMolGPT: A Conditional Generative Pre-Trained Transformer for Target-Specific De Novo Molecular Generation. Y Wang, H Zhao, S Sciabola, W Wang, Molecules. 282023</p>
<p>Optimization of Molecules via Deep Reinforcement Learning. Z Zhou, S Kearnes, L Li, R N Zare, P Riley, Scientific Reports. 9107522019. 24 July 2019</p>
<p>Mining for Potent Inhibitors through Artificial Intelligence and Physics: A Unified Methodology for Ligand Based and Structure Based Drug Design. J Li, O Zhang, K Sun, Y Wang, X Guan, D Bagni, M Haghighatlari, F L Kearns, C Parks, R E Amaro, T Head-Gordon, Journal of Chemical Information and Modeling. 2024</p>
<p>Polosukhin, I. Attention Is All You Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, 2023</p>
<p>Sutskever, I. Language Models are Unsupervised Multitask Learners. A Radford, J Wu, R Child, D Luan, D Amodei, 2018</p>
<p>. Openai, GPT-4 Technical Report</p>
<p>The Llama 3 Herd of Models. A Dubey, </p>
<p>Language Models are Few-Shot Learners. T B Brown, </p>
<p>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks. S Jain, R Kirk, E S Lubana, R P Dick, H Tanaka, E Grefenstette, T Rocktäschel, D S Krueger, 2023</p>
<p>Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs. A Ahmadian, C Cremer, M Gallé, M Fadaee, J Kreutzer, O Pietquin, A Üstün, S Hooker, 2024</p>
<p>D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, Irving, Tuning Language Models from Human Preferences. </p>
<p>Deep reinforcement learning from human preferences. P Christiano, J Leike, T B Brown, M Martic, S Legg, D Amodei, 2023</p>
<p>R Rafailov, A Sharma, E Mitchell, S Ermon, C D Manning, C Finn, Direct Preference Optimization: Your Language Model is Secretly a Reward Model. </p>
<p>A Templeton, Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet. Transformer Circuits Thread. 2024</p>
<p>Activation Addition: Steering Language Models Without Optimization. A M Turner, L Thiergart, G Leech, D Udell, J J Vazquez, U Mini, M Macdiarmid, 2024</p>
<p>Mechanistically Eliciting Latent Behaviors in Language Models. A Mack, A Turner, AI Alignment Forum. 2024</p>
<p>others Towards monosemanticity: Decomposing language models with dictionary learning. T Bricken, A Templeton, J Batson, B Chen, A Jermyn, T Conerly, N Turner, C Anil, C Denison, A Askell, Transformer Circuits Thread 2023</p>
<p>Sparse Autoencoders Find Highly Interpretable Features in Language Models. H Cunningham, A Ewart, L Riggs, R Huben, L Sharkey, 2023</p>
<p>. A Gaulton, L J Bellis, A P Bento, J Chambers, M Davies, A Hersey, Y Light, S Mcglinchey, D Michalovich, B Al-Lazikani, J P Overington, Chembl, 40</p>
<p>G Landrum, Rdkit, Open-Source Cheminformatics Software. 2016</p>
<p>The 'rule of three' for fragment-based drug discovery: where are we now?. H Jhoti, G Williams, D C Rees, C W Murray, Nature Publishing Group12Publisher</p>
<p>Molecular Properties That Influence the Oral Bioavailability of Drug Candidates. D F Veber, S R Johnson, H.-Y Cheng, B R Smith, K W Ward, K D Kopple, American Chemical Society45Publisher</p>
<p>Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development. K Huang, T Fu, W Gao, Y Zhao, Y Roohani, J Leskovec, C W Coley, C Xiao, J Sun, M Zitnik, Proceedings of Neural Information Processing Systems, NeurIPS Datasets and Benchmarks. Neural Information Processing Systems, NeurIPS Datasets and Benchmarks2021</p>
<p>Artificial intelligence foundation for therapeutic science. K Huang, T Fu, W Gao, Y Zhao, Y Roohani, J Leskovec, C W Coley, C Xiao, J Sun, M Zitnik, Nature Chemical Biology. 2022</p>
<p>. A Velez-Arce, K Huang, M Li, X Lin, W Gao, T Fu, M Kellis, B L Pentelute, M Zitnik, Tdc-2, bioRxiv. 2024Multimodal Foundation for Therapeutic Science</p>
<p>Sample efficiency matters: a benchmark for practical molecular optimization. W Gao, T Fu, J Sun, C Coley, Advances in Neural Information Processing Systems. 202235</p>
<p>. W Lian, Axolotl, </p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora, Low-Rank Adaptation of Large Language Models. </p>
<p>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. T Dao, D Y Fu, S Ermon, A Rudra, C Ré, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. T Dao, International Conference on Learning Representations (ICLR). 2024</p>
<p>A Method for Stochastic Optimization. D P Kingma, J Ba, Adam, </p>
<p>Accelerate: Training and inference at scale made simple. S Gugger, L Debut, T Wolf, P Schmid, Z Mueller, S Mangrulkar, M Sun, B Bossan, 2022</p>
<p>S Rajbhandari, J Rasley, O Ruwase, Y He, Zero, arXiv:1910.02054memory optimizations toward training trillion parameter models. 2019arXiv preprint</p>
<p>DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. J Rasley, S Rajbhandari, O Ruwase, Y He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. M Zhang, Y He, Advances in Neural Information Processing Systems. 2020</p>
<p>J Ren, S Rajbhandari, R Y Aminabadi, O Ruwase, S Yang, M Zhang, D Li, Y He, Zero-Offload, arXiv:2101.06840Democratizing Billion-Scale Model Training. 2021arXiv preprint</p>
<p>H Tang, S Gan, A A Awan, S Rajbhandari, C Li, X Lian, J Liu, C Zhang, He, Y. 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. International Conference on Machine Learning. 2021</p>
<p>S Rajbhandari, O Ruwase, J Rasley, S Smith, Y He, Zero-Infinity, arXiv:2104.07857Breaking the GPU Memory Wall for Extreme Scale Deep Learning. 2021arXiv preprint</p>
<p>1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. C Li, A A Awan, H Tang, S Rajbhandari, Y He, IEEE 28th International Conference on High Performance Computing, Data, and Analytics. 2022</p>
<p>C Li, M Zhang, Y He, The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models. 2022Advances in Neural Information Processing Systems</p>
<p>Y Lu, C Li, M Zhang, C De Sa, Y He, arXiv:2202.06009Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. 2022arXiv preprint</p>
<p>S Rajbhandari, C Li, Z Yao, M Zhang, R Y Aminabadi, A A Awan, J Rasley, Y He, Deepspeed-Moe, Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale. International Conference on Machine Learning. 2022</p>
<p>S Smith, M Patwary, B Norick, P Legresley, S Rajbhandari, J Casper, Z Liu, S Prabhumoye, G Zerveas, V Korthikanti, arXiv:2201.11990others Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. 2022arXiv preprint</p>
<p>Extreme Compression for Pre-trained Transformers Made Simple and Efficient. X Wu, Z Yao, M Zhang, C Li, Y He, Advances in Neural Information Processing Systems. 2022</p>
<p>ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. Z Yao, R Y Aminabadi, M Zhang, X Wu, C Li, Y He, Advances in Neural Information Processing Systems. 2022</p>
<p>DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. R Y Aminabadi, S Rajbhandari, M Zhang, A A Awan, C Li, D Li, E Zheng, J Rasley, S Smith, O Ruwase, Y He, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and Analysis2022</p>
<p>Z Yao, X Wu, C Li, C Holmes, M Zhang, C Li, Y He, arXiv:2211.11586Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. 2022arXiv preprint</p>
<p>C Li, Z Yao, X Wu, M Zhang, Y He, arXiv:2212.03597DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. 2022arXiv preprint</p>
<p>X Wu, C Li, R Y Aminabadi, Z Yao, Y He, Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. International Conference on Machine Learning. 2023</p>
<p>S Zawad, C Li, Z Yao, E Zheng, Y He, F Yan, Dysr, Adaptive Super-Resolution via Algorithm and System Co-design. International Conference on Learning Representations. 2023</p>
<p>Scaling Vision-Language Models with Sparse Mixture of Experts. S Shen, Z Yao, C Li, T Darrell, K Keutzer, Y He, 2023EMNLP 2023</p>
<p>MCR-DL: Mix-and-Match Communication Runtime for Deep Learning. Q Anthony, A A Awan, J Rasley, Y He, A Shafi, M Abduljabbar, H Subramoni, D Panda, IEEE International Parallel and Distributed Processing Symposium. 2023</p>
<p>A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training. S Singh, O Ruwase, A A Awan, S Rajbhandari, Y He, A Bhatele, Proceedings of the 37th International Conference on Supercomputing. the 37th International Conference on Supercomputing2023</p>
<p>G Wang, H Qin, S A Jacobs, X Wu, C Holmes, Z Yao, S Rajbhandari, O Ruwase, F Yan, L Yang, Y He, Zero++, arXiv:2306.10209Extremely Efficient Collective Communication for Giant Model Training. 2023arXiv preprint</p>
<p>P A Golnari, Z Yao, Y Selective He, Guidance, arXiv:2305.09847Are All the Denoising Steps of Guided Diffusion Important?. 2023arXiv preprint</p>
<p>X Wu, Z Yao, Y He, Zeroquant-Fp, arXiv:2307.09782Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. 2023arXiv preprint</p>
<p>Z Yao, R Y Aminabadi, O Ruwase, S Rajbhandari, X Wu, A A Awan, J Rasley, M Zhang, C Li, C Holmes, arXiv:2308.01320others DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. 2023arXiv preprint</p>
<p>S L Song, B Kruft, M Zhang, C Li, S Chen, C Zhang, M Tanaka, X Wu, J Rasley, A A Awan, arXiv:2310.04610others DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies. 2023arXiv preprint</p>
<p>Z Yao, X Wu, C Li, S Youn, Y He, arXiv:2303.08302ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation. 2023arXiv preprint</p>
<p>H Xia, Z Zheng, X Wu, S Chen, Z Yao, S Youn, A Bakhtiari, M Wyatt, D Zhuang, Z Zhou, O Ruwase, Y He, S L Song, Fp6-Llm, arXiv:2401.14112Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. 2024arXiv preprint</p>
<p>S A Jacobs, M Tanaka, C Zhang, M Zhang, R Y Aminadabi, S L Song, S Rajbhandari, Y He, System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. 2024arXiv preprint</p>
<p>X Lian, S A Jacobs, L Kurilenko, M Tanaka, S Bekman, O Ruwase, M Zhang, arXiv:2406.18820Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training. 2024arXiv preprint</p>
<p>Matplotlib: A 2D graphics environment. J D Hunter, Computing in Science &amp; Engineering. 92007</p>
<p>Benchmarking Models for de Novo Molecular Design. N Brown, M Fiscato, M H Segler, A C Vaucher, Guacamol, Journal of Chemical Information and Modeling. 592019</p>
<p>Fréchet ChemNet distance: a metric for generative models for molecules in drug discovery. K Preuer, P Renz, T Unterthiner, S Hochreiter, G Klambauer, J. Chem. Inform. Model. 582018</p>
<p>. Enamine Essential Fragment Library. </p>
<p>Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings1PII of original article: S0169-409X(96)00423-1. The article was originally published in Advanced. C A Lipinski, F Lombardo, B W Dominy, P J Feeney, Special issue dedicated to Dr. Eric Tomlinson. 231997. 2001. 1991-1998Advanced Drug Delivery Reviews. Advanced Drug Delivery Reviews, A Selection of the Most Highly Cited Articles</p>
<p>Preference Optimization for Molecular Language Models. R Park, R Theisen, N Sahni, M Patek, A Cichońska, R Rahman, </p>
<p>Multi-Objective Molecule Generation using Interpretable Substructures. W Jin, R Barzilay, T Jaakkola, 2020</p>
<p>Translation between Molecules and Natural Language. C Edwards, T Lai, K Ros, G Honke, K Cho, H Ji, arXiv:2204.118172022</p>
<p>Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. Q Pei, W Zhang, J Zhu, K Wu, K Gao, L Wu, Y Xia, R Yan, Biot5, ArXiv 2023, abs/2310.07276</p>
<p>Efficient Evolutionary Search Over Chemical Space with Large Language Models. H Wang, M Skreta, C.-T Ser, W Gao, L Kong, F Strieth-Kalthoff, C Duan, Y Zhuang, Y Yu, Y Zhu, Y Du, A Aspuru-Guzik, K Neklyudov, C Zhang, arXiv:2406.169762024</p>
<p>Improving Targeted Molecule Generation through Language Model Fine-Tuning Via Reinforcement Learning. S J Ahmed, M A Elattar, arXiv:2405.068362024cs, q-bio</p>
<p>Small Molecule Optimization with Large Language Models. P Guevorguian, M Bedrosian, T Fahradyan, G Chilingaryan, H Khachatrian, A Aghajanyan, 1</p>
<p>Large Language Models as Molecular Design Engines. D Bhattacharya, H Cassady, M Hickner, W Reinhart, 2024</p>
<p>Open Large Language Model for Few-shot Molecule Generation. X Liu, Y Guo, H Li, J Liu, S Huang, B Ke, J Lv, Drugllm, ArXiv. 2024</p>
<p>Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings1PII of original article: S0169-409X(96)00423-1. The article was originally published in Advanced. C A Lipinski, F Lombardo, B W Dominy, P J Feeney, Special issue dedicated to Dr. Eric Tomlinson. 231997. 2001. 1991-1998Advanced Drug Delivery Reviews. Advanced Drug Delivery Reviews, A Selection of the Most Highly Cited Articles</p>
<p>Topological Polar Surface Area: A Useful Descriptor in 2D-QSAR. S Prasanna, R J Doerksen, 16</p>
<p>Fsp3: A new parameter for drug-likeness. Drug Discovery Today. W Wei, S Cherukupalli, L Jing, X Liu, P Zhan, 202025</p>
<p>Chemistry: Chemical con artists foil drug discovery. J Baell, M A Walters, Nature Publishing Group513Publisher</p>
<p>Mining for Potent Inhibitors through Artificial Intelligence and Physics: A Unified Methodology for Ligand Based and Structure Based Drug Design. J Li, O Zhang, K Sun, Y Wang, X Guan, D Bagni, M Haghighatlari, F L Kearns, C Parks, R E Amaro, T Head-Gordon, Journal of Chemical Information and Modeling. 2024</p>
<p>Generative Recurrent Networks for De Novo Drug Design. A Gupta, A T Müller, B J H Huisman, J A Fuchs, P Schneider, G Schneider, Molecular Informatics. 372017</p>
<p>MolGPT: Molecular Generation Using a Transformer-Decoder Model. V Bagal, R Aggarwal, P K Vinod, U Priyakumar, Journal of chemical information and modeling. 622021</p>
<p>Chemical language modeling with structured state space sequence models. R Özçelik, S De Ruiter, E Criscuolo, F Grisoni, Nature Communications. 1561762024</p>
<p>Quantifying the chemical beauty of drugs. G R Bickerton, G V Paolini, J Besnard, S Muresan, A L Hopkins, Nature Chem. 42012</p>
<p>Prediction of physicochemical parameters by atomic contributions. S A Wildman, G M Crippen, J. Chem. Inform. Comp. Sci. 391999</p>
<p>Topological polar surface area: a useful descriptor in 2D-QSAR. S , P Rj, D , Curr Med Chem. 162009</p>
<p>Lessons learnt from assembling screening libraries for drug discovery for neglected diseases. R Brenk, A Schipani, D James, A Krasowski, I H Gilbert, J Frearson, P G Wyatt, ChemMedChem. 34352008</p>            </div>
        </div>

    </div>
</body>
</html>
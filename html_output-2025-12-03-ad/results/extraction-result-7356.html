<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7356 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7356</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7356</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-276782280</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.03180v1.pdf" target="_blank">Enhancing Cybersecurity in Critical Infrastructure with LLM-Assisted Explainable IoT Systems</a></p>
                <p><strong>Paper Abstract:</strong> Ensuring the security of critical infrastructure has become increasingly vital with the proliferation of Internet of Things (IoT) systems. However, the heterogeneous nature of IoT data and the lack of human-comprehensible insights from anomaly detection models remain significant challenges. This paper presents a hybrid framework that combines numerical anomaly detection using Autoencoders with Large Language Models (LLMs) for enhanced preprocessing and interpretability. Two preprocessing approaches are implemented: a traditional method utilizing Principal Component Analysis (PCA) to reduce dimensionality and an LLM-assisted method where GPT-4 dynamically recommends feature selection, transformation, and encoding strategies.Experimental results on the KDDCup99 10% corrected dataset demonstrate that the LLM-assisted preprocessing pipeline significantly improves anomaly detection performance. The macro-average F1 score increased from 0.49 in the traditional PCAbased approach to 0.98 with LLM-driven insights. Additionally, the LLM generates natural language explanations for detected anomalies, providing contextual insights into their causes and implications. This framework highlights the synergy between numerical AI models and LLMs, delivering an accurate, interpretable, and efficient solution for IoT cybersecurity in critical infrastructure.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7356.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7356.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (LLM-assisted preprocessing and explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is used in this work to (1) analyze tabular IoT/network features and recommend automated preprocessing (feature selection, transformations, encodings) and (2) generate natural-language explanations for anomalies flagged by an autoencoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large proprietary decoder‑only transformer LLM used via prompting to generate preprocessing scripts and natural-language explanations (instruction-style prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>LLM-assisted preprocessing combined with an Autoencoder reconstruction-based anomaly detector (reconstruction error thresholding); LLM used for preprocessing recommendations and for post-hoc natural-language explanations of autoencoder-flagged anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Two prompting modes: (A) Preprocessing recommendation: provide dataset feature statistics (variance, correlations, sparsity) and request recommended feature selection, transformations (e.g., binarization, averaging correlated features), and encoding strategies; (B) Explanation of anomaly: provide 'Anomaly <id> Reconstruction Error: <value>' followed by feature values (e.g., src_bytes=..., dst_bytes=..., protocol_type_tcp=..., flag_REJ=...) and request a human-readable diagnosis, likely cause, and suggested follow-up steps. (Example in paper: 'Anomaly 1 Reconstruction Error: 0.0001 Features: src_bytes=1.4883707192251517e-06, dst_bytes=0.0, protocol_type_tcp=0.0, flag_REJ=0.0' -> GPT-4 generates prediction/insight and steps.)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tabular network traffic features (numeric and categorical), including numeric vectors and categorical fields (protocol type, flag); essentially network flow / IoT telemetry.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>KDD-Cup99 10% corrected dataset</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Reconstruction error distributions, training/validation loss curves, accuracy, false positive rate (FPR), macro-average precision, macro-average recall, macro-average F1-score (reported as key metric).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LLM-assisted preprocessing + Autoencoder: macro-average F1 = 0.98. Baseline PCA-based preprocessing + Autoencoder: macro-average F1 = 0.49. Paper also reports higher accuracy, lower false positive rate, tighter reconstruction-error distribution for normals, clearer outliers for anomalies, and faster convergence (fewer epochs) for the LLM-assisted pipeline (no numeric values for accuracy or FPR given).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baseline: Traditional PCA-based preprocessing (Min-Max scaling + one-hot encoding, PCA reduced to 25 components preserving ~95% variance) fed to an Autoencoder; baseline macro-average F1 = 0.49. The paper compares reconstruction error distributions, loss curves, accuracy, and FPR between the PCA baseline and LLM-assisted pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot prompting / instruction prompting (no fine-tuning of GPT-4 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper does not report detailed failure modes of GPT-4; reported limitations include that experiments were performed only on the KDD-Cup99 dataset (limited validation across other critical-infrastructure datasets), no real-time log integration was implemented (not validated in streaming/real-time settings), and no computational/resource cost or thorough error analysis (per-class failure cases) for the LLM's recommendations or explanations was provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Cybersecurity in Critical Infrastructure with LLM-Assisted Explainable IoT Systems', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models for forecasting and anomaly detection: A systematic literature review <em>(Rating: 2)</em></li>
                <li>Anomaly detection on unstable logs with gpt models <em>(Rating: 2)</em></li>
                <li>Experience report: Log mining using natural language processing and application to anomaly detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7356",
    "paper_id": "paper-276782280",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (LLM-assisted preprocessing and explanation)",
            "brief_description": "GPT-4 is used in this work to (1) analyze tabular IoT/network features and recommend automated preprocessing (feature selection, transformations, encodings) and (2) generate natural-language explanations for anomalies flagged by an autoencoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large proprietary decoder‑only transformer LLM used via prompting to generate preprocessing scripts and natural-language explanations (instruction-style prompting).",
            "model_size": null,
            "anomaly_detection_approach": "LLM-assisted preprocessing combined with an Autoencoder reconstruction-based anomaly detector (reconstruction error thresholding); LLM used for preprocessing recommendations and for post-hoc natural-language explanations of autoencoder-flagged anomalies.",
            "prompt_template": "Two prompting modes: (A) Preprocessing recommendation: provide dataset feature statistics (variance, correlations, sparsity) and request recommended feature selection, transformations (e.g., binarization, averaging correlated features), and encoding strategies; (B) Explanation of anomaly: provide 'Anomaly &lt;id&gt; Reconstruction Error: &lt;value&gt;' followed by feature values (e.g., src_bytes=..., dst_bytes=..., protocol_type_tcp=..., flag_REJ=...) and request a human-readable diagnosis, likely cause, and suggested follow-up steps. (Example in paper: 'Anomaly 1 Reconstruction Error: 0.0001 Features: src_bytes=1.4883707192251517e-06, dst_bytes=0.0, protocol_type_tcp=0.0, flag_REJ=0.0' -&gt; GPT-4 generates prediction/insight and steps.)",
            "training_data": null,
            "data_type": "Tabular network traffic features (numeric and categorical), including numeric vectors and categorical fields (protocol type, flag); essentially network flow / IoT telemetry.",
            "dataset_name": "KDD-Cup99 10% corrected dataset",
            "evaluation_metric": "Reconstruction error distributions, training/validation loss curves, accuracy, false positive rate (FPR), macro-average precision, macro-average recall, macro-average F1-score (reported as key metric).",
            "performance": "LLM-assisted preprocessing + Autoencoder: macro-average F1 = 0.98. Baseline PCA-based preprocessing + Autoencoder: macro-average F1 = 0.49. Paper also reports higher accuracy, lower false positive rate, tighter reconstruction-error distribution for normals, clearer outliers for anomalies, and faster convergence (fewer epochs) for the LLM-assisted pipeline (no numeric values for accuracy or FPR given).",
            "baseline_comparison": "Baseline: Traditional PCA-based preprocessing (Min-Max scaling + one-hot encoding, PCA reduced to 25 components preserving ~95% variance) fed to an Autoencoder; baseline macro-average F1 = 0.49. The paper compares reconstruction error distributions, loss curves, accuracy, and FPR between the PCA baseline and LLM-assisted pipeline.",
            "zero_shot_or_few_shot": "Zero-shot prompting / instruction prompting (no fine-tuning of GPT-4 reported).",
            "limitations_or_failure_cases": "The paper does not report detailed failure modes of GPT-4; reported limitations include that experiments were performed only on the KDD-Cup99 dataset (limited validation across other critical-infrastructure datasets), no real-time log integration was implemented (not validated in streaming/real-time settings), and no computational/resource cost or thorough error analysis (per-class failure cases) for the LLM's recommendations or explanations was provided.",
            "computational_cost": null,
            "uuid": "e7356.0",
            "source_info": {
                "paper_title": "Enhancing Cybersecurity in Critical Infrastructure with LLM-Assisted Explainable IoT Systems",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models for forecasting and anomaly detection: A systematic literature review",
            "rating": 2,
            "sanitized_title": "large_language_models_for_forecasting_and_anomaly_detection_a_systematic_literature_review"
        },
        {
            "paper_title": "Anomaly detection on unstable logs with gpt models",
            "rating": 2,
            "sanitized_title": "anomaly_detection_on_unstable_logs_with_gpt_models"
        },
        {
            "paper_title": "Experience report: Log mining using natural language processing and application to anomaly detection",
            "rating": 1,
            "sanitized_title": "experience_report_log_mining_using_natural_language_processing_and_application_to_anomaly_detection"
        }
    ],
    "cost": 0.007186499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Cybersecurity in Critical Infrastructure with LLM-Assisted Explainable IoT Systems
5 Mar 2025</p>
<p>Ashutosh Ghimire 
Department of Computer Science and Engineering
Department of Electrical and Computer Engineering
Wright State University
Western New England University</p>
<p>Ghazal Ghajari 
Department of Computer Science and Engineering
Department of Electrical and Computer Engineering
Wright State University
Western New England University</p>
<p>Karma Gurung 
Department of Computer Science and Engineering
Department of Electrical and Computer Engineering
Wright State University
Western New England University</p>
<p>Love K Sah love.sah@wne.edu 
Department of Computer Science and Engineering
Department of Electrical and Computer Engineering
Wright State University
Western New England University</p>
<p>Fathi Amsaad fathi.amsaad@wright.edu 
Department of Computer Science and Engineering
Department of Electrical and Computer Engineering
Wright State University
Western New England University</p>
<p>Enhancing Cybersecurity in Critical Infrastructure with LLM-Assisted Explainable IoT Systems
5 Mar 20250E31A0C675DA314FDCF07AD333A2E65CarXiv:2503.03180v1[cs.CR]† {ashutosh.ghimireghajari.2gurung.14IOTLLMAutoencoderExplainable AI
Ensuring the security of critical infrastructure has become increasingly vital with the proliferation of Internet of Things (IoT) systems.However, the heterogeneous nature of IoT data and the lack of human-comprehensible insights from anomaly detection models remain significant challenges.This paper presents a hybrid framework that combines numerical anomaly detection using Autoencoders with Large Language Models (LLMs) for enhanced preprocessing and interpretability.Two preprocessing approaches are implemented: a traditional method utilizing Principal Component Analysis (PCA) to reduce dimensionality and an LLM-assisted method where GPT-4 dynamically recommends feature selection, transformation, and encoding strategies.Experimental results on the KDDCup99 10% corrected dataset demonstrate that the LLM-assisted preprocessing pipeline significantly improves anomaly detection performance.The macroaverage F1 score increased from 0.49 in the traditional PCAbased approach to 0.98 with LLM-driven insights.Additionally, the LLM generates natural language explanations for detected anomalies, providing contextual insights into their causes and implications.This framework highlights the synergy between numerical AI models and LLMs, delivering an accurate, interpretable, and efficient solution for IoT cybersecurity in critical infrastructure.</p>
<p>I. INTRODUCTION</p>
<p>Critical infrastructure, including power grids, healthcare systems, and transportation networks, relies heavily on Internet of Things (IoT) devices for automation, monitoring, and operational efficiency.However, this integration introduces substantial cybersecurity risks, as IoT systems generate heterogeneous, high-volume data and operate on resourceconstrained devices [1].Traditional security mechanisms often fail to manage these complexities, leaving critical systems vulnerable to threats such as Denial-of-Service (DoS) attacks, network intrusions, and data breaches [2].</p>
<p>Anomaly detection methods for IoT systems predominantly utilize machine learning techniques, including clustering, Support Vector Machines (SVMs), and deep learning models like Autoencoders.While effective at identifying anomalies, these approaches face three significant challenges: (1) operating as opaque "black box" models, which lack interpretability and fail to provide explanations for their decisions, (2) neglecting robust preprocessing despite the noisy and redundant nature of IoT data, and (3) underutilizing valuable unstructured textual data, such as logs or event descriptions, which hold critical contextual insights [3].</p>
<p>To address these challenges, a hybrid framework is introduced that integrates Large Language Models (LLMs), such as GPT-4, with Autoencoder-based anomaly detection for IoT cybersecurity.The framework enhances anomaly detection performance through optimized, LLM-assisted preprocessing and improves interpretability by generating human-readable explanations for detected anomalies [4].Specifically, two preprocessing pipelines are employed: a traditional PCAbased approach for dimensionality reduction and an LLMdriven approach, where GPT-4 recommends feature selection, transformation, and encoding strategies.</p>
<p>The contributions of this work are twofold.First, the proposed LLM-assisted preprocessing pipeline intelligently handles noisy, redundant, and categorical features in IoT data, leading to a significant improvement in anomaly detection performance, as demonstrated by an increase in the macroaverage F1 score from 0.49 (PCA-based preprocessing) to 0.98 (LLM-assisted preprocessing).Second, GPT-4 enhances the explainability of the framework by producing natural language descriptions of anomalies, enabling cybersecurity analysts to understand and trust the model's outputs.By addressing these limitations, the proposed framework demonstrates the synergy between numerical AI models and LLMs, achieving robust, interpretable, and efficient anomaly detection.This work not only improves detection accuracy but also fosters trust in AIdriven IoT cybersecurity systems through intelligent preprocessing and explainable results.</p>
<p>II. RELATED WORKS</p>
<p>The integration of artificial intelligence (AI) and machine learning (ML) techniques into IoT cybersecurity has been a topic of extensive research.Given the increasing complexity of IoT networks and their critical role in infrastructure systems, various approaches have been explored to detect, mitigate, and explain cyber threats [5].This section reviews existing methods, highlights their limitations, and identifies the gap that this research aims to address.</p>
<p>A. AI and ML-Based Anomaly Detection in IoT Cybersecurity</p>
<p>In recent years, traditional and deep learning techniques have been widely applied to anomaly detection in IoT systems.Supervised learning methods such as Support Vector Machines (SVMs), Decision Trees, and Random Forests have been employed to classify IoT network traffic into normal and attack categories [6].For instance, datasets like KDDCup99 and NSL-KDD have been used to train classification models for intrusion detection.While these models are effective when labeled data is available, their reliance on labeled training samples limits their applicability to real-world IoT systems, where data labeling is time-consuming and expensive [7].</p>
<p>To overcome the limitations of supervised learning, unsupervised learning methods such as clustering (e.g., k-Means, DBSCAN) and anomaly detection techniques have been explored.These methods identify patterns or deviations in unlabeled data that signify anomalies.Among these approaches, Autoencoders have shown significant promise.Autoencoders are deep neural networks that learn to reconstruct input data and flag instances with high reconstruction error as anomalies.Researchers have applied Autoencoders to IoT traffic analysis, time-series data, and sensor readings to detect abnormal behaviors indicative of cyberattacks [8].However, despite their effectiveness in identifying anomalies, Autoencoders suffer from a lack of interpretability.They function as black-box models, providing no meaningful explanation for why a particular instance was flagged as an anomaly.</p>
<p>B. The Role of Natural Language Processing and Large Language Models in Cybersecurity</p>
<p>Natural Language Processing (NLP) has emerged as a promising tool for addressing the limitations of traditional numerical AI models in cybersecurity.While traditional anomaly detection focuses primarily on structured, numerical IoT data, real-world IoT systems generate substantial amounts of unstructured textual data, including logs, alerts, and error messages [9].NLP techniques can extract meaningful insights from this textual data, enhancing the context and interpretability of anomaly detection systems.</p>
<p>Recently, Large Language Models (LLMs), such as Ope-nAI's GPT-4, have demonstrated unprecedented capabilities in understanding and generating human-like text.In the context of cybersecurity, LLMs have been applied to various tasks, including log analysis, where they summarize system logs, detect anomalies in textual alerts, and identify patterns in error messages [10].Additionally, LLMs are used for incident reporting, generating natural language explanations of detected incidents to assist human operators, and for threat intelligence, which involves extracting knowledge from security reports and describing potential attack vectors [11].</p>
<p>Despite significant advancements, Large Language Models (LLMs) have not been effectively integrated with numerical anomaly detection models.Most existing approaches treat numerical and textual analyses as separate tasks, failing to combine their strengths to create a holistic cybersecurity solution.IoT data is inherently noisy, redundant, and heterogeneous [12].Existing numerical AI models often depend on manual or simplistic preprocessing methods, which limit their performance.While LLMs offer enhanced interpretability, they remain underutilized in preprocessing IoT data-a critical bottleneck in anomaly detection pipelines.</p>
<p>III. PROPOSED FRAMEWORK</p>
<p>This section outlines our proposed framework, which integrates numerical anomaly detection models with Large Language Models (LLMs) to address limitations in traditional IoT cybersecurity solutions.The framework consists of three main components: data preparation, anomaly detection, and natural language-based anomaly explanation.Two distinct approaches are implemented for data preprocessing: (1) PCA-based numerical preprocessing and (2) LLM-assisted preprocessing.</p>
<p>A. Dataset</p>
<p>The experiments in this study are conducted on the KDD-Cup99 10% corrected dataset, a standard benchmark for intrusion detection systems [13].The dataset comprises numerical features (e.g., src bytes, dst bytes) and categorical features (e.g., protocol type, flag).Despite its utility, the dataset poses challenges such as redundancy, noise, and feature heterogeneity, which require careful preprocessing for effective anomaly detection.</p>
<p>B. Data Preprocessing</p>
<p>To evaluate the effectiveness of LLMs in enhancing anomaly detection, two distinct preprocessing strategies are employed:</p>
<p>1) Traditional Preprocessing Using PCA: In the traditional preprocessing approach using Principal Component Analysis (PCA), the steps included standardizing numerical features through Min-Max scaling and one-hot encoding of categorical features such as protocol type and f lag.PCA is then applied to reduce the input feature space to 25 principal components, preserving approximately 95% of the variance.The resulting PCA-transformed data served as input to the anomaly detection model.</p>
<p>2) LLM−Assisted Preprocessing: The second approach leveraged GPT−4 to automate and optimize the preprocessing pipeline.The LLM analyzed the dataset by reviewing feature statistics, such as variance and correlation, to identify redundant and low−variance features.It recommended specific feature transformations, such as binarization for sparse columns and averaging for highly correlated features, and suggested encoding techniques for categorical variables to ensure compatibility with machine learning models.By generating automated preprocessing scripts, the LLM−assisted method reduced manual effort and produced cleaner, more meaningful input data for anomaly detection.</p>
<p>C. Anomaly Detection Using Autoencoder</p>
<p>The preprocessed datasets from both methods (PCA-based and LLM-assisted) are used to train separate Autoencoder models.An Autoencoder is a type of unsupervised neural network that learns to compress and reconstruct input data.During inference, instances with high reconstruction errors are flagged as anomalies.</p>
<p>The Autoencoder model consists of two main components:
z = σ(W e x + b e ), z ∈ R m , m &lt; n (1) x = σ(W d z + b d ), x ∈ R n(2)
where x ∈ R n represents the input data, z is the latent representation, and x is the reconstructed input.The parameters W e , W d and biases b e , b d are learned during training.The reconstruction loss is computed as:
L recon = 1 n n i=1 (x i − xi ) 2(3)
Anomalies are detected by thresholding the reconstruction error.</p>
<p>D. LLM-Driven Anomaly Explanation</p>
<p>To bridge the interpretability gap in anomaly detection, GPT-4 is integrated into the framework to generate humanreadable explanations for detected anomalies.The process involves:</p>
<p>1) Extracting anomaly-related features, such as reconstruction error, src bytes, and protocol type.2) Structuring the data into natural language prompts and feeding them to GPT-4.3) Generating textual explanations that describe the anomaly's potential cause and implications.</p>
<p>For example, an anomaly with unusually low traffic volume (src bytes) and a rejected TCP connection may be explained as a failed network scanning attempt.</p>
<p>E. Comparison and Evaluation</p>
<p>The comparison and evaluation of the two preprocessing approaches were conducted using multiple metrics to assess the overall performance of the anomaly detection framework.The evaluation included the reconstruction error distributions of the Autoencoder, as well as training and validation loss curves, to analyze the model's convergence and generalization capabilities.Additionally, standard anomaly detection performance metrics, such as accuracy, false positive rate, and macroaverage precision, recall, and F1-score for both the normal and attack classes, were used to provide a comprehensive assessment.The macro-average F1-score, in particular, highlighted the balance between precision and recall for the two classes, ensuring the model's ability to detect rare anomalies without bias toward the dominant class.</p>
<p>Furthermore, the quality of GPT-4 explanations was evaluated in terms of their ability to generate actionable insights for detected anomalies, enhancing the interpretability of the system.The comparative results clearly demonstrated that the LLM-assisted preprocessing approach not only improved anomaly detection accuracy but also provided more meaningful and interpretable outputs, making the system more reliable for real-world applications.</p>
<p>IV. RESULTS AND DISCUSSION</p>
<p>This section presents the results of our proposed framework, comparing the performance of traditional numerical anomaly detection methods (PCA-based preprocessing) with LLM-assisted preprocessing.Additionally, we demonstrate the ability of LLMs to provide interpretable explanations for detected anomalies.The results are analyzed in terms of training/validation loss, reconstruction error distribution, and performance metrics.</p>
<p>A. Autoencoder Performance Analysis</p>
<p>To evaluate the impact of LLM-assisted preprocessing, two Autoencoder models are trained:</p>
<p>1) Reconstruction Error Distribution:</p>
<p>The reconstruction error distributions for both models are compared in Fig. 3 and Fig. 2. The LLM-integrated Autoencoder produces a tighter error distribution for normal samples and more distinct outliers for anomalies.This indicates that the LLM-assisted preprocessing enhanced the model's ability to differentiate between normal and anomalous patterns.</p>
<p>2) Performance Comparison: Fig. 4 provides a performance comparison between the traditional Autoencoder and the LLM-integrated Autoencoder.The LLM-integrated model achieves a higher accuracy and lower false positive rate due to its improved preprocessing pipeline.</p>
<p>B. LLM-Driven Explanation of Anomalies</p>
<p>The integration of GPT-4 allows for human-readable explanations of detected anomalies.This section demonstrates the ability of the LLM to analyze anomalies flagged by the Autoencoder and provide actionable insights for cybersecurity analysts.</p>
<p>Figs. 5 and 6 illustrate example anomalies detected by the Autoencoder, along with their GPT-generated explanations.These explanations provide contextual insights, such as potential attack vectors, failed connection attempts, or unusual traffic patterns.</p>
<p>The ability of GPT-4 to describe anomalies in natural language enhances the interpretability of the system.For instance, Prompt Response: Anomaly 1 Reconstruction Error: 0.0001 Features:</p>
<p>• src_bytes=1.4883707192251517e-06,dst_bytes=0.0• protocol_type_tcp=0.0, flag_REJ=0.0Prediction: Attack Generated Insight: The anomaly suggests minimal source bytes (src_bytes) and zero destination bytes (dst_bytes).The TCP protocol is inactive (protocol_type_tcp=0.0), and there are no rejection flags (flag_REJ=0.0).This behavior might indicate low-volume traffic anomalies or stealth reconnaissance attempts where connections do not exchange meaningful data.</p>
<p>Steps for Further Analysis:</p>
<p>1) Network Logs: Verify the source IP and check for repeated connection attempts.2) Port Analysis: Determine if specific ports are targeted.3) Threshold Adjustment: Reassess detection thresholds for anomalies with near-zero payloads.anomalies with high reconstruction errors may be explained as unusual TCP connection refusals or potential port scanning activities.Such insights are crucial for cybersecurity analysts to take informed actions.</p>
<p>C. Discussion</p>
<p>The experimental results clearly demonstrate the advantages of the proposed framework.First, improved detection performance was achieved through LLM-assisted preprocessing, which refined the data input and led to more accurate anomaly detection.Second, the faster convergence of the LLM-integrated Autoencoder was evident, as it required fewer epochs to converge, as shown in the loss curves.Third, the framework provided enhanced interpretability by leveraging GPT-4 to generate actionable insights for anomalies, effectively bridging the gap between numerical outputs and human understanding.By combining numerical AI models with advanced language capabilities, the proposed framework addresses the limitations of existing anomaly detection systems and fosters trust in AI-driven cybersecurity for critical infrastructure.</p>
<p>V. CONCLUSION</p>
<p>This paper proposed a hybrid framework for anomaly detection in IoT systems, integrating Autoencoder-based models with GPT-4 for enhanced preprocessing and interpretability.Two approaches were explored: a traditional PCA-based method and an LLM-assisted pipeline where GPT-4 dynamically optimized feature selection and transformation.</p>
<p>Experimental results on the KDDCup99 dataset showed that the LLM-assisted preprocessing improved anomaly detection performance, leading to faster convergence, better Prompt Response: Anomaly 2 Reconstruction Error: 0.0008 Features:</p>
<p>• src_bytes=3.9661041452220616e-07,dst_bytes=6.013033152373364e-05• protocol_type_tcp=1.0, flag_REJ=0.0Prediction: Attack Generated Insight: The anomaly indicates a very small src_bytes value and slightly higher dst_bytes, while the TCP protocol is active (protocol_type_tcp=1.0).There are no rejections (flag_REJ=0.0).This could indicate port scanning or probing behavior where minimal traffic is sent to test open ports or system responses.</p>
<p>Steps for Further Analysis:</p>
<p>1) Traffic Context: Analyze surrounding traffic for sequential port scans.2) Protocol Analysis: Verify if the TCP handshake completes or drops midway.3) Source IP Tracking: Look for repeated connections from the same IP.reconstruction error separation, and enhanced accuracy.Additionally, GPT-4 provided natural language explanations for detected anomalies, improving interpretability and aiding human decision-making.In summary, the proposed framework demonstrates the potential of combining numerical AI models with LLMs to achieve accurate and explainable anomaly detection in IoT systems.Future work will focus on real-time IoT log integration and validation across additional critical infrastructure datasets.</p>
<p>Fig. 1 :
1
Fig. 1: Proposed framework for LLM-assisted anomaly detection in IoT systems.</p>
<p>Fig. 2 :
2
Fig. 2: Reconstruction error distribution of the LLM-integrated Autoencoder.</p>
<p>Fig. 3 :
3
Fig. 3: Reconstruction error distribution of the traditional Autoencoder.</p>
<p>Fig. 4 :
4
Fig. 4: Comparison of macro-average performance metrics between the traditional Autoencoder and the LLM-integrated Autoencoder.</p>
<p>Fig. 5 :
5
Fig. 5: Example 1: GPT-generated explanation of a detected anomaly.</p>
<p>Fig. 6 :
6
Fig.6: Example 2: GPT-generated explanation of a detected anomaly.</p>
<p>• Traditional Autoencoder: Using PCA-based preprocessing with 25 principal components. • LLM insights Integrated Autoencoder: Using LLMdriven preprocessing, where GPT optimized data preparation by suggesting feature transformations, removal of redundancy, and encoding strategies.</p>
<p>Internet of things meet internet of threats: New concern cyber security issues of critical cyber infrastructure. A Djenna, S Harous, D E Saidouni, Applied Sciences. 111045802021</p>
<p>A comprehensive review of cyber security vulnerabilities, threats, attacks, and solutions. Ö Aslan, S S Aktug, M Ozkan-Okay, A A Yilmaz, E Akin, Electronics. 12613332023</p>
<p>Anomaly detection: A survey. V Chandola, A Banerjee, V Kumar, ACM computing surveys (CSUR). 200941</p>
<p>Large language models for forecasting and anomaly detection: A systematic literature review. J Su, C Jiang, X Jin, Y Qiao, T Xiao, H Ma, R Wei, Z Jing, J Xu, J Lin, arXiv:2402.103502024arXiv preprint</p>
<p>Secure internet of things (iot)-based smart-world critical infrastructures: Survey, case study and research opportunities. X Liu, C Qian, W G Hatcher, H Xu, W Liao, W Yu, IEEE Access. 72019</p>
<p>Implementation of secure and privacy-aware ai hardware using distributed federated learning. A Ghimire, A N Asiri, B Hildebrand, F Amsaad, 2023 IEEE 16th Dallas Circuits and Systems Conference (DCAS). IEEE2023</p>
<p>Intrusion detection system classification using different machine learning algorithms on kdd-99 and nslkdd datasets-a review paper. R D Ravipati, M Abualkibash, International Journal of Computer Science &amp; Information Technology (IJCSIT). 112019</p>
<p>Tsmae: a novel anomaly detection approach for internet of things time series data using memory-augmented autoencoder. H Gao, B Qiu, R J D Barroso, W Hussain, Y Xu, X Wang, IEEE Transactions on network science and engineering. 1052022</p>
<p>Experience report: Log mining using natural language processing and application to anomaly detection. C Bertero, M Roy, C Sauvanaud, G Trédan, 2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE). IEEE2017</p>
<p>Anomaly detection on unstable logs with gpt models. F Hadadi, Q Xu, D Bianculli, L Briand, arXiv:2406.074672024arXiv preprint</p>
<p>Deployment corrections: An incident response framework for frontier ai models. J O'brien, S Ee, Z Williams, arXiv:2310.003282023arXiv preprint</p>
<p>Heterogeneous data and big data analytics. L Wang, Automatic Control and Information Sciences. 312017</p>
<p>A detailed analysis of the kdd cup 99 data set. M Tavallaee, E Bagheri, W Lu, A A Ghorbani, 2009 IEEE symposium on computational intelligence for security and defense applications. Ieee2009</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6218 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6218</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6218</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-8bc313e04cbd39847eb50b22af0a698ff2971a35</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8bc313e04cbd39847eb50b22af0a698ff2971a35" target="_blank">Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation, and investigate several prompting designs, and propose a new prompting method called EAPrompt by combining Chain-of-Thoughts and Error Analysis.</p>
                <p><strong>Paper Abstract:</strong> Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called \textbf{\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and \textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6218.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6218.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EAPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Error Analysis Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step prompting strategy that combines Chain-of-Thought and explicit error analysis to make LLMs emulate MQM-style human evaluation by (1) identifying itemized major and minor translation errors and (2) counting them to compute a severity-weighted score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Machine Translation evaluation (WMT22: En-De, En-Ru, Zh-En)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5-Turbo (primary), Llama2-70b-Chat, Mixtral-8x7b-Instruct; limited GPT-4 subset (600 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>MQM human judgments from WMT22: 106,758 segments across 54 systems and 4 domains; MQM annotations produced by professional translators (En-De & Zh-En by Google: 11 translators total; En-Ru by Unbabel: 4 translators). Each segment annotated by 2–3 annotators; reported pairwise inter-rater agreement ≈0.584 (En-De) and 0.412 (Zh-En).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>System-level pairwise accuracy (all-pairs ranking agreement with MQM), segment-level group-by-item pairwise accuracy with tie calibration (acc_cq*), Pearson correlation, Kendall/Tau (subset), and PERM-BOTH significance tests (1000 resamples, p=0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>EAPrompt substantially improves LLM agreement with MQM: GPT-3.5-Turbo with EAPrompt achieves 91.2% system-level pairwise accuracy (new SOTA among tested metrics), and EAPrompt outperforms GEMBA in 8 of 9 segment-level scenarios. Segment-level bests reported: En-De 56.7, En-Ru 53.4, Zh-En 50.2 (pairwise accuracy %). However, even with EAPrompt LLMs generally do not surpass the best fine-tuned model-based metrics at segment level; improvements are larger at system level than at segment level.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Observed limitations include: instability in outputs without temperature control (response variability), sensitivity to prompt-response verbosity leading to miscounted or misclassified errors, input-order bias when multiple translations are evaluated together, occasional invalid or off-task answers (e.g., returning BLEU scores), differences in minor-error frequency across models (Mixtral reports more minor errors), and limited improvement to segment-level performance compared to specialized fine-tuned metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Specific cases reported: (1) Mixtral-8x7b-Instruct on En-De yielded lower segment-level accuracy with EAPrompt versus GEMBA (−1.0), possibly due to weaker error identification for that pair; (2) repeated runs with same input sometimes produced different numeric scores (e.g., 98, 95, 100) when temperature not controlled; (3) evaluating multiple translations in one query led to order-dependent ranking changes; (4) some responses were invalid or relied on existing metric heuristics (e.g., returning BLEU), requiring special handling.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Recommended mitigations include: use 2-step separated queries (identify errors, then count) and itemized error demonstrations in prompts; set temperature = 0 (GPT-3.5-Turbo) or low nonzero (0.05) to stabilize outputs; query one translation at a time to avoid input-order bias; when counting errors, replace the second LLM counting query with regular-expression matching on the model's itemized output to cut inference cost (minimal performance impact); adopt one-shot in-context examples tailored per language pair; tune the major-error weight (w_major) and use values ≥5 for stable behavior; apply iterative randomization with increasing temperature and accept the first valid-scoring response to handle occasional invalid outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>EAPrompt also produces an error-count distribution more closely aligned with MQM (major/minor distribution), and experiments show that treating w_major as a latent variable and selecting per-LLM optimal values (GPT-3.5-Turbo: 6; Llama2-70b-Chat: 10; Mixtral-8x7b-Instruct: 10) yields best meta-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6218.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6218.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEMBA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GEMBA Prompting (GEMBADA variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior zero-shot prompting approach that asks LLMs to directly output a translation quality score; shown by prior work to achieve strong system-level performance but weaker segment-level correlation with human MQM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are state-of-the-art evaluators of translation quality</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Machine Translation evaluation (WMT22 used as baseline comparison in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Applied to same LLMs in this study: GPT-3.5-Turbo, Llama2-70b-Chat, Mixtral-8x7b-Instruct; GEMBA-MQM/GPT-4 referenced in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same MQM human judgments from WMT22 (see EAPrompt entry). GEMBA was compared directly against those MQM annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>System-level pairwise accuracy and segment-level pairwise accuracy with tie calibration, Pearson correlations, and significance tests vs. MQM.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>GEMBA provides competitive system-level performance (prior work reported SOTA for GPT variants) but exhibits weaker segment-level correlation with MQM relative to EAPrompt; this paper finds EAPrompt surpasses GEMBA at system level for the tested LLMs and outperforms GEMBA in 8/9 segment scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>GEMBA's direct-score zero-shot prompting lacks interpretable error explanations and underperforms at segment granularity; it is more prone to poor segment-level agreement with MQM compared to EAPrompt.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Prior and reproduced observations: strong system-level rankings but unrealistic/weak segment-level performance, motivating more explanation-driven prompting (EAPrompt).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Paper's mitigation: augment GEMBA-style evaluation with explicit error analysis and chain-of-thought style prompting (the EAPrompt approach), employ few-shot/one-shot examples to improve task understanding, and separate identification/counting steps to reduce miscounting and improve segment-level alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 2)</em></li>
                <li>GEMBA-MQM: Detecting translation quality error spans with GPT-4 <em>(Rating: 2)</em></li>
                <li>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation <em>(Rating: 2)</em></li>
                <li>Toward human-like evaluation for natural language generation with error analysis <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6218",
    "paper_id": "paper-8bc313e04cbd39847eb50b22af0a698ff2971a35",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "EAPrompt",
            "name_full": "Error Analysis Prompting",
            "brief_description": "A two-step prompting strategy that combines Chain-of-Thought and explicit error analysis to make LLMs emulate MQM-style human evaluation by (1) identifying itemized major and minor translation errors and (2) counting them to compute a severity-weighted score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Machine Translation evaluation (WMT22: En-De, En-Ru, Zh-En)",
            "llm_judge_model": "GPT-3.5-Turbo (primary), Llama2-70b-Chat, Mixtral-8x7b-Instruct; limited GPT-4 subset (600 samples)",
            "human_evaluation_setup": "MQM human judgments from WMT22: 106,758 segments across 54 systems and 4 domains; MQM annotations produced by professional translators (En-De & Zh-En by Google: 11 translators total; En-Ru by Unbabel: 4 translators). Each segment annotated by 2–3 annotators; reported pairwise inter-rater agreement ≈0.584 (En-De) and 0.412 (Zh-En).",
            "metrics_compared": "System-level pairwise accuracy (all-pairs ranking agreement with MQM), segment-level group-by-item pairwise accuracy with tie calibration (acc_cq*), Pearson correlation, Kendall/Tau (subset), and PERM-BOTH significance tests (1000 resamples, p=0.05).",
            "reported_differences": "EAPrompt substantially improves LLM agreement with MQM: GPT-3.5-Turbo with EAPrompt achieves 91.2% system-level pairwise accuracy (new SOTA among tested metrics), and EAPrompt outperforms GEMBA in 8 of 9 segment-level scenarios. Segment-level bests reported: En-De 56.7, En-Ru 53.4, Zh-En 50.2 (pairwise accuracy %). However, even with EAPrompt LLMs generally do not surpass the best fine-tuned model-based metrics at segment level; improvements are larger at system level than at segment level.",
            "llm_specific_limitations": "Observed limitations include: instability in outputs without temperature control (response variability), sensitivity to prompt-response verbosity leading to miscounted or misclassified errors, input-order bias when multiple translations are evaluated together, occasional invalid or off-task answers (e.g., returning BLEU scores), differences in minor-error frequency across models (Mixtral reports more minor errors), and limited improvement to segment-level performance compared to specialized fine-tuned metrics.",
            "notable_failure_cases": "Specific cases reported: (1) Mixtral-8x7b-Instruct on En-De yielded lower segment-level accuracy with EAPrompt versus GEMBA (−1.0), possibly due to weaker error identification for that pair; (2) repeated runs with same input sometimes produced different numeric scores (e.g., 98, 95, 100) when temperature not controlled; (3) evaluating multiple translations in one query led to order-dependent ranking changes; (4) some responses were invalid or relied on existing metric heuristics (e.g., returning BLEU), requiring special handling.",
            "mitigation_strategies": "Recommended mitigations include: use 2-step separated queries (identify errors, then count) and itemized error demonstrations in prompts; set temperature = 0 (GPT-3.5-Turbo) or low nonzero (0.05) to stabilize outputs; query one translation at a time to avoid input-order bias; when counting errors, replace the second LLM counting query with regular-expression matching on the model's itemized output to cut inference cost (minimal performance impact); adopt one-shot in-context examples tailored per language pair; tune the major-error weight (w_major) and use values ≥5 for stable behavior; apply iterative randomization with increasing temperature and accept the first valid-scoring response to handle occasional invalid outputs.",
            "additional_notes": "EAPrompt also produces an error-count distribution more closely aligned with MQM (major/minor distribution), and experiments show that treating w_major as a latent variable and selecting per-LLM optimal values (GPT-3.5-Turbo: 6; Llama2-70b-Chat: 10; Mixtral-8x7b-Instruct: 10) yields best meta-evaluation.",
            "uuid": "e6218.0",
            "source_info": {
                "paper_title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GEMBA (baseline)",
            "name_full": "GEMBA Prompting (GEMBADA variant)",
            "brief_description": "A prior zero-shot prompting approach that asks LLMs to directly output a translation quality score; shown by prior work to achieve strong system-level performance but weaker segment-level correlation with human MQM.",
            "citation_title": "Large language models are state-of-the-art evaluators of translation quality",
            "mention_or_use": "use",
            "task_domain": "Machine Translation evaluation (WMT22 used as baseline comparison in this paper)",
            "llm_judge_model": "Applied to same LLMs in this study: GPT-3.5-Turbo, Llama2-70b-Chat, Mixtral-8x7b-Instruct; GEMBA-MQM/GPT-4 referenced in related work.",
            "human_evaluation_setup": "Same MQM human judgments from WMT22 (see EAPrompt entry). GEMBA was compared directly against those MQM annotations.",
            "metrics_compared": "System-level pairwise accuracy and segment-level pairwise accuracy with tie calibration, Pearson correlations, and significance tests vs. MQM.",
            "reported_differences": "GEMBA provides competitive system-level performance (prior work reported SOTA for GPT variants) but exhibits weaker segment-level correlation with MQM relative to EAPrompt; this paper finds EAPrompt surpasses GEMBA at system level for the tested LLMs and outperforms GEMBA in 8/9 segment scenarios.",
            "llm_specific_limitations": "GEMBA's direct-score zero-shot prompting lacks interpretable error explanations and underperforms at segment granularity; it is more prone to poor segment-level agreement with MQM compared to EAPrompt.",
            "notable_failure_cases": "Prior and reproduced observations: strong system-level rankings but unrealistic/weak segment-level performance, motivating more explanation-driven prompting (EAPrompt).",
            "mitigation_strategies": "Paper's mitigation: augment GEMBA-style evaluation with explicit error analysis and chain-of-thought style prompting (the EAPrompt approach), employ few-shot/one-shot examples to improve task understanding, and separate identification/counting steps to reduce miscounting and improve segment-level alignment.",
            "uuid": "e6218.1",
            "source_info": {
                "paper_title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 2
        },
        {
            "paper_title": "GEMBA-MQM: Detecting translation quality error spans with GPT-4",
            "rating": 2
        },
        {
            "paper_title": "The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation",
            "rating": 2
        },
        {
            "paper_title": "Toward human-like evaluation for natural language generation with error analysis",
            "rating": 2
        },
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 1
        }
    ],
    "cost": 0.010689999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models</h1>
<p>Qingyu Lu<em>, Baopu Qiu ${ }^{\text {a }}$, Liang Ding ${ }^{\text { }}$, Kanjian Zhang ${ }^{\text {a, }}$, Tom Kocmi ${ }^{\text { }}$, Dacheng Tao ${ }^{\text { }}$<br></em>Southeast University ${ }^{\text {a }}$ Nanjing University ${ }^{\text { }}$ The University of Sydney<br>${ }^{\text {a }}$ Southeast University Shenzhen Research Institute ${ }^{\text { }}$ Microsoft ${ }^{\text {a }}$ Nanyang Technological University<br>luqingyu@seu.edu.cn, qiubaopu@smail.nju.edu.cn, liangding.liam@gmail.com, tomkocmi@microsoft.com<br>https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt</p>
<h4>Abstract</h4>
<p>Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023b) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but performs poorly at the segment level. To further improve the performance of LLMs on MT quality assessment, we conduct an investigation into several prompting designs, and propose a new prompting method called Error Analysis Prompting (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and produces explainable and reliable MT evaluations at both the system and segment level. Experimental Results from WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs), especially Generative Pre-trained Transformer (GPT) models (Radford et al., 2019; Brown et al., 2020) such as ChatGPT (Ouyang et al., 2022; Achiam et al., 2023), have shown remarkable performance in various natural language processing (NLP) tasks (Qin et al., 2023; Zhong et al., 2023) and complex reasoning and agent applications (Zhong et al., 2024; Ren</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2024). LLMs are capable of integrating multiple NLP tasks and can generate detailed and comprehensive responses to human inquiries. Additionally, they can respond appropriately to follow-up questions and maintain sensitivity throughout several turns of conversation.</p>
<p>Previous research has demonstrated that LLMs can perform as well as or even better than other LLMs in machine translation task (Hendy et al., 2023; Jiao et al., 2023; Peng et al., 2023). Given the high cost and time-intensive nature of human evaluation, there is a growing demand for MT metrics that offer both explainability and reliability. Therefore, LLMs hold promise in serving as ideal evaluators, capable of generating both judgments and explanations for the translations.</p>
<p>Concurrent to our research, GEMBA (Kocmi and Federmann, 2023b) presents an encouraging finding that GPT models can surpass current best MT metrics at the system level quality assessment using straightforward zero-shot standard prompting, confirming the reliability and potential of this technique. However, such prompts exhibit unrealistic performance at the segment level, and cannot offer additional interpretable information regarding translation errors, thus detracting from the goal of achieving a "human-like" evaluation.</p>
<p>To this end, we take the further step by carefully investigating advanced prompting strategies upon various LLMs for MT quality assessment and propose a novel prompting strategy - Error Analysis Prompting (EAPrompt), combining the Chain-ofThought (CoT, Wei et al. (2022)) and Error Analysis (EA, Lu et al. (2023)). We give an example of EAPrompt in Figure 1. The idea is to prompt LLMs to emulate the human evaluation framework - MQM (Freitag et al., 2021) by (1) identifying major\&amp;minor errors, and (2) scoring the translations according to the severity of these errors.</p>
<p>We conduct experiments using the test set from the WMT22 metrics shared task, compris-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A comparative overview between GEMBA Prompting and our proposed Error Analysis Prompting in assessing the MT quality with LLMs.</p>
<p>ing 106,758 segments on 54 MT systems across diverse domains to verify the effectiveness of our approach. Our findings reveal that:</p>
<ul>
<li>EAPrompt significantly enhances the performance of LLMs at the system level. Notably, prompting GPT-3.5-Turbo with EAPrompt outperforms all other metrics and prompting strategies, establishing a new state-of-the-art.</li>
<li>EAPrompt surpasses GEMBA in 8 out of 9 test scenarios across various language models and language pairs, demonstrating superior performance at the segment level.</li>
<li>The findings regarding EAPrompt's strong performance remain consistent even in reference-less settings, highlighting its suitability for quality estimation tasks.</li>
<li>When designing prompts, we recommend the EAPrompt variant featuring a 2-step separated prompting approach and itemized error demonstrations.</li>
<li>Further analysis confirms that EAPrompt adeptly distinguishes major errors from minor ones, closely aligning its error distribution with MQM.</li>
<li>Optimizing the inference costs of EAPrompt can be achieved by leveraging Regular Expressions instead of counting queries.</li>
</ul>
<p>This study provides an initial exploration of utilizing error analysis to prompt LLMs as evaluators. EAPrompt can also be extended to benefit other evaluation scenarios within language generation, including summarization and data-to-text tasks.</p>
<h2>2 Prompt LLMs with Error Analysis</h2>
<h3>2.1 Translation Evaluation Metric</h3>
<p>Translation evaluation metrics are used to assess the performance of machine translation systems on specific test sets (Freitag et al., 2022; Mathur et al., 2020b). These metrics typically take inputs from three sources: the sentence from source language ("Source"), the reference translation provided by human translators ("Reference"), and the hypothesis being evaluated ("Translation"). In scenarios where reference signals are not provided, this "reference-less" metric can also be utilized for quality estimation purposes (Zerva et al., 2022; Specia et al., 2010; Qiu et al., 2022). The output of the metric is a score or rank indicating the translation quality of each hypothesis.</p>
<p>To verify the reliability of MT metrics, Multidimensional Quality Metric (MQM) has been adopted recently in WMT as a high-quality human evaluation strategy (Freitag et al., 2021). It asks human experts to annotate the errors in the hypothesis and categorize them into "Major" and "Minor" indicating their severity. A detailed description of</p>
<p>MQM annotation is presented in Appendix A.</p>
<h3>2.2 Prompt LLMs as Evaluation Metrics</h3>
<p>When prompting LLMs as evaluation metrics, it is crucial to design appropriate instructions that describe the evaluation task. In this paper, we mainly adopt two prompting strategies: "GEMBA Prompting" and "Error Analysis Prompting".</p>
<p>GEMBA <em>Kocmi and Federmann (2023b)</em> is a zero-shot prompting approach that directly asks LLMs to generate a score that reflects the quality of the translation, which shows state-of-the-art performance on GPT models when compared to other model-based metrics. However, they also observe that the performance at the segment level is relatively poorer. This highlights the importance of combining Chain-of-Thought with the Error Analysis Strategy to prompt LLMs in a manner that more closely resembles human evaluation.</p>
<h3>2.3 Error Analysis Prompting</h3>
<p>Motivated by the MQM framework in human evaluation, the idea of the Error Analysis (EA) paradigm, as introduced by <em>Lu et al. (2023)</em>, is to enhance the automatic scoring process by explicitly incorporating error identification, thus providing a more human-like evaluation.</p>
<p>The Chain-of-Thought (CoT) prompting strategy was first proposed by <em>Wei et al. (2022)</em>. Instead of directly generating the answer, CoT prompts LLMs to think step-by-step. This approach has shown significant performance improvements on reasoning tasks, such as GSM8K <em>Cobbe et al. (2021)</em>. CoT is an emergent ability of LLMs and has been incorporated in instruction fine-tuning of LLMs <em>Chung et al. (2022)</em> as well as in benchmarks designed to evaluate LLM capabilities <em>Suzgun et al. (2022)</em>.</p>
<p>In this work, we combine the CoT and EA paradigms, introducing a novel prompting strategy called Error Analysis Prompting (EAPrompt). As shown in Figure 1, EAPrompt divides the scoring process into two stages: First, the LLM is instructed to identify major and minor errors in the translation ("Instruction: Identify Errors"). Subsequently, the number of these two types of errors is counted ("Instruction: Count Errors"). Distinguished from GEMBA prompting, EAPrompt emulates the evaluation process of MQM and produces more explainable and reliable automatic evaluations.</p>
<p>After exploring several prompt contexts in initial experiments, we made the following modifications to EAPrompt as follows:</p>
<ul>
<li>we adopt the one-shot learning format <em>Brown et al. (2020)</em> to enhance the LLMs’ understanding of the task (§3.5); different in-context examples are used for different language pairs;</li>
<li>we employ itemized error demonstration in the template response, enabling clearer identification and quantification of errors (§3.6);</li>
<li>we partition the evaluation process into two stages to enhance the reliability of metric performance. Additionally, we present a simplified alternative to optimize inference costs by counting errors automatically (§4.3).</li>
</ul>
<h3>2.4 Post-processing of LLM responses</h3>
<p>After obtaining the number of major and minor errors, we compute the final score of the translation using the following equation:</p>
<p>$\text{score}=-w_{\text{major}}n_{\text{major}}-w_{\text{minor}}n_{\text{minor}},$ (1)</p>
<p>where $n_{\text{major}}$ and $n_{\text{minor}}$ denotes the number of major and minor errors respectively, while $w_{\text{major}}$ and $w_{\text{minor}}$ represent the severity weight assigned to major and minor errors. Since different LLMs may apply distinct criteria for major and minor errors, we follow <em>Lu et al. (2023)</em> to adopt a flexible scoring approach by fixing the $w_{\text{minor}}=1$ while treating $w_{\text{major}}$ as a latent variable within EAPrompt. We present an analysis on the influence of this variable in §4.2 and the detailed implementation in experiments is described in Appendix B.</p>
<h2>3 Experimental Results</h2>
<h3>3.1 Experiment Setup</h3>
<p>Dataset We utilize the test set from the WMT22 shared tasks <em>Freitag et al. (2022)</em> in English-German (En-De), English-Russian (En-Ru), and Chinese-English (Zh-En) across 4 different domains - conversational, e-commerce, news, and social. This study necessitates the evaluation of 106,758 segments from 54 MT systems. In accordance with GEMBA <em>Kocmi and Federmann (2023a)</em>, we deliberately exclude previous datasets such as WMT21 and WMT20. This reduces the potential risk of data contamination, as LLMs may have utilized these datasets in their training sets. Table 1 provides statistics about our test set. For future studies, we plan to incorporate more recent testsets, such as WMT23 <em>Freitag et al. (2023)</em>.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Language Pair</th>
<th>Segments</th>
<th>Systems</th>
<th>Domains</th>
</tr>
</thead>
<tbody>
<tr>
<td>WMT22</td>
<td>En-De</td>
<td>2037</td>
<td>17</td>
<td>conversational, e-commerce, news, social</td>
</tr>
<tr>
<td></td>
<td>En-Ru</td>
<td>2037</td>
<td>17</td>
<td>conversational, e-commerce, news, social</td>
</tr>
<tr>
<td></td>
<td>Zh-En</td>
<td>1875</td>
<td>20</td>
<td>conversational, e-commerce, news, social</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of testset. Source, reference texts, and translations are from the WMT22 metrics shared task.</p>
<p>Human Evaluation We utilize MQM (Freitag et al., 2021) as human judgments, which is annotated by human experts and has been widely adopted in recent WMT metrics shared tasks (Freitag et al., 2022) and quality estimation tasks (Zerva et al., 2022).</p>
<p>Meta Evaluation We follow the standard meta-evaluation approach to measure the performance of MT evaluation metrics (Freitag et al., 2023). At the system level, we use pairwise accuracy across all three language pairs, which calculates the proportion of all possible pairs of MT systems that are ranked the same by the metric and human scores (Kocmi et al., 2021). At the segment level, we adopt the group-by-item pairwise accuracy with tie calibration as described by Deutsch et al. (2023). We use the acc${}_{c q}^{*}$ variant to compare vectors of metric and gold scores for each segment, then average the results over segments. All the meta-evaluation are calculated with MTME, a metric evaluation tool recommended by WMT (Freitag et al., 2022) to maintain comparability with other metrics.</p>
<p>Significance Analysis At the segment level, we follow WMT22 metrics shared task to utilize PERM-BOTH hypothesis test (Deutsch et al., 2021) to assess the significance of metrics. We use 1000 re-sampling runs and set $p=0.05$.</p>
<h3>3.2 Baselines and Large Language Models</h3>
<p>Baseline Metrics Given the reported unreliability of BLEU (Papineni et al., 2002), we compare our method with several model-based metrics for MT evaluation. BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020) are supervised neural metrics fine-tuned on human evaluation. We employ the BLEURT20 and COMET-22 for reference-based metrics, and COMET-QE for the reference-less metric. UniTE (Wan et al., 2022) is also a learnt metric that evaluates MT outputs combining three different evaluation scenarios.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>We also adopt UniTE-src for comparing referenceless metrics. MetricX-XXL (Juraska et al., 2023) is a large-scale multi-task metric that fine-tunes LLM checkpoints using diverse human feedback data. For reference-less metrics, we also reproduce MaTESe-QE (Perrella et al., 2022), a metric leveraging transformer-based multilingual encoders to identify error spans in translations.</p>
<p>Large Language Models For prioprietary models, we use the OpenAI API to experiment with GPT-3.5-Turbo. We also experiment with an in-house human-aligned Llama2-70B series model (Touvron et al., 2023b) fine-tuned with multilingual translation data, noted as "Llama2-70b-Chat" in experimental results. We also use a high-quality sparse mixture-of-experts model, Mixtral-8x7b (Jiang et al., 2024). We use a state-of-the-art checkpoint Mixtral-8x7b-Instruct which has been optimised through supervised fine-tuning and direct preference optimisation to follow instructions.</p>
<h3>3.3 Prompts for LLM evaluators</h3>
<p>For GEMBA Prompting, we adopt the GEMBADA variant as suggested by (Kocmi and Federmann, 2023b), given its widespread usage and superior performance across three language pairs (Kocmi and Federmann, 2023a).</p>
<p>For Error Analysis Prompting (EAPrompt), we conduct a comparison of various prompting strategies of EAPrompt in §3.6, and use the best-performing variant for other experiments. We show the detailed prompt contexts in Appendix C.</p>
<h3>3.4 Inference Costs for EAPrompt</h3>
<p>It is importatant to note that using a more complex prompting strategy may lead to higher inference costs. In EAPrompt, there are two steps that may require querying LLM for each translation:</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Metrics / Prompts</th>
<th>Ref?</th>
<th>System-Level Acc.</th>
<th>Segment-Level Acc*</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>All (3 LPs)</td>
<td>En-De</td>
<td>En-Ru</td>
<td>Zh-En</td>
</tr>
<tr>
<td>Baselines</td>
<td>MetricsX-XXL</td>
<td>$\checkmark$</td>
<td>85.0</td>
<td>60.4</td>
<td>60.6</td>
<td>54.4</td>
</tr>
<tr>
<td></td>
<td>BLEURT20</td>
<td>$\checkmark$</td>
<td>84.7</td>
<td>56.8</td>
<td>54.0</td>
<td>48.9</td>
</tr>
<tr>
<td></td>
<td>COMET22</td>
<td>$\checkmark$</td>
<td>83.9</td>
<td>59.4</td>
<td>57.7</td>
<td>53.6</td>
</tr>
<tr>
<td></td>
<td>UniTE</td>
<td>$\checkmark$</td>
<td>82.8</td>
<td>59.8</td>
<td>57.7</td>
<td>51.7</td>
</tr>
<tr>
<td></td>
<td>COMET-QE</td>
<td>$\chi$</td>
<td>78.1</td>
<td>55.5</td>
<td>53.4</td>
<td>48.3</td>
</tr>
<tr>
<td></td>
<td>UniTE-src</td>
<td>$\chi$</td>
<td>75.9</td>
<td>58.2</td>
<td>55.4</td>
<td>50.8</td>
</tr>
<tr>
<td></td>
<td>MaTESe-QE</td>
<td>$\chi$</td>
<td>74.8</td>
<td>57.2</td>
<td>49.9</td>
<td>49.4</td>
</tr>
<tr>
<td>Llama2-70b-Chat</td>
<td>GEMBA</td>
<td>$\checkmark$</td>
<td>74.1</td>
<td>53.7</td>
<td>48.8</td>
<td>45.4</td>
</tr>
<tr>
<td></td>
<td>EAPrompt</td>
<td>$\checkmark$</td>
<td>$85.4(+11.3)$</td>
<td>$55.2(+1.5)$</td>
<td>$51.4 \dagger(+2.6)$</td>
<td>$\mathbf{5 0 . 2} \dagger(+4.8)$</td>
</tr>
<tr>
<td></td>
<td>GEMBA</td>
<td>$\chi$</td>
<td>72.6</td>
<td>54.1</td>
<td>47.8</td>
<td>45.0</td>
</tr>
<tr>
<td></td>
<td>EAPrompt</td>
<td>$\chi$</td>
<td>$85.8(+13.2)$</td>
<td>$55.0 \dagger(+0.9)$</td>
<td>$51.6 \dagger(+3.8)$</td>
<td>$49.3 \dagger(+4.3)$</td>
</tr>
<tr>
<td>Mixtral-8x7b-Instruct</td>
<td>GEMBA</td>
<td>$\checkmark$</td>
<td>69.7</td>
<td>54.8</td>
<td>48.3</td>
<td>46.7</td>
</tr>
<tr>
<td></td>
<td>EAPrompt</td>
<td>$\checkmark$</td>
<td>$84.0(+14.3)$</td>
<td>$53.8(-1.0)$</td>
<td>$50.6 \dagger(+2.3)$</td>
<td>$48.2 \dagger(+1.5)$</td>
</tr>
<tr>
<td></td>
<td>GEMBA</td>
<td>$\chi$</td>
<td>74.1</td>
<td>54.8</td>
<td>47.5</td>
<td>46.2</td>
</tr>
<tr>
<td></td>
<td>EAPrompt</td>
<td>$\chi$</td>
<td>$82.5(+8.4)$</td>
<td>$54.1(-0.7)$</td>
<td>$49.9 \dagger(+2.4)$</td>
<td>$48.3 \dagger(+1.1)$</td>
</tr>
<tr>
<td>GPT-3.5-Turbo</td>
<td>GEMBA</td>
<td>$\checkmark$</td>
<td>86.5</td>
<td>55.2</td>
<td>49.5</td>
<td>48.2</td>
</tr>
<tr>
<td></td>
<td>EAPrompt</td>
<td>$\checkmark$</td>
<td>$\underline{\mathbf{9 1 . 2}}(+4.7)$</td>
<td>$\mathbf{5 6 . 7} \dagger(+1.5)$</td>
<td>$53.3 \dagger(+3.8)$</td>
<td>$50.0 \dagger(+1.8)$</td>
</tr>
<tr>
<td></td>
<td>GEMBA</td>
<td>$\chi$</td>
<td>86.9</td>
<td>54.7</td>
<td>50.0</td>
<td>47.6</td>
</tr>
<tr>
<td></td>
<td>EAPrompt</td>
<td>$\chi$</td>
<td>$89.4(+2.5)$</td>
<td>$55.7 \dagger(+1.0)$</td>
<td>$\mathbf{5 3 . 4} \dagger(+3.4)$</td>
<td>$48.8 \dagger(+1.2)$</td>
</tr>
</tbody>
</table>
<p>Table 2: The performance of metrics using pairwise accuracy (\%) at the system level and pairwise accuracy with tie calibration (\%) at the segment level. All results are compared with human-annotated MQM scores. The best results among the same model are highlighted in bold. The best results among all metrics are underlined. " $\dagger$ " denotes cases where one metric is significantly better than the other.</p>
<p>Identifying Errors This step generates a maximum of 256 tokens in total and may terminate early if all errors are listed.</p>
<p>Counting the Number of Errors This step can be implemented in two ways. If querying LLM, it outputs only two numbers with a fixed output format, costing less than 10 tokens. Alternatively, if a regular expression matching algorithm is employed, this step does not require inference, thus reducing costs. As described in our experimental results later 4.3 these two approaches have minimal influence on the final performance.</p>
<h3>3.5 Experimental Results</h3>
<p>We compute system\&amp;segment level performance of EAPrompt with LLMs in Table 2. We see that:
(i) At the system level, EAPrompt empowers GPT-3.5-Turbo to surpass all other metrics and achieves state-of-the-art performance. Consistent with the findings of <em>Kocmi and Federmann (2023b)</em>, LLMs achieve state-of-the-art performance across all three language pairs at the system level, significantly outperforming traditional metrics ("Baselines") by a large margin.</p>
<p>Remarkably, when prompting all LLMs with EAPrompt, the performance notably surpasses GEMBA at the system level, achieving the highest
pairwise accuracy of $91.2 \%$ on GPT-3.5-Turbo, thus establishing a new SOTA.
(ii) At the segment level, EAPrompt outperforms GEMBA in 8 out of 9 tested scenarios. At the segment level, despite previous findings by <em>Kocmi and Federmann (2023b)</em> regarding the weak correlation between LLMs as evaluators and human judgments, prompting with EAPrompt addresses this drawback of LLM evaluators, outperforming GEMBA's performance on nearly all tested LLMs and language pairs by a significant margin. The best segment-level results are achieved by GPT-3.5-Turbo for En-De (56.7) and En-Ru (53.4), and by Llama2-70b-Chat for Zh-En (50.2). This validates the effectiveness of our EAPrompt.</p>
<p>The only exception of the result is observed for En-De Mixtral-8x7b-Instruct, where the segmentlevel accuracy is lower than GEMBA by 1.0. This discrepancy might be attributed to the limited capability of identifying translation errors in En-De language pair. Another notable finding is that prompting with LLMs, both with GEMBA and EAPrompt, fails to surpass current best metrics ("Baselines") at the segment level. This could be because these baseline metrics have been fine-tuned using extensive translation and human evaluation datasets, while the LLMs employed in our experiment are</p>
<table>
<thead>
<tr>
<th>Prompt</th>
<th>Demo of Errors</th>
<th></th>
<th>Type of Queries</th>
<th></th>
<th>Mixtral-8x7b-Instruct</th>
<th></th>
<th></th>
<th></th>
<th>Llama2-70b-Chat</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Detailed</td>
<td>Itemized</td>
<td>1-step</td>
<td>2-step</td>
<td>All (3 LPs)</td>
<td>En-De</td>
<td>En-Ru</td>
<td>Zh-En</td>
<td>All (3 LPs)</td>
<td>En-De</td>
<td>En-Ru</td>
<td>Zh-En</td>
</tr>
<tr>
<td>GEMBA</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>69.7</td>
<td>54.8</td>
<td>48.3</td>
<td>46.7</td>
<td>74.1</td>
<td>53.7</td>
<td>48.8</td>
<td>45.4</td>
</tr>
<tr>
<td>EAPrompt</td>
<td>$\checkmark$</td>
<td></td>
<td>$\checkmark$</td>
<td></td>
<td>75.2</td>
<td>53.4</td>
<td>50.0</td>
<td>45.0</td>
<td>62.0</td>
<td>53.7</td>
<td>47.0</td>
<td>47.8</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td></td>
<td>$\checkmark$</td>
<td>75.5</td>
<td>53.4</td>
<td>47.9</td>
<td>45.5</td>
<td>84.7</td>
<td>53.5</td>
<td>46.9</td>
<td>47.5</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>60.2</td>
<td>53.4</td>
<td>45.1</td>
<td>45.6</td>
<td>56.9</td>
<td>53.7</td>
<td>48.4</td>
<td>50.2</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td></td>
<td>$\checkmark$</td>
<td>84.0</td>
<td>53.7</td>
<td>50.6</td>
<td>48.2</td>
<td>85.4</td>
<td>55.2</td>
<td>51.4</td>
<td>50.2</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of the system level ("All (3 LPs)") and segment level ("En-De", "En-Ru", "Zh-En") performance of LLMs with different variants of prompts for EAPrompt. We compare itemized or detailed responses to demonstrate identified errors. We also compare the instructions, whether separated into two queries (marked as "2-step", one for identifying errors and another for scoring) or combined into a single query (marked as "1-step"). The best results among all prompt variants are highlighted in bold.
versatile models guided by few-shot prompts.
(iii) EAPrompt enhances the performance of LLMs as translation evaluators in reference-less scenarios. Our main findings remain consistent with both reference-based and reference-less settings (indicated by " $\checkmark$ " and " $\&amp;$ " in Ref?, respectively), where EAPrompt continues to outperform GEMBA across all three tested LLMs at the system level, and in 8 out of 9 scenarios at the segment level. The improvement is slightly lower compared to scenarios with referenced signals.</p>
<p>These results underscore the impressive crosslingual capabilities of LLMs and their suitability for quality estimation under EAPrompt, even in the absence of reference translations, which poses a significant challenge on MT evaluation.</p>
<h3>3.6 Ablation Study of Prompt Variants</h3>
<p>Given the crucial significance of the prompt design, we investigate several versions of in-context prompt contexts and present an analysis in Table 3. The prompt contexts used in our experiment are detailed in Appendix C. Due to budget constraints, we utilize two LLMs, Mixtral-8x7b-Instruct and Llama2-70b-Chat, as the test bed for this ablation study. Our findings indicate that:
(i) Itemized error demonstration is superior to detailed illustration. We assume that when identifying translation errors, providing detailed descriptions may impede the LLM's capability to accurately identify errors and count the number of them. As illustrated in the "Demo of Errors" column, employing itemized error demonstrations instead of detailed paragraphs yields improved performance at both the system and segment levels for both tested LLMs.</p>
<p>In our initial study, we observed that generating excessively detailed responses could lead to in-
correct error counting or misclassification of error severity. Therefore, it is recommended to employ clear and concise error descriptions in a format that is easily processed and comprehended by LLMs.
(ii) Separating the scoring process from error identification with two queries will enhance the performance of LLMs as translation evaluators. Another consideration in prompt design is the division of the evaluation process into error identification and error counting. As depicted in the "Type of Queries" column, it is evident that the performance of using a single prompting step is considerably lower than that of employing a 2-step prompting approach. This may be because separating the scoring process allows LLMs to concentrate on a single task in each query, thereby facilitating more accurate judgments and reducing the likelihood of incorrectly counting the number of errors.
(iii) Among the prompting strategies, EAPrompt appears to be more suitable for the LLMs as translation evaluators. When compared with GEMBA prompting strategies, the EAPrompt variant featuring a 2-step separated prompting approach and itemized response achieves superior performance in enhancing LLMs' effectiveness as translation evaluators. Consequently, we recommend employing this particular variant for LLMs as translation evaluators.</p>
<h2>4 Analysis</h2>
<h3>4.1 EAPrompt aligns with human judgment through similar distribution of major and minor errors across most LLMs</h3>
<p>To investigate can LLMs align with gold human judgement MQM through similar distributions of major and minor errors, we present the error distribution across various test scenarios in Figure 2.</p>
<p>We can see that, for major errors, all tested LLMs exhibit distributions that closely resemble MQM. Regarding minor errors, Mixtral-8x7b-Instruct appears to produce a slightly higher frequency of such errors compared to other LLMs, while the distribution of other LLMs remains consistent with MQM. This observation further validates the efficacy of EAPrompt.</p>
<p>This finding provides valuable insights into enhancing the reliability of LLMs as translation evaluators. It suggests a potential focus on guiding LLMs to more accurately identify minor errors, such as clarifying the specific categories and severity of minor errors.</p>
<h3>4.2 EAPrompt empowers LLMs to distinguish major errors from minor ones</h3>
<p>A potential concern on EAPrompt is whether this technique can prompt LLMs to distinguish major errors from minor ones. To address this concern, we adjust the weight assigned to major errors ( $w_{\text {major }}$ ) in the score computation process outlined in $\S 2.4$. We visualize the impact of this adjustment on both the system and segment-level performance in Figure 3. If the metric effectively distinguishes major errors from minor ones, we anticipate a noticeable performance decrease when the weight of major errors $w_{\text {major }}$ approaches that of minor errors ( $w_{\text {minor }}=1$ in this study).</p>
<p>Our findings reveal that for all three LLMs tested, adjusting $w_{\text {major }}&lt;3$ results in a substantial performance decline, indicating that prompting error analysis with all tested LLMs possesses the ability to discriminate major errors from minor ones.</p>
<p>Another noteworthy observation from this analysis is that when $w_{\text {major }} \geq 5$, both the systemlevel and segment level-accuracies exhibit minimal fluctuation, suggesting that the performance of EAPrompt remains nearly unaffected by this latent variable during score computation.</p>
<h3>4.3 EAPrompt optimizes inference costs by utilizing regular expressions instead of counting queries</h3>
<p>Since EAPrompt adopts a two-step prompting strategy, one related question is: can we simplify the query process to reduce inference costs? One potential approach involves substituting the scoring query step with an algorithm that identifies major and minor errors using regular expressions (Repr) to detect bullet points or initial numbers. A detailed description of the Repr matching strategy is pro-
vided in the Appendix. The analysis, as depicted in Table 4, indicates that employing Repr matching strategy, as opposed to the original query for counting errors (indicated by " $\lambda$ " in Repr?), yields minimal performance variation at both system and segment levels. Thus, if inference costs are a concern for this metric, substituting the second query step of EAPrompt with regular expressions could be a viable option. Note that for different LLMs, a tailored regular expression pattern may be necessary to encompass various response structures.</p>
<h3>4.4 Case Study</h3>
<p>We discuss potential issues encountered by LLMs and their corresponding solutions in Appendix F, including invalid responses, input order bias, etc. We aim to provide insights that should be considered when utilizing LLMs as translation evaluators.</p>
<h2>5 Related Work</h2>
<p>Translation Evaluation Metrics MT Evaluation metrics are of crucial importance to the development of MT systems (Freitag et al., 2022). Studies have shown that traditional surface-based metrics such as BLEU (Papineni et al., 2002) are no longer suitable for evaluating high-quality MT systems (Mathur et al., 2020a). Modern metrics like COMET (Rei et al., 2020), MetricsX-XXL (Juraska et al., 2023), BLEURT (Sellam et al., 2020), and UniTE (Wan et al., 2022) leverage human evaluations and high-quality translations for training. While these metrics achieve strong correlation with human judgements such as MQM (Freitag et al., 2021), there is a growing demand for explainability in their evaluation. Despite progress, recent research struggles to strike a balance between the reliability and explainability of these metrics (Lu et al., 2023; Xu et al., 2022; Perrella et al., 2022). In this work, we delve into the potential of LLMs for "human-like" translation evaluation, as they possess the capability to explicitly identify translation errors without further fine-tuning, which resembles the evaluation process of human.</p>
<p>LLMs as Evaluators LLMs refers to language models with hundreds of billion of parameters which are trained on massive textual data (Chang et al., 2024; Zhao et al., 2023). Since the emergence of ChatGPT, LLMs have shown remarkable proficiency across various NLP tasks (Achiam et al., 2023; Touvron et al., 2023b). A prevalent application of LLMs is harnessing them as evaluators</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Distribution of identified error counts across various LLMs and human evaluation (MQM), for the language pairs En-De, En-Ru and Zh-En, respectively.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Repr?</th>
<th>System-Level Acc.</th>
<th>Segment-Level Acc*</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>All (3 LPs)</td>
<td>En-De</td>
</tr>
<tr>
<td>Llama2-70b-Chat</td>
<td>✓</td>
<td>85.0</td>
<td>55.6</td>
</tr>
<tr>
<td></td>
<td>✗</td>
<td>85.4</td>
<td>55.2</td>
</tr>
<tr>
<td>Mixtral-8x7b-Instruct</td>
<td>✓</td>
<td>82.8</td>
<td>53.7</td>
</tr>
<tr>
<td></td>
<td>✗</td>
<td>84.0</td>
<td>53.7</td>
</tr>
<tr>
<td>GPT-3.5-Turbo</td>
<td>✓</td>
<td>90.1</td>
<td>56.8</td>
</tr>
<tr>
<td></td>
<td>✗</td>
<td>91.2</td>
<td>56.7</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance comparison of EAPrompt between utilizing the Regular Expression Matching strategy ("✓" in Repr?) and the counting query strategy ("✗" in Repr?) across various LLMs.</p>
<p>for assessing the performance of Chatbots (Zheng et al., 2023) or some self-criticize/improvement procedures (Zhang et al., 2024; Valmeekam et al., 2023). Recent studies also show LLM's efficacy in evaluating NLG tasks like summarization and dialogue generation through multi-step prompting (Liu et al., 2023). GEMBA (Kocmi and Federmann, 2023b) is the pioneering effort in utilizing LLMs as translation evaluators via a zero-shot prompting approach with GPT models. In this work, EAPrompt innovatively combines the basic ideas of error analysis (Lu et al., 2023) and chain-of-thought (Wei et al., 2022) to prompt LLMs for achieving human-like translation evaluation.</p>
<p>Subsequent work follows our work to further explore the potential of LLMs as translation evaluators. AutoMQM (Fernandes et al., 2023) parallels our approach, utilizing PaLM-2 model (Anil et al., 2023) as the testbed. GEMBA-MQM (Kocmi and Federmann, 2023a) further improves EAPrompt by employing a few-shot prompting technique using GPT-4, making this approach universally applicable across languages. Another line of research focuses on fine-tuning LLMs to accurately predict error spans in translations. For instance, InstructScore (Xu et al., 2023) fine-tunes a Llama model (Touvron et al., 2023a), while XCOMET (Guerreiro et al., 2023) scales from COMETKiwi (Rei et al., 2023) to achieve this goal.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we explore the potential of LLMs as a metric for evaluating translations. We design a novel one-shot prompting strategy EAPrompt based on chain-of-thought and error analysis, and show that this strategy significantly improves the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Effect of varying major error weight (w<sub>major</sub>) on EAPrompt across different LLMs at both system and segment levels.</p>
<p>Evaluation performance on both the system and segment levels. We compare different EAPrompt variants and ultimately opt for a 2-step prompting approach with itemized error demonstrations. Further analysis confirms EAPrompt's proficiency in error identification and its alignment with the commonly accepted human evaluations MQM. In future work, we would like to experiment with a broader range of LLMs (Barrault et al., 2019; Anastasopoulos et al., 2021; Kocmi et al., 2022; Zan et al., 2022), to make our conclusion more convincing. Lastly, it will be interesting to test the capabilities of LLMs for other MT-related tasks, such as grammatical error correction and automatic post-editing (Wu et al., 2023; Vidal et al., 2022).</p>
<h3>Limitations</h3>
<p>The limitations of this work are three-fold:</p>
<ul>
<li><strong>Potential Test Data Contamination</strong>: Although we utilized WMT22 to minimize the risk of test set leakage in the training data of LLMs, it is still possible that some contamination from the test data remains. Therefore, future researchers utilizing these datasets should be cautious and carefully address this issue, as it may affect the availability of the test set for comparison purposes.</li>
<li><strong>Budget Constraints</strong>: Due to limited resources, we were unable to explore more prompt choices comprehensively in our research. The findings presented in this study only reflect our initial experiments. We leave the impact of different prompt choices for further investigation.</li>
<li><strong>Limited Range of LLMs Tested</strong>: In this study, we focused on evaluating a limited number of LLMs that we believed possessed potential and capability as translation evaluators. However, it is important to note that not all existing LLMs can necessarily serve as reliable evaluators under the EAPrompt approach. Future research could explore and experiment with a broader range of LLMs, examining their effectiveness and assessing their suitability as evaluators.</li>
</ul>
<h3>Ethics Statement</h3>
<p>We take ethical considerations very seriously, and strictly adhere to the Code of Ethics. All procedures performed in this study are in accordance with the ethical standards. This paper focuses on evaluating the capabilities of LLM as a translation evaluator. Our proposed approach, EAPrompt, does not include statements that induce the model to generate harmful information. Additionally, this method solely extracts and processes the numerical scores from the model's response, thereby further mitigating the potential risks. Both the datasets and models used in this paper are publicly available and have been widely adopted by researchers. Our model will not learn from user inputs or cause potential risks to the NLP community. We ensure that the findings and conclusions of this paper are reported accurately and objectively. Informed consent was obtained from all individual participants included in this study.</p>
<h3>Acknowledgments</h3>
<p>We thank the anonymous reviewers and the area chair for their insightful comments and suggestions. This work was supported in part by the National Natural Science Foundation of China under Grant 61973083, and in part by the Shenzhen Science and Technology Program JCYJ20210324121213036.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint.</p>
<p>Antonios Anastasopoulos, Ondřej Bojar, Jacob Bremerman, Roldano Cattoni, Maha Elbayad, Marcello Federico, et al. 2021. Findings of the IWSLT 2021 evaluation campaign. In IWSLT.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint.</p>
<p>Loïc Barrault, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, et al. 2019. Findings of the 2019 conference on machine translation (WMT19). In WMT.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. 2020. Language models are few-shot learners. NeurIPS.</p>
<p>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2024. A survey on evaluation of large language models. $A C M$.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint.</p>
<p>Daniel Deutsch, Rotem Dror, and Dan Roth. 2021. A statistical analysis of summarization evaluation metrics using resampling methods. TACL.</p>
<p>Daniel Deutsch, George Foster, and Markus Freitag. 2023. Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration. In EMNLP.</p>
<p>Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In WMT.</p>
<p>Markus Freitag, George Foster, David Grangier, et al. 2021. Experts, errors, and context: A large-scale study of human evaluation for machine translation. TACL.</p>
<p>Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In WMT.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, et al. 2022. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In WMT.</p>
<p>Nuno M Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André FT Martins. 2023. xcomet: Transparent machine translation evaluation through fine-grained error detection. arXiv preprint.</p>
<p>Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, et al. 2023. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint.</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? a preliminary study. arXiv preprint.</p>
<p>Juraj Juraska, Mara Finkelstein, Daniel Deutsch, Aditya Siddhant, Mehdi Mirzazadeh, and Markus Freitag. 2023. MetricX-23: The Google submission to the WMT 2023 metrics shared task. In WMT.</p>
<p>Tom Kocmi, Rachel Bawden, Ondřej Bojar, et al. 2022. Findings of the 2022 conference on machine translation (WMT22). In WMT.</p>
<p>Tom Kocmi and Christian Federmann. 2023a. GEMBAMQM: Detecting translation quality error spans with GPT-4. In WMT.</p>
<p>Tom Kocmi and Christian Federmann. 2023b. Large language models are state-of-the-art evaluators of translation quality. In EAMT.</p>
<p>Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In WMT.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignment. In EMNLP.</p>
<p>Qingyu Lu, Liang Ding, Liping Xie, Kanjian Zhang, Derek F. Wong, and Dacheng Tao. 2023. Toward human-like evaluation for natural language generation with error analysis. In $A C L$.</p>
<p>Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020a. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In $A C L$.</p>
<p>Nitika Mathur, Johnny Wei, Markus Freitag, Qingsong Ma, and Ondřej Bojar. 2020b. Results of the WMT20 metrics shared task. In WMT.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In $A C L$.</p>
<p>Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao. 2023. Towards making the most of chatgpt for machine translation. arXiv preprint.</p>
<p>Stefano Perrella, Lorenzo Proietti, Alessandro Scirè, Niccolò Campolungo, and Roberto Navigli. 2022. MaTESe: Machine translation evaluation as a sequence tagging problem. In WMT.</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint.</p>
<p>Baopu Qiu, Liang Ding, Di Wu, Lin Shang, Yibing Zhan, and Dacheng Tao. 2022. Original or translated? on the use of parallel data for translation quality estimation. arXiv preprint.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog.</p>
<p>Ricardo Rei, Nuno M. Guerreiro, JosÃ© Pombal, Daan van Stigt, Marcos Treviso, Luisa Coheur, José G. C. de Souza, and André Martins. 2023. Scaling up CometKiwi: Unbabel-IST 2023 submission for the quality estimation shared task. In WMT.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In EMNLP.</p>
<p>Zhiyao Ren, Yibing Zhan, Baosheng Yu, Liang Ding, and Dacheng Tao. 2024. Healthcare copilot: Eliciting the power of general llms for medical consultation. arXiv preprint.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In $A C L$.</p>
<p>Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Machine translation evaluation versus quality estimation. Machine translation.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint.</p>
<p>Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023. Investigating the effectiveness of self-critiquing in LLMs solving planning tasks. In NeurIPS Foundation Models for Decision Making Workshop.</p>
<p>Blanca Vidal, Albert Llorens, and Juan Alonso. 2022. Automatic post-editing of MT output using large language models. In AMTA.</p>
<p>Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek Wong, and Lidia Chao. 2022. UniTE: Unified translation evaluation. In $A C L$.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint.</p>
<p>Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, and Michael Lyu. 2023. Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark. arXiv preprint.</p>
<p>Wenda Xu, Yi-Lin Tuan, Yujie Lu, Michael Saxon, Lei Li, and William Yang Wang. 2022. Not all errors are equal: Learning text generation metrics using stratified error synthesis. In EMNLP.</p>
<p>Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li. 2023. INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. In EMNLP.</p>
<p>Changtong Zan, Keqin Peng, Liang Ding, Baopu Qiu, et al. 2022. Vega-MT: The JD explore academy machine translation system for WMT22. In WMT.</p>
<p>Chrysoula Zerva, Frédéric Blain, Ricardo Rei, et al. 2022. Findings of the WMT 2022 shared task on quality estimation. In WMT.</p>
<p>Yuqi Zhang, Liang Ding, Lefei Zhang, and Dacheng Tao. 2024. Intention analysis prompting makes large language models a good jailbreak defender. arXiv preprint.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint.</p>
<p>Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. 2023. Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert. arXiv preprint.</p>
<p>Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, and Dacheng Tao. 2024. Achieving&gt; 97\% on gsm8k: Deeply understanding the problems makes llms perfect reasoners. arXiv preprint.</p>
<h2>A Description of MQM</h2>
<p>Multidimensional Quality Metric (MQM) is a human evaluation framework commonly used in WMT metrics shared tasks as the golden standard (Freitag et al., 2021, 2023). It is developed to evaluate and categorize errors in translations. The annotations from human experts are open-sourced and available from WMT22 metrics shared tasks for scientific research (Freitag et al., 2022).</p>
<p>In WMT22, MQM annotations for En-De and Zh -En were sponsored and executed by Google, using 11 professional translators ( 7 for En-De, 4 for Zh -En). The annotations for En-Ru were provided by Unbabel who utilized 4 professional, native language annotators with ample translation experience. They have access to the full document context.</p>
<p>About the inter-rater agreement. In MQM, each segment is annotated by 2 or 3 annotators. The final segment-level score is an average over scores from all annotators. As depicted in (Freitag et al., 2021), the pairwise inter-rater agreement is about 0.584 for En-De, and 0.412 for Zh-En, which is significantly better than other evaluation protocols such as Scalar Quality Metric and Direct Assessment.</p>
<p>In this paper, EAPrompt emulates MQM to identify major and minor errors, providing insightful explanations for the translation. Table 5 shows an example annotated through MQM framework.</p>
<h2>B Post-processing of EAPrompt</h2>
<p>As described in $\S 2.4$, we treat $w_{\text {major }}$ as a latent variable within EAPrompt. In our experiments, we select this latent variable with the best averaging performance for each LLMs denoted as $w_{\text {major }}^{*}$. The value was reported in Table 6.</p>
<h2>C Prompt Contexts of EAPrompt</h2>
<p>Figure 4 provides the prompt contexts implemented in EAPrompt, along with the detailed error demonstration and combined query instruction discussed in $\S 3.6$ for reproduction of our experiments.</p>
<h2>D Counting Errors using Regular Expressions Matching</h2>
<p>In Figure 5, we present an overview of our errormatching strategy utilized in $\S 4.3$ to automatically identify the number of major and minor errors. The procedure can be listed as follows:</p>
<ol>
<li>Locate "major error" and "minor error" within the response, then segment the response accordingly.</li>
<li>Utilize Regular Expression matching to identify the initial numbers of major and minor errors. For implementation, we include three different initial number formats: "1.", "1)" and "(1)" (using "1" as an example);</li>
<li>Record the number of major and minor errors.</li>
</ol>
<h2>E Additional Results</h2>
<h2>E. 1 Results on GPT-4</h2>
<p>In order to further verify the effect of EAPrompt on state-of-the-art LLMs, we test our approach on GPT-4 as an initial experiment, with a subset of 600 samples, due to budget constraints. Results are shown in Table 7. We can see that on powerful GPT4, EAPrompt shows a slight performance advantage over GEMBA-DA, outperforming it by 1.05 points.</p>
<h2>E. 2 Results at the system-level</h2>
<p>To complement our main findings, we also compare our method, EAPrompt, with GEMBA using pearson correlation and pairwise accuracy. In addition to Table 2, which groups three language pairs together to present comprehensive and unbiased results, Table 8 provides a system-level performance comparison across different language pairs for a more detailed analysis.</p>
<p>We can see that at the system level, across all LLMs and for all three language pairs, EAPrompt consistently outperforms GEMBA in most scenarios, aligning with the primary conclusions drawn in our paper. Additionally, we note a few cases where the two metrics exhibit slight discrepancies. This discrepancy, typically less than 1.0 point, may</p>
<table>
<thead>
<tr>
<th>System</th>
<th>Online-A.en</th>
</tr>
</thead>
<tbody>
<tr>
<td>Domain</td>
<td>conversational</td>
</tr>
<tr>
<td>Doc_id</td>
<td>1</td>
</tr>
<tr>
<td>Seg_id</td>
<td>6</td>
</tr>
<tr>
<td>Source(Zh)</td>
<td>请问，订单情况现在是什么样?</td>
</tr>
<tr>
<td>Reference(En)</td>
<td>May I ask what the status of the order is now?</td>
</tr>
<tr>
<td>Translation(En)</td>
<td>Please ask, what is the order situation now?</td>
</tr>
<tr>
<td>Major Error(s)</td>
<td>"Please ask" - Accuracy/Mistranslation</td>
</tr>
<tr>
<td>Minor Error(s)</td>
<td>"situation" - Style/Awkward</td>
</tr>
</tbody>
</table>
<p>Table 5: An example of MQM, comprising information of the test sample along with human-annotated errors.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$w_{\text {major }}^{*}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5-Turbo</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70b-Chat</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8x7b-Instruct</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p>Table 6: Optimal values of $w_{\text {major }}^{*}$ for each LLM. To ensure fair comparison, we maintain this variable constant across all tested scenarios for every LLM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">GEMBA</th>
<th style="text-align: center;">EAPrompt</th>
<th style="text-align: center;">$\delta$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">44.92</td>
<td style="text-align: center;">45.97</td>
<td style="text-align: center;">+1.05</td>
</tr>
</tbody>
</table>
<p>Table 7: Segment Level Kendall Correlation on WMT22 Zh-En using GPT-4 with a subset of 600 samples.
be attributed to Pearson correlation's sensitivity to low-quality MT systems [Mathur et al. (2020a)], potentially leading to an unfair judgment compared to Pairwise Accuracy.</p>
<h3>E. 3 Results at the segment-level</h3>
<p>We present more detailed results, including Pearson correlation, as shown in Table 9. We can observe that when using Pearson correlation as the metaevaluation method, EAPrompt surpasses GEMBA by a significant margin across all scenarios considered in our experiments. This further validates the effectiveness and strong performance of EAPrompt compared to GEMBA.</p>
<h2>F Case Study</h2>
<p>In Figure 6, we list several typical issues with the case study that should be aware of when using LLMs such as ChatGPT as translation evaluators.</p>
<h3>F. 1 Potential instability in the responses without temperature control</h3>
<p>Issue: When evaluating translations using LLMs, the generated responses may vary significantly. See in Case 1, we regenerate several responses with the
same input and obtain 3 different scores (98, 95, 100) for the translation.
Solution: We control the temperature parameter to mitigate the variability in LLM judgments. Accordingly, for all experiments detailed in this paper, we set the temperature to 0 for GPT-3.5-Turbo. For the other two models, namely Llama2-70b-Chat and Mixtral-8x7b-Instruct, we opted for a temperature setting of 0.05 since the inference parameter from these two models should be above zero.</p>
<h2>F. 2 Input order bias when evaluating multiple translations simultaneously</h2>
<p>Issue: An alternative prompting strategy is to present multiple translations together as a single input to LLMs for evaluation, reducing the number of queries and potentially saving budget. However, we observe a bias where translations presented earlier tend to get higher scores compared to those presented later. As shown in Case 2, we provide 8 translations along with their corresponding source and reference sentences. At the first time, we present the translations sequentially and ask LLM to rank them according to their translation quality. Then, we reverse the order of translations and obtain an entirely different sequence of ranks. Solution: The contradictory results may be attributed to the auto-regressive nature of the decoder model, which gives more attention to the latter input, potentially leading to greater identification of errors for the translation input later. Therefore, we recommend that researchers input one translation at a time instead of providing multiple translations.</p>
<h2>F. 3 LLMs may generate invalid answers for all prompting strategies</h2>
<p>Issue: We observe that in certain cases, LLMs may not function as translation evaluators that may produce invalid answers with textual explanations. A typical case is illustrated in Case 3, where ChatGPT tends to prioritize the BLEU score instead of</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Prompts</th>
<th style="text-align: center;">Ref?</th>
<th style="text-align: center;">En-De</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">En-Ru</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Zh-En</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">Acc.</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-70b-Chat</td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">77.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">98.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">74.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">84.6</td>
</tr>
<tr>
<td style="text-align: center;">Mixtral-8x7b-Instruct</td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">74.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">95.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">82.4</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">82.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">92.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">85.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">89.0</td>
</tr>
</tbody>
</table>
<p>Table 8: The performance of metrics for each language pair using pearson correlation ( $\rho$ ) and pairwise accuracy (Acc.) at the system level. All results are compared with human-annotated MQM scores. The best results among the same model are highlighted in bold. The best results among all metrics are underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Prompts</th>
<th style="text-align: center;">Ref?</th>
<th style="text-align: center;">En-De</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">En-Ru</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Zh-En</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">Acc*</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">Acc*</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">Acc*</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-70b-Chat</td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">45.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">27.8 $\dagger$</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">28.1 $\dagger$</td>
<td style="text-align: center;">51.4 $\dagger$</td>
<td style="text-align: center;">39.1 $\dagger$</td>
<td style="text-align: center;">50.2 $\dagger$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">45.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">26.2 $\dagger$</td>
<td style="text-align: center;">55.0 $\dagger$</td>
<td style="text-align: center;">29.8 $\dagger$</td>
<td style="text-align: center;">51.6 $\dagger$</td>
<td style="text-align: center;">38.6 $\dagger$</td>
<td style="text-align: center;">49.3 $\dagger$</td>
</tr>
<tr>
<td style="text-align: center;">Mixtral-8x7b-Instruct</td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">46.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">30.1 $\dagger$</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">28.5 $\dagger$</td>
<td style="text-align: center;">50.6 $\dagger$</td>
<td style="text-align: center;">40.2 $\dagger$</td>
<td style="text-align: center;">48.2 $\dagger$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">46.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">26.7 $\dagger$</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">24.5 $\dagger$</td>
<td style="text-align: center;">49.9 $\dagger$</td>
<td style="text-align: center;">37.9 $\dagger$</td>
<td style="text-align: center;">48.3 $\dagger$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">48.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">39.9 $\dagger$</td>
<td style="text-align: center;">56.7 $\dagger$</td>
<td style="text-align: center;">34.6 $\dagger$</td>
<td style="text-align: center;">53.3 $\dagger$</td>
<td style="text-align: center;">41.9 $\dagger$</td>
<td style="text-align: center;">50.0 $\dagger$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GEMBA</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">47.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAPrompt</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">33.3 $\dagger$</td>
<td style="text-align: center;">55.7 $\dagger$</td>
<td style="text-align: center;">30.2 $\dagger$</td>
<td style="text-align: center;">53.4 $\dagger$</td>
<td style="text-align: center;">36.2 $\dagger$</td>
<td style="text-align: center;">48.8 $\dagger$</td>
</tr>
</tbody>
</table>
<p>Table 9: The performance of metrics for each language pair using pearson correlation ( $\rho$ ) and pairwise accuracy with ties (Acc*) at the segment level. All results are compared with human-annotated MQM scores. The best results among the same model are highlighted in bold. The best results among all metrics are underlined. " $\dagger$ " denotes cases where one metric is significantly better than the other.
offering judgments based on its inherent capabilities.
Solution: We follow the method mentioned in Kocmi and Federmann (2023b) for handling invalid answers, where we introduce randomness to LLMs by iteratively increasing the temperature. Subsequently, we take the first response that falls within the expected score range.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>I think the mistranslation of "subjects" should be categorized into a major error, and the omission in "BEIJING" should also considered as a major error. "households of various", "festival" and "supervision" are three mistranslation errors, they should be categorized into minor errors. The terminology, "Beijing Municipal Market Supervision Bureau" is Inappropriate for context, and should also be categorized into a minor error. "BEIJING" also has a spelling error, which is considered as a minor error.</p>
<p>Figure 4: The prompt contexts employed in EAPrompt. We present itemized/detailed responses for error demonstrations and separated/combined instructions for different types of queries.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The regular expression matching strategy utilized in $\S 4.3$ to automatically count the number of major and minor errors in the LLM response.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Case study of potential issues in LLMs. All three cases are from GPT-3.5-Turbo model ("ChatGPT"). Top: LLM exhibits variations in its responses upon multiple regenerations; Medium: different input order of samples may affect the judgment of LLM; Bottom: LLM sometimes relies on existing metrics during translation evaluation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/google-research/ mt-metrics-eval&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ We use the 0613 OpenAI model version. Due to budget constraints, we verify EAPrompt on GPT-4 with a limited number of samples. Please refer to Appendix E.1 for more details.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
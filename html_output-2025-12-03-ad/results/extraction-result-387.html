<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-387 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-387</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-387</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-266162524</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.05402v1.pdf" target="_blank">Towards Controlled Table-to-Text Generation with Scientific Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> The sheer volume of scientific experimental results and complex technical statements, often presented in tabular formats, presents a formidable barrier to individuals acquiring preferred information. The realms of scientific reasoning and content generation that adhere to user preferences encounter distinct challenges. In this work, we present a new task for generating fluent and logical descriptions that match user preferences over scientific tabular data, aiming to automate scientific document analysis. To facilitate research in this direction, we construct a new challenging dataset CTRLSciTab consisting of table-description pairs extracted from the scientific literature, with highlighted cells and corresponding domain-specific knowledge base. We evaluated popular pre-trained language models to establish a baseline and proposed a novel architecture outperforming competing approaches. The results showed that large models struggle to produce accurate content that aligns with user preferences. As the first of its kind, our work should motivate further research in scientific domains. 1</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e387.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e387.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TSDAE-adapted retriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TSDAE-inspired conditional unsupervised sentence retriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised sentence-embedding retriever inspired by TSDAE, adapted to retrieve domain-specific knowledge sentences conditioned on table contents and highlighted user-preference cells for scientific table-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tsdae: Using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>TSDAE-based conditional sentence retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Learns unsupervised sentence embeddings by reconstructing domain-specific sentences from perturbed versions (TSDAE style) but with conditioning on table contents (T) and highlighted cells (H). The model optimizes a conditional reconstruction objective P(B | B_perturbed, T, H) (expressed in the paper as a JSDAE loss) and uses the learned embeddings to retrieve top-n domain-specific knowledge sentences relevant to a table and user preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / retrieval technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general unsupervised sentence embedding / NLP (sentence representation learning)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>scientific table-to-text retrieval (scientific NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Imposed conditional constraints on the embedding objective to incorporate tabular data T and highlighted cells H (user preferences) into the reconstruction process, i.e., training the embedding to model P(B|B_perturbed, T, H) rather than unconditional reconstruction. Retrieval returns top-3 related domain-specific sentences per table for downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — the adapted retriever outperformed a TF-IDF baseline when plugged into the generation pipeline (Retriever + Direct Generation (Bart-base) BLEU = 2.0 vs TF-IDF + DG (Bart) BLEU = 1.6), and contributed to the stronger overall CTRLSciTabNet (Bart) system which achieved BLEU = 16.9. The paper reports improved automatic metrics and better human-evaluation results when domain-specific retrieval is included.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Large amount of supporting domain-specific sentences per table (average ~20) makes selection difficult; noisy/automatic annotations required human correction; moderate inter-annotator agreement (66.7% for highlighted cells, 70.6% for domain-knowledge) indicates ambiguity and labeling noise impacting retrieval targets.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of extracted domain-specific sentence pool (from arXiv articles) and highlighted cells as conditioning signals enabled conditioning; inspiration and prior architecture (TSDAE) provided a suitable unsupervised training objective.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires the parsed domain-specific knowledge corpus linked to tables, highlighted-cell annotations (user preference signals), computational resources for unsupervised embedding training, and domain expert verification to correct automatic annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Likely generalizable to other domain-specific table-to-text tasks where (1) a corpus of domain sentences aligned to structured data exists and (2) user preference signals can be provided; may require re-tuning/conditioning for domains with very different terminology or much larger knowledge pools.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical principles (unsupervised embedding training procedure and conditional objective)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Controlled Table-to-Text Generation with Scientific Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e387.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e387.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLM transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Application of general-domain pre-trained language models (BART, T5, GPT-3.5) to scientific table-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses widely used pre-trained language models (BART-base, T5-small, and GPT-3.5) as generators for producing scientific descriptions from tables, testing how models pretrained on open-domain corpora perform in a scientific, controlled generation task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Pre-trained Language Model (PLM) application to scientific text generation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Employs BART-base and T5-small as seq2seq generators fine-tuned (or used) to map inputs [H : T : B] (highlighted cells, linearized table, retrieved domain knowledge) to target descriptions; also evaluates direct generation from GPT-3.5 as a baseline. Training objective for finetuned generators is cross-entropy minimization over reference descriptions using encoder representations of the inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / model transfer / fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>open-domain pretraining / general NLP</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>scientific table-to-text natural language generation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (fine-tuning and conditioning with retrieved domain knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Conditioned the generators on tabular inputs and retrieved domain-specific sentences plus highlighted cells; in CTRLSciTabNet the generator uses retrieved B alongside T and H. The paper evaluates both models with and without domain-specific knowledge (W/o BKG).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>mixed — BART and T5 achieved reasonable performance when combined with retrieved domain knowledge (CTRLSciTabNet (Bart) BLEU = 16.9; CTRLSciTabNet (T5) BLEU = 6.6). Models without domain-specific knowledge had substantially worse performance (examples: T5 w/o BKG BLEU = 2.1; DG w/o BKG GPT-3.5 BLEU = 3.07), and GPT-3.5 generally underperformed on this domain-specific reasoning task. Human evaluation showed improvements when domain knowledge was present but persistent fidelity and recall shortcomings remain.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Pretraining on open-domain text leaves gaps in domain expertise causing hallucinations and low faithfulness; popular large models (GPT-3.5) struggled on domain-specific scientific reasoning; automatic metrics inadequately capture controlled alignment to highlighted cells.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Access to a curated domain-specific knowledge base (CTRLSciTab) and highlighted cells as control signals improved performance; finetuning and retrieval-augmented inputs helped adapt PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Domain-specific knowledge corpus, highlighted cell annotations, finetuning data (CTRLSciTab labeled pairs) and human evaluation to measure faithfulness were necessary for effective transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Applicable to other scientific domains but requires domain-specific corpora and careful retrieval/fine-tuning; success depends on the PLM size, available fine-tuning data, and quality of retrieved knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (model fine-tuning and conditioning), representational/model knowledge (weights trained on domain data)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Controlled Table-to-Text Generation with Scientific Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e387.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e387.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TF-IDF baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TF-IDF-based retrieval for domain-specific sentence selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical information-retrieval method (TF-IDF) used as a baseline retriever to fetch domain-specific sentences relevant to a table for downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An information-theoretic perspective of tf-idf measures</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>TF-IDF retrieval for domain-specific knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Computes TF-IDF vectors for candidate domain-specific sentences and table/query text and returns top-ranked sentences by similarity as external knowledge to feed the generator. Used without additional conditioning or embedding learning.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / retrieval technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>information retrieval / general NLP</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>scientific domain-specific sentence retrieval for table-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without modification</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>None beyond standard application to the parsed domain-specific sentence pool; used as a baseline in the retrieval-generator pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>limited — TF-IDF + Direct Generation (Bart) yielded BLEU = 1.6 (Table 2), which was outperformed by the adapted retriever (Retriever + DG (Bart) BLEU = 2.0) and by the full CTRLSciTabNet system (CTRLSciTabNet (Bart) BLEU = 16.9).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Lacks deep semantic understanding required for scientific reasoning and precise mapping from table semantics to sentences; retrieval quality insufficient for faithful scientific description generation.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Simplicity and low computational cost make TF-IDF a viable baseline; easy to implement against extracted sentence pools.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires a pre-extracted candidate sentence pool aligned to documents/tables; no specialized equipment but needs pre-processing of documents to text.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Directly applicable as a baseline across domains but unlikely to be sufficient for high-fidelity scientific table-to-text tasks without semantic or embedding-based enhancements.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (classic IR algorithm application)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Controlled Table-to-Text Generation with Scientific Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e387.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e387.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDF->XML pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PDF parsing and greedy content-matching pipeline for mining domain-specific knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that parses arXiv PDF articles to XML, uses a greedy content-matching algorithm to align parsed sentences with tables, deduplicates matches, performs entity detection, and applies expert verification to produce table-sentence pairs and a domain-specific knowledge base.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>PDF parsing and greedy content matching for dataset construction</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Transforms arXiv articles into XML, runs a greedy matching algorithm to align sentences to tables based on parsed content and table metadata, deduplicates overlapping text, detects entities referenced in descriptions, and uses expert annotators to correct irrelevant highlights and refine domain-specific knowledge, producing controlled table-description pairs with highlighted cells and knowledge sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data extraction and annotation pipeline / dataset construction protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>document parsing and information extraction (document engineering / NLP preprocessing)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>scientific dataset construction for table-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Adapted standard PDF parsing and content-matching techniques to (1) preserve entity-referencing sentences, (2) deduplicate with table descriptions, and (3) generate highlighted-cell annotations; added an expert verification stage where annotators (computer science undergraduates) corrected automatically annotated data.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful in producing a new dataset: CTRLSciTab with 8,967 controlled table-description pairs, average ~52 cells per table, ~34-word descriptions, highlighted cells covering ~20% of cells, and ~20 domain-specific knowledge sentences per table. Annotation agreement rates reported: 66.7% for highlighted cells and 70.6% for domain-specific knowledge, indicating moderate success but non-trivial noise.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Noisy automatic alignments requiring manual correction; moderate inter-annotator agreement indicates ambiguity; challenges parsing PDFs and reliably aligning free-text sentences to structured table cells.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of arXiv metadata and documents, and the ability to run automated parsing and greedy matching provided a scalable starting point; human annotators enabled quality improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to original PDF/arXiv sources, XML conversion tools, entity-detection tooling, domain-aware annotators for verification, and compute infrastructure for parsing large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Approach is generalizable to other scientific domains with accessible PDFs and table metadata, but dependent on quality of PDF to XML conversion and available annotator expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>tacit/practical know-how (document processing and annotation workflow) and explicit procedural steps (pipeline stages)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Controlled Table-to-Text Generation with Scientific Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e387.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e387.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CTRLSciTabNet (RAG-like)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CTRLSciTabNet retriever-generator framework (retrieval-augmented generation adapted for controlled scientific table-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-then-generation architecture that first retrieves top-n domain-relevant sentences and then conditions a pre-trained seq2seq generator (BART/T5) on the retrieved sentences plus table and highlighted cells to produce controlled scientific descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Retrieval-augmented generation for controlled scientific table-to-text (CTRLSciTabNet)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Two-step pipeline: (1) an unsupervised conditional retriever selects the most relevant domain-specific sentences for a given table and highlighted cells; (2) a generator (BART-base or T5-small) consumes linearized table T, highlighted cells H, and retrieved domain knowledge B to generate an analytical description aligned to user preferences. Training minimizes cross-entropy loss of generator outputs conditioned on encoder representations of [H : T : B].</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / hybrid retrieval-generation architecture</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>retrieval-augmented generation methods in open-domain NLP</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>controlled scientific table-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (incorporates highlighted cells as control and domain-specific KB)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Incorporated highlighted cells H as explicit control signals in both retrieval and generation steps; constrained retriever embeddings with table context; limited retrieved set to top-3 sentences; tailored generator inputs to [H : T : B] and fine-tuned on CTRLSciTab data.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful relative to baselines — CTRLSciTabNet (Bart) substantially outperformed baseline pipelines on automatic metrics (BLEU = 16.9 vs TF-IDF+DG (Bart) BLEU = 1.6) and human evaluations (improvements in fluency, faithfulness, recall, and valid facts), though fidelity and recall of highlighted cells remain imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Persistent hallucinations and incorrect facts even with retrieved knowledge; automatic metrics insufficient to fully capture controlled alignment; models still failed to fully capture highlighted content (low recall).</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Explicit control signals (highlighted cells) and an available domain-specific KB; combining retrieval with pre-trained generators allowed leveraging both external knowledge and generation fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Domain-specific knowledge sentences, highlighted-cell annotations, computation for retrieval/generation training, and human evaluation to assess faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Approach is likely generalizable to other controlled-data-to-text tasks but requires domain-specific knowledge corpora and domain-aware control signals (e.g., highlighted cells) for effective transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (architecture and training protocol) and interpretive frameworks (how to condition retrieval/generation on user preferences)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Controlled Table-to-Text Generation with Scientific Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tsdae: Using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning. <em>(Rating: 2)</em></li>
                <li>Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. <em>(Rating: 2)</em></li>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer. <em>(Rating: 2)</em></li>
                <li>An information-theoretic perspective of tf-idf measures <em>(Rating: 2)</em></li>
                <li>Scigen: a dataset for reasoning-aware text generation from scientific tables. <em>(Rating: 1)</em></li>
                <li>Towards table-to-text generation with numerical reasoning. <em>(Rating: 1)</em></li>
                <li>Totto: A controlled table-to-text generation dataset. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-387",
    "paper_id": "paper-266162524",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "TSDAE-adapted retriever",
            "name_full": "TSDAE-inspired conditional unsupervised sentence retriever",
            "brief_description": "An unsupervised sentence-embedding retriever inspired by TSDAE, adapted to retrieve domain-specific knowledge sentences conditioned on table contents and highlighted user-preference cells for scientific table-to-text generation.",
            "citation_title": "Tsdae: Using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning.",
            "mention_or_use": "use",
            "procedure_name": "TSDAE-based conditional sentence retrieval",
            "procedure_description": "Learns unsupervised sentence embeddings by reconstructing domain-specific sentences from perturbed versions (TSDAE style) but with conditioning on table contents (T) and highlighted cells (H). The model optimizes a conditional reconstruction objective P(B | B_perturbed, T, H) (expressed in the paper as a JSDAE loss) and uses the learned embeddings to retrieve top-n domain-specific knowledge sentences relevant to a table and user preferences.",
            "procedure_type": "computational method / retrieval technique",
            "source_domain": "general unsupervised sentence embedding / NLP (sentence representation learning)",
            "target_domain": "scientific table-to-text retrieval (scientific NLP)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Imposed conditional constraints on the embedding objective to incorporate tabular data T and highlighted cells H (user preferences) into the reconstruction process, i.e., training the embedding to model P(B|B_perturbed, T, H) rather than unconditional reconstruction. Retrieval returns top-3 related domain-specific sentences per table for downstream generation.",
            "transfer_success": "partially successful — the adapted retriever outperformed a TF-IDF baseline when plugged into the generation pipeline (Retriever + Direct Generation (Bart-base) BLEU = 2.0 vs TF-IDF + DG (Bart) BLEU = 1.6), and contributed to the stronger overall CTRLSciTabNet (Bart) system which achieved BLEU = 16.9. The paper reports improved automatic metrics and better human-evaluation results when domain-specific retrieval is included.",
            "barriers_encountered": "Large amount of supporting domain-specific sentences per table (average ~20) makes selection difficult; noisy/automatic annotations required human correction; moderate inter-annotator agreement (66.7% for highlighted cells, 70.6% for domain-knowledge) indicates ambiguity and labeling noise impacting retrieval targets.",
            "facilitating_factors": "Availability of extracted domain-specific sentence pool (from arXiv articles) and highlighted cells as conditioning signals enabled conditioning; inspiration and prior architecture (TSDAE) provided a suitable unsupervised training objective.",
            "contextual_requirements": "Requires the parsed domain-specific knowledge corpus linked to tables, highlighted-cell annotations (user preference signals), computational resources for unsupervised embedding training, and domain expert verification to correct automatic annotations.",
            "generalizability": "Likely generalizable to other domain-specific table-to-text tasks where (1) a corpus of domain sentences aligned to structured data exists and (2) user preference signals can be provided; may require re-tuning/conditioning for domains with very different terminology or much larger knowledge pools.",
            "knowledge_type": "explicit procedural steps and theoretical principles (unsupervised embedding training procedure and conditional objective)",
            "uuid": "e387.0",
            "source_info": {
                "paper_title": "Towards Controlled Table-to-Text Generation with Scientific Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PLM transfer",
            "name_full": "Application of general-domain pre-trained language models (BART, T5, GPT-3.5) to scientific table-to-text generation",
            "brief_description": "Uses widely used pre-trained language models (BART-base, T5-small, and GPT-3.5) as generators for producing scientific descriptions from tables, testing how models pretrained on open-domain corpora perform in a scientific, controlled generation task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Pre-trained Language Model (PLM) application to scientific text generation",
            "procedure_description": "Employs BART-base and T5-small as seq2seq generators fine-tuned (or used) to map inputs [H : T : B] (highlighted cells, linearized table, retrieved domain knowledge) to target descriptions; also evaluates direct generation from GPT-3.5 as a baseline. Training objective for finetuned generators is cross-entropy minimization over reference descriptions using encoder representations of the inputs.",
            "procedure_type": "computational method / model transfer / fine-tuning",
            "source_domain": "open-domain pretraining / general NLP",
            "target_domain": "scientific table-to-text natural language generation",
            "transfer_type": "adapted/modified for new context (fine-tuning and conditioning with retrieved domain knowledge)",
            "modifications_made": "Conditioned the generators on tabular inputs and retrieved domain-specific sentences plus highlighted cells; in CTRLSciTabNet the generator uses retrieved B alongside T and H. The paper evaluates both models with and without domain-specific knowledge (W/o BKG).",
            "transfer_success": "mixed — BART and T5 achieved reasonable performance when combined with retrieved domain knowledge (CTRLSciTabNet (Bart) BLEU = 16.9; CTRLSciTabNet (T5) BLEU = 6.6). Models without domain-specific knowledge had substantially worse performance (examples: T5 w/o BKG BLEU = 2.1; DG w/o BKG GPT-3.5 BLEU = 3.07), and GPT-3.5 generally underperformed on this domain-specific reasoning task. Human evaluation showed improvements when domain knowledge was present but persistent fidelity and recall shortcomings remain.",
            "barriers_encountered": "Pretraining on open-domain text leaves gaps in domain expertise causing hallucinations and low faithfulness; popular large models (GPT-3.5) struggled on domain-specific scientific reasoning; automatic metrics inadequately capture controlled alignment to highlighted cells.",
            "facilitating_factors": "Access to a curated domain-specific knowledge base (CTRLSciTab) and highlighted cells as control signals improved performance; finetuning and retrieval-augmented inputs helped adapt PLMs.",
            "contextual_requirements": "Domain-specific knowledge corpus, highlighted cell annotations, finetuning data (CTRLSciTab labeled pairs) and human evaluation to measure faithfulness were necessary for effective transfer.",
            "generalizability": "Applicable to other scientific domains but requires domain-specific corpora and careful retrieval/fine-tuning; success depends on the PLM size, available fine-tuning data, and quality of retrieved knowledge.",
            "knowledge_type": "explicit procedural steps (model fine-tuning and conditioning), representational/model knowledge (weights trained on domain data)",
            "uuid": "e387.1",
            "source_info": {
                "paper_title": "Towards Controlled Table-to-Text Generation with Scientific Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "TF-IDF baseline",
            "name_full": "TF-IDF-based retrieval for domain-specific sentence selection",
            "brief_description": "A classical information-retrieval method (TF-IDF) used as a baseline retriever to fetch domain-specific sentences relevant to a table for downstream generation.",
            "citation_title": "An information-theoretic perspective of tf-idf measures",
            "mention_or_use": "use",
            "procedure_name": "TF-IDF retrieval for domain-specific knowledge",
            "procedure_description": "Computes TF-IDF vectors for candidate domain-specific sentences and table/query text and returns top-ranked sentences by similarity as external knowledge to feed the generator. Used without additional conditioning or embedding learning.",
            "procedure_type": "computational method / retrieval technique",
            "source_domain": "information retrieval / general NLP",
            "target_domain": "scientific domain-specific sentence retrieval for table-to-text generation",
            "transfer_type": "direct application without modification",
            "modifications_made": "None beyond standard application to the parsed domain-specific sentence pool; used as a baseline in the retrieval-generator pipeline.",
            "transfer_success": "limited — TF-IDF + Direct Generation (Bart) yielded BLEU = 1.6 (Table 2), which was outperformed by the adapted retriever (Retriever + DG (Bart) BLEU = 2.0) and by the full CTRLSciTabNet system (CTRLSciTabNet (Bart) BLEU = 16.9).",
            "barriers_encountered": "Lacks deep semantic understanding required for scientific reasoning and precise mapping from table semantics to sentences; retrieval quality insufficient for faithful scientific description generation.",
            "facilitating_factors": "Simplicity and low computational cost make TF-IDF a viable baseline; easy to implement against extracted sentence pools.",
            "contextual_requirements": "Requires a pre-extracted candidate sentence pool aligned to documents/tables; no specialized equipment but needs pre-processing of documents to text.",
            "generalizability": "Directly applicable as a baseline across domains but unlikely to be sufficient for high-fidelity scientific table-to-text tasks without semantic or embedding-based enhancements.",
            "knowledge_type": "explicit procedural steps (classic IR algorithm application)",
            "uuid": "e387.2",
            "source_info": {
                "paper_title": "Towards Controlled Table-to-Text Generation with Scientific Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PDF-&gt;XML pipeline",
            "name_full": "PDF parsing and greedy content-matching pipeline for mining domain-specific knowledge",
            "brief_description": "A pipeline that parses arXiv PDF articles to XML, uses a greedy content-matching algorithm to align parsed sentences with tables, deduplicates matches, performs entity detection, and applies expert verification to produce table-sentence pairs and a domain-specific knowledge base.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "PDF parsing and greedy content matching for dataset construction",
            "procedure_description": "Transforms arXiv articles into XML, runs a greedy matching algorithm to align sentences to tables based on parsed content and table metadata, deduplicates overlapping text, detects entities referenced in descriptions, and uses expert annotators to correct irrelevant highlights and refine domain-specific knowledge, producing controlled table-description pairs with highlighted cells and knowledge sentences.",
            "procedure_type": "data extraction and annotation pipeline / dataset construction protocol",
            "source_domain": "document parsing and information extraction (document engineering / NLP preprocessing)",
            "target_domain": "scientific dataset construction for table-to-text generation",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Adapted standard PDF parsing and content-matching techniques to (1) preserve entity-referencing sentences, (2) deduplicate with table descriptions, and (3) generate highlighted-cell annotations; added an expert verification stage where annotators (computer science undergraduates) corrected automatically annotated data.",
            "transfer_success": "successful in producing a new dataset: CTRLSciTab with 8,967 controlled table-description pairs, average ~52 cells per table, ~34-word descriptions, highlighted cells covering ~20% of cells, and ~20 domain-specific knowledge sentences per table. Annotation agreement rates reported: 66.7% for highlighted cells and 70.6% for domain-specific knowledge, indicating moderate success but non-trivial noise.",
            "barriers_encountered": "Noisy automatic alignments requiring manual correction; moderate inter-annotator agreement indicates ambiguity; challenges parsing PDFs and reliably aligning free-text sentences to structured table cells.",
            "facilitating_factors": "Availability of arXiv metadata and documents, and the ability to run automated parsing and greedy matching provided a scalable starting point; human annotators enabled quality improvement.",
            "contextual_requirements": "Access to original PDF/arXiv sources, XML conversion tools, entity-detection tooling, domain-aware annotators for verification, and compute infrastructure for parsing large corpora.",
            "generalizability": "Approach is generalizable to other scientific domains with accessible PDFs and table metadata, but dependent on quality of PDF to XML conversion and available annotator expertise.",
            "knowledge_type": "tacit/practical know-how (document processing and annotation workflow) and explicit procedural steps (pipeline stages)",
            "uuid": "e387.3",
            "source_info": {
                "paper_title": "Towards Controlled Table-to-Text Generation with Scientific Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CTRLSciTabNet (RAG-like)",
            "name_full": "CTRLSciTabNet retriever-generator framework (retrieval-augmented generation adapted for controlled scientific table-to-text)",
            "brief_description": "A retrieval-then-generation architecture that first retrieves top-n domain-relevant sentences and then conditions a pre-trained seq2seq generator (BART/T5) on the retrieved sentences plus table and highlighted cells to produce controlled scientific descriptions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Retrieval-augmented generation for controlled scientific table-to-text (CTRLSciTabNet)",
            "procedure_description": "Two-step pipeline: (1) an unsupervised conditional retriever selects the most relevant domain-specific sentences for a given table and highlighted cells; (2) a generator (BART-base or T5-small) consumes linearized table T, highlighted cells H, and retrieved domain knowledge B to generate an analytical description aligned to user preferences. Training minimizes cross-entropy loss of generator outputs conditioned on encoder representations of [H : T : B].",
            "procedure_type": "computational method / hybrid retrieval-generation architecture",
            "source_domain": "retrieval-augmented generation methods in open-domain NLP",
            "target_domain": "controlled scientific table-to-text generation",
            "transfer_type": "adapted/modified for new context (incorporates highlighted cells as control and domain-specific KB)",
            "modifications_made": "Incorporated highlighted cells H as explicit control signals in both retrieval and generation steps; constrained retriever embeddings with table context; limited retrieved set to top-3 sentences; tailored generator inputs to [H : T : B] and fine-tuned on CTRLSciTab data.",
            "transfer_success": "successful relative to baselines — CTRLSciTabNet (Bart) substantially outperformed baseline pipelines on automatic metrics (BLEU = 16.9 vs TF-IDF+DG (Bart) BLEU = 1.6) and human evaluations (improvements in fluency, faithfulness, recall, and valid facts), though fidelity and recall of highlighted cells remain imperfect.",
            "barriers_encountered": "Persistent hallucinations and incorrect facts even with retrieved knowledge; automatic metrics insufficient to fully capture controlled alignment; models still failed to fully capture highlighted content (low recall).",
            "facilitating_factors": "Explicit control signals (highlighted cells) and an available domain-specific KB; combining retrieval with pre-trained generators allowed leveraging both external knowledge and generation fluency.",
            "contextual_requirements": "Domain-specific knowledge sentences, highlighted-cell annotations, computation for retrieval/generation training, and human evaluation to assess faithfulness.",
            "generalizability": "Approach is likely generalizable to other controlled-data-to-text tasks but requires domain-specific knowledge corpora and domain-aware control signals (e.g., highlighted cells) for effective transfer.",
            "knowledge_type": "explicit procedural steps (architecture and training protocol) and interpretive frameworks (how to condition retrieval/generation on user preferences)",
            "uuid": "e387.4",
            "source_info": {
                "paper_title": "Towards Controlled Table-to-Text Generation with Scientific Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tsdae: Using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning.",
            "rating": 2,
            "sanitized_title": "tsdae_using_transformerbased_sequential_denoising_autoencoderfor_unsupervised_sentence_embedding_learning"
        },
        {
            "paper_title": "Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension.",
            "rating": 2,
            "sanitized_title": "denoising_sequencetosequence_pretraining_for_natural_language_generation_translation_and_comprehension"
        },
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
            "rating": 2,
            "sanitized_title": "exploring_the_limits_of_transfer_learning_with_a_unified_texttotext_transformer"
        },
        {
            "paper_title": "An information-theoretic perspective of tf-idf measures",
            "rating": 2,
            "sanitized_title": "an_informationtheoretic_perspective_of_tfidf_measures"
        },
        {
            "paper_title": "Scigen: a dataset for reasoning-aware text generation from scientific tables.",
            "rating": 1,
            "sanitized_title": "scigen_a_dataset_for_reasoningaware_text_generation_from_scientific_tables"
        },
        {
            "paper_title": "Towards table-to-text generation with numerical reasoning.",
            "rating": 1,
            "sanitized_title": "towards_tabletotext_generation_with_numerical_reasoning"
        },
        {
            "paper_title": "Totto: A controlled table-to-text generation dataset.",
            "rating": 1,
            "sanitized_title": "totto_a_controlled_tabletotext_generation_dataset"
        }
    ],
    "cost": 0.01488125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TOWARDS CONTROLLED TABLE-TO-TEXT GENERATION WITH SCIENTIFIC REASONING
8 Dec 2023</p>
<p>Zhixin Guo 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Jianping Zhou 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Jiexing Qi 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Mingxuan Yan 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Ziwei He 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Xinbing Wang 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Chenghu Zhou 
IGSNRR
Chinese Academy of Sciences
BeijingChina</p>
<p>TOWARDS CONTROLLED TABLE-TO-TEXT GENERATION WITH SCIENTIFIC REASONING
8 Dec 20231C0965B68996A4F9A346FA19AE1C929AarXiv:2312.05402v1[cs.CL]Table-to-text GenerationScientific ReasoningControlled Generation
The sheer volume of scientific experimental results and complex technical statements, often presented in tabular formats, presents a formidable barrier to individuals acquiring preferred information.The realms of scientific reasoning and content generation that adhere to user preferences encounter distinct challenges.In this work, we present a new task for generating fluent and logical descriptions that match user preferences over scientific tabular data, aiming to automate scientific document analysis.To facilitate research in this direction, we construct a new challenging dataset CTRLSciTab consisting of table-description pairs extracted from the scientific literature, with highlighted cells and corresponding domain-specific knowledge base.We evaluated popular pre-trained language models to establish a baseline and proposed a novel architecture outperforming competing approaches.The results showed that large models struggle to produce accurate content that aligns with user preferences.As the first of its kind, our work should motivate further research in scientific domains.</p>
<p>INTRODUCTION</p>
<p>Table-to-text generation is a significant research problem involving the generation of analytical descriptions from tabular data.Recently, pre-trained language model (PLM)-based approaches have demonstrated impressive improvements in text fluency and fidelity compared to traditional template-based methods, as seen in the popular table-to-text challenges, WikiBio [1], LogicNLG [2], RottoWire [3], and ToTTo [4].However, their success is highly dependent on pretraining using a large corpus of open-domain text, such as the two billion words of Wikipedia [5].</p>
<p>Scientific domains that require advanced expertise present a significant challenge for PLM-based NLG systems in generating descriptions with scientific reasoning.Scientific reasoning is a cognitive process characterized by systematic, logical, and evidence-based reasoning to understand the natural world, develop hypotheses, design experiments, evaluate evidence, and draw conclusions based on empirical data and observations [7], [8], [9].In the context of scientific natural language processing (NLP), scientific reasoning represents a significant bottleneck, relying primarily on domain-specific knowledge tailored to specific scientific phenomena.</p>
<p>While existing challenges such as SciGen [10] and numeric-NLG [11] address table-to-text generation related to scientific content, they predominantly focus on numerical reasoning.We argue that scientific reasoning in the context of scientific NLP presents a unique and comprehensive challenge for table-to-text generation.Moreover, with the increasing use of PLM-based natural language  Fig. 1.An illustration of controlled table-to-text generation incorporating explicit scientific reasoning stages.(a) represents the input information, (b) illustrates the inherent reasoning processes of language models, and (c) displays the resultant descriptions.Yellow highlights user preferences; red relates to tabular knowledge, and blue indicates scientific reasoning content.Potential scientific reasoning steps are outlined at the bottom.The original table is adapted from [6]. generation (NLG) systems in real-world applications, the focus has shifted from the generation of generic content to the generation of customized content that is aligned with user preferences.However, the currently popular methods of table-to-text tasks may not consistently match user preferences due to the broad nature of the generated output.</p>
<p>In this work, we propose a new task of controlled table-to-text generation with scientific reasoning, which aims to generate analytical descriptions that are consistent with domain-specific knowledge and align with user preferences.To facilitate research on this task, we present CTRLSciTab, which consists of 8,967 scientific tabledescription pairs with an external domain-specific knowledge base and highlighted cells.CTRLSciTab represents a challenging tableto-text generation task with two unique features: (1) all pairs are from the scientific literature, requiring scientific reasoning, and (2) all descriptions are aligned with user preferences.</p>
<p>We conducted extensive experiments on popular PLM-based models, revealing their poor performance in scientific domains requiring advanced expertise, a persistent challenge for the NLP community.To address this, we propose a retrieval-based pre-trained model trained on our dataset, which shows a better performance.However, evaluations, both automatic and human, indicate that the generated sentences may sometimes contain incorrect and hallucinatory content, highlighting this data set's potential as a valuable</p>
<p>THE CTRLSCITAB DATASET</p>
<p>Data Preparation.CTRLSciTab, a broad dataset, consolidates table-to-text pairs from the [10] and [11] datasets, which originally obtained these pairs from scientific literature with a focus on numerical reasoning.To cater to controlled generation coupled with scientific reasoning, we embarked on a retrieval process for original scientific articles from arXiv.org using tabular metadata.Subsequently, we instituted a procedure to extract sentences associated with tables, thereby constructing a domain-specific knowledge base for scientific reasoning.We also highlighted cells mentioned within table descriptions, serving as prompts instructing PLMs to mirror user preferences.As depicted in Figure 2, the extraction we first transform articles into XML-encoded files.A greedy algorithm is then engaged for content matching, aligning parsed content with tables, and preserving entity-referencing sentences.Deduplication removes any overlap with table descriptions, and entity detection identifies pertinent entities within the descriptions.Annotation Procedure.Potential inaccuracies in the annotated data, such as irrelevant highlighted cells, are rectified to ensure quality.Expert annotators, who are computer science undergraduates, verify the automatically annotated data.The tasks involve refining domain-specific knowledge and annotating highlighted cells.Annotators eliminate sentences unrelated to tabular data or descriptions, retaining content explicitly stated or logically inferred from the data or descriptions.To quantify the annotation agreement, we randomly selected 100 samples, and two expert annotators achieved a 66.7% agreement rate on the highlighted cells and a 70.6% agreement rate on domain-specific knowledge, thereby underscoring the necessity of domain-specific knowledge.</p>
<p>Data Analysis.CTRLSciTab is a large data set that contains 8,967 controlled table-description pairs with rich domain-specific knowledge in various scientific domains.Each table in CTRLSc-iTab consists of 52 cells on average, with a description of 34 words and highlighted cells as controlled preferences which occupy about 20% of the cells.To support common sense generation, we construct an average of 20 domain-specific knowledge sentences per table.</p>
<p>In the context of  [12].Traditional generation datasets focused predominantly on a single phase, simplifying the overarching complexity of the task.ToTTo [4] was seminal in presenting a controlled table-to-text generation task with an emphasis on content selection.However, its design does not adequately address the needs of scientific scenarios that require expertise.Some recent datasets have proposed incorporating complex reasoning, such as numerical [11], [10] and logical reasoning [2], into the task by framing it as a summarization problem focused on surface realization.A deeper exploration of the chain of thought [13] in Large Language Models (LLMs) has led to remarkable progress in such generation tasks.CTRLSciTab uniquely challenges generation systems by imposing controlled constraints on content selection while requiring sophisticated scientific reasoning for surface realization.</p>
<p>TASK DEFINITION</p>
<p>This task aims to generate natural language descriptions that are both fluent and accurate, incorporating domain-specific knowledge while remaining consistent with the tabular data and user preferences.The input consists of structured data, highlighted cells, and domain-specific knowledge, denoted as D = (T, H, B).Here, T signifies a linearized table, with T = t1, • • • , t |n| .Each tabular data, ti, consists of an attribute-value pair, where ai and vi can take values such as strings, numbers, phrases, or sentences.The highlighted cells are akin to ti and are denoted by H = h1, • • • , h |n| , acting as prompts reflecting user preferences.Furthermore, B = bi, • • • , b |m| represents domain-specific knowledge, with each bi corresponding to a sentence associated with the tabular data.The expected output is an analytical description aligned with user preferences and incorporating domain-specific knowledge R.</p>
<p>METHODS</p>
<p>The abundance of supporting domain-specific knowledge for each table, with an average of over 20 sentences, makes it challenging to leverage using popular PLMs.To address this, we propose a retriever-generator framework, CTRLSciTabNet, as our initial approach to the problem.The overall framework of our method is illustrated in Figure 3.The retriever retrieves supporting domainspecific knowledge based on the table contents, which is then used by the generator to produce descriptions according to the highlighted cells.</p>
<p>Retriever.In this study, as delineated in Figure 3 (a), we introduce an unsupervised sentence embedding technique under controlled conditions.Drawing inspiration from [14], our approach seeks to derive embeddings of domain-specific sentences represented by B through the process of reconstructing them from a perturbed denoted as B. Further refining the embedding process, we impose constraintsH and T , signifying user preferences and the original tabular data, respectively.The conditional distribution P ( B|B, T, H).The training goal of the retriever is defined by Equation 1:
JSDAE(θ) = Ex∼D[log P θ (x|x)] = Ex∼D[ L t=1 log P θ (x|x)] = E x∼[B:T :H] <a href="1"> l |B| +l |T | +l |H| t=1 log exp(h T t et) N i=1 exp(h T t ei) )</a>
where L = l |B| + l |T | + +l |H| denotes the total length of input tokens, including the highlighted cells, table contents, and domainspecific knowledge.</p>
<p>Generator.We aim to develop a sentence generator that can produce coherent and sensible descriptions based on tabular data and the highlighted cells while incorporating domain-specific knowledge according to user preferences.To achieve this goal, we employ BART-base [15] and T5-small [16] for the sentence generator.The learning objective of our sentence generator is to minimize the crossentropy loss.Specifically, given the tabular data T , highlighted cells H, domain-specific knowledge B, and corresponding table descriptions R, the learning objective is defined as Equation 2:
LLM = − |R| i=1 log PG(Ri|R&lt;i; E([H : T : B]))(2)
where E represents the encoder.</p>
<p>EXPERIMENTS</p>
<p>Baselines.To evaluate the effectiveness of our approach, we compare it to several popular strategies by replacing the retriever and sequence generator as baselines.Specifically, we consider the following methods: TF-IDF + Direct Generation (Bart-base).We implement TF-IDF [17], a commonly utilized retrieval method, to access domainspecific knowledge.The architectural design remains consistent with our framework, utilizing the Bart-base model without finetuning for the generative tasks.Retriever + Direct Generation (Bart-base).To assess the efficacy of our proposed retrieval method, we substitute the TF-IDF retriever with our approach, maintaining all other settings.</p>
<p>Retriever + Direct Generation (GPT-3.5).We conduct an additional investigation using our dataset to further explore the functionality of GPT-3.5 [18].</p>
<p>Automatic Evaluation.To evaluate the performance of our model, we use five automatic metrics: BLEU [7], METEOR [8], BertScore [9], and BLEURT [10].BLEU and METEOR are widely used metrics that measure the informativeness of generated text.BertScore and BLEURT are pre-trained metrics to measure the similarity between the generated descriptions and the reference sentence.Table 2 delivers a thorough assessment of all tested systems using metrics defined in §5.To evaluate the retrievers' performance, we maintain our model's architecture and retrieve the top-3 related domain-specific knowledge sentences using our proposed and TF-IDF-based retrievers.We then generate the corresponding descriptions directly from the retrieved sentences.Our proposed retriever outperforms the TF-IDF-based retriever based on the automatic evaluation metrics.Subsequently, we use the same retriever results as input for all generator models, which allows us to evaluate the performance of the generator part.It is worth noting that the currently popular large-scale GPT-3.5 still does not perform well on tasks that require domain-specific knowledge for scientific reasoning.</p>
<p>In addition, we provide a breakdown of the performance of using domain-specific knowledge.When we use only the tabular data as input, the performance for all PLMs (GPT-3.5,Bart-base, and T5small) significantly lags behind the results obtained when external domain-specific knowledge is incorporated.These results demon- Table 3. Performance of human evaluation result of CTRLSciTab-Net using the BART-base and T5-small models based on several metrics, including fluency, faithfulness, recall of covered highlighted cells, and valid factors.↑ denotes a higher score representing a better performance.The best models are bold, and the second best ones are underlined within each evaluation.W/o BKG denotes the model without the use of domain-specific knowledge.</p>
<p>strate the importance and necessity of domain-specific knowledge as expertise for scientific reasoning.Except for BertScore, the performance of all baseline systems is significantly poor in the remaining metrics.Our human evaluation in §5 reveals that the majority of the generated descriptions are of poor quality.Additionally, existing automatic evaluation metrics fail to evaluate the coherence of the highlighted cells and the generated descriptions for controlled table-to-text generation.Furthermore, CTRLSciTabNet, based on the Bart-base model, outperforms the other models on most of the automatic evaluation metrics.These results highlight the inadequacy of automatic evaluation metrics for this challenge.</p>
<p>Human Evaluation.Existing automatic metrics have difficulty assessing the coherence between highlighted cells and generated sentences, requiring human evaluation using four different criteria: fluency, fidelity, recall, and valid facts.Fluency, measured on a five-point Likert scale, assesses the grammatical correctness and naturalness of sentences.Faithfulness measures sentence accuracy against table data, with annotators determining the proportion of faithful content.Recall assesses sentence relevance to highlighted cells, with all sentence parts aligned with cells deemed relevant.Annotators determine the proportion of such facts within the generated descriptions.Finally, Valid Facts determines the proportion of unique and accurate content within generated sentences relative to reference sentences.</p>
<p>Table 3 shows the results of human evaluation, highlighting the superior performance of models using external domain-specific knowledge in terms of fluency, fidelity, recall, and valid facts.The data suggest that advanced domain knowledge is essential for producing scientifically sound results in the scientific domain.Our proposed method, CTRLSciTabNet (Bart-base w BKG), consistently outperforms all evaluated metrics, in line with the results of automatic evaluation metrics.</p>
<p>However, the poor fidelity score indicates a significant gap within the CTRLSciTab dataset that needs to be addressed.In addition, the poor recall of highlighted cells implies that the evaluated models fall short in performing controlled generation independent of scientific reasoning.Both automatic and human evaluation results support CTRLSciTab as a practical testbed for controlled table-totext generation with scientific reasoning.In particular, CTRLSc-iTabNet (Bart-base w BKG) shows the best performance across all metrics (sign test with a p-value &lt;0.05).</p>
<p>Case Study.knowledge outperforms its counterpart without such knowledge, in particular showing significant improvements in fluency and fidelity.</p>
<p>In contrast, the model without domain-specific knowledge tends to hallucinate and generate content that is inconsistent with the original tabular data.However, despite these improvements, both models show inadequate performance in capturing highlighted content.</p>
<p>CONCLUSION</p>
<p>We introduce a rigorous task, controlled table-to-text generation with scientific reasoning, to assess the machine's ability to produce analytical descriptions that satisfy domainspecific knowledge and user preferences.To support research in this area, we present a novel dataset, CTRLSciTab, which is characterized by two features: (1) its table-description pairs from the scientific literature require domain-specific knowledge for scientific reasoning, and (2) all descriptions are consistent with user preferences.To evaluate the effectiveness of CTRLSciTab, we employ different baselines and perform comprehensive automatic and human evaluations.Our study reveals key findings: (1) existing evaluation metrics inadequately measure controlled table-to-text generation with scientific reasoning; (2) the dominant GPT-3.5 model struggles with tasks requiring advanced expertise; (3) several challenges are associated with the CTRLSciTab dataset.Despite these issues, the initial results lay the groundwork for future research on pre-training tasks for complex, realistic domains.Overall, we position CTRLSciTab as a valuable asset to the research community and expect it to inspire further exploration in this area.</p>
<p>ACKNOWLEDGMENTS</p>
<p>ConvS2S,Fig. 3 .
3
Fig. 3.An illustration of the CTRLSciTabNet structure: (a) depicts the architecture of our unsupervised retriever; (b) outlines the two-step operation of CTRLSciTabNet, which involves a retriever selecting the top-n related domain-relevant sentences, followed by a pre-trained language model, the generator, utilizing this data alongside tabular inputs and highlighted cells.</p>
<p>Figure 4
4
shows an output generated by the Bartbase model, providing a deeper insight into how our framework works.The text colored in green indicates facts confirmed by tabular data, while the text colored in red indicates incorrect facts.These results suggest that the model trained with domain-specific</p>
<p>Fig. 4 .
4
Fig. 4. Case study of CTRLSciTabNet.Contents in yellow cells indicate the highlighted cells.W/o BKG denotes the model without the use of domain-specific knowledge.Green text indicates the correct statements supported by the tabular data, and red text indicates the incorrect statements.</p>
<p>Table Caption :
Caption
The Transformer achieves better BLEU scores …… at a fraction of the training cost.
Model ConvS2S EnsembleBLEU EN-DE EN-FR 41.29 26.36Domain-specific Knowledge: reducing sequential computation …… ConvS2S, all of which use convolutional neural networks …… Transformer uses multi-head attention……BLEUTransformer (base model)27.338.1stands for "Bilingual ……training costs to otherTransformer (big)28.441.0model architectures from the literature.Highlighted Cells: ConvS2S Ensemble,Transformer (big), 26.36, 28.4</p>
<p>Table Description :
Description
…..the big transformer model outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.
(a) Input(b) Languange Model ReasoningIntuitive Reasoning Steps:(1)A higher BLEU indicates better performance(2) Previous best model is ConvS2S Ensemble, with 26.36 BLEU(3) Transformer (big) achieves 28.4 BLEU(4) Compared to previous by more than 2.0 BLEU(c) Output</p>
<p>Mining domain-specific knowledge from PDF sources Aligning Highlighted Cells to table descriptions Expert Verification +dict. Domain-specific knowledge: Three types
arXivdocumentsThree types of task descriptions are constructedin the WOZ ……A rule-oftask descriptions are….. for collecting dialogdata. Rule-based tracking scheme is used toaccumulate the turn-level: ……</p>
<p>based tracking scheme…… Domain-specific knowledge: Three types of task descriptions are constructedin the WOZ experiment as the guidence for collecting dialog …… PDF Parsing Content Matching DeduplicatingTable 2 .
2
……
RNN, NBTModelTurn-levelJointRequest+dict., EDST, 87.5,+dict means …… and EDST on WOZ2.0.EDST91.6 92.885.2 87.595.2 95.3Highlight Cells 85.2</p>
<p>Table 2 .
2
Results for delexicalisation-based RNN, NBT and EDST on WOZ2.0.……</p>
<p>The Highlighted Cells:+dict., EDST, 87.5, 85.2 Expert Annotation Entity Detection +dict.
ModelTurn-levelJointRequest92.887.595.3EDST91.685.295.2Fig. 2. Overview of CTRLSciTab construction steps, including min-ing domain-specific knowledge from PDF source, aligning high-lighted cells to table descriptions, and expert verification.benchmark for evaluating controlled table-to-text generation withscientific reasoning.</p>
<p>Table 1 .
1
table-to-text generation datasets, CTRLSciTab introduces a distinct perspective on content selection and surface realization that distinguishes it from other existing datasets.Prior to Comparison of CTRLSciTab and previous table-to-text generation data sets.The pairs represent the number of annotated structure data in each data set.Cell denotes the average number of total cells.
Data setPairs CellDomainScientific Reasoning ControlledWikiBIO400K17Open✗✗ToTTo136K3Open✗✓LogicNLG37K91Open✗✗SciGen53K55Scientific✗✗numericNLG 1.3K46Scientific✗✗CTRLSciTab 9.0K52Scientific✓✓
the advent of neural approaches, generation systems typically separated content selection (what to say) from surface realization (how to say it)</p>
<p>Table 2 .
2
Automatic
BaselinesBLEU↑METEOR↑BertScore↑BLEURT↑TF-IDF + DG (Bart)1.600.090.75-0.93Retriever + DG (Bart)2.000.090.78-1.00Retriever + DG (GPT-3.5)4.760.200.84-0.51CTRLSciTabNet (Bart)16.900.340.87-0.32CTRLSciTabNet (T5)6.600.290.85-0.68DG w/o BKG (GPT-3.5)3.070.170.82-0.65Bart w/o BKG14.900.310.87-0.38T5 w/o BKG2.100.180.82-0.87
evaluation results of all methods.The symbol ↑ indicates that a higher score represents better performance.The best result is bold, and the second best is underlined within each metric.W/o BKG denotes the model without using domain-specific knowledge.DG denotes direct generate from the pre-trained model without fine-tuning</p>
<p>Table Caption :
Caption
Comparison of Accuracy (in%) in CN14</p>
<p>Table Description :
Description
The CN14 dataset is designed for answering commonsense questions like Is a camel capable of journeying across desert.The proposed NAM models answer this question by calculating the association probability Pr(E2|E1) where E1=\ {camel,capable of} and E2=journey across desert.We can see that both NAM methods outperform NTN in this task, and the DNN and RMNN models obtain similar performance.As shown in the table, the proposed NAM model significantly outperforms the NTN in both the positive and the negative measures.This results indicate the effectiveness of our proposed model.The ecosystem is in excellent The ecosystem is in excellent The ecosystem is in excellent The ecosystem is in excellent.
Total 84.6 85.7 CTRLSciTabNet (Bart-base): CTRLSciTabNet (Bart-base W/o BKG): Negative 86.5 86.0 87.1 86.1
We thank the anonymous reviewers for their thoughtful comments.This work was partially supported by National Key R&amp;D Program of China (No.2022YFB3904204), NSF China (No. 42050105, 62272301, 62020106005, 62061146002, 61960206002).This work is supported by the Deep-time Digital Earth (DDE) Big Science Program.
Table-to-text generation by structure-aware seq2seq. Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, Zhifang Sui, </p>
<p>Thirty-Second AAAI Conference on Artificial Intelligence. 2018</p>
<p>Logical natural language generation from open-domain tables. Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, William Yang, Wang , arXiv:2004.104042020arXiv preprint</p>
<p>Challenges in data-to-document generation. Sam Wiseman, Stuart M Shieber, Alexander M Rush, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017</p>
<p>Totto: A controlled table-to-text generation dataset. Xuezhi Ankur P Parikh, Sebastian Wang, Manaal Gehrmann, Bhuwan Faruqui, Diyi Dhingra, Dipanjan Yang, Das, arXiv:2004.143732020arXiv preprint</p>
<p>The wikipedia xml corpus. Ludovic Denoyer, Patrick Gallinari, ACM SIGIR Forum. New York, NY, USAACM200640</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>The generality/specificity of expertise in scientific reasoning. D Christian, Schunn, John R Anderson, Cognitive science. 2331999</p>
<p>Learning and scientific reasoning. Lei Bao, Tianfan Cai, Kathy Koenig, Kai Fang, Jing Han, Jing Wang, Qing Liu, Lin Ding, Lili Cui, Ying Luo, Science. 32359142009</p>
<p>The development of scientific reasoning skills. Corinne Zimmerman, Developmental review. 2012000</p>
<p>Scigen: a dataset for reasoning-aware text generation from scientific tables. Nafise Sadat Moosavi, Andreas Rücklé, Dan Roth, Iryna Gurevych, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Towards table-to-text generation with numerical reasoning. Lya Hulliyyatus Suadaa, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura, Hiroya Takamura, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Building applied natural language generation systems. Ehud Reiter, Robert Dale, Natural Language Engineering. 311997</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Tsdae: Using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning. Kexin Wang, Nils Reimers, Iryna Gurevych, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the. the 58th Annual Meeting of theBartAssociation for Computational Linguistics2020</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 211402020</p>
<p>An information-theoretic perspective of tf-idf measures q. Akiko Aizawa, Information Processing and Management. 200339</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>            </div>
        </div>

    </div>
</body>
</html>
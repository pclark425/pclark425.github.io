<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6570 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6570</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6570</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-271218853</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.11511v3.pdf" target="_blank">Multi-step Reasoning with Large Language Models, A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks. The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years. This article reviews the field of multi-step reasoning with LLMs. We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future. We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many studies in multi-step methods use reinforcement learning for finetuning, external optimization loops, in-context reinforcement learning, and self-reflection.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6570.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6570.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that treats LLM reasoning as a tree-search: the LLM is prompted to generate candidate 'thoughts' (substeps) and to evaluate them while an external search algorithm (breadth‑ or depth‑first, beam search, etc.) controls lookahead and backtracking to solve combinatorial puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Game of 24</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>combinatorial puzzle / deliberative problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>LLM generation + evaluation prompts (Tree-of-thoughts style) with external tree-search control</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>External tree search (breadth-first or depth-first) with LLM-generated candidate steps and LLM-based evaluation; supports lookahead and backtracking</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>External search controller (tree-search algorithm) that issues prompts to the LLM to (1) generate candidate thoughts and (2) evaluate them; manages state, branching, and backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy / solve rate (task-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>74% accuracy on a Game of 24 task (reported in the referenced Tree-of-thoughts work, as summarized in this survey)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Tree-of-thoughts enables lookahead and backtracking by combining LLM thought-generation and evaluation with an external searcher; systematically explores reasoning trajectories and succeeds on combinatorial puzzle instances where greedy chain-of-thought fails.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Compared against Chain-of-thought and Self-consistency baselines (survey reports Tree-of-thoughts achieving 74% on Game of 24); exact baseline numbers are not enumerated in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires an external controller and many LLM calls (computational cost); context/state must be explicitly managed outside the model; some tasks (creative writing) were evaluated qualitatively rather than with numeric benchmarks; survey notes that LLMs alone (without external state management) often fail to keep enumeration state correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-step Reasoning with Large Language Models, A Survey', 'publication_date_yy_mm': '2025-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6570.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6570.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Buffer-of-thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Buffer of thoughts: Thought-augmented reasoning with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metacognitive prompting scaffold that maintains a meta-buffer of distilled high‑level thought templates (problem distiller) which are instantiated to guide LLM reasoning and to manage limited context capacity, improving performance on puzzles requiring structured, higher‑level planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Buffer of thoughts: Thought-augmented reasoning with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Game of 24; checkmating (chess checkmate puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>combinatorial puzzles / board-game (checkmating is spatial/combinatorial)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Meta-buffer instantiation + hand-crafted prompt templates; prompts augmented by high-level thought-templates from a problem distiller</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Use of a meta-buffer of thought-templates that are instantiated into prompts for the LLM; external management of buffer capacity and template selection to guide multi-step reasoning/search</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>High-level thought-templates (meta-buffer) distilled from prior problems and instantiated per task</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Problem distiller and meta-buffer manager (external module) that stores, retrieves and manages high-level thought-templates and composes prompts for the LLM; used to mitigate context-window limits and provide metacognitive guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>relative improvement over baselines (percentage points)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>+11% on Game of 24 and +51% on checkmating compared to other methods (reported improvements summarized in the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Meta-buffered templates help reduce context limitations and provide higher-level guidance (metacognitive support), yielding substantial gains on puzzles that benefit from structured templates and state management.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Reported to outperform other methods (e.g., Chain-of-thought, other prompt-only baselines) by the cited percentage gains; precise experimental ablation details and base numbers are not enumerated in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance depends on the quality of the distilled thought-templates and buffer management; selection and distillation process can be sensitive; may require external distiller training and careful capacity management; some tasks still evaluated qualitatively in parts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-step Reasoning with Large Language Models, A Survey', 'publication_date_yy_mm': '2025-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6570.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6570.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that iteratively prompts an LLM to generate code/skills for embodied control in Minecraft, uses self-verification and a skill library to accelerate exploration and discovery of in-game tools and capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Minecraft exploration / tool-acquisition tasks</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>embodied spatial game / open-ended exploration</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Iterative prompting that generates executable code/scripts for the environment; includes self-verification and a growing skill library</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Code-generation for action sequences, self-verification of generated solutions, and a stored skill library enabling novelty search and curriculum-like improvement</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Skill library (catalog of reusable code/skills) plus generated action-code scripts</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Generated code is executed in the Minecraft environment (simulator/emulator) to realize actions; environment provides feedback used for self-verification and to update skill library.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>speed / efficiency of exploration and tool acquisition (comparative runtime/throughput)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to reach high scores by acquiring many tools 15× faster than the baseline (as summarized in the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Iterative code-generation plus a skill library and self-verification substantially accelerate exploration and tool acquisition in a spatial, embodied environment; demonstrates LLMs acting as code-producing agents for spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Survey summarizes that Voyager significantly outperforms a baseline agent in speed of acquiring tools (15× faster); detailed ablations not provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on an external environment/simulator and execution of generated code (not pure in‑context reasoning); results may be environment-specific (Minecraft) and may not generalize to other spatial puzzles without significant engineering; requires mechanisms to verify and correct executed code/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-step Reasoning with Large Language Models, A Survey', 'publication_date_yy_mm': '2025-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Buffer of thoughts: Thought-augmented reasoning with large language models <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Game of thoughts: Iterative reasoning in game-theoretic domains with large language models <em>(Rating: 2)</em></li>
                <li>Mastering board games by external and internal planning with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6570",
    "paper_id": "paper-271218853",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "Tree-of-thoughts",
            "name_full": "Tree of thoughts: Deliberate problem solving with large language models",
            "brief_description": "An approach that treats LLM reasoning as a tree-search: the LLM is prompted to generate candidate 'thoughts' (substeps) and to evaluate them while an external search algorithm (breadth‑ or depth‑first, beam search, etc.) controls lookahead and backtracking to solve combinatorial puzzles.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "puzzle_name": "Game of 24",
            "puzzle_type": "combinatorial puzzle / deliberative problem solving",
            "dataset_name": null,
            "prompting_method": "LLM generation + evaluation prompts (Tree-of-thoughts style) with external tree-search control",
            "reasoning_technique": "External tree search (breadth-first or depth-first) with LLM-generated candidate steps and LLM-based evaluation; supports lookahead and backtracking",
            "internal_representation": null,
            "use_of_external_tool": true,
            "external_tool_description": "External search controller (tree-search algorithm) that issues prompts to the LLM to (1) generate candidate thoughts and (2) evaluate them; manages state, branching, and backtracking.",
            "evaluation_metric": "accuracy / solve rate (task-dependent)",
            "performance": "74% accuracy on a Game of 24 task (reported in the referenced Tree-of-thoughts work, as summarized in this survey)",
            "analysis_findings": "Tree-of-thoughts enables lookahead and backtracking by combining LLM thought-generation and evaluation with an external searcher; systematically explores reasoning trajectories and succeeds on combinatorial puzzle instances where greedy chain-of-thought fails.",
            "ablation_comparison": "Compared against Chain-of-thought and Self-consistency baselines (survey reports Tree-of-thoughts achieving 74% on Game of 24); exact baseline numbers are not enumerated in the survey text.",
            "limitations": "Requires an external controller and many LLM calls (computational cost); context/state must be explicitly managed outside the model; some tasks (creative writing) were evaluated qualitatively rather than with numeric benchmarks; survey notes that LLMs alone (without external state management) often fail to keep enumeration state correctly.",
            "uuid": "e6570.0",
            "source_info": {
                "paper_title": "Multi-step Reasoning with Large Language Models, A Survey",
                "publication_date_yy_mm": "2025-11"
            }
        },
        {
            "name_short": "Buffer-of-thoughts",
            "name_full": "Buffer of thoughts: Thought-augmented reasoning with large language models",
            "brief_description": "A metacognitive prompting scaffold that maintains a meta-buffer of distilled high‑level thought templates (problem distiller) which are instantiated to guide LLM reasoning and to manage limited context capacity, improving performance on puzzles requiring structured, higher‑level planning.",
            "citation_title": "Buffer of thoughts: Thought-augmented reasoning with large language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "puzzle_name": "Game of 24; checkmating (chess checkmate puzzles)",
            "puzzle_type": "combinatorial puzzles / board-game (checkmating is spatial/combinatorial)",
            "dataset_name": null,
            "prompting_method": "Meta-buffer instantiation + hand-crafted prompt templates; prompts augmented by high-level thought-templates from a problem distiller",
            "reasoning_technique": "Use of a meta-buffer of thought-templates that are instantiated into prompts for the LLM; external management of buffer capacity and template selection to guide multi-step reasoning/search",
            "internal_representation": "High-level thought-templates (meta-buffer) distilled from prior problems and instantiated per task",
            "use_of_external_tool": true,
            "external_tool_description": "Problem distiller and meta-buffer manager (external module) that stores, retrieves and manages high-level thought-templates and composes prompts for the LLM; used to mitigate context-window limits and provide metacognitive guidance.",
            "evaluation_metric": "relative improvement over baselines (percentage points)",
            "performance": "+11% on Game of 24 and +51% on checkmating compared to other methods (reported improvements summarized in the survey)",
            "analysis_findings": "Meta-buffered templates help reduce context limitations and provide higher-level guidance (metacognitive support), yielding substantial gains on puzzles that benefit from structured templates and state management.",
            "ablation_comparison": "Reported to outperform other methods (e.g., Chain-of-thought, other prompt-only baselines) by the cited percentage gains; precise experimental ablation details and base numbers are not enumerated in the survey summary.",
            "limitations": "Performance depends on the quality of the distilled thought-templates and buffer management; selection and distillation process can be sensitive; may require external distiller training and careful capacity management; some tasks still evaluated qualitatively in parts.",
            "uuid": "e6570.1",
            "source_info": {
                "paper_title": "Multi-step Reasoning with Large Language Models, A Survey",
                "publication_date_yy_mm": "2025-11"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "An agent that iteratively prompts an LLM to generate code/skills for embodied control in Minecraft, uses self-verification and a skill library to accelerate exploration and discovery of in-game tools and capabilities.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "puzzle_name": "Minecraft exploration / tool-acquisition tasks",
            "puzzle_type": "embodied spatial game / open-ended exploration",
            "dataset_name": null,
            "prompting_method": "Iterative prompting that generates executable code/scripts for the environment; includes self-verification and a growing skill library",
            "reasoning_technique": "Code-generation for action sequences, self-verification of generated solutions, and a stored skill library enabling novelty search and curriculum-like improvement",
            "internal_representation": "Skill library (catalog of reusable code/skills) plus generated action-code scripts",
            "use_of_external_tool": true,
            "external_tool_description": "Generated code is executed in the Minecraft environment (simulator/emulator) to realize actions; environment provides feedback used for self-verification and to update skill library.",
            "evaluation_metric": "speed / efficiency of exploration and tool acquisition (comparative runtime/throughput)",
            "performance": "Reported to reach high scores by acquiring many tools 15× faster than the baseline (as summarized in the survey)",
            "analysis_findings": "Iterative code-generation plus a skill library and self-verification substantially accelerate exploration and tool acquisition in a spatial, embodied environment; demonstrates LLMs acting as code-producing agents for spatial tasks.",
            "ablation_comparison": "Survey summarizes that Voyager significantly outperforms a baseline agent in speed of acquiring tools (15× faster); detailed ablations not provided in the survey text.",
            "limitations": "Relies on an external environment/simulator and execution of generated code (not pure in‑context reasoning); results may be environment-specific (Minecraft) and may not generalize to other spatial puzzles without significant engineering; requires mechanisms to verify and correct executed code/actions.",
            "uuid": "e6570.2",
            "source_info": {
                "paper_title": "Multi-step Reasoning with Large Language Models, A Survey",
                "publication_date_yy_mm": "2025-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Buffer of thoughts: Thought-augmented reasoning with large language models",
            "rating": 2,
            "sanitized_title": "buffer_of_thoughts_thoughtaugmented_reasoning_with_large_language_models"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Game of thoughts: Iterative reasoning in game-theoretic domains with large language models",
            "rating": 2,
            "sanitized_title": "game_of_thoughts_iterative_reasoning_in_gametheoretic_domains_with_large_language_models"
        },
        {
            "paper_title": "Mastering board games by external and internal planning with language models",
            "rating": 1,
            "sanitized_title": "mastering_board_games_by_external_and_internal_planning_with_language_models"
        }
    ],
    "cost": 0.0195315,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multi-Step Reasoning with Large Language Models, a Survey
2 Nov 2025</p>
<p>Aske Plaat 
Annie Wong 
Suzan Verberne 
Joost Broekens 
Niki Van Stein 
Thomas Bäck </p>
<p>Aske Plaat
Annie Wong</p>
<p>Suzan Verberne
Joost Broekens
Niki van Stein
Thomas Bäck
LIACS
Leiden University
Netherlands</p>
<p>Multi-Step Reasoning with Large Language Models, a Survey
2 Nov 20256E8479DB310ECEF6024DB243FD8F1D6210.1145/nnnnnnn.nnnnnnnarXiv:2407.11511v3[cs.AI]Manuscript submitted to ACM Manuscript submitted to ACM Manuscript submitted to ACM
Large language models (LLMs) with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for.Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks.However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks.The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years.This article reviews the field of multi-step reasoning with LLMs.We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning.We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future.We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools.Many studies in multi-step methods use reinforcement learning for finetuning, external optimization loops, in-context reinforcement learning, and self-reflection.CCS Concepts: • Computing methodologies → Natural language processing.</p>
<p>Introduction</p>
<p>Transformer-based Large Language Models (LLMs) that are trained on large datasets have achieved breakthrough performance at text generation tasks that directly build on next token prediction [121,160,172]; they are good at natural language understanding (GLUE, SQUAD, Xsum) [104,125,163,164], translation [74,111,136], question answering [150], and other language generation tasks.The success of models such as ChatGPT [107] is impressive.</p>
<p>Transformer-based generative language models whose size is beyond hundreds of billions parameters are not only good at language generation, they also enable a new type of machine learning, called in-context learning [16].In-context learning, also known as prompt-based learning, is an emergent ability that occurs in LLMs beyond a certain size (hundreds of billions of parameters-less, with judicious prompting) that have been finetuned for conversational responses [172].In-context learning is inference-time, prompt-based, few-shot learning with instructions.As opposed to finetuning, model parameters are not adapted by in-context learning.</p>
<p>Scope</p>
<p>The question whether LLMs can reason prompted the development of the GSM8k benchmark [31] in 2021.GSM8k is a benchmark of 8500 easy reasoning problems (of the type: "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.How many clips did Natalia sell altogether in April and May? Answer: 72").This benchmark was specifically designed to test if LLMs can answer basic mathematical multi-step reasoning questions, or if they can not reason at all.Initially, performance was poor, but a year later Wei et al. [173] showed that by providing multi-step examples in the prompt-a chain of thought-much better performance was possible.Many works followed, and reasoning in language models has become an active area of study.In this survey, we focus on prompt-based multi-step reasoning algorithms.After starting with math word problems, work has more recently been extended to combinatorial games, puzzles, robotics, logic, and other fields of reasoning.At the end of this survey, we discuss connections to other fields, such as self-reflection and in-context reinforcement learning.</p>
<p>Before we discuss the works on reasoning, this section reviews background terminology on LLMs.Our overview is brief and we focus on the multi-step reasoning approaches that were inspired by Chain-of-thought.Excellent related surveys exist.For a longer overview of general LLM topics, see, for example, the surveys Minaee et al. [99] and Zhao et al. [194].For surveys that focus on the nature of reasoning and its definition, see [27,59,189].For surveys on evaluating logical reasoning in LLMs, see [91,102,109].For a survey on reinforcement learning in reasoning, see [182],</p>
<p>for reasoning in language see [189], and for a survey on Chain-of-thought itself, see [28].</p>
<p>The papers for this survey were selected as follows.We started by selecting papers on the ability to solve math word benchmarks (as a proxy for reasoning ability), that contained the search terms reasoning and large language model in their title or abstract, with a focus on papers that reference the Chain-of-thought paper.Although multi-step reasoning with LLMs initially was aimed at solving math world problems, it is now wider, including benchmarks and approaches for computer code, game play, puzzles, robot movement, and webpage navigation (see Table 2).We selected recent papers (two years prior to the writing of the survey) that show experimental results on selected benchmark datasets.</p>
<p>We focus on prompt-based, in-context learning, methods based on Chain-of-thought, that are used in reasoning LLMs such as OpenAI o1 and o3 [62,177,182].We include a few papers that work with external algorithms, finetuning, or supervised learning, that have contributed significantly to the in-context approaches.</p>
<p>We discuss the generic training pipeline for LLMs, we discuss how in-context learning works, and we discuss commonly used benchmarks.We start with the generic language model training pipeline.</p>
<p>Language Model Training Pipeline</p>
<p>LLMs are typically constructed in a sequence of stages, from data preparation, through training, to inference.The training pipeline for most LLMs is quite elaborate.We will now list a brief pipeline of the most commonly used stages, based on the survey by Minaee et al. [99].</p>
<p>In training an LLM, the first stage is to acquire a large, general, unlabeled, high-quality text corpus.Some considerations on the selection of the texts are discussed in Brown et al. [16].The next stage is pretraining the transformer model [160] on this large corpus.This stage yields a generative language model.Pretraining is done using self-supervised autoregressive training on the unlabeled dataset (text corpus).Then the general model is finetuned to a specific (narrow) task, for example, question answering.This can be done using supervised learning with a new labeled dataset consisting of prompts and answers (supervised finetuning, SFT), for example with low-rank optimization [57,99,172],</p>
<p>or reinforcement learning [26,147].A specific form of finetuning is instruction tuning, to improve instruction following Manuscript submitted to ACM for a certain task.Instruction tuning is supervised by a labeled dataset of instruction prompts and corresponding outputs.</p>
<p>Depending on the purpose of the model the next step is alignment of the finetuned model with user expectations (preference alignment).Alignment is usually performed to ensure that the model produces more ethically and socially acceptable answers, preventing, for example, hate speech.A machine learning method that is commonly used in this stage is Reinforcement Learning with Human Feedback [107], Direct Preference Optimization [123], or Reinforcement Learning with Verifiable Rewards [79].Optionally, model training can be optimized to improve cost-effectiveness, for example, mixed precision training [98], quantization [65], or knowledge distillation [49,184].</p>
<p>Once the model has been trained in the steps described above, it can be used further in the inference stage.Here, the model is used to provide an answer to the prompt.The inference-time stage is post-training, no model parameters are changed anymore [16,37]; in-context learning, or prompt-learning, takes place in this stage.</p>
<p>In-context learning has a low barrier of entry.Since there is no need for expensive pretraining or finetuning, this form of advanced prompt engineering has become a popular training method.This is the stage on which most of the surveyed papers focus, using prompts for the LLM to perform a complex multi-step reasoning task.The following section provides a brief introduction to in-context learning.</p>
<p>In-Context Learning</p>
<p>In large models, beyond hundreds of billions of parameters, a new kind of machine learning has emerged, that has been called in-context learning or prompt-learning [16].It occurs not when the model's parameters are trained, but when the model is used, at inference time.Since no parameters are changed at this stage, it is not a model training stage; in-context-learning "learns" inside the context, or prompt, using information that is already encoded in the trained model parameters and the prompt, not by training the model anymore.In-context learning is often able to give good results with few examples, so-called few-shot learning, learning from the few examples in the prompt in combination with the knowledge of the model.The large size of the model, containing rich and general knowledge, is enabling the few-shot learning (see Dong et al. [37] for a survey).</p>
<p>In in-context learning, a prompt, consisting of a piece of demonstration context, is concatenated with a query question, and is given to the language decoder model, for text generation [92].For example, when the task is emotion recognition in a social media post, "I missed the bus today, " can be followed by "I felt so [<strong><em>]", and the model could answer with "bad".Alternatively, for translation, we could follow "I missed the bus today, " by "French: [</em></strong>]" to request a translation [92].The prompt contains background information that is recognized by the model, selecting the desired model context.</p>
<p>In-context learning works when language models contain enough knowledge, allowing them to generalize on the (few) examples provided in the prompt.achieve generalization, and are computationally expensive (see, for example, [42] or [56,63] for a survey).In-context learning, in comparison, is computationally cheap, and has become a popular research approach.</p>
<p>Prompts provide a user-friendly interface to LLMs.The success of in-context learning tends to be quite sensitive to the way in which a prompt is formulated; a new field called prompt engineering has emerged to help human users to learn how to make LLMs do what they want them to do [47,121,131,172].The current survey thus discusses advanced prompt engineering methods.</p>
<p>Multi-step Reasoning Benchmarks</p>
<p>Progress in artificial intelligence is measured by benchmarks.Benchmarks define the goal that researchers aim to achieve in their experiments.In natural language processing, a wide array of benchmarks exist to measure progress, such as on question answering (for example, CommonsenseQA [149]), word prediction (for example, LAMBADA [110]), translation (for example, WMT'22 [74]), language understanding (for example, GLUE [163,164]), and text summarization (for example, Xsum [104]).</p>
<p>The field of LLMs is quite active.We will mention relevant benchmarks for testing the multi-step reasoning abilities of LLMs.The research on reasoning with LLMs started with math word problem benchmarks.The benchmark that is most frequently associated with multi-step reasoning with LLMs is the dataset of grade school math word problems GSM8K [31].GSM8K was created with the aim of providing high quality, high diversity, moderate difficulty, problems and solutions in natural language.It consists of 8500 human-written math problems.Language models struggled to achieve good performance on this dataset before Chain-of-thought was introduced.An example of a math word task follows.Problem: Beth bakes 4 trays with two dozen batches of cookies in a week.If these cookies are shared amongst 16 people equally, how many cookies does each person consume?Answer: 4 × 2 × 12/16 = 6.</p>
<p>Other benchmarks of similar math word problems are the SVAMP varying structures benchmarks [112], the ASDiv dataset of diverse math problems [97], the AQuA dataset of algebraic word problems [88], and the MAWPS benchmark [76].Table 1 summarizes the accuracy of Chain-of-thought on these basic math word problems, against the baseline of GPT-3 175B as LLM [173], as percentage of benchmark questions answered correctly.We see that Chain-of-thought performs well against the baseline of GPT-3 on some benchmarks, but there is certainly room for further improvement on others.</p>
<p>In addition to the initial set of math word benchmarks, further reasoning approaches have been introduced that test performance in other fields of reasoning.Benchmarks have been developed on Advanced mathematical questions [32],</p>
<p>Computer code comprehension (Human evaluation, Spider [190], Transcoder [128]), Robotic movement (Alfworld [142],</p>
<p>Kitchen [2]), Puzzle solving (Game24 [188]), Creative writing [188]), Gaming (Checkmate problems [185], MineCraft [40]), and Webpage navigation (WebShop [186]).These benchmarks are used by other approaches in our survey, as we will see in more detail in Section 3.</p>
<p>Manuscript submitted to ACM</p>
<p>The scope of this survey is limited to multi-step reasoning approaches.Other studies have published more challenging benchmarks, to study the performance of Chain-of-thought and self-reflection in LLMs in puzzles and (logic) games [108,129,175].</p>
<p>In this survey we will mention benchmark results as reported by the surveyed papers, for the sake of concreteness.</p>
<p>However, it can be difficult to directly compare different benchmarking results, since there may be differences in the way the experiments were conducted.Kamoi et al. [70] provide a critical analysis of benchmarking problems in LLM reasoning.Despite these challenges, general conclusions can be drawn.Section 4.1 provides guidance on how to choose reasoning approaches for different problem types.</p>
<p>Step Generation, Evaluation and Control</p>
<p>This survey examines how LLMs based on the transformer architecture can be prompted to solve multi-step reasoning tasks.The Chain-of-thought paper shows how a simple command can prompt an LLM to perform reasoning steps, yielding much better performance in math word problems.Much research has further explored this approach, trying to build stronger general problem solvers for other types of reasoning problems.</p>
<p>A typical approach to solve a complex problem is to subdivide it into smaller steps and to solve those.This approach is related to classical divide and conquer [6].It consists of three stages.New steps are (1) generated, (2) evaluated, and the search of the generated steps is (3) controlled in some way.The in-context reasoning approaches that we survey follow a general three-stage pipeline [95]:</p>
<p>(1) Generate: prompt the model to generate reasoning steps,</p>
<p>(2) Evaluate: prompt the model to evaluate the generated steps,</p>
<p>(3) Control: prompt the model to control the number of steps that are generated and how deep ahead the reasoning process will look.</p>
<p>This three-stage pipeline is the basis of our taxonomy.We will now discuss the three stages more deeply; for ease of reference they are numbered according to the Subsection in which they are described in more detail (3.1, 3.2, 3.3).Please also refer to Figure 1, and to Table 2, for a diagram of the categories and subcategories of different approaches for the generation, evaluation, and control of reasoning steps.1</p>
<p>(3.1) Generation.The first stage is to create a prompt that instructs the LLM to generate reasoning steps.The problem must be split into substeps.This can be achieved with a problem-specific prompt that contains elements of the problem, such as: "First calculate how many marbles Mary had originally, then how many her friend has, and finally how many they have together." In general, it is possible to prompt an LLM to fill in the blanks in a step-by-step fashion.In the papers that we discuss, there are three main approaches for generating the step-by-step prompt, numbered with the Subsection in which the approaches are described.First, the prompt may be handcrafted for the problem by the researchers:  Generating the subproblem-steps is the first stage that is necessary for in-context learning to perform reasoning.</p>
<p>Each paper in our survey performs at least this stage of the reasoning pipeline.In some of the early papers (around 2022) it is the only stage of the pipeline that is performed.</p>
<p>(3.2) Evaluation.After the prompt has been generated and the model has answered it, the next stage is to evaluate the quality of this answer.Such evaluation is often necessary to improve the answer and perform well on the benchmark.</p>
<p>Again, we see three main approaches for substep evaluation.First, the steps may be evaluated by the model itself:</p>
<p>(3. (3.3) Control.The third stage is control.A reasoning process that consists of multiple steps is a sequential decision process [89].When a single chain of reasoning steps is generated, the control flow of the reasoning process is simple:</p>
<p>greedily evaluate the first step and then the next one, if present.The control flow of the reasoning process may also be more intricate.Some reasoning problems can be divided into multiple subproblems.To execute, evaluate and combine the results of all substeps, a separate controller may be needed.This controller can be a prompt or an external algorithm.</p>
<p>Again, we distinguish three approaches.Most papers use a (3.3.1)greedy selection approach: a single prompt with a single chain of steps is generated, and these steps are directly executed and followed.The second approach is to generate an (3.3.2) ensemble strategy of reasoning steps, evaluate them, combine the individual results, and present them as the result of the ensemble.Finally, a full tree-search or a (3.3.3)reinforcement learning (RL) algorithm can be used as scaffolding [116].In this case, when a step is followed and evaluated, the LLM can roll back and try a different reasoning step.Going further, a full reinforcement learning approach can be used [117,147] to find an optimal policy for the sequential decision process.In general a Markov Decision Process of state, action, transition, and reward function can be specified, and step control can become a process where prompts are generated dynamically under the control of an external RL algorithm, or as in-context reinforcement learning (ICRL) [82,101].Self-taught-reasoner [191] math word finetune augmentation greedy/feedback CommonsenseQA =72%
Taxonomy Table.
Least-to-most [198] math word hand-written self-assessment curriculum
SCAN =99%
Progressive-hint [195] math word model-generated self-assessment stable prompt</p>
<p>AddSub +2%, MultiArith +0%, SingleEQ +2%, SVAMP +3%, GSM8K +5%, AQuA +1%</p>
<p>Self-refine [ use natural language problems.For example, the subgroup of Codex to Program-aided-language focuses on formal languages.They generate code or math equations, typically in Python, to formalize the steps of the reasoning problem, or as result of the task.LLMs that are trained on computer code achieve good performance at code generation, Chen et al. [18] report up to 77% accuracy on the HumanEval dataset.Gao et al. [45] leverage this ability by translating problems into code, achieving between 61% and 99% accuracy on various mathematics tasks.The use of code also allows the approaches to call external programs such as interpreters and debuggers to evaluate the correctness of the reasoning steps that are generated.</p>
<p>There is also a special subgroup, Refiner to Self-improvement, that uses finetuning in addition to prompt learning.</p>
<p>Here, new data is generated based on reasoning exemplars, which is then used to further train the model.The extra data is often generated as a separate dataset, sometimes called critic or corrector.</p>
<p>There are two approaches, Say-can and Inner-monologue, whose application domain is control of robot movement.</p>
<p>Robotic movement is constrained by the laws of physics (both in the body of the robot as in aspects of its environment).</p>
<p>The laws of physics are learned and used to ground the reasoning steps in reality (to reduce hallucination).</p>
<p>Manuscript submitted to ACM</p>
<p>The third group, Least-to-most to Voyager, addresses step control (shown in bold in this column).Whereas in the previous approaches the reasoning steps are written in a single, static, prompt, these approaches generate the steps in multiple, dynamic, prompts.This allows control of the space of reasoning steps.Various search control approaches are used, all in the form of an external algorithm that performs calls to the LLM with different prompts.The control methods range from simple greedy and depth-first search to elaborate beam search and reinforcement learning schemes.</p>
<p>The last column of the Table summarizes reported benchmark results.The '=' symbol indicates absolute scores on the benchmarks, while '+' indicates relative improvement in percentage points over a baseline LLM, typically GPT-3.5.</p>
<p>Results vary strongly, both between approaches and within a single approach between benchmarks.Also, different</p>
<p>LLMs were used, from early stage to more mature models, open and commercial, and the baselines differ.For some benchmarks, such as the Creative Writing benchmark in Tree-of-thoughts, results are best reported qualitatively.The source papers provide more measurement details, Section 4.1 discusses when to which approach to use for different applications.</p>
<p>In conclusion, we see a diverse array of methods that often achieve high performance in reasoning on their respective domains.To better understand the approaches, we discuss them in more detail, starting with the generation of steps.</p>
<p>Generation of Steps</p>
<p>Originally, LLMs performed poorly on math word problems such as GSM8K [31].Different approaches were tried unsuccessfully, for example scaling up the size of the LLM [122].The LLM architecture, based on transformers, is designed to produce a single token.When we prompt such an architecture to produce an answer, it does so.What we should do instead, is to prompt it to follow intermediate steps, answer those, and thus work towards the final answer, just as a student is taught to break down a complex problem into smaller steps.We should guide the model to explicitly produce intermediate steps, and combine the intermediate results.This idea was used by Nye et al. [106] in Scratchpads, a transformer model that performs multi-step computations by asking it to emit intermediate computation steps into a scratchpad.They train the model by supervised learning (not prompt-based in-context learning).Figure 2 shows an example.On experiments with addition, polynomial evaluation, and Python code execution, versions that produced the intermediate steps on a scratchpad performed considerably better than versions that did not, going from 35% to 95%, from 32% to 51%, and from 30% to 42% accuracy, respectively.</p>
<p>If supervised learning can produce intermediate steps, would prompt learning be able to do so, too?</p>
<p>3.1.1Hand-written Prompt.This question was studied by Wei et al. [173], amongst others.A basic way to instruct an LLM to generate steps by prompt-learning is to manually write a prompt for the large language model to follow the reasoning steps.When the LLM is prompted to rephrase information from the question as intermediate reasoning steps in its answer, the LLM performed much better than when it was prompted to answer a math problem directly, without reproducing information from the question in its answer in multiple steps.The example from the Chain-of-thought paper is shown in Figure 3. Table 1 shows that the largest accuracy increase is on GSM8K, from 16% to 47%.</p>
<p>The idea that an LLM can be made to follow step-by-step instructions, and the performance improvement by Chain-of-thought have caused much excitement and have opened up further research on reasoning with LLMs.In the original paper the prompts were handwritten by the researchers for the individual types of problems, and evaluations Fig. 3. Different chain-of-though (CoT) prompting techniques.At the top the prompts, at the bottom the answers.When shown the longer example prompt, the LLM follows the longer example when answering the question (Few-Shot CoT [173]).Without example answer and using Let's think step by step results in similar answers (Zero-Shot CoT [75]).With Program-aided-language models [45] similar reasoning can be achieved.</p>
<p>are conducted with benchmarks (not by an LLM). 2 In a later work the prompts were generated automatically by the LLM [193], and evaluated.</p>
<p>Kojima et al. [75] go a step further.They show that the addition of a single text to the prompt (Let's think step by step) significantly improves performance.Since this text does not contain problem-related elements, it is as a form of zero-shot learning.Figure 3 (third column) compares the approaches.Experiments further show that with this addition to the prompt significant performance gains are also achieved on other reasoning benchmarks, including arithmetic, symbolic, and logical reasoning (achieving 70% accuracy on GSM8K/PaLM when Self-consistency is also included).</p>
<p>The Chain-of-thought idea itself is inspired by earlier work where natural language steps are generated for arithmetic reasoning [31,88], and the use of formal languages for reasoning [3,21,23,127].</p>
<p>3.1.2Prompt using External Knowledge.Chain-of-thought prompts are written manually, by the researchers, an approach that does not scale.We can use external information about the problem to improve the prompt.Press et al. [119] study how subproblems are related to the main problem, which they call compositional reasoning.They study how often a model is able to answer the subproblems, but not the overall problem.This difference is called the compositionality gap.They find that in GPT-3, as model size increases, the single-hop question-answering performance improves faster than the multi-hop performance: while more powerful models memorize and recall more factual knowledge, no improvement in their compositional reasoning occurs.The ability to reason does not depend on the size of the model.</p>
<p>Subsequently, a method called Self-ask is proposed, that asks elicitive follow-up questions (like Chain-of-thought, but with the follow up: prompt), that the model then answers.Self-ask can also use an external search engine to answer intermediate prompts, instead of the model.The initial subquestion is fed into the search engine, and the answer is processed by the model, which generates another subquestion, and so on, until it produces the final answer.Self-ask was tested on three benchmarks that were specifically designed for multi-hop questions.Although it performs only a few percentage points better than vanilla Chain-of-thought, it showed how external knowledge can be used in a reasoning setting.</p>
<p>3.1.3Model-Generated Prompt.In addition to manually writing prompts or using external information, we can also let the LLM itself study the problem to write the best reasoning-prompt.An example of such self-improvement is Auto-chain-of-thought [193].This approach builds on the observation by Kojima et al. [75] that large language models are zero-shot reasoners.First, Auto-chain generates specific questions for a given dataset and partitions them into clusters.Then an external algorithm uses the model to generate examples that are sampled for diversity.The constructed demonstrations augment the in-context prompt.This approach also performed a few percentage points better than hand-written Chain-of-thought prompts, on ten benchmarks, using GPT-3 (see Table 2).</p>
<p>Fu et al. [44] introduce Complexity-based prompting.Inspired by Chain-of-thought and Self-consistency, their work specifically studies the impact of the complexity of the reasoning chain (the number of steps), and introduces a related reasoning approach (Complexity-based prompting).They find that prompts with the largest complexity perform best, and also that answers with the highest complexity are the best.Complexity-based prompting achieves somewhat higher performance on three math reasoning benchmarks: GSM8K improves 7 points, MathQA 6 points, and the Penguins benchmark from Big Bench Hard improves 3 percentage points.</p>
<p>We see that the initial approaches showed larger improvements than the later approaches.It is time to look at another category of approaches, that focus on the evaluation of reasoning steps.</p>
<p>Evaluation of Steps</p>
<p>After discussing prompts for the generation of reasoning steps, the next stage in the generation/evaluation/control pipeline is evaluation of the results of the steps.This stage focuses on reducing error accumulation of multi-step reasoning chains.We will start with approaches where the same model performs step-generation and step-evaluation.Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?</p>
<p>A: There are 3 cars in the parking lot already. 2 more arrive.Now there are3 + 2 = 5 cars.The answer is 5. … Q: Janet's ducks lay 16 eggs per day.She eats three for breakfast every morning and bakes mu ns for her friends every day with four.She sells the remainder for $2 per egg.How much does she make every day?A:</p>
<p>Reasoning 1 She has 16 -3 -4 = 9 eggs left.So she makes $2 * 9 = $18 per day.</p>
<p>The answer is $18.</p>
<p>Output</p>
<p>The answer is $18.</p>
<p>Reasoning 2</p>
<p>This means she sells the remainder for $2 * (16 -4 -3) = $26 per day.</p>
<p>The answer is $26.</p>
<p>Reasoning 3</p>
<p>She eats 3 so she has 16 -3 = 13 left.</p>
<p>Then she bakes, so 13 -4 = 9 eggs.9 eggs * $2 = $18.</p>
<p>The answer is $18.</p>
<p>Fig. 4. Self-Consistency (Adapted from [170]), showing majority voting over answers that the model produces accumulation (logical, factual, ethical, or otherwise) [176,179].Several methods have been developed to prevent error accumulation.One approach is to create a new model to separately evaluate the results.Shen et al. [138] and Li et al.</p>
<p>[87] train an external verifier to check results.In contrast, Weng et al. [176] propose an automated approach using evaluation by the same LLM, called Self-verification.They note that human reasoning also suffers from the problem of accumulating errors, and that in human reasoning we frequently revisit our thought process to verify the accuracy of our reasoning steps.The LLM is prompted to use the conclusion of the Chain-of-thought reasoning chain as a condition for solving the original problem and then compare the answer, going back to the original question.The LLM is given variations of its own conclusion and is instructed to choose the one with the highest similarity to the original question.(Note that there can be feedback issues using an LLM to evaluate itself, for a discussion see Zheng et al. [196].)</p>
<p>Experiments are reported on GPT-3 [18] and on Instruct-GPT [107].The accuracy of Chain-of-thought was improved by a few percentage points on arithmetic and general reasoning tasks (GSM8K 65%, AQuA 48%, SVAMP 77%).</p>
<p>A popular related approach is Self-consistency [170].Self-consistency is a straightforward ensemble approach (a well-known machine learning technique to make a strong learner out of multiple weaker learners [13,130]).Greedy single-path decoding is replaced by sampling diverse reasoning paths, evaluating them, and selecting the most consistent answer.Self-consistency asks the LLM to simply perform the same query multiple times, and takes the majority-vote of the answers, or decoding paths.Self-consistency works since complex reasoning problems typically allow different reasoning paths that lead to the correct answer.Figure 4 summarizes the approach.Self-consistency has been evaluated on arithmetic reasoning, commonsense reasoning and symbolic reasoning, on a variety of LLMs, including GPT-3 [16,25,151,153].Self-consistency further improves the performance of Chain-of-thought by 10-20 percentage points, and has been used as a baseline in many of the other approaches in this survey.</p>
<p>Manuscript submitted to ACM 3.2.2Tool-based Validation.Another approach to improve the accuracy of evaluating the reasoning steps is to switch from a natural to a formal language.The advantage of a formal language is that it is less ambiguous than a natural language.Examples are computer languages, such as Python, or mathematical equations.Using a formal language for reasoning is a popular approach, and we discuss seven papers. 3 Many approaches generate the steps in Python, and the code can then be evaluated by a formal evaluator, such as a compiler, debugger, or interpreter.</p>
<p>LLMs have been quite successful in generating computer code from natural language prompts.Chen et al. [18] introduced Codex, a GPT model that was trained on publicly available code in the repository GitHub.A production version of this work was introduced under the name GitHub Copilot.Codex is often able to generate correct programs from descriptions in natural language, such as commentary strings.</p>
<p>The work on Codex is used as a basis for further research on reasoning in LLMs.Human programmers, when writing code, typically follow a cycle of writing some code, executing it to look for errors, and then using the feedback to improve the code.This step-by-step approach is followed in Self-debugging [22].It follows the same steps of code generation, code execution, and code explanation.Self-debugging is able to identify mistakes in its own code by investigating the execution results, and can also provide an explanation of the generated code, in natural language.It achieves strong performance: the text-to-SQL Spider benchmark improves by 9 points, and the C++ to Python Transcoder benchmark improves by 12 percentage points.</p>
<p>Several works generate working code tuned for solving specific problems automatically, without human feedback.</p>
<p>Romera-Paredes et al. [126] introduced FunSearch, an approach that integrates formal methods and LLMs to enhance mathematical reasoning and code generation.FunSearch uses a genetic approach with multiple populations of candidate solutions (programs), which are evaluated using a function depending on the problem specification.In addition to the evaluation function, also an initial program is given to the LLM in the first prompt.After evaluating a number of generated programs from the starting prompt, a new prompt is created, in an iterative fashion, combining a selection of sampled programs sorted according to their evaluation score, and the LLM is requested to generate a new program.</p>
<p>Another work leverages evolutionary computation methods to generate and optimize evolutionary algorithms [158].This approach, LLaMEA (Large Language Model Evolutionary Algorithm), utilizes LLMs to design and optimize evolutionary algorithms.The approach uses LLMs to generate initial algorithmic structures, which are then refined through mutation and selection.This enhances the efficiency of algorithm design, particularly in fields requiring innovative and adaptive solutions, improving accuracy on the Black-Box Optimization Benchmark suite [52] (BBOB) by 11 percentage points.A key difference between FunSearch and LLaMEA is that LLaMEA uses a sample-efficient elitism strategy by iteratively improving the best-so-far solution, requiring significantly fewer prompt evaluations than the large-population strategy proposed in FunSearch.Evolutionary approaches let the LLM discover new algorithms, solving existing problems in new ways, or solving entirely new problems.Another method, Evolution-of-heuristics [90], was proposed for evolving code snippets for guided local search to solve combinatorial optimization problems, such as the Traveling Salesperson Problem.</p>
<p>To improve prompt-based reasoning, Codex is used in an ensemble approach named MathPrompter [64].This approach generates multiple algebraic expressions or Python functions, which then solve the same math problem.</p>
<p>The results are compared, just like in Self-consistency and Self-verification, raising the confidence level in the results.</p>
<p>MathPrompter achieved state-of-the-art accuracy on the MultiArith dataset (78.7% → 92.5%), evaluated on GPT-3 175B.</p>
<p>Two other approaches that use a formal language are Program-of-thought [19] and Program-aided-language [45].</p>
<p>Both approaches use the LLM to generate Python and then use an interpreter to evaluate the result.The approaches are similar although Program-aided-language uses generic prompts, and has been tested on more benchmarks.Figure 3 (fourth column) illustrates the Program-aided-language approach.When the evaluation of the reasoning steps is off-loaded to the Python interpreter, decomposing the natural language problem into executable code steps remains the only task for the LLM.(Earlier work in math word problems showed how to decompose a problem and reach an answer [88].)Gao et al. [45] provide extensive experimental evidence about the synergy between the neural LLM and the symbolic interpreter.Experiments are performed on 13 mathematical, symbolic, and algorithmic reasoning tasks, achieving more accurate results than much larger models (for example, Program-aided-language reported 72% on GSM8K and 93% on Penguins).</p>
<p>External Model Validation.</p>
<p>We have seen many examples of successful prompt-based in-context reasoning and evaluation (at inference time-where no parameters were changed).We will now look at reasoning approaches that follow a more traditional parameter training approach.All approaches evaluate the output of the model and generate corrective data.That data is then added to the training pipeline, and the model is subsequently finetuned.</p>
<p>Finetuning.The Refiner approach [113] uses a generator model and a critic model that provide fine-grained feedback on reasoning errors.The generator generates multiple reasoning hypotheses, and the critic evaluates results by randomly selecting a hypothesis for feedback.The generator model is then finetuned based on its reasoning errors.A small supervised model is used to overcome the cold-start problem.The approach achieves 78% accuracy on GSM8K and also works well on related problems.</p>
<p>Welleck et al. [174] follow a similar approach, which they call Self-correction.Here, the corrector is a separate model specialized in refining the outputs of the generator.Unlike Refiner, where the generator is finetuned based on the critic, Self-correction finetunes the corrector to rectify errors in the hypotheses produced by the generator.Self-corrector is not applied to math word problems, but to program synthesis, where a small corrector reduces toxicity to 0%.</p>
<p>A third finetuning approach is Self-improvement [58].Here, too, the base model data is augmented by LLM-generated rationales, and then finetuned.The authors achieve 82% accuracy on GSM8K and similarly high scores on question answering and adversarial benchmarks.Noteworthy in all three finetuning approaches is that LLMs are capable of improving themselves by training on their own generated output, and that stability problems from feedback loops are overcome.</p>
<p>Dataset Augmentation.The final finetuning approach that we discuss uses dataset augmentation.In Self-taughtreasoner [191], an intermediate reasoning is generated, called a rationale.Rationales are shown to be valuable across diverse tasks such as mathematical and commonsense reasoning, code evaluation, social bias inference, and natural language inference.First an augmentation dataset is created by attempting to solve the original dataset.Next, the dataset is augmented using rationalizations and ground-truth answers to problems the model failed to solve.Finally, the model is finetuned on the combined dataset.Self-taught-reasoner performs comparably (72%) to finetuning a 30 times larger model on CommonsenseQA.</p>
<p>Reasoning about Robot Behavior.In addition to math word problems, computer code, and common sense, prompt-based reasoning has also been used to improve robot behavior.Language models contain a large amount of information about the real world [2].In theory, this should allow them to reason realistically about robotic behavior.However, the models do not have knowledge about specific embodied aspects of a particular robot.If we could compare a Scratchpad-like Manuscript submitted to ACM list of intermediate reasoning steps with a list of possible movements of the robot in its environment, then we could prevent the model from suggesting impossible joint movements and actions, and prevent failures.</p>
<p>Say-can [2] learns a value function [68] of the behavior of a robot and its environment using temporal difference reinforcement learning [146].This value function is combined with prompt-based reasoning by the language model, to constrain it from suggesting impossible actions.The goal of Say-can is to ground language in robotic affordances.In contrast to Scratchpad, which used supervised learning, the affordance model is learned interactively by reinforcement learning, and then applied using prompt-based learning on the LLM.The language model has high-level semantic knowledge about the task (Say).The learned affordance function (Can) provides an environment-grounding on what is possible.Say-can achieves a 31% success rate on 101 real-world robotic kitchen tasks.</p>
<p>Where Say-can learns affordances as a separate function, Inner-monologue [61] formulates robotic planning directly as part of the language prompt, internally.The input consists of many elements: textual descriptions from InstructGPT [16] for multi-step planning, scripted modules for object recognition, success detection, task-progress scene description, and language-conditioned pick-and-place primitives, similar to CLIPort [143].The language feedback that is thus generated significantly improves performance on three benchmarks, achieving 90% accuracy on simulated and real table top rearrangement tasks and 60% on the kitchen environment.There are many other studies into robotic behavior.</p>
<p>An approach related to Inner-monologue is Chain-of-tools, which proposes a plan-execute-observe pipeline to ground reasoning about tool behavior [139,140].</p>
<p>This concludes our discussion of the second stage of the reasoning pipeline, evaluation of the reasoning steps.</p>
<p>Control of Steps</p>
<p>The third stage is control.This stage controls how many sub-steps are generated, and how deep into the future the reasoning chain is generated.There are three main approaches: (3.</p>
<p>Greedy Selection.</p>
<p>Most earlier works on prompt-based reasoning follow the greedy approach: generate a single prompt with a single sequence of steps and follow them.Among the greedy reasoners are Chain-of-thought, Auto-CoT, and Zero-shot CoT.Inner Monologue and Say-Can also use greedy reasoning.</p>
<p>In Least-to-most prompting [198], the key idea is to break down a complex problem into simpler subproblems and then solve these in sequence, explicitly encoding them in the prompt, related to Complexity-based prompting.</p>
<p>In Least-to-most the answers to previously solved subproblems help in finding the answer, as a curriculum [8].On symbolic manipulation, compositional generalization, and math reasoning, Least-to-most prompting generalizes well, achieving 99% accuracy on a compositional generalization benchmark.</p>
<p>Ensemble Strategy.</p>
<p>The second kind of reasoning control is based on an ensemble of (sequences of) reasoning steps.For most problems, multiple different options exist for the next step.When all or some of these are generated and evaluated, then the consensus result can be reported as the outcome of an ensemble of steps.Self-consistency [170] and Self-verification [176] (in Section 3.2.1)are popular ensemble approaches to evaluate the results of reasoning steps, in which greedy single-path decoding used in Chain-of-thought prompting is replaced by a diverse set of paths.</p>
<p>Taking this further, Chain-of-experts uses a mixture-of-experts ensemble for complex combinatorial problems [180].</p>
<p>Manuscript submitted to ACM Program-aided-language and MathPrompter also use the ensemble approach.The ensemble approach is popular in reasoning with LLM.</p>
<p>Reinforcement Learning.</p>
<p>In reasoning, often multiple valid steps are possible, but pursuing all possibilities over multiple trajectories may lead to an infeasible number of possibilities.The third kind of reasoning control is to use a full-fledged controller that can traverse a tree, or perform reinforcement learning to do so [68,117,147].When decomposing the problem, multiple alternative steps are generated that can be searched multiple steps into the future.</p>
<p>Then, backtracking can be performed, allowing alternative steps to be tried.</p>
<p>Where greedy and ensemble processes can be controlled with a prompt by the LLM, this third category is more complex, and an external algorithm is used to control the reasoning process.The external algorithms call the LLM as a subroutine prompting it to perform requested tasks.The external algorithm allows more complex reasoning control, but we are now beyond prompt-based self-reasoning: control has been given to an algorithm that is external to the LLM and external to prompt-learning.</p>
<p>We start our discussion of control strategies with depth-first and breadth-first search, then go to beam search, and then to full reinforcement learning.A complex reasoning space can be traversed with a search algorithm.Tree-of-thoughts [188] uses breadth-first or depth-first search to dynamically follow different reasoning steps.Tree-of-thoughts calls the LLM with two different types of prompt: to generate the sub-problems, and to evaluate them.Together, the trio of generation (LLM-prompt-1), evaluation (LLM-prompt-2), and control (search-algorithm) allow systematic exploration of the space of reasoning steps with look-ahead and backtracking.The authors compare their approach to Chain-of-thought and Self-consistency on the Game of 24, Creative writing, and Mini crossword, achieving an accuracy of 74% on a Game of 24 task.The other tasks are evaluated qualitatively.Figure 5 illustrates the different reasoning structures. 4nother approach, Buffer-of-thoughts [185], goes a step towards metareasoning [46].It introduces a meta-buffer that stores high-level thought-templates.These thought-templates are derived from a variety of tasks by a problem distiller.</p>
<p>They are then instantiated for specific tasks.To address size restrictions of the LLM context window, the meta-buffer is managed for capacity and thoughts can be combined.Figure 5 compares the Buffer-of-thoughts approach to other approaches such as Chain-of-thought and Tree-of-thoughts.Buffer-of-thoughts outperforms other methods in puzzles such as Game of 24 (by 11%) and checkmating (by 51%).Thought templates are related to metacognition (thinking about thinking), which is further discussed in Section 4.3.3.</p>
<p>A related search method is Beam-search-for-reasoning [181].When the space of possible reasoning paths is large, Beam-search searches a promising part of this space.It uses self-evaluation to control exploration and to evaluate (decode) reasoning steps.Beam search uses Program-aided-language models for math word problems [45].Using a Codex backbone [18], it surpasses the few-shot baselines by 6.34%, 9.56%, and 5.46% on the GSM8K, AQuA, and StrategyQA benchmarks, respectively.</p>
<p>Reinforcement learning is another step in the sophistication of optimization algorithms.It learns by interactive sampling, improving its policy based on rewards from the environment [147].To use reinforcement learning, the reasoning problem is formulated as a Markov Decision Process: the agent-algorithm creates a prompt for a next step (an action), to sample a step () and get an answer (state, reward) from the environment-model.The answer can then be used to improve the prompt (next action), using the rewards to improve its policy of best actions for each state.The approaches that use reinforcement learning also do so in the form of an external algorithm.Progressive-hint-prompting (PHP) uses reinforcement learning to interactively improve prompts [195].PHP calls the LLM with dynamic prompts, using previously generated answers as hints, to progressively prompt the LLM towards the correct answers.It works as follows: (1) given a question (prompt), the LLM provides a base answer, and (2) by combining the question and answer, the LLM is queried and obtains a subsequent answer.We (3) repeat operation</p>
<p>(2) until the answer becomes stable, as a regular policy-optimizing reinforcement learning algorithm.The authors have combined PHP with Chain-of-thought and with Self-consistency.Using GPT-4, state-of-the-art performance was achieved on grade school math questions (95%), simple math word problems (91%) and algebraic question answering (79%).</p>
<p>Another approach that is motivated by improving answers from feedback, is Self-refine [95].Like PHP, the LLM generates an initial output and provides feedback for its answer, using the LLM to refine itself, iteratively.Self-refine prompts the LLM in three ways: (1) for initial generation, (2) for feedback, and (3) for refinement, following a greedy reasoning chain.Self-refine has been used with GPT-3.5 and GPT-4 as base LLMs, and has been benchmarked beyond math word problems on dialogue response generation [4], code optimization, code readability improvement, math reasoning, sentiment reversal, acronym generation, and constrained generation, showing substantial improvements over the base models (typically around 30 percentage points, see Table 2).</p>
<p>Another approach that combines reinforcement learning and LLMs is ReAct [187].However, ReAct does so in a different way.Most works focus on reasoning by the LLM, not on actions by an agent.The goal of ReAct is to combine progress in reasoning with action plan generation.(Or, to put it differently, other approaches use RL to improve LLM-reasoning, ReAct uses LLMs to improve RL agent policies.)ReAct uses Chain-of-thought prompt-learning as part of an RL framework that also uses external knowledge sources (Wikipedia) and finetuning; for error reduction, Manuscript submitted to ACM Fig. 6.Architecture of Reflexion [141], showing a close resemblance to the agent/environment structure of reinforcement learning [147] grounding, and for reducing hallucination.The framework allows hand-written prompts.On two interactive decision making benchmarks (Alfworld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively.</p>
<p>The ReAct work has been developed further.Reflexion [141] creates AI agents that learn by reflecting on failure and enhance their results, much like humans do.Reflexion uses three language models: actor, evaluator, and reflector.</p>
<p>It works as follows: (1) an actor generates text and actions, (2) an evaluator model scores the outputs produced by the actor, and (3) a self-reflection model generates verbal reinforcement cues to assist the actor to self-improve (see Figure 6).For the actor, Chain-of-thought and ReAct can be used.Reflexion is evaluated on decision-making, reasoning, and coding tasks.Improvements of 10-20 percentage points are reported.</p>
<p>The reinforcement learning approaches that we discussed so far-React, Self-refine, Tree-of-thoughts, Buffer-ofthoughts, Reflexion-use external algorithms to manage state for the prompt improvement loop.A more elegant solution would be to perform reinforcement learning fully in-context, within-prompt.Indeed, Krishnamurthy et al. [78] explicitly asked the question whether LLMs can explore in-context.This is the goal of the Algorithm-of-thoughts (AoT) approach [135].Following work by Demircan et al. [35], Lee et al. [82], Wang et al. [169] that showed that transformer architectures can be pretrained and finetuned to perform in-context reinforcement learning, AoT aims to do so in general LLMs such as GPT-4, Claude and Gemini 1.5.They achieve results comparable to Tree-of thoughts on GSM8K, StrategyQA, and Crosswords.Achieving these results with in-context RL is a promising result for in-context learning.</p>
<p>Other search-like in-context algorithms are studied by Schultz et al. [133] and Kempinski et al. [72].Further works in games find that LLMs struggle with the difference between generating an algorithm for a problem, and executing that algorithm correctly (the knowing-doing gap [108,129]), and struggle with the difference between using and mentioning game concepts [30,159].Work on implicit reasoning is ongoing, Li et al. [84] provide a survey.</p>
<p>Manuscript submitted to ACM To conclude this overview of reinforcement learning approaches, we discuss an application in the games domain.</p>
<p>Voyager [166] is an agent for the game of Minecraft that uses an iterative prompting mechanism that generates code for embodied control.The agent includes self-verifcation and a skill library to maximize exploration.The goal is to discover diverse items in Minecraft, a form of novelty search [39].Voyager performs well, reaching high scores by acquiring many tools (see Figure 7) 15 times faster than the baseline.</p>
<p>The applications of reinforcement learning in LLM reasoning are many, and the connections run deep [120].Wang et al. [167] use the similarity between RL timesteps and LLM reasoning steps to jointly train a value function together with the LLM policy by optimizing the Bellman equation, achieving 85% accuracy on GSM8K and 81% on Alfworld.</p>
<p>Du et al. [38], Guo et al. [50] replace supervised finetuning by reinforcement learning, integrating it with reasoning, achieving large efficiency gains in the training pipeline in the DeepSeek r1 and Kimi models.</p>
<p>Discussion</p>
<p>We have reviewed many reasoning approaches.It is now time to reflect, to discuss limitations, and to look for promising areas of future work.First we discuss how to choose an approach for an application.Next, we discuss issues concerning hallucination, faithful reasoning, and scaling.Then we discuss what LLMs can and cannot do.We highlight connections with sequential decision processes, metacognition, and programming languages, and we end with a research agenda.</p>
<p>Matching Methods and Applications</p>
<p>This survey has discussed different approaches, applications, and benchmarks.It is often difficult to directly compare benchmark results between individual papers, due to differences in measurement setup.For example, Kamoi et al.</p>
<p>[70] critically compare self-reflection studies.Despite the challenges in comparing benchmark results, we can provide general guidance on what approach to use for different types of applications.</p>
<p>For simple reasoning situations, that require a single sequence of reasoning steps, Chain-of-thought is typically used.When the error rate becomes too high, verification methods such as Self-consistency are popular.Indeed, this combination is used in many modern reasoning LLMs such as DeepSeek and GPT-o1 [38,50,62,79].</p>
<p>Manuscript submitted to ACM For combinatorial puzzles and games that would traditionally require backtracking, tree-search or reinforcement learning solution methods must keep track of the enumeration state.However, an LLM without an external algorithm often fails to keep track of this state correctly between calls to the model.An approach such as Tree-of-thoughts, or ReAct, should be considered.Here, the state is managed externally, and the LLM is called for application-dependent functional processing.Various in-context reinforcement learning methods are being developed where state is managed in-context, as in, for example, Algorithm-of-thoughts.When performance is not sufficient, finetuning or pretraining for the domain at hand can be used.</p>
<p>For complex problems, where a sequence of steps, or external control, fail to perform well, other reasoning approaches are needed.The LLM can be asked to generate a program for the problem.In this approach, the LLM acts as a coding agent for its user.The LLM generates a program or a sequence of commands, which are then executed by an external tool, such as an interpreter, a robot, a planner, or a logic solver.In robotics, vision-language-action models have achieved impressive results [73], but also strong results in logistics [12], finance and medicine are reported [118].</p>
<p>Hallucination, Faithfulness and Scaling</p>
<p>Reasoning by LLMs is not error-free, and many studies aim to provide deeper insight into the reasoning processes in language models.Saparov and He [132] introduce a synthetic question/answer dataset designed to evaluate the reasoning abilities of LLMs.The work showed that LLMs are capable of reasoning to a certain degree, but that Chainof-thought struggles with proof trees with a wide branching factor.In another study, Wang et al. [165] aim to increase our understanding of how Chain-of-thought works.The authors find that the order of the reasoning steps is important.</p>
<p>Prompts should be relevant to the question, and coherent (steps should be in the correct order).Jin et al. [66] also study the impact of reasoning step length on LLMs, again finding a strong positive correlation between the length of the prompt and reasoning abilities.Next, we discuss works on errors in the Chain-of-thought approach, studying whether the reasoning of the LLM is faithful, or that it gives the right answer for the wrong reason.</p>
<p>4.2.1 Faithfulness.Chain-of-thought approaches prompt a language model to take certain steps to solve the problem that the prompt specifies.One can ask the question whether those steps are indeed the steps that the model has followed (faithful reasoning) or whether it took another road to arrive at the same answer (unfaithful reasoning).A few studies measure the faithfulness of reasoning with LLMs.Lanham et al. [80] notes that just like organic reasoners, a model's reasoning may be post-hoc, it may be constructed after a certain conclusion has been found.By deliberately adding mistakes to the chain of thought, the authors measure the faithfulness of the model.They find a wide variation of post-hoc reasoning, with a tendency of larger models to be less faithful.Like regular LLMs, when not properly grounded, (Chain-of-thought) reasoning suffers from hallucination [60].</p>
<p>Another study adds deliberate bias to the prompt.For example, in a multiple-choice setting, they always make answer (A) the correct answer [155].They find that a bias towards wrong answers can cause significant drops in accuracy, and that models frequently generate Chain-of-though explanations rationalizing wrong answers.To address issues of faithfulness, Lyu et al. [94], Xu et al. [183] propose Faithful-chain-of-thought. This approach involves two stages.</p>
<p>First, the natural language query is translated into a formal symbolic language.Second, the problem-solving stage processes the formal language, and can explain the reasoning steps it has thus taken.For the symbolic language, Python, Datalog, or PDDL is suggested.Another approach, mechanistic interpretability, studies methods to target individual representations inside the LLM, to see if the expected behavior occurs in practice [9,20,124].LLMs, and, specifically, how reasoning capabilities can be transferred to smaller language models.Scaling laws of LLMs are an active area of study, see for example [54,55,71].Distillation of reasoning to smaller models can work surprisingly well in situations with more explicit instructions, and given the computational cost of training LLMs, there is much interest in transferring knowledge to small language models.Comprehensive surveys on knowledge distillation are Gu et al. [49], Xu et al. [184].For reasoning specifically, Magister et al. [96] have studied reasoning in small language models, using a student model that learns from a teacher model, by finetuning.Other works focus on prompt distillation for retrieval [34], recommendation [85], embodied agents [24], and LLM graph reasoning [192].</p>
<p>Another study related to Self-taught-reasoner focuses on explanation in small language models, also achieving good results of knowledge transfer [86].</p>
<p>Limitations: What LLMs Can and Cannot do</p>
<p>The capabilities of LLMs are impressive.LLMs can be seen as large text-based surrogate models of the world (or the world how we describe it on the internet), and thus allow reasoning about a large variety of contexts and problems.</p>
<p>Reasoning, such as in math word problems, were one of the capabilities that LLMs could not achieve, until recently.Let us look more closely at what language models currently can and cannot do.A taxonomy of generate-evaluate-control is able to describe the structure of the current LLM reasoning literature.</p>
<p>Furthermore, the accuracy of the reasoning chains can be improved with ensemble methods, and self-verification.</p>
<p>Hallucination can be reduced by grounding the model with external models, such as for robotic affordances, and information retrieval from search engines and Wikipedia.Going a step further, using external control algorithms as scaffolding (such as search or reinforcement learning), dynamic prompts can use the LLMs to perform complex and dynamic reasoning patterns.Note that the reasoning control is now outside the core LLM: an external control algorithm, on top of in-context-learning, dynamically generating prompts for the LLM.</p>
<p>At this point, it is interesting to note the confluence of two schools of thought in artificial intelligence: symbolic and connectionist. 5Search and reinforcement learning are rooted in the symbolic tradition, while LLMs and deep learning are rooted in the connectionist tradition.The literature in this survey combines the two traditions.Higher performance reasoning is created with a (symbolic) searcher/learner on top of a (connectionist) LLM.In other fields similar combinations can be seen (for example, AlphaFold [17,67] and retrosynthesis of molecules [134]).The LLM helps ground symbolic reasoning methods in language; symbolic methods help create prompts that let the LLM perform dynamic reasoning. 5Reasoning and planning have been studied since the start of artificial intelligence, starting with logic and reasoning [105], search algorithms in puzzles and board games [77,115,116], robot planning [41], classical machine learning such as decision trees and support vector machines [13,33,43], through knowledge representation and the semantic web [157].Ever since the success of the connectionist approach [15,48,81] (deep learning, including LLMs) researchers have tried to join the two approaches [144,152,168].</p>
<p>Manuscript submitted to ACM</p>
<p>We note that benchmarks such as GSM8K have been central for the progress of the field, and that while reasoning started with math word problems, the field has extended to robotics, autonomous agents, games, and most emphatically computer code.Formal languages play an important role in the intermediate multi-step reasoning chains.Prompt engineering and prompt control play a crucial role in the reasoning that we have seen in this survey.Models can be instructed to write their own reasoning prompts; however, such Auto-GPT or Auto-CoT prompts are prone to feedback-loop problems, and need careful evaluation, verification, and grounding in the real world, to prevent degeneration into a hallucinatory world of their own.Models can also be instructed to interact with the world, and become the tool of external scaffolding that evaluates, controls and improves the prompts [118].Some of what we experience as reasoning by the LLM, is controlled by the prompt or the scaffolding algorithm.Studies into in-context reinforcement learning aim to answer the question if prompt learning is able get the LLM to create a prompt to exhibit dynamic reasoning by itself [35,82,108,133].</p>
<p>Some studies on symbolic planning are critical on the abilities of LLMs [156], and show examples of planning failures, arguing that LLMs are better used to improve heuristic elements of traditional planners, such as PDDL [69], to strengthen traditional symbolic planning approaches.</p>
<p>Other works study the dangers of the size of LLMs.Bender et al. [7] mention the environmental risks associated with the large computational training demands, as well as the difficulty of understanding the training data, for example in the context of bias.Furthermore, there are ethical, legal, and copyright concerns regarding the data that LLMs are trained on.Finally, to prevent putting too much trust in the outcome of LLMs, we should understand their failure modes better, such as the well-publicized problems of hallucination (inventing facts that look right but are not).Here, mechanistic interpretability can be used to explore LLM representations and understand where they go wrong [9,20,124].</p>
<p>Some of the names of the approaches surveyed in this paper are suggestive of self-awareness and self-reflective capabilities.True (human) self-reflection, or metacognition, is still largely outside the capabilities of current LLMs.</p>
<p>LLMs can be prompted to reason, to take small steps, to self-evaluate, and their search process can be controlled by an external algorithm.The self-reflective type of "intelligence" is written into the prompt by the prompt engineer or the control algorithm.We are unaware of any LLM that has been made to reflect on, or even control, its reasoning processes, controlling how many reasoning steps it should take, or limiting its reasoning once the answer had become good enough.True self-reflection remains future work, although some steps have been taken, as we will discuss next.</p>
<p>Reasoning towards Metacognition.</p>
<p>Human thought exhibits the ability to reason about self, we are able to think about our own thinking processes.Metacognition studies this phenomenon [161].Prompted by the success of Chain-of-thought and related works, metacognition has also been studied in the context of LLMs [154].</p>
<p>Many reasoning approaches highlight self-reflective aspects in their names and in how they work.The prompts that prompt the models to reason are being improved with the outcome of the reasoning process, and in Buffer-of-thoughts thought-templates are used that are derived from other reasoning processes.Wang and Zhao [171] study Metacognitiveprompting.Inspired by Chain-of-thought and Self-consistency, they create manually designed prompts to increase the understanding of language models.Another work, again inspired by Chain-of-thought and Self-consistency, connects psychology and LLMs.Didolkar et al. [36] study metacognitive capabilities of LLMs in mathematical problem solving, both on GSM8K and on the harder MATH problems [53].First, the model is prompted to find a skill name for each Manuscript submitted to ACM problem instance in the dataset.For 7000 instances of GSM8K, 500 skill names were found by the model.Next, these 500 names are clustered down to 22 skills.They find that by using the names of these 22 skills in Chain-of-thought-like prompts, more problems are solved than with standard Chain-of-Thought/Self-consistency/Program-aided-language prompts.Examples of the 22 skill names are multiplication-and-addition, basic-arithmetic, subtraction, and algebra.</p>
<p>Interestingly, the authors find that the skill exemplar repository that is trained on a strong model (GPT-4), also downtranslates to a weak model (GPT-3).The performance of the weaker model benefits from the skill-name-enhanced prompts.Begus et al. [5] propose a program for the study of metalinguistic abilities of LLMs.LLMs struggle with analogies [83,100] and analogical reasoning.Metacognitive reasoning with LLMs is still in its early stages.</p>
<p>LLMs that Generate Computer code</p>
<p>A persistent thread in the results in this survey is that LLMs are good at generating formal languages, such as PDDL, logic, math equations, and Python.The agentic approach in which LLMs are combined with external tools such as interpreters or debuggers often yields good performance [118].LLMs can be used in different modes, which we will now discuss.</p>
<p>When LLMs are used in direct mode, a prompt is provided with an instruction, and an answer.This is the mode in which most casual users use an LLM.</p>
<p>LLMs can also be used in algorithmic mode, or Chain-of-thought mode, where an extra element is added to the prompt that gives an example of which steps to take to arrive at the answer [135].[169] show that the transformer architecture can be trained such that search and reinforcement learning algorithms can be executed in-context.</p>
<p>LLMs do not preserve state between calls, and in order to allow for a natural dialogue to occur, chatbots such as ChatGPT insert the most recent history of preceding calls into each prompt before the model is called [16].For harder strategic reasoning tasks more advanced state management solutions are necessary, that call the LLM in an explicit external mode [187,188].Approaches such as Tree-of-thoughts use the LLM in a stateless functional way, with the prompt containing the instructions, not unlike a natural language variant of a functional programming language such as LISP or Haskell.</p>
<p>Finally, an LLM can be used in code generation mode.The prompt specifies a problem, and the LLM is asked to generate computer code (such as Python or PDDL), to circumvent the knowing-doing gap [108].The code may contain variables that manage the state correctly.The code can be executed directly, or first be checked and improved by a programmer.</p>
<p>We expect that the use LLMs as code generators, and prompt-engineering as natural-language-programming, will continue to grow.</p>
<p>Research Agenda</p>
<p>At the end of this discussion, we list promising topics for future work.Reasoning with LLMs is an active field of research.</p>
<p>It brings together elements of symbolic reasoning, connectionism, natural language, autonomous agents, affective reasoning [14] and metacognition.First we discuss current topics for the field of LLM-reasoning itself, then we discuss more general machine learning topics that are important for progress in LLM-reasoning, and finally we discuss more longer term, fundamental topics.</p>
<p>Specific research topics for reasoning with LLMs are:</p>
<p>Manuscript submitted to ACM</p>
<p>• In Context Reinforcement Learning-Search control beyond greedy search is often implemented as an external reinforcement learning algorithm.Is it possible to incorporate the control stage of the reasoning pipeline into one static prompt, for implicit reasoning or in-context reinforcement learning (ICRL)?Studies indicate that models can be trained for decision making [35,82], but general LLMs struggle to perform ICRL well [108,129,133,159].• Code-Progress in reasoning using formal languages and computer code has been quite promising.GitHub</p>
<p>Copilot is a success.Further integration of LLM-reasoning with software engineering tools is a promising area of research that can have a large practical impact on how software is written.</p>
<p>• Grounding-Reasoning in LLMs has been successfully applied in autonomous agents, robotics, and games.A challenge is the grounding of the reasoning process in the environment.Retrieval augmented generative methods (RAG) can help LLMs to actively find new information when the reasoning outcome is uncertain.RAG, search, and agentic LLMs, are also active areas of research [34,118,162].</p>
<p>Generic topics in machine learning that also influence prompt-based reasoning research are:</p>
<p>• Benchmarks-Progress in LLMs depends on the availability of the right benchmarks.As the field has progressed beyond math word problems, other benchmarks become prevalent, with more difficult and diverse tasks.</p>
<p>• Faithfulness-Our theoretical understanding of prompt-based reasoning with LLMs is incomplete.The research on faithfulness highlights one example of our lack of understanding.In general, more insight into implicit reasoning [84] and the working of multi-step in-context learning in LLMs is dearly needed.</p>
<p>• Smaller language models-Efficiency is an important element for wide adoption of language models.Smaller models have many advantages over larger models: a smaller environmental footprint, more accessible to people with fewer computational resources, and the possibility to finetune them.Models may have fewer parameters, or the parameters may be quantized with fewer bits.Unfortunately, with a smaller number of parameters, the models also have less reasoning power.It is important for future work to investigate this trade-off.</p>
<p>For longer term future work, the following more fundamental questions are important:</p>
<p>• Symbolic and Connectionist Computation-How can we further improve LLM-reasoning: how can LLMs benefit from reasoning prompts based on the symbolic AI literature, and how can LLMs help ground symbolic reasoning in language?</p>
<p>• Multimodal World-Models with Embodied Grounding-How can an LLM maintain a unified, continually updated representation that integrates text, vision, audio, and sensorimotor feedback, enabling it to reason over events across modalities, and refine its world-model through closed perception-action loops in real-world environments?• Norm-Sensitive and Value-Aligned Reasoning-How do we represent, adapt, and audit diverse cultural norms and ethical values within the reasoning process, ensuring that each step of an LLM's Chain-of-thought respects context-dependent moral constraints while remaining transparent and verifiable?• Reasoning in other languages than English-The majority of the research into LLM reasoning is for English.</p>
<p>Although there are efforts in creating benchmarks to test LLM capabilities in other languages 6 , these contain mainly NLP tasks such as question answering (QA) and natural language inference (NLI).In addition, challenges related to low-resource languages (languages for which very limited training data is available) have not been addressed yet.</p>
<p>• Reasoning in specialized domains-In an effort to guide development and evaluation of new methods, research in AI has a strong focus on benchmarking: standardized datasets with a limited set of problems that are realistic to evaluate.In the real world, however, reasoning problems also occur in more challenging contexts.A few examples are: legal reasoning for the interpretation of case law or writing contracts; scientific reasoning for advancing scientific fields; and medical reasoning for AI-assisted diagnostics.</p>
<p>Conclusion</p>
<p>When large language models of sufficient size are prompted with examples, they can perform few-shot learning in-context, providing an answer at inference time, without retraining model parameters.Although they achieve good performance on language tasks, simple prompting methods do not perform well on reasoning tasks-tasks that humans typically solve in a step by step fashion.Chain-of-thought is an in-context prompting method to guide an LLM to "think step by step." Chain-of-thought was originally developed for solving grade school math word problems.GSM8K, the most popular reasoning benchmarks in this survey, contains 8500 grade school math word problems.With older LLMs such as GPT-3, reasoning approaches show an improvement of 20-50% points over standard prompting methods.This success has spawned many new reasoning approaches.A potential problem in the Chain-of-thought method is that errors may accumulate over multiple reasoning steps.Prompting the LLM to reformulate problems in Python code (or another formal language) can successfully reduce the error rate.The success of reasoning methods has attracted more applications, and with them, benchmarks are diverging.In the field of autonomous agents and robotic action, good performance has been achieved by grounding reasoning answers in the environment and the physical constraints of robotic movement.</p>
<p>Many current LLMs have adopted Chain-of-thought approaches.In this survey we categorize the approaches on how they generate, evaluate, and control the reasoning steps.Inference-time reasoning methods are also used for LLM finetuning.Reinforcement learning with verifiable rewards (RLVR) and group relative policy optimization (GRPO) use inference-time reasoning results to augment finetuning for reasoning tasks.</p>
<p>For complex tasks the number of reasoning steps that is generated may be large.The size of the multi-step reasoning space can be controlled dynamically by external search or reinforcement learning algorithms, often in the form of a wrapper of Python code that generates LLM-prompts.Many of the names of the approaches in this survey suggest a link to metacognition (Reflexion, Self-refine, Self-improvement, Inner-monologue)-the act of thinking about one's own thought process.The first preliminary experiments of language models that reason about their reasoning skills have appeared.</p>
<p>The field of reasoning with LLMs is quite new, and theoretical understanding is lacking in important areas, such as faithful reasoning (models may sometimes find the right answer using an incorrect reasoning chain).LLMs hallucinate and suffer from bias, and their use poses ethical and societal dangers.Self-verification methods have been developed to reduce error-accumulation, and retrieval augmentation methods (RAG) ground LLM output directly in sources such as Wikipedia.However, ethical and societal dangers remain, especially since also reflection is not error-free.</p>
<p>Although prompt-based learning allows few-shot learning at inference time, the computational needs of LLM pretraining and finetuning are still high, hence the interest in small language models.Reasoning skills that work in large models can often be distilled to small models.LLM-reasoning is an active field of research that shows great progress.Based on current limitations and open questions we provide a research agenda highlighting opportunities for further progress.Among the items mentioned are reinforcement learning for finetuning and self-reflection, LLM agents that generate code for external tools, multimodal world models, value-aligned reasoning, and small language models.Manuscript submitted to ACM</p>
<p>Contexts that contain a few examples are said to perform few-shot learning.Contexts that contain only an instruction, with zero examples, are said to perform zero-shot learning.In-context learning takes place at inference time, after the computationally intensive training stages where parameters have been pretrained and finetuned, when the model is queried by the user to provide answers.No parameters are changed anymore with in-context learning.This is quite different from the common approach in supervised deep learning-or self-supervised deep learning-where large datasets are used during training to update model parameters with backward propagation in lengthy and costly training epochs [48].Indeed, in-context learning takes place fully at inference time, no parameters are trained, instead, learning now refers to adjusting the answers to the examples in the prompt and the internal knowledge acquired during training.</p>
<p>(3.1.1)hand-written prompt.Second, the prompt or prompts may come from a source that is external to the model, such as another model or dataset: (3.1.2) external knowledge-based prompt.Third, the model itself can be prompted to generate a (series of) prompt(s) to analyze the problem (3.1.3)model-generated prompt.As we will see, all three approaches have their advantages and disadvantages.</p>
<p>Fig. 1 .
1
Fig. 1.Taxonomy of LLM-Reasoning Approaches: Prompt Generation, Evaluation, and Control</p>
<p>3.1) greedy selection, which generates a step and then follows it, (3.3.2) ensemble strategy, which generates a set of possible next steps, and (3.3.3) a (reinforcement learning) search which generates multiple options for the steps, traversing a search tree with backtracking, controlling an exponential search space[182].</p>
<p>Fig. 5 .
5
Fig. 5. Reasoning structure of Simple prompting (a), Chain-of-thought (b), Self-Consistency (c), Tree-of-Thoughts (d) and Buffer of Thoughts (e); Prompts in (a) consist of an instruction, and, perhaps, few-shot examples of answers, providing no reasoning guidance; Chain-of-thought (b) guides the model with few-shot examples of reasoning steps; Self-Consistency (c) takes the majority vote of existing answers in the model; Tree-of-thoughts (d) uses an external prompt-optimizing algorithm to guide the model to perform a tree search over reasoning steps, exploring multiple alternatives; Buffer-of-thoughts (e) uses a problem distiller that stores high-level thoughts distilled from different problems in a meta-buffer, which is actively managed for capacity</p>
<p>Fig. 7 .
7
Fig. 7. Performance of Voyager in Minecraft [166]; Voyager performs well, reaching high scores by acquiring many tools</p>
<ol>
<li>
<p>3 . 1
31
What Can LLMs Do?With the right prompt, LLMs are able to solve many of the problems in grade school math word benchmarks and beyond.Prompt-based learning is able to perform reasoning tasks such as math word problems, robotic movement gaming, and Python code generation, at inference time, without expensive parameter training.</p>
</li>
<li>
<p>3 . 2
32
What Can LLMs Not Do?Now that grade school math word problems are largely solvable, harder reasoning benchmarks in other domains are appearing[1].Most of the reasoning capabilities exhibited by LLMs are due to the representational powers of the transformer architecture and how in-context learning is able to harness them.</p>
</li>
</ol>
<p>Table 1 .
1
[173]n approaches to few-shot learning, such as metalearning, do include training and finetuning of parameters to Accuracy of GPT-3 and Chain-of-thought on popular Math Word Problems benchmarks[173]
Manuscript submitted to ACM</p>
<p>Table 2
2lists the main papers of this survey. We show the domain they work on, the type of promptgeneration, the evaluation of the result, and the control method. These three categories of approaches-indicated bytheir Sections (3.1) generation, (3.2) evaluation, (3.3) control-are shown in the table as groups divided by horizontallines. The first group in the Table, from Scratchpad to Self-ask, focuses on creating a prompt that generates the reasoningsteps. The entries in the cells of this column are shown in bold, highlighting the focus of the approaches. The approachesin this group are the start of the field of LLM-reasoning. The Chain-of-thought approach is especially an inspiration formany works. The prompts are often written manually by the researchers for each problem; the steps are encoded inone prompt, and step control is greedy. There is no specific evaluation of the steps, other than comparing results tothe benchmark. The Scratchpad approach is special in that it uses supervised learning, not prompt-learning; the workshowed that LLMs can generate internal reasoning steps by supervised learning, paving the way for in-context works.The second group, from Self-verification to Self-taught-reasoner, focuses on evaluation of the reasoning steps in theprompt. This column is shown in bold in the table. The approaches in this group aim to improve the Chain-of-thoughtresults by reducing error accumulation that occurs when multiple steps are taken in a reasoning chain. A variety ofstep control methods is used by these approaches, which is discussed in more detail later. Note that not all approachesManuscript submitted to ACM</p>
<p>Table 2 .
2
Taxonomy of approaches: Generation, Evaluation, and Control.Reported benchmark results: '=' is absolute score, '+' is improvement to a baseline
ApproachDomain(3.1) Step generation (3.2) Step evaluation (3.3) Step control ResultScratchpad [106]math wordhand-wr/superv-greedy/1promptPolyEval +19%, Python +21%Chain-of-thought [173]math wordhand-written-greedy/1promptGSM8K +39%, SVAMP +10%, ASDiv +2%, AQuA +11%,MAWPS +14%, CSQA +1.8%, StrategyQA +0.2%ZS-CoT [75]math wordhand-written-greedy/1promptMultiArith =89%, GSM8K =70%Auto-CoT [193]math wordmodel-generated-clusteringMultiArith +0.3%, GSM8K +1%, AddSub +3.5%,AQuA +0.7%, SingleEq +0.4%, SVAMP +0.6%,CSQA +1%, StrategyQA +0%, Letter +0.7%, Coin +2.7%Complexity [44]math wordhand-writtenself-consistencygreedy/1promptGSM8K +7%, MultiArith +3%, Penguins +3%Self-ask [119]math wordext knowledgeLLMmulti-hop q.Bamboogle =60%, 2Wiki =40%, Musique =15%Self-verification [176]math wordhand-writtenback-verifyensembleGSM8K +4%, SingleEq +2%, AddSub +4%,MultiArith +3%, AQuA +3%, SVAMP +1%Self-consistency [170]math wordhand-writtenmajorityensembleGSM8K +18%, SVAMP +11%, AQuA +12%,StrategyQA +6%, ARC-c +4%Codex [18]code-tool-based-HumanEval =70%Self-debugging [22]codehand-writtentool-basedgreedySpider +9%, MBPP +12%, TransCoder +12%Fun-search [126]codehand-writtentool-basedevolutionary algcap set 8 =512LLaMEa [158]codehand-writtentool-basedevolutionary algBBOB +11%MathPrompter [64]mathhand-writtentool-basedensembleMultiArith =92%Program-of-thoughts [19]math wordhand-wr, CodexPython+Consist.split reason/cmput GSM8K =71%, SVAMP =85%, ASDIV =85%,AddSub =92%, MultiArith = 99%Program-aided-lang [45]math wordhand-wr, CodexNLP/PythonensembleGSM8K =72%, SVAMP =79%, ASDIV =79%,SingleEQ =96%, SingleOP =94%, AddSub = 92%,MultiArith = 99%, Penguins = 93%Refiner [113]math wordfinetunecritic modelgen/crit feedbackSVAMP =72%, GSM8K =78%Self-correction [174]math wordfinetunecorrector modelgen/corr feedbackMathProgSynth =24%, LexConstrGen =98%,ToxicityControl =0.0%Self-improvement [58]math wordfinetuneself-assessmentCoT/consistencyGSM8K =82%, DROP =83%, ARC-c =90%,OpenBookQA =94%, ANLI-A3 =68%Say-can [2]robotmodel-generatedexternal modelgreedyKitchen =31%Inner-monologue [61]robothand-writtenvariousgreedyTableTop =90%, Kitchen =60%</p>
<p>[106].Example of input and target for supervised learning on a long addition problem of adding two numbers.The carry is recorded in the C: digit.Comments (after #) are not part of the learning target (adapted from[106])
Input2 9 + 5 7OutputTarget:<scratch>2 9 + 5 7 , C: 02 + 5 , 6 C: 1 # added 9 + 7 = 6 carry 1, 8 6 C: 0 # added 2 + 5 + 1 = 8 carry 00 8 6</scratch>8 6InputInputInputQ: Lisa has 23 apples.If she ate 20 and bought 6 more,How many apples does she have?Let's think step by step.OutputOutputOutputA: The answer is 27.A: Lisa started with 23 apples.A: Lisa started with 23 apples.She ate 20, so she has 23-20She ate 20, so she has 23-20=3 apples left. She bought 6,=3 apples left. She bought 6,so 3 + 6 = 9. The answer is 9.so 3 + 6 = 9.Manuscript submitted to ACM
Q: Roger has 5 tennis balls.He buys two cans of 3 balls.How many tennis balls does he have now?A: The answer is 11.Q: Lisa has 23 apples.If she ate 20 and bought 6 more, How many apples does she have?Q: Roger has 5 tennis balls.He buys two cans of 3 balls.How many tennis balls does he have now?A: Rogers started with 5 balls. 2 cans of 3 balls each is 6 balls.5 + 6 = 11.The answer is 11.Q: Lisa has 23 apples.If she ate 20 and bought 6 more, How many apples does she have?Input Output A: Lisa started with 23 apples.apples = 23 She ate 20 and the bought 6 eaten = 20; bought = 6 anwer = apples -eaten + bought Q: Roger has 5 tennis balls.He buys two cans of 3 balls.How many tennis balls does he have now?A: Rogers started with 5 balls.tennis_balls = 5 2 cans of 3 balls each is bought_balls = 2 * 3 tennis balls.The answer answer = tennis_balls + bought_balls Q: Lisa has 23 apples.If she ate 20 and bought 6 more, How many apples does she have?</p>
<p>Faithfulness studies tell us more about how models reason.Further surveys on this topic are Chuang et al.[29], Luo et al. [93], Mondorf and Plank [102], Paul et al. [114], 4.2.2Scaling.The emergent abilities of LLMs have prompted research into the nature of scaling and reasoning with</p>
<p>We show the approaches in the Figure in their main category only. Some approaches show innovations in two categories, and are shown twice. (Since all approaches have a generation, an evaluation, and a control aspect, all could in principle occur three times, and all three columns can be found in Table
).Manuscript submitted to ACM
The Chain-of-thought idea is about prompt generation, not about the evaluation or the search control of the reasoning steps. Hence, in Table2Chain-of-thought is labeled as greedy without an evaluation.
In addition to the languages discussed in this survey, there is a long tradition in AI of using formal reasoning based on logic. The interest in Chain-ofthought has also stimulated research into external symbolic logic solvers that are called by the LLM[51,109,132,145,148,183,197], or where LLMs transform the problem in a logic problem. The use of LLMs for generating formal languages is further discussed in
section 4.4.
A similarly named approach is Graph-of-thoughts[11]. Graph-of-thoughts allows more general reasoning graphs, providing a formal framework, where the different elements can then be specified manually.
Manuscript submitted to ACM
https://github.com/NaiveNeuron/awesome-multilingual-llm-benchmarks Manuscript submitted to ACM
AcknowledgementsWe thank the anonymous reviewers for their valuable suggestions that have considerably improved the article.
. Nye, 106Chain-of-Thought</p>
<p>Auto-CoT. Wei, 173Buffer-of-ThoughtsZhang et al. [193</p>
<p>Self-Ask. Yang , 185Press et al. [119</p>
<p>Self-Assessment Tool-based Evaluation External Model Validation Self-Verification. </p>
<p>. Weng, 176Self-Debugging</p>
<p>. Chen , Fun-Search22</p>
<p>. Romera-Paredes, 126Program-of-Thoughts</p>
<p>Program-Aided-Language. Chen , 1945Self-Corrector. Welleck et al. [174] Self-Improvement</p>
<p>. Huang, 58</p>
<p>. Saycan; Ahn, 2] Inner Monologue; Huang et al. [61</p>
<p>Data Augmentation: Self-Taught Reasoner. Zelikman, 191</p>
<p>Greedy Selection Ensemble Strategy Reinforcement Learning Complexity (Fu et al. 2022) Least-to-Most (Zhou et al. 2022) Self-Consistency. Wang , 170</p>
<p>. Weng, 176Progressive-Hint</p>
<p>. Zheng, 195Self-Refine</p>
<p>. Madaan, 95Tree-of-Thoughts</p>
<p>. Yao, 188Buffer-of-Thoughts</p>
<p>Algorithm-of-thoughts. Yang , 185</p>
<p>Large language models for mathematical reasoning: Progresses and challenges. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, arXiv:2402.001572024Association for Computational LinguisticsarXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, Conference on Robot Learning. 2022</p>
<p>Mathqa: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American Chapterthe Association for Computational Linguistics2019</p>
<p>Solid: Selfseeding and multi-intent self-instructing llms for generating intent-aware information-seeking dialogs. Arian Askari, Roxana Petcu, Chuan Meng, Mohammad Aliannejadi, Amin Abolghasemi, Evangelos Kanoulas, Suzan Verberne, arXiv:2402.11633Association for Computational Linguistics. 2024arXiv preprint</p>
<p>Large linguistic models: Investigating llms' metalinguistic abilities. Gasper Begus, Maksymilian Dabkowski, Ryan Rhodes, IEEE Transactions on Artificial Intelligence. 2025</p>
<p>Dynamic programming. Richard Bellman, Science. 15337311966</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Mechanistic interpretability for ai safety-a review. Leonard Bereska, Efstratios Gavves, arXiv:2404.14082Transactions on Machine Learning Research. 2024arXiv preprint</p>
<p>The reversal curse: Llms trained on a is b fail to learn b is a. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans, International Conference on Learning Representations. 2024</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Exploring and benchmarking the planning capabilities of large language models. Bernd Bohnet, Azade Nova, Aaron T Parisi, Kevin Swersky, Katayoon Goshvadi, Hanjun Dai, Dale Schuurmans, Noah Fiedel, Hanie Sedghi, arXiv:2406.130942024arXiv preprint</p>
<p>Random forests. Leo Breiman, Machine learning. 452001</p>
<p>Fine-grained affective processing capabilities emerging from large language models. Joost Broekens, Bernhard Hilpert, Suzan Verberne, Kim Baraka, Patrick Gebhard, Aske Plaat, 2023 11th Intl Conf on Affective Computing and Intelligent Interaction (ACII). IEEE2023</p>
<p>Elephants don't play chess. Rodney A Brooks, Robotics and autonomous systems. 61-21990</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Improved prediction of protein-protein interactions using alphafold2. Patrick Bryant, Gabriele Pozzati, Arne Elofsson, Nature communications. 13112652022</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.12588Transactions of Machine Learning Research. 2023arXiv preprint</p>
<p>How does chain of thought think? mechanistic interpretability of chain-of-thought reasoning with sparse autoencoding. Xi Chen, Aske Plaat, Niki Van Stein, arXiv:2507.229282025arXiv preprint</p>
<p>Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, Quoc V Le, International Conference on Learning Representations. 2019</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Semantically-aligned equation generation for solving and reasoning math word problems. Ting-Rui Chiang, Yun-Nung Chen, 2019Association for Computational Linguistics</p>
<p>Embodied cot distillation from llm to off-the-shelf agents. Wonje Choi, Kyung Woo, Minjong Kim, Honguk Yoo, Woo, Forty-first International Conference on Machine Learning. </p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023Manuscript submitted to ACM</p>
<p>Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Sergey Quoc V Le, Yi Levine, Ma, arXiv:2501.17161Sft memorizes, rl generalizes: A comparative study of foundation model post-training. 2025arXiv preprint</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu, 2024Association for Computational Linguistics</p>
<p>Navigate through enigmatic labyrinth a survey of chain of thought reasoning: Advances, frontiers and future. Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Faithlm: Towards faithful explanations for large language models. Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Ruixiang Tang, Fan Yang, Mengnan Du, Xuanting Cai, Xia Hu, arXiv:2402.046782024arXiv preprint</p>
<p>Nathan Cloos, Meagan Jens, Michelangelo Naim, Yen-Ling Kuo, Ignacio Cases, Andrei Barbu, Christopher J Cueva, arXiv:2407.13729Baba is ai: Break the rules to beat the benchmark. 2024arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>American invitational mathematics examination-aime. Maa Codeforces, 2024, 2024</p>
<p>Support-vector networks. Corinna Cortes, Vladimir Vapnik, Machine learning. 201995</p>
<p>Promptagator: Few-shot dense retrieval from 8 examples. Zhuyun Dai, Y Vincent, Ji Zhao, Yi Ma, Jianmo Luan, Jing Ni, Anton Lu, Kelvin Bakalov, Keith B Guu, Ming-Wei Hall, Chang, International Conference on Learning Representations. 2023</p>
<p>Can Demircan, Tankred Saanum, K Akshay, Marcel Jagadish, Eric Binz, Schulz, arXiv:2410.01280Sparse autoencoders reveal temporal difference learning in large language models. 2024arXiv preprint</p>
<p>Metacognitive capabilities of llms: An exploration in mathematical problem solving. Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, Sanjeev Arora, arXiv:2405.122052024arXiv preprint</p>
<p>A survey on in-context learning. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, 2023Association for Computational Linguistics</p>
<p>Kimi k1.5: Scaling reinforcement learning with llms. Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, arXiv:2501.125992025arXiv preprint</p>
<p>Diversity is all you need: Learning skills without a reward function. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine, International Conference on Learning Representations. 2019</p>
<p>Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Zhu, Advances in Neural Information Processing Systems. 202235</p>
<p>Strips: A new approach to the application of theorem proving to problem solving. E Richard, Nils J Fikes, Nilsson, Artificial intelligence. 23-41971</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, International conference on machine learning. PMLR2017</p>
<p>Machine learning: the art and science of algorithms that make sense of data. Peter Flach, 2012Cambridge university press</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Peizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, Furu Wei, arXiv:2406.11698Meta reasoning for large language models. 2024arXiv preprint</p>
<p>Prompt engineering with chatgpt: a guide for academic writers. Louie Giray, Annals of biomedical engineering. 51122023</p>
<p>Deep learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016MIT press</p>
<p>Minillm: Knowledge distillation of large language models. Yuxian Gu, Li Dong, Furu Wei, Minlie Huang, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, arXiv:2209.00840Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Black-box optimization benchmarking of newuoa compared to bipop-cma-es: on the bbob noiseless testbed. Nikolaus Hansen, Raymond Ros, Proceedings of the 12th annual conference companion on Genetic and evolutionary computation. the 12th annual conference companion on Genetic and evolutionary computation2010</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Advances in Neural Information Processing Systems. 2021</p>
<p>Scaling laws for autoregressive generative modeling. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Prafulla Tom B Brown, Scott Dhariwal, Gray, arXiv:2010.147012020arXiv preprint</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, Advances in Neural Information Processing Systems. 2022Manuscript submitted to ACM</p>
<p>Meta-learning in neural networks: A survey. Timothy Hospedales, Antreas Antoniou, Paul Micaelli, Amos Storkey, IEEE transactions on pattern analysis and machine intelligence. 202144</p>
<p>Lora: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, 2023In Association for Computational Linguistics</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, Assoc for Computational Linguistics. 2023</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, arXiv:2311.05232ACM Transaction on Information Systems. 4322025arXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Conference on Robot Learning. 2022</p>
<p>O1 replication journey-part 2: Surpassing o1-preview through simple distillation. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, Pengfei Liu, arXiv:2411.164892024arXiv preprint</p>
<p>A survey of deep meta-learning. Mike Huisman, Jan N Van Rijn, Aske Plaat, Artificial Intelligence Review. 5462021</p>
<p>Mathematical reasoning using large language models. Shima Imani, H Du, Shrivastava, Mathprompter, 2023Association for Computational Linguistics</p>
<p>Quantization and training of neural networks for efficient integer-arithmetic-only inference. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, Dmitry Kalenichenko, Computer vision and pattern recognition. 2018</p>
<p>The impact of reasoning step length on large language models. Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, 2024Association for Computational Linguistics</p>
<p>Highly accurate protein structure prediction with alphafold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, nature. 59678732021</p>
<p>Reinforcement learning: A survey. Leslie Pack, Kaelbling Michael L Littman, Andrew W Moore, Journal of artificial intelligence research. 41996</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, Anil Murthy, International Conference on Machine Learning. 2024</p>
<p>When can llms actually correct their own mistakes? a critical survey of self-correction of llms. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, Rui Zhang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Game of thoughts: Iterative reasoning in game-theoretic domains with large language models. Benjamin Kempinski, Ian Gemp, Kate Larson, Marc Lanctot, Yoram Bachrach, Tal Kachman, Proc. of the 24th International Conference on Autonomous Agents and Multiagent Systems. of the 24th International Conference on Autonomous Agents and Multiagent Systems2025</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Findings of the 2022 conference on machine translation (wmt22). Tom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)2022</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Mawps: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, 2016In 2016 conference of the north american chapter of the association for computational linguistics: human language technologies</p>
<p>Algorithms and theory of computation handbook, 22-17, chapter Artificial intelligence search algorithms. Richard E Korf, 1999Citeseer</p>
<p>Can large language models explore in-context?. Akshay Krishnamurthy, Keegan Harris, Dylan J Foster, Cyril Zhang, Aleksandrs Slivkins, arXiv:2403.153712024arXiv preprint</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, arXiv:2411.15124Pushing frontiers in open language model post-training. 20243arXiv preprint</p>
<p>Measuring faithfulness in chain-of-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, arXiv:2307.137022023arXiv preprint</p>
<p>Deep learning. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, Nature. 52175532015</p>
<p>Supervised pretraining can learn in-context reinforcement learning. Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, Emma Brunskill, Advances in Neural Information Processing Systems. 202336</p>
<p>Evaluating the robustness of analogical reasoning in large language models. Martha Lewis, Melanie Mitchell, arXiv:2411.142152024arXiv preprintManuscript submitted to ACM</p>
<p>Jindong Li, Yali Fu, Li Fan, Jiahong Liu, Yao Shu, Chengwei Qin, Menglin Yang, Irwin King, Rex Ying, arXiv:2509.02350Implicit reasoning in large language models: A comprehensive survey. 2025arXiv preprint</p>
<p>Prompt distillation for efficient llm-based recommendation. Lei Li, Yongfeng Zhang, Li Chen, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. the 32nd ACM International Conference on Information and Knowledge Management2023</p>
<p>Explanations from large language models make small reasoners better. Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Association for the advancement of artificial intelligence. 2024</p>
<p>Making language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, 2023Association for Computational Linguistics</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 2017Association for Computational Linguistics</p>
<p>Algorithms for sequential decision-making. Michael Lederman, Littman , 1996Brown University</p>
<p>Evolution of heuristics: Towards efficient automatic algorithm design using large language model. Fei Liu, Tong Xialiang, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, Qingfu Zhang, Forty-first International Conference on Machine Learning. 2024</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Reasoning on graphs: Faithful and interpretable large language model reasoning. Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan, International Conference on Learning Representations. 2024</p>
<p>Faithful chain-of-thought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, 2023Association for Computational Linguistics</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202336</p>
<p>Teaching small language models to reason. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn, 2023Association for Computational Linguistics</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, Annual Meeting of the Association for Computational Linguistics. 2020</p>
<p>Ganesh Venkatesh, et al. Mixed precision training. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, International Conference on Learning Representations. 2018</p>
<p>Large language models: A survey. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, arXiv:2402.061962024arXiv preprint</p>
<p>The emergence of understanding in a computer model of concepts and analogy-making. Melanie Mitchell, Douglas R Hofstadter, Physica D: Nonlinear Phenomena. 421-31990</p>
<p>A survey of in-context reinforcement learning. Amir Moeini, Jiuqi Wang, Jacob Beck, Ethan Blaser, Shimon Whiteson, Rohan Chandra, Shangtong Zhang, arXiv:2502.079782025arXiv preprint</p>
<p>Beyond accuracy: Evaluating the reasoning behavior of large language models-a survey. Philipp Mondorf, Barbara Plank, Conference on Language Modeling. Philadelphia2024</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, arXiv:2501.19393Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. 2025arXiv preprint</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, 2018Association for Computational Linguistics</p>
<p>Computer simulation of human thinking: A theory of problem solving expressed as a computer program permits simulation of thinking processes. Allen Newell, Herbert A Simon, Science. 13434951961</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, ICLR 2022 Workshop DL4C. 2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kuciński, Lerrel Pinto, Rob Fergus, arXiv:2411.13543Benchmarking agentic llm and vlm reasoning on games. 2024arXiv preprint</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>The lambada dataset: Word prediction requiring a broad discourse context. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan, Raffaella Pham, Sandro Bernardi, Marco Pezzelle, Gemma Baroni, Raquel Boleda, Fernández, 2016Association for Computational Linguistics</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002Manuscript submitted to ACM</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Bhattamishra, Goyal, 2021In Association for Computational Linguistics</p>
<p>Refiner: Reasoning feedback on intermediate representations. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, 2024Association for Computational Linguistics</p>
<p>Making reasoning matter: Measuring and improving faithfulness of chain-of-thought reasoning. Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings, 2024Association for Computational Linguistics</p>
<p>Re: search &amp; Re-search. Aske Plaat, Research, 1996Erasmus University RotterdamPhD thesis</p>
<p>Learning to play: reinforcement learning and games. Aske Plaat, 2020Springer Nature</p>
<p>Deep reinforcement learning. Aske Plaat, 2022SpringerSingapore</p>
<p>Agentic large language models, a survey. Aske Plaat, Max Van Duijn, Niki Van Stein, Mike Preuss, Kees Joost Peter Van Der Putten, Batenburg, arXiv:2503.23037Journal of Artificial Intelligence Research. 2025arXiv preprint</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, 2023Association for Computational Linguistics</p>
<p>The rl/llm taxonomy tree: Reviewing synergies between reinforcement learning and large language models. Moschoula Pternea, Prerna Singh, Abir Chakraborty, Yagna Oruganti, Mirco Milletari, Sayli Bapat, Kebei Jiang, arXiv:2402.01874Journal of Artificial Intelligence Research. 802024arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.114462021arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362024</p>
<p>A practical review of mechanistic interpretability for transformer-based language models. Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, Ziyu Yao, arXiv:2407.026462024arXiv preprint</p>
<p>Squad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 2016Association for Computational Linguistics</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Nature. 62579952024</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, 2015Association for Computational Linguistics</p>
<p>Unsupervised translation of programming languages. Marie-Anne Baptiste Roziere, Lowik Lachaux, Guillaume Chanussot, Lample, Advances in neural information processing systems. 202033</p>
<p>Lmact: A benchmark for in-context imitation learning with long multimodal demonstrations. Anian Ruoss, Fabio Pardo, Harris Chan, Bonnie Li, Volodymyr Mnih, Tim Genewein, arXiv:2412.014412024arXiv preprint</p>
<p>Ensemble learning: A survey. Omer Sagi, Lior Rokach, Wiley interdisciplinary reviews: data mining and knowledge discovery. 20188e1249</p>
<p>A systematic survey of prompt engineering in large language models. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.079272024Techniques and applications. arXiv preprint</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, International Conference on Learning Representations. 2023</p>
<p>Mastering board games by external and internal planning with language models. John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel Hennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, arXiv:2412.121192024arXiv preprint</p>
<p>Planning chemical syntheses with deep neural networks and symbolic ai. Marwin Segler, Preuss, Waller, Nature. 55576982018</p>
<p>Algorithm of thoughts: Enhancing exploration of ideas in large language models. Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, Ming Jin, arXiv:2308.103792023arXiv preprint</p>
<p>Improving neural machine translation models with monolingual data. Rico Sennrich, Barry Haddow, Alexandra Birch, 2016Association for Computational Linguistics</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Li, Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Generate &amp; rank: A multi-task framework for math word problems. Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, Qun Liu, 2021Association for Computational Linguistics</p>
<p>Chain of tools: Large language model is an automatic multi-tool learner. Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Suzan Verberne, Zhaochun Ren, arXiv:2405.165332024arXiv preprint</p>
<p>Learning to use tools via cooperative and interactive agents. Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, Zhaochun Ren, 2024Association for Computational Linguistics</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436Manuscript submitted to ACM</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, International Conference on Learning Representations. 2021</p>
<p>Cliport: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Conference on robot learning. PMLR2022</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, nature. 55076762017</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Learning to predict by the methods of temporal differences. Richard S Sutton, Machine learning. 31988</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT Press</p>
<p>Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, arXiv:2012.13048Proofwriter: Generating implications, proofs, and abductive statements over natural language. 2020arXiv preprint</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 2019Association for Computational Linguistics</p>
<p>Can ChatGPT replace traditional KBQA models? An in-depth analysis of the question answering performance of the GPT LLM family. Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, Guilin Qi, International Semantic Web Conference. Springer2023</p>
<p>Ul2: Unifying language learning paradigms. Yi Tay, Mostafa Dehghani, Xavier Vinh Q Tran, Jason Garcia, Xuezhi Wei, Hyung Won Wang, Siamak Chung, Dara Shakeri, Tal Bahri, Schuster, International Conference on Learning Representations. 2023</p>
<p>Temporal difference learning and td-gammon. Gerald Tesauro, Communications of the ACM. 3831995</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Language models for dialog applications. 2022arXiv preprint</p>
<p>Metacognition is all you need? using introspection in generative agents to improve goal-directed behavior. Jason Toy, Josh Macadam, Phil Tabor, arXiv:2401.109102024arXiv preprint</p>
<p>Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel Bowman, Advances in Neural Information Processing Systems. 202436</p>
<p>On the planning abilities of large language models-a critical investigation. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 362023</p>
<p>Handbook of knowledge representation. Frank Van Harmelen, Vladimir Lifschitz, Bruce Porter, 2008Elsevier</p>
<p>Llamea: A large language model evolutionary algorithm for automatically generating metaheuristics. Niki Van Stein, Thomas Bäck, arXiv:2405.20132IEEE Transactions on Evolutionary Computation. 2024arXiv preprint</p>
<p>Aske Fien Van Wetten, Max Plaat, Van Duijn, arXiv:2506.19095Baba is llm: Reasoning in a game with dynamic rules. 2025arXiv preprint</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>Metacognition and learning: Conceptual and methodological considerations. Bernadette Ham Marcel Vj Veenman, Peter Van Hout-Wolters, Afflerbach, Metacognition and learning. 12006</p>
<p>Is the search engine of the future a chatbot? Inaugural lecture. Suzan Verberne, 2024Leiden University</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, 2018Association for Computational Linguistics</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 322019</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, 2023Association for Computational Linguistics</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.16291Transactions of Machine Learning Research. 2024arXiv preprint</p>
<p>Offline reinforcement learning for llm multi-step reasoning. Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, Yi Wu, arXiv:2412.161452024arXiv preprint</p>
<p>Analysis of hyper-parameters for alphazero-like deep reinforcement learning. Hui Wang, Michael Emmerich, Mike Preuss, Aske Plaat, International Journal of Information Technology &amp; Decision Making. 22022023</p>
<p>Jiuqi Wang, Ethan Blaser, Hadi Daneshmand, Shangtong Zhang, arXiv:2405.13861Transformers learn temporal difference methods for in-context reinforcement learning. 2024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, International Conference on Learning Representations. 2023</p>
<p>Metacognitive prompting improves understanding in large language models. Yuqing Wang, Yun Zhao, 2024Association for Computational Linguistics</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Transactions of Machine Learning Research. 2022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, International Conference on Learning Representations. 2023</p>
<p>Thinkpatterns-21k: A systematic study on the impact of thinking patterns in llms. Pengcheng Wen, Jiaming Ji, Chi-Min Chan, Juntao Dai, Donghai Hong, Yaodong Yang, Sirui Han, Yike Guo, arXiv:2503.129182025arXiv preprint</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, 2023Association for Computational Linguistics</p>
<p>Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, arXiv:2410.13639A comparative study on reasoning patterns of openai's o1 model. 2024arXiv preprint</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim, 2024 Conference of the North American Chapter. Long Papers. the Association for Computational Linguistics20241</p>
<p>A survey on non-autoregressive generation for neural machine translation and beyond. Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, Tie-Yan Liu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>Chain-of-experts: When llms meet complex operations research problems. Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Jessica Yuan, Xiongwei Wang, Xiaojin Han, Tao Fu, Jia Zhong, Mingli Zeng, Song, 12th International Conference on Learning Representations. 2023</p>
<p>Self-evaluation guided beam search for reasoning. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, Michael Xie, Advances in Neural Information Processing Systems. 202436</p>
<p>Towards large reasoning models: A survey on scaling llm reasoning capabilities. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, arXiv:2501.096862025arXiv preprint</p>
<p>Faithful logical reasoning via symbolic chain-of-thought. Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, Wynne Hsu, arXiv:2405.183572024arXiv preprint</p>
<p>Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou, arXiv:2402.13116A survey on knowledge distillation of large language models. 2024arXiv preprint</p>
<p>Buffer of thoughts: Thoughtaugmented reasoning with large language models. Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, Advances in Neural Information Processing Systems. 2024</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202235</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations. 2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Natural language reasoning, a survey. Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang, 2023ACM Computing Surveys</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, 2018Association for Computational Linguistics</p>
<p>Star: Self-taught reasoner bootstrapping reasoning with reasoning. Eric Zelikman, Jesse Mu, Yuhuai Tony Noah D Goodman, Wu, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Can llm graph reasoning generalize beyond pattern memorization?. Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xiaochuang Han, Tianxing He, Yulia Tsvetkov, 2024In Association for Computational Linguistics</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, International Conference on Learning Representations. 2023</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, arXiv:2304.097972023arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>Analytical reasoning of text. Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining Chen, Jiahai Wang, Jian Yin, Ming Zhou, Nan Duan, Findings of the Association for Computational Linguistics: NAACL 2022. 2022</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1423 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1423</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1423</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-87821f5fd000d873beb5ba55689ba90def31a8c1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/87821f5fd000d873beb5ba55689ba90def31a8c1" target="_blank">Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> This work aims to relieve this Dreamer's bottleneck and enhance its performance by means of removing the decoder, and derives a likelihood- free and InfoMax objective of contrastive learning from the evidence lower bound of Dreamer.</p>
                <p><strong>Paper Abstract:</strong> In the present paper, we propose a decoder-free extension of Dreamer, a leading model-based reinforcement learning (MBRL) method from pixels. Dreamer is a sample- and cost-efficient solution to robot learning, as it is used to train latent state-space models based on a variational autoencoder and to conduct policy optimization by latent trajectory imagination. However, this autoencoding based approach often causes object vanishing, in which the autoencoder fails to perceives key objects for solving control tasks, and thus significantly limiting Dreamer's potential. This work aims to relieve this Dreamer's bottleneck and enhance its performance by means of removing the decoder. For this purpose, we firstly derive a likelihood- free and InfoMax objective of contrastive learning from the evidence lower bound of Dreamer. Secondly, we incorporate two components, (i) independent linear dynamics and (ii) the random crop data augmentation, to the learning scheme so as to improve the training performance. In comparison to Dreamer and other recent model-free reinforcement learning methods, our newly devised Dreamer with InfoMax and without generative decoder (Dreaming) achieves the best scores on 5 difficult simulated robotics tasks, in which Dreamer suffers from object vanishing.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1423.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1423.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreaming</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer with InfoMax and without generative decoder (Dreaming)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's proposed decoder-free variant of Dreamer that trains an RSSM latent dynamics model using an InfoMax / contrastive (Info-NCE) objective, with an independent linear forward dynamics for contrastive learning and random-crop augmentation; used end-to-end for model-based RL from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreaming (decoder-free Dreamer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent world model built on an RSSM (latent state z = (s,h) with deterministic GRU hidden h and probabilistic s) but trained without a pixel decoder; instead uses a discriminator p(z|x) implemented as an exponentiated bilinear similarity between latent z and CNN image features e, optimized with an Info-NCE (contrastive/InfoMax) objective across multi-step (overshooting) predictions. An independent linear forward model \u0394p(\u03b6) (z_t' = W_z z_{t-1} + W_a a_{t-1}) is used only for the contrastive (InfoMax) objective to regularize temporal smoothness; the expressive GRU-based RSSM dynamics p(z_t|z_{t-1},a_{t-1}) are retained for planning and policy optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RSSM-based) with contrastive/discriminative training</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Simulated robotic manipulation and control tasks (DeepMind Control Suite and two original manipulation tasks: UR5-reach and Connector-insert), plus other DMC locomotion and swing-up tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>No explicit numeric pixel-reconstruction fidelity reported for Dreaming since decoder is removed; training objectives relate to mutual information and predictive KL: (i) Info-NCE / InfoMax lower bound on mutual information I(x_t; z_{t-k}), (ii) multi-step KL objective (latent overshooting) between inference q and generative p. Task fidelity is reported indirectly via downstream task scores.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No direct prediction-error numbers reported; model fidelity inferred from task-level performance: e.g., Cup-catch (100K) Dreaming score 925 ± 48 vs Dreamer w/ likelihood 698 ± 350; Reacher-hard 868 ± 272 vs Dreamer 8 ± 33; Finger-turn-hard 752 ± 325 vs Dreamer 264 ± 368 (Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: learned latent embeddings preserve principal control features (positions, orientations) as seen in open-loop video prediction and reconstructions generated from latent states; authors claim the linear tilde-dynamics regularizes temporally-smooth latent space, making latent transitions more interpretable/structured. Not a fully interpretable (symbolic) model — still a neural latent model.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of open-loop video predictions and reconstructed context frames; qualitative assessment of latent-space smoothness induced by linear forward model; implicit inspection of object preservation vs 'object vanishing' through task success and visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No exact compute or parameter counts reported. Paper argues decoder training requires high-capacity CNN decoder with many parameters; Dreaming removes decoder training (reducing training cost and memory) but still trains RSSM (GRU) and discriminator (CNN + bilinear). Training/evaluation performed on standard RL training budgets reported in environment steps (e.g., 100K–500K).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to decoder-based Dreamer, Dreaming avoids decoder optimization (reducing parameter/compute needs for decoder) and achieves better sample efficiency on several difficult manipulation tasks; exact wall-clock or FLOP savings not reported. Paper reports improved sample efficiency in task scores at the same environment steps for several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported task scores (mean ± std) at given environment steps (Table I). Representative results: Cup-catch (100K) 925 ± 48 (Dreaming) vs 698 ± 350 (Dreamer w/ likelihood); Reacher-hard 868 ± 272 vs 8 ± 33; Connector-insert 629 ± 391 vs 169 ± 348. Dreaming outperforms Dreamer and some leading model-free baselines on the five difficult manipulation tasks where Dreamer suffers object vanishing; however, Dreaming underperforms Dreamer on several planar locomotion tasks (e.g., Walker-walk: Dreaming 469 ± 123 vs Dreamer 955 ± 19).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Dreaming's InfoMax contrastive training prioritizes task-relevant and discriminative visual features, which reduces 'object vanishing' and translates to substantially improved policy performance on tasks requiring small-object perception and fine pixel control. However, when task-relevant information is encoded in background texture (e.g., floors providing velocity cues in locomotion), contrastive objective and data augmentation can obscure necessary cues and reduce performance — indicating a case where reduced reconstruction fidelity (no decoder) harms task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs explicitly discussed: removing decoder (decoder-free InfoMax) improves perception of small, task-critical objects and sample efficiency on manipulation tasks but can hurt tasks that require background texture information (planar locomotion). The linear independent forward model enforces smooth temporally-consistent latents (benefit) but reduces capacity to model complex non-linear transitions in the contrastive path (design trade-off). Data augmentation (random crop) aids focusing on control-relevant features but can remove discriminative background cues needed for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: (1) Replace pixel decoder + reconstruction objective with InfoMax / Info-NCE contrastive objective derived from ELBO; (2) Use independent linear forward dynamics \u223c\u0394p for contrastive multi-step predictions (tilde p deterministic linear: z_t' = W_z z_{t-1} + W_a a_{t-1}); (3) Use overshooting (multi-step) NCE terms (K recommended = 3); (4) Use random-crop image augmentation to force invariance to irrelevant image shifts; (5) Keep expressive GRU-based RSSM dynamics for planning while using linear tilde dynamics only for contrastive loss.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically to decoder-based Dreamer (ELBO likelihood + KL), Dreamer w/ vanilla NCE, and leading model-free baselines (CURL, DrQ, RAD). Dreaming outperforms decoder-based Dreamer and MFRL baselines on five difficult manipulation tasks (Table I), but underperforms Dreamer on several locomotion tasks where background textures are informative. Vanilla NCE without the proposed linear tilde dynamics and data augmentation degraded performance versus the proposed multi-step InfoMax + linear tilde dynamics scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends: (1) use independent linear forward dynamics for contrastive learning rather than sharing expressive dynamics; (2) use random-crop augmentation; (3) use overshooting distance K = 3 (ablation shows K=3 effective); these constitute the optimal configuration for the authors' manipulation benchmarks where object vanishing is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1423.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1423.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination (Dreamer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL method that learns a recurrent latent state-space model (RSSM) with an autoencoding variational objective (decoder + reconstruction) and uses latent imagination (backpropagated Bellman errors on imagined trajectories) for policy optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer (RSSM with decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent dynamics model (RSSM) with deterministic GRU hidden state h and stochastic state s; uses a VAE-style decoder p(x_t | h_t, s_t) and trains via ELBO: reconstruction likelihood term (pixel decoder) plus multi-step KL (latent overshooting). The learned latent model is used for imagined trajectories to train a policy via backpropagated value / reward predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE-style RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Control from pixels: DeepMind Control Suite (manipulation, locomotion, swing-up) and Atari in original works; used here as baseline on the same DMC tasks and custom manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction log-likelihood (pixel-space reconstruction loss) and multi-step latent KL (ELBO / latent overshooting). For downstream performance, task scores are used.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Paper does not report explicit pixel MSE or likelihood numbers in this article; baseline task scores from Table I indicate weaker performance than Dreaming on tasks with small objects (e.g., Reacher-hard 8 ± 33), but stronger on some locomotion tasks (e.g., Walker-walk 955 ± 19).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent representation is partly interpretable in that reconstructions and open-loop predictions can be visualized; but model remains a black-box neural latent model with no explicit symbolic interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of reconstructed frames and open-loop predictions; use of decoder to inspect what latents represent.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Requires training a pixel decoder (high-capacity CNN) in addition to RSSM and reward/policy networks; paper notes decoder imposes significant parameter and compute cost but does not provide exact FLOPs/parameter counts here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to decoder-free Dreaming, Dreamer with reconstruction can suffer from 'object vanishing' (weak supervision for small-object pixels), requiring potentially more data or decoder capacity to capture small objects; Dreamer still performs well on tasks where pixel reconstruction of backgrounds encode relevant signals (e.g., locomotion).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Baseline task scores reported in Table I. Example: Cup-catch (100K) 698 ± 350; Reacher-hard 8 ± 33; Finger-turn-hard 264 ± 368; Walker-walk 955 ± 19.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High pixel-reconstruction fidelity (when achieved) can preserve small-object details useful for control, but the reconstruction objective can prioritize large-area textures over small but task-critical objects, leading to 'object vanishing' and poor downstream control in manipulation domains; hence high reconstruction fidelity in mean pixel terms does not guarantee task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Reconstruction objective provides direct pixel fidelity but can overlook small objects due to their small contribution to pixel-wise loss; decoder adds computational cost. Removing decoder (Dreaming) trades reconstruction fidelity for contrastive InfoMax objectives that emphasize discriminative features — improving some tasks but harming others that rely on background/pixel textures.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses ELBO with pixel decoder and latent overshooting for long-term prediction; learns reward head in the latent space; uses GRU for deterministic hidden state; contrastive variants (as discussed) were also explored in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally: Dreamer (decoder-based ELBO) outperforms vanilla decoder-free NCE Dreamer on many tasks; Dreaming (this paper) improves on Dreamer for manipulation tasks prone to object vanishing by replacing decoder with InfoMax and adding linear tilde-dynamics and random-crop augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Original Dreamer optimal configuration (per Hafner et al.) uses decoder-based ELBO with latent overshooting and expressive GRU dynamics; in contexts with small critical objects, authors of this paper recommend decoder-free InfoMax + linear tilde-dynamics as an alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1423.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1423.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent State Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent dynamics model combining deterministic recurrent hidden states (GRU) and stochastic latent states, originally used in PlaNet/Dreamer for long-horizon pixel-based prediction and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RSSM (stochastic + deterministic latent state model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent state z_t decomposed as (s_t, h_t) where h_t is deterministic GRU hidden state updated by (h_t = GRU(h_{t-1}, s_{t-1}, a_{t-1})) and s_t is a probabilistic latent sampled from p(s_t | h_t); observation decoder p(x_t | h_t, s_t) (in decoder-based variants) and inference q(s_t | h_t, x_t). Used both for multi-step predictions and for generating imagined trajectories for policy optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent recurrent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based RL from pixels (robotic manipulation, control tasks in DMC and Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO components: reconstruction log-likelihood p(x_t|z_t) and KL between q and p for latent transitions; also multi-step (overshooting) KL terms for long-term prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No explicit generic fidelity numbers reported here separate from Dreamer/Dreaming task outcomes; used as core predictive model for both variants.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent decomposition (stochastic s and deterministic h) provides some structural interpretability; open-loop predictions and reconstructed frames can be used to inspect what the RSSM captures. Authors emphasize smoothness when combined with linear tilde-dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of open-loop predictions and latents; analysis of temporal smoothness via linear contrastive forward model.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Expressive (GRU-based) RSSM used for planning; computational cost stems from GRU and associated networks; paper does not provide absolute compute/parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>RSSM yields accurate long-term predictions enabling latent imagination planning, making MBRL sample-efficient compared to purely model-free methods in literature; computational vs model-free trade-offs not numerically specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>RSSM is the backbone for both Dreamer and Dreaming; resulting task performance corresponds to the respective methods (see Dreamer and Dreaming entries).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM's ability to produce imagined trajectories is central to Dreamer/Dreaming policy training; its predictive fidelity translates to better policies when the representation contains task-relevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Expressive RSSM dynamics can enable large latent jumps between temporally adjacent states; when used directly for contrastive learning this can produce unsmooth latent spaces — motivating separate low-capacity linear tilde-dynamics for the contrastive objective.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Two-part latent (s,h), GRU for deterministic dynamics, multi-step latent overshooting KL terms for long-term consistency; in Dreaming, RSSM is still used for planning while contrastive pathways use a separate linear dynamics model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to non-recurrent or simpler forward models (e.g., CPC-style models without dynamics): RSSM supports action-conditioned imagined trajectories needed for planning, unlike purely observation-only contrastive models.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors keep expressive GRU-based RSSM for imagination/planning but pair it with a lower-capacity linear dynamics for contrastive learning to balance prediction accuracy and latent smoothness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1423.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1423.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear tilde-dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Independent linear forward dynamics (\u223ctilde{p})</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic linear forward model used only for the contrastive InfoMax objective: z_t' = W_z z_{t-1} + W_a a_{t-1}, implemented as a Dirac-delta deterministic predictor to regularize temporal smoothness of latents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Independent linear forward dynamics (tilde p)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A low-capacity deterministic linear mapping from previous latent and action to next latent used to generate multi-step predictions for the contrastive (Info-NCE) terms; implemented as a delta distribution at z_t' = W_z z_{t-1} + W_a a_{t-1}.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>low-capacity latent predictor (linear dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used inside Dreaming during representation learning for model-based RL from pixels (manipulation/control tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>No explicit numeric predictive-fidelity reported for this component; its role is regularization to keep temporally-consecutive latent embeddings close and smooth rather than precise high-fidelity prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported numerically; ablation shows replacing linear tilde-dynamics with shared expressive dynamics degrades downstream task performance (Table II ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Highly interpretable due to linear structure; contributes to more smooth and regular latent transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Architectural choice (linear mapping) itself provides interpretability; evaluated via improved smoothness qualitatively and via ablation experiments showing its importance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Very low-cost: linear mapping with small parameter footprint compared to expressive GRU dynamics or pixel decoder; exact parameter counts not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More computationally efficient than using expressive shared dynamics for the contrastive branch; empirically leads to better downstream performance by avoiding extreme latent jumps that hinder contrastive optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Ablation: models using this linear tilde-dynamics score substantially higher on manipulation tasks (see Table II), indicating strong positive effect on task performance when paired with InfoMax training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>By constraining contrastive multi-step predictions to be linear and smooth, the representation focuses on temporally-coherent, control-relevant features, which improves policy learning on manipulation tasks sensitive to small-object perception.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off: reduced modeling capacity in contrastive prediction path versus smoother, more learnable latent space; authors argue this reduces latent gaps that harm Info-NCE optimization. Using expressive dynamics instead allows more precise prediction but may scatter temporally-adjacent latents and hurt contrastive training.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Deliberate low-capacity deterministic linear model for tilde p used only in contrastive loss; separate expressive RSSM dynamics retained for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared in ablation to using the expressive p for contrastive predictions (i.e., tilde p := p). Linear tilde p yields better performance on tasks with object vanishing issues.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors found linear tilde-dynamics + random-crop + multi-step NCE (K=3) to be the best configuration for their difficult manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1423.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1423.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InfoMax / Info-NCE objective</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InfoMax contrastive objective derived from ELBO (multi-step Info-NCE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A likelihood-free contrastive objective for RSSM training derived from reformulating the ELBO: an Info-NCE bound that maximizes mutual information I(x_t; z_t-k) across multi-step (overshooting) predictions and uses a bilinear discriminator p(z|x) implemented with z^T W e.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-step Info-NCE (contrastive InfoMax)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive loss computed between predicted multi-step latents (from tilde p) and image features e from a CNN; uses exponentiated bilinear similarity (logit = z^T W e) and softmax cross-entropy over positive and many negatives within a batch and across K-step overshooting predictions. The objective is summed with multi-step KL (latent overshooting) terms for training RSSM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>contrastive (discriminative) training objective for latent world models</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Representation learning for model-based RL from pixels (robotic control tasks in DMC and custom manipulation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Objective approximates a lower bound on mutual information I(x_t; z_{t-k}); serves as surrogate for likelihood/reconstruction in measuring how informative z is about x. No direct pixel-MSE fidelity reported when using this objective.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in pixel-error terms. Ablations and experiments show multi-step Info-NCE with linear tilde-dynamics yields superior downstream task performance versus vanilla Info-NCE and reconstruction in object-vanishing-critical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Encourages embeddings to be discriminative (separated) and to preserve mutual information with observations; does not make internal model transparent but yields latents that capture task-relevant features as evidenced by improved task scores and visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Theoretical link to mutual information bounds presented in Appendix I; empirical validation via open-loop predictions and task performance comparisons; uses bilinear logits which can be inspected.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Computationally cheaper at inference/training than full decoder-based likelihood because no decoder backpropagation, but includes cross-entropy over batch negatives (B*K)^2 logits which can incur compute/memory cost; exact resource numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Vanilla Info-NCE (single-step or shared dynamics) performed worse in experiments; the paper's multi-step Info-NCE with tilde-dynamics and augmentation is more sample-efficient (better task scores at same environment steps) on manipulation tasks. Compared to reconstruction, Info-NCE reduces decoder-related compute.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When paired with linear tilde-dynamics and random-crop augmentation, multi-step Info-NCE enables Dreaming to achieve state-of-the-art results on five difficult manipulation tasks (see Dreaming task_performance field). By contrast, vanilla NCE Dreamer degraded performances versus reconstruction-based Dreamer in many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>InfoMax prioritizes discriminative, control-relevant latent features over minimizing pixel-wise reconstruction loss — this aligns representation learning with downstream control utility, reducing object vanishing and improving policy learning for tasks requiring small-object perception.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>While InfoMax improves critical-object perception, it can discard background/pixel cues necessary for some tasks (locomotion); also, contrastive losses rely on large and diverse negative sets and can be sensitive to augmentation strategy (random crop is necessary here).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Derivation from ELBO to Info-NCE; use of exponentiated bilinear discriminator p(z|x) with CNN image features; multi-step (overshooting) NCE terms for temporal correlation; approximating importance weights and using tilde p for computational simplicity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to vanilla NCE (single-step) and ELBO reconstruction: multi-step Info-NCE + linear tilde-dynamics + random-crop outperforms vanilla NCE and reconstruction in the object-vanishing manipulation benchmarks; however, reconstruction retains advantages when background textures encode task signals.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors recommend multi-step (K=3) Info-NCE, bilinear discriminator, linear tilde-dynamics, and random-crop augmentation as the practical configuration for difficult manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1423.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1423.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CURL / CPC / CFM (related)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive unsupervised representation learning methods (CURL, Contrastive Predictive Coding (CPC), Contrastive Forward Model (CFM))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative contrastive/discriminative representation-learning methods cited as related work: CPC predicts future latent representations, CFM applies contrastive estimation to forward dynamics, and CURL applies contrastive learning with random-crop augmentation for RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CURL / CPC / CFM (related contrastive methods)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CPC: unsupervised contrastive predictive coding that maximizes mutual information between past context and future observations using contrastive losses; CFM: contrastive forward model that uses contrastive objectives for predictive representations; CURL: contrastive unsupervised representations for RL using image augmentations (random crop) combined with RL algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>contrastive representation learning methods (used as auxiliary or baseline methods)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Representation learning for vision-based RL and predictive modeling (used as baselines or related methods in DMC/Atari/robotics contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically measured via Info-NCE lower bounds and downstream task performance; specific metrics vary by paper (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Generally produce discriminative latent representations; interpretability varies by implementation and is typically assessed via downstream performance and qualitative visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Contrastive objective analysis, visualization, and downstream RL performance; CURL uses random-crop augmentation to encourage invariances.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable to other contrastive methods: requires computing logits against batch negatives; computational cost scales with batch size and number of negatives; exact numbers not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>CURL and related methods are effective model-free baselines; the paper compares Dreaming against CURL, DrQ, RAD on tasks and reports Dreaming outperforming these MFRL baselines on certain difficult manipulation tasks (Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Cited baseline results in Table I: e.g., CURL Cup-catch 693 ± 334, DrQ 882 ± 174, RAD 792 ± 315 (100K steps for Cup-catch); performance varies by task and method.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Contrastive methods can provide strong invariances and sample-efficiency for RL, especially with augmentations; however, purely contrastive (non-dynamics-aware) methods may not support action-conditioned imagination needed for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Purely observation-contrastive methods may lack action-conditioned dynamics necessary for latent imagination; CFM tries to address this but shares dynamics between contrastive and behavior learning which the authors argue can be suboptimal.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Contrastive objectives, different choices of augmentations (random crop, color jitter), whether to include action-conditioned dynamics, and whether to separate dynamics models for contrastive vs planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Authors situate their method relative to these: CPC lacks action-conditioned dynamics and is only auxiliary for MFRL; CFM shares expressive dynamics for contrastive and behavior learning whereas Dreaming uses independent linear tilde-dynamics for contrastive training; CURL and augmentation-heavy MFRL methods are strong baselines but Dreaming outperforms them on several manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Representation learning with contrastive predictive coding <em>(Rating: 2)</em></li>
                <li>CURL: Contrastive unsupervised representations for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning predictive representations for deformable objects using contrastive estimation <em>(Rating: 1)</em></li>
                <li>A simple framework for contrastive learning of visual representations <em>(Rating: 1)</em></li>
                <li>Reinforcement learning with augmented data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1423",
    "paper_id": "paper-87821f5fd000d873beb5ba55689ba90def31a8c1",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Dreaming",
            "name_full": "Dreamer with InfoMax and without generative decoder (Dreaming)",
            "brief_description": "This paper's proposed decoder-free variant of Dreamer that trains an RSSM latent dynamics model using an InfoMax / contrastive (Info-NCE) objective, with an independent linear forward dynamics for contrastive learning and random-crop augmentation; used end-to-end for model-based RL from pixels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Dreaming (decoder-free Dreamer)",
            "model_description": "Latent world model built on an RSSM (latent state z = (s,h) with deterministic GRU hidden h and probabilistic s) but trained without a pixel decoder; instead uses a discriminator p(z|x) implemented as an exponentiated bilinear similarity between latent z and CNN image features e, optimized with an Info-NCE (contrastive/InfoMax) objective across multi-step (overshooting) predictions. An independent linear forward model \\u0394p(\\u03b6) (z_t' = W_z z_{t-1} + W_a a_{t-1}) is used only for the contrastive (InfoMax) objective to regularize temporal smoothness; the expressive GRU-based RSSM dynamics p(z_t|z_{t-1},a_{t-1}) are retained for planning and policy optimization.",
            "model_type": "latent world model (RSSM-based) with contrastive/discriminative training",
            "task_domain": "Simulated robotic manipulation and control tasks (DeepMind Control Suite and two original manipulation tasks: UR5-reach and Connector-insert), plus other DMC locomotion and swing-up tasks",
            "fidelity_metric": "No explicit numeric pixel-reconstruction fidelity reported for Dreaming since decoder is removed; training objectives relate to mutual information and predictive KL: (i) Info-NCE / InfoMax lower bound on mutual information I(x_t; z_{t-k}), (ii) multi-step KL objective (latent overshooting) between inference q and generative p. Task fidelity is reported indirectly via downstream task scores.",
            "fidelity_performance": "No direct prediction-error numbers reported; model fidelity inferred from task-level performance: e.g., Cup-catch (100K) Dreaming score 925 ± 48 vs Dreamer w/ likelihood 698 ± 350; Reacher-hard 868 ± 272 vs Dreamer 8 ± 33; Finger-turn-hard 752 ± 325 vs Dreamer 264 ± 368 (Table I).",
            "interpretability_assessment": "Partially interpretable: learned latent embeddings preserve principal control features (positions, orientations) as seen in open-loop video prediction and reconstructions generated from latent states; authors claim the linear tilde-dynamics regularizes temporally-smooth latent space, making latent transitions more interpretable/structured. Not a fully interpretable (symbolic) model — still a neural latent model.",
            "interpretability_method": "Visualization of open-loop video predictions and reconstructed context frames; qualitative assessment of latent-space smoothness induced by linear forward model; implicit inspection of object preservation vs 'object vanishing' through task success and visualization.",
            "computational_cost": "No exact compute or parameter counts reported. Paper argues decoder training requires high-capacity CNN decoder with many parameters; Dreaming removes decoder training (reducing training cost and memory) but still trains RSSM (GRU) and discriminator (CNN + bilinear). Training/evaluation performed on standard RL training budgets reported in environment steps (e.g., 100K–500K).",
            "efficiency_comparison": "Compared to decoder-based Dreamer, Dreaming avoids decoder optimization (reducing parameter/compute needs for decoder) and achieves better sample efficiency on several difficult manipulation tasks; exact wall-clock or FLOP savings not reported. Paper reports improved sample efficiency in task scores at the same environment steps for several tasks.",
            "task_performance": "Reported task scores (mean ± std) at given environment steps (Table I). Representative results: Cup-catch (100K) 925 ± 48 (Dreaming) vs 698 ± 350 (Dreamer w/ likelihood); Reacher-hard 868 ± 272 vs 8 ± 33; Connector-insert 629 ± 391 vs 169 ± 348. Dreaming outperforms Dreamer and some leading model-free baselines on the five difficult manipulation tasks where Dreamer suffers object vanishing; however, Dreaming underperforms Dreamer on several planar locomotion tasks (e.g., Walker-walk: Dreaming 469 ± 123 vs Dreamer 955 ± 19).",
            "task_utility_analysis": "Dreaming's InfoMax contrastive training prioritizes task-relevant and discriminative visual features, which reduces 'object vanishing' and translates to substantially improved policy performance on tasks requiring small-object perception and fine pixel control. However, when task-relevant information is encoded in background texture (e.g., floors providing velocity cues in locomotion), contrastive objective and data augmentation can obscure necessary cues and reduce performance — indicating a case where reduced reconstruction fidelity (no decoder) harms task utility.",
            "tradeoffs_observed": "Trade-offs explicitly discussed: removing decoder (decoder-free InfoMax) improves perception of small, task-critical objects and sample efficiency on manipulation tasks but can hurt tasks that require background texture information (planar locomotion). The linear independent forward model enforces smooth temporally-consistent latents (benefit) but reduces capacity to model complex non-linear transitions in the contrastive path (design trade-off). Data augmentation (random crop) aids focusing on control-relevant features but can remove discriminative background cues needed for some tasks.",
            "design_choices": "Key choices: (1) Replace pixel decoder + reconstruction objective with InfoMax / Info-NCE contrastive objective derived from ELBO; (2) Use independent linear forward dynamics \\u223c\\u0394p for contrastive multi-step predictions (tilde p deterministic linear: z_t' = W_z z_{t-1} + W_a a_{t-1}); (3) Use overshooting (multi-step) NCE terms (K recommended = 3); (4) Use random-crop image augmentation to force invariance to irrelevant image shifts; (5) Keep expressive GRU-based RSSM dynamics for planning while using linear tilde dynamics only for contrastive loss.",
            "comparison_to_alternatives": "Compared empirically to decoder-based Dreamer (ELBO likelihood + KL), Dreamer w/ vanilla NCE, and leading model-free baselines (CURL, DrQ, RAD). Dreaming outperforms decoder-based Dreamer and MFRL baselines on five difficult manipulation tasks (Table I), but underperforms Dreamer on several locomotion tasks where background textures are informative. Vanilla NCE without the proposed linear tilde dynamics and data augmentation degraded performance versus the proposed multi-step InfoMax + linear tilde dynamics scheme.",
            "optimal_configuration": "Paper recommends: (1) use independent linear forward dynamics for contrastive learning rather than sharing expressive dynamics; (2) use random-crop augmentation; (3) use overshooting distance K = 3 (ablation shows K=3 effective); these constitute the optimal configuration for the authors' manipulation benchmarks where object vanishing is critical.",
            "uuid": "e1423.0",
            "source_info": {
                "paper_title": "Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "Dreamer",
            "name_full": "Dream to control: Learning behaviors by latent imagination (Dreamer)",
            "brief_description": "A model-based RL method that learns a recurrent latent state-space model (RSSM) with an autoencoding variational objective (decoder + reconstruction) and uses latent imagination (backpropagated Bellman errors on imagined trajectories) for policy optimization.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "use",
            "model_name": "Dreamer (RSSM with decoder)",
            "model_description": "Latent dynamics model (RSSM) with deterministic GRU hidden state h and stochastic state s; uses a VAE-style decoder p(x_t | h_t, s_t) and trains via ELBO: reconstruction likelihood term (pixel decoder) plus multi-step KL (latent overshooting). The learned latent model is used for imagined trajectories to train a policy via backpropagated value / reward predictions.",
            "model_type": "latent world model (VAE-style RSSM)",
            "task_domain": "Control from pixels: DeepMind Control Suite (manipulation, locomotion, swing-up) and Atari in original works; used here as baseline on the same DMC tasks and custom manipulation tasks.",
            "fidelity_metric": "Reconstruction log-likelihood (pixel-space reconstruction loss) and multi-step latent KL (ELBO / latent overshooting). For downstream performance, task scores are used.",
            "fidelity_performance": "Paper does not report explicit pixel MSE or likelihood numbers in this article; baseline task scores from Table I indicate weaker performance than Dreaming on tasks with small objects (e.g., Reacher-hard 8 ± 33), but stronger on some locomotion tasks (e.g., Walker-walk 955 ± 19).",
            "interpretability_assessment": "Latent representation is partly interpretable in that reconstructions and open-loop predictions can be visualized; but model remains a black-box neural latent model with no explicit symbolic interpretability.",
            "interpretability_method": "Visualization of reconstructed frames and open-loop predictions; use of decoder to inspect what latents represent.",
            "computational_cost": "Requires training a pixel decoder (high-capacity CNN) in addition to RSSM and reward/policy networks; paper notes decoder imposes significant parameter and compute cost but does not provide exact FLOPs/parameter counts here.",
            "efficiency_comparison": "Compared to decoder-free Dreaming, Dreamer with reconstruction can suffer from 'object vanishing' (weak supervision for small-object pixels), requiring potentially more data or decoder capacity to capture small objects; Dreamer still performs well on tasks where pixel reconstruction of backgrounds encode relevant signals (e.g., locomotion).",
            "task_performance": "Baseline task scores reported in Table I. Example: Cup-catch (100K) 698 ± 350; Reacher-hard 8 ± 33; Finger-turn-hard 264 ± 368; Walker-walk 955 ± 19.",
            "task_utility_analysis": "High pixel-reconstruction fidelity (when achieved) can preserve small-object details useful for control, but the reconstruction objective can prioritize large-area textures over small but task-critical objects, leading to 'object vanishing' and poor downstream control in manipulation domains; hence high reconstruction fidelity in mean pixel terms does not guarantee task utility.",
            "tradeoffs_observed": "Reconstruction objective provides direct pixel fidelity but can overlook small objects due to their small contribution to pixel-wise loss; decoder adds computational cost. Removing decoder (Dreaming) trades reconstruction fidelity for contrastive InfoMax objectives that emphasize discriminative features — improving some tasks but harming others that rely on background/pixel textures.",
            "design_choices": "Uses ELBO with pixel decoder and latent overshooting for long-term prediction; learns reward head in the latent space; uses GRU for deterministic hidden state; contrastive variants (as discussed) were also explored in prior work.",
            "comparison_to_alternatives": "Compared experimentally: Dreamer (decoder-based ELBO) outperforms vanilla decoder-free NCE Dreamer on many tasks; Dreaming (this paper) improves on Dreamer for manipulation tasks prone to object vanishing by replacing decoder with InfoMax and adding linear tilde-dynamics and random-crop augmentation.",
            "optimal_configuration": "Original Dreamer optimal configuration (per Hafner et al.) uses decoder-based ELBO with latent overshooting and expressive GRU dynamics; in contexts with small critical objects, authors of this paper recommend decoder-free InfoMax + linear tilde-dynamics as an alternative.",
            "uuid": "e1423.1",
            "source_info": {
                "paper_title": "Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "RSSM",
            "name_full": "Recurrent State Space Model (RSSM)",
            "brief_description": "A latent dynamics model combining deterministic recurrent hidden states (GRU) and stochastic latent states, originally used in PlaNet/Dreamer for long-horizon pixel-based prediction and planning.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "use",
            "model_name": "RSSM (stochastic + deterministic latent state model)",
            "model_description": "Latent state z_t decomposed as (s_t, h_t) where h_t is deterministic GRU hidden state updated by (h_t = GRU(h_{t-1}, s_{t-1}, a_{t-1})) and s_t is a probabilistic latent sampled from p(s_t | h_t); observation decoder p(x_t | h_t, s_t) (in decoder-based variants) and inference q(s_t | h_t, x_t). Used both for multi-step predictions and for generating imagined trajectories for policy optimization.",
            "model_type": "latent recurrent world model",
            "task_domain": "Model-based RL from pixels (robotic manipulation, control tasks in DMC and Atari)",
            "fidelity_metric": "ELBO components: reconstruction log-likelihood p(x_t|z_t) and KL between q and p for latent transitions; also multi-step (overshooting) KL terms for long-term prediction.",
            "fidelity_performance": "No explicit generic fidelity numbers reported here separate from Dreamer/Dreaming task outcomes; used as core predictive model for both variants.",
            "interpretability_assessment": "Latent decomposition (stochastic s and deterministic h) provides some structural interpretability; open-loop predictions and reconstructed frames can be used to inspect what the RSSM captures. Authors emphasize smoothness when combined with linear tilde-dynamics.",
            "interpretability_method": "Visualization of open-loop predictions and latents; analysis of temporal smoothness via linear contrastive forward model.",
            "computational_cost": "Expressive (GRU-based) RSSM used for planning; computational cost stems from GRU and associated networks; paper does not provide absolute compute/parameter counts.",
            "efficiency_comparison": "RSSM yields accurate long-term predictions enabling latent imagination planning, making MBRL sample-efficient compared to purely model-free methods in literature; computational vs model-free trade-offs not numerically specified here.",
            "task_performance": "RSSM is the backbone for both Dreamer and Dreaming; resulting task performance corresponds to the respective methods (see Dreamer and Dreaming entries).",
            "task_utility_analysis": "RSSM's ability to produce imagined trajectories is central to Dreamer/Dreaming policy training; its predictive fidelity translates to better policies when the representation contains task-relevant information.",
            "tradeoffs_observed": "Expressive RSSM dynamics can enable large latent jumps between temporally adjacent states; when used directly for contrastive learning this can produce unsmooth latent spaces — motivating separate low-capacity linear tilde-dynamics for the contrastive objective.",
            "design_choices": "Two-part latent (s,h), GRU for deterministic dynamics, multi-step latent overshooting KL terms for long-term consistency; in Dreaming, RSSM is still used for planning while contrastive pathways use a separate linear dynamics model.",
            "comparison_to_alternatives": "Compared implicitly to non-recurrent or simpler forward models (e.g., CPC-style models without dynamics): RSSM supports action-conditioned imagined trajectories needed for planning, unlike purely observation-only contrastive models.",
            "optimal_configuration": "Authors keep expressive GRU-based RSSM for imagination/planning but pair it with a lower-capacity linear dynamics for contrastive learning to balance prediction accuracy and latent smoothness.",
            "uuid": "e1423.2",
            "source_info": {
                "paper_title": "Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "Linear tilde-dynamics",
            "name_full": "Independent linear forward dynamics (\\u223ctilde{p})",
            "brief_description": "A deterministic linear forward model used only for the contrastive InfoMax objective: z_t' = W_z z_{t-1} + W_a a_{t-1}, implemented as a Dirac-delta deterministic predictor to regularize temporal smoothness of latents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Independent linear forward dynamics (tilde p)",
            "model_description": "A low-capacity deterministic linear mapping from previous latent and action to next latent used to generate multi-step predictions for the contrastive (Info-NCE) terms; implemented as a delta distribution at z_t' = W_z z_{t-1} + W_a a_{t-1}.",
            "model_type": "low-capacity latent predictor (linear dynamics)",
            "task_domain": "Used inside Dreaming during representation learning for model-based RL from pixels (manipulation/control tasks).",
            "fidelity_metric": "No explicit numeric predictive-fidelity reported for this component; its role is regularization to keep temporally-consecutive latent embeddings close and smooth rather than precise high-fidelity prediction.",
            "fidelity_performance": "Not reported numerically; ablation shows replacing linear tilde-dynamics with shared expressive dynamics degrades downstream task performance (Table II ablation).",
            "interpretability_assessment": "Highly interpretable due to linear structure; contributes to more smooth and regular latent transitions.",
            "interpretability_method": "Architectural choice (linear mapping) itself provides interpretability; evaluated via improved smoothness qualitatively and via ablation experiments showing its importance.",
            "computational_cost": "Very low-cost: linear mapping with small parameter footprint compared to expressive GRU dynamics or pixel decoder; exact parameter counts not reported.",
            "efficiency_comparison": "More computationally efficient than using expressive shared dynamics for the contrastive branch; empirically leads to better downstream performance by avoiding extreme latent jumps that hinder contrastive optimization.",
            "task_performance": "Ablation: models using this linear tilde-dynamics score substantially higher on manipulation tasks (see Table II), indicating strong positive effect on task performance when paired with InfoMax training.",
            "task_utility_analysis": "By constraining contrastive multi-step predictions to be linear and smooth, the representation focuses on temporally-coherent, control-relevant features, which improves policy learning on manipulation tasks sensitive to small-object perception.",
            "tradeoffs_observed": "Trade-off: reduced modeling capacity in contrastive prediction path versus smoother, more learnable latent space; authors argue this reduces latent gaps that harm Info-NCE optimization. Using expressive dynamics instead allows more precise prediction but may scatter temporally-adjacent latents and hurt contrastive training.",
            "design_choices": "Deliberate low-capacity deterministic linear model for tilde p used only in contrastive loss; separate expressive RSSM dynamics retained for planning.",
            "comparison_to_alternatives": "Compared in ablation to using the expressive p for contrastive predictions (i.e., tilde p := p). Linear tilde p yields better performance on tasks with object vanishing issues.",
            "optimal_configuration": "Authors found linear tilde-dynamics + random-crop + multi-step NCE (K=3) to be the best configuration for their difficult manipulation tasks.",
            "uuid": "e1423.3",
            "source_info": {
                "paper_title": "Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "InfoMax / Info-NCE objective",
            "name_full": "InfoMax contrastive objective derived from ELBO (multi-step Info-NCE)",
            "brief_description": "A likelihood-free contrastive objective for RSSM training derived from reformulating the ELBO: an Info-NCE bound that maximizes mutual information I(x_t; z_t-k) across multi-step (overshooting) predictions and uses a bilinear discriminator p(z|x) implemented with z^T W e.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multi-step Info-NCE (contrastive InfoMax)",
            "model_description": "Contrastive loss computed between predicted multi-step latents (from tilde p) and image features e from a CNN; uses exponentiated bilinear similarity (logit = z^T W e) and softmax cross-entropy over positive and many negatives within a batch and across K-step overshooting predictions. The objective is summed with multi-step KL (latent overshooting) terms for training RSSM.",
            "model_type": "contrastive (discriminative) training objective for latent world models",
            "task_domain": "Representation learning for model-based RL from pixels (robotic control tasks in DMC and custom manipulation tasks).",
            "fidelity_metric": "Objective approximates a lower bound on mutual information I(x_t; z_{t-k}); serves as surrogate for likelihood/reconstruction in measuring how informative z is about x. No direct pixel-MSE fidelity reported when using this objective.",
            "fidelity_performance": "Not reported in pixel-error terms. Ablations and experiments show multi-step Info-NCE with linear tilde-dynamics yields superior downstream task performance versus vanilla Info-NCE and reconstruction in object-vanishing-critical tasks.",
            "interpretability_assessment": "Encourages embeddings to be discriminative (separated) and to preserve mutual information with observations; does not make internal model transparent but yields latents that capture task-relevant features as evidenced by improved task scores and visualizations.",
            "interpretability_method": "Theoretical link to mutual information bounds presented in Appendix I; empirical validation via open-loop predictions and task performance comparisons; uses bilinear logits which can be inspected.",
            "computational_cost": "Computationally cheaper at inference/training than full decoder-based likelihood because no decoder backpropagation, but includes cross-entropy over batch negatives (B*K)^2 logits which can incur compute/memory cost; exact resource numbers not provided.",
            "efficiency_comparison": "Vanilla Info-NCE (single-step or shared dynamics) performed worse in experiments; the paper's multi-step Info-NCE with tilde-dynamics and augmentation is more sample-efficient (better task scores at same environment steps) on manipulation tasks. Compared to reconstruction, Info-NCE reduces decoder-related compute.",
            "task_performance": "When paired with linear tilde-dynamics and random-crop augmentation, multi-step Info-NCE enables Dreaming to achieve state-of-the-art results on five difficult manipulation tasks (see Dreaming task_performance field). By contrast, vanilla NCE Dreamer degraded performances versus reconstruction-based Dreamer in many tasks.",
            "task_utility_analysis": "InfoMax prioritizes discriminative, control-relevant latent features over minimizing pixel-wise reconstruction loss — this aligns representation learning with downstream control utility, reducing object vanishing and improving policy learning for tasks requiring small-object perception.",
            "tradeoffs_observed": "While InfoMax improves critical-object perception, it can discard background/pixel cues necessary for some tasks (locomotion); also, contrastive losses rely on large and diverse negative sets and can be sensitive to augmentation strategy (random crop is necessary here).",
            "design_choices": "Derivation from ELBO to Info-NCE; use of exponentiated bilinear discriminator p(z|x) with CNN image features; multi-step (overshooting) NCE terms for temporal correlation; approximating importance weights and using tilde p for computational simplicity.",
            "comparison_to_alternatives": "Compared to vanilla NCE (single-step) and ELBO reconstruction: multi-step Info-NCE + linear tilde-dynamics + random-crop outperforms vanilla NCE and reconstruction in the object-vanishing manipulation benchmarks; however, reconstruction retains advantages when background textures encode task signals.",
            "optimal_configuration": "Authors recommend multi-step (K=3) Info-NCE, bilinear discriminator, linear tilde-dynamics, and random-crop augmentation as the practical configuration for difficult manipulation tasks.",
            "uuid": "e1423.4",
            "source_info": {
                "paper_title": "Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "CURL / CPC / CFM (related)",
            "name_full": "Contrastive unsupervised representation learning methods (CURL, Contrastive Predictive Coding (CPC), Contrastive Forward Model (CFM))",
            "brief_description": "Representative contrastive/discriminative representation-learning methods cited as related work: CPC predicts future latent representations, CFM applies contrastive estimation to forward dynamics, and CURL applies contrastive learning with random-crop augmentation for RL.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "CURL / CPC / CFM (related contrastive methods)",
            "model_description": "CPC: unsupervised contrastive predictive coding that maximizes mutual information between past context and future observations using contrastive losses; CFM: contrastive forward model that uses contrastive objectives for predictive representations; CURL: contrastive unsupervised representations for RL using image augmentations (random crop) combined with RL algorithms.",
            "model_type": "contrastive representation learning methods (used as auxiliary or baseline methods)",
            "task_domain": "Representation learning for vision-based RL and predictive modeling (used as baselines or related methods in DMC/Atari/robotics contexts).",
            "fidelity_metric": "Typically measured via Info-NCE lower bounds and downstream task performance; specific metrics vary by paper (not detailed here).",
            "fidelity_performance": null,
            "interpretability_assessment": "Generally produce discriminative latent representations; interpretability varies by implementation and is typically assessed via downstream performance and qualitative visualization.",
            "interpretability_method": "Contrastive objective analysis, visualization, and downstream RL performance; CURL uses random-crop augmentation to encourage invariances.",
            "computational_cost": "Comparable to other contrastive methods: requires computing logits against batch negatives; computational cost scales with batch size and number of negatives; exact numbers not provided in this paper.",
            "efficiency_comparison": "CURL and related methods are effective model-free baselines; the paper compares Dreaming against CURL, DrQ, RAD on tasks and reports Dreaming outperforming these MFRL baselines on certain difficult manipulation tasks (Table I).",
            "task_performance": "Cited baseline results in Table I: e.g., CURL Cup-catch 693 ± 334, DrQ 882 ± 174, RAD 792 ± 315 (100K steps for Cup-catch); performance varies by task and method.",
            "task_utility_analysis": "Contrastive methods can provide strong invariances and sample-efficiency for RL, especially with augmentations; however, purely contrastive (non-dynamics-aware) methods may not support action-conditioned imagination needed for planning.",
            "tradeoffs_observed": "Purely observation-contrastive methods may lack action-conditioned dynamics necessary for latent imagination; CFM tries to address this but shares dynamics between contrastive and behavior learning which the authors argue can be suboptimal.",
            "design_choices": "Contrastive objectives, different choices of augmentations (random crop, color jitter), whether to include action-conditioned dynamics, and whether to separate dynamics models for contrastive vs planning tasks.",
            "comparison_to_alternatives": "Authors situate their method relative to these: CPC lacks action-conditioned dynamics and is only auxiliary for MFRL; CFM shares expressive dynamics for contrastive and behavior learning whereas Dreaming uses independent linear tilde-dynamics for contrastive training; CURL and augmentation-heavy MFRL methods are strong baselines but Dreaming outperforms them on several manipulation tasks.",
            "optimal_configuration": null,
            "uuid": "e1423.5",
            "source_info": {
                "paper_title": "Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction",
                "publication_date_yy_mm": "2020-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "Representation learning with contrastive predictive coding",
            "rating": 2
        },
        {
            "paper_title": "CURL: Contrastive unsupervised representations for reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Learning predictive representations for deformable objects using contrastive estimation",
            "rating": 1
        },
        {
            "paper_title": "A simple framework for contrastive learning of visual representations",
            "rating": 1
        },
        {
            "paper_title": "Reinforcement learning with augmented data",
            "rating": 1
        }
    ],
    "cost": 0.018709999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction</h1>
<p>Masashi Okada ${ }^{1, \star}$ and Tadahiro Taniguchi ${ }^{1,2}$</p>
<h4>Abstract</h4>
<p>In the present paper, we propose a decoder-free extension of Dreamer, a leading model-based reinforcement learning (MBRL) method from pixels. Dreamer is a sample- and cost-efficient solution to robot learning, as it is used to train latent state-space models based on a variational autoencoder and to conduct policy optimization by latent trajectory imagination. However, this autoencoding based approach often causes object vanishing, in which the autoencoder fails to perceives key objects for solving control tasks, and thus significantly limiting Dreamer's potential. This work aims to relieve this Dreamer's bottleneck and enhance its performance by means of removing the decoder. For this purpose, we firstly derive a likelihoodfree and InfoMax objective of contrastive learning from the evidence lower bound of Dreamer. Secondly, we incorporate two components, (i) independent linear dynamics and (ii) the random crop data augmentation, to the learning scheme so as to improve the training performance. In comparison to Dreamer and other recent model-free reinforcement learning methods, our newly devised Dreamer with InfoMax and without generative decoder (Dreaming) achieves the best scores on 5 difficult simulated robotics tasks, in which Dreamer suffers from object vanishing.</p>
<h2>I. INTRODUCTION</h2>
<p>In the present paper, we focus on model-based reinforcement learning (MBRL) from pixels without complex reconstruction. MBRL is a promising technique to build controllers in a sample efficient manner, which trains forward dynamics models to predict future states and rewards for the purpose of planning and/or policy optimization. The recent study of MBRL in fully-observable environments [1]-[4] have achieved both sample efficiency and competitive performance with the state-of-the-art model-free reinforcement learning (MFRL) methods like soft-actor-critic (SAC) [5]. Although real-robot learning has been achieved with fullyobservable MBRL [6]-[11], there has been an increasing demand for robot learning in partially observable environments in which only incomplete information (especially vision) is available. MBRL from pixels can be realized by introducing deep generative models based on autoencoding variational Bayes [12].</p>
<p>Object vanishing is a critical problem of the autoencoding based MBRL from pixels. Previous studies in this field [13][19] train autoencoding models along with latent dynamics models that generate imagined trajectories that are used for planning or policy optimization. However, the autoencoder often fails to perceive small objects in pixel space. The top</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. The overview of the motivation and concept of this work. (Top) The concept of Dreamer's autoencoding-based representation learning [16], which often causes object vanishing as shown with the dashed-line ovals. The left three tasks are from DeepMind Control Suite [20], and the remaining two tasks are our original tasks which are assumed to represent industrial applications. (Bottom) The concept of Dreaming's representation learning, which trains a discriminator instead of the decoder to embed different samples to be spaced apart from each other. The learning scheme is characterized with the two key components; (i) Linear dynamics successfully constrains temporally consecutive samples are not distributed too further away. (ii) Image augmentation encourages only key features for control to be embedded into the latent space.
part of Fig. 1 exemplifies this kind of failures where small (or thin) and important objects are not reconstructed in their correct positions. This shows the failure to successfully embed their information into the latent space, which significantly limits the training performance. This problem is a result of a log-likelihood objective (reconstruction loss) defined in the pixel space. Since the reconstruction errors of small objects in the pixel space are insignificant compared to the errors of other objects and uninformative textures that occupy most parts of the image region, it is hard to train the encoder to perceive the small objects from the weak error signals. Also, we have to train the decoder which requires a high model capacity with massive parameters from the convolutional neural networks (CNN), although the trained models are not exploited both in the planning and policy optimization.</p>
<p>To avoid this complex reconstruction, some previous MBRL studies have proposed decoder-free representation learning [16], [21] based on contrastive learning [22], [23], which trains a discriminator instead of the decoder. The discriminator is trained by categorical cross-entropy optimization, which encourages latent embeddings to be sufficiently distinguishable among different embeddings. Nevertheless, to the best of our knowledge, no MBRL methods have achieved the state-of-the-art results on the difficult bench-</p>
<p>mark tasks of DeepMind Control Suite (DMC) [20] without reconstruction.</p>
<p>Motivated by these observations, this paper aims to achieve the state-of-the-art results with MBRL from pixels without reconstruction. This paper mainly focuses on the latest autoencoding-based MBRL method Dreamer, considering the success of a variety of control tasks (i.e., DMC and Atari Games [24]), and tries to extend this method to be a decoder-free fashion. We adopt Dreamer's policy optimization without any form of modification. We call our extended Dreamer as Dreamer with InfoMax and without generative decoder (Dreaming). The concept of this proposed method is illustrated in the bottom part of Fig. 1. Our primary contributions are summarized as follows.</p>
<ul>
<li>We derive a likelihood-free (decoder-free) and InfoMax objective for contrastive learning by reformulating the variational evidence lower bound (ELBO) of the graphical model of the partially observable Markov decision process.</li>
<li>We show that two key components, (i) an independent and linear forward dynamics, which is only utilized for contrastive learning, and (ii) appropriate data augmentation (i.e., random crop), are indispensable to achieve the state-of-the-art results.
In comparison to Dreamer and the recent cutting edge MFRL methods, Dreaming can achieve the state-of-the-art results on difficult simulated robotics tasks exhibited in Fig. 1 in which Dreamer suffers from object vanishing. The remainder of this paper is organized as follows. In Sec. II, key differences from related work are discussed. In Sec. III, we provide a brief review of Dreamer and contrastive learning. In Sec. IV, we first describe the proposed contrastive learning scheme in detail, and then introduce Dreaming. In Sec. V, the effectiveness of Dreaming is demonstrated through simulated evaluations. Finally, Sec. VI concludes this paper.</li>
</ul>
<h2>II. Related Work</h2>
<p>Some of the most related work are contrastive predictive coding (CPC) [22] and contrastive forward model (CFM) [21]. Our work is highly inspired by CPC, and our contrastive learning scheme has similar components with CPC; e.g., a recurrent neural network and a bilinear similarity function. However, CPC has no action-conditioned dynamics models. Since CPC alone cannot generate imagined trajectories from arbitrary actions, CPC is only used as an auxiliary objective of MFRL. CFM heuristically introduces a similar decoder-free objective function like ours. A primary difference between ours and CFM is that CFM exploits a shared and non-linear forward model for both contrastive and behavior learning. In addition, the relation between the ELBO of time-series variational inference is not discussed in the above two literature. Meanwhile, the original Dreamer paper [16] has also derived a contrastive objective from the ELBO. However, dynamics models and temporal correlation of observations are not involved in the contrastive objective. Furthermore, CPC, CFM, and Dreamer do not introduce data augmentation.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Graphical model of the partially observable Markov decision process.</p>
<p>MFRL methods with representation learning: A state-of-the-art MFRL method, contrastive unsupervised representation for reinforcement learning (CURL) [25], also makes use of contrastive learning with the random crop data augmentation. Deep bisimulation for control (DBC) [26] and discriminative particle filter reinforcement learning (DPFRL) [27] are other types of cutting edge MFRL methods, which utilize different concepts of representation learning without reconstruction.</p>
<p>MFRL methods without representation learning: Recently proposed MFRL methods, which include reinforcement learning with augmented data (RAD) [28], dataregularized Q (DrQ) [29], and simple unified framework for reinforcement learning using ensembles (SUNRISE) [30], have achieved state-of-the-art result without representation learning. All these work employ the random crop data augmentation as an important component of their method.</p>
<h2>III. Preliminary</h2>
<h2>A. Autoencoding Variational Bayes for Time-series</h2>
<p>Let us begin by considering the graphical model illustrated in Fig. 2, whose joint distribution is defined as follows:</p>
<p>$$
p\left(\boldsymbol{z}<em _T="&lt;T">{\leq T}, \boldsymbol{a}</em>}, \boldsymbol{x<em t="t">{\leq T}\right)=\prod</em>} p\left(\boldsymbol{z<em t="t">{t+1} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em t="t">{t}\right) p\left(\boldsymbol{x}</em>\right)
$$} \mid \boldsymbol{z}_{t</p>
<p>where $\boldsymbol{z}, \boldsymbol{x}$, and $\boldsymbol{a}$ denote latent state, observation, and action, respectively. As in the case of well-known variational autoencoders (VAEs) [12], generative models $p\left(\boldsymbol{z}<em t="t">{t+1} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em t="t">{t}\right)$, $p\left(\boldsymbol{x}</em>} \mid \boldsymbol{z<em t="t">{t}\right)$ and inference model $q\left(\boldsymbol{z}</em>} \mid \boldsymbol{x<em _t="&lt;t">{\leq t}, \boldsymbol{a}</em>\right)$ can be trained by maximizing the evidence lower bound [15]:</p>
<p>$$
\begin{aligned}
&amp; \log p\left(\boldsymbol{x}<em T="T" _leq="\leq">{\leq T} \mid \boldsymbol{a}</em>}\right)=\log \int p\left(\boldsymbol{z<em _T="&lt;T">{\leq T}, \boldsymbol{a}</em>}, \boldsymbol{x<em _leq="\leq" t="t">{\leq T}\right) d \boldsymbol{z}</em> \geq \
&amp; \sum_{t}\left(\underbrace{\mathbb{E}<em t="t">{q\left(\boldsymbol{z}</em>} \mid \boldsymbol{x<em _t="&lt;t">{\leq t}, \boldsymbol{a}</em>}\right)}\left[\log p\left(\boldsymbol{x<em t="t">{t} \mid \boldsymbol{z}</em>}\right)\right]<em q_left_boldsymbol_z="q\left(\boldsymbol{z">{:=\mathcal{J} \text { likelihood }}\right. \
&amp; \left.\underbrace{-\mathbb{E}</em><em t_1="t+1">{t}\right) \mid}\left[\mathrm{KL}\left[q\left(\boldsymbol{z}</em>} \mid \boldsymbol{x<em _t_1="&lt;t+1">{\leq t+1}, \boldsymbol{a}</em>}\right) | p\left(\boldsymbol{z<em t="t">{t+1} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em>{t}\right)\right]\right]}</em>\right) .
\end{aligned}
$$}_{\mathrm{KL}}</p>
<p>If the models are defined to be differentiable and trainable, this objective can be maximized by the stochastic gradient ascent via backpropagation.</p>
<p>Multi-step variational inference is proposed in [15] to improve long-term predictions. This inference, named latent overshooting, involves the multi-step objective $\mathcal{J}_{k}^{\mathrm{KL}}$ defined</p>
<p>as:</p>
<p>$$
\begin{gathered}
\mathcal{J}^{\mathrm{KL}} \geq \mathcal{J}<em p_left_boldsymbol_z="p\left(\boldsymbol{z">{k}^{\mathrm{KL}}:=\mathbb{E}</em><em t-k="t-k">{t} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em t-k="t-k">{&lt;t}\right) q\left(\boldsymbol{z}</em>} \mid \boldsymbol{x<em _t-k="&lt;t-k">{\leq t-k}, \boldsymbol{a}</em>[\Leftrightarrow \
\mathrm{KL}\left[q\left(\boldsymbol{z}}\right)<em _leq="\leq" t_1="t+1">{t+1} \mid \boldsymbol{x}</em>}, \boldsymbol{a<em t_1="t+1">{&lt;t+1}\right) | p\left(\boldsymbol{z}</em>} \mid \boldsymbol{z<em t="t">{t}, \boldsymbol{a}</em>\right)\right]
\end{gathered}
$$</p>
<p>where</p>
<p>$$
p\left(\boldsymbol{z}<em t-k="t-k">{t} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em p_left_boldsymbol_z="p\left(\boldsymbol{z">{&lt;t}\right):=\mathbb{E}</em><em t-k="t-k">{t-1} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em t="t">{&lt;t-1}\right)}\left[p\left(\boldsymbol{z}</em>} \mid \boldsymbol{z<em t-1="t-1">{t-1}, \boldsymbol{a}</em>\right)\right]
$$</p>
<p>is the multi-step prediction model.
For the purpose of planning or policy optimization, not only for the dynamics model $p\left(\boldsymbol{z}<em t="t">{t+1} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em t="t">{t}\right)$ but also the reward function $p\left(r</em>} \mid \boldsymbol{z<em t="t">{t}\right)$ is also required. To do this, we can simply regard the rewards as observations and learn the reward function $p\left(r</em>} \mid \boldsymbol{z<em t="t">{t}\right)$ along with the decoder $p\left(\boldsymbol{x}</em>} \mid \boldsymbol{z<em t="t">{t}\right)$. For readability, we omit the specifications of the reward function $p\left(r</em>\right)$ in the following discussion. Although we remove the decoder later, the reward function and its likelihood objective are kept untouched.} \mid \boldsymbol{z}_{t</p>
<h2>B. Recurrent State Space Model and Dreamer</h2>
<p>The recurrent state space model (RSSM) is a latent dynamics model equipped with an expressive recurrent neural network, realizing accurate long-term prediction. RSSM is used as an essential component of various MBRL methods from pixels [15]-[17], [19], [31] including Dreamer. RSSM assumes the latent $\boldsymbol{z}<em t="t">{t}$ comprises $\boldsymbol{z}</em>}=\left(\boldsymbol{s<em t="t">{t}, \boldsymbol{h}</em>}\right)$ where $\boldsymbol{s<em t="t">{t}, \boldsymbol{h}</em>$ are the probabilistic and deterministic variables, respectively. RSSM's generative and inference models are defined as:</p>
<p>$$
\text { Generative models : }\left{\begin{array}{l}
\boldsymbol{h}<em t-1="t-1">{t}=f^{\mathrm{GRU}}\left(\boldsymbol{h}</em>}, \boldsymbol{s<em t-1="t-1">{t-1}, \boldsymbol{a}</em>\right) \
\boldsymbol{s}<em t="t">{t} \sim p\left(\boldsymbol{s}</em>} \mid \boldsymbol{h<em t="t">{t}\right) \
\boldsymbol{x}</em>} \sim p\left(\boldsymbol{x<em t="t">{t} \mid \boldsymbol{h}</em>\right)
\end{array}\right.
$$}, \boldsymbol{s}_{t</p>
<p>Inference model : $\boldsymbol{s}<em t="t">{t} \sim q\left(\boldsymbol{s}</em>} \mid \boldsymbol{h<em t="t">{t}, \boldsymbol{x}</em>\right)$,
where deterministic $\boldsymbol{h}<em t="t">{t}$ is considered to be the hidden state of the gated recurrent unit (GRU) $f^{\mathrm{GRU}}(\cdot)$ [32] so that historical information can be embedded into $\boldsymbol{h}</em>$.</p>
<p>Dreamer [16] makes use of RSSM as a differentiable dynamics and efficiently learns the behaviors via backpropagation of Bellman errors estimated from imagined trajectories. Dreamer's training procedure is simply summarized as follows: (1) Train RSSM with a given dataset by optimizing Eq. (2). (2) Train a policy from the latent imaginations. (3) Execute the trained policy in a real environment and augment the dataset with the observed results. The above steps are iteratively executed until the policy performs as expected.</p>
<h2>C. Contrastive Learning of RSSM</h2>
<p>The original Dreamer paper [16] also introduced a likelihood-free objective by reformulating $\mathcal{J}^{\text {likelihood }}$ of Eq. (2). By adding a constant $\log p\left(\boldsymbol{x}_{t}\right)$ and applying Bayes'
theorem, we get a decoder-free objective:</p>
<p>$$
\begin{aligned}
&amp; \mathcal{J}^{\text {likelihood }} \stackrel{\circ}{=} \mathbb{E}<em t="t">{q\left(\boldsymbol{z}</em>} \mid \cdot\right)}\left[\log p\left(\boldsymbol{x<em z="z">{t} \mid \boldsymbol{z}</em>}\right)-\log p\left(\boldsymbol{x<em q_left_boldsymbol_z="q\left(\boldsymbol{z">{t}\right)\right] \
&amp; =\mathbb{E}</em><em t="t">{t} \mid \cdot\right)}\left[\log p\left(\boldsymbol{z}</em>} \mid \boldsymbol{x<em t="t">{t}\right)-\log p\left(\boldsymbol{z}</em>\right)\right] \
&amp; \geq \mathbb{E}<em t="t">{q\left(\boldsymbol{z}</em>} \mid \cdot\right)}\left[\log p\left(\boldsymbol{z<em t="t">{t} \mid \boldsymbol{x}</em>\right)\right] \
&amp; :=\mathcal{J}^{\mathrm{NCE}}
\end{aligned}
$$}\right)-\log \sum_{\boldsymbol{x}^{\prime} \in \mathcal{D}} p\left(\boldsymbol{z}_{t} \mid \boldsymbol{x}^{\prime</p>
<p>where $\mathcal{D}$ denotes the mini-batch and the lower bound in the second line was from the Info-NCE (noise-contrastive estimator) mini-batch bound [33]. Let $B$ be the batch size of $\mathcal{D}, \mathcal{J}^{\mathrm{NCE}}$ is considered as a $B$-class categorical cross entropy objective to discriminate the positive pair $\left(\boldsymbol{z}<em t="t">{t}, \boldsymbol{x}</em>}\right)$ among the other negative pairs $\left(\boldsymbol{z<em t="t">{t}, \boldsymbol{x}^{\prime}\left(\neq \boldsymbol{x}</em>}\right)\right)$. In this interpretation, $p\left(\boldsymbol{z<em t="t">{t} \mid \boldsymbol{x}</em>$.}\right)$ can be considered as a discriminator to discern the positive pairs. Representation learning with this type of objective is known as contrastive learning [22], [23] that encourages the embeddings to be sufficiently seperated from each other in the latent space. However, the experiment in [16] has demonstrated that this objective significantly degrades the performance compared to the original objective $\mathcal{J}^{\text {likelihood }</p>
<h2>IV. Proposed Contrastive Learning and MBRL Method</h2>
<h2>A. Deriving Another Contrastive Objective</h2>
<p>We propose to further reformulate $\mathcal{J}^{\mathrm{NCE}}$ of Eq. (5) by introducing a multi-step prediction model: $\tilde{p}\left(\boldsymbol{z}<em t-k="t-k">{t} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em _tilde_p="\tilde{p">{&lt;t}\right):=\mathbb{E}</em>}\left(\boldsymbol{z<em t-k="t-k">{t-1} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em t="t">{&lt;t-1}\right)}\left[\tilde{p}\left(\boldsymbol{z}</em>} \mid \boldsymbol{z<em t-1="t-1">{t-1}, \boldsymbol{a}</em>}\right)\right]$. The accent of $\tilde{p}$ implies that an independent dynamics model from $p\left(\boldsymbol{z<em t-1="t-1">{t} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em q_left_boldsymbol_z="q\left(\boldsymbol{z">{t-1}\right)$ in Eq. (2) can be employed here. By multiplying a constant $\mathbb{E}</em>$ as:}_{t-k} \mid \cdot\right)}\left[\tilde{p}(\cdot) / \tilde{p}(\cdot)\right]=1$, we obtain an importance sampling form of $\mathcal{J}^{\mathrm{NCE}</p>
<p>$$
\begin{aligned}
\mathcal{J}^{\mathrm{NCE}}= &amp; \mathbb{E}<em t="t">{\tilde{p}\left(\boldsymbol{z}</em>} \mid \boldsymbol{z<em _t="&lt;t">{t-k}, \boldsymbol{a}</em>}\right) q\left(\boldsymbol{z<em t="t">{t-k} \mid \cdot\right)}\left[\frac{q\left(\boldsymbol{z}</em>} \mid \cdot\right)}{\tilde{p}\left(\boldsymbol{z<em t="t">{t} \mid \cdot\right)} \times \leftarrow\right. \
&amp; \left.\left(\log p\left(\boldsymbol{z}</em>} \mid \boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">{t}\right)-\log \sum</em>\right)\right)\right]
\end{aligned}
$$}^{\prime}} p\left(\boldsymbol{z}_{t} \mid \boldsymbol{x}^{\prime</p>
<p>For computational simplicity, we approximate the likelihood ratio $q\left(\boldsymbol{z}<em t="t">{t} \mid \cdot\right) / \tilde{p}\left(\boldsymbol{z}</em>$, where} \mid \cdot\right)$ as a constant and assume that the summation of $\mathcal{J}^{\mathrm{NCE}}$ across batch and time dimension is approximated as: $\sum \mathcal{J}^{\mathrm{NCE}} \widetilde{\propto} \sum \mathcal{J}_{k}^{\mathrm{NCE}</p>
<p>$$
\begin{aligned}
&amp; \mathcal{J}<em _tilde_p="\tilde{p">{k}^{\mathrm{NCE}}:= \
&amp; \mathbb{E}</em>}\left(\boldsymbol{z<em t-k="t-k">{t} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em t-k="t-k">{&lt;t}\right) q\left(\boldsymbol{z}</em>} \mid \cdot\right)}\left[\log p\left(\boldsymbol{z<em t="t">{t} \mid \boldsymbol{x}</em>\right)\right]
\end{aligned}
$$}\right)-\log \sum_{\boldsymbol{x}^{\prime}} p\left(\boldsymbol{z}_{t} \mid \boldsymbol{x}^{\prime</p>
<p>We further import the concept of overshooting and optimize $\mathcal{J}<em k="k">{k}^{\mathrm{NCE}}$ along with $\mathcal{J}</em>$ on multi-step prediction of varying $k$ s. Finally, the objective we use to train RSSM is:}^{\mathrm{KL}</p>
<p>$$
\mathcal{J}:=\sum_{k=0}^{K}\left(\mathcal{J}<em k="k">{k}^{\mathrm{NCE}}+\mathcal{J}</em>\right)
$$}^{\mathrm{KL}</p>
<p>Note that $\mathcal{J}<em k="k">{k}^{\mathrm{NCE}}$ and $\mathcal{J}</em>}^{\mathrm{KL}}$ have different dynamics model (i.e., $\tilde{p}\left(\boldsymbol{z<em t="t">{t} \mid \cdot\right)$ and $p\left(\boldsymbol{z}</em> \mid \cdot\right)$, respectively).</p>
<h3>V-B Relation among the Objectives</h3>
<p>As shown in Appx. I, $\mathcal{J}^{\mathrm{NCE}}$ is a lower bound of the mutual information $I(\boldsymbol{x}<em t="t">{t} ;\boldsymbol{z}</em>})$, while $\mathcal{J<em t="t">{k}^{\mathrm{NCE}}$ is a bound of $I(\boldsymbol{x}</em>} ;\boldsymbol{z<em t="t">{t-k})$. Since the latent state sequence is Markovian, we have the data processing inequality as $I(\boldsymbol{x}</em>} ;\boldsymbol{z<em t="t">{t}) \geq$ $I(\boldsymbol{x}</em>} ; \boldsymbol{z<em k="k">{t-k})$. In other words, $\mathcal{J}^{\mathrm{NCE}}$ and approximately derived $\mathcal{J}</em>}^{\mathrm{NCE}}$ share the same InfoMax upper bound metrics. An intuitive motivation to introduce $\mathcal{J<em t="t">{k}^{\mathrm{NCE}}$ instead of $\mathcal{J}^{\mathrm{NCE}}$ is so that we can incorporate temporal correlation between $t$ and $t-k$. Another motivation is that we can increase the model capacity of the discriminator $p\left(\boldsymbol{z}</em>} \mid \boldsymbol{x<em t="t">{t}\right)$ by incorporating the independent dynamics model $\tilde{p}\left(\boldsymbol{z}</em> \mid \cdot\right)$.</p>
<h2>C. Model Definitions</h2>
<p>This section discusses how we define the discriminator components: $p\left(\boldsymbol{z}<em t="t">{t} \mid \boldsymbol{x}</em>}\right)$ and $\tilde{p}\left(\boldsymbol{z<em t-1="t-1">{t} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em t="t">{t-1}\right)$. Ref. [34] has empirically shown that the inductive bias from model architectures is a significant factor for contrastive learning. As experimentally recommended in the literature, we define $p\left(\boldsymbol{z}</em>} \mid \boldsymbol{x<em _boldsymbol_z="\boldsymbol{z">{t}\right)$ as an exponentiated bilinear similarity function parameterized with $W</em>$ :} \mid \boldsymbol{x}</p>
<p>$$
p\left(\boldsymbol{z}<em t="t">{t} \mid \boldsymbol{x}</em>}\right) \propto \exp \left(\boldsymbol{z<em _boldsymbol_z="\boldsymbol{z">{t}^{\top} W</em>\right)
$$} \mid \boldsymbol{x}} \boldsymbol{e}_{t</p>
<p>where $\boldsymbol{e}<em t="t">{t}:=f^{\mathrm{CNN}}\left(\boldsymbol{x}</em>}\right)$ and $f^{\mathrm{CNN}}(\cdot)$ denotes feature extraction by a CNN unit. With this definition, $\mathcal{J<em t="t">{k}^{\mathrm{NCE}}$ is simply a softmax cross-entropy objective with logits $\boldsymbol{z}^{\top} W \boldsymbol{e}$. Contrary to the previous contrastive learning literature [22], [34], the definition of newly introduced $\tilde{p}\left(\boldsymbol{z}</em>} \mid \boldsymbol{z<em t-1="t-1">{t-1}, \boldsymbol{a}</em>\right)$ is required. Here, we propose to apply linear modeling to define the model deterministically as:</p>
<p>$$
\begin{aligned}
&amp; \tilde{p}\left(\boldsymbol{z}<em t-1="t-1">{t} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em t="t">{t-1}\right):=\delta\left(\boldsymbol{z}</em>}-\boldsymbol{z<em t="t">{t}^{\prime}\right) \
&amp; \text { where } \boldsymbol{z}</em>}^{\prime}:=W_{\boldsymbol{z}} \boldsymbol{z<em _boldsymbol_a="\boldsymbol{a">{t-1}+W</em>
\end{aligned}
$$}} \boldsymbol{a}_{t-1</p>
<p>$\delta$ is the Dirac delta function, and $W_{\boldsymbol{z}, \boldsymbol{a}}$ are linear parameters.
This linear modeling of $\tilde{p}\left(\boldsymbol{z}<em k="k">{t} \mid \cdot\right)$ successfully regularizes $\mathcal{J}</em>}^{\mathrm{NCE}}$ and contributes to construct smooth latent space. We can alternatively define $\tilde{p}\left(\boldsymbol{z<em t="t">{t} \mid \cdot\right):=p\left(\boldsymbol{z}</em>} \mid \cdot\right)$, where $p\left(\boldsymbol{z<em k="k">{t} \mid \cdot\right)$ is generally defined as an expressive model aiming at precise prediction. However, the high model capacity allows to embed temporally consecutive samples too distant from each other to sufficiently optimize $\mathcal{J}</em>$, thus yielding unsmooth latent space.}^{\mathrm{NCE}</p>
<h2>D. Instantiation with RSSM</h2>
<p>Figure 3 illustrates the architecture to compute $\mathcal{J}<em _leq="\leq" t="t">{k}^{\mathrm{NCE}}$. We describe the two paramount components which characterize our proposed contrastive learning scheme as follows:
(i) Independent linear forward dynamics: As previously proposed in Sec. IV-C, we employ a simple linear forward dynamics $\tilde{p}$, which is used only for contrastive learning. During the policy optimization phase, the expressive model with GRU is alternatively utilized to make the most out of its long-term prediction accuracy.
(ii) Data augmentation: We append two independent image preprocessors which process two sets of input images (i.e., $\boldsymbol{x}</em>$ ). Considering the empirical success
}$ and $\boldsymbol{x}_{t+1: t+K<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. The RSSM-based architecture to compute $\mathcal{J}<em t="t">{k}^{\mathrm{NCE}}$. CNN, GRU and FC represent a convolutional neural network, a GRU-cell, and a fullyconnected layer, respectively. In module (a), latent states $\boldsymbol{s}</em>}$ are recurrently inferred given $\boldsymbol{x<em t_1:="t+1:" t_K="t+K">{\leq t}$. In module (b), $\boldsymbol{s}</em>}$ are sequentially predicted by the linear model $W_{\boldsymbol{z}, \boldsymbol{a}}$, and then they are compared with the observations $\boldsymbol{x<em k="k">{t+1: t+K}$ to compute logits. For readability, we only illustrate positive logits, however, negative logits are also computed by pairing samples from different time-steps or frames, yielding $(B \times K)^{2}$ logits. To compute $\mathcal{J}</em>-1$ logits are used for negative logits.
of the previous literature [23], [25], [28]-[30], we adopt the random crop of images. In our implementation, the original image shaped $(72,72)$ is cropped to be $(64,64)$. The origin of the crop rectangle is determined at each preprocessor randomly and indenpendently. This makes it difficult for the contrastive learner to discriminate correct positive pairs, encouraging only informative features for control to be extracted.}^{\mathrm{NCE}}$ of a certain positive logit, remaining $(B \times K)^{2</p>
<p>We propose a decoder-free variant of Dreamer, which we call Dreamer with InfoMax and without generative decoder (Dreaming). Dreaming trains a policy as almost same way with the original Dreamer. The only difference between the methods is that we alternatively use the contrastive learning scheme introduced in the previous section to train RSSM. We implement Dreaming in TensorFlow [35] by modifying the official source code of Dreamer ${ }^{1}$. We keep all hyperparameters and experimental conditions similar to the original ones. A newly introduced hyperparameter $K$ in Eq. (8) (overshooting distance) is set to be $K=3$ based on the ablation study in Appx. III.</p>
<h2>V. EXPERIMENTS</h2>
<h2>A. Comparison to State-of-the-art Methods</h2>
<p>The main objective of this experiment is to demonstrate that Dreaming has advantages over the baseline method Dreamer [16] on difficult 5 manipulation tasks exhibited in Fig. 1, in which Dreamer suffers from object vanishing. We also prepare a likelihood-free variant of Dreamer introduced in Sec. III-C, which utilizes the vanilla contrastive objective $\mathcal{J}^{\mathrm{NCE}}$ instead of $\mathcal{J}_{k}^{\mathrm{NCE}}$ and $\mathcal{J}^{\text {likelihood }}$. The specifications of the two original tasks, UR5-reach and Connector-insert, are described in Appx. II. For the difficult 5 tasks, we also compare the performance with the latest cutting edge MFRL methods, which are CURL [25], DrQ [29] and RAD [28].</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>TABLE I
PERFORMANCE ON 15 BENCHMARK TASKS AROUND 500K ENVIRONMENT STEPS (100K ONLY FOR CUP-CATCH).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dreaming (ours)</th>
<th style="text-align: center;">Dreamer w/ $\mathcal{J}^{\text {likelihood }}[16]$</th>
<th style="text-align: center;">Dreamer w/ $\mathcal{J}^{\text {NCE }}[16]$</th>
<th style="text-align: center;">CURL [25]</th>
<th style="text-align: center;">DrQ [29]</th>
<th style="text-align: center;">RAD [28]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(A) Manipulation tasks where object vanishing is critical</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Cup-catch (100K)</td>
<td style="text-align: center;">$925 \pm 48$</td>
<td style="text-align: center;">$698 \pm 350$</td>
<td style="text-align: center;">$609 \pm 404$</td>
<td style="text-align: center;">$693 \pm 334$</td>
<td style="text-align: center;">$882 \pm 174$</td>
<td style="text-align: center;">$792 \pm 315$</td>
</tr>
<tr>
<td style="text-align: center;">Reacher-hard</td>
<td style="text-align: center;">$868 \pm 272$</td>
<td style="text-align: center;">$8 \pm 33$</td>
<td style="text-align: center;">$115 \pm 298$</td>
<td style="text-align: center;">$431 \pm 435$</td>
<td style="text-align: center;">$616 \pm 464$</td>
<td style="text-align: center;">$783 \pm 370$</td>
</tr>
<tr>
<td style="text-align: center;">Finger-turn-hard</td>
<td style="text-align: center;">$752 \pm 325$</td>
<td style="text-align: center;">$264 \pm 368$</td>
<td style="text-align: center;">$222 \pm 379$</td>
<td style="text-align: center;">$339 \pm 443$</td>
<td style="text-align: center;">$270 \pm 427$</td>
<td style="text-align: center;">$303 \pm 443$</td>
</tr>
<tr>
<td style="text-align: center;">UR5-reach</td>
<td style="text-align: center;">$845 \pm 147$</td>
<td style="text-align: center;">$652 \pm 230$</td>
<td style="text-align: center;">$592 \pm 271$</td>
<td style="text-align: center;">$729 \pm 201$</td>
<td style="text-align: center;">$633 \pm 312$</td>
<td style="text-align: center;">$642 \pm 274$</td>
</tr>
<tr>
<td style="text-align: center;">Connector-insert</td>
<td style="text-align: center;">$629 \pm 391$</td>
<td style="text-align: center;">$169 \pm 348$</td>
<td style="text-align: center;">$304 \pm 399$</td>
<td style="text-align: center;">$297 \pm 384$</td>
<td style="text-align: center;">$183 \pm 361$</td>
<td style="text-align: center;">$367 \pm 387$</td>
</tr>
<tr>
<td style="text-align: center;">(B) Manipulation tasks where object vanishing is NOT critical</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Reacher-easy</td>
<td style="text-align: center;">$905 \pm 210$</td>
<td style="text-align: center;">$947 \pm 145$</td>
<td style="text-align: center;">$183 \pm 325$</td>
<td style="text-align: center;">$834 \pm 286$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Finger-turn-easy</td>
<td style="text-align: center;">$661 \pm 394$</td>
<td style="text-align: center;">$689 \pm 394$</td>
<td style="text-align: center;">$232 \pm 398$</td>
<td style="text-align: center;">$576 \pm 464$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Finger-spin</td>
<td style="text-align: center;">$762 \pm 113$</td>
<td style="text-align: center;">$763 \pm 188$</td>
<td style="text-align: center;">$886 \pm 169$</td>
<td style="text-align: center;">$922 \pm 55$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">(C) Pole-swingup tasks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Pendulum-swingup</td>
<td style="text-align: center;">$811 \pm 98$</td>
<td style="text-align: center;">$432 \pm 408$</td>
<td style="text-align: center;">$825 \pm 106$</td>
<td style="text-align: center;">$46 \pm 207$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Acrobot-swingup</td>
<td style="text-align: center;">$267 \pm 177$</td>
<td style="text-align: center;">$98 \pm 119$</td>
<td style="text-align: center;">$48 \pm 54$</td>
<td style="text-align: center;">$4 \pm 14$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Cartpole-swingup-sparse</td>
<td style="text-align: center;">$465 \pm 328$</td>
<td style="text-align: center;">$317 \pm 345$</td>
<td style="text-align: center;">$197 \pm 79$</td>
<td style="text-align: center;">$17 \pm 17$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">(D) Locomotion tasks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Quadrupled-walk</td>
<td style="text-align: center;">$719 \pm 193$</td>
<td style="text-align: center;">$441 \pm 219$</td>
<td style="text-align: center;">$201 \pm 272$</td>
<td style="text-align: center;">$188 \pm 174$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Walker-walk</td>
<td style="text-align: center;">$469 \pm 123$</td>
<td style="text-align: center;">$955 \pm 19$</td>
<td style="text-align: center;">$483 \pm 111$</td>
<td style="text-align: center;">$914 \pm 33$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Cheetah-run</td>
<td style="text-align: center;">$566 \pm 118$</td>
<td style="text-align: center;">$781 \pm 132$</td>
<td style="text-align: center;">$303 \pm 174$</td>
<td style="text-align: center;">$580 \pm 56$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Hopper-hop</td>
<td style="text-align: center;">$78 \pm 55$</td>
<td style="text-align: center;">$172 \pm 114$</td>
<td style="text-align: center;">$25 \pm 29$</td>
<td style="text-align: center;">$10 \pm 17$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>In addition, another variety of 10 DMC tasks are evaluated, which are categorized into three classes namely; manipulation, pole-swingup, and locomotion. For the additional 10 tasks, only CURL is selected as an MFRL representative.</p>
<p>Table I summarizes the training results benchmarked at certain environment steps. The results show the mean and standard deviation averaged 4 seeds and 10 consecutive trajectories. This table shows a similar result as in [16] that decoder-free Dreamer with the vanilla contrastive objective $\mathcal{J}^{\mathrm{NCE}}$ degrades the performances on most of tasks than decoder-based Dreamer with $\mathcal{J}^{\text {likelihood }}$. In the following discussions, we use the decoder-based Dreamer as a primary baseline. (A) We put much focus on these difficult tasks and it can be seen that Dreaming consistently achieves better performance than Dreamer. Hence, this indicates that the decoder-free nature of the proposed method successfully surmounts the object vanishing problem. In addition, Dreaming achieves outperforming performance than the leading MFRL methods. (B) On other manipulation tasks, there are no significant difference between Dreaming and Dreamer because the key objects are large enough. (C) Since the poleswingup tasks also cause vanishing of thin poles, Dreaming takes better performance than Dreamer. (D) Dreaming lags behind the Dreamer on 3 of 4 locomotion tasks, i.e., planar locomotion tasks (Walker-walk, Cheetah-run and Hopperhop). On these tasks, the cameras always track the center of locomotive robots, and this causes the key control information (i.e., velocity) to be extracted from the background texture. We suppose that this robot-centric nature makes it difficult for the contrastive learner to extract such information because only robots' attitudes provide enough information to discriminate different samples.</p>
<p>Figure 4 shows video prediction by Dreaming, in which principal features for control (e.g., positions and orientations)
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Open-loop video predictions. The left 5 consecutive images show reconstructed context frames and the remaining images are generated open-loop. The decoder is trained independently without backpropagating reconstruction errors to other models.
are successfully reconstructed from the embeddings learned without likelihood objective. However on Cheeta-run, another kind of object vanishing arises; the checkered floor pattern, which is required to extract the velocity information, is vanished.</p>
<h2>B. Ablation Study</h2>
<p>This experiment is conducted to analyze how the major components of the proposed representation learning, introduced in Sec. IV-D, contribute to the overall performance. For this purpose, some variants of the proposed method have been prepared: (i) the effect of independent linear dynamics is demonstrated with a variant that has shared dynamics</p>
<p>TABLE II: ABLATION STUDY: THE EFFECTS OF (i) LINEAR FORWARD DYNAMICS (LEFT) AND (ii) DATA AUGMENTATION (RIGHT).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\tilde{p}$ linear <br> as Eq. (10)</th>
<th style="text-align: center;">$\tilde{p}:=p$ <br> as in [21]</th>
<th style="text-align: center;">Random crop</th>
<th style="text-align: center;">-</th>
<th style="text-align: center;">✓</th>
<th style="text-align: center;">-</th>
<th style="text-align: center;">✓</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Cup-catch (100K)</td>
<td style="text-align: center;">925 $\pm$</td>
<td style="text-align: center;">575 $\pm$</td>
<td style="text-align: center;">449</td>
<td style="text-align: center;">Color jitter</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">✓</td>
</tr>
<tr>
<td style="text-align: center;">Reacher-hard</td>
<td style="text-align: center;">868 $\pm$</td>
<td style="text-align: center;">272</td>
<td style="text-align: center;">232 $\pm$</td>
<td style="text-align: center;">370</td>
<td style="text-align: center;">Cup-catch (100K)</td>
<td style="text-align: center;">866 $\pm$</td>
<td style="text-align: center;">133</td>
</tr>
<tr>
<td style="text-align: center;">Finger-turn-hard</td>
<td style="text-align: center;">752 $\pm$</td>
<td style="text-align: center;">325</td>
<td style="text-align: center;">263 $\pm$</td>
<td style="text-align: center;">369</td>
<td style="text-align: center;">Reacher-hard</td>
<td style="text-align: center;">11 $\pm$</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Finger-turn-hard</td>
<td style="text-align: center;">114 $\pm$</td>
<td style="text-align: center;">283</td>
</tr>
</tbody>
</table>
<p>$\tilde{p}\left(\boldsymbol{z}<em t-1="t-1">{t} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em t="t">{t-1}\right):=p\left(\boldsymbol{z}</em>} \mid \boldsymbol{z<em t-1="t-1">{t-1}, \boldsymbol{a}</em>$, (ii) the effect of data augmentation is demonstrated by removing the image preprocessors shown in Fig. 3. We also prepare another data augmentation called color jittering [23], [28], [29], for reference. Only three tasks, which are Cup-catch, Reacherhard, and Finger-turn-hard, are taken into this experiment. Tables II summarize the results from the performed ablation study, which reveals that both of the proposed components are essential to achieve state-of-the-art results.}\right)^{2</p>
<h2>VI. CONCLUSION</h2>
<p>In the present paper, we proposed Dreaming, a decoderfree extension of the state-of-the-art MBRL method from pixels, Dreamer. A likelihood-free contrastive objective was derived by reformulating the original ELBO of Dreamer. We incorporated the two indispensable components below to the contrastive learning: (i) independent and linear forward dynamics, (ii) the random crop data augmentation. By making the most of the decoder-free nature and the two components, Dreaming was able to outperform the baseline methods on difficult tasks especially where Dreamer suffers from object vanishing.</p>
<p>An disadvantage we observed in the experiments was that Dreaming degraded the training performance on planar locomotion tasks (e.g., Walker-walk), where the contrastive learner has to focus on not only robots but also the background texture. This weak point should be resolved in future work as it may affect industrial manipulation tasks where first-person-view from robots dynamically changes. Another future research direction is to incorporate the uncertaintyaware concepts proposed in recent MBRL studies [1], [2], [19], [30]. Although we have achieved state-of-the-art results on some difficult tasks, we have often observed overfitted behaviors during the early training phase. We believe that this model-bias problem [36] can be successfully solved by the above state-of-the-art strategy.</p>
<h2>APPENDIX I</h2>
<p>DERIVATION
In this section, we clarify that $\mathcal{J}<em t="t">{k}^{\mathrm{NCE}}$ is a lower bound of $I\left(\boldsymbol{x}</em>}, \boldsymbol{z<em k="k">{t-k}\right) . \mathcal{J}</em>$ can be rewriten as:}^{\mathrm{NCE}</p>
<p>$$
\begin{aligned}
&amp; \mathcal{J}<em q_left_boldsymbol_z="q\left(\boldsymbol{z">{k}^{\mathrm{NCE}}= \
&amp; \mathbb{E}</em><em t="t">{t-k} \mid \cdot\right)}\left[\log f\left(\boldsymbol{x}</em>}, \boldsymbol{z<em _boldsymbol_x="\boldsymbol{x">{t-k}\right)-\log \sum</em>\right)\right]
\end{aligned}
$$}^{\prime} \in \mathcal{D}} f\left(\boldsymbol{x}^{\prime}, \boldsymbol{z}_{t-k</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. UR5-reach (left) is to bring the robot end effector to goal positions. The observation is a blended image of two different views, implicitly providing depth information. Connector-insert (right) is to insert a millimeter-sized connector gripped by a robot to a socket. This tasks is originally introduced in [37]. Since the gap between the connector and socket is very tight, pixel-wise precise control is required. In the both tasks, the goal positions are initialized at random.</p>
<p>TABLE III
ABLATION STUDY: THE EFFECT OF THE OVERSHOOTING DISTANCE $K$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$K=1$</th>
<th style="text-align: center;">$K=3$</th>
<th style="text-align: center;">$K=5$</th>
<th style="text-align: center;">$K=7$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Cup-catch (100K)</td>
<td style="text-align: center;">$280 \pm 437$</td>
<td style="text-align: center;">$\mathbf{9 2 5} \pm 48$</td>
<td style="text-align: center;">$734 \pm 378$</td>
<td style="text-align: center;">$736 \pm 378$</td>
</tr>
<tr>
<td style="text-align: left;">Reacher-hard</td>
<td style="text-align: center;">$234 \pm 364$</td>
<td style="text-align: center;">$\mathbf{8 6 8} \pm 272$</td>
<td style="text-align: center;">$\underline{561} \pm 447$</td>
<td style="text-align: center;">$471 \pm 433$</td>
</tr>
<tr>
<td style="text-align: left;">Finger-turn-hard</td>
<td style="text-align: center;">$354 \pm 438$</td>
<td style="text-align: center;">$\mathbf{7 5 2} \pm 325$</td>
<td style="text-align: center;">$468 \pm 432$</td>
<td style="text-align: center;">$715 \pm 375$</td>
</tr>
</tbody>
</table>
<p>where $f\left(\boldsymbol{x}<em t-k="t-k">{t}, \boldsymbol{z}</em>}\right)$ includes deterministic multi-step prediction with $\tilde{p}\left(\boldsymbol{z<em t-k="t-k">{t} \mid \boldsymbol{z}</em>}, \boldsymbol{a<em _t="&lt;t">{&lt;t}\right)$ and computation of the bilinear similarity by Eq. (9). For ease of notation, actions $\boldsymbol{a}</em>$ in the conditioning set are omitted from $f(\cdot)$. As already shown in [22], the optimal value of $f(\cdot)$ is given by:</p>
<p>$$
f\left(\boldsymbol{x}<em t-k="t-k">{t}, \boldsymbol{z}</em>}\right) \propto p\left(\boldsymbol{x<em t-k="t-k">{t} \mid \boldsymbol{z}</em>\right)
$$}\right) / p\left(\boldsymbol{x}_{t</p>
<p>By applying Bayes' theorem $f\left(\boldsymbol{x}<em t-k="t-k">{t}, \boldsymbol{z}</em>}\right) \quad \propto$ $p\left(\boldsymbol{z<em t="t">{t-k} \mid \boldsymbol{x}</em>\right)$ and inserting this to Eq. (11), we get:}\right) / p\left(\boldsymbol{z}_{t-k</p>
<p>$$
\begin{aligned}
\mathcal{J}<em q_left_boldsymbol_z="q\left(\boldsymbol{z">{k}^{\mathrm{NCE}} &amp; \propto \mathbb{E}</em><em t-k="t-k">{t-k} \mid \cdot\right)}\left[\log p\left(\boldsymbol{z}</em>} \mid \boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">{t}\right)-\log \sum</em>}^{\prime}} p\left(\boldsymbol{z<em q_left_boldsymbol_z="q\left(\boldsymbol{z">{t-k} \mid \boldsymbol{x}^{\prime}\right)\right] \
&amp; \leq \mathbb{E}</em><em t-k="t-k">{t-k} \mid \cdot\right)}\left[\log p\left(\boldsymbol{z}</em>} \mid \boldsymbol{x<em t-k="t-k">{t}\right)-\log p\left(\boldsymbol{z}</em>\right)\right]
\end{aligned}
$$</p>
<p>By marginalizing Eq. (13) with respect to the data distribution, we finally obtain: $\mathbb{E}\left[\mathcal{J}<em t="t">{k}^{\mathrm{NCE}}\right] \leq I\left(\boldsymbol{x}</em>} ; \boldsymbol{z<em t="t">{t-k}\right)$. Note that setting $k=0$ derives $\mathbb{E}\left[\mathcal{J}^{\mathrm{NCE}}\right] \leq I\left(\boldsymbol{x}</em>\right)$.} ; \boldsymbol{z}_{t</p>
<h2>APPENDIX II</h2>
<h2>SPECIFICATIONS OF THE ORIGINAL TASKS</h2>
<p>Figure 5 exhibits the specifications of newly introduced robotics tasks.</p>
<h2>APPENDIX III</h2>
<h2>Ablation Study of Overshooting Distance</h2>
<p>Table III summarizes the ablation study of the overshooting distance $K$, which demonstrates that incorporating temporal correlation of appropriate multi-steps $(K=3)$ is effective.</p>
<h2>REFERENCES</h2>
<p>[1] K. Chua, R. Calandra, R. McAllister, and S. Levine, "Deep reinforcement learning in a handful of trials using probabilistic dynamics models," in NeurIPS, 2018.
[2] M. Okada and T. Taniguchi, "Variational inference MPC for Bayesian model-based reinforcement learning," in CoRL, 2019.
[3] E. Langlois, S. Zhang, G. Zhang, P. Abbeel, and J. Ba, "Benchmarking model-based reinforcement learning," arXiv:1907.02057, 2019.
[4] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, et al., "Model-based reinforcement learning for Atari," in $I C L R, 2020$.
[5] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, "Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor," in ICML, 2018.
[6] S. Bechtle, Y. Lin, A. Rai, L. Righetti, and F. Meier, "Curious iLQR: Resolving uncertainty in model-based RL," in CoRL, 2019.
[7] A. Nagabandi, K. Konolige, S. Levine, and V. Kumar, "Deep dynamics models for learning dexterous manipulation," in CoRL, 2019.
[8] Y. Yang, K. Calawaerts, A. Iscen, T. Zhang, J. Tan, and V. Sindhwani, "Data efficient reinforcement learning for legged robots," in CoRL, 2019.
[9] Y. Zhang, I. Clavera, B. Tsai, and P. Abbeel, "Asynchronous methods for model-based reinforcement learning," in CoRL, 2019.
[10] G. R. Williams, B. Goldfain, K. Lee, J. Gibson, J. M. Rehg, and E. A. Theodorou, "Locally weighted regression pseudo-rehearsal for adaptive model predictive control," in CoRL, 2019.
[11] K. Fang, Y. Zhu, A. Garg, S. Savarese, and L. Fei-Fei, "Dynamics learning with cascaded variational inference for multi-step manipulation," in CoRL, 2019.
[12] D. P. Kingma and M. Welling, "Auto-encoding variational bayes," in $I C L R, 2014$.
[13] D. Ha and J. Schmidhuber, "Recurrent world models facilitate policy evolution," in NeurIPS, 2018.
[14] A. X. Lee, A. Nagabandi, P. Abbeel, and S. Levine, "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model," arXiv:1907.00953, 2019.
[15] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson, "Learning latent dynamics for planning from pixels," in ICML, 2019.
[16] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, "Dream to control: Learning behaviors by latent imagination," ICLR, 2020.
[17] D. Han, K. Doya, and J. Tani, "Variational recurrent models for solving partially observable control tasks," in $I C L R, 2020$.
[18] D. Yarats, A. Zhang, I. Kostrikov, B. Amos, J. Pineau, and R. Fergus, "Improving sample efficiency in model-free reinforcement learning from images," arXiv:1910.01741, 2019.
[19] M. Okada, N. Kosaka, and T. Taniguchi, "PlaNet of the Bayesians: Reconsidering and improving deep planning network by incorporating Bayesian inference," in IROS, 2020.
[20] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, et al., "DeepMind control suite," arXiv:1801.00690, 2018.
[21] W. Yan, A. Vangipuram, P. Abbeel, and L. Pinto, "Learning predictive representations for deformable objects using contrastive estimation," arXiv:2003.05436, 2020.
[22] A. v. d. Oord, Y. Li, and O. Vinyals, "Representation learning with contrastive predictive coding," arXiv:1807.03748, 2018.
[23] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, "A simple framework for contrastive learning of visual representations," in $I C L R, 2020$.
[24] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, "The arcade learning environment: An evaluation platform for general agents," Journal of Artificial Intelligence Research, vol. 47, pp. 253-279, 2013.
[25] A. Srinivas, M. Laskin, and P. Abbeel, "CURL: Contrastive unsupervised representations for reinforcement learning," in ICML, 2020.
[26] A. Zhang, R. McAllister, R. Calandra, Y. Gal, and S. Levine, "Learning invariant representations for reinforcement learning without reconstruction," arXiv:2006.10742, 2020.
[27] X. Ma, P. Karkus, D. Hsu, W. S. Lee, and N. Ye, "Discriminative particle filter reinforcement learning for complex partial observations," in $I C L R, 2020$.
[28] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas, "Reinforcement learning with augmented data," arXiv:2004.14990, 2020.
[29] I. Kostrikov, D. Yarats, and R. Fergus, "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels," arXiv:2004.13649, 2020.
[30] K. Lee, M. Laskin, A. Srinivas, and P. Abbeel, "SUNRISE: A simple unified framework for ensemble learning in deep reinforcement learning," arXiv:2007.04938, 2020.
[31] R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak, "Planning to explore via self-supervised world models," arXiv:2005.05960, 2020.
[32] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, et al., "Learning phrase representations using RNN encoder-decoder for statistical machine translation," arXiv:1406.1078, 2014.
[33] B. Poole, S. Ozair, A. v. d. Oord, A. A. Alemi, and G. Tucker, "On variational bounds of mutual information," in ICML, 2019.
[34] M. Tschannen, J. Djolonga, P. K. Rubenstein, S. Gelly, and M. Lucic, "On mutual information maximization for representation learning," $I C L R, 2020$.
[35] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, et al., "TensorFlow: Large-scale machine learning on heterogeneous systems," 2015.
[36] M. Deisenroth and C. E. Rasmussen, "PILCO: A model-based and data-efficient approach to policy search," in ICML, 2011.
[37] R. Okamura, M. Okada, and T. Taniguchi, "Domain-adversarial and -conditional state space model for imitation learning," in IROS, 2020.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/google-research/dreamer&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2888 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2888</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2888</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-272987039</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.18991v1.pdf" target="_blank">Surveying the MLLM Landscape: A Meta-Review of Current Surveys</a></p>
                <p><strong>Paper Abstract:</strong> The rise of Multimodal Large Language Models (MLLMs) has become a transformative force in the field of artificial intelligence, enabling machines to process and generate content across multiple modalities, such as text, images, audio, and video. These models represent a significant advancement over traditional unimodal systems, opening new frontiers in diverse applications ranging from autonomous agents to medical diagnostics. By integrating multiple modalities, MLLMs achieve a more holistic understanding of information, closely mimicking human perception. As the capabilities of MLLMs expand, the need for comprehensive and accurate performance evaluation has become increasingly critical. This survey aims to provide a systematic review of benchmark tests and evaluation methods for MLLMs, covering key topics such as foundational concepts, applications, evaluation methodologies, ethical concerns, security, efficiency, and domain-specific applications. Through the classification and analysis of existing literature, we summarize the main contributions and methodologies of various surveys, conduct a detailed comparative analysis, and examine their impact within the academic community. Additionally, we identify emerging trends and underexplored areas in MLLM research, proposing potential directions for future studies. This survey is intended to offer researchers and practitioners a comprehensive understanding of the current state of MLLM evaluation, thereby facilitating further progress in this rapidly evolving field.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A survey on game playing agents and large models: Methods, applications, and challenges. <em>(Rating: 2)</em></li>
                <li>Large language models and games: A survey and roadmap. <em>(Rating: 2)</em></li>
                <li>The rise and potential of large language model based agents: A survey. <em>(Rating: 2)</em></li>
                <li>A survey on large language model based autonomous agents. <em>(Rating: 2)</em></li>
                <li>If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents. <em>(Rating: 1)</em></li>
                <li>Large multimodal agents: A survey. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2888",
    "paper_id": "paper-272987039",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A survey on game playing agents and large models: Methods, applications, and challenges.",
            "rating": 2,
            "sanitized_title": "a_survey_on_game_playing_agents_and_large_models_methods_applications_and_challenges"
        },
        {
            "paper_title": "Large language models and games: A survey and roadmap.",
            "rating": 2,
            "sanitized_title": "large_language_models_and_games_a_survey_and_roadmap"
        },
        {
            "paper_title": "The rise and potential of large language model based agents: A survey.",
            "rating": 2,
            "sanitized_title": "the_rise_and_potential_of_large_language_model_based_agents_a_survey"
        },
        {
            "paper_title": "A survey on large language model based autonomous agents.",
            "rating": 2,
            "sanitized_title": "a_survey_on_large_language_model_based_autonomous_agents"
        },
        {
            "paper_title": "If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents.",
            "rating": 1,
            "sanitized_title": "if_llm_is_the_wizard_then_code_is_the_wand_a_survey_on_how_code_empowers_large_language_models_to_serve_as_intelligent_agents"
        },
        {
            "paper_title": "Large multimodal agents: A survey.",
            "rating": 1,
            "sanitized_title": "large_multimodal_agents_a_survey"
        }
    ],
    "cost": 0.008036999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SURVEYING THE MLLM LANDSCAPE: A META-REVIEW OF CURRENT SURVEYS
17 Sep 2024</p>
<p>Ming Li 
Equal contribution
Georgia Institute of Technology</p>
<p>Keyu Chen 
Equal contribution
Georgia Institute of Technology</p>
<p>University of Wisconsin-Madison</p>
<p>Ziqian Bi 
Equal contribution
Indiana University</p>
<p>Ming Liu 
Purdue University</p>
<p>Benji Peng 
Qian Niu 
Kyoto University</p>
<p>Junyu Liu 
Equal contribution
Georgia Institute of Technology</p>
<p>Jinlang Wang jinlang.wang@wisc.edu 
Equal contribution
Georgia Institute of Technology</p>
<p>Equal contribution
Georgia Institute of Technology</p>
<p>University of Wisconsin-Madison</p>
<p>Sen Zhang sen.z@rutgers.edu 
Equal contribution
Georgia Institute of Technology</p>
<p>Equal contribution
Indiana University</p>
<p>University of Wisconsin-Madison</p>
<p>Xuanhe Pan 
Equal contribution
Georgia Institute of Technology</p>
<p>Equal contribution
Indiana University</p>
<p>Purdue University</p>
<p>Jiawei Xu 
Equal contribution
Georgia Institute of Technology</p>
<p>Purdue University</p>
<p>Pohsun Feng 
Equal contribution
Georgia Institute of Technology</p>
<p>Kyoto University</p>
<p>‡ ‡ Rutgers University</p>
<p>University of Wisconsin-Madison</p>
<p>¶ ¶ Purdue University</p>
<p>Corresponding author
National Taiwan Normal University</p>
<p>SURVEYING THE MLLM LANDSCAPE: A META-REVIEW OF CURRENT SURVEYS
17 Sep 2024778BB699699AF0CB2588D3A3DAE35E3EarXiv:2409.18991v1[cs.CL]
The rise of Multimodal Large Language Models (MLLMs) has become a transformative force in the field of artificial intelligence, enabling machines to process and generate content across multiple modalities, such as text, images, audio, and video.These models represent a significant advancement over traditional unimodal systems, opening new frontiers in diverse applications ranging from autonomous agents to medical diagnostics.By integrating multiple modalities, MLLMs achieve a more holistic understanding of information, closely mimicking human perception.As the capabilities of MLLMs expand, the need for comprehensive and accurate performance evaluation has become increasingly critical.This survey aims to provide a systematic review of benchmark tests and evaluation methods for MLLMs, covering key topics such as foundational concepts, applications, evaluation methodologies, ethical concerns, security, efficiency, and domain-specific applications.Through the classification and analysis of existing literature, we summarize the main contributions and methodologies of various surveys, conduct a detailed comparative analysis, and examine their impact within the academic community.Additionally, we identify emerging trends and underexplored areas in MLLM research, proposing potential directions for future studies.This survey is intended to offer researchers and practitioners a comprehensive understanding of the current state of MLLM evaluation, thereby facilitating further progress in this rapidly evolving field.</p>
<p>Multimodal Large Language Models (MLLMs) represent a significant advancement in artificial intelligence, allowing systems to process and generate content across diverse modalities, such as text, images, audio, and video.By integrating multiple data types, MLLMs move beyond the limitations of unimodal models, enabling more comprehensive and sophisticated applications in areas ranging from autonomous systems to medical diagnostics.</p>
<p>Given the rapid development of MLLMs, the field has produced a wealth of surveys, each exploring specific aspects of these models.However, the sheer volume and diversity of this literature can make it difficult for researchers and practitioners to grasp the current state of the field.To address this, we present a "survey of surveys" that synthesizes key insights across existing reviews and organizes them into 11 core areas: General, Evaluation, Security, Bias, Agents, Applications, Retrieval-Augmented Generation (RAG), Graphs, Data, Continual Learning, and Efficient Learning.</p>
<p>This paper aims to:</p>
<p>• Synthesize and categorize findings from various surveys, providing a structured overview of the most critical advancements in MLLM research.• Identify major themes, trends, and challenges in MLLM evaluation, highlighting benchmarks, datasets, and performance metrics.• Examine current methodologies for MLLM application and assessment, identifying gaps and suggesting improvements.• Highlight future research directions and underexplored areas within the MLLM landscape.</p>
<p>By offering a comprehensive synthesis of existing literature, this "survey of surveys" serves as a valuable resource for navigating the evolving MLLM field, fostering deeper understanding and guiding future research.</p>
<p>Classical Natural Language Processing Methods</p>
<p>The development of Natural Language Processing (NLP) has undergone multiple stages including statistical and rulebased methods, neural networks, word embeddings, deep word embeddings, attention mechanisms, and context-aware models.Each stage has significant technological advancements and the introduction of important models.Early research mainly focused on statistical and rule-based methods, which were later followed by the introduction of neural networks and deep learning concepts, greatly enhancing the performance and application range of NLP models.</p>
<p>Early language model research was primarily based on the Markov process.Markov chains were used to describe random processes and sequence data, introducing basic methods for handling sequential data [1,2].Subsequently, Hidden Markov Models (HMMs) were proposed to perform continuous speech recognition via maximum likelihood estimation, dealing with hidden states in sequential data [3].In the 1990s, n-gram-based statistical language models were widely used.N-gram models predicted the next word based on the context, establishing a simple yet effective foundation for language models [4].With increased computational power, neural networks began to be applied in NLP.In 1997, Long Short-Term Memory (LSTM) networks were proposed to solve the problem of long-term dependencies using memory cells, making RNNs more effective in handling long sequential data [5,6].In 1998, Yann LeCun and others further developed RNNs for document recognition, capable of handling sequential data [7].Word embedding techniques achieved significant breakthroughs in the early 2010s.In 2003, the Word2Vec model was introduced, capturing semantic relationships between words through efficient word vector training methods [8].This method introduced the Skip-gram and CBOW models, greatly improving the quality of word vectors.</p>
<p>In 2009, ImageNet, a large visual database containing over 14 million annotated images, was launched for object recognition and image classification tasks [9].The advent of ImageNet provided deep learning with abundant data resources, making it possible to train deep neural networks on large-scale datasets.In 2012, AlexNet's success in the ImageNet [9] competition marked the resurgence of deep learning [10].AlexNet significantly improved image classification performance through deep convolutional neural networks, bringing deep learning back to the forefront of research and inspiring researchers to explore its potential in other fields like NLP.In 2014, the GloVe model trained word vectors using global word co-occurrence matrices, further improving semantic representation of words [11].Additionally, the sequence-to-sequence learning (Seq2Seq) model was proposed for machine translation, handling sequence transformation tasks through an encoder-decoder structure [12].In 2015, Bahdanau et al. introduced the attention mechanism, which dynamically aligned source and target sequences, improving machine translation performance [13].In 2016, the Byte Pair Encoding (BPE) algorithm was proposed to handle rare and out-of-vocabulary words through subword units, significantly enhancing model generalization ability [14].In 2018, the ELMo (Embeddings from Language Models) model was introduced, capturing contextual information through bidirectional LSTM (Long Short-Term Memory) networks.Specifically, ELMo used two independently trained LSTMs, one processing text from left to right and the other from right to left, concatenating the hidden states of these two LSTMs to form the final word representation.This method improved the quality of word representations by capturing lexical semantics and syntactic information while fully utilizing contextual information [15].</p>
<p>Early language models, which relied on simple statistical methods, difficult-to-train and scale long short-term memory networks (LSTMs), or neural networks with smaller parameter sizes, often produced unsatisfactory results.These models struggled to handle the diversity and complexity of language, particularly in understanding synonyms or words in different contexts.Simple statistical methods typically considered only word frequency and order, lacking a deep understanding of semantics and context.While LSTMs and small-scale neural networks improved the model's memory capacity and ability to handle complexity to some extent, their performance gains were limited due to the complexity of the training process and computational constraints.These limitations made early language models inadequate for handling complex natural language processing tasks in practical applications.</p>
<p>3 Large Language Models (LLM) and Multimodal Large Language Models (MLLM)</p>
<p>Transformer, BERT, and GPT</p>
<p>Transformer, BERT, and GPT are the foundation models of large language models.These models play a crucial role in the field of natural language processing (NLP), driving the development of numerous applications and research advancements.</p>
<p>Transformer</p>
<p>Vaswani et al. introduced the Transformer, a deep learning model widely used for natural language processing (NLP) tasks [16].As the cornerstone of large-scale language models (LLMs) today, the core of the Transformer model is the attention mechanism, which captures relationships between different positions in the input sequence without relying on traditional recurrent neural network (RNN) structures.This enables parallel computation and improves training efficiency.</p>
<p>The Transformer model consists of an encoder and a decoder, each composed of multiple identical layers.Each layer in the encoder contains two sub-layers: a multi-head self-attention mechanism and a feed-forward neural network.Each layer in the decoder, however, includes three sub-layers: a self-attention mechanism, an encoder-decoder attention mechanism, and a feed-forward neural network.The encoder maps the input sequence to a continuous representation, while the decoder generates the output sequence based on this representation.</p>
<p>Self-Attention Mechanism</p>
<p>The self-attention mechanism operates on three matrices: the query matrix Q, the key matrix K, and the value matrix V .The computation is performed as follows:
Attention(Q, K, V ) = softmax QK T √ d k V,
where d k is the dimension of the key matrix.</p>
<p>Multi-Head Attention Mechanism</p>
<p>The multi-head attention mechanism computes multiple self-attention heads in parallel and concatenates the results:
MultiHead(Q, K, V ) = Concat(head 1 , head 2 , . . . , head h )W O ,
Each head is computed as:
head i = Attention(QW Q i , KW K i , V W V i ),
where
W Q i , W K i , W V i
, and W O are trainable weight matrices.</p>
<p>Feed-Forward Neural Network The feed-forward neural network within each layer consists of two linear transformations and a ReLU activation function:
FFN(x) = ReLU(xW 1 + b 1 )W 2 + b 2 ,
Here, W 1 , W 2 , b 1 , and b 2 are trainable weights and biases.This network helps in transforming the input data into a more abstract representation before passing it to the next layer.</p>
<p>BERT and GPT</p>
<p>The encoder of the Transformer, when used independently and after undergoing pre-training and fine-tuning, becomes BERT; similarly, the decoder, after similar pre-training and fine-tuning, becomes GPT.BERT and GPT are applied to different natural language processing tasks: BERT is primarily used for understanding tasks, while GPT is mainly used for generation tasks.</p>
<p>BERT (Bidirectional Encoder Representations from Transformers) is a pre-training model proposed by Devlin et al. [17].</p>
<p>The core idea of BERT is to generate contextual representations of words using a bidirectional Transformer encoder.It achieves performance improvements across various NLP tasks by performing unsupervised pre-training on large-scale corpora, followed by fine-tuning on specific tasks.BERT's pre-training involves two tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP).</p>
<p>In the MLM task, BERT randomly masks some words in the input text and predicts these masked words using the model, thereby learning the relationships between words.In the NSP task, BERT learns the relationships between sentences by determining whether two sentences are continuous.</p>
<p>GPT (Generative Pre-trained Transformer) is a series of generative pre-training models proposed by OpenAI [18,19,20].Unlike BERT, GPT mainly uses a unidirectional Transformer decoder to generate text.GPT's training consists of two stages: first, pre-training on large-scale unlabelled data, and then fine-tuning on specific tasks.During the pre-training stage, the model uses an autoregressive method to predict the next word, thereby learning the sequence information of words.</p>
<p>GPT performs exceptionally well in generation tasks because it can generate coherent and meaningful text based on the given context.With the development of the GPT series, GPT-2 and GPT-3 introduced more parameters and larger model scales, further improving generation quality and task generalization capabilities.</p>
<p>Multimodal Large Language Models (MLLM)</p>
<p>Multimodal Large Language Models (MLLM) represent a significant advancement in the field of artificial intelligence by integrating multiple types of data-such as text, images, and audio-into a unified framework.Unlike traditional models that operate on a single modality, MLLMs are designed to understand and generate content across different modalities, enhancing their versatility and applicability to a wider range of tasks.The core challenge in developing MLLMs is achieving effective modality alignment, which involves mapping different types of data into a common representation space.This alignment allows the model to seamlessly interpret and interrelate information from various sources, thereby improving performance on tasks like image captioning, visual question answering, and multimodal translation.As a result, MLLMs hold the promise of enabling more natural and comprehensive human-computer interactions, paving the way for innovations in areas such as virtual assistants, content creation, and accessible technologies.</p>
<p>In the context of aligning text and images, the initial challenge was the necessity of vast amounts of data to ensure models could effectively understand and correlate different modalities.Early methods relied on manually labeled data, which was both costly and inefficient.Some of these early methods include DeViSE (A Deep Visual-Semantic Embedding Model) [21], VSE (Visual Semantic Embeddings) [22] and CCA (Canonical Correlation Analysis) [23].These models attempted to align images and texts in a shared representation space using labeled datasets, but their performance was limited due to the restricted size of the data.</p>
<p>Contrastive Learning-based Multimodal Alignment</p>
<p>With the advent of contrastive learning, this field has significant progress.The core idea of contrastive learning is to bring similar sample pairs (such as related image and text pairs) closer together while pushing dissimilar pairs apart in the shared representation space, thus achieving multimodal alignment.Early contrastive learning methods focused primarily on the visual domain, such as MoCo (Momentum Contrast) [24,25], SimCLR (Simple Framework for Contrastive Learning of Visual Representations) [26,27], Instance Discrimination [28], CPC (Contrastive Predictive Coding) [29], CMC (Contrastive Multiview Coding) [30], PIRL (Pretext-Invariant Representation Learning) [31], SwAV (Swapping Assignments between Multiple Views of the Same Image) [32], and BYOL (Bootstrap Your Own Latent) [33].These methods leveraged large unlabeled image datasets and used contrastive loss functions to significantly enhance image feature representations.</p>
<p>In the realm of multimodal data alignment, contrastive learning methods have made remarkable strides.CLIP (Contrastive Language-Image Pre-Training) [34] represents a major breakthrough in this area.CLIP leverages a large-scale dataset of image-text pairs and employs a contrastive loss function to achieve robust text-image alignment.Specifically, CLIP encodes images and texts into a shared representation space to align them.Given an image I and its corresponding text description T , the image encoder f (•) and text encoder g(•) map them into a common representation space: the image feature vector f (I) and the text feature vector g(T ).In this space, matched image-text pairs exhibit high similarity, while mismatched pairs show low similarity.Pretraining on hundreds of millions of image-text pairs allows CLIP to learn rich cross-modal representations, excelling in downstream tasks.</p>
<p>CLIP possesses strong zero-shot learning capabilities, meaning it can be directly used for feature extraction and applications without task-specific fine-tuning.For instance, in image classification, CLIP can classify images using its pretrained representations without additional labeled data.This ability makes CLIP highly flexible and efficient in practical applications, quickly adapting to new tasks and datasets.CLIP's text and image encoders can be used independently; inputting text or images into the respective encoders yields high-dimensional space features (embeddings).Specifically, the text encoder converts input text into text feature vectors g(T ), and the image encoder converts input images into image feature vectors f (I), which can be utilized in various downstream tasks such as feature extraction and representation learning.</p>
<p>There are numerous models that employ CLIP visual or text encoders, demonstrating their broad applicability in multimodal alignment.For instance, METER (Multimodal End-to-end TransformER) [35] integrates CLIP encoders to align image and text representations and employs optimal transport techniques to improve the performance of vision-language tasks.SimVLM (Simple Visual Language Model) [36] uses CLIP encoders for weakly supervised pretraining, achieving excellent performance in vision-language tasks.BLIP-2 (Bootstrapping Language-Image Pretraining) [37] utilizes frozen CLIP image encoders with large language models to improve image-text pretraining.The Stable Diffusion series, including high-resolution image synthesis [38], personalized stylization [39], and improved high-resolution image synthesis [40], also employs CLIP encoders to optimize image generation and super-resolution tasks.</p>
<p>Model-Based Techniques for Multimodal Alignment</p>
<p>Model-based techniques for multimodal alignment aim to create unified representations of different modalities, such as images and text, by leveraging large-scale models that handle both vision and language tasks.ALIGN (Large-scale Image-Language Pretraining) [41] employs a dual-encoder architecture that aligns images and text in a shared embedding space using contrastive learning, significantly improving zero-shot image classification and retrieval.Similarly, Florence [42] incorporates a vision transformer model with a text encoder to align image and text modalities, demonstrating superior performance in visual understanding tasks.These models focus on pretraining using vast amounts of weakly labeled image-text data, contributing to better generalization across a range of vision-language benchmarks.</p>
<p>Other models utilize fine-grained alignment strategies to enhance performance in more specialized tasks.UNITER (UNiversal Image-TExt Representation) [43] jointly learns image-text alignments using a transformer-based architecture, introducing a unique masked language modeling task to improve contextual understanding between modalities.On the other hand, ViLT (Vision-and-Language Transformer) [44] reduces the complexity of multimodal fusion by directly feeding visual patch embeddings into a transformer, bypassing the need for separate image encoders, while still maintaining competitive performance in tasks like image-text retrieval and visual question answering.These modelbased techniques highlight the versatility and scalability of transformer-based approaches in multimodal alignment tasks, addressing both high-level semantic matching and fine-grained object-level alignment.</p>
<p>4 Taxonomy and Categorization</p>
<p>Methodology</p>
<p>This paper synthesizes findings from 58 most recent and most forefront surveys, categorized into key themes like model architectures, evaluation, applications, security, bias, and future directions.Surveys were selected based on their recency and breadth of coverage in the MLLM domain, spanning from general overviews to specific applications and challenges.</p>
<p>Each survey is analyzed based on:</p>
<p>• Technical focus: architectures, models, datasets.</p>
<p>• Applications: computer vision, healthcare, robotics, etc.</p>
<p>• Security and biases: model safety, fairness, robustness.</p>
<p>• Emerging trends: future directions, new paradigms.</p>
<p>Applications and Agents</p>
<p>Law The survey [45] focuses on the integration of LLMs into the legal domain, where they have been used for tasks such as legal advice, document processing, and judicial assistance.The key trend here is the application of specialized LLMs, such as LawGPT and LexiLaw, which address the nuances of legal language and reasoning.One significant challenge identified is maintaining judicial independence and addressing the ethical implications of biased data in legal decision-making.</p>
<p>Autonomous Driving In the field of autonomous driving, MLLMs are increasingly being used to improve perception, decision-making, and human-vehicle interaction, as noted in [46].A key trend is the integration of multimodal data, such as LiDAR, maps, and images, which enhance the vehicle's ability to process complex driving environments.However, challenges related to real-time data processing and ensuring safety in diverse driving conditions remain significant.</p>
<p>Mathematics [47] explores the application of LLMs in solving mathematical tasks, such as calculation and reasoning.</p>
<p>Techniques like Chain-of-Thought (CoT) prompting have significantly improved model performance in complex mathematical problems.However, the scarcity of high-quality datasets and the complexity of mathematical reasoning pose ongoing challenges.</p>
<p>Healthcare The healthcare survey [48] reviews the use of multimodal learning for tasks like image fusion, report generation, and cross-modal retrieval.A key trend is the rise of foundation models such as GPT-4 and CLIP in processing medical data.Despite advancements, these models have not yet achieved universal intelligence, and concerns related to data integration and ethical considerations remain major barriers.</p>
<p>Robotics The use of LLMs in robotics [49] focuses on their ability to improve perception, decision-making, and control in robotic systems.The main trend identified is the potential for LLMs to advance embodied intelligence, where robots understand and interact with the physical world.However, challenges related to real-time perception, control, and integration with existing robotic technologies persist.</p>
<p>Multilingualism In multilingual settings, the ability of LLMs to process multiple languages is highlighted in [50].There is significant progress in handling multiple languages, but low-resource languages and security issues in multilingual models remain challenges.Emerging techniques like multilingual Chain-of-Thought reasoning show promise for future development.</p>
<p>Gaming The survey [51] focuses on the role of LLMs in gaming, particularly in generating dynamic Non-Player Characters (NPCs), enhancing player interaction, and even assisting in game design.The challenge of hallucinations, where LLMs generate plausible but incorrect outputs, is a major limitation in real-time gaming environments.Improving context management and memory within gaming systems is a future research priority.</p>
<p>Audio Processing [52] discusses the application of LLMs in audio processing tasks, such as Automatic Speech Recognition (ASR) and music generation.The integration of multimodal data from speech, music, and environmental sounds into a single model, like SeamlessM4T, marks a significant trend.However, scalability and data diversity remain issues to address.</p>
<p>Video Understanding In [53], the focus is on video understanding through the use of Vid-LLMs (Video-Large Language Models), which combine video, text, and audio inputs to analyze and understand video content.While these models are promising for tasks like video summarization and captioning, challenges related to processing long videos and maintaining contextual coherence need further exploration.</p>
<p>Citation Analysis The use of LLMs for citation tasks is discussed in [54], where LLMs significantly improve citation recommendation, classification, and summarization tasks.Additionally, citation data enhances LLM performance by enabling multi-hop knowledge across documents.Future research needs to address the expansion of citation networks and integration of non-academic sources.</p>
<p>Evaluation and Benchmarks</p>
<p>Evaluation and benchmarking of Multimodal Large Language Models (MLLMs) are crucial for understanding their performance across a diverse range of tasks and datasets.Existing evaluations can be categorized based on the models' core abilities, the datasets used, and the complexity of the tasks involved.</p>
<p>Core Evaluation Domains</p>
<p>Perception and Understanding This domain evaluates how well MLLMs interpret multimodal inputs, such as text, images, and audio, and integrate information across modalities.Benchmarks in this category include tasks like object detection, scene understanding, and feature extraction.For example, VQA datasets like VQAv2 are foundational for evaluating these abilities but are limited by biases that can inflate model performance [55].</p>
<p>Cognition and Reasoning Higher-level capabilities, such as logical reasoning, problem-solving, and multimodal reasoning, are captured in this domain.Tasks that require more sophisticated reasoning across modalities, like visual question answering with complex scenarios, are included in this category.This domain tests the models' deeper understanding of the relationships between the modalities [56].</p>
<p>Advanced Evaluation Areas</p>
<p>Robustness and Safety Models need to be evaluated for their robustness, particularly when faced with adversarial prompts or out-of-distribution data.The benchmarks in this category assess how well MLLMs perform under conditions that simulate real-world challenges.Robustness is also critical in ensuring the models' safety, especially when deployed in domains like autonomous driving or healthcare [57,58].Furthermore, this area assesses how models manage hallucinations and mitigate biases.</p>
<p>Domain-Specific Capabilities</p>
<p>This category includes evaluations focused on specialized domains, such as medical image interpretation or legal text analysis, where models must combine general multimodal abilities with domainspecific knowledge.Benchmarks like TallyQA, which test complex reasoning in specific domains, fall into this category [56].</p>
<p>Task-Specific Benchmarks</p>
<p>Traditional vs. Advanced Datasets Early datasets, such as VQAv2, have been widely used to evaluate MLLMs but exhibit limitations due to their susceptibility to language bias and lack of task complexity.In contrast, more recent datasets like TDIUC and DVQA are designed to evaluate models on fine-grained visual understanding, reasoning, and OCR tasks.These datasets provide a more rigorous assessment of the models' capabilities [55].</p>
<p>Multimodal Reasoning and Interaction</p>
<p>This category focuses on evaluating the interaction between modalities, which is key in tasks like multimodal dialogue or visual question answering that require reasoning across both text and images.Advanced datasets, such as VQDv1, which require complex reasoning and the identification of multiple objects in visual contexts, push models to demonstrate a deeper understanding of multimodal relationships [55].</p>
<p>Ethical and Societal Implications</p>
<p>Fairness and Bias This domain addresses the importance of ensuring that MLLMs do not perpetuate or amplify societal biases.Models must be evaluated on their ability to perform equitably across demographic groups, and benchmarks in this category assess the fairness of the models [59].</p>
<p>Trustworthiness and Safety Ensuring trust in MLLMs is critical, especially in applications where safety is paramount.This category evaluates whether models produce harmful or misleading content and assesses their reliability in sensitive domains.It also evaluates how MLLMs handle uncertainty and avoid hallucinations [60,61].</p>
<p>In summary, evaluation and benchmarking in MLLMs have evolved from using broad, general-purpose datasets to more specialized, task-specific benchmarks that provide a deeper insight into models' true capabilities.The taxonomy proposed here synthesizes insights from existing literature, offering a structured approach to categorizing and evaluating MLLMs across multiple dimensions.</p>
<p>Taxonomy of Benchmarking Criteria</p>
<p>Micro vs. Macro Performance: The benchmarking process involves analyzing both micro performance (where each example is weighted equally) and macro performance (averaged across different question types).This distinction helps in understanding how well a model performs across varied tasks and datasets, providing a clearer picture of its strengths and weaknesses.</p>
<p>Generalization Across Tasks: Models are evaluated not just on their ability to perform specific tasks but also on their generalization capabilities.For example, while models like GPT-4V may excel in straightforward tasks like counting, they often face challenges in more complex reasoning tasks, revealing gaps in their overall performance.</p>
<p>Efficiency and Adaptation</p>
<p>The surveys on resource-efficient Multimodal Large Language Models (MLLMs) explore various approaches aimed at reducing computational costs while maintaining performance.Xu et al. [62] and Jin et al. [63] emphasize the growing need for MLLMs to be more accessible, particularly in resource-constrained environments like edge computing.Both surveys provide comprehensive taxonomies, covering advancements in architectures, vision-language integration, training methods, and benchmarks that optimize MLLM efficiency.Key methods discussed include vision token compression, parameter-efficient fine-tuning, and the exploration of transformer-alternative models like Mixture of Experts (MoE) and state space models.These efforts collectively aim to balance computational efficiency with task performance, driving MLLM adoption across various practical applications.</p>
<p>On the other hand, Liu et al. [64] address the challenge of adapting MLLMs to specific tasks with limited labeled data.</p>
<p>The survey categorizes approaches into prompt-based, adapter-based, and external knowledge-based methods, which help these models generalize better in fine-grained domains such as medical imaging and remote sensing.Few-shot adaptation techniques, such as visual prompt tuning and adapter fine-tuning, are critical for extending the usability of large multimodal models without relying on extensive labeled datasets.Despite advancements, these surveys highlight ongoing challenges, including domain adaptation, model selection, and the integration of external knowledge.Both fields point toward a future where MLLMs are not only more efficient but also more flexible and adaptive in handling diverse, real-world tasks.</p>
<p>Data-centric</p>
<p>Bai et al. [65] presents a comprehensive survey on the role of data in the development of Multimodal Large Language Models (MLLMs).The paper emphasizes the importance of the quality, diversity, and volume of multimodal data, which includes text, images, audio, and video, in training MLLMs effectively.It identifies significant challenges in multimodal data collection, such as data sparsity and noise, and explores potential solutions like synthetic data generation and active learning to mitigate these issues.The authors advocate for a more data-centric approach, where the refinement and curation of data take precedence, ultimately improving model performance and advancing MLLM capabilities in an increasingly complex landscape.</p>
<p>Yang et al. [66] investigates the intersection of large language models (LLMs) and code, framing code as a vital tool that enhances LLMs' capacity to operate as intelligent agents.The paper discusses how code empowers LLMs in tasks such as automation, code generation, and software development.It also tackles key challenges around code correctness, efficiency, and security, which are essential when deploying LLMs in practical applications.By analyzing the integration of code with LLMs, the authors showcase the potential of these models to evolve into autonomous agents capable of managing complex tasks across various domains, thereby broadening the scope and impact of LLMs in AI-driven innovations.</p>
<p>Contunual Learning</p>
<p>Feng et al. [67] provides a thorough overview of how Large Language Models (LLMs) are being integrated with external knowledge to enhance their capabilities.The survey categorizes the integration methods into two main approaches: knowledge editing and retrieval augmentation.Knowledge editing involves modifying the input or the model itself to update the outdated or incorrect information, while retrieval augmentation fetches external information during inference without altering the model's core parameters.The authors present a taxonomy covering these methods, benchmarks for evaluation, and applications such as LangChain and ChatDoctor, which leverage these strategies to address domain-specific challenges.Additionally, the paper explores the handling of knowledge conflicts and suggests future research directions for improving LLM performance in complex, real-world tasks through better integration of multi-source knowledge.</p>
<p>Shi et al. [68] offer an extensive survey on the continual learning (CL) of Large Language Models (LLMs), addressing the critical challenges and methodologies in this field.presents an extensive overview of the challenges and techniques related to the continual learning (CL) of Large Language Models (LLMs).The authors focus on two primary directions of continuity: vertical continual learning (adapting models from general to specific domains) and horizontal continual learning (adapting models over time across various domains).They discuss the problems of "catastrophic forgetting," where models lose knowledge of previous tasks, and the complexity of continually updating models to maintain performance on both old and new tasks.The survey outlines key CL methods, including continual pre-training, domain-adaptive pre-training, and continual fine-tuning.It also evaluates various CL techniques, such as replaybased, regularization-based, and architecture-based methods, to mitigate forgetting and ensure knowledge retention.</p>
<p>The authors call for more research into evaluation benchmarks and methodologies to counter forgetting and support knowledge transfer in LLMs, making this an underexplored yet crucial area of machine learning research</p>
<p>Evaluation Benchmarks</p>
<p>Lu et al. [55] tackle the key challenges in evaluating multimodal large language models (MLLMs), focusing on the unique complexities these models present.They propose a taxonomy of evaluation metrics designed specifically for multimodal tasks, such as cross-modal retrieval, caption generation, and visual question answering (VQA).The authors highlight how current evaluation frameworks often fail to capture the nuances of multimodal interactions, suggesting the need for more tailored metrics to address these limitations.</p>
<p>Li and Lu [69] provide a comprehensive overview of benchmarking datasets and performance metrics in MLLMs.They argue that the absence of standardized protocols across different modalities hinders the fair comparison of models.Their work calls for the establishment of consistent evaluation frameworks that ensure reliable and equitable performance assessment across varied multimodal tasks.</p>
<p>Chang et al. [57] examine the evaluation methodologies for large language models (LLMs), emphasizing the importance of moving beyond task-specific performance metrics like knowledge reasoning.They point out that many existing benchmarks, such as HELM and BIG-Bench, overlook critical issues such as hallucinations, fairness, and societal implications.The authors advocate for more holistic evaluation practices that take into account not only the technical capabilities of LLMs but also their trustworthiness, robustness, and ethical alignment in real-world applications.Their work underscores the need to assess both the technical and societal impacts of these models to ensure responsible AI deployment.</p>
<p>Agents and Autonomous Systems</p>
<p>Xi et al. [70], Wang et al. [71] and a few more papers [72,73,62] provide comprehensive surveys on the development of LLM-based autonomous agents, highlighting the key modules and frameworks that form the foundation of these systems.At their core, autonomous agents leveraging large language models (LLMs) rely on four essential components: perception, memory, planning, and action.These modules work in synergy to enable agents to perceive their environment, recall previous interactions, and plan and execute actions in real-time.As Xi et al. [70] describe, this architecture allows agents to be highly adaptable across a variety of domains, from digital assistants to autonomous vehicles.Wang et al. [71] further emphasize the importance of expanding the perception-action loop by incorporating multimodal inputs, ensuring agents can effectively handle complex real-world scenarios, such as those found in industrial automation and gaming.</p>
<p>Despite these advancements, LLM-based agents face several critical challenges.Both surveys identify knowledge boundaries as a significant issue, where agents are constrained when operating in specialized or underexplored domains.Prompt robustness is another challenge, as even minor changes in prompts can lead to unpredictable or erroneous behavior, including hallucinations, where agents generate false information.Shi et al. [68] further explore the challenge of catastrophic forgetting, where agents fail to retain knowledge from previous tasks after being updated with new information.Addressing these challenges requires ongoing improvements in how agents interact with external tools, refine prompt structures, and manage memory to prevent knowledge decay and ensure consistent behavior.</p>
<p>LLM-based agents have demonstrated significant versatility across a wide range of applications.Xi et al. [70] discuss various applications involving single agents, multi-agent systems, and human-agent interactions.Wang et al. [71] categorize applications into three key areas: social sciences, natural sciences, and engineering, illustrating the transformative potential of LLM-based agents in empowering these fields.Xie et al. [72] further categorize agents by their application scenarios, highlighting areas such as robotics and embodied AI, where agents make real-time decisions based on multimodal inputs like images, sensor data, and text.Additionally, as noted by Xi et al. [70] and Xie et al. [72], LLM-based agents are becoming increasingly prominent in scientific research, automating tasks such as experiment design, planning, and execution.</p>
<p>To ensure the robustness and reliability of LLM-based agents, future research must focus on overcoming the current limitations.Xie et al. [72] and Shi et al. [68] emphasize the need to develop more robust multimodal perception systems, improve LLM inference efficiency, and establish ethical frameworks to guide agent decision-making.Enhancing memory integration and refining prompt design will also be critical to preventing issues like hallucinations and ensuring agents can effectively transfer knowledge across tasks without degradation.</p>
<p>MLLMs in Graph Learning</p>
<p>Multimodal large language models (MLLMs) have been increasingly applied to graph learning tasks, outperforming traditional graph neural networks (GNNs).By incorporating textual attributes and other modalities, MLLMs enhance the representational power of GNNs, enabling improved performance in classification, prediction, and reasoning tasks.Jin et al. proposed a taxonomy categorizing the integration of MLLMs with graphs into enhancers, predictors, and alignment components, highlighting their utility in various graph tasks.Similarly, Chen et al. and Li et al. discuss the integration of LLMs with knowledge graphs, illustrating the benefits of combining graph structures with multimodal data for broader applications [74,75,76].</p>
<p>Retrieval-Augmented Generation (RAG) in MLLM</p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in generating and understanding content across various modalities, such as text, images, and audio.However, their reliance on static training data limits their ability to provide accurate, up-to-date responses, especially in rapidly changing contexts.Retrieval-Augmented Generation (RAG) addresses this issue by dynamically retrieving relevant external information before the generation process.By incorporating real-time and contextually accurate information, RAG enhances both the factuality and robustness of MLLM outputs.In multimodal tasks, RAG not only retrieves textual data but also incorporates multimodal data sources like images and videos, significantly improving the knowledge richness and generation quality of MLLMs [77,78].</p>
<p>Hu et al. [77] provide a comprehensive survey of RAG's application in natural language processing, highlighting how retrieving external knowledge during generation effectively mitigates long-tail knowledge gaps and hallucination issues.Zhao et al. [79] further explore how multimodal information retrieval augments generation by improving diversity and robustness.By leveraging information from different modalities, RAG empowers MLLMs to generate more accurate and contextually grounded outputs, especially in tasks requiring cross-modal reasoning such as visual question answering and complex dialogue generation [80,78].</p>
<p>5 Major Themes Emerging from Surveys</p>
<p>MLLM Architectures</p>
<p>A recurring topic across most surveys is the architecture of MLLMs, where Transformer-based models dominate.Innovations like CLIP, DALL-E, and Flamingo exemplify the progress in aligning text and visual data.Surveys compare early fusion (integrating modalities early in the model pipeline) and late fusion strategies (processing modalities separately before combining outputs).</p>
<p>Datasets and Training</p>
<p>The field is characterized by massive, multimodal datasets such as MS-COCO, Visual Genome, and custom-curated sets like LAION.Pretraining on large-scale datasets remains a core strategy, with some surveys, like "Large-scale Multi-Modal Pre-trained Models", etc. [81], offering a comprehensive taxonomy of pretraining methodologies.</p>
<p>Evaluation and Metrics</p>
<p>Evaluation challenges are a key theme, where traditional language or vision metrics fall short.Cross-modal retrieval, image captioning, and visual question answering (VQA) are popular benchmarks, but new methods to evaluate multimodal coherence and reasoning are discussed in evaluation-focused surveys like "A Survey on Evaluation of Large Language Models", etc. [57,82,83].</p>
<p>Security: Adversarial Attacks</p>
<p>Adversarial attacks have been a critical concern for MLLMs.Caffagni et al. [84]  Zhao et al. [87] emphasize the complexity of multimodal adversarial attacks, where both image and text inputs are manipulated.These attacks exploit the model's difficulty in handling cross-modal perturbations, making it challenging to detect malicious inputs.To counter these attacks, researchers suggest improving cross-modal alignment algorithms and developing new defense mechanisms to enhance robustness across modalities.</p>
<p>Another vulnerability arises in the cross-modal alignment process, which connects the features extracted from images with those from textual data.As Shayegani et al. [88] explain, attackers can exploit this process to disrupt the model's understanding of multimodal data, compromising its predictions.This highlights a need for robust alignment mechanisms that can defend against such manipulations.</p>
<p>Bias: Hallucinations and Data Bias</p>
<p>Liu et al. [89] describe the hallucination phenomenon in Multimodal Large Language Models (MLLMs), where the generated outputs deviate from the visual input, leading to the fabrication of objects or misrepresentation of relationships in an image.This issue is often a byproduct of data bias in the training sets, as well as the limitations of vision encoders in accurately grounding images.</p>
<p>Hallucinations can be traced to several causes, ranging from the models' reliance on noisy or incomplete training data to the inherent limitations of cross-modal alignment mechanisms [61].For instance, models may incorporate statistically frequent objects that are not present in the specific image or confuse the relationships between depicted elements.This happens because MLLMs may memorize biases present in training datasets, such as common object pairings or overly simplistic image-text associations.As a result, hallucinations not only mislead users but also exacerbate existing biases in the models' outputs, leading to incorrect assumptions about objects' existence, attributes, or relationships [60].</p>
<p>Addressing these hallucinations requires improving the modality alignment mechanisms to better synchronize textual and visual data.This involves enhancing the capabilities of vision encoders to more accurately represent fine-grained details of the images and ensuring that the language models respect the constraints imposed by visual context.Bai et al. [61] advocate for the use of specialized evaluation benchmarks and techniques, such as leveraging better-aligned datasets or employing post-processing corrections.Furthermore, strategies like counterfactual data augmentation and the introduction of diverse visual instructions can help models generalize better to unseen scenarios, thus reducing the frequency of hallucinations and aligning model predictions with true visual inputs [90].</p>
<p>Fairness: Adversarial Training and Human Feedback</p>
<p>To address both security and fairness, several defense strategies have been proposed.Shayegani et al. [88] suggest employing adversarial training and data augmentation as methods to strengthen the model's robustness.By introducing adversarial examples during the training phase, models can learn to identify and resist adversarial inputs.Furthermore, safety steering vectors and multimodal input validation can dynamically detect and correct potentially biased or adversarial outputs during inference.</p>
<p>Liu et al. [91] explore techniques to improve fairness by leveraging Reinforcement Learning from Human Feedback (RLHF).This method allows models to adjust their outputs based on user feedback, ensuring that the generated text aligns with societal values and ethical standards.The approach ensures that models maintain fairness by incorporating diverse viewpoints and mitigating biases present in training data.</p>
<p>Defense Against Jailbreaking Attacks</p>
<p>Jin et al. [92] classify different types of jailbreaking attacks and propose defense strategies such as prompt tuning and gradient-based defenses.These approaches focus on adjusting the input prompts and strengthening alignment algorithms to reduce the success rate of jailbreaking attacks, ensuring that the model remains secure and aligned with ethical guidelines.</p>
<p>In conclusion, the security, bias, and fairness challenges in MLLMs primarily revolve around defending against adversarial attacks, addressing training data biases, and ensuring fair handling of multimodal inputs.Future research must focus on optimizing vision encoders and cross-modal alignment mechanisms to improve the robustness and fairness of MLLMs.</p>
<p>6 Emerging Trends and Gaps</p>
<p>Current Trends</p>
<p>As Multimodal Large Language Models continues to advance, several key trends have emerged that reflect both the growing capabilities and the challenges associated with these models.Recent surveys provide valuable insights into how MLLMs are evolving and highlight significant areas of focus that are shaping the current landscape of research and application.Below, we outline the most discussed and impactful trends observed in the literature.</p>
<p>Increased Integration of Multimodality</p>
<p>One of the most prominent trends in the surveyed literature is the enhanced integration of multiple modalities-such as text, images, and audio-within MLLMs.Surveys like "The (R)Evolution of Multimodal Large Language Models", etc. [93,94,95,96] emphasize the shift from unimodal to multimodal systems as a transformative leap, enabling models to more closely mimic human perception.This trend is echoed across multiple papers, highlighting the field's focus on achieving a more holistic understanding of information by combining different types of data inputs.</p>
<p>Applications in Diverse Domains</p>
<p>A recurring theme in recent surveys is the expansion of MLLM applications into various domains, particularly those requiring complex, multimodal understanding, such as autonomous agents and medical diagnostics.For instance, the survey "A Survey on Multimodal Large Language Models" [97] traces the significant impact of MLLMs across different industries, suggesting a growing trend toward domain-specific adaptations of these models.This trend points to the increasing specialization of MLLMs to meet the demands of specific fields.</p>
<p>Focus on Evaluation Metrics and Benchmarking</p>
<p>As MLLMs become more prevalent, the need for robust evaluation metrics and benchmarking has become increasingly critical.The surveyed literature frequently discusses the development of new benchmarks designed to assess the performance of MLLMs across different modalities.For example, "A Survey of Large Language Models" [98] provides an in-depth analysis of existing evaluation methodologies, indicating a trend towards more comprehensive and standardized performance assessments.</p>
<p>Ethical and Security Considerations</p>
<p>Another notable trend is the growing concern over the ethical implications and security risks associated with MLLMs.</p>
<p>Recent surveys reflect an increased focus on the responsible development and deployment of these models.Ethical concerns, such as bias in multimodal data processing and the potential for misuse in generating deceptive content, are frequently discussed, highlighting the importance of addressing these issues as the technology advances.</p>
<p>Efficiency and Optimization</p>
<p>Efficiency in training and deploying MLLMs is an emerging trend, particularly as models become larger and more complex.The surveyed literature suggests a focus on optimizing the computational resources required for MLLMs, making them more accessible and scalable.This trend is crucial for the broader adoption of these models in both research and industry.</p>
<p>These trends underscore the rapid evolution of MLLMs and the diverse challenges and opportunities they present.The surveyed literature not only identifies these key areas but also sets the stage for future research directions, which are discussed in the subsequent sections.[99, 100]</p>
<p>Research Gaps</p>
<p>Certain areas of MLLMs remain underexplored or lack comprehensive analysis.Identifying these research gaps guides future studies to ensure a balanced development in the field.Below, we highlight key areas that needs further investigation.</p>
<p>Integration of Lesser-Known Modalities</p>
<p>While most surveys focus on the integration of text, images, and, to some extent, audio, there is a noticeable gap in exploring the potential of other modalities, such as haptic feedback, olfactory data, and advanced sensory inputs.The survey like "The (R)Evolution of Multimodal Large Language Models", etc. [93,101,102,103,104] acknowledges the importance of holistic integration but primarily emphasizes traditional modalities.Future research could expand on how these lesser-known modalities can be incorporated into MLLMs to create even more comprehensive models.</p>
<p>Longitudinal Studies on Model Performance</p>
<p>Current surveys tend to focus on the performance of MLLMs at a specific point in time, often lacking longitudinal studies that track the evolution of model capabilities and limitations over extended periods.For instance, "A Survey of Large Language Models" [98,105,106,107] provides a snapshot of the state of MLLMs but does not address how these models might evolve with new data, architectures, or training techniques.Longitudinal studies could provide valuable insights into the long-term viability and scalability of MLLMs.</p>
<p>Cross-Domain Applications and Transfer Learning</p>
<p>There is limited coverage on the application of MLLMs across vastly different domains and the effectiveness of transfer learning in these contexts.Surveys such as "Large-scale Multi-Modal Pre-trained Models: A Survey" [81] touch upon domain-specific applications but do not fully explore how models trained in one domain perform when transferred to another.This is a critical area for research, especially in understanding the generalization capabilities of MLLMs across diverse fields.</p>
<p>Ethical Implications in Non-Textual Modalities</p>
<p>While ethical concerns in textual data have been widely discussed, there is a research gap in exploring the ethical implications of non-textual modalities, particularly in images and video.The survey "Multimodal Large Language Models: A Survey" [102] addresses general ethical concerns but lacks a deep dive into how these issues manifest in non-textual data.Future research should focus on understanding the ethical challenges unique to each modality, including biases, privacy concerns, and the potential for misuse.</p>
<p>Impact of Multilingualism on Multimodality</p>
<p>The intersection of multilingualism and multimodality is another underexplored area.Although "Multilingual Large Language Model: A Survey of Surveys" [108] addresses multilingual capabilities, it does not fully explore how these capabilities interact with multimodal inputs.Research in this area could lead to more inclusive MLLMs that better serve global populations by effectively integrating multilingual and multimodal data.</p>
<p>Conclusion 7.1 Summary of Insights</p>
<p>In our survey of surveys on Multimodal Large Language Models (MLLMs), several key insights have emerged.First, the multimodal capabilities of MLLMs have significantly enhanced performance across various tasks, particularly in combining and understanding data from multiple sources such as text, images, and videos.Second, as the size of models and the amount of training data increases, the intelligence of the models in perception and reasoning improves, but at the cost of increasing computational resource demands.Additionally, we observed challenges related to robustness, interpretability, and fairness of the models in practical applications.Finally, MLLMs are expanding their potential applications across numerous industries, driven by rapid technological advancements.</p>
<p>Future Directions</p>
<p>Future surveys can be improved and expanded in the following ways to better capture emerging trends in the rapidly evolving field of MLLMs:</p>
<p>• Emerging Areas: With the development of generative AI and self-supervised learning, future surveys should focus on new trends emerging from the integration of these technologies with MLLMs.</p>
<p>• Data Diversity and Challenges: There should be a deeper discussion of the challenges posed by the diversity and complexity of multimodal data, particularly regarding the construction, annotation, and management of large-scale datasets.</p>
<p>• Model Evaluation and Standardization: Future surveys should systematically analyze evaluation standards, including performance metrics across different tasks and domains, and evaluate the robustness and fairness of the models.</p>
<p>• Real-World Applications and Ethics: A more thorough examination of real-world applications of MLLMs is needed, including issues of privacy, security, and ethical considerations, with recommendations on how to balance innovation and risks in various application scenarios.</p>
<p>• Optimization of Computational Resources: There should be further exploration of efficient use of computational resources and model compression techniques, providing guidance for MLLM applications in resource-constrained environments.</p>
<ol>
<li>1 7 14 7. 1 14 7. 2 1 1 . 1
17141142111
Current Trends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13 6.2 Research Gaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14 Conclusion Summary of Insights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15 Introduction Purpose of the Survey</li>
</ol>
<p>Figure 1 :
1
Figure 1: Literature Survey Tree</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: Distribution of Survey Domains in MLLM Studies</p>
<p>[86]light that MLLMs are particularly susceptible to adversarial attacks that exploit weaknesses in visual inputs.By introducing imperceptible noise into images, attackers can manipulate the vision encoder, leading to incorrect or even harmful text generation.Wang et al.[85]elaborate on jailbreaking attacks, where adversaries bypass the model's safety alignment mechanisms.Techniques like prompt injection can disrupt the model's chain-of-thought reasoning, leading to the generation of dangerous or inappropriate content.Especially in white-box attacks, attackers utilize the model's gradient information to craft adversarial examples that precisely control the model's outputs.[86]</p>
<p>. Evgeniȋ Borisovich Dynkin. Markov Processes. 1965Springer</p>
<p>Markov chains. Kai Lai, Chung , 1967Springer-VerlagNew York</p>
<p>A maximum likelihood approach to continuous speech recognition. Frederick Lalit R Bahl, Robert L Jelinek, Mercer, IEEE transactions on pattern analysis and machine intelligence. 1983</p>
<p>Dan Jurafsky, James H Martin, Speech and Language Processing. 3rd ed. draft). 2024. Draft chapters and slides available at</p>
<p>Recurrent neural networks. Stephen Grossberg, Scholarpedia. 8218882013</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. 981997</p>
<p>Gradient-based learning applied to document recognition. Yann Lecun, Léon Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 86111998</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, arXiv:1301.3781Efficient estimation of word representations in vector space. 2013arXiv preprint</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee2009</p>
<p>Imagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. 252012</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)2014</p>
<p>Sequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. 201427</p>
<p>Neural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.04732014arXiv preprint</p>
<p>Rico Sennrich, Barry Haddow, Alexandra Birch, arXiv:1508.07909Neural machine translation of rare words with subword units. 2015arXiv preprint</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, CoRR, abs/1802.05365Deep contextualized word representations. 2018</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Devise: A deep visual-semantic embedding model. Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc ' , Aurelio Ranzato, Tomas Mikolov, Advances in neural information processing systems. 201326</p>
<p>Unifying visual-semantic embeddings with multimodal neural language models. Ryan Kiros, Ruslan Salakhutdinov, Richard S Zemel, arXiv:1411.25392014arXiv preprint</p>
<p>Canonical correlation analysis: An overview with application to learning methods. Sandor David R Hardoon, John Szedmak, Shawe-Taylor, Neural computation. 16122004</p>
<p>Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He, arXiv:2003.04297Improved baselines with momentum contrastive learning. 2020arXiv preprint</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, International conference on machine learning. PMLR2020</p>
<p>Big self-supervised models are strong semi-supervised learners. Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey E Hinton, Advances in neural information processing systems. 202033</p>
<p>Unsupervised feature learning via non-parametric instance discrimination. Zhirong Wu, Yuanjun Xiong, Stella X Yu, Dahua Lin, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Representation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.037482018arXiv preprint</p>
<p>Contrastive multiview coding. Yonglong Tian, Dilip Krishnan, Phillip Isola, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part XI 16</p>
<p>Self-supervised learning of pretext-invariant representations. Ishan Misra, Laurens Van Der Maaten, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Unsupervised learning of visual features by contrasting cluster assignments. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin, Advances in neural information processing systems. 202033</p>
<p>Bootstrap your own latent-a new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Advances in neural information processing systems. 202033</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>An empirical study of training end-to-end vision-and-language transformers. Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao, Simvlm, arXiv:2108.10904Simple visual language model pretraining with weak supervision. 2021arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>High-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Pixel-aware stable diffusion for realistic image superresolution and personalized stylization. Tao Yang, Peiran Ren, Xuansong Xie, Lei Zhang, arXiv:2308.144692023arXiv preprint</p>
<p>Sdxl: Improving latent diffusion models for high-resolution image synthesis. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach, arXiv:2307.019522023arXiv preprint</p>
<p>Scaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, CoRR, abs/2102.059182021</p>
<p>Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, Pengchuan Zhang, Florence, CoRR, abs/2111.11432A new foundation model for computer. 2021</p>
<p>UNITER: learning universal image-text representations. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, CoRR, abs/1909.117402019</p>
<p>Vilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, 2021</p>
<p>Large language models: A survey. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, arXiv:2402.061962024arXiv preprint</p>
<p>A survey on multimodal large language models for autonomous driving. Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, Tianren Gao, Erlong Li, Kun Tang, Zhipeng Cao, Tong Zhou, Ao Liu, Xinrui Yan, Shuqi Mei, Jianguo Cao, Ziran Wang, Chao Zheng, 2023</p>
<p>Mathematical language models: A survey. Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen, Bo Jiang, Aimin Zhou, arXiv:2312.076222023arXiv preprint</p>
<p>Erik Cambria, and Mengling Feng. Has multimodal learning delivered universal intelligence in healthcare? a comprehensive survey. Qika Lin, Yifan Zhu, Xin Mei, Ling Huang, Jingying Ma, Kai He, Zhen Peng, 2024</p>
<p>Large language models for robotics: A survey. Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S Yu, arXiv:2311.072262023arXiv preprint</p>
<p>Kaiyu Huang, Fengran Mo, Hongliang Li, You Li, Yuanchi Zhang, Weijian Yi, Yulong Mao, Jinchen Liu, Yuzhuang Xu, Jinan Xu, Jian-Yun Nie, Yang Liu, A survey on large language models with multilingualism: Recent advances and new frontiers. 2024</p>
<p>Roberto Gallotta, Graham Todd, Marvin Zammit, Sam Earle, Antonios Liapis, Julian Togelius, Georgios N Yannakakis, arXiv:2402.18659Large language models and games: A survey and roadmap. 2024arXiv preprint</p>
<p>Sparks of large audio models: A survey and outlook. Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Yi Ren, Heriberto Cuayáhuitl, Wenwu Wang, Xulong Zhang, Roberto Togneri, Erik Cambria, arXiv:2308.127922023arXiv preprint</p>
<p>Video understanding with large language models: A survey. Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, arXiv:2312.174322023arXiv preprint</p>
<p>When large language models meet citation: A survey. Yang Zhang, Yufei Wang, Kai Wang, Lina Quan Z Sheng, Adnan Yao, Wei Emma Mahmood, Rongying Zhang, Zhao, arXiv:2309.097272023arXiv preprint</p>
<p>Jian Lu, Shikhar Srivastava, Junyu Chen, Robik Shrestha, Manoj Acharya, Kushal Kafle, Christopher Kanan, arXiv:2408.05334Revisiting multi-modal llm evaluation. 2024arXiv preprint</p>
<p>A survey on benchmarks of multimodal large language models. Jian Li, Weiheng Lu, 2024</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>Large language models and cognitive science: A comprehensive review of similarities, differences, and challenges. Qian Niu, Junyu Liu, Ziqian Bi, Pohsun Feng, Benji Peng, Keyu Chen, arXiv:2409.023872024arXiv preprint</p>
<p>A survey on fairness in large language models. Yingji Li, Mengnan Du, Rui Song, Xin Wang, Ying Wang, arXiv:2308.101492023arXiv preprint</p>
<p>Bias and fairness in large language models: A survey. Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K Ahmed, Computational Linguistics. 2024</p>
<p>Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, Mike Zheng Shou, arXiv:2404.18930Hallucination of multimodal large language models: A survey. 2024arXiv preprint</p>
<p>Xinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, Børje F Karlsson, arXiv:2403.10249A survey on game playing agents and large models: Methods, applications, and challenges. 2024arXiv preprint</p>
<p>. Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai Wu, Zhengkai Jiang, Muyang He, Bo Zhao, Xin Tan, Zhenye Gan, Yabiao Wang, Chengjie Wang, Lizhuang Ma, 2024Efficient multimodal large language models: A survey</p>
<p>Few-shot adaptation of multi-modal foundation models: A survey. Fan Liu, Tianshu Zhang, Wenwen Dai, Wenwen Cai, Xiaocong Zhou, Delong Chen, 2024</p>
<p>A survey of multimodal large language model from a data-centric perspective. Tianyi Bai, Hao Liang, Binwang Wan, Yanran Xu, Xi Li, Shiyu Li, Ling Yang, Bozhou Li, Yifan Wang, Bin Cui, Ping Huang, Jiulong Shan, Conghui He, Binhang Yuan, Wentao Zhang, 2024</p>
<p>If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents. Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, arXiv:2401.008122024arXiv preprint</p>
<p>Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, arXiv:2311.05876Trends in integration of knowledge and large language models: A survey and taxonomy of methods, benchmarks, and applications. 2023arXiv preprint</p>
<p>Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Hao Wang, arXiv:2404.16789Continual learning of large language models: A comprehensive survey. 2024arXiv preprint</p>
<p>A survey on benchmarks of multimodal large language models. Jian Li, Weiheng Lu, 2024</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen, arXiv [cs.AI]A survey on large language model based autonomous agents. August 2023</p>
<p>Large multimodal agents: A survey. Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, Guanbin Li, arXiv [cs.CV]February 2024</p>
<p>Large language models empowered agent-based modeling and simulation: A survey and perspectives. Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, Yong Li, arXiv:2312.119702023arXiv preprint</p>
<p>Large language models on graphs: A comprehensive survey. Gang Bowen Jin, Chi Liu, Meng Han, Heng Jiang, Jiawei Ji, Han, arXiv:2312.027832023arXiv preprint</p>
<p>Knowledge graphs meet multi-modal learning: A comprehensive survey. Zhuo Chen, Yichi Zhang, Yin Fang, Yuxia Geng, Lingbing Guo, Xiang Chen, Qian Li, Wen Zhang, Jiaoyan Chen, Yushan Zhu, arXiv:2402.053912024arXiv preprint</p>
<p>Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu, Yu , arXiv:2311.12399A survey of graph meets large language model: Progress and future directions. 2023arXiv preprint</p>
<p>Yucheng Hu, Yuxing Lu, arXiv:2404.19543Rag and rau: A survey on retrieval-augmented language model in natural language processing. 2024arXiv preprint</p>
<p>Mllm is a strong reranker: Advancing multimodal retrieval-augmented generation via knowledge-enhanced reranking and noise-injected training. Zhanpeng Chen, Chengjin Xu, Yiyan Qi, Jian Guo, 2024</p>
<p>Retrieving multimodal information for augmented generation: A survey. Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, Shafiq Joty, 2023</p>
<p>Large language models for generative information extraction: A survey. Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Enhong Chen, arXiv:2312.176172023arXiv preprint</p>
<p>Large-scale multi-modal pre-trained models: A comprehensive survey. Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, Wen Gao, 2024</p>
<p>Wentao Ge, Shunian Chen, Guiming Hardy Chen, Zhihong Chen, Junying Chen, Shuo Yan, Chenghao Zhu, Ziyue Lin, Wenya Xie, Xinyi Zhang, Yichen Chai, Xiaoyu Liu, Dingjie Song, Xidong Wang, Anningzhe Gao, Zhiyi Zhang, Jianquan Li, Xiang Wan, and Benyou Wang. Mllm-bench: Evaluating multimodal llms with per-sample criteria. 2024</p>
<p>Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, Lichao Sun, 2024</p>
<p>The revolution of multimodal large language models: A survey. Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara, 2024</p>
<p>From llms to mllms: Exploring the landscape of multimodal jailbreaking. Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, arXiv:2406.148592024arXiv preprint</p>
<p>Mllm-protector: Ensuring mllm's safety without hurting performance. Renjie Pi, Tianyang Han, Jianshu Zhang, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang, 2024</p>
<p>A survey on safe multi-modal learning systems. Tianyi Zhao, Liangliang Zhang, Yao Ma, Lu Cheng, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh, arXiv:2310.10844Survey of vulnerabilities in large language models revealed by adversarial attacks. 2023arXiv preprint</p>
<p>A survey on hallucination in large vision-language models. Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, Wei Peng, 2024</p>
<p>Internal consistency and self-feedback in large language models: A survey. Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai Li, Rong-Hua Li, Feiyu Xiong, Zhiyu Li, arXiv:2407.145072024arXiv preprint</p>
<p>Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li, arXiv:2308.05374Trustworthy llms: A survey and guideline for evaluating large language models' alignment. 2023arXiv preprint</p>
<p>Haibo Jin, Leyang Hu, Xinuo Li, Peiyan Zhang, Chonghan Chen, Jun Zhuang, Haohan Wang, arXiv:2407.01599Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models. 2024arXiv preprint</p>
<p>The (r) evolution of multimodal large language models: A survey. Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara, arXiv:2402.124512024arXiv preprint</p>
<p>Large language models meet computer vision: A brief survey. Raby Hamadi, arXiv:2311.166732023arXiv preprint</p>
<p>Shezheng Song, Xiaopeng Li, Shasha Li, arXiv:2311.07594How to bridge the gap between modalities: A comprehensive survey on multimodal large language model. 2023arXiv preprint</p>
<p>A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, arXiv:2302.094192023arXiv preprint</p>
<p>Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen, arXiv:2306.13549A survey on multimodal large language models. 2023arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, A survey of large language models. 2023</p>
<p>Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: A gpt-4v level mllm on your phone. 2024</p>
<p>Ee-mllm: A data-efficient and compute-efficient multimodal large language model. Feipeng Ma, Yizhou Zhou, Hebei Li, Zilong He, Siying Wu, Fengyun Rao, Yueyi Zhang, Xiaoyan Sun, 2024</p>
<p>Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects. Rizwan Muhammad Usman Hadi, Abbas Qureshi, Muhammad Shah, Anas Irfan, Muhammad Zafar, Naveed Bilal Shaikh, Jia Akhtar, Seyedali Wu, Mirjalili, 2023Authorea Preprints</p>
<p>Multimodal large language models: A survey. J Wu, W Gan, Z Chen, S Wan, P S Yu, 2023 IEEE International Conference on Big Data (BigData). Los Alamitos, CA, USAIEEE Computer Societydec 2023</p>
<p>Foundational models defining a new era in vision: A survey and outlook. Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Fahad Shahbaz Khan, arXiv:2307.137212023arXiv preprint</p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, Xia Hu, ACM Transactions on Knowledge Discovery from Data. 1862024</p>
<p>The future of gpt: A taxonomy of existing chatgpt research, current challenges, and possible future directions. Saquib Shahab, Faiza Sohail, Yassine Farhat, Mohammad Himeur, Dag Nadeem, Yashbir Øivind Madsen, Shadi Singh, Wathiq Atalla, Mansoor, Current Challenges, and Possible Future Directions. April 8, 20232023</p>
<p>A survey of gpt-3 family large language models including chatgpt and gpt-4. Katikapalli Subramanyam, Kalyan , Natural Language Processing Journal. 1000482023</p>
<p>Kilian Carolan, Laura Fennelly, Alan F Smeaton, arXiv:2404.01322A review of multi-modal large language and vision models. 2024arXiv preprint</p>
<p>Libo Qin, Qiguang Chen, Yuhang Zhou, Zhi Chen, Yinghui Li, Lizi Liao, Min Li, Wanxiang Che, Philip S Yu, Multilingual large language model: A survey of resources, taxonomy and frontiers. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
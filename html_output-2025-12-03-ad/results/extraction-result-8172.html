<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8172 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8172</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8172</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-278327095</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.02099v1.pdf" target="_blank">MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents</a></p>
                <p><strong>Paper Abstract:</strong> Recently, large language model based (LLM-based) agents have been widely applied across various fields. As a critical part, their memory capabilities have captured significant interest from both industrial and academic communities. Despite the proposal of many advanced memory models in recent research, however, there remains a lack of unified implementations under a general framework. To address this issue, we develop a unified and modular library for developing advanced memory models of LLM-based agents, called MemEngine. Based on our framework, we implement abundant memory models from recent research works. Additionally, our library facilitates convenient and extensible memory development, and offers user-friendly and pluggable memory usage. For benefiting our community, we have made our project publicly available at https://github.com/nuster1128/MemEngine.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8172.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8172.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FUMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A long-context memory that naively concatenates all stored information into a single string for use as context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>FUMemory (as memory module)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory module that provides long-context storage by concatenating all past information.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-context / external buffer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>All information is concatenated into a single string and included in the prompt/context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw historical observations and messages concatenated as text</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation (no explicit retrieval filter)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No experimental ablations or numeric comparisons reported in this paper; described as one implemented model in the library.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Implemented as a simple baseline memory strategy in the unified framework; useful for long-context scenarios but not optimized with retrieval or summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scales poorly with context length and token limits; no experimental evaluation provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8172.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8172.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LTMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long-term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic-retrieval based memory that uses text embeddings to find the most relevant past information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LTMemory (as memory module)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieval-augmented memory storing embeddings and performing semantic similarity search to answer queries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented long-term memory / episodic</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Encodes texts to embeddings (via Encoder function) and retrieves relevant items by semantic similarity</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored text entries with corresponding embeddings and possibly summaries/indices</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic search via embeddings (similarity ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No comparative performance or ablation reported in this paper; described as an implemented model that shares recall ops with MTMemory.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Implemented as a standard semantic-retrieval long-term memory in the modular framework to support reuse across models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No empirical evaluation within this paper; implementation details (e.g., encoder model) are configurable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8172.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8172.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Short-term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recency-focused memory that maintains the most recent information and concatenates it into context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>STMemory (as memory module)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A short-term buffer that keeps recent observations and concatenates them to the prompt for immediate context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working / recency-based short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Maintain a rolling buffer of recent observations and concatenate into the prompt</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recent observations and messages (raw text)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency-based selection (most recent entries)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No experimental ablations reported; provided as a basic memory option.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Useful for capturing immediate context and short-horizon dependencies within the unified framework.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited to recent context and constrained by token limits; no empirical comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8172.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8172.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Memory model inspired by 'Generative Agents' that uses weighted retrieval combination and a self-reflection mechanism to store and recall important observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (GAMemory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory model that assigns importance scores to observations (LLMJudge) and uses weighted retrieval plus reflection to form higher-level memories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic with reflection and importance-weighted retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Judge (LLM) assigns importance on store; recall uses weighted combination of memories; includes reflect/management operations for insight extraction</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Observations with importance scores and possibly generated summaries/reflections</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Weighted retrieval combining multiple relevant memories, influenced by judged importance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No quantitative ablations in this library paper; GAMemory implemented by reusing memory functions like LLMJudge in the framework.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GAMemory's judge and reflection components are available as modular functions, enabling importance-weighted recall and higher-level reflection within MemEngine.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper does not present performance evaluations; complexity of judge/reflection depends on LLM calls and configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8172.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8172.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MBMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank (MBMemory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-layered memory model that uses dynamic summarization and forgetting mechanisms to manage long-term knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank (MBMemory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A hierarchical/multi-layer memory system that summarizes and forgets to maintain useful long-term information.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>multi-layer long-term memory with dynamic summarization and forgetting</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Dynamic summarization to compress memory layers and forgetting mechanisms to prune less useful content</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Layered summaries and selected detailed entries (summaries + raw items)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic retrieval across layers, potentially prioritized by importance or recency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No experimental ablations provided in this library paper; described as implemented with reasonable adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MemEngine implements MemoryBank's multi-layer summarization and forgetting as reusable operations to support long-term memory management.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No direct empirical evaluation in this paper; relies on configurable summarization and forgetting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8172.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8172.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Controlled Memory (SCM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-controlled memory framework designed to recall minimal but necessary information for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhancing large language model with self-controlled memory framework.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SCM (SCMemory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory module that selectively recalls concise necessary information to support inference rather than retrieving many items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>selective/concise retrieval (minimal necessary memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Self-control logic chooses minimal subset of memory to recall for a given query (focus on necessity)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Key succinct facts or summaries sufficient for current inference</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Selective retrieval with emphasis on minimal but necessary items (heuristic/judged selection)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No ablation results in this paper; SCM is included as an implemented model in MemEngine.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SCM is supported in the framework to allow experiments that prefer brevity/necessity over retrieving many items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The library description does not include evaluation of the trade-offs between minimal retrieval and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8172.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8172.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MGMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT Memory (MGMemory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical memory model that conceptualizes the memory system as an operating system for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memgpt: Towards llms as operating systems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT (MGMemory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A hierarchical memory design where memory modules and operations are structured analogously to an OS managing resources.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hierarchical memory / system-level memory management</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Hierarchical organization of memory with operations treated as system calls (modular operations and functions)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured hierarchical records and summaries across levels</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Hierarchy-aware retrieval possibly combining levels (implementation configurable)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No quantitative comparisons presented in this library paper; MGMemory implemented as part of the available models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MemEngine includes MemGPT-style hierarchical memory abstractions to support system-level organization of memory operations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No benchmarked evidence in this paper; behavior depends on the specific hierarchical strategies chosen by users.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8172.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8172.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion Memory (RFMemory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory model from Reflexion where agents learn to memorize from past trajectories via optimization (verbal reinforcement learning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion (RFMemory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory/learning approach that optimizes what to memorize from previous trajectories, using reflection-like optimization procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>learn-to-memorize / optimization-based memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Uses extra trials and trajectories to optimize memory content (train-to-memorize/meta-insight extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Lessons/meta-insights derived from past trajectories and possibly stored summaries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Optimized retrieval guided by learned memorization strategies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No experimental ablation results in this library paper; RFMemory included as an implemented model focusing on optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MemEngine implements optimize-style operations to support Reflexion's learn-to-memorize paradigm within agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No empirical results here; optimization procedures require extra trials and trajectories which may be compute-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8172.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8172.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemTree (MTMemory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic tree-structured semantic memory representation that organizes information hierarchically as a tree.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemTree (MTMemory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory model that organizes memories into a tree-structured semantic representation supporting dynamic updates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hierarchical tree-structured memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Tree-structured semantic representation with specialized store (MTMemoryStore) and shared recall ops (LTMemoryRecall)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Semantic nodes in a tree (summaries, relations, branches capturing structure of information)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic retrieval adapted to tree structure (shares LTMemoryRecall for similarity-based retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No ablation or numeric comparisons reported in this library paper; MemTree implemented with adapted store and recall operations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MTMemory's tree structure is implemented as specialized store/recall ops, enabling structured organization of memories within MemEngine.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No experimental validation in this paper; complexity and benefits depend on downstream task, not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior. <em>(Rating: 2)</em></li>
                <li>Memorybank: Enhancing large language models with long-term memory. <em>(Rating: 2)</em></li>
                <li>Memgpt: Towards llms as operating systems. <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning. <em>(Rating: 2)</em></li>
                <li>From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs. <em>(Rating: 2)</em></li>
                <li>Enhancing large language model with self-controlled memory framework. <em>(Rating: 2)</em></li>
                <li>A survey on the memory mechanism of large language model based agents. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8172",
    "paper_id": "paper-278327095",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "FUMemory",
            "name_full": "Full Memory",
            "brief_description": "A long-context memory that naively concatenates all stored information into a single string for use as context.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "FUMemory (as memory module)",
            "agent_description": "A memory module that provides long-context storage by concatenating all past information.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "memory_used": true,
            "memory_type": "long-context / external buffer",
            "memory_mechanism": "All information is concatenated into a single string and included in the prompt/context",
            "memory_representation": "Raw historical observations and messages concatenated as text",
            "memory_retrieval_method": "Prompt concatenation (no explicit retrieval filter)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No experimental ablations or numeric comparisons reported in this paper; described as one implemented model in the library.",
            "key_findings": "Implemented as a simple baseline memory strategy in the unified framework; useful for long-context scenarios but not optimized with retrieval or summarization.",
            "limitations_or_challenges": "Scales poorly with context length and token limits; no experimental evaluation provided here.",
            "uuid": "e8172.0",
            "source_info": {
                "paper_title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LTMemory",
            "name_full": "Long-term Memory",
            "brief_description": "A semantic-retrieval based memory that uses text embeddings to find the most relevant past information.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "LTMemory (as memory module)",
            "agent_description": "A retrieval-augmented memory storing embeddings and performing semantic similarity search to answer queries.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "memory_used": true,
            "memory_type": "retrieval-augmented long-term memory / episodic",
            "memory_mechanism": "Encodes texts to embeddings (via Encoder function) and retrieves relevant items by semantic similarity",
            "memory_representation": "Stored text entries with corresponding embeddings and possibly summaries/indices",
            "memory_retrieval_method": "Semantic search via embeddings (similarity ranking)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No comparative performance or ablation reported in this paper; described as an implemented model that shares recall ops with MTMemory.",
            "key_findings": "Implemented as a standard semantic-retrieval long-term memory in the modular framework to support reuse across models.",
            "limitations_or_challenges": "No empirical evaluation within this paper; implementation details (e.g., encoder model) are configurable.",
            "uuid": "e8172.1",
            "source_info": {
                "paper_title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "STMemory",
            "name_full": "Short-term Memory",
            "brief_description": "A recency-focused memory that maintains the most recent information and concatenates it into context.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "STMemory (as memory module)",
            "agent_description": "A short-term buffer that keeps recent observations and concatenates them to the prompt for immediate context.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "memory_used": true,
            "memory_type": "working / recency-based short-term memory",
            "memory_mechanism": "Maintain a rolling buffer of recent observations and concatenate into the prompt",
            "memory_representation": "Recent observations and messages (raw text)",
            "memory_retrieval_method": "Recency-based selection (most recent entries)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No experimental ablations reported; provided as a basic memory option.",
            "key_findings": "Useful for capturing immediate context and short-horizon dependencies within the unified framework.",
            "limitations_or_challenges": "Limited to recent context and constrained by token limits; no empirical comparisons provided.",
            "uuid": "e8172.2",
            "source_info": {
                "paper_title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GAMemory",
            "name_full": "Generative Agents Memory",
            "brief_description": "Memory model inspired by 'Generative Agents' that uses weighted retrieval combination and a self-reflection mechanism to store and recall important observations.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior.",
            "mention_or_use": "use",
            "agent_name": "Generative Agents (GAMemory)",
            "agent_description": "A memory model that assigns importance scores to observations (LLMJudge) and uses weighted retrieval plus reflection to form higher-level memories.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "memory_used": true,
            "memory_type": "episodic with reflection and importance-weighted retrieval",
            "memory_mechanism": "Judge (LLM) assigns importance on store; recall uses weighted combination of memories; includes reflect/management operations for insight extraction",
            "memory_representation": "Observations with importance scores and possibly generated summaries/reflections",
            "memory_retrieval_method": "Weighted retrieval combining multiple relevant memories, influenced by judged importance",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No quantitative ablations in this library paper; GAMemory implemented by reusing memory functions like LLMJudge in the framework.",
            "key_findings": "GAMemory's judge and reflection components are available as modular functions, enabling importance-weighted recall and higher-level reflection within MemEngine.",
            "limitations_or_challenges": "Paper does not present performance evaluations; complexity of judge/reflection depends on LLM calls and configuration.",
            "uuid": "e8172.3",
            "source_info": {
                "paper_title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MBMemory",
            "name_full": "MemoryBank (MBMemory)",
            "brief_description": "A multi-layered memory model that uses dynamic summarization and forgetting mechanisms to manage long-term knowledge.",
            "citation_title": "Memorybank: Enhancing large language models with long-term memory.",
            "mention_or_use": "use",
            "agent_name": "MemoryBank (MBMemory)",
            "agent_description": "A hierarchical/multi-layer memory system that summarizes and forgets to maintain useful long-term information.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "memory_used": true,
            "memory_type": "multi-layer long-term memory with dynamic summarization and forgetting",
            "memory_mechanism": "Dynamic summarization to compress memory layers and forgetting mechanisms to prune less useful content",
            "memory_representation": "Layered summaries and selected detailed entries (summaries + raw items)",
            "memory_retrieval_method": "Semantic retrieval across layers, potentially prioritized by importance or recency",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No experimental ablations provided in this library paper; described as implemented with reasonable adaptations.",
            "key_findings": "MemEngine implements MemoryBank's multi-layer summarization and forgetting as reusable operations to support long-term memory management.",
            "limitations_or_challenges": "No direct empirical evaluation in this paper; relies on configurable summarization and forgetting strategies.",
            "uuid": "e8172.4",
            "source_info": {
                "paper_title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "SCMemory",
            "name_full": "Self-Controlled Memory (SCM)",
            "brief_description": "A self-controlled memory framework designed to recall minimal but necessary information for inference.",
            "citation_title": "Enhancing large language model with self-controlled memory framework.",
            "mention_or_use": "use",
            "agent_name": "SCM (SCMemory)",
            "agent_description": "Memory module that selectively recalls concise necessary information to support inference rather than retrieving many items.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "memory_used": true,
            "memory_type": "selective/concise retrieval (minimal necessary memory)",
            "memory_mechanism": "Self-control logic chooses minimal subset of memory to recall for a given query (focus on necessity)",
            "memory_representation": "Key succinct facts or summaries sufficient for current inference",
            "memory_retrieval_method": "Selective retrieval with emphasis on minimal but necessary items (heuristic/judged selection)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No ablation results in this paper; SCM is included as an implemented model in MemEngine.",
            "key_findings": "SCM is supported in the framework to allow experiments that prefer brevity/necessity over retrieving many items.",
            "limitations_or_challenges": "The library description does not include evaluation of the trade-offs between minimal retrieval and performance.",
            "uuid": "e8172.5",
            "source_info": {
                "paper_title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MGMemory",
            "name_full": "MemGPT Memory (MGMemory)",
            "brief_description": "A hierarchical memory model that conceptualizes the memory system as an operating system for LLMs.",
            "citation_title": "Memgpt: Towards llms as operating systems.",
            "mention_or_use": "use",
            "agent_name": "MemGPT (MGMemory)",
            "agent_description": "A hierarchical memory design where memory modules and operations are structured analogously to an OS managing resources.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "memory_used": true,
            "memory_type": "hierarchical memory / system-level memory management",
            "memory_mechanism": "Hierarchical organization of memory with operations treated as system calls (modular operations and functions)",
            "memory_representation": "Structured hierarchical records and summaries across levels",
            "memory_retrieval_method": "Hierarchy-aware retrieval possibly combining levels (implementation configurable)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No quantitative comparisons presented in this library paper; MGMemory implemented as part of the available models.",
            "key_findings": "MemEngine includes MemGPT-style hierarchical memory abstractions to support system-level organization of memory operations.",
            "limitations_or_challenges": "No benchmarked evidence in this paper; behavior depends on the specific hierarchical strategies chosen by users.",
            "uuid": "e8172.6",
            "source_info": {
                "paper_title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RFMemory",
            "name_full": "Reflexion Memory (RFMemory)",
            "brief_description": "A memory model from Reflexion where agents learn to memorize from past trajectories via optimization (verbal reinforcement learning).",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "mention_or_use": "use",
            "agent_name": "Reflexion (RFMemory)",
            "agent_description": "A memory/learning approach that optimizes what to memorize from previous trajectories, using reflection-like optimization procedures.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "memory_used": true,
            "memory_type": "learn-to-memorize / optimization-based memory",
            "memory_mechanism": "Uses extra trials and trajectories to optimize memory content (train-to-memorize/meta-insight extraction)",
            "memory_representation": "Lessons/meta-insights derived from past trajectories and possibly stored summaries",
            "memory_retrieval_method": "Optimized retrieval guided by learned memorization strategies",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No experimental ablation results in this library paper; RFMemory included as an implemented model focusing on optimization.",
            "key_findings": "MemEngine implements optimize-style operations to support Reflexion's learn-to-memorize paradigm within agents.",
            "limitations_or_challenges": "No empirical results here; optimization procedures require extra trials and trajectories which may be compute-intensive.",
            "uuid": "e8172.7",
            "source_info": {
                "paper_title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MTMemory",
            "name_full": "MemTree (MTMemory)",
            "brief_description": "A dynamic tree-structured semantic memory representation that organizes information hierarchically as a tree.",
            "citation_title": "From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs.",
            "mention_or_use": "use",
            "agent_name": "MemTree (MTMemory)",
            "agent_description": "A memory model that organizes memories into a tree-structured semantic representation supporting dynamic updates.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "memory_used": true,
            "memory_type": "hierarchical tree-structured memory",
            "memory_mechanism": "Tree-structured semantic representation with specialized store (MTMemoryStore) and shared recall ops (LTMemoryRecall)",
            "memory_representation": "Semantic nodes in a tree (summaries, relations, branches capturing structure of information)",
            "memory_retrieval_method": "Semantic retrieval adapted to tree structure (shares LTMemoryRecall for similarity-based retrieval)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No ablation or numeric comparisons reported in this library paper; MemTree implemented with adapted store and recall operations.",
            "key_findings": "MTMemory's tree structure is implemented as specialized store/recall ops, enabling structured organization of memories within MemEngine.",
            "limitations_or_challenges": "No experimental validation in this paper; complexity and benefits depend on downstream task, not evaluated here.",
            "uuid": "e8172.8",
            "source_info": {
                "paper_title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior.",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory.",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Memgpt: Towards llms as operating systems.",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs.",
            "rating": 2,
            "sanitized_title": "from_isolated_conversations_to_hierarchical_schemas_dynamic_tree_memory_representation_for_llms"
        },
        {
            "paper_title": "Enhancing large language model with self-controlled memory framework.",
            "rating": 2,
            "sanitized_title": "enhancing_large_language_model_with_selfcontrolled_memory_framework"
        },
        {
            "paper_title": "A survey on the memory mechanism of large language model based agents.",
            "rating": 1,
            "sanitized_title": "a_survey_on_the_memory_mechanism_of_large_language_model_based_agents"
        }
    ],
    "cost": 0.011533499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents
4 May 2025</p>
<p>Zeyu Zhang zeyuzhang@ruc.edu.cn 
Quanyu Dai daiquanyu@huawei.com 
Xu Chen xu.chen@ruc.edu.cn 
Rui Li 
Zhongyang Li lizhongyang6@huawei.com 
Zhenhua Dong dongzhenhua@huawei.com 
Memengine </p>
<p>Renmin University of China
BeijingChina</p>
<p>Huawei Noah's Ark Lab Shenzhen
China</p>
<p>Renmin University of China
BeijingChina</p>
<p>Renmin University of China
BeijingChina</p>
<p>Huawei Technologies Ltd
BeijingChina</p>
<p>Huawei Noah's Ark Lab Shenzhen
China</p>
<p>MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents
4 May 202518FE73C34DC94149CF6C10E7FC79970010.1145/3701716.3715299arXiv:2505.02099v1[cs.AI]Large Language ModelAutonomous AgentMemory MechanismInformation RetrievalLibrary Resource
Recently, large language model based (LLM-based) agents have been widely applied across various fields.As a critical part, their memory capabilities have captured significant interest from both industrial and academic communities.Despite the proposal of many advanced memory models in recent research, however, there remains a lack of unified implementations under a general framework.To address this issue, we develop a unified and modular library for developing advanced memory models of LLM-based agents, called MemEngine.Based on our framework, we implement abundant memory models from recent research works.Additionally, our library facilitates convenient and extensible memory development, and offers user-friendly and pluggable memory usage.For benefiting our community, we have made our project publicly available at https://github.com/nuster1128/MemEngine.CCS Concepts Information systems  Users and interactive retrieval;  Software and its engineering  Software libraries and repositories.</p>
<p>Introduction</p>
<p>With the rapid development of large language models (LLMs), LLMbased agents have been widely applied across various fields, due to their capabilities to perform complex tasks and fulfill different roles [11].Among various internal modules, memory is one of the most critical components for agents, as it determines how they store historical data, reflect on existing knowledge, and recall useful information to support decision-making [13].Specifically, in complex tasks, memory enables the recording of critical information in the past agent-environment interactions, and provides task-related experiences from previous trajectories.In role-playing and social simulations, it highlights the characteristic and personality of each role, allowing for distinctiveness among different roles.</p>
<p>Although some recent works have proposed various memory models for LLM-based agents, they are implemented under different pipelines and lack a unified framework.This inconsistency presents challenges for developers to attempt different models in their experiments.Moreover, many basic functions (such as retrieval and summarization) are duplicated across different models, and researchers often need to implement them repeatedly when developing new models.Besides, many academic models are tightly integrated with agents in a non-pluggable manner, making them difficult to apply across different agents.</p>
<p>In order to address the above problems, we develop a unified and modular library named MemEngine, which facilitates the development of advanced memory models for LLM-based agents.The primary features of our library are summarized as follows:</p>
<p>Unified and Modular Memory Framework.We propose a unified memory framework composed of three hierarchical levels to organize and implement existing research models under a general structure.The lowest level comprises memory functions, implementing basic functions (e.g., retrieval) as foundational supports for different memory operations.The intermediate level encompasses memory operations, constituting basic operations (e.g., memory recall) to construct different memory models.The highest level involves memory models, implementing various existing research models (e.g., MemoryBank [14]) that can be conveniently applied in different agents.All these three levels are modularized inside our framework, where higher-level modules can reuse lower-level modules, thereby improving efficiency and consistency in implementation.Besides, we provide a configuration module for easy modification of hyper-parameters and prompts at different levels.We also implement a utility module to conveniently save and demonstrate memory contents.</p>
<p>Abundant Memory Implementation.Based on our unified and modular framework, we implement a wide range of memory models from recent research works, many of which are widely applied in diverse applications.All of these models can be easily switched and tested under our framework, with different configurations of hyper-parameters and prompts that can be adjusted for better application across various agents and tasks.</p>
<p>Convenient and Extensible Memory Development.Based on our modular memory operations and memory functions, researchers can conveniently develop their own advanced memory models.They can also extend existing operations and functions to develop their own ones.To better support researchers' development, we provide detailed instructions and examples in our document to guide the customization.</p>
<p>Pluggable and User-friendly Memory Usage.Our library offers multiple deployment options to empower LLM-based agent with powerful memory capabilities.Besides, we provide various memory usage modes, including default, configurable, and automatic modes.Moreover, our memory modules are pluggable and can be easily utilized across different agent frameworks.Our library is also compatible with some prominent frameworks of LLM-based agents, such as AutoGPT.These features collectively contribute to making our library more user-friendly.</p>
<p>In summary, MemEngine is the first library that implements a wide variety of memory models from research works under a unified and modular framework, facilitating both convenient development and ease of use.To further benefit the community, we have made our project publicly available at Github repository 1 .Additionally, we have also organized a comprehensive documentation2 for both application and development purposes.</p>
<p>Comparison with Relevant Libraries</p>
<p>Several existing libraries can also empower memory capabilities for LLM-based agents, including (1) memory modules integrated into agent libraries, and (2) independent memory libraries.Specifically, AutoGen3 , MetaGPT [3], CAMEL [4], AgentScope [2], LangChain4 , AgentLite [5], CrewAI5 , AutoGPT6 , and AgentVerse [1] are prominent open-source libraries for building LLM-based AI agent systems.Besides, Memary 7 is an open-source library to empower AI agents with memory for continuous improvement.Cognee8 provides a scalable and modular pipeline to interconnect and retrieve previous information for AI applications.Mem09 offers an artificial memory layer for LLM-based agents and assistants to make them personalized.Agentmemory10 implements easy-to-use memory for LLM-based agents with document search and more.We present a comprehensive comparison between MemEngine and relevant libraries in Table 1.While most of these libraries offer plug-and-play memory components to store and recall information for LLM-based agents, few of them implement advanced memory operations like reflection and optimization.Moreover, compared with other libraries, as a major contribution, MemEngine implements comprehensive research models under a unified framework, along with providing modular operations and functions that assist researchers in customizing advanced memory models.</p>
<p>Memory Functions Memory Operations</p>
<p>Memory Models Memory Configurations Memory Utilities
Encoder
3 MemEngine Library</p>
<p>Overview</p>
<p>The framework of our library is present in Figure 1.The lowest level implements basic functions as standard support.The intermediate level comprises memory operations for fundamental processes.The highest level features various memory models from previous research works.Additionally, we provide a configuration module and a utility module to facilitate convenient development and usage.</p>
<p>Memory Models</p>
<p>We implement a variety of memory models from recent research works under a general structure, allowing seamless switching among them.Specifically, these models are implemented with the interfaces including reset, store, recall, manage, and optimize.</p>
<p>The implemented memory models are described as follows:  FUMemory (Full Memory): Naively concatenates all the information into a single string, also known as long-context memory.</p>
<p> LTMemory (Long-term Memory): Calculates semantic similarities with text embeddings to retrieve the most relevant information.</p>
<p> STMemory (Short-term Memory): Maintains the most recent information and concatenates them into a single string.</p>
<p> GAMemory (Generative Agents [7]): A pioneer memory model with weighted retrieval combination and self-reflection mechanism.</p>
<p> MBMemory (MemoryBank [14]): A multi-layered memory model with dynamic summarization and forgetting mechanism.</p>
<p> SCMemory (SCM [10]): A self-controlled memory model that can recall minimal but necessary information for inference.</p>
<p> MGMemory (MemGPT [6]): A hierarchical memory model that treats the memory system as an operation system.</p>
<p> RFMemory (Reflexion [9]): A prominent memory model that can learn to memorize from previous trajectories by optimization.</p>
<p> MTMemory (MemTree [8]): A dynamic memory model with a tree-structured semantic representation to organize information.</p>
<p>All of these memory models are implemented by combining various memory operations, and we make some reasonable adaptations
      Comprehensive Default Models        Advanced Model Customization       
in their implementations.Further details can be found in our project documentation and source code.</p>
<p>Memory Operations</p>
<p>We implement various types of memory operations for constructing memory models, including store, recall, manage, and optimize.Memory Store Operation intends to receive observations from the environment, processing them to obtain memory contents and adding them into memory storage.Another critical function of the memory store operation is to establish foundations for the memory recall operation, such as creating indexes and summaries.</p>
<p>Memory Recall Operation intends to obtain useful information to assist agents in their decision-making.Typically, the input is a query or observation representing the current state of agents.Some human-like agents may also endow the memory recall operation with certain retention characteristics like human memory.</p>
<p>Memory Manage Operation intends to reorganize existing information for better utilization, such as memory reflection.Besides, simulation-orientated agents may be equipped with a forgetting mechanism during the memory manage operation.</p>
<p>Memory Optimize Operation intends to optimize the memory capability of LLM-based agents by using extra trials and trajectories.It enables agents to extract meta-insight from historical experiences, which can be considered as a learn-to-memorize procedure.</p>
<p>Different memory models may share common memory operations or implement their unique operations according to their requirements.For example, MTMemory and LTMemory share the common memory recall operation LTMemoryRecall, while MTMemory has its own memory store operation MTMemoryStore to implement the tree-structured information update.</p>
<p>Memory Functions</p>
<p>We implement various types of memory functions to support the construction of memory operations, which are listed as follows.</p>
<p>Encoder can transfer textual messages into embeddings to represent in latent space by pre-trained models, such as E5 [12].</p>
<p>Retrieval is utilized to find the most useful information for the current query or observation, commonly by different aspects like semantic relevance, importance, recency, and so on.</p>
<p>Reflector aims to draw new insights in a higher level from existing information, commonly for reflection and optimization.</p>
<p>Summarizer can summarize texts into a brief summary, which can decrease the lengths of texts and emphasize critical points.</p>
<p>Trigger is designed to call functions or tools in extensible manners.One significant instance is utilizing LLMs to determine which function should be called with certain arguments.</p>
<p>Utilization aims to deal with several different parts of memory contents, formulating this information into a unified output.</p>
<p>Forget is typically applied in simulation-oriented agents, such as role-playing and social simulations.It empowers agents with features of human cognitive psychology, aligning with human roles.</p>
<p>Truncation helps to formulate memory contexts under the limitations of token numbers by certain LLMs.</p>
<p>Judge intends to assess given observations or intermediate messages on certain aspects.For example, GAMemory judges the importance score of each observation when stored into memory, as an auxiliary criterion for the retrieval process.</p>
<p>LLM provides a convenient interface to utilize the powerful capability of different large language models.</p>
<p>All these memory functions are designed to conveniently construct different memory operations for various models.For example, GAMemoryStore utilizes LLMJudge to provide the importance score on each observation.</p>
<p>Memory Configurations</p>
<p>In order to improve convenience for developers and facilitate parameter tuning by researchers, we have developed a unified memory configuration module.First, we design a hierarchical memory configuration module corresponding to our three-level memory framework, enabling adjustments to both hyper-parameters and prompts within the memory models.Second, we provide a comprehensive set of default hyper-parameters and prompts, where developers and researchers can adjust only the specific parts without altering others.Finally, our configuration supports both statistic manners (e.g., files) and dynamic manners (e.g., dictionaries).</p>
<p>Memory Utilities</p>
<p>We implement extra utilities as auxiliary components, which are loosely coupled with the above modules.We implement a storage module as a database to retain the contents of information.We also provide a display module to visualize the specific contents within the memory.Besides, we offer a client module to utilize MemEngine through remote deployment on a server implemented by FastAPI.We also implement an automatic selector to assist developers to choose memory models with hyper-parameters for their own tasks.</p>
<p>Usage of MemEngine</p>
<p>In this section, we describe the usage of MemEngine to empower LLM-based agents with advanced memory capabilities.We divide our usage into two aspects: (1) utilize pre-implemented memory models, and (2) customize new memory models.</p>
<p>Utilize Pre-implemented Memory Models</p>
<p>There are two primary ways to deploy our library as follows.</p>
<p>Local Deployment.Developers can easily install our library in their Python environment via pip, conda, or from source code.Then, they can create memory modules for their agents, and utilize unified interfaces to perform memory operations within programs.</p>
<p>Remote Deployment.Alternatively, developers can install our library on computing servers and launch the service through a port.Then, they can initiate a client to perform memory operations by sending HTTP requests remotely from their lightweight devices.</p>
<p>After deployment, there are three modes available for utilizing pre-implemented memory models.In the default mode, the library provides a comprehensive set of hyper-parameters and prompts for default usage.In the configurable mode, developers can adjust certain hyper-parameters and prompts to better adapt to their applications.In the automatic mode, the library automatically selects the appropriate memory models, hyper-parameters, and prompts from the provided ranges, based on a specific task's criteria.</p>
<p>Additionally, our library offers compatibility with several wellknown tools and frameworks, such as vllm and AutoGPT.</p>
<p>Customize New Memory Models</p>
<p>Our library provides support for developers to customize advanced memory models, offering comprehensive documentation and examples.There are three major aspects to customizing new models.</p>
<p>Customize Memory Functions.Researchers may need to implement new functions in their models to extend existing ones for additional features.For example, they may extend LLMJudge to design a BiasJudge for poisoning detection.</p>
<p>Customize Memory Operations.In developing a new model, customizing memory operations is crucial as they constitute the major pipelines of the detailed processes.For instance, a new memory recall operation can be implemented with a series of memory functions with advanced design and combination.</p>
<p>Customize Memory Models.By integrating newly customized memory operations with existing ones, researchers can design their models with various combinations to best suit their applications.</p>
<p>Conclusion</p>
<p>In this paper, we introduce a unified and modular library for developing advanced memory of LLM-based agents.In the future, we plan to provide support for multi-modal memory (such as visual and audio memory) to further enrich and enhance the memory capabilities of LLM-based agents for wider applications.</p>
<p>Figure 1 :
1
Figure 1: An overview framework of MemEngine Library.We present a comprehensive comparison between MemEngine and relevant libraries in Table1.While most of these libraries offer plug-and-play memory components to store and recall information for LLM-based agents, few of them implement advanced memory operations like reflection and optimization.Moreover, compared with other libraries, as a major contribution, MemEngine implements comprehensive research models under a unified framework, along with providing modular operations and functions that assist researchers in customizing advanced memory models.</p>
<p>Table 1 :
1
Comparison with relevant open-source libraries.We focus on both memory modules integrated in prominent agent libraries, and independent memory libraries.Besides the libraries mentioned below, AutoGPT and AgentVerse do not specify their memory support in their library.AutoGen supports MemGPT, Mem0, and Zep as extensions.
Memory Integrated in Agent LibrariesMemory FeaturesAutoGen MetaGPT CAMELAgentScopeLangchainAgentLiteCrewAIPlug-and-play IntegrationBasic Read and Write SupportReflection and Optimization SupportComprehensive Default ModelsAdvanced Model CustomizationIndependent Memory LibrariesMemory FeaturesMemaryCogneeMem0 Agentmemory MemoryScopeZepMemEngine (Ours)Plug-and-play IntegrationBasic Read and Write SupportReflection and Optimization Support
https://github.com/nuster1128/MemEngine
https://memengine.readthedocs.io/en/latest/
https://github.com/microsoft/autogen
https://github.com/langchain-ai/langchain
https://github.com/crewAIInc/crewAI
https://github.com/Significant-Gravitas/AutoGPT
https://github.com/kingjulio8238/Memary
https://github.com/topoteretes/cognee
https://github.com/mem0ai/mem0
https://github.com/ai16z/agentmemory
https://www.mindspore.cn/
AcknowledgmentsThis work is supported in part by National Natural Science Foundation of China (No. 62422215 and No. 62472427), Beijing Outstanding Young Scientist Program NO.BJJWZYJH012019100020098, Intelligent Social Governance Platform, Major Innovation &amp; Planning Interdisciplinary Platform for the "DoubleFirst Class" Initiative, Renmin University of China, Public Computing Cloud, Renmin University of China, fund for building world-class universities (disciplines) of Renmin University of China, Intelligent Social Governance Platform.This work is also sponsored in part by Huawei Innovation Research Program.We gratefully acknowledge the support from Mindspore 11 , CANN (Compute Architecture for Neural Networks) and Ascend AI Processor used for this research.
Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, ICLR. 2023</p>
<p>Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang, Yuexiang Xie, Daoyuan Chen, arXiv:2402.14034Agentscope: A flexible yet robust multi-agent platform. 2024. 2024arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Metagpt: Meta programming for multi-agent collaborative framework. 2023. 2023arXiv preprint</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, NeurIPS. 362023. 2023</p>
<p>Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Tian Prafulla K Choubey, Jason Lan, Huan Wu, Wang, arXiv:2402.15538AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System. 2024. 2024arXiv preprint</p>
<p>Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2023. 2023arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, UIST. 2023</p>
<p>Alireza Rezazadeh, Zichao Li, Wei Wei, Yujia Bao, arXiv:2410.14052From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs. 2024. 2024arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, NeurIPS. 362024. 2024</p>
<p>Enhancing large language model with self-controlled memory framework. Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li, arXiv:2304.133432023. 2023arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, FCS. 181863452024. 2024</p>
<p>Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv:2212.03533Text embeddings by weakly-supervised contrastive pre-training. 2022. 2022arXiv preprint</p>
<p>Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, arXiv:2404.13501A survey on the memory mechanism of large language model based agents. 2024. 2024arXiv preprint</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, In AAAI. 382024. 19724-19731</p>            </div>
        </div>

    </div>
</body>
</html>
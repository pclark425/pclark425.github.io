<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-194 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-194</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-194</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-10.html">extraction-schema-10</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <p><strong>Paper ID:</strong> paper-0a962d107e8fee1cc97ecca2ae777b2bb48a031f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0a962d107e8fee1cc97ecca2ae777b2bb48a031f" target="_blank">ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work curates a dataset of over 1200 questions across six domains and benchmarks six top-performing LLMs, finding that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over 60% of the time.</p>
                <p><strong>Paper Abstract:</strong> Retrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over 60% of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs -- namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e194.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e194.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (evaluated variant in ClashEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A top-performing commercial LLM evaluated in ClashEval for how it reconciles its parametric prior with retrieved contextual evidence across multiple domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fact-based question answering across six domains (drugs, news, records, dates, names, locations) with and without retrieved contextual documents (RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>retrieved documents containing factual statements (including systematically perturbed/incorrect facts)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>retrieved documents from web/Wikipedia/UpToDate with synthetic perturbations generated for the study</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (contexts were engineered to be aligned or contradictory to the model's prior depending on the perturbation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Acc. without context (all datasets): 0.467 (fraction); per-dataset without-context accuracies listed in Table 5 (e.g., Drugs 0.578, News 0.088). Mean prior probability across all datasets: 0.675 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Acc. with correct single-document context (k=1, all datasets): 0.941 (fraction); per-dataset with-correct-context values in Table 5 (e.g., Drugs 0.863, News 0.971). Context bias (probability of following incorrect context when prior was correct): 0.304; Prior bias (following prior when context was correct): 0.021 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — correct evidence strongly increases accuracy (e.g., from 0.467 to 0.941 overall), while incorrect contextual evidence often decreases accuracy because the model frequently adopts the incorrect context (context bias ≈ 30.4%); overall adding evidence helps when evidence is correct but can harm when evidence is incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Models tend to 'defer' to contextual evidence when their internal token-confidence for the prior answer is low (selective deferral); they also show prompt-sensitive sycophancy (wording influences literal adherence), and token-probability outputs are differently distributed for prior vs. context responses (uncalibrated).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4o strongly benefits from correct retrieved documents (large increase in accuracy), but exhibits substantial context bias: it frequently adopts incorrect retrieved facts (e.g., when prior is correct, it chooses the incorrect context answer ≈58.2% in some breakdowns and context bias ≈30.4% in aggregate). The likelihood of adopting context decreases with (1) larger deviation of the context from the prior (negative correlation) and (2) higher model token-probability/confidence for the prior (inverse relationship).</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e194.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e194.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo-0125 (evaluated variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used LLM baseline evaluated for RAG resilience; shows different balance between prior and context compared to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0125)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same fact-based QA with/without retrieved contextual documents and perturbed evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>retrieved documents with factual statements (including perturbed errors)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>retrieved documents from news/Wikipedia with synthetic perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Acc. without context (all datasets): 0.344 (Table 5 'All'); per-dataset without-context values e.g., Drugs 0.446, News 0.063. Mean prior probability across datasets: 0.573.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Acc. with correct single-document context (k=1, all datasets): 0.879 (Table 5); context bias 0.313, prior bias 0.028 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — correct retrieved evidence substantially increases accuracy; incorrect retrieved evidence often misleads the model (context bias ≈31.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Selective deferral based on token-probability confidence of the prior; uncalibrated token probabilities cause context responses to exhibit different probability distributions than priors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 tends to be susceptible to incorrect context similar to other models; the paper found an inverse relationship between prior token probability and chance of adopting context (i.e., the less confident GPT-3.5 is in its prior, the more likely it is to follow context).</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e194.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e194.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3-7B-Instruct (evaluated variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller instruction-tuned LLM evaluated for RAG behavior, showing different failure modes (more 'neither' responses).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fact-based QA with/without contextual retrieved documents; measures whether the model uses prior, context, or neither.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>retrieved documents containing factual statements (and perturbed variants)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>retrieved documents from Wikipedia/news with perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Acc. without context (all datasets): 0.228 (Table 5 'All'); per-dataset examples: Drugs 0.317, News 0.0714. Mean prior probability across datasets: 0.732.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Acc. with correct single-document context (k=1, all datasets): 0.805 (Table 5); context bias 0.264, prior bias 0.021 (Table 4). Llama-3 also has a higher 'neither chosen' rate (often produces answers that match neither prior nor context).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — correct evidence raises accuracy substantially; incorrect evidence still misleads the model but Llama-3 often produces answers that match neither prior nor context (higher 'neither' frequency), so effects are qualitatively different from larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Less consistent adherence to either source; model shows higher tendency to produce answers outside both prior and context possibly due to weaker parametric memory or different generation dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Llama-3 is less likely to strictly follow context compared to some larger models but achieves lower overall accuracy; it produces 'neither' answers more often, suggesting different integration behavior when evidence conflicts with priors.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e194.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e194.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude Opus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude Opus (evaluated variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Claude-family model evaluated in ClashEval; highest performing model on the benchmark with lower context bias than many competitors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fact-based QA under RAG conditions with perturbed evidence to measure prior vs. context preference.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>retrieved documents containing factual statements and perturbed variants</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>retrieved documents from multiple corpora with synthetic perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Acc. without context (all datasets): 0.463 (Table 6 'All'); per-dataset without-context e.g., Drugs 0.566, News 0.109. Mean prior prob not tabulated in same table but reported per-dataset in Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Acc. with correct single-document context (k=1, all datasets): 0.939 (Table 6 'All'); context bias 0.157 (much lower than many other models), prior bias 0.021, overall accuracy 0.743 (Table 4 / Appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive when context is correct (substantial accuracy gains) and comparatively less harmful when context is incorrect (lower context bias); overall more robust to incorrect evidence than many models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Has a stronger internal resistance to incorrect context (lower intercept and more negative slope in context-preference vs deviation), meaning its prior distribution over truthfulness and decision boundary favor rejecting implausible contextual deviations more than other models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Claude Opus achieved the best balance of accuracy, low context bias (0.157), and low prior bias (0.021) in ClashEval; it is more resistant to adopting incorrect contextual evidence and benefits greatly from correct evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e194.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e194.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude Sonnet (evaluated variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller Claude-family model evaluated for RAG behavior; intermediate performance between top and baseline models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>FACT QA with/without retrieved contextual evidence (perturbed).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>retrieved documents with factual statements (and perturbations)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>retrieved documents (Wikipedia/news/UpToDate) with synthetic perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Acc. without context (all datasets): 0.400 (per-dataset values in Appendix / Table 5); e.g., Drugs 0.534, News 0.0966.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Acc. with correct single-document context (k=1): 0.995 on names and generally high per-dataset improvements; aggregate accuracy 0.658 (Table 4 / Appendix). Context bias 0.201, prior bias 0.025 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — substantial accuracy gains with correct context; non-negligible susceptibility to incorrect context (context bias ~20.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Selective deferral to context moderated by model-specific prior distribution; similar qualitative mechanisms to other LMs but quantitatively different sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Claude Sonnet benefits strongly from correct evidence but still exhibits context bias; intermediate robustness compared to Claude Opus and other models.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e194.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e194.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.5 Flash (evaluated variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Google-family LLM evaluated for RAG behavior; shows a notable tendency to follow context in many settings with measurable context bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.5 Flash</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fact-based QA under RAG conditions with perturbed retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>retrieved documents containing factual statements and perturbed variants</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>retrieved documents (news/Wikipedia) with synthetic perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Acc. without context (all datasets): 0.463? (per-dataset listed in Appendix — Table 5 shows per-dataset values; overall accuracy in Table 4 is 0.624). Example per-dataset without-context: Drugs 0.213, News 0.0840.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Acc. with correct single-document context (k=1): per-dataset high improvements (e.g., Names 0.995), aggregate performance with correct context in Table 4 indicates strong gains; context bias 0.245, prior bias 0.037 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — correct evidence improves accuracy markedly; incorrect evidence leads to nontrivial error adoption (context bias ~24.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Selective deferral correlated with prior token probability and sensitivity to prompt/context phrasing; behaves similarly to other large models but quantitative sensitivity differs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gemini 1.5 gains large accuracy improvements from correct context but still exhibits context bias; shows domain-specific differences in susceptibility.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e194.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e194.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TokenProbCorr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token Probability Correction (method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple post-hoc correction that compares mean token probabilities of the model's generated answers with and without context and chooses the answer with higher mean token probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to models exposing token probabilities (GPT-4o, GPT-3.5, Llama-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Method to resolve conflicts between prior and contextual answers by comparing generation token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>use of model-internal token-probability estimates for r(q) and r(q|c)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>model-internal probability outputs (log-probs of tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>n/a (method acts on model outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline accuracies (no correction) for GPT-4o/GPT-3.5/Llama-3 were 0.615 / 0.539 / 0.500 respectively (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>After Token Probability Correction, accuracies improved to 0.693 (GPT-4o), 0.596 (GPT-3.5), 0.556 (Llama-3) (Table 3); context bias reduced moderately, prior bias increased modestly.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive (improves overall accuracy and reduces context bias compared to baseline), but introduces some increase in prior bias.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Comparing token-probability estimates approximates which answer the model 'prefers' given each condition; choosing the higher-probability generation helps revert incorrect context overrides when the prior is intrinsically more probable.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple token-probability comparison yields moderate accuracy improvements and reduces tendency to follow incorrect context, demonstrating that token-probabilities contain actionable signal about when to trust the model's prior vs context.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e194.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e194.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CalibratedTokenProb</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Calibrated Token Probability Correction (method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An improved version of Token Probability Correction that compares percentile-calibrated token-probability scores (rather than raw probabilities) to correct for distributional differences between prior and context responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to models exposing token probabilities (GPT-4o, GPT-3.5, Llama-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Calibration-based post-hoc correction to decide between prior and contextual answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>model-internal token-probability percentiles</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>model-internal probability outputs calibrated into percentiles</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>n/a (method operates on output distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline accuracies (no correction) in Table 3: GPT-4o 0.615, GPT-3.5 0.539, Llama-3 0.500.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Calibrated correction improved accuracies to 0.754 (GPT-4o), 0.701 (GPT-3.5), 0.649 (Llama-3) and substantially reduced context bias (Table 3); however prior bias increased (e.g., GPT-4o prior bias rose to 0.085).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive overall (large accuracy gains and reduced context bias), but tradeoff: increases prior bias (models revert to priors more often, sometimes incorrectly).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Calibration compensates for distributional mismatch between raw token probabilities produced with and without context, making comparisons more meaningful and improving decision of which answer to trust.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Calibrated percentile comparison between prior and context token-probabilities substantially improved accuracy (~14% absolute improvement reported) and reduced context bias (~20% reduction), demonstrating that probability calibration is a practical baseline for mitigating harmful adherence to incorrect context.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e194.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e194.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptSensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt sensitivity (Strict vs Loose prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed effect that small changes in prompt wording (strict vs loose instructions) significantly alter how readily LLMs follow context vs prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across models (examples shown for GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measurement of how different RAG prompt templates change context-preference behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>instructional prompt wording that frames how to use context (literal vs. judgment-based)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>experimental prompt templates used in ClashEval</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>neutral (prompt frames how to treat evidence rather than being evidence itself)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>N/A (effect concerns behavior when context is present).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Loose prompts (encouraging reasoned judgment) produced lower baseline context preference and steeper decreases in context preference with increasing prior probability compared to Strict prompts (which enforce literal adherence) — i.e., prompt wording changed how frequently models adopted context evidence (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>prompt wording can either increase or decrease adherence to contextual evidence (i.e., positive or negative depending on desired outcome); Loose prompts made models more likely to reject implausible incorrect context.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Sycophancy to prompt instructions and explicit framing: 'strict' wording encourages literal copying of context, 'loose' wording encourages model internal reasoning and checking.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt framing is a practical lever: stricter literal instructions increase context adherence (even to incorrect facts), while looser instructions encourage selective use of context and reduce erroneous adoption of incorrect evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e194.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e194.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeviationEffect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context deviation effect (degree of context modification vs preference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that the more the contextual evidence deviates from truth (or from the model prior), the less likely the model is to adopt that context; the relationship is negatively correlated and model-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across models (quantified per-model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Analysis of context-preference rate as a function of absolute deviation between context value and reference/prior (numerical multipliers, years offsets, categorical perturbations).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>measured perturbation magnitude of contextual facts (numerical multipliers, year shifts, name/city perturbation levels)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>systematic synthetic perturbations applied to retrieved documents in ClashEval</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory when perturbation ≠ reference; aligned when perturbation equals reference</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>N/A (relation concerns how context influence changes with deviation magnitude).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Context preference rate decreases as deviation magnitude increases; stronger models (e.g., Claude Opus) show lower intercept and steeper negative slope — i.e., they are more resistant to incorrect context for the same deviation than weaker models (Figure 3 / Section 4.3).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>negative for incorrect evidence but magnitude-dependent: small/subtle perturbations are more likely to be adopted (harmful), large/comical perturbations are less likely to be adopted (protective).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Models appear to integrate a plausibility prior over context; larger deviations from expected truth reduce the model's propensity to accept context—this differs per model, reflecting different prior distributions over truthfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>There is a clear inverse relationship between degree of context modification and context preference rate; subtle errors are especially dangerous because models are more likely to accept them, whereas blatant errors are more often rejected.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e194.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e194.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PriorProbEffect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prior token-probability effect (confidence vs context preference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical relationship showing that higher token-probability (confidence) of a model's prior answer strongly reduces the probability the model will adopt contextual evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across models (where token probabilities available)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Examining how the model's token-level probability/confidence of r(q) (answer without context) predicts the model's tendency to follow retrieved context r(q|c).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>model-generated token-probabilities (logprobs) for prior responses</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>internal model outputs measured in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (probability is a signal about alignment of prior to truth)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Mean prior probabilities per model/dataset reported (e.g., GPT-4o mean prior prob All: 0.675; per-dataset values in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Context preference rate falls as prior token probability increases; slopes ranged between approx. -0.1 and -0.45 across datasets/models — e.g., a slope of -0.45 interpreted as ~4.5% drop in context-adoption likelihood for every 10% increase in prior probability (Figure 4 / Section 4.4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>adding context is more likely to change model output when the model's prior probability is low (positive effect if context correct) and less likely when prior probability is high (protective against bad context).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Token-probabilities reflect model internal confidence; models use this internal signal implicitly to weigh external evidence (more deferral when internal confidence is low).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>There is a consistent inverse relationship across datasets and models between prior token-probability and context-preference; this provides an actionable signal for interventions (e.g., probability-based corrections) and explains why models sometimes correctly defer to evidence or stubbornly keep incorrect priors.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e194.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e194.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MultiDocEffect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-document RAG effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finding that adding multiple retrieved documents (k>1) often lowers overall accuracy and increases the frequency of responses that match neither prior nor any single context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed for GPT-4o and Claude Opus (and discussed generally)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assessing how adding 4 extra retrieved documents affects model adherence to context and final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>multiple retrieved documents (additional contextual sources ranked by embedding similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>retrieved documents from embeddings-based retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (additional docs can be correct or incorrect)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Single-document correct-context accuracy for GPT-4o, Claude Opus (k=1) reported in Table 6; multi-document (k=5) accuracy is lower (e.g., GPT-4o All: 0.922 vs 0.941 for k=1; Claude Opus All: 0.843 vs 0.939).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Adding more documents (k=5) reduced accuracy for both GPT-4o and Claude Opus and raised 'neither chosen' rates; multi-document context reduced context bias but increased confusion/other errors (Tables 6 and 7).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>negative overall in this study (more/context length led to lower accuracy and more ambiguous outputs), though it can reduce context bias by diluting a single incorrect document.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Longer or more heterogeneous context increases retrieval noise and distractors; models may struggle to integrate multiple possibly conflicting documents and therefore produce neither-prior-nor-context answers or incorrect syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-document RAG reduced model accuracy and increased ambiguous outputs; however it also lowered context bias because models were less likely to latch onto a single incorrect document.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Entity-based knowledge conflicts in question answering <em>(Rating: 2)</em></li>
                <li>Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts <em>(Rating: 2)</em></li>
                <li>Retrieval augmentation reduces hallucination in conversation <em>(Rating: 1)</em></li>
                <li>Certifiably robust rag against retrieval corruption <em>(Rating: 2)</em></li>
                <li>Benchmarking large language models in Retrieval-Augmented generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-194",
    "paper_id": "paper-0a962d107e8fee1cc97ecca2ae777b2bb48a031f",
    "extraction_schema_id": "extraction-schema-10",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (evaluated variant in ClashEval)",
            "brief_description": "A top-performing commercial LLM evaluated in ClashEval for how it reconciles its parametric prior with retrieved contextual evidence across multiple domains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": null,
            "task_description": "Fact-based question answering across six domains (drugs, news, records, dates, names, locations) with and without retrieved contextual documents (RAG).",
            "evidence_type": "retrieved documents containing factual statements (including systematically perturbed/incorrect facts)",
            "evidence_source": "retrieved documents from web/Wikipedia/UpToDate with synthetic perturbations generated for the study",
            "parametric_knowledge_alignment": "mixed (contexts were engineered to be aligned or contradictory to the model's prior depending on the perturbation)",
            "performance_without_evidence": "Acc. without context (all datasets): 0.467 (fraction); per-dataset without-context accuracies listed in Table 5 (e.g., Drugs 0.578, News 0.088). Mean prior probability across all datasets: 0.675 (Table 5).",
            "performance_with_evidence": "Acc. with correct single-document context (k=1, all datasets): 0.941 (fraction); per-dataset with-correct-context values in Table 5 (e.g., Drugs 0.863, News 0.971). Context bias (probability of following incorrect context when prior was correct): 0.304; Prior bias (following prior when context was correct): 0.021 (Table 4).",
            "evidence_effect": "mixed — correct evidence strongly increases accuracy (e.g., from 0.467 to 0.941 overall), while incorrect contextual evidence often decreases accuracy because the model frequently adopts the incorrect context (context bias ≈ 30.4%); overall adding evidence helps when evidence is correct but can harm when evidence is incorrect.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Models tend to 'defer' to contextual evidence when their internal token-confidence for the prior answer is low (selective deferral); they also show prompt-sensitive sycophancy (wording influences literal adherence), and token-probability outputs are differently distributed for prior vs. context responses (uncalibrated).",
            "key_findings": "GPT-4o strongly benefits from correct retrieved documents (large increase in accuracy), but exhibits substantial context bias: it frequently adopts incorrect retrieved facts (e.g., when prior is correct, it chooses the incorrect context answer ≈58.2% in some breakdowns and context bias ≈30.4% in aggregate). The likelihood of adopting context decreases with (1) larger deviation of the context from the prior (negative correlation) and (2) higher model token-probability/confidence for the prior (inverse relationship).",
            "counterintuitive_behavior": true,
            "uuid": "e194.0",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-turbo-0125 (evaluated variant)",
            "brief_description": "A widely used LLM baseline evaluated for RAG resilience; shows different balance between prior and context compared to larger models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-0125)",
            "model_size": null,
            "task_description": "Same fact-based QA with/without retrieved contextual documents and perturbed evidence.",
            "evidence_type": "retrieved documents with factual statements (including perturbed errors)",
            "evidence_source": "retrieved documents from news/Wikipedia with synthetic perturbations",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "Acc. without context (all datasets): 0.344 (Table 5 'All'); per-dataset without-context values e.g., Drugs 0.446, News 0.063. Mean prior probability across datasets: 0.573.",
            "performance_with_evidence": "Acc. with correct single-document context (k=1, all datasets): 0.879 (Table 5); context bias 0.313, prior bias 0.028 (Table 4).",
            "evidence_effect": "mixed — correct retrieved evidence substantially increases accuracy; incorrect retrieved evidence often misleads the model (context bias ≈31.3%).",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Selective deferral based on token-probability confidence of the prior; uncalibrated token probabilities cause context responses to exhibit different probability distributions than priors.",
            "key_findings": "GPT-3.5 tends to be susceptible to incorrect context similar to other models; the paper found an inverse relationship between prior token probability and chance of adopting context (i.e., the less confident GPT-3.5 is in its prior, the more likely it is to follow context).",
            "counterintuitive_behavior": true,
            "uuid": "e194.1",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Llama-3",
            "name_full": "Llama-3-7B-Instruct (evaluated variant)",
            "brief_description": "A smaller instruction-tuned LLM evaluated for RAG behavior, showing different failure modes (more 'neither' responses).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-7B-Instruct",
            "model_size": "7B",
            "task_description": "Fact-based QA with/without contextual retrieved documents; measures whether the model uses prior, context, or neither.",
            "evidence_type": "retrieved documents containing factual statements (and perturbed variants)",
            "evidence_source": "retrieved documents from Wikipedia/news with perturbations",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "Acc. without context (all datasets): 0.228 (Table 5 'All'); per-dataset examples: Drugs 0.317, News 0.0714. Mean prior probability across datasets: 0.732.",
            "performance_with_evidence": "Acc. with correct single-document context (k=1, all datasets): 0.805 (Table 5); context bias 0.264, prior bias 0.021 (Table 4). Llama-3 also has a higher 'neither chosen' rate (often produces answers that match neither prior nor context).",
            "evidence_effect": "mixed — correct evidence raises accuracy substantially; incorrect evidence still misleads the model but Llama-3 often produces answers that match neither prior nor context (higher 'neither' frequency), so effects are qualitatively different from larger models.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Less consistent adherence to either source; model shows higher tendency to produce answers outside both prior and context possibly due to weaker parametric memory or different generation dynamics.",
            "key_findings": "Llama-3 is less likely to strictly follow context compared to some larger models but achieves lower overall accuracy; it produces 'neither' answers more often, suggesting different integration behavior when evidence conflicts with priors.",
            "counterintuitive_behavior": true,
            "uuid": "e194.2",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Claude Opus",
            "name_full": "Claude Opus (evaluated variant)",
            "brief_description": "A Claude-family model evaluated in ClashEval; highest performing model on the benchmark with lower context bias than many competitors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude Opus",
            "model_size": null,
            "task_description": "Fact-based QA under RAG conditions with perturbed evidence to measure prior vs. context preference.",
            "evidence_type": "retrieved documents containing factual statements and perturbed variants",
            "evidence_source": "retrieved documents from multiple corpora with synthetic perturbations",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "Acc. without context (all datasets): 0.463 (Table 6 'All'); per-dataset without-context e.g., Drugs 0.566, News 0.109. Mean prior prob not tabulated in same table but reported per-dataset in Appendix.",
            "performance_with_evidence": "Acc. with correct single-document context (k=1, all datasets): 0.939 (Table 6 'All'); context bias 0.157 (much lower than many other models), prior bias 0.021, overall accuracy 0.743 (Table 4 / Appendix).",
            "evidence_effect": "positive when context is correct (substantial accuracy gains) and comparatively less harmful when context is incorrect (lower context bias); overall more robust to incorrect evidence than many models.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Has a stronger internal resistance to incorrect context (lower intercept and more negative slope in context-preference vs deviation), meaning its prior distribution over truthfulness and decision boundary favor rejecting implausible contextual deviations more than other models.",
            "key_findings": "Claude Opus achieved the best balance of accuracy, low context bias (0.157), and low prior bias (0.021) in ClashEval; it is more resistant to adopting incorrect contextual evidence and benefits greatly from correct evidence.",
            "counterintuitive_behavior": false,
            "uuid": "e194.3",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Claude Sonnet",
            "name_full": "Claude Sonnet (evaluated variant)",
            "brief_description": "A smaller Claude-family model evaluated for RAG behavior; intermediate performance between top and baseline models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude Sonnet",
            "model_size": null,
            "task_description": "FACT QA with/without retrieved contextual evidence (perturbed).",
            "evidence_type": "retrieved documents with factual statements (and perturbations)",
            "evidence_source": "retrieved documents (Wikipedia/news/UpToDate) with synthetic perturbations",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "Acc. without context (all datasets): 0.400 (per-dataset values in Appendix / Table 5); e.g., Drugs 0.534, News 0.0966.",
            "performance_with_evidence": "Acc. with correct single-document context (k=1): 0.995 on names and generally high per-dataset improvements; aggregate accuracy 0.658 (Table 4 / Appendix). Context bias 0.201, prior bias 0.025 (Table 4).",
            "evidence_effect": "mixed — substantial accuracy gains with correct context; non-negligible susceptibility to incorrect context (context bias ~20.1%).",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Selective deferral to context moderated by model-specific prior distribution; similar qualitative mechanisms to other LMs but quantitatively different sensitivity.",
            "key_findings": "Claude Sonnet benefits strongly from correct evidence but still exhibits context bias; intermediate robustness compared to Claude Opus and other models.",
            "counterintuitive_behavior": true,
            "uuid": "e194.4",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Gemini 1.5",
            "name_full": "Gemini 1.5 Flash (evaluated variant)",
            "brief_description": "A Google-family LLM evaluated for RAG behavior; shows a notable tendency to follow context in many settings with measurable context bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini 1.5 Flash",
            "model_size": null,
            "task_description": "Fact-based QA under RAG conditions with perturbed retrieved evidence.",
            "evidence_type": "retrieved documents containing factual statements and perturbed variants",
            "evidence_source": "retrieved documents (news/Wikipedia) with synthetic perturbations",
            "parametric_knowledge_alignment": "mixed",
            "performance_without_evidence": "Acc. without context (all datasets): 0.463? (per-dataset listed in Appendix — Table 5 shows per-dataset values; overall accuracy in Table 4 is 0.624). Example per-dataset without-context: Drugs 0.213, News 0.0840.",
            "performance_with_evidence": "Acc. with correct single-document context (k=1): per-dataset high improvements (e.g., Names 0.995), aggregate performance with correct context in Table 4 indicates strong gains; context bias 0.245, prior bias 0.037 (Table 4).",
            "evidence_effect": "mixed — correct evidence improves accuracy markedly; incorrect evidence leads to nontrivial error adoption (context bias ~24.5%).",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Selective deferral correlated with prior token probability and sensitivity to prompt/context phrasing; behaves similarly to other large models but quantitative sensitivity differs.",
            "key_findings": "Gemini 1.5 gains large accuracy improvements from correct context but still exhibits context bias; shows domain-specific differences in susceptibility.",
            "counterintuitive_behavior": true,
            "uuid": "e194.5",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "TokenProbCorr",
            "name_full": "Token Probability Correction (method)",
            "brief_description": "A simple post-hoc correction that compares mean token probabilities of the model's generated answers with and without context and chooses the answer with higher mean token probability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to models exposing token probabilities (GPT-4o, GPT-3.5, Llama-3)",
            "model_size": null,
            "task_description": "Method to resolve conflicts between prior and contextual answers by comparing generation token probabilities.",
            "evidence_type": "use of model-internal token-probability estimates for r(q) and r(q|c)",
            "evidence_source": "model-internal probability outputs (log-probs of tokens)",
            "parametric_knowledge_alignment": "n/a (method acts on model outputs)",
            "performance_without_evidence": "Baseline accuracies (no correction) for GPT-4o/GPT-3.5/Llama-3 were 0.615 / 0.539 / 0.500 respectively (Table 3).",
            "performance_with_evidence": "After Token Probability Correction, accuracies improved to 0.693 (GPT-4o), 0.596 (GPT-3.5), 0.556 (Llama-3) (Table 3); context bias reduced moderately, prior bias increased modestly.",
            "evidence_effect": "positive (improves overall accuracy and reduces context bias compared to baseline), but introduces some increase in prior bias.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Comparing token-probability estimates approximates which answer the model 'prefers' given each condition; choosing the higher-probability generation helps revert incorrect context overrides when the prior is intrinsically more probable.",
            "key_findings": "Simple token-probability comparison yields moderate accuracy improvements and reduces tendency to follow incorrect context, demonstrating that token-probabilities contain actionable signal about when to trust the model's prior vs context.",
            "counterintuitive_behavior": false,
            "uuid": "e194.6",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CalibratedTokenProb",
            "name_full": "Calibrated Token Probability Correction (method)",
            "brief_description": "An improved version of Token Probability Correction that compares percentile-calibrated token-probability scores (rather than raw probabilities) to correct for distributional differences between prior and context responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to models exposing token probabilities (GPT-4o, GPT-3.5, Llama-3)",
            "model_size": null,
            "task_description": "Calibration-based post-hoc correction to decide between prior and contextual answers.",
            "evidence_type": "model-internal token-probability percentiles",
            "evidence_source": "model-internal probability outputs calibrated into percentiles",
            "parametric_knowledge_alignment": "n/a (method operates on output distributions)",
            "performance_without_evidence": "Baseline accuracies (no correction) in Table 3: GPT-4o 0.615, GPT-3.5 0.539, Llama-3 0.500.",
            "performance_with_evidence": "Calibrated correction improved accuracies to 0.754 (GPT-4o), 0.701 (GPT-3.5), 0.649 (Llama-3) and substantially reduced context bias (Table 3); however prior bias increased (e.g., GPT-4o prior bias rose to 0.085).",
            "evidence_effect": "positive overall (large accuracy gains and reduced context bias), but tradeoff: increases prior bias (models revert to priors more often, sometimes incorrectly).",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Calibration compensates for distributional mismatch between raw token probabilities produced with and without context, making comparisons more meaningful and improving decision of which answer to trust.",
            "key_findings": "Calibrated percentile comparison between prior and context token-probabilities substantially improved accuracy (~14% absolute improvement reported) and reduced context bias (~20% reduction), demonstrating that probability calibration is a practical baseline for mitigating harmful adherence to incorrect context.",
            "counterintuitive_behavior": true,
            "uuid": "e194.7",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PromptSensitivity",
            "name_full": "Prompt sensitivity (Strict vs Loose prompts)",
            "brief_description": "Observed effect that small changes in prompt wording (strict vs loose instructions) significantly alter how readily LLMs follow context vs prior knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across models (examples shown for GPT-4)",
            "model_size": null,
            "task_description": "Measurement of how different RAG prompt templates change context-preference behavior.",
            "evidence_type": "instructional prompt wording that frames how to use context (literal vs. judgment-based)",
            "evidence_source": "experimental prompt templates used in ClashEval",
            "parametric_knowledge_alignment": "neutral (prompt frames how to treat evidence rather than being evidence itself)",
            "performance_without_evidence": "N/A (effect concerns behavior when context is present).",
            "performance_with_evidence": "Loose prompts (encouraging reasoned judgment) produced lower baseline context preference and steeper decreases in context preference with increasing prior probability compared to Strict prompts (which enforce literal adherence) — i.e., prompt wording changed how frequently models adopted context evidence (Figure 6).",
            "evidence_effect": "prompt wording can either increase or decrease adherence to contextual evidence (i.e., positive or negative depending on desired outcome); Loose prompts made models more likely to reject implausible incorrect context.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Sycophancy to prompt instructions and explicit framing: 'strict' wording encourages literal copying of context, 'loose' wording encourages model internal reasoning and checking.",
            "key_findings": "Prompt framing is a practical lever: stricter literal instructions increase context adherence (even to incorrect facts), while looser instructions encourage selective use of context and reduce erroneous adoption of incorrect evidence.",
            "counterintuitive_behavior": true,
            "uuid": "e194.8",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DeviationEffect",
            "name_full": "Context deviation effect (degree of context modification vs preference)",
            "brief_description": "Empirical finding that the more the contextual evidence deviates from truth (or from the model prior), the less likely the model is to adopt that context; the relationship is negatively correlated and model-dependent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across models (quantified per-model)",
            "model_size": null,
            "task_description": "Analysis of context-preference rate as a function of absolute deviation between context value and reference/prior (numerical multipliers, years offsets, categorical perturbations).",
            "evidence_type": "measured perturbation magnitude of contextual facts (numerical multipliers, year shifts, name/city perturbation levels)",
            "evidence_source": "systematic synthetic perturbations applied to retrieved documents in ClashEval",
            "parametric_knowledge_alignment": "contradictory when perturbation ≠ reference; aligned when perturbation equals reference",
            "performance_without_evidence": "N/A (relation concerns how context influence changes with deviation magnitude).",
            "performance_with_evidence": "Context preference rate decreases as deviation magnitude increases; stronger models (e.g., Claude Opus) show lower intercept and steeper negative slope — i.e., they are more resistant to incorrect context for the same deviation than weaker models (Figure 3 / Section 4.3).",
            "evidence_effect": "negative for incorrect evidence but magnitude-dependent: small/subtle perturbations are more likely to be adopted (harmful), large/comical perturbations are less likely to be adopted (protective).",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Models appear to integrate a plausibility prior over context; larger deviations from expected truth reduce the model's propensity to accept context—this differs per model, reflecting different prior distributions over truthfulness.",
            "key_findings": "There is a clear inverse relationship between degree of context modification and context preference rate; subtle errors are especially dangerous because models are more likely to accept them, whereas blatant errors are more often rejected.",
            "counterintuitive_behavior": false,
            "uuid": "e194.9",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PriorProbEffect",
            "name_full": "Prior token-probability effect (confidence vs context preference)",
            "brief_description": "Empirical relationship showing that higher token-probability (confidence) of a model's prior answer strongly reduces the probability the model will adopt contextual evidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across models (where token probabilities available)",
            "model_size": null,
            "task_description": "Examining how the model's token-level probability/confidence of r(q) (answer without context) predicts the model's tendency to follow retrieved context r(q|c).",
            "evidence_type": "model-generated token-probabilities (logprobs) for prior responses",
            "evidence_source": "internal model outputs measured in experiments",
            "parametric_knowledge_alignment": "mixed (probability is a signal about alignment of prior to truth)",
            "performance_without_evidence": "Mean prior probabilities per model/dataset reported (e.g., GPT-4o mean prior prob All: 0.675; per-dataset values in Table 5).",
            "performance_with_evidence": "Context preference rate falls as prior token probability increases; slopes ranged between approx. -0.1 and -0.45 across datasets/models — e.g., a slope of -0.45 interpreted as ~4.5% drop in context-adoption likelihood for every 10% increase in prior probability (Figure 4 / Section 4.4).",
            "evidence_effect": "adding context is more likely to change model output when the model's prior probability is low (positive effect if context correct) and less likely when prior probability is high (protective against bad context).",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Token-probabilities reflect model internal confidence; models use this internal signal implicitly to weigh external evidence (more deferral when internal confidence is low).",
            "key_findings": "There is a consistent inverse relationship across datasets and models between prior token-probability and context-preference; this provides an actionable signal for interventions (e.g., probability-based corrections) and explains why models sometimes correctly defer to evidence or stubbornly keep incorrect priors.",
            "counterintuitive_behavior": false,
            "uuid": "e194.10",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MultiDocEffect",
            "name_full": "Multi-document RAG effect",
            "brief_description": "Finding that adding multiple retrieved documents (k&gt;1) often lowers overall accuracy and increases the frequency of responses that match neither prior nor any single context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed for GPT-4o and Claude Opus (and discussed generally)",
            "model_size": null,
            "task_description": "Assessing how adding 4 extra retrieved documents affects model adherence to context and final accuracy.",
            "evidence_type": "multiple retrieved documents (additional contextual sources ranked by embedding similarity)",
            "evidence_source": "retrieved documents from embeddings-based retrieval",
            "parametric_knowledge_alignment": "mixed (additional docs can be correct or incorrect)",
            "performance_without_evidence": "Single-document correct-context accuracy for GPT-4o, Claude Opus (k=1) reported in Table 6; multi-document (k=5) accuracy is lower (e.g., GPT-4o All: 0.922 vs 0.941 for k=1; Claude Opus All: 0.843 vs 0.939).",
            "performance_with_evidence": "Adding more documents (k=5) reduced accuracy for both GPT-4o and Claude Opus and raised 'neither chosen' rates; multi-document context reduced context bias but increased confusion/other errors (Tables 6 and 7).",
            "evidence_effect": "negative overall in this study (more/context length led to lower accuracy and more ambiguous outputs), though it can reduce context bias by diluting a single incorrect document.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Longer or more heterogeneous context increases retrieval noise and distractors; models may struggle to integrate multiple possibly conflicting documents and therefore produce neither-prior-nor-context answers or incorrect syntheses.",
            "key_findings": "Multi-document RAG reduced model accuracy and increased ambiguous outputs; however it also lowered context bias because models were less likely to latch onto a single incorrect document.",
            "counterintuitive_behavior": true,
            "uuid": "e194.11",
            "source_info": {
                "paper_title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Entity-based knowledge conflicts in question answering",
            "rating": 2,
            "sanitized_title": "entitybased_knowledge_conflicts_in_question_answering"
        },
        {
            "paper_title": "Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts",
            "rating": 2,
            "sanitized_title": "adaptive_chameleon_or_stubborn_sloth_unraveling_the_behavior_of_large_language_models_in_knowledge_conflicts"
        },
        {
            "paper_title": "Retrieval augmentation reduces hallucination in conversation",
            "rating": 1,
            "sanitized_title": "retrieval_augmentation_reduces_hallucination_in_conversation"
        },
        {
            "paper_title": "Certifiably robust rag against retrieval corruption",
            "rating": 2,
            "sanitized_title": "certifiably_robust_rag_against_retrieval_corruption"
        },
        {
            "paper_title": "Benchmarking large language models in Retrieval-Augmented generation",
            "rating": 1,
            "sanitized_title": "benchmarking_large_language_models_in_retrievalaugmented_generation"
        }
    ],
    "cost": 0.018675249999999997,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence</h1>
<p>Kevin Wu*<br>Department of Biomedical Data Science<br>Stanford University<br>Stanford, CA 94305<br>kevinywu@stanford.edu</p>
<p>Eric Wu*<br>Department of Electrical Engineering<br>Stanford University<br>Stanford, CA 94305<br>wue@stanford.edu<br>James Zou<br>Department of Biomedical Data Science<br>Stanford University<br>Stanford, CA 94305<br>jamesz@stanford.edu</p>
<h4>Abstract</h4>
<p>Retrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over $60 \%$ of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs - namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect. Our dataset, called ClashEval, and evaluations are open-sourced to allow for future benchmarking on top-performing models at https://github.com/kevinwu23/StanfordClashEval.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) are prone to hallucinations and incorrect answers [Pal et al., 2023, Sun et al., 2024, Ahmad et al., 2023]. Additionally, they are constrained to knowledge contained in their training corpus and are unable to answer queries about recent events or publicly restricted information. Retrieval augmented generation (RAG) is a commonly used framework that provides</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A schematic of generating modified documents for each dataset. A question is posed to the LLM with and without a reference document containing information relevant to the query. This document is then perturbed to contain modified information and given as context to the LLM. We then observe whether the LLM prefers the modified information or its own prior answer.
relevant retrieved content in the LLM prompt and can significantly improve model accuracy [Mao et al., 2020, Chen et al., 2024a, Lewis et al., 2020].</p>
<p>Most commercial LLMs, like ChatGPT [OpenAI, 2023], Gemini [Gemini Team, 2023], and Perplexity.ai, already employ RAG in their Web interfaces. For example, ChatGPT employs a Bing search, whereas Gemini accesses Google Search results. While this can greatly enhance the model's ability to answer questions, it also raises concern for when the retrieved documents or webpages contain incorrect or harmful information [Dash et al., 2023, Daws, 2020, Nastasi et al., 2023]. Indeed, examples of this behavior have already surfaced in widely deployed LLMs. For example, recent headlines showed Google's AI Summary recommending people to "eat rocks" or "put glue on their pizza" [Hart, 2024, Williams, 2024], presumably due to erroneous or satirical webpages being retrieved. While stricter document filtering or improved retrieval may help reduce this occurrence, it by no means is a cure-all against this problem. At its core, LLMs should not blindly repeat information presented in context but should be able to arbitrate when external information conflicts with its own internal knowledge. While the aforementioned example is one in which the retrieved document is the source of error, the converse is also a significant problem: when the LLM insists on its own incorrect prior answer despite correct external information.</p>
<p>Some studies have previously investigated the nature of this tension between a model's internal prior knowledge and contextual information. Longpre et al. [2021] found that LLMs exhibited a strong preference for information in the training data even when facts in the context were substituted with similar but incorrect information. More recently, Xie et al. [2023] showed that models can either be highly susceptible to context or very biased towards its priors depending on how the context is framed. Our study extends these works in two important ways. First, we present a dataset that contains examples not only when the context is wrong and the model is right but the converse (where the context is right but the model is wrong). This is important since a dataset that only measures the LLM's ability to reject wrong context can trivially excel at this task by simply always ignoring the context. Instead, our dataset uniquely tests the LLM's ability to arbitrate between its own parametric knowledge and the contextual information to determine the most accurate response. Second, we elicit a quantitative relationship between the LLM's preference of prior or context and two important variables: (1) the model's confidence in its prior response (via measuring the token probabilities of the initial response), and (2) the degree to which the contextual information provided deviates from the reference answer. Measuring these two dynamics is important for understanding how models transition between choosing the prior and the context and their inherent biases towards their priors or the context.</p>
<h1>Our contributions</h1>
<ul>
<li>We introduce ClashEval, a question-answering benchmark dataset of over 1200 questions spanning six domains that include the relevant contextual document for answering each</li>
</ul>
<p>question. The answer in each document is perturbed across a range of erroneous values, from subtle to extreme.</p>
<ul>
<li>We benchmark six top-performing LLMs (GPT-4o, GPT-3.5, Llama-3-8b-instruct, Gemini 1.5, Claude Opus, and Claude Sonnet) on this dataset and report three relevant metrics.</li>
<li>We provide a systematic analysis of context preference rates across three models on (1) varying degrees of perturbation on the contextual information and (2) the token probabilities of the prior responses.</li>
<li>We propose a simple way to improve performance on ClashEval by incorporating token probabilities.</li>
</ul>
<h1>2 Related Works</h1>
<p>The issue of hallucination in LLMs has been explored in multiple contexts and models [Ji et al., 2023, Kaddour et al., 2023]. As a response, RAG systems have been shown to reduce hallucination [Shuster et al., 2021, Kang et al., 2023]. Previous works have explored automated RAG evaluation frameworks in various settings [Es et al., 2023a, Hoshi et al., 2023, Saad-Falcon et al., 2023a, Zhang et al., 2024]. For example, some studies use LLMs to evaluate the faithfulness, answer relevance, and context relevance of RAG systems by using GPT-3.5 as an evaluator [Es et al., 2023b, Saad-Falcon et al., 2023b]. In another study, the authors propose metrics such as noise robustness, negative rejection, information integration, and counterfactual robustness [Chen et al., 2024b]. Multiple studies have shown that RAG can mislead LLMs in the presence of complex or misleading search results and that such models can still make mistakes even when given the correct response [Foulds et al., 2024, Shuster et al., 2021]. In relation to understanding model priors, other works have used log probabilities to assess the LLM's confidence in responses [Mitchell et al., 2023, Zhao et al., 2024]. However, so far there has not been a systematic exploration of a model's confidence (via logprobs) and the model's preference for RAG-provided information. Previous work has also focused on ways to address model adherence to incorrect context. For example, Longpre et al. [2021] suggests pretraining on substituted facts to improve future robustness and Xiang et al. [2024] proposes ensembling isolated answers across multiple documents. In this work, we focus on the case where LLMs are available only via inference, and only one document is being used as context.</p>
<h2>3 Methods</h2>
<h3>3.1 Definitions and Metrics</h3>
<p>Following the notation from Longpre et al. [2021], Xie et al. [2023], we start with a QA instance $x=(q, c)$ where $q$ is the query and $c$ is the context provided to answer the query. A model's prior response is $r(q)$, where the model is asked to answer the question with only its parametric knowledge. A model's contextual response is $r(q \mid c)$, where its response to the query is conditioned on the provided context.
In our study, we define the following metrics:</p>
<ul>
<li>Accuracy $=P r[r(q \mid c)$ is right $\mid c$ is right or $r(q)$ is right $]$, the probability the model responds correctly given that either the context is right or the prior is right.</li>
<li>Prior Bias $=P r[r(q \mid c)$ is wrong $\mid c$ is right and $r(q)$ is wrong $]$, the probability the model uses its prior while the context is correct.</li>
<li>Context Bias $=P r[r(q \mid c)$ is wrong $\mid c$ is wrong and $r(q)$ is right $]$, the probability the model uses the context while the prior is correct.</li>
</ul>
<p>Our main analysis consists of evaluating the RAG question-answering capabilities of six LLMs when introducing varying levels of perturbations on the RAG documents. For this study, our dataset consists of 1,294 total questions across 6 different domains. We evaluate the following models: GPT-4o, GPT3.5 (gpt-3.5-turbo-0125), Llama-3 (Llama-3-7B-Instruct), Claude Opus, Claude Sonnet, and Gemini 1.5 Flash. For our contextual responses, we use a standard prompt template that is based on RAG prompts used on popular LLM open-source libraries, with over 800k downloads as of March 2024 (LangChain and LlamaIndex). In addition to this standard prompt, we experiment with "strict" and "loose" prompts, with results in 6 . Full prompts used are provided in our GitHub repository.</p>
<h1>3.2 Dataset</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset Name</th>
<th style="text-align: center;"># Questions</th>
<th style="text-align: center;"># Perturbations</th>
<th style="text-align: left;">Example Question</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Drug Dosage</td>
<td style="text-align: center;">249</td>
<td style="text-align: center;">10</td>
<td style="text-align: left;">What is the maximum daily dosage in mg <br> for extended release oxybutynin in adults <br> with overactive bladder?</td>
</tr>
<tr>
<td style="text-align: left;">News</td>
<td style="text-align: center;">238</td>
<td style="text-align: center;">10</td>
<td style="text-align: left;">How many points did Paige Bueckers score <br> in the Big East Tournament title game on <br> March 6, 2023?</td>
</tr>
<tr>
<td style="text-align: left;">Wikipedia Dates</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">10</td>
<td style="text-align: left;">In which year was the census conducted that <br> reported the population of Lukhi village in <br> Iran as 35, in 8 families?</td>
</tr>
<tr>
<td style="text-align: left;">Sports Records</td>
<td style="text-align: center;">191</td>
<td style="text-align: center;">10</td>
<td style="text-align: left;">What is the Olympic record for Men's 100 <br> metres in athletics (time)?</td>
</tr>
<tr>
<td style="text-align: left;">Names</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Which former United States Senator, born <br> in 1955, also shares the surname with other <br> senators at the state level in Wisconsin, Min- <br> nesota, Massachusetts, Puerto Rico, and <br> New York City?</td>
</tr>
<tr>
<td style="text-align: left;">Locations</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">3</td>
<td style="text-align: left;">What is the name of the hamlet in Canada <br> that shares its name with a Scottish sur- <br> name?</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics for each dataset, including number of questions, number of perturbations applied to each question, and an example question.</p>
<p>We generate questions from six subject domains (summarized in 1. To generate a large set of question-and-answer pairs, we extract a corpus of content webpages and then query GPT-4o to generate a question based on the text, along with the ground truth answer and the excerpt used to generate the question. Additionally, we select six different datasets to cover diverse knowledge domains and difficulties. For example, news articles are included as examples of out-of-distribution questions that cannot be answered properly without context. For each dataset below, we provide the full prompts used to generate questions in our GitHub repository. Generated questions significantly transform the original data and are covered under fair use; full document content may be covered under copyright, but we provide the accompanying code to reproduce the data. As our data is sourced from the Associated Press and Wikipedia, there is no personally identifiable information or offensive content to our knowledge. UpToDate contains drug information and does not contain PHI or offensive content.</p>
<p>Drug Dosages We initially randomly sampled 500 drug information pages from UpToDate.com, a medical reference website widely used by clinicians. To constrain the scope of questions, we specify in the prompt that the answer must be numerical and in milligrams. To filter out generated questions that did not meet the specified criteria (e.g. ambiguous question, incorrect units, etc.), we perform an additional quality control step, where we ask GPT-4o to verify that the generated question fulfills all criteria. After this step, we have 249 question-answer pairs.</p>
<p>Sports Records We pulled Olympic records pages from Wikipedia.org across 9 sports: athletics, weightlifting, swimming, archery, track cycling, rowing, shooting, short-track speed skating, and speed skating. Records are extracted in a table format, from which questions are generated for each record entry. In total, after filtering, we extracted 191 unique questions and answers.</p>
<p>News Top headlines are pulled from the Associated Press RSS feed for dates ranging from 03/15/24 to 03/25/24. From an initial corpus of 1486 news articles, we use GPT-4o to generate one question per article, instructing it to produce questions for which there is a clear numerical answer. We performed another GPT-4o quality control step, which resulted in 238 unique question-answer pairs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Example <br> Question</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Response <br> w/o Context</th>
<th style="text-align: center;">Modification</th>
<th style="text-align: center;">Value <br> in document</th>
<th style="text-align: center;">Response <br> w/ Context</th>
<th style="text-align: center;">Preferred <br> Context?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Drug <br> Dosages</td>
<td style="text-align: center;">What is the maximum <br> daily dosage of olanzapine <br> for the treatment of <br> agitation/aggression <br> associated with psychiatric <br> disorders in adults?</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.1 x</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.4 x</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.5 x</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10 x</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Sports <br> Records</td>
<td style="text-align: center;">What is the Olympic <br> record for Men's 16,000 <br> metres in speed skating <br> (time)?</td>
<td style="text-align: center;">49.45</td>
<td style="text-align: center;">49.45</td>
<td style="text-align: center;">0.1 x</td>
<td style="text-align: center;">4.904</td>
<td style="text-align: center;">49.45</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.4 x</td>
<td style="text-align: center;">19.618</td>
<td style="text-align: center;">19.618</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">49.45</td>
<td style="text-align: center;">49.45</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.5 x</td>
<td style="text-align: center;">1:13.567</td>
<td style="text-align: center;">1:13.567</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10 x</td>
<td style="text-align: center;">8:10.450</td>
<td style="text-align: center;">8:10.450</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;">Dates</td>
<td style="text-align: center;">In what year did Frank <br> Thompson Jr. become the <br> chairman of the House <br> Administration <br> Committee?</td>
<td style="text-align: center;">1976</td>
<td style="text-align: center;">1975</td>
<td style="text-align: center;">-77</td>
<td style="text-align: center;">1899</td>
<td style="text-align: center;">1975</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-11</td>
<td style="text-align: center;">1965</td>
<td style="text-align: center;">1965</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">1976</td>
<td style="text-align: center;">1976</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">1987</td>
<td style="text-align: center;">1977</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2053</td>
<td style="text-align: center;">1975</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Names</td>
<td style="text-align: center;">Who did Whitney Jones <br> partner with in the doubles <br> draw at the 2007 Sunfeast <br> Open?</td>
<td style="text-align: center;">Sandy <br> Gumulya</td>
<td style="text-align: center;">Tatiana <br> Poutchek</td>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">Sandy Gumulya</td>
<td style="text-align: center;">Sandy Gumulya</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Slight</td>
<td style="text-align: center;">Sandra Gumulya</td>
<td style="text-align: center;">Sandra <br> Gumulya</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Comical</td>
<td style="text-align: center;">Sandy <br> Bubbleyumya</td>
<td style="text-align: center;">Sandy Gumulya</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Locations</td>
<td style="text-align: center;">Which city was Ivan <br> Rybovalev born in on <br> November 29, 1981?</td>
<td style="text-align: center;">Simferopol</td>
<td style="text-align: center;">Kharkiv</td>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">Simferopol</td>
<td style="text-align: center;">Simferopol</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Slight</td>
<td style="text-align: center;">Sevastopol</td>
<td style="text-align: center;">Sevastopol</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Comical</td>
<td style="text-align: center;">Simpsonsopolis</td>
<td style="text-align: center;">Simferopol</td>
<td style="text-align: center;">$\times$</td>
</tr>
</tbody>
</table>
<p>Figure 2: Examples from three datasets demonstrating differential LLM responses (GPT-4o) across various types of context modifications. Responses in red indicate wrong responses (different than the answer); responses in green indicate correct responses.</p>
<p>Dates, Names, and Cities We begin with a random sample of 1000 articles from Huggingface's Wikipedia dataset (20220301.en, [Foundation]). We use GPT-4o to generate questions related to each field (dates, names, and cities) and filter out responses where the excerpt is not exactly found in the context. To reduce ambiguity when matching groundtruth answers, we restrict the answers to fit certain formats. For dates, we require that the answer adheres to a four-digit year (YYYY). For names, we require a first and last name (eg. George Washington). For cities, we remove any other identities (eg. Seattle, not Seattle, WA). For each domain, among the remaining question-answer pairs that fit these criteria, we randomly sample 200 for our evaluation set.</p>
<h3>3.3 Modifying the Retrieved Documents</h3>
<p>We perform systematic perturbations on each question/answer pair (as visualized in Figure 1. In three datasets with numerical answers (Drug Dosages, Sports Records, Latest News), we produce ten modifications that act as multipliers on the original value: $0.1,0.2,0.4,0.8,1.2,1.5,2.0,3.0,5.0,10.0$. In the Wikipedia Years dataset, we perform ten absolute modifications in increments of 20 years for a range of $[-100,100]$. For the Wikipedia Names and Locations, the discrete categories required more hand-crafted levels of variation. For each, we performed three categorical perturbations via prompting: slight, significant, and comical. We provide the full prompts used in our study in our GitHub repository. For example, for a name like Bob Green, a slight modification implies a small tweak to another real name (Rob Greene), whereas a significant modification produces a similar but fictitious name (Bilgorn Grevalle), and a comical modification is an absurd variant (Blob Lawnface). For a city name like Miami, a slight modification changes the name of the most similar city (Fort Lauderdale), a significant modification produces a fictitious city name (Marisole), and a comical modification produces an absurd variant (Miameme). Because of differences in how each modified fact might appear in the retrieved text, we utilize GPT-4o to generate the perturbed excerpts for</p>
<p>drug dosages and news. Each modified fact is replaced in the original retrieved text. Then, both the question and context are posed to GPT-4, from which the answers, along with the log probabilities of the output tokens, are collected.</p>
<h1>4 Results</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Chosen</th>
<th style="text-align: center;">Prior Correct</th>
<th style="text-align: center;">Context Correct</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Claude Opus</td>
<td style="text-align: center;">Prior</td>
<td style="text-align: center;">$0.585(0.550,0.619)$</td>
<td style="text-align: center;">$0.042(0.027,0.058)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Context</td>
<td style="text-align: center;">$0.313(0.282,0.346)$</td>
<td style="text-align: center;">$0.901(0.879,0.923)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Neither</td>
<td style="text-align: center;">$0.102(0.082,0.125)$</td>
<td style="text-align: center;">$0.057(0.040,0.075)$</td>
</tr>
<tr>
<td style="text-align: center;">Claude Sonnet</td>
<td style="text-align: center;">Prior</td>
<td style="text-align: center;">$0.436(0.403,0.469)$</td>
<td style="text-align: center;">$0.051(0.037,0.067)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Context</td>
<td style="text-align: center;">$0.401(0.374,0.434)$</td>
<td style="text-align: center;">$0.881(0.859,0.903)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Neither</td>
<td style="text-align: center;">$0.163(0.138,0.186)$</td>
<td style="text-align: center;">$0.068(0.052,0.086)$</td>
</tr>
<tr>
<td style="text-align: center;">Gemini 1.5</td>
<td style="text-align: center;">Prior</td>
<td style="text-align: center;">$0.388(0.362,0.416)$</td>
<td style="text-align: center;">$0.074(0.058,0.091)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Context</td>
<td style="text-align: center;">$0.490(0.461,0.521)$</td>
<td style="text-align: center;">$0.860(0.838,0.881)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Neither</td>
<td style="text-align: center;">$0.122(0.103,0.143)$</td>
<td style="text-align: center;">$0.066(0.051,0.082)$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">Prior</td>
<td style="text-align: center;">$0.327(0.293,0.358)$</td>
<td style="text-align: center;">$0.041(0.027,0.056)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Context</td>
<td style="text-align: center;">$0.608(0.571,0.643)$</td>
<td style="text-align: center;">$0.903(0.881,0.923)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Neither</td>
<td style="text-align: center;">$0.065(0.047,0.083)$</td>
<td style="text-align: center;">$0.056(0.040,0.072)$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">Prior</td>
<td style="text-align: center;">$0.237(0.213,0.263)$</td>
<td style="text-align: center;">$0.057(0.043,0.072)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Context</td>
<td style="text-align: center;">$0.626(0.598,0.657)$</td>
<td style="text-align: center;">$0.841(0.817,0.865)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Neither</td>
<td style="text-align: center;">$0.137(0.113,0.160)$</td>
<td style="text-align: center;">$0.102(0.082,0.123)$</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3</td>
<td style="text-align: center;">Prior</td>
<td style="text-align: center;">$0.208(0.185,0.230)$</td>
<td style="text-align: center;">$0.041(0.029,0.054)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Context</td>
<td style="text-align: center;">$0.529(0.499,0.558)$</td>
<td style="text-align: center;">$0.793(0.767,0.818)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Neither</td>
<td style="text-align: center;">$0.263(0.236,0.291)$</td>
<td style="text-align: center;">$0.166(0.145,0.191)$</td>
</tr>
</tbody>
</table>
<p>Table 2: We report model behavior given a subset of the data where either the prior or the context is correct. A model exhibits prior bias by choosing its prior when only the context is correct, while it exhibits context bias by choosing the context when only the prior is correct. We also report when neither the prior nor context answer is used in the model response.</p>
<h3>4.1 Prior vs. Context Conflict Resolution</h3>
<p>In Table 2, Table 4, Table 5, and Figure 5, we report the responses for each of the six models when only the prior is correct or only the context is correct. On one end, models like Llama-3 and GPT-3.5 are at near random accuracy at the task of discerning when to use the prior or context answer. On the other hand, the top performing model on all three metrics is Claude Opus, with an accuracy of $74.3 \%$, a context bias of $15.7 \%$, and a prior bias of $2.1 \%$. Interestingly, while GPT-4o is the current highest performing model on LMSYS Chatbot Area (as of June 2024), it has a higher context bias than all other models but GPT-3.5. While Llama-3 has a lower context bias than GPT-4o, it also has a lower accuracy because it has a higher rate of choosing neither the prior nor the context in its response. Examples of questions and model responses are shown in 2.</p>
<h3>4.2 Multi-document Contextual Information</h3>
<p>We further examine how model adherence to context changes when there are more than one document. We analyze responses from GPT-4o and Claude Opus by adding four additional documents for each query based on embedding cosine similarity. We find that adding more contextual documents lowers overall model accuracy and increases the rate of responses that are neither the prior nor the context (Table 6, Table 7). At the same time, due to the lower rate of adherence to context, multi-document RAG also reduces the context bias found in models. These findings are consistent with related works, where models generally perform worse on longer contexts Levy et al. [2024] but multiple documents can also protect against hallucination Xiang et al. [2024].</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: We observe an inverse relationship between the context preference rate (y-axis) and the amount of deviation from the prior (x-axis). Each plot visualizes absolute deviation from the reference information (for numerical datasets, up to two log-fold changes (along with the trendline); for "Years", the absolute number of years; for categorical datasets, a total of four modification categories) against context preference rate.</p>
<h1>4.3 Context Preference Rate vs. Degree of Context Modification</h1>
<p>We consider the degree of deviation between the model's prior response and the value contained in the retrieved context (Figure 3). After fitting a linear model over the data, we find a clear negative correlation between the degree of modification in the context to the context preference rate. Models that perform stronger on ClashEval exhibit both a lower intercept and a more negative slope, indicating higher resistance to incorrect context. For example, Claude Opus adheres to incorrect contextual information $30 \%$ less than GPT-4o for the same degrees of modification. Interestingly, these results suggest that each model has a different prior distribution over truthfulness across each domain.</p>
<h3>4.4 Context Preference Rate vs. Prior Token Probability</h3>
<p>In Figure 4, we observe a consistent negative relationship between the token probability of the model's prior answer and the associated RAG preference rate for all six QA datasets. To visualize an even distribution across probabilities, we bin the probabilities into ten equidistant bins in the range of $[0.0,1.0]$. The slope indicates the effect of stronger model confidence on the model's preference for the information presented in the retrieved context; we observe different slopes (ranging from -0.1 to -0.45 ), suggesting that the effectiveness of RAG in different QA domains can be characterized as being relatively susceptible (e.g., with Dates questions) or robust (e.g., with News questions) to the model's internal prior knowledge confidence. Specifically, a slope of -0.45 , for instance, can be interpreted as expecting a $4.5 \%$ decrease in the likelihood of the LLM preferring the contextual information for every $10 \%$ increase in the probability of the model's prior response.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: We additionally observe an inverse relationship between the context preference rate (yaxis) and the model's prior response probability (x-axis). Context preference rate is defined as the proportion of responses that align with the information presented in the prompt as context. The model's prior response probability is computed from the average log probability of the response tokens queried without context. Each plot visualizes the prior probability (grouped into 10 bins) against the context preference rate, along with the best-fit trend line and slope. Models that allow access to token probabilities are shown.</p>
<h1>4.4.1 Initial Methods for Improving Prior vs. Context Conflict Resolution</h1>
<p>Based on our observations from the relationship between the token probabilities and the rates of preference for context, we posit that comparing token probabilities between $r(q)$ and $r(q \mid c)$ can improve the abilities of models to resolve conflicts. In Table 3, Token Probability Correction is done by comparing the mean token probabilities of the model's response with and without context. If the probability is higher for the prior than the contextual response, then we use the model's generation without context as its final response. Otherwise, we just use the response with context. We find that this method improves the overall accuracy of all three models with a moderate increase in the prior bias of each model. Next, we observe that the probability distributions between prior responses and context-given responses are uncalibrated, where context-given response probabilities are extremely right-tailed while prior probabilities are nearly uniform. As a simple adjustment, we compare the percentiles rather than raw probability scores of each score, or the Calibrated Token Probability Correction. We find that calibrated token probability correction improves all models' overall accuracy by $14 \%$ and context bias by $20 \%$. At the same time, this introduces more prior bias, from $2 \%$ to $8.5 \%$. However, this method outperforms a baseline of randomly replacing the final response with its prior - at the same bias rate of $8.5 \%$, the random baseline has an accuracy of $57.5 \%$ as compared to the $75.4 \%$ from the method. While this paper focuses on developing the ClashEval benchmark, these results suggest that probability calibration is a promising approach to reduce prior and context bias deserving further investigation. It also is a natural baseline for future methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Correction</th>
<th style="text-align: center;">Accuracy $\uparrow$</th>
<th style="text-align: center;">Context Bias $\downarrow$</th>
<th style="text-align: center;">Prior Bias $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-40</td>
<td style="text-align: center;">No correction (Baseline)</td>
<td style="text-align: center;">$0.615(0.595,0.636)$</td>
<td style="text-align: center;">$0.304(0.287,0.321)$</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 1 ( 0 . 0 1 4 , 0 . 0 2 8 )}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Token Probability Correction</td>
<td style="text-align: center;">$0.693(0.672,0.714)$</td>
<td style="text-align: center;">$0.194(0.177,0.210)$</td>
<td style="text-align: center;">$0.043(0.032,0.053)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Calibrated Token Prob. Correction</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 4 ( 0 . 7 3 3 , 0 . 7 7 5 )}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 0 7 ( 0 . 0 9 3 , 0 . 1 2 2 )}$</td>
<td style="text-align: center;">$0.085(0.072,0.098)$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">No correction (Baseline)</td>
<td style="text-align: center;">$0.539(0.521,0.557)$</td>
<td style="text-align: center;">$0.313(0.298,0.328)$</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 8 ( 0 . 0 2 1 , 0 . 0 3 6 )}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Token Probability Correction</td>
<td style="text-align: center;">$0.596(0.575,0.616)$</td>
<td style="text-align: center;">$0.253(0.237,0.269)$</td>
<td style="text-align: center;">$0.056(0.046,0.067)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Calibrated Token Prob. Correction</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 1 ( 0 . 6 7 8 , 0 . 7 2 2 )}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 0 ( 0 . 0 9 8 , 0 . 1 2 4 )}$</td>
<td style="text-align: center;">$0.147(0.132,0.164)$</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3</td>
<td style="text-align: center;">No correction (Baseline)</td>
<td style="text-align: center;">$0.500(0.483,0.515)$</td>
<td style="text-align: center;">$0.264(0.250,0.279)$</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 1 ( 0 . 0 1 5 , 0 . 0 2 7 )}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Token Probability Correction</td>
<td style="text-align: center;">$0.556(0.537,0.574)$</td>
<td style="text-align: center;">$0.235(0.220,0.249)$</td>
<td style="text-align: center;">$0.046(0.037,0.055)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Calibrated Token Prob. Correction</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 9 ( 0 . 6 2 7 , 0 . 6 6 9 )}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 1 ( 0 . 0 9 9 , 0 . 1 2 2 )}$</td>
<td style="text-align: center;">$0.188(0.173,0.204)$</td>
</tr>
</tbody>
</table>
<p>Table 3: For models which provide token probabilities, we evaluate the accuracy, context bias, and prior bias under three conditions: (1) No correction, which is the baseline result from this paper, (2) the token probability correction, and (3) the calibrated token probability correction.</p>
<h1>5 Discussion</h1>
<p>The ClashEval benchmark dataset and evaluations provide novel insights into how LLMs arbitrate between their own internal knowledge and contextual information when the two are in conflict.
A key finding is that even the most advanced LLMs like GPT-40 exhibit a strong context bias, overriding their own correct prior knowledge over $60 \%$ of the time when presented with incorrect information in the retrieved documents. However, this bias is not absolute - the degree to which the retrieved content deviates from truth negatively correlates with the context preference rate. Interestingly, each LLM exhibits a different prior distribution over truthfulness across domains, such that the same perturbation level affects each model differently. For instance, for a given magnitude of deviation, Claude Opus adheres to incorrect contextual information 30\% less often than GPT-40. While GPT-40 achieves state-of-the-art results on general-purpose tasks, it exhibits higher context bias compared to smaller models like Claude Sonnet. This finding suggests that performance on knowledge-based benchmarks may not automatically mean it is most suitable for RAG settings. Additionally, we find that LLMs are calibrated to selectively defer to external evidence when they are less certain about a given query. However, each model differs in how well-calibrated they are. While strong priors are not inherently problematic, the lack of explicit expectations around how models will decide to use contextual information remains a risk. We propose a simple method for improving models under ClashEval, and hope that future work can improve upon this baseline.
Our analyses have several key limitations. First, RAG systems can be deployed to many more domains than can be covered by our analyses. Second, to make our experiments tractable, our question-generation process is strictly fact-based and does not require multi-step logic, document synthesis, or other higher-level reasoning. Third, our dataset contains an enriched rate of contextual errors, so the reported metrics are not meant to represent bias rates in the wild. Fourth, our proposed token probability method only applies to models which provide probability outputs. Finally, even though this dataset is intended to improve an LLM's ability to provide users with accurate information, bad actors could use such information to exploit the shortcomings of certain models described in this paper.
As retrieval-augmented AI systems become increasingly prevalent, we hope our dataset and insights spur further research into improving the robustness and calibration of such models. Resolving the tension between parametric priors and retrieved information is a crucial challenge on the path to safe and trustworthy language models.</p>
<h2>Acknowledgements</h2>
<p>The authors acknowledge the use of UpToDate ${ }^{\circledR}$ and/or any of its content is subject to licensing agreements and is only permitted as set forth in those agreements or otherwise expressly authorized by Wolters Kluwer and UpToDate, Inc. in writing.</p>
<h1>References</h1>
<p>Muhammad Aurangzeb Ahmad, Ilker Yaramis, and Taposh Dutta Roy. Creating trustworthy LLMs: Dealing with hallucinations in healthcare AI. September 2023.</p>
<p>Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in Retrieval-Augmented generation. AAAI, 38(16):17754-17762, March 2024a.</p>
<p>Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17754-17762, 2024b.</p>
<p>Debadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H Chen, Saurabh Gombar, Lance Downing, Rachel Pedreira, Ethan Goh, Angel Arnaout, Garret Kenn Morris, Honor Magon, Matthew P Lungren, Eric Horvitz, and Nigam H Shah. Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. April 2023.</p>
<p>Ryan Daws. Medical chatbot using OpenAI's GPT-3 told a fake patient to kill themselves. https://www.artificialintelligence-news.com/2020/10/28/ medical-chatbot-openai-gpt3-patient-kill-themselves/, October 2020. Accessed: 2024-1-19.</p>
<p>Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. RAGAS: Automated evaluation of retrieval augmented generation. September 2023a.</p>
<p>Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023b.</p>
<p>Philip Feldman Foulds, R James, and Shimei Pan. Ragged edges: The double-edged sword of retrieval-augmented chatbots. arXiv preprint arXiv:2403.01193, 2024.</p>
<p>Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org.
Gemini Team. Gemini: A family of highly capable multimodal models. December 2023.
Robert Hart. Google restricts ai search tool after "nonsensical" answers told people to eat rocks and put glue on pizza, May 2024. URL https://www.forbes.com/sites/roberthart/2024/05/31/ google-restricts-ai-search-tool-after-nonsensical-answers-told-people-to-eat-rocks-and-put-glue?sh=64183b617f61.</p>
<p>Yasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii, and Jun Deguchi. RaLLe: A framework for developing and evaluating Retrieval-Augmented large language models. August 2023.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.</p>
<p>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023.</p>
<p>Haoqiang Kang, Juntong Ni, and Huaxiu Yao. Ever: Mitigating hallucination in large language models through real-time verification and rectification. arXiv preprint arXiv:2311.09114, 2023.</p>
<p>Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the impact of input length on the reasoning performance of large language models. arXiv preprint arXiv:2402.14848, 2024.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktüschel, and Others. Retrieval-augmented generation for knowledge-intensive nlp tasks. Adv. Neural Inf. Process. Syst., 33:9459-9474, 2020.</p>
<p>Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. arXiv preprint arXiv:2109.05052, 2021.</p>
<p>Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. Generation-Augmented retrieval for open-domain question answering. September 2020.</p>
<p>E Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. DetectGPT: Zero-shot machine-generated text detection using probability curvature. ICML, pages 24950-24962, January 2023.</p>
<p>Anthony J Nastasi, Katherine R Courtright, Scott D Halpern, and Gary E Weissman. Does ChatGPT provide appropriate and equitable medical advice?: A vignette-based, clinical evaluation across care contexts. March 2023.</p>
<p>OpenAI. GPT-4 technical report. March 2023.
Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Med-HALT: Medical domain hallucination test for large language models. July 2023.</p>
<p>Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. ARES: An automated evaluation framework for Retrieval-Augmented generation systems. November 2023a.</p>
<p>Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Ares: An automated evaluation framework for retrieval-augmented generation systems. arXiv preprint arXiv:2311.09476, 2023b.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.</p>
<p>Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. TrustLLM: Trustworthiness in large language models. January 2024.</p>
<p>Rhiannon Williams. Why google's AI overviews gets things wrong. MIT Technology Review, May 2024.</p>
<p>Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556, 2024.</p>
<p>Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts. arXiv preprint arXiv:2305.13300, 2023.</p>
<p>Zihan Zhang, Meng Fang, and Ling Chen. RetrievalQA: Assessing adaptive Retrieval-Augmented generation for short-form Open-Domain question answering. February 2024.</p>
<p>Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, and Stephen Gould. The first to know: How token distributions reveal hidden knowledge in large Vision-Language models? March 2024 .</p>
<h1>A Appendix</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Context Bias $\downarrow$</th>
<th style="text-align: left;">Prior Bias $\downarrow$</th>
<th style="text-align: left;">Accuracy $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Claude Opus</td>
<td style="text-align: left;">$\mathbf{0 . 1 5 7}(0.141,0.174)$</td>
<td style="text-align: left;">$\mathbf{0 . 0 2 1}(0.014,0.029)$</td>
<td style="text-align: left;">$\mathbf{0 . 7 4 3}(0.723,0.763)$</td>
</tr>
<tr>
<td style="text-align: left;">Claude Sonnet</td>
<td style="text-align: left;">$0.201(0.184,0.215)$</td>
<td style="text-align: left;">$0.025(0.018,0.033)$</td>
<td style="text-align: left;">$0.658(0.641,0.678)$</td>
</tr>
<tr>
<td style="text-align: left;">Gemini 1.5</td>
<td style="text-align: left;">$0.245(0.231,0.260)$</td>
<td style="text-align: left;">$0.037(0.029,0.046)$</td>
<td style="text-align: left;">$0.624(0.607,0.641)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: left;">$0.304(0.287,0.321)$</td>
<td style="text-align: left;">$0.021(0.013,0.028)$</td>
<td style="text-align: left;">$0.615(0.594,0.633)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">$0.313(0.298,0.329)$</td>
<td style="text-align: left;">$0.028(0.021,0.036)$</td>
<td style="text-align: left;">$0.539(0.522,0.558)$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3</td>
<td style="text-align: left;">$0.264(0.250,0.280)$</td>
<td style="text-align: left;">$0.021(0.015,0.027)$</td>
<td style="text-align: left;">$0.500(0.482,0.518)$</td>
</tr>
</tbody>
</table>
<p>Table 4: We compare six top-performing models across three metrics. Context bias is when the model chooses the context answer when its prior was correct. Prior bias is when the model chooses its prior when the context answer is correct. Finally, accuracy is a straightforward measure of the fraction of times it can produce the correct answer. We find that Claude Opus performs the best across all metrics with a context bias rate of 0.157 .
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: We plot the data from Table 4 - each model's performance across three metrics in different colors, along with $95 \%$ confidence intervals.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Effect of different prompts using GPT-4 on context preference rate vs prior probability. The "Strict" prompt strongly enforces literal adherence to the retrieved context, while the "Loose" prompt encourages the model to make a reasonable judgment in light of the provided context. We observe lower and steeper drops in context preference with the loose vs strict prompts, suggesting that prompt wording plays a significant factor in controlling context preference. Full prompts are provided in our GitHub repository.</p>
<table>
<thead>
<tr>
<th>Claude Opus</th>
<th>Acc. Without Context</th>
<th>Acc. With Correct Context</th>
</tr>
</thead>
<tbody>
<tr>
<td>Drugs</td>
<td>0.566</td>
<td>0.827</td>
</tr>
<tr>
<td>Locations</td>
<td>0.550</td>
<td>0.935</td>
</tr>
<tr>
<td>Names</td>
<td>0.400</td>
<td>0.995</td>
</tr>
<tr>
<td>News</td>
<td>0.109</td>
<td>0.966</td>
</tr>
<tr>
<td>Records</td>
<td>0.717</td>
<td>0.953</td>
</tr>
<tr>
<td>Years</td>
<td>0.490</td>
<td>0.980</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Claude Sonnet</th>
<th>Acc. Without Context</th>
<th>Acc. With Correct Context</th>
</tr>
</thead>
<tbody>
<tr>
<td>Drugs</td>
<td>0.534</td>
<td>0.775</td>
</tr>
<tr>
<td>Locations</td>
<td>0.405</td>
<td>0.930</td>
</tr>
<tr>
<td>Names</td>
<td>0.285</td>
<td>0.995</td>
</tr>
<tr>
<td>News</td>
<td>0.0966</td>
<td>0.937</td>
</tr>
<tr>
<td>Records</td>
<td>0.508</td>
<td>0.880</td>
</tr>
<tr>
<td>Years</td>
<td>0.215</td>
<td>0.980</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Gemini 1.5 Flash</th>
<th>Acc. Without Context</th>
<th>Acc. With Correct Context</th>
</tr>
</thead>
<tbody>
<tr>
<td>Drugs</td>
<td>0.213</td>
<td>0.735</td>
</tr>
<tr>
<td>Locations</td>
<td>0.325</td>
<td>0.920</td>
</tr>
<tr>
<td>Names</td>
<td>0.200</td>
<td>0.995</td>
</tr>
<tr>
<td>News</td>
<td>0.0840</td>
<td>0.958</td>
</tr>
<tr>
<td>Records</td>
<td>0.508</td>
<td>0.843</td>
</tr>
<tr>
<td>Years</td>
<td>0.205</td>
<td>0.990</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>GPT-4o</th>
<th>Acc. Without Context</th>
<th>Acc. With Correct Context</th>
<th>Mean Prior Prob</th>
</tr>
</thead>
<tbody>
<tr>
<td>Drugs</td>
<td>0.578</td>
<td>0.863</td>
<td>0.818</td>
</tr>
<tr>
<td>Locations</td>
<td>0.575</td>
<td>0.925</td>
<td>0.877</td>
</tr>
<tr>
<td>Names</td>
<td>0.445</td>
<td>0.990</td>
<td>0.847</td>
</tr>
<tr>
<td>News</td>
<td>0.0882</td>
<td>0.971</td>
<td>0.469</td>
</tr>
<tr>
<td>Records</td>
<td>0.628</td>
<td>0.921</td>
<td>0.498</td>
</tr>
<tr>
<td>Years</td>
<td>0.540</td>
<td>0.990</td>
<td>0.773</td>
</tr>
<tr>
<td>All</td>
<td>0.467</td>
<td>0.941</td>
<td>0.675</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>GPT-3.5</th>
<th>Acc. Without Context</th>
<th>Acc. With Correct Context</th>
<th>Mean Prior Prob</th>
</tr>
</thead>
<tbody>
<tr>
<td>Drugs</td>
<td>0.446</td>
<td>0.751</td>
<td>0.727</td>
</tr>
<tr>
<td>Locations</td>
<td>0.410</td>
<td>0.875</td>
<td>0.838</td>
</tr>
<tr>
<td>Names</td>
<td>0.295</td>
<td>0.985</td>
<td>0.819</td>
</tr>
<tr>
<td>News</td>
<td>0.0630</td>
<td>0.908</td>
<td>0.232</td>
</tr>
<tr>
<td>Records</td>
<td>0.592</td>
<td>0.796</td>
<td>0.578</td>
</tr>
<tr>
<td>Years</td>
<td>0.295</td>
<td>0.980</td>
<td>0.596</td>
</tr>
<tr>
<td>All</td>
<td>0.344</td>
<td>0.879</td>
<td>0.573</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Llama 3</th>
<th>Acc. Without Context</th>
<th>Acc. With Correct Context</th>
<th>Mean Prior Prob</th>
</tr>
</thead>
<tbody>
<tr>
<td>Drugs</td>
<td>0.317</td>
<td>0.598</td>
<td>0.793</td>
</tr>
<tr>
<td>Locations</td>
<td>0.290</td>
<td>0.915</td>
<td>0.853</td>
</tr>
<tr>
<td>Names</td>
<td>0.165</td>
<td>0.925</td>
<td>0.770</td>
</tr>
<tr>
<td>News</td>
<td>0.0714</td>
<td>0.912</td>
<td>0.608</td>
</tr>
<tr>
<td>Records</td>
<td>0.377</td>
<td>0.524</td>
<td>0.757</td>
</tr>
<tr>
<td>Years</td>
<td>0.160</td>
<td>0.975</td>
<td>0.720</td>
</tr>
<tr>
<td>All</td>
<td>0.228</td>
<td>0.805</td>
<td>0.732</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy and Mean Prior Prob Comparison Across Models and Datasets</p>
<table>
<thead>
<tr>
<th style="text-align: left;">GPT-40</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">Acc. Without Context</td>
<td style="text-align: center;">Acc. With Correct Context $(\mathbf{k = 1})$</td>
<td style="text-align: center;">Acc. With Correct Context $(\mathbf{k = 5})$</td>
</tr>
<tr>
<td style="text-align: left;">Drugs</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.819</td>
</tr>
<tr>
<td style="text-align: left;">Locations</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.925</td>
</tr>
<tr>
<td style="text-align: left;">Names</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.985</td>
</tr>
<tr>
<td style="text-align: left;">News</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.924</td>
</tr>
<tr>
<td style="text-align: left;">Records</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">0.911</td>
</tr>
<tr>
<td style="text-align: left;">Years</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.990</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.922</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Claude Opus</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">Acc. Without Context</td>
<td style="text-align: center;">Acc. With Correct Context $(\mathbf{k = 1})$</td>
<td style="text-align: center;">Acc. With Correct Context $(\mathbf{k = 5})$</td>
</tr>
<tr>
<td style="text-align: left;">Drugs</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.719</td>
</tr>
<tr>
<td style="text-align: left;">Locations</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.875</td>
</tr>
<tr>
<td style="text-align: left;">Names</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.880</td>
</tr>
<tr>
<td style="text-align: left;">News</td>
<td style="text-align: center;">0.109</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.853</td>
</tr>
<tr>
<td style="text-align: left;">Records</td>
<td style="text-align: center;">0.717</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.822</td>
</tr>
<tr>
<td style="text-align: left;">Years</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;">0.935</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">0.843</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracy comparison of GPT-40 and Claude Opus datasets without context, with correct context for $\mathrm{k}=1$, and with correct context for $\mathrm{k}=5$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Claude Opus, $\mathbf{k = 1}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Prior Correct</td>
<td style="text-align: center;">Context Correct</td>
</tr>
<tr>
<td style="text-align: left;">Prior Chosen</td>
<td style="text-align: center;">$0.608(0.575,0.646)$</td>
<td style="text-align: center;">$0.042(0.028,0.058)$</td>
</tr>
<tr>
<td style="text-align: left;">Context Chosen</td>
<td style="text-align: center;">$0.287(0.255,0.318)$</td>
<td style="text-align: center;">$0.901(0.878,0.923)$</td>
</tr>
<tr>
<td style="text-align: left;">Neither Chosen</td>
<td style="text-align: center;">$0.105(0.082,0.129)$</td>
<td style="text-align: center;">$0.057(0.039,0.074)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Claude Opus, $\mathbf{k = 5}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Prior Correct</td>
<td style="text-align: center;">Context Correct</td>
</tr>
<tr>
<td style="text-align: left;">Prior Chosen</td>
<td style="text-align: center;">$0.618(0.584,0.652)$</td>
<td style="text-align: center;">$0.067(0.050,0.085)$</td>
</tr>
<tr>
<td style="text-align: left;">Context Chosen</td>
<td style="text-align: center;">$0.237(0.209,0.267)$</td>
<td style="text-align: center;">$0.778(0.747,0.810)$</td>
</tr>
<tr>
<td style="text-align: left;">Neither Chosen</td>
<td style="text-align: center;">$0.145(0.121,0.172)$</td>
<td style="text-align: center;">$0.155(0.130,0.181)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-40, $\mathbf{k = 1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Prior Correct</td>
<td style="text-align: center;">Context Correct</td>
</tr>
<tr>
<td style="text-align: left;">Prior Chosen</td>
<td style="text-align: center;">$0.355(0.321,0.388)$</td>
<td style="text-align: center;">$0.041(0.027,0.057)$</td>
</tr>
<tr>
<td style="text-align: left;">Context Chosen</td>
<td style="text-align: center;">$0.582(0.549,0.617)$</td>
<td style="text-align: center;">$0.903(0.881,0.925)$</td>
</tr>
<tr>
<td style="text-align: left;">Neither Chosen</td>
<td style="text-align: center;">$0.064(0.048,0.081)$</td>
<td style="text-align: center;">$0.056(0.039,0.074)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-40, $\mathbf{k = 5}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Prior Correct</td>
<td style="text-align: center;">Context Correct</td>
</tr>
<tr>
<td style="text-align: left;">Prior Chosen</td>
<td style="text-align: center;">$0.535(0.498,0.569)$</td>
<td style="text-align: center;">$0.044(0.029,0.060)$</td>
</tr>
<tr>
<td style="text-align: left;">Context Chosen</td>
<td style="text-align: center;">$0.383(0.349,0.416)$</td>
<td style="text-align: center;">$0.868(0.843,0.894)$</td>
</tr>
<tr>
<td style="text-align: left;">Neither Chosen</td>
<td style="text-align: center;">$0.082(0.061,0.102)$</td>
<td style="text-align: center;">$0.088(0.069,0.111)$</td>
</tr>
</tbody>
</table>
<p>Table 7: Comparison of prior and context choices between Claude Opus and GPT-40 for $\mathrm{k}=1$ and $\mathrm{k}=5$ documents within the context.</p>            </div>
        </div>

    </div>
</body>
</html>
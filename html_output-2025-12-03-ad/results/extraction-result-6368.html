<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6368 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6368</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6368</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-125.html">extraction-schema-125</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language model agents that use memory to solve text‑based games, including details about the memory type, representation, update mechanism, benchmark used, performance with and without memory, training method, and any reported challenges or ablations.</div>
                <p><strong>Paper ID:</strong> paper-232352537</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/2021.naacl-main.247.pdf" target="_blank">Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including Zork I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6368.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6368.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language model agents that use memory to solve text‑based games, including details about the memory type, representation, update mechanism, benchmark used, performance with and without memory, training method, and any reported challenges or ablations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Q-learning agent for text games that encodes observations and candidate actions with separate GRU encoders and scores action candidates via an MLP on the concatenated encodings; trained with a TD loss and prioritized experience replay.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with a natural language action space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Q-network Qφ(o,a) with two GRU encoders (fo for observations, fa for actions) whose outputs are concatenated and passed to an MLP decoder g; trained by minimizing TD loss and using a softmax policy over Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent hidden state (GRU) for per-step encoding; prioritized experience replay buffer (episodic replay memory) for training</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>GRU hidden vectors for current observation and action encodings; experience replay stores tuples (o, a, r, o')</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Replay buffer appended with transitions during episodes; training samples tuples from prioritized replay and updates network weights by gradient descent on TD loss; GRU hidden states updated per step recurrently during episode rollouts</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Jericho (subset: ZORK I, ZORK III, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Human-written interactive fiction (text-adventure) games with quest-based puzzle solving and sparse rewards; used from the Jericho benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (Q-learning style TD updates) with prioritized experience replay; 8 parallel environment instances; valid action handicap used to reduce action set; trained for 100k steps per game (per run).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final score (average of final 100 episodes), maximum score seen during training, and average normalized score (raw score divided by game total score) across games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average normalized final score .21 (average normalized max .38) across the 12-game set; example per-game: ZORK I final 39.4 (max 53) as reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_noted</strong></td>
                            <td>Representations can degenerate / overfit to the TD loss (representing unseen states in a different subspace from seen states); susceptible to memorization of Q-values rather than encoding transferable semantics; performance drops on stochastic games; valid action handicap reduces language difficulty and encourages memorization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6368.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6368.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language model agents that use memory to solve text‑based games, including details about the memory type, representation, update mechanism, benchmark used, performance with and without memory, training method, and any reported challenges or ablations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HASH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hashing-based semantic ablation (fixed random encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of DRRN that breaks semantic continuity by hashing observation and action texts to integers and mapping them to fixed random Gaussian vectors (via a seeded generator), so encoders are fixed and semantics are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>HASH (DRRN with hashed, fixed encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same DRRN Q-network architecture, but the text encoders are replaced by a fixed mapping: h: string -> integer hash, and a pseudo-random generator G seeded by the hash yields a fixed Gaussian embedding; encoders are not trained.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>fixed, non-learned embeddings plus Q-value memorization via network weights and experience replay (episodic buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Fixed random Gaussian vectors for each hashed observation/action token sequence; Q-network parameters implicitly store learned Q-values for these fixed identifiers; replay buffer stores transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>No update of the hashed embeddings (they are fixed); learning occurs by updating Q-network weights with TD loss on sampled transitions from prioritized replay buffer, enabling memorization of Q-values associated with hashed identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Jericho (subset used in experiments, e.g., ZORK I, PENTARI, LIBRARY, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Human-written interactive fiction games from the Jericho benchmark; rich, complex, and diverse semantics but here texts are replaced by non-semantic identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (same DRRN training protocol) with prioritized replay buffer, 8 parallel envs, valid action handicap, 100k steps per game.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final score (average of final 100 episodes), maximum score; average normalized score across games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average normalized final score .25 (average normalized max .39) across games; per-table examples: PENTARI final 51.9 / max 60; overall HASH had lower final score than DRRN on only one game (ZORK I final 35.5 / max 50) and achieved best average normalized final score across games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_noted</strong></td>
                            <td>Although HASH breaks semantics, the agent can still perform well by memorizing Q-values tied to fixed identifiers, indicating agents may rely on memorization rather than semantic understanding; hashing scatters semantically-similar states in representation space (visualized via t-SNE).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6368.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6368.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language model agents that use memory to solve text‑based games, including details about the memory type, representation, update mechanism, benchmark used, performance with and without memory, training method, and any reported challenges or ablations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MIN-OB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minimizing Observation ablation (MIN-OB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that reduces observation texts to only a location phrase (loc(o)) to isolate and minimize semantic information from observations, leaving action text intact.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MIN-OB (DRRN with minimized observations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DRRN architecture where the observation encoder receives only the current location's phrase (loc(o)) rather than the full observation text, to reduce available semantic signal in observations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>reduced observation identifiers (location names) plus experience replay buffer and Q-network parameter memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Location phrase identifiers (short strings) encoded by the standard GRU encoder; transitions stored in the prioritized replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Replay buffer updated with transitions as usual; Q-network trained on sampled tuples via TD loss; GRU encodings updated during training.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Jericho (subset of 12 games used in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Interactive fiction (text-adventure) games; here observations are reduced to discrete location names to remove detailed semantic content.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (DRRN TD updates) with prioritized replay, 8 parallel envs, valid action handicap, 100k steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final score (average of final 100 episodes), maximum score, average normalized score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average normalized final score .12 (average normalized max .35) across games; per-table example ZORK I final 29 / max 46 (lower than base DRRN).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_noted</strong></td>
                            <td>MIN-OB explores similar maximum scores to base DRRN on most games but fails to 'memorize' good experiences and achieve high episodic scores, suggesting that detailed observation text aids in distinguishing observation instances for effective retrieval/memorization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6368.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6368.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language model agents that use memory to solve text‑based games, including details about the memory type, representation, update mechanism, benchmark used, performance with and without memory, training method, and any reported challenges or ablations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INV-DY</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Dynamics Decoder (INV-DY)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An auxiliary inverse-dynamics task added to DRRN to regularize learned representations: predict/ decode the action given current and next observation encodings, and reconstruct actions from their encodings, with an intrinsic reward for poorly-predicted inverse dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>INV-DY (DRRN + inverse dynamics auxiliary losses)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DRRN augmented with an MLP g_inv that predicts an action representation from concatenated fo(o), fo(o'); and a GRU decoder d that decodes action tokens from predicted representations; adds inverse dynamics loss L_inv and action reconstruction loss L_dec to TD loss and supplies an intrinsic exploration reward r+ = L_inv.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>learned compact observation/action representations (GRU hidden states) regularized by reconstruction; replay buffer for experience; intrinsic reward stored via RL updates</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>GRU encodings of observations and actions; predicted action representations from g_inv; actions decoded back to text by GRU decoder d; replay stores (o,a,r,o').</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Parameters (φ, θ) updated by joint optimization of TD loss L_TD and auxiliary losses L_inv and L_dec via gradient descent; intrinsic reward derived from inverse dynamics loss encourages exploration where inverse mapping is not well-learned.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Jericho (12-game subset; highlighted: ZORK I, DRAGON, OMNIQUEST)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Human-written interactive fiction games; used to evaluate whether semantics-regularized representations help exploration and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning with combined loss L = L_TD + λ1 L_inv + λ2 L_dec (λ1 = λ2 = 1); prioritized replay buffer; intrinsic reward used to guide exploration; 8 parallel envs; 100k steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final score (average of final 100 episodes), maximum score seen during training, average normalized score; also transfer score when freezing encoders and re-training Q-network on a related game (ZORK III).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average normalized final score .23 (average normalized max .40) across games; notable per-game: ZORK I final 43.1 / max 87 (max 87 seen only by INV-DY runs); INV-DY explored higher maxima on DRAGON, OMNIQUEST, ZORK I.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td>On some games (notably ZORK I) INV-DY explored higher maximum scores than other models (e.g., max 87 vs max ≤55 for others); in transfer, INV-DY encoder (frozen) achieved ~1 point on ZORK III after 10k steps, outperforming models trained from scratch (~0.4 after 100k steps).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_noted</strong></td>
                            <td>Auxiliary inverse dynamics improves representation semantics on some games but not uniformly; DRRN representations can overfit the TD objective and transfer worse than simple HASH; training with inverse dynamics adds complexity (additional decoder and loss terms) and requires balancing with TD loss; stochastic games still reduce absolute performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6368.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6368.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language model agents that use memory to solve text‑based games, including details about the memory type, representation, update mechanism, benchmark used, performance with and without memory, training method, and any reported challenges or ablations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExperienceReplay</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritized Experience Replay Buffer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An episodic replay memory storing transition tuples (o, a, r, o') used during RL training to stabilize learning; sampling prioritized by some criterion to focus learning on important transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prioritized Replay Buffer (experience replay)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A replay buffer that stores observed transitions and is sampled from during training; the paper uses a prioritized replay buffer for all DRRN variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic replay buffer (prioritized experience memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Tuples of (observation text / encoding, action, reward, next observation text / encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Transitions appended to buffer during gameplay; sampled (with prioritization) to form minibatches for TD updates; used across all model variants in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Jericho benchmark (12 games used)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Text-adventure interactive fiction games; replay buffer stores transitions collected during agent interaction with these games.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning with prioritized experience replay feeding TD updates for DRRN-based agents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Indirect: supports performance measured as final/max scores and average normalized score reported for agents; not directly evaluated as an independent metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_noted</strong></td>
                            <td>Replay memory contributes to the agent's ability to 'memorize' high-Q observation-action pairs and can facilitate overfitting to the reward structure of a specific game; the paper does not ablate removal of replay memory, so its isolated effect is not quantified here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic knowledge graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Keep CALM and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Nail: A general interactive fiction agent <em>(Rating: 1)</em></li>
                <li>How to avoid being eaten by a grue: Exploration strategies for text-adventure agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6368",
    "paper_id": "paper-232352537",
    "extraction_schema_id": "extraction-schema-125",
    "extracted_data": [
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network",
            "brief_description": "A Q-learning agent for text games that encodes observations and candidate actions with separate GRU encoders and scores action candidates via an MLP on the concatenated encodings; trained with a TD loss and prioritized experience replay.",
            "citation_title": "Deep reinforcement learning with a natural language action space",
            "mention_or_use": "use",
            "paper_title": "Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents",
            "model_name": "DRRN",
            "model_description": "Q-network Qφ(o,a) with two GRU encoders (fo for observations, fa for actions) whose outputs are concatenated and passed to an MLP decoder g; trained by minimizing TD loss and using a softmax policy over Q-values.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "recurrent hidden state (GRU) for per-step encoding; prioritized experience replay buffer (episodic replay memory) for training",
            "memory_representation": "GRU hidden vectors for current observation and action encodings; experience replay stores tuples (o, a, r, o')",
            "memory_update_mechanism": "Replay buffer appended with transitions during episodes; training samples tuples from prioritized replay and updates network weights by gradient descent on TD loss; GRU hidden states updated per step recurrently during episode rollouts",
            "text_game_name": "Jericho (subset: ZORK I, ZORK III, etc.)",
            "text_game_description": "Human-written interactive fiction (text-adventure) games with quest-based puzzle solving and sparse rewards; used from the Jericho benchmark.",
            "training_method": "Reinforcement learning (Q-learning style TD updates) with prioritized experience replay; 8 parallel environment instances; valid action handicap used to reduce action set; trained for 100k steps per game (per run).",
            "evaluation_metric": "Final score (average of final 100 episodes), maximum score seen during training, and average normalized score (raw score divided by game total score) across games.",
            "performance_with_memory": "Average normalized final score .21 (average normalized max .38) across the 12-game set; example per-game: ZORK I final 39.4 (max 53) as reported in Table 1.",
            "performance_without_memory": null,
            "performance_improvement": null,
            "ablation_study": false,
            "challenges_noted": "Representations can degenerate / overfit to the TD loss (representing unseen states in a different subspace from seen states); susceptible to memorization of Q-values rather than encoding transferable semantics; performance drops on stochastic games; valid action handicap reduces language difficulty and encourages memorization.",
            "uuid": "e6368.0"
        },
        {
            "name_short": "HASH",
            "name_full": "Hashing-based semantic ablation (fixed random encoding)",
            "brief_description": "An ablation of DRRN that breaks semantic continuity by hashing observation and action texts to integers and mapping them to fixed random Gaussian vectors (via a seeded generator), so encoders are fixed and semantics are removed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents",
            "model_name": "HASH (DRRN with hashed, fixed encodings)",
            "model_description": "Same DRRN Q-network architecture, but the text encoders are replaced by a fixed mapping: h: string -&gt; integer hash, and a pseudo-random generator G seeded by the hash yields a fixed Gaussian embedding; encoders are not trained.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "fixed, non-learned embeddings plus Q-value memorization via network weights and experience replay (episodic buffer)",
            "memory_representation": "Fixed random Gaussian vectors for each hashed observation/action token sequence; Q-network parameters implicitly store learned Q-values for these fixed identifiers; replay buffer stores transitions.",
            "memory_update_mechanism": "No update of the hashed embeddings (they are fixed); learning occurs by updating Q-network weights with TD loss on sampled transitions from prioritized replay buffer, enabling memorization of Q-values associated with hashed identifiers.",
            "text_game_name": "Jericho (subset used in experiments, e.g., ZORK I, PENTARI, LIBRARY, etc.)",
            "text_game_description": "Human-written interactive fiction games from the Jericho benchmark; rich, complex, and diverse semantics but here texts are replaced by non-semantic identifiers.",
            "training_method": "Reinforcement learning (same DRRN training protocol) with prioritized replay buffer, 8 parallel envs, valid action handicap, 100k steps per game.",
            "evaluation_metric": "Final score (average of final 100 episodes), maximum score; average normalized score across games.",
            "performance_with_memory": "Average normalized final score .25 (average normalized max .39) across games; per-table examples: PENTARI final 51.9 / max 60; overall HASH had lower final score than DRRN on only one game (ZORK I final 35.5 / max 50) and achieved best average normalized final score across games.",
            "performance_without_memory": null,
            "performance_improvement": null,
            "ablation_study": true,
            "challenges_noted": "Although HASH breaks semantics, the agent can still perform well by memorizing Q-values tied to fixed identifiers, indicating agents may rely on memorization rather than semantic understanding; hashing scatters semantically-similar states in representation space (visualized via t-SNE).",
            "uuid": "e6368.1"
        },
        {
            "name_short": "MIN-OB",
            "name_full": "Minimizing Observation ablation (MIN-OB)",
            "brief_description": "An ablation that reduces observation texts to only a location phrase (loc(o)) to isolate and minimize semantic information from observations, leaving action text intact.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents",
            "model_name": "MIN-OB (DRRN with minimized observations)",
            "model_description": "DRRN architecture where the observation encoder receives only the current location's phrase (loc(o)) rather than the full observation text, to reduce available semantic signal in observations.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "reduced observation identifiers (location names) plus experience replay buffer and Q-network parameter memory",
            "memory_representation": "Location phrase identifiers (short strings) encoded by the standard GRU encoder; transitions stored in the prioritized replay buffer.",
            "memory_update_mechanism": "Replay buffer updated with transitions as usual; Q-network trained on sampled tuples via TD loss; GRU encodings updated during training.",
            "text_game_name": "Jericho (subset of 12 games used in paper)",
            "text_game_description": "Interactive fiction (text-adventure) games; here observations are reduced to discrete location names to remove detailed semantic content.",
            "training_method": "Reinforcement learning (DRRN TD updates) with prioritized replay, 8 parallel envs, valid action handicap, 100k steps.",
            "evaluation_metric": "Final score (average of final 100 episodes), maximum score, average normalized score.",
            "performance_with_memory": "Average normalized final score .12 (average normalized max .35) across games; per-table example ZORK I final 29 / max 46 (lower than base DRRN).",
            "performance_without_memory": null,
            "performance_improvement": null,
            "ablation_study": true,
            "challenges_noted": "MIN-OB explores similar maximum scores to base DRRN on most games but fails to 'memorize' good experiences and achieve high episodic scores, suggesting that detailed observation text aids in distinguishing observation instances for effective retrieval/memorization.",
            "uuid": "e6368.2"
        },
        {
            "name_short": "INV-DY",
            "name_full": "Inverse Dynamics Decoder (INV-DY)",
            "brief_description": "An auxiliary inverse-dynamics task added to DRRN to regularize learned representations: predict/ decode the action given current and next observation encodings, and reconstruct actions from their encodings, with an intrinsic reward for poorly-predicted inverse dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents",
            "model_name": "INV-DY (DRRN + inverse dynamics auxiliary losses)",
            "model_description": "DRRN augmented with an MLP g_inv that predicts an action representation from concatenated fo(o), fo(o'); and a GRU decoder d that decodes action tokens from predicted representations; adds inverse dynamics loss L_inv and action reconstruction loss L_dec to TD loss and supplies an intrinsic exploration reward r+ = L_inv.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "learned compact observation/action representations (GRU hidden states) regularized by reconstruction; replay buffer for experience; intrinsic reward stored via RL updates",
            "memory_representation": "GRU encodings of observations and actions; predicted action representations from g_inv; actions decoded back to text by GRU decoder d; replay stores (o,a,r,o').",
            "memory_update_mechanism": "Parameters (φ, θ) updated by joint optimization of TD loss L_TD and auxiliary losses L_inv and L_dec via gradient descent; intrinsic reward derived from inverse dynamics loss encourages exploration where inverse mapping is not well-learned.",
            "text_game_name": "Jericho (12-game subset; highlighted: ZORK I, DRAGON, OMNIQUEST)",
            "text_game_description": "Human-written interactive fiction games; used to evaluate whether semantics-regularized representations help exploration and transfer.",
            "training_method": "Reinforcement learning with combined loss L = L_TD + λ1 L_inv + λ2 L_dec (λ1 = λ2 = 1); prioritized replay buffer; intrinsic reward used to guide exploration; 8 parallel envs; 100k steps.",
            "evaluation_metric": "Final score (average of final 100 episodes), maximum score seen during training, average normalized score; also transfer score when freezing encoders and re-training Q-network on a related game (ZORK III).",
            "performance_with_memory": "Average normalized final score .23 (average normalized max .40) across games; notable per-game: ZORK I final 43.1 / max 87 (max 87 seen only by INV-DY runs); INV-DY explored higher maxima on DRAGON, OMNIQUEST, ZORK I.",
            "performance_without_memory": null,
            "performance_improvement": "On some games (notably ZORK I) INV-DY explored higher maximum scores than other models (e.g., max 87 vs max ≤55 for others); in transfer, INV-DY encoder (frozen) achieved ~1 point on ZORK III after 10k steps, outperforming models trained from scratch (~0.4 after 100k steps).",
            "ablation_study": true,
            "challenges_noted": "Auxiliary inverse dynamics improves representation semantics on some games but not uniformly; DRRN representations can overfit the TD objective and transfer worse than simple HASH; training with inverse dynamics adds complexity (additional decoder and loss terms) and requires balancing with TD loss; stochastic games still reduce absolute performance.",
            "uuid": "e6368.3"
        },
        {
            "name_short": "ExperienceReplay",
            "name_full": "Prioritized Experience Replay Buffer",
            "brief_description": "An episodic replay memory storing transition tuples (o, a, r, o') used during RL training to stabilize learning; sampling prioritized by some criterion to focus learning on important transitions.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents",
            "model_name": "Prioritized Replay Buffer (experience replay)",
            "model_description": "A replay buffer that stores observed transitions and is sampled from during training; the paper uses a prioritized replay buffer for all DRRN variants.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "episodic replay buffer (prioritized experience memory)",
            "memory_representation": "Tuples of (observation text / encoding, action, reward, next observation text / encoding)",
            "memory_update_mechanism": "Transitions appended to buffer during gameplay; sampled (with prioritization) to form minibatches for TD updates; used across all model variants in experiments.",
            "text_game_name": "Jericho benchmark (12 games used)",
            "text_game_description": "Text-adventure interactive fiction games; replay buffer stores transitions collected during agent interaction with these games.",
            "training_method": "Reinforcement learning with prioritized experience replay feeding TD updates for DRRN-based agents.",
            "evaluation_metric": "Indirect: supports performance measured as final/max scores and average normalized score reported for agents; not directly evaluated as an independent metric.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "performance_improvement": null,
            "ablation_study": false,
            "challenges_noted": "Replay memory contributes to the agent's ability to 'memorize' high-Q observation-action pairs and can facilitate overfitting to the reward structure of a specific game; the paper does not ablate removal of replay memory, so its isolated effect is not quantified here.",
            "uuid": "e6368.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on text-based games",
            "rating": 2
        },
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in text-based games",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "Nail: A general interactive fiction agent",
            "rating": 1
        },
        {
            "paper_title": "How to avoid being eaten by a grue: Exploration strategies for text-adventure agents",
            "rating": 1
        }
    ],
    "cost": 0.01416275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents
June 6-11, 2021</p>
<p>Shunyu Yao shunyuy@princeton.edu 
Princeton University ‡ Microsoft Research</p>
<p>Karthik Narasimhan karthikn@princeton.edu 
Princeton University ‡ Microsoft Research</p>
<p>Matthew Hausknecht matthew.hausknecht@microsoft.com 
Princeton University ‡ Microsoft Research</p>
<p>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</p>
<p>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJune 6-11, 20213097
Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including ZORK I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.</p>
<p>Introduction</p>
<p>Text adventure games such as ZORK I (Figure 1 (a)) have been a testbed for developing autonomous agents that operate using natural language. Since interactions in these games (input observations, action commands) are through text, the ability to understand and use language is deemed necessary and critical to progress through such games. Previous work has deployed a spectrum of methods for language processing in this domain, including word vectors (Fulda et al., 2017), recurrent neural networks (Narasimhan et al., 2015;, pre-trained language models (Yao * Work partly done during internship at Microsoft Research. Project page: https://blindfolded.cs. princeton.edu. et al., 2020), open-domain question answering systems , knowledge graphs Adhikari et al., 2020), and reading comprehension systems .</p>
<p>Meanwhile, most of these models operate under the reinforcement learning (RL) framework, where the agent explores the same environment in repeated episodes, learning a value function or policy to maximize game score. From this perspective, text games are just special instances of a partially observable Markov decision process (POMDP) (S, T, A, O, R, γ), where players issue text actions a ∈ A, receive text observations o ∈ O and scalar rewards r = R(s, a), and the underlying game state s ∈ S is updated by transition s = T (s, a).</p>
<p>However, what distinguishes these games from other POMDPs is the fact that the actions and observations are in language space L. Therefore, a certain level of decipherable semantics is attached to text observations o ∈ O ⊂ L and actions a ∈ A ⊂ L. Ideally, these texts not only serve as observation or action identifiers, but also provide clues about the latent transition function T and reward function R. For example, issuing an action "jump" based on an observation "on the cliff" would likely yield a subsequent observation such as "you are killed" along with a negative reward. Human players often rely on their understanding of language semantics to inform their choices, even on games they have never played before, while replacing texts with non-semantic identifiers such as their corresponding hashes (Figure 1 (c)) would likely render games unplayable for people. However, would this type of transformation affect current RL agents for such games? In this paper, we ask the following question: To what extent do current reinforcement learning agents leverage semantics in text-based games?</p>
<p>To shed light on this question, we investi-(a) ZORK I Observation 21: You are in the living room. There is a doorway to the east, a wooden door with strange gothic lettering to the west, which appears to be nailed shut, a trophy case, and a large oriental rug in the center of the room. You are carrying: A brass lantern . . .</p>
<p>Action 21: move rug</p>
<p>Observation 22: With a great effort, the rug is moved to one side of the room, revealing the dusty cover of a closed trap door... Living room... You are carrying: ... gate the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016), a top-performing RL model that uses gated recurrent units (GRU) (Cho et al., 2014) to encode texts. We conduct three experiments on a set of interactive fiction games from the Jericho benchmark  to probe the effect of different semantic representations on the functioning of DRRN. These include (1) using just a location phrase as the input observation (Figure 1 (b)), (2) hashing text observations and actions (Figure 1 (c)), and (3) regularizing vector representations using an auxiliary inverse dynamics loss. While reducing observations to location phrases leads to decreased scores and enforcing inverse dynamics decoding leads to increased scores on some games, hashing texts to break semantics surprisingly matches or even outperforms the baseline DRRN on almost all games considered. This implies current RL agents for textbased games might not be sufficiently leveraging the semantic structure of game texts to learn good policies, and points to the need for developing better experiment setups and agents that have a finer grasp of natural language.</p>
<p>2 Models DRRN Baseline Our baseline RL agent DRRN (He et al., 2016) learns a Q-network Q φ (o, a) parametrized by φ. The model encodes the observation o and each action candidate a using two separate GRU encoders f o and f a , and then aggregates the representations to derive the Q-value through a MLP decoder g:
Q φ (o, a) = g(concat(f o (o), f a (a)))(1)
For learning φ, tuples (o, a, r, o ) of observation, action, reward and the next observation are sampled from an experience replay buffer and the following temporal difference (TD) loss is minimized:
L TD (φ) = (r + γ max a ∈A Q φ (o , a ) − Q φ (o, a)) 2
(2) During gameplay, a softmax exploration policy is used to sample an action:
π φ (a|o) = exp(Q φ (o, a)) a ∈A exp(Q φ (o, a ))(3)
Note that when the action space A is large, (2) and (3) become intractable. A valid action handicap  or a language model (Yao et al., 2020) can be used to generate a reduced action space for efficient exploration. For all the modifications below, we use the DRRN with the valid action handicap as our base model.</p>
<p>Reducing Semantics via Minimizing Observation (MIN-OB)</p>
<p>Unlike other RL domains such as video games or robotics control, at each step of text games the (valid) action space is constantly changing, and it reveals useful information about the current state. For example, knowing "unlock box" is valid leaks the existence of a locked box. Also, sometimes action semantics indicate its value even unconditional on the state, e.g. "pick gold" usually seems good. Given these, we minimize the observation to only a location phrase o → loc(o) (Figure 1 (b)) to isolate the action semantics: given a hash function from strings to integers h : L → Z, and a pseudo-random generator G : Z → R d that turns an integer seed to a random Gaussian vector, a hashing encoder (1) are trainable,f is fixed throughout RL, and ensures two texts that only differ by a word would have completely different representations. In this sense, hashing breaks semantics and only serves to identify different observations and actions in an abstract MDP problem (Figure 1 (c)):
Q loc φ (o, a) = g(f o (loc(o)), f a (a))f = G • h : L → R d can be composed. While f o and f a inQ hash φ (o, a) = g(f (o),f (a))
.</p>
<p>Regularizing Semantics via Inverse Dynamics</p>
<p>Decoding (INV-DY) The GRU representations in DRRN f o (o), f a (a) are only optimized for the TD loss (2). As a result, text semantics can degenerate during encoding, and the text representation might arbitrarily overfit to the Q-values. To regularize and encourage more game-related semantics to be encoded, we take inspiration from Pathak et al. (2017) and propose an inverse dynamics auxiliary task during RL. Given representations of current and next observations f o (o), f o (o ), we use a MLP g inv to predict the action representation, and a GRU decoder d to decode the action back to text * . The inverse dynamics loss is defined as
L inv (φ, θ) = − log p d (a|g inv (concat(f o (o), f o (o )))
where θ denote weights of g inv and d, and p d (a|x) is the probability of decoding token sequence a using GRU decoder d with initial hidden state as x. To also regularize the action encoding, action reconstruction from f a is also used as a loss term:</p>
<p>L dec (φ, θ) = − log p d (a|f a (a)) * Directly defining an L1/L2 loss between fa(a) and ginv(concat(fo(o), fo(o ))) in the representation space will collapse text representations together. And during experience replay, these two losses are optimized along with the TD loss:
L(φ, θ) = L TD (φ) + λ 1 L inv (φ, θ) + λ 2 L dec (φ, θ)
An intrinsic reward r + = L inv (φ, θ) is also used to explore toward where the inverse dynamics is not learned well yet. All in all, the aim of INV-DY is threefold: (1) regularize both action and observation representations to avoid degeneration by decoding back to the textual domain, (2) encourage f o to encode action-relevant parts of observations, and (3) provide intrinsic motivation for exploration.</p>
<p>Results</p>
<p>Setup We train on 12 games † from the Jericho benchmark . These human-written interactive fictions are rich, complex, and diverse in semantics. ‡ For each game, we train DRRN asynchronously on 8 parallel instances of the game environment for 10 5 steps, using a prioritized replay buffer. Following prior practice , we augment observations with location and inventory descriptions by issuing the 'look' and 'inventory' commands. We train three independent runs for each game and report their average score. For HASH, we use the Python built-in hash function to process text as a tuple of token IDs, and implement the random vector generator G by seeding PyTorch with the hash value. For INV-DY, we use λ 1 = λ 2 = 1.</p>
<p>Scores Table 1 reports the final score (the average score of the final 100 episodes during training), and the maximum score seen in each game for different models. Average normalized score (raw score divided by game total score) over all games is also reported. Compared to the base DRRN, MIN-OB turns out to explore similar maximum scores on † We omit games where DRRN cannot score. ‡ Please refer to  for more details about these games. most games (except DEEPHOME and DRAGON), but fails to memorize the good experience and reach high episodic scores, which suggests the importance of identifying different observations using language details. Most surprisingly, HASH has a lower final score than DRRN on only one game (ZORK I), while on PENTARI it almost doubles the DRRN final score. It is also the model with the best average normalized final score across games, which indicates that the DRRN model can perform as well without leveraging any language semantics, but instead simply by identifying different observations and actions with random vectors and memorizing the Q-values. Lastly, we observe on some games (DRAGON, OMNIQUEST, ZORK I) INV-DY can explore high scores that other models cannot. Most notably, on ZORK I the maximum score seen is 87 (average of 54, 94, 113), while any run of other models does not explore a score more than 55. This might indicate potential benefit of developing RL agents with more semantic representations.</p>
<p>Transfer We also investigate if representations of different models can transfer to a new language environment, which is a potential benefit of learning natural language semantics. So we consider the two most similar games in Jericho, ZORK I and ZORK III, fix the language encoders of different ZORK I models, and re-train the Q-network on ZORK III for 10,000 steps. As shown in Figure 2, INV-DY representations can achieve a score around 1, which surpasses the best result of models trained from scratch on ZORK III for 100,000 steps (around 0.4), showing great promise in better gameplay by leveraging language understanding from other games. HASH transfer is equivalent to training from scratch as the representations are not learnt, and a score around 0.3 is achieved. Finally, DRRN representations transfer worse than HASH, possibly due to overfitting to the TD loss (2).</p>
<p>Visualizations Finally, we use t-SNE (Maaten and Hinton, 2008) to visualize representations of some ZORK I walkthrough states in Figure 3. The first 30 walkthrough states (red, score 0-45) are well experienced by the models during exploration, whereas the last 170 states (blue, score 157-350) are unseen § . We also encircle the subset of states at location 'living room' for their shared semantics.</p>
<p>First, we note that the HASH representations for living room states are scattered randomly, unlike the other two models with GRU language encoders. Further, the base DRRN overfits to the TD loss (2), representing unseen states (blue) in a different subspace to seen states (red) without regarding their semantic similarity. IND-DY is able to extrapolate to unseen states and represent them similarly to seen states for their shared semantics, which may explain its better performance on this game.</p>
<p>Game stochasticity All the above experiments were performed using a fixed game random seed for each game, following prior work . To investigate if randomness in games affects our conclusions, we run one trial of each game with episode-varying random seeds ¶ . We find the average normalized score for the base DRRN, HASH, INV-DY to be all around 17%, with performance drop mainly on three stochastic games (DRAGON, ZORK I, ZORK III). Notably, the core finding that the base DRRN and HASH perform similarly still holds. Intuitively, even though the Q-values would be lower overall with unexpected transitions, RL would still memorize observations and actions that lead to high Q-values.</p>
<p>Discussion</p>
<p>At a high level, RL agents for text-based games succeed by (1) exploring trajectories that lead to high scores, and (2) learning representations to stably reach high scores. Our experiments show that a semantics-regularized INV-DY model manages to explore higher scores on some games (DRAGON, OMNIQUEST, ZORK I), while the HASH model manages to memorize scores better on other games (LIBRARY, LUDICORP, PENTARI) using just a fixed, random, non-semantic representation. This leads us to hypothesize two things. First, fixed, stable representations might make Q-learning easier. Second, it might be desirable to represent similar texts very differently for better gameplay, e.g. the Q-value can be much higher when a key object is mentioned, even if it only adds a few words to a long observation text. This motivates future thought into the structural vs. functional use of language semantics in these games.</p>
<p>Our findings also urge a re-thinking of the popular 'RL + valid action handicap' setup for these games. On one hand, RL sets training and evaluation in the same environment, with limited text corpora, and sparse, mostly deterministic rewards as the only optimization objective. Such a combination easily results in overfitting to the reward system of a specific game (Figure 2), or even just a specific stage of the game (Figure 3). On the other hand, the valid action handicap reduces the action set to a small size tractable for memorization, and reduces the language understanding challenge for the RL agent. Thus for future research on text-based games, we advocate for more attention towards alternative setups without RL or handicaps (Hausknecht et al., 2019;Yao et al., 2020;. Particularly, in a 'RL + no valid action handicap' setting, generating action candidates rather than simply choosing from a set entails more opportunities and challenges with respect to learning grounded language semantics (Yao et al., 2020). Additionally, training agents on a distribution of games and evaluating them on a separate set of unseen games would require more general semantic understanding. Semantic evaluation of these proposed paradigms is outside the scope of this paper, but we hope it will spark a productive discussion on the next steps toward building agents with stronger semantic understanding.</p>
<p>Ethical Considerations</p>
<p>Autonomous decision-making agents are potentially impactful in our society, and it is of great ethical consideration to make sure their understanding of the world and their objectives align with humans. Humans use natural language to convey and understand concepts as well as inform decisions, and in this work we investigate whether autonomous agents leverage language semantics similarly to humans in the environment of text-based games. Our findings suggest that the current generation of agents optimized for reinforcement learning objectives might not exhibit human-like language understanding, a phenomenon we should pay attention to and further study.</p>
<p>Figure 1 :
1(a): Sample original gameplay from ZORK I. (b) (c): Our proposed semantic ablations. (b) MIN-OB reduces observations to only the current location name, and (c) HASH replaces observation and action texts by their string hash values.</p>
<p>Figure 2 :
2Transfer results from ZORK I.</p>
<p>Figure 3
3: t-SNE visualization of seen and unseen state observations of ZORK I. DRRN (base) represents unseen states separated from seen states while INV-DY mixes them by semantic similarity.</p>
<p>). Breaking Semantics via Hashing (HASH) GRU encoders f o and f a in the Q-network (1) generally ensure that similar texts (e.g. a single word change) are given similar representations, and therefore similar values. To study whether such a semantics continuity is useful, we break it by hashing observation and action texts. Specifically,Table 1: Final/maximum score of different models.Game DRRN </p>
<p>MIN-OB 
HASH 
INV-DY </p>
<p>Max </p>
<p>balances 10 / 10 
10 / 10 
10 / 10 
10 / 10 
51 
deephome 57 / 66 
8.5 / 27 
58 / 67 
57.6 / 67 300 
detective 290 / 337 86.3 / 350 290 / 317 290 / 323 360 
dragon -5.0 / 6 
-5.4 / 3 
-5.0 / 7 
-2.7 / 8 
25 
enchanter 20 / 20 
20 / 40 
20 / 30 
20 / 30 
400 
inhumane 21.1 / 45 12.4 / 40 
21.9 / 45 19.6 / 45 90 
library 15.7 / 21 12.8 / 21 
17 / 21 
16.2 / 21 30 
ludicorp 12.7 / 23 11.6 / 21 
14.8 / 23 13.5 / 23 150 
omniquest 4.9 / 5 
4.9 / 5 
4.9 / 5 
5.3 / 10 
50 
pentari 26.5 / 45 21.7 / 45 
51.9 / 60 37.2 / 50 70 
zork1 39.4 / 53 29 / 46 
35.5 / 50 43.1 / 87 350 
zork3 0.4 / 4.5 
0.0 / 4 
0.4 / 4 
0.4 / 4 
7 </p>
<p>Avg. Norm .21 / .38 
.12 / .35 
.25 / .39 
.23 / .40 </p>
<p>§ The rest 150 states in the middle (score 45-157) are omitted as they might be seen by some model but not others. ¶ Randomness includes transition uncertainty (e.g. thief showing up randomly in ZORK I) and occasional paraphrasing of text observations.
AcknowledgementsWe appreciate helpful suggestions from anonymous reviewers as well as members of the Princeton NLP Group and MSR RL Group.
Learning dynamic knowledge graphs to generalize on text-based games. Ashutosh Adhikari, ( Xingdi, ) Eric, Marc-Alexandre Yuan, Mikulas Côté, Marc-Antoine Zelinka, Romain Rondeau, Pascal Laroche, Jian Poupart, Adam Tang, William L Trischler, Hamilton, NeurIPS. Ashutosh Adhikari, Xingdi (Eric) Yuan, Marc- Alexandre Côté, Mikulas Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton . 2020. Learning dynamic knowledge graphs to gen- eralize on text-based games. In NeurIPS 2020.</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew J Hausknecht, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020OpenReview.netPrithviraj Ammanabrolu and Matthew J. Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. In 8th Inter- national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. Prithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, Mark O Riedl, arXiv:2002.08795arXiv preprintPrithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, and Mark O Riedl. 2020. How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. arXiv preprint arXiv:2002.08795.</p>
<p>On the properties of neural machine translation: Encoder-decoder approaches. Kyunghyun Cho, Dzmitry Bart Van Merriënboer, Yoshua Bahdanau, Bengio, 10.3115/v1/W14-4012Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical TranslationDoha, QatarAssociation for Computational LinguisticsKyunghyun Cho, Bart van Merriënboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder ap- proaches. In Proceedings of SSST-8, Eighth Work- shop on Syntax, Semantics and Structure in Statisti- cal Translation, pages 103-111, Doha, Qatar. Asso- ciation for Computational Linguistics.</p>
<p>What can you do with a rock? affordance extraction via word embeddings. Nancy Fulda, Daniel Ricks, Ben Murdoch, David Wingate, 10.24963/ijcai.2017/144Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017. the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017Melbourne, Australiaijcai.orgNancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. 2017. What can you do with a rock? affor- dance extraction via word embeddings. In Proceed- ings of the Twenty-Sixth International Joint Con- ference on Artificial Intelligence, IJCAI 2017, Mel- bourne, Australia, August 19-25, 2017, pages 1039- 1045. ijcai.org.</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu Chang, 10.18653/v1/2020.emnlp-main.624Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Mur- ray Campbell, and Shiyu Chang. 2020. Interac- tive fiction game playing as multi-paragraph read- ing comprehension with reinforcement learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7755-7765, Online. Association for Computa- tional Linguistics.</p>
<p>Nail: A general interactive fiction agent. Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, Jason D Williams, arXiv:1902.04259arXiv preprintMatthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. 2019. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259.</p>
<p>Interactive fiction games: A colossal adventure. Matthew J Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceMatthew J. Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. 2020. In- teractive fiction games: A colossal adventure. In The Thirty-Fourth AAAI Conference on Artificial In- telligence, AAAI 2020, The Thirty-Second Innova- tive Applications of Artificial Intelligence Confer- ence, IAAI 2020, The Tenth AAAI Symposium on Ed- ucational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7903-7910. AAAI Press.</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 10.18653/v1/P16-1153Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyLong Papers1Association for Computational LinguisticsJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li- hong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language ac- tion space. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630, Berlin, Germany. Association for Computational Linguis- tics.</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579-2605.</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, 10.18653/v1/D15-1001Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsKarthik Narasimhan, Tejas Kulkarni, and Regina Barzi- lay. 2015. Language understanding for text-based games using deep reinforcement learning. In Pro- ceedings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing, pages 1-11, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, PMLRProceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, Australia70Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. 2017. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learn- ing Research, pages 2778-2787. PMLR.</p>
<p>Keep CALM and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 10.18653/v1/2020.emnlp-main.704Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. 2020. Keep CALM and ex- plore: Language models for action generation in text-based games. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 8736-8754, Online. As- sociation for Computational Linguistics.</p>
<p>Deriving commonsense inference tasks from interactive fictions. Mo Yu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael Greenspan, Murray Campbell, arXiv:2010.09788arXiv preprintMo Yu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael Greenspan, and Murray Campbell. 2020. Deriving commonsense inference tasks from inter- active fictions. arXiv preprint arXiv:2010.09788.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7407 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7407</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7407</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-272770247</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.13317v1.pdf" target="_blank">JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Recent developments in Japanese large language models (LLMs) primarily focus on general domains, with fewer advancements in Japanese biomedical LLMs. One obstacle is the absence of a comprehensive, large-scale benchmark for comparison. Furthermore, the resources for evaluating Japanese biomedical LLMs are insufficient. To advance this field, we propose a new benchmark including eight LLMs across four categories and 20 Japanese biomedical datasets across five tasks. Experimental results indicate that: (1) LLMs with a better understanding of Japanese and richer biomedical knowledge achieve better performance in Japanese biomedical tasks, (2) LLMs that are not mainly designed for Japanese biomedical domains can still perform unexpectedly well, and (3) there is still much room for improving the existing LLMs in certain Japanese biomedical tasks. Moreover, we offer insights that could further enhance development in this field. Our evaluation tools tailored to our benchmark as well as the datasets are publicly available in https://huggingface.co/datasets/Coldog2333/JMedBench to facilitate future research.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7407.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7407.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt templates (Minimal/Standard/English-centric/Instructed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt template categories: Minimal, Standard, English-centric, and Instructed</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluates four prompt-template styles (Minimal, Standard, English-centric, Instructed) and reports how template choice affects zero-shot and few-shot performance across multiple Japanese biomedical tasks, finding modest template sensitivity in zero-shot and reduced sensitivity with few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models (Llama2-7B, Llama3-8B, Qwen2-7B, Mistral-7B, Meditron-7B, llm-jp-13B, SwallowLM-7B, MMed-Llama3-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various transformer-based autoregressive LLMs including generalist open models and domain-adapted or language-adapted variants; continuously pre-trained or trained from scratch with differing pretraining corpora (English, Japanese, biomedical corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 8B / 13B (models evaluated in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot and few-shot evaluation across tasks (MCQA, NER, MT, DC, STS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification, sequence labeling, translation, and regression-style similarity tasks in Japanese biomedical domain assessed under different prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt template style (Minimal vs Standard vs English-centric vs Instructed)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Four explicit template categories defined: Minimal (very little instruction, e.g., only question), Standard (commonly used prompts per task), English-centric (instructions phrased in/optimized for English or English-style prompts), and Instructed (explicit task instructions); evaluated in zero-shot and few-shot settings; reported results take maximal score of multiple runs using these diverse templates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task-dependent (accuracy for MCQA/DC, F1-entity for NER, BLEU for MT, Pearson for STS)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: In zero-shot, Standard/English-centric/Instructed do not differ substantially overall, but English-centric templates usually achieve slightly better performance (especially for English-centric LLMs); in few-shot, templates' differences are reduced and minimal templates benefit notably from few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative: English-centric templates show slight positive effect in zero-shot (esp. for English-centric models); few-shot demonstrations reduce inter-template variance and substantially improve Minimal template performance (no single absolute number reported for this effect).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot and few-shot runs; for MCQA/DC/STS, answers selected by computing likelihood/perplexity across candidate outputs; reported maximal score across prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7407.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7407.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot / In-context learning (ICL) demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot (in-context learning) demonstrations and their effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that adding few-shot examples into the prompt substantially improves model performance across tasks (MCQA, NER, MT, DC, STS), attributing gains both to exposure to task format and domain-specific knowledge in demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models (same set as above; examples include Llama2-7B, Llama3-8B, Qwen2-7B, MMed-Llama3-8B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs with varying in-context learning capabilities; some models (e.g., Llama3) explicitly show improved ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 8B / 13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks (MCQA, NER, MT, DC, STS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks range from multi-choice QA to sequence-labeling NER and classification/translation/similarity evaluation in Japanese biomedical domain.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot demonstrations included in the prompt (1-3 examples depending on dataset and token limits)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type / prompt style (example-based)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Few-shot example counts: typically 3 shots for most NER datasets; 1 shot for very long NER examples (MRNER-Disease, MRNER-Medicine, NRNER) due to token limits; few-shot training/validation sets translated to Japanese for demonstrations; experimental comparisons between zero-shot and few-shot reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (MCQA/DC), F1-entity (NER), BLEU (MT), Pearson correlation (STS)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NER: few-shot yields large gains — average F1-entity improvements reported in the paper range from +19.36% to +33.33% absolute; other tasks: across-the-board improvements with few-shot (qualitatively reported), e.g., MCQA and DC accuracy increases for all models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot performance on same tasks (reported in paper tables)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>NER: +19.36% to +33.33% F1-entity absolute improvement (few-shot vs zero-shot); MCQA/DC/other tasks: qualitative increases (absolute numbers in paper tables per model/task).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot counts vary by dataset (1–3 examples); demonstrations translated into Japanese when needed; selection via likelihood/perplexity for MCQA/DC/STS.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7407.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7407.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Likelihood/perplexity answer selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Answer selection via likelihood / perplexity scoring over candidate options</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>To constrain autoregressive LLMs in multi-choice and classification tasks, the paper computes the generation likelihood (perplexity) of each candidate answer/class and selects the option with highest likelihood, instead of depending on unconstrained free-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple autoregressive models evaluated (Llama2, Llama3, Qwen2, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLMs whose decoding is unconstrained; likelihood scoring used to compare candidate discrete outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 8B / 13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MCQA, Document Classification (DC), Semantic Text Similarity (STS configured as discrete levels)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Choose one answer from multiple choices (MCQA), assign a class (DC), or select discrete similarity level (STS); evaluated by choosing highest-likelihood candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice/class candidates scored by token-level likelihood / perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>answer selection / decoding strategy</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>For MCQA and DC, compute likelihood/perplexity of each given option/class under the model and pick the most likely; for STS, compute likelihood of generating values 0–5 and pick highest; this mitigates unconstrained generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (MCQA/DC), Pearson (STS)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Method used as evaluation mechanism; paper reports task performances computed using this likelihood-selection approach (e.g., MCQA accuracies given in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Autoregressive decoding; likelihood/perplexity computed per candidate; used both in zero-shot and few-shot evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7407.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7407.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entity-first translation order (NER dataset augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instance-level NER translation with 'entity-first' instruction versus text-first</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When translating NER datasets from English to Japanese using GPT models, prompting the model to translate entities first then the surrounding text reduced invalid translated samples by about 10%, improving dataset validity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT models used for translation (ChatGPT / GPT-3.5 / GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large GPT-family models used as machine translation engines for dataset augmentation (gpt-3.5-turbo, gpt-4-0613 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>API models (not parameterized in paper); gpt-3.5 and gpt-4 series used</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NER dataset machine translation (BLURB-derived datasets to Japanese)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate instance-level NER samples (fields: entity type, text, entities) while preserving entity spans so entities appear in translated text.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Translation prompt order: translate entities first then text (entity-first) vs text-first</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / translation instruction order</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Two-phase translation: machine translation then manual correction; entity-first prompt instructs model to translate entity strings first and then the sentence so translated entities appear in text; reported reduction in invalid samples by ~10% when using entity-first prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Invalid sample rate (percentage of translated entries invalid due to JSON error or missing entity in translated text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Entity-first prompting reduced invalid samples by approximately 10% relative to text-first prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Text-first translation ordering (baseline invalid sample rate higher by ≈10%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-≈10% invalid-sample rate (relative reduction) when using entity-first ordering</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Used ChatGPT/GPT-4 for translating training/validation/test sets; for invalid samples they re-called GPT-4 with temperature set to 0.5 up to 5 times and then manually fixed remaining 223 entries (0.34% of dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7407.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7407.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instance-level translation for MCQA (question+options)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instance-level MCQA translation (translate question together with options)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For MCQA dataset augmentation, the authors translate at the instance level (question together with its options) rather than sentence level, arguing it gives the model the full scenario and yields better translations that keep option meaning consistent with context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT models used for translation (GPT-4 for test sets, cheaper GPT-3.5 for some training/validation translations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-family models used as MT engines; test-set translations done with stronger GPT-4 checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>API models (gpt-4, gpt-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MCQA dataset translation (MedMCQA, MedQA, USMLE-QA, PubMedQA, MMLU-medical etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate multiple-choice QA instances (question + all candidate options) from English to Japanese while preserving option semantics and scenario context.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instance-level translation (question and all options in a single prompt) instead of sentence-level translation</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>translation prompt style / input structuring</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompts supplied question plus options together (see Appendix Table 6); translators used GPT-4 for test sets and cheaper GPT-3.5 for some few-shot translation support; intended to help LLMs better understand scenario and produce correct translations for options.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Indirect: translation quality as validated by downstream MCQA performance and manual checks (no single numeric MT metric reported for this comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: instance-level translation considered better because it preserves scenario and option consistency; authors observed high translation quality and stable downstream task applicability but did not provide a direct numeric comparison vs sentence-level translation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Sentence-level translation (not quantitatively compared in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-4 used for generating translated testing sets; cheaper API used for some training/validation translations due to budget constraints; some manual corrections applied.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sensitivity and robustness of large language models to prompt template in Japanese text classification tasks <em>(Rating: 2)</em></li>
                <li>Rethinking the role of demonstrations: What makes in-context learning work? <em>(Rating: 2)</em></li>
                <li>Leveraging large language models for multiple choice question answering <em>(Rating: 2)</em></li>
                <li>Can large language models reason about medical questions? <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7407",
    "paper_id": "paper-272770247",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Prompt templates (Minimal/Standard/English-centric/Instructed)",
            "name_full": "Prompt template categories: Minimal, Standard, English-centric, and Instructed",
            "brief_description": "The paper evaluates four prompt-template styles (Minimal, Standard, English-centric, Instructed) and reports how template choice affects zero-shot and few-shot performance across multiple Japanese biomedical tasks, finding modest template sensitivity in zero-shot and reduced sensitivity with few-shot examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple models (Llama2-7B, Llama3-8B, Qwen2-7B, Mistral-7B, Meditron-7B, llm-jp-13B, SwallowLM-7B, MMed-Llama3-8B)",
            "model_description": "Various transformer-based autoregressive LLMs including generalist open models and domain-adapted or language-adapted variants; continuously pre-trained or trained from scratch with differing pretraining corpora (English, Japanese, biomedical corpora).",
            "model_size": "7B / 8B / 13B (models evaluated in the paper)",
            "task_name": "Zero-shot and few-shot evaluation across tasks (MCQA, NER, MT, DC, STS)",
            "task_description": "Classification, sequence labeling, translation, and regression-style similarity tasks in Japanese biomedical domain assessed under different prompt templates.",
            "problem_format": "Prompt template style (Minimal vs Standard vs English-centric vs Instructed)",
            "format_category": "prompt style",
            "format_details": "Four explicit template categories defined: Minimal (very little instruction, e.g., only question), Standard (commonly used prompts per task), English-centric (instructions phrased in/optimized for English or English-style prompts), and Instructed (explicit task instructions); evaluated in zero-shot and few-shot settings; reported results take maximal score of multiple runs using these diverse templates.",
            "performance_metric": "Task-dependent (accuracy for MCQA/DC, F1-entity for NER, BLEU for MT, Pearson for STS)",
            "performance_value": "Qualitative: In zero-shot, Standard/English-centric/Instructed do not differ substantially overall, but English-centric templates usually achieve slightly better performance (especially for English-centric LLMs); in few-shot, templates' differences are reduced and minimal templates benefit notably from few-shot examples.",
            "baseline_performance": null,
            "performance_change": "Qualitative: English-centric templates show slight positive effect in zero-shot (esp. for English-centric models); few-shot demonstrations reduce inter-template variance and substantially improve Minimal template performance (no single absolute number reported for this effect).",
            "experimental_setting": "Zero-shot and few-shot runs; for MCQA/DC/STS, answers selected by computing likelihood/perplexity across candidate outputs; reported maximal score across prompt templates.",
            "statistical_significance": null,
            "uuid": "e7407.0",
            "source_info": {
                "paper_title": "JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Few-shot / In-context learning (ICL) demonstrations",
            "name_full": "Few-shot (in-context learning) demonstrations and their effect",
            "brief_description": "The paper shows that adding few-shot examples into the prompt substantially improves model performance across tasks (MCQA, NER, MT, DC, STS), attributing gains both to exposure to task format and domain-specific knowledge in demonstrations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple models (same set as above; examples include Llama2-7B, Llama3-8B, Qwen2-7B, MMed-Llama3-8B, etc.)",
            "model_description": "Transformer LLMs with varying in-context learning capabilities; some models (e.g., Llama3) explicitly show improved ICL.",
            "model_size": "7B / 8B / 13B",
            "task_name": "Multiple tasks (MCQA, NER, MT, DC, STS)",
            "task_description": "Tasks range from multi-choice QA to sequence-labeling NER and classification/translation/similarity evaluation in Japanese biomedical domain.",
            "problem_format": "Few-shot demonstrations included in the prompt (1-3 examples depending on dataset and token limits)",
            "format_category": "question type / prompt style (example-based)",
            "format_details": "Few-shot example counts: typically 3 shots for most NER datasets; 1 shot for very long NER examples (MRNER-Disease, MRNER-Medicine, NRNER) due to token limits; few-shot training/validation sets translated to Japanese for demonstrations; experimental comparisons between zero-shot and few-shot reported.",
            "performance_metric": "Accuracy (MCQA/DC), F1-entity (NER), BLEU (MT), Pearson correlation (STS)",
            "performance_value": "NER: few-shot yields large gains — average F1-entity improvements reported in the paper range from +19.36% to +33.33% absolute; other tasks: across-the-board improvements with few-shot (qualitatively reported), e.g., MCQA and DC accuracy increases for all models.",
            "baseline_performance": "Zero-shot performance on same tasks (reported in paper tables)",
            "performance_change": "NER: +19.36% to +33.33% F1-entity absolute improvement (few-shot vs zero-shot); MCQA/DC/other tasks: qualitative increases (absolute numbers in paper tables per model/task).",
            "experimental_setting": "Few-shot counts vary by dataset (1–3 examples); demonstrations translated into Japanese when needed; selection via likelihood/perplexity for MCQA/DC/STS.",
            "statistical_significance": null,
            "uuid": "e7407.1",
            "source_info": {
                "paper_title": "JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Likelihood/perplexity answer selection",
            "name_full": "Answer selection via likelihood / perplexity scoring over candidate options",
            "brief_description": "To constrain autoregressive LLMs in multi-choice and classification tasks, the paper computes the generation likelihood (perplexity) of each candidate answer/class and selects the option with highest likelihood, instead of depending on unconstrained free-text generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple autoregressive models evaluated (Llama2, Llama3, Qwen2, etc.)",
            "model_description": "Autoregressive transformer LLMs whose decoding is unconstrained; likelihood scoring used to compare candidate discrete outputs.",
            "model_size": "7B / 8B / 13B",
            "task_name": "MCQA, Document Classification (DC), Semantic Text Similarity (STS configured as discrete levels)",
            "task_description": "Choose one answer from multiple choices (MCQA), assign a class (DC), or select discrete similarity level (STS); evaluated by choosing highest-likelihood candidate.",
            "problem_format": "Multiple-choice/class candidates scored by token-level likelihood / perplexity",
            "format_category": "answer selection / decoding strategy",
            "format_details": "For MCQA and DC, compute likelihood/perplexity of each given option/class under the model and pick the most likely; for STS, compute likelihood of generating values 0–5 and pick highest; this mitigates unconstrained generation.",
            "performance_metric": "Accuracy (MCQA/DC), Pearson (STS)",
            "performance_value": "Method used as evaluation mechanism; paper reports task performances computed using this likelihood-selection approach (e.g., MCQA accuracies given in tables).",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Autoregressive decoding; likelihood/perplexity computed per candidate; used both in zero-shot and few-shot evaluations.",
            "statistical_significance": null,
            "uuid": "e7407.2",
            "source_info": {
                "paper_title": "JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Entity-first translation order (NER dataset augmentation)",
            "name_full": "Instance-level NER translation with 'entity-first' instruction versus text-first",
            "brief_description": "When translating NER datasets from English to Japanese using GPT models, prompting the model to translate entities first then the surrounding text reduced invalid translated samples by about 10%, improving dataset validity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT models used for translation (ChatGPT / GPT-3.5 / GPT-4)",
            "model_description": "Large GPT-family models used as machine translation engines for dataset augmentation (gpt-3.5-turbo, gpt-4-0613 etc.).",
            "model_size": "API models (not parameterized in paper); gpt-3.5 and gpt-4 series used",
            "task_name": "NER dataset machine translation (BLURB-derived datasets to Japanese)",
            "task_description": "Translate instance-level NER samples (fields: entity type, text, entities) while preserving entity spans so entities appear in translated text.",
            "problem_format": "Translation prompt order: translate entities first then text (entity-first) vs text-first",
            "format_category": "prompt style / translation instruction order",
            "format_details": "Two-phase translation: machine translation then manual correction; entity-first prompt instructs model to translate entity strings first and then the sentence so translated entities appear in text; reported reduction in invalid samples by ~10% when using entity-first prompting.",
            "performance_metric": "Invalid sample rate (percentage of translated entries invalid due to JSON error or missing entity in translated text)",
            "performance_value": "Entity-first prompting reduced invalid samples by approximately 10% relative to text-first prompting.",
            "baseline_performance": "Text-first translation ordering (baseline invalid sample rate higher by ≈10%)",
            "performance_change": "-≈10% invalid-sample rate (relative reduction) when using entity-first ordering",
            "experimental_setting": "Used ChatGPT/GPT-4 for translating training/validation/test sets; for invalid samples they re-called GPT-4 with temperature set to 0.5 up to 5 times and then manually fixed remaining 223 entries (0.34% of dataset).",
            "statistical_significance": null,
            "uuid": "e7407.3",
            "source_info": {
                "paper_title": "JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Instance-level translation for MCQA (question+options)",
            "name_full": "Instance-level MCQA translation (translate question together with options)",
            "brief_description": "For MCQA dataset augmentation, the authors translate at the instance level (question together with its options) rather than sentence level, arguing it gives the model the full scenario and yields better translations that keep option meaning consistent with context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT models used for translation (GPT-4 for test sets, cheaper GPT-3.5 for some training/validation translations)",
            "model_description": "OpenAI GPT-family models used as MT engines; test-set translations done with stronger GPT-4 checkpoint.",
            "model_size": "API models (gpt-4, gpt-3.5)",
            "task_name": "MCQA dataset translation (MedMCQA, MedQA, USMLE-QA, PubMedQA, MMLU-medical etc.)",
            "task_description": "Translate multiple-choice QA instances (question + all candidate options) from English to Japanese while preserving option semantics and scenario context.",
            "problem_format": "Instance-level translation (question and all options in a single prompt) instead of sentence-level translation",
            "format_category": "translation prompt style / input structuring",
            "format_details": "Prompts supplied question plus options together (see Appendix Table 6); translators used GPT-4 for test sets and cheaper GPT-3.5 for some few-shot translation support; intended to help LLMs better understand scenario and produce correct translations for options.",
            "performance_metric": "Indirect: translation quality as validated by downstream MCQA performance and manual checks (no single numeric MT metric reported for this comparison)",
            "performance_value": "Qualitative: instance-level translation considered better because it preserves scenario and option consistency; authors observed high translation quality and stable downstream task applicability but did not provide a direct numeric comparison vs sentence-level translation.",
            "baseline_performance": "Sentence-level translation (not quantitatively compared in paper)",
            "performance_change": null,
            "experimental_setting": "GPT-4 used for generating translated testing sets; cheaper API used for some training/validation translations due to budget constraints; some manual corrections applied.",
            "statistical_significance": null,
            "uuid": "e7407.4",
            "source_info": {
                "paper_title": "JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sensitivity and robustness of large language models to prompt template in Japanese text classification tasks",
            "rating": 2,
            "sanitized_title": "sensitivity_and_robustness_of_large_language_models_to_prompt_template_in_japanese_text_classification_tasks"
        },
        {
            "paper_title": "Rethinking the role of demonstrations: What makes in-context learning work?",
            "rating": 2,
            "sanitized_title": "rethinking_the_role_of_demonstrations_what_makes_incontext_learning_work"
        },
        {
            "paper_title": "Leveraging large language models for multiple choice question answering",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_for_multiple_choice_question_answering"
        },
        {
            "paper_title": "Can large language models reason about medical questions?",
            "rating": 1,
            "sanitized_title": "can_large_language_models_reason_about_medical_questions"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.013838999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models</p>
<p>Junfeng Jiang jiangjf@is.s.u-tokyo.ac.jp 
The University of Tokyo § National Institute of Informatics</p>
<p>Jiahao Huang jiahao-huang@g.ecc.u-tokyo.ac.jp 
The University of Tokyo § National Institute of Informatics</p>
<p>Akiko Aizawa aizawa@nii.ac.jp 
The University of Tokyo § National Institute of Informatics</p>
<p>JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models
EB201C5981D761A37B488CC0F2F2F45A
Recent developments in Japanese large language models (LLMs) primarily focus on general domains, with fewer advancements in Japanese biomedical LLMs.One obstacle is the absence of a comprehensive, large-scale benchmark for comparison.Furthermore, the resources for evaluating Japanese biomedical LLMs are insufficient.To advance this field, we propose a new benchmark including eight LLMs across four categories and 20 Japanese biomedical datasets across five tasks.Experimental results indicate that: (1) LLMs with a better understanding of Japanese and richer biomedical knowledge achieve better performance in Japanese biomedical tasks, (2) LLMs that are not mainly designed for Japanese biomedical domains can still perform unexpectedly well, and (3) there is still much room for improving the existing LLMs in certain Japanese biomedical tasks.Moreover, we offer insights that could further enhance development in this field.Our evaluation tools tailored to our benchmark as well as the datasets are publicly available to facilitate future research. 1</p>
<p>Introduction</p>
<p>Large language models (LLMs) show excellent performances in various tasks in general domains including Question Answering (QA) (Brown, 2020;Taori et al., 2023), Machine Translation (MT) (He et al., 2024), Summarization (Ravaut et al., 2024), Machine Reading Comprehension (MRC) (Zhou et al., 2023), Sentiment Analysis (Zhang et al., 2024), and so on.Some researchers design proper prompts for solving biomedical tasks (Singhal et al., 2023;Liévin et al., 2024;Nori et al., 2023).However, most of the existing LLMs have been pretrained with texts in general domains, lacking domain-specific knowledge.To overcome this challenge, biomedical LLMs are proposed through pre-training on biomedical corpora (Chen et al., 2023;Wu et al., 2024), fine-tuning with instruction data (Han et al., 2023), or reinforcement learning with human feedback (Yang et al., 2024b).</p>
<p>With the chain-of-thought prompting technique, Liévin et al. ( 2024) have achieved 60.2% accuracy on USMLE-QA (Jin et al., 2021), passing the medical licensing examination in the United States.In the most recent work, with the help of multiple agents, Nori et al. (2023) have achieved 93.06% accuracy on the USMLE-QA dataset, similar to the performance of a human expert.With this series of techniques, biomedical LLMs are greatly promoted in English biomedical tasks.However, biomedical LLMs in other languages still have much room for improvement (e.g., Japanese, Chinese, French, etc.).Besides the relative unpopularity of existing Japanese LLMs, another important obstacle is the lack of a comprehensive benchmark for evaluation and comparison.Therefore, in this paper, we focus on constructing a benchmark for evaluating Japanese biomedical LLMs.</p>
<p>We selected five tasks that are widely used for evaluating LLMs and real-world applications, including multi-choice question-answering (MCQA), named entity recognition (NER), machine translation (MT), document classification (DC), and semantic text similarity (STS).Since there are only a few Japanese biomedical datasets exist and they are generally small (e.g., IgakuQA (Kasai et al., 2023) only has 1,600 samples for testing), to reduce the fluctuation of evaluation results, we translate large-scale and high-quality datasets from other languages (e.g., English) to Japanese, augmenting the scale of our benchmark.Furthermore, in the field of Japanese biomedical LLM, a solid leaderboard is missing.Therefore, we select eight representative models to conduct extensive experiments, providing a standard for comparison.We hope our work can make future comparisons more convenient and fair, promoting the development in this field.In summary, our contributions are in three folds.</p>
<p>• We construct a large-scale benchmark including 20 Japanese biomedical datasets across five tasks for a comprehensive evaluation.</p>
<p>• We evaluate eight representative models across four categories in our benchmark to provide a standard for future comparison.</p>
<p>• We conduct extensive analysis from aspects of the dataset, model, and prompt template, providing valuable insights for future researchers.</p>
<p>Related Works</p>
<p>Benchmarking is essential for the development of a specific field.ImageNet Challenge (Deng et al., 2009) is a famous benchmark in Computer Vision.Many remarkable works on image recognition have been proposed (Krizhevsky et al., 2012;He et al., 2016;Tan, 2019) throughout history and the development has increased rapidly.One reason for this success is the convenience of comparison and evaluation in this field.The GLUE (Wang, 2018) is another famous benchmark for evaluating and analyzing natural language understanding (NLU) systems to promote research in developing general and robust NLU systems.However, these works mainly focus on English tasks, limiting the scope of evaluating other languages like Japanese.Kurihara et al. (2022) constructed the JGLUE from scratch without using any translation, including six datasets, which facilitates the research in Japanese natural language processing (NLP) (Yano et al., 2024;Enomoto et al., 2024;Aizawa et al., 2024).</p>
<p>Considering the wide applications of language models (LMs), researchers are trying to explore LMs' power in biomedical tasks.Gu et al. (2021) collected 13 biomedical NLP datasets in six tasks from different isolated work to form a benchmark called BLURB for evaluating biomedical models.MMLU (Chang et al., 2024) is a benchmark consisting of multiple topics.Specially, it contains some biomedical questions like medical questions at the college level.DrBenchmark (Labrak et al., 2024) is an NLU benchmark for evaluating French biomedical models.However, they are not applicable in Japanese.JMMLU2 is a translated version of the MMLU.The researchers recruited human translators to check and remove those that were difficult to translate, irrelevant, or inconsistent with the Japanese culture.Recently, Qiu et al. (2024) have proposed a multilingual benchmark with six languages for evaluating medical LMs.These benchmarks reflect some shortages of existing LLMs and provide insights into improving the Japanese biomedical LLMs, but they only focus on the MCQA tasks, which hinders the completeness of the evaluation.Considering these shortages, in this paper, we are dedicated in constructing a largescale benchmark with diverse tasks for evaluating Japanese biomedical large language models.Table 1 shows a comparison of these benchmarks.</p>
<p>MCQA</p>
<p>JMedBench</p>
<p>Our benchmark construction consists of two parts.The first part is the dataset collection, while another part is the protocol for evaluation.Firstly, we introduce the rationality of dataset selection and how we augment our benchmark with datasets from other languages.Then, we propose a protocol to obtain robust evaluation results and discuss its necessity for evaluating Japanese biomedical LLMs. Figure 1 is an overview of our benchmark.</p>
<p>Datasets</p>
<p>In the JMedBench, we include 20 datasets across five tasks containing 38K testing samples.We collect some human-manufactured Japanese datasets, like IgakuQA (Kasai et al., 2023).We also translate some high-quality large-scale English datasets into Japanese to enhance the robustness of JMed-Bench.Considering the convenience and performance of using OpenAI's API, we use ChatGPT3 and GPT-4 (Achiam et al., 2023) to create our evaluation datasets when translation is needed.To ensure the quality of the translated testing sets, we use the most powerful model from OpenAI, the GPT-44 , to perform machine translation.Incontext learning is a common practice for adapting an LLM to an unseen task.Therefore, we also translate the training or validation sets to support few-shot evaluation.Due to the limitation of our budgets, we use the cheapest API5 from OpenAI to translate these samples.Though the translation may not be perfect, producing unfaithful content sometimes, it is good enough to provide information like some domain-specific knowledge and task format during the few-shot evaluation.Previous works (Hendy et al., 2023;Sanz-Valdivieso and López-Arroyo, 2023;AlAfnan, 2024) also have similar findings that ChatGPT has already had a comparable MT performance with specialized Neural Machine Translation systems.Here listed are the involved biomedical tasks and corresponding datasets.Detailed statistics can be found in Table 5 in the Appendix.</p>
<p>• MCQA is one of the most commonly used tasks for evaluating LLMs since other tasks can be easily reformulated into the MCQA task.We included IgakuQA (Kasai et al., 2023), JMMLU-medical6 , and translated MedMCQA (Pal et al., 2022), MedQA (Jin et al., 2021), USMLE-QA, PubMedQA (Jin et al., 2019), and MMLU-medical (Hendrycks et al., 2021b,a).</p>
<p>• MT is an important natural language generation (NLG) task.In the biomedical do-main, researchers usually need to refer to some English terminologies or communicate with other researchers.Therefore, we expect LLMs can handle cross-lingual tasks besides monolingual tasks.We included the EJMMT (Hayakawa and Arase, 2020) dataset to evaluate the cross-lingual ability of LLMs.</p>
<p>• NER is an NLU task aiming to extract named entities like biomedical terminologies, medicines, etc.We included three Japanese medical NER datasets from JMED-LLM7 : MRNER-disease, MRNER-medicine, and NRNER.To improve the diversity of the dataset, we also follow the BLURB benchmark and include translated BC2GM (Smith et al., 2008), BC5Chem, BC5-Disease (Li et al., 2016), JNLPBA (Collier et al., 2004), andNCBI Disease (Dogan et al., 2014).</p>
<p>• DC aims to classify documents into specific categories.We include three datasets from JMED-LLM: CRADE, RRTNM, and SMDIS.</p>
<p>• STS is a regression task to compute the semantic similarity between two biomedical sentences.We reformulate it as a classification task to output the discrete level of similarity.</p>
<p>We include the JCSTS (Mutinda et al., 2021).</p>
<p>Evaluation Dataset Augmentation</p>
<p>To enlarge the size of JMedBench for obtaining robust evaluation results, we select several biomedical datasets in English, because of its popularity.</p>
<p>Multi-choice Question-Answering</p>
<p>Different from previous works that usually conduct machine translation at the sentence level, we perform translation at the instance level.Specifically, we translate questions and options meanwhile, so that LLM can understand the scenario better to provide more correct translations.Detailed prompt template can be found in Table 6 in the Appendix.</p>
<p>Named Entity Recognition</p>
<p>We also translate the NER datasets from the BLURB benchmark to improve the amount and diversity of JMedBench.There are three fields in the NER samples: entity type, text, and entities.</p>
<p>To ensure the consistency of the translated entity types, we manually translate them into Japanese based on a dictionary (e.g., gene →遺伝子).As for the text and entities, we also perform translation at the instance level, as described in Section 3.2.1.The prompt template for translating the biomedical NER datasets is also shown in Table 7 in the Appendix.</p>
<p>One of the challenges is that the translated entities may not appear in the translated text.To solve this issue, we conduct the translation in two phases: machine translation and manual modification.We first use ChatGPT and GPT-4 to translate the training and testing sets, respectively.We then collect all the invalid samples, mainly due to JSON format error and failure to include the translated entities, and re-translate them using GPT-4.We increase the temperature to 0.5 and call the GPT-4 API again at most 5 times to seek a valid sample.After the machine translation phase, 223 translated entries (0.34%) still remain invalid and then we manually modify these entries to make them valid.</p>
<p>During machine translation, we find that translating entities first instead of text first can reduce about 10% of invalid samples.We speculate that with the entity-first prompt, LLM can refer to the already translated entities when translating the text, thus, the translated entities are usually contained in the following translated sentence.However, since this is not the main focus of this paper, we did not conduct further analysis to verify this hypothesis.We hope this finding can inspire future researchers when performing instance-level machine translation.Despite a small number of failure cases during the machine translation phase (some bad cases can be found in the Appendix A.3), we realized that the translation quality is very high when we conduct manual modification, which also reflects the reliability of our data augmentation method.</p>
<p>Evaluation Protocols</p>
<p>LLMs are usually sensitive to the prompt templates, especially in zero-shot evaluation (Gan and Mori, 2023).To obtain a robust and fair result, we suggest reporting the maximal score of multiple runs using diverse prompt templates for benchmarking.We have also considered computing an average score using different templates, whereas this reported performance may be easily implicated by inappropriate prompts (e.g., using an English-centric prompt for a Japanese-only LLM).In the following evaluation, we use four types of prompt templates:</p>
<p>• Minimal: We include information as little as possible in the prompt.For example, for completing the MCQA task, we only input the question, and compute the likelihood of each possible option, namely, {question}\n.</p>
<p>• Standard: We use commonly used prompt templates in each task.For example, we follow (Robinson and Wingate, 2023) for evaluating MCQA tasks.</p>
<p>• English-centric: Some of the existing Japanese LLMs were continually pre-trained from English-centric LLMs.Therefore, we intend to explore whether an English-centric prompt template is beneficial.</p>
<p>• Instructed: Besides the standard input, we include a brief task instruction, evaluating the instruction-following ability of LLMs.</p>
<p>As for the MCQA and DC tasks, it is difficult to constrain the auto-regressive LLMs to generate one of the given options or classes.Therefore, we follow Gao et al. (2024) to compute the likelihood perplexity of each possible answer and select the one that has the highest generation possibility as the final answer.We report accuracy on these two tasks.As for the STS task, we also calculate the likelihood perplexity of generating 0-5 and select the one that has the highest generation possibility as the final output.We use the Pearson Correlation as the evaluation metric.As for the MT and NER tasks, we generate the output and compute the BLEU (Papineni et al., 2002) score and entity-level F1 score, respectively.</p>
<p>Experiments</p>
<p>Comparison Methods</p>
<p>In our experiments, we included four categories of popular and excellent LLMs to construct our benchmark, including general LLMs in other languages: Llama2 (Touvron et al., 2023), Llama3 (Dubey et al., 2024), Qwen-2 (Yang et al., 2024a), Mistral (Jiang et al., 2023); biomedical LLM in other languages: Meditron (Chen et al., 2023); Japanese general LLMs: llm-jp (Aizawa et al., 2024), SwallowLM (Fujii et al., 2024); and Japanese biomedical LLM: MMed-Llama3 (Qiu et al., 2024).The specific checkpoints are listed in Table 9   have the 7B version of the model, we still include the llm-jp with 13B parameters in our benchmark.</p>
<p>Experimental Results</p>
<p>Multi-choice Question-Answering</p>
<p>Table 2 shows the benchmark results on Japanese biomedical MCQA tasks.Surprisingly, Qwen2 outperforms all models in MCQA, followed by MMed-Llama3.Note that Qwen2 was primarily pre-trained with Chinese and English texts.We hypothesize that one reason for its success is the considerable overlap in tokens between Chinese and Japanese.MMed-Llama3 was continually pretrained on biomedical texts in multiple languages including Japanese, explaining its superior performance over Llama3.These observations highlight the importance of understanding the Japanese language and injecting domain knowledge.With fewshot demonstrations, all models have improved.</p>
<p>We attribute this to the task format (Min et al., 2022) and some domain-specific knowledge provided by the demonstrations.Comparing Llama2 and Llama3, we find that the performance gap under the zero-shot setting is larger than that under the few-shot setting.The additional improvement should be attributed to the improved in-context learning (ICL) ability of Llama3, highlighting the need to enhance the ICL ability of LLMs.</p>
<p>Although there is a human-translated version of MMLU-medical, namely, the JMMLU-medical dataset, we still translate the original MMLUmedical dataset using GPT-4 to enrich our benchmark.According to the performances of these two datasets (i.e., JMM &amp; MML in Table 2), the differences between performances on these two datasets do not exceed 5% of accuracy.Furthermore, the ranking of the performances on the translated MMLU-medical dataset also reflects the ranking on the human-translated JMMLU-medical dataset.These observations confirm the quality and the applicability of our translated datasets.</p>
<p>Meditron was continually pre-trained with largescale English biomedical texts from the Llama2 checkpoint.Chen et al. (2023) showed that Meditron has been successfully shifted to the biomedical domain, outperforming the vanilla Llama2 in various biomedical MCQA tasks.However, we realize that Meditron performs worse than Llama2 in the JMedBench.Such multilingual ability degradation is probably due to the catastrophic forgetting issue during continual pre-training.How to improve an LLM safely without losing any other ability should be considered in future research.Besides, since the SwallowLM and MMed-Llama3 were continually pre-trained with additional Japanese texts from Llama2 and Llama3, respectively, they are improved by approximately 1% ∼ 5% average accuracy, indicating the importance of local-  language adaptation.</p>
<p>Named Entity Recognition</p>
<p>Table 3 shows the results on Japanese biomedical NER datasets.In the few-shot evaluation of BC2GM, BC5Chem, BC5Disease, JNLPBA, and NCBI-Disease datasets, we use three shots of examples.However, for MRNER-Disease, MRNER-Medicine and NRNER, we only use one shot of example because texts in these datasets are so long that multiple shots will exceed the input token limit of several models.According to the results, we find that Llama3-8B outperforms other LLMs in both zero-shot and few-shot evaluations, with average F1-entity score of 40.69% and 61.69% respectively.The Japanese biomedical LLM, MMed-Llama3, has the secondbest performance in both settings.Few-shot examples can significantly improve the performance of models on the NER tasks, ranging from 19.36% to 33.33% F1-entity improvement.Similar to the observations on MCQA tasks, we believe these examples help LLMs better understand the entity types' definition and output format.Besides, we find that LLMs perform generally worse on datasets including MRNER-Disease, MRNER-Medicine, and NRNER which are derived from JMED-LLM.Note that the average text lengths of datasets from these two sources are 69.82 and 247.81Japanese characters, while the numbers of entities are 1.33 and 2.66, respectively.Considering the longer input text, larger number of entities and sparser entity distribution, we believe these are the main reasons why the datasets derived from JMED-LLM are more challenging.</p>
<p>Machine Translation</p>
<p>Table 4 shows the BLEU scores for involved comparison methods on EJMMT.MMed-Llama3-8B and Llama3-8B achieve the best and second-best performance in our benchmark under the zero-shot setting.Interestingly, we find that the Englishcentric models (e.g., Llama2, Mistral) tend to perform better on translating Japanese texts into English, while the Japanese-centric models (e.g., SwallowLM) perform much better in translating English texts into Japanese.We believe the main reason is the text generation ability in different languages.Therefore, when applying LLMs to the MT task, we should consider more on the language generation ability instead of the language understanding ability.Although the llm-jp is also a Japanese-centric LLM, according to Aizawa et al. (2024), it was pre-trained with 50-50 Japanese-English mixed data.Therefore, it has a balanced bilingual NLU and NLG ability.Furthermore, with few-shot demonstrations displaying the task format, llm-jp achieves the best performance in the MT task, which shows the prospect of developing Japanese LLMs from scratch instead of continually pre-training from checkpoints in other languages.Besides, comparing Llama2 and the continually pre-trained Meditron and SwallowLM, we find that continually pre-training with texts in biomedical domains or Japanese texts only will lead to forgetting issues.Continual Learning (Wang et al., 2024) is a potential solution,but it is still challenging to continually improve the existing LLMs while maintaining their original ability.</p>
<p>Document Classification</p>
<p>Performances of the DC task are also shown in Table 4.We find that Qwen2 achieves the best performance again.In the zero-shot setting, Meditron achieves the second-best performance, while MMed-Llama3 achieves the second-best performance.Most of the comparison methods achieve better performance when few-shot demonstrations are given.We believe it is because of the provided task format as we discuss in Section 4.2.1.Moreover, LLMs can also recognize the fine-grained differences between different classes given fewshot demonstrations, making better decisions in classification.Especially, we notice that Meditron performs badly under the few-shot evaluation.We attribute it to the language degradation issue since it accepts a few long documents in the context, amplifying the noise when understanding Japanese.</p>
<p>Semantic Text Similairity</p>
<p>The performances on the STS task are varied dramatically.Qwen2 achieves excellent performance on this task, while the prediction of other models like Llama2-based models (i.e., Llama2, Meditron, SwallowLM) is close to random guess.One possible reason is that the distribution of generating numbers is close to a uniform distribution for these models.Recent works also show the shortage of LLMs from this aspect (Shah et al., 2023;Avnat et al., 2024).However, understanding and generating numbers accurately is essential in the biomedical domains (e.g., on blood test reports).Therefore, it is also a promising search direction in the field of biomedical NLP.</p>
<p>Discussions</p>
<p>In this section, we will conduct an integral and in-depth analysis on the experimental results.performance and visualize the rankings in Figure 6 as shown in the Appendix.A larger distance from the center represents a higher ranking and better performance.From the radar charts, we can find out that basically, MMed-Llama3, Qwen2, and Llama3 are the most outstanding LLMs on various tasks.Few-shot examples also significantly improve the model performances in all tasks.</p>
<p>Comparison of Model Performances</p>
<p>Effect of Prompt Templates</p>
<p>We also hope to understand the performance of prompt templates across different tasks and models.In zero-shot evaluation, Figure 3 illustrates that the performance of Standard, English-centric, and Instructed prompt templates do not differ significantly, but using English-centric templates usually achieves a slightly better performance.This phenomenon is even more evident in English-centric LLMs.We believe it is because these models have a greater advantage in understanding English instructions, even when facing cross-lingual contexts.Moreover, Figure 4 shows that few-shot demonstrations reduce the differences between prompt templates to a certain extent, with a particularly noticeable enhancement for minimal prompt templates.We believe it is because the output relies less on the instructions and can instead understand the task format from the few-shot examples.In this paper, we discuss an urgent need for the field of Japanese biomedical LLMs that requires a solid benchmark for evaluation and comparison.We collect a large collection of Japanese datasets in diverse biomedical tasks, including MCQA, MT, NER, DC, and STS.Considering the scale of the human-manufactured datasets, we translate several large-scale datasets with high quality in English to ensure robust benchmarking results.</p>
<p>Based on the constructed dataset collection, we conduct an evaluation of four types of models, including Japanese biomedical LLMs, Japanese General LLMs, biomedical LLMs in other languages, and general LLMs in other languages.Reported performances reveal some insights for improving existing Japanese LLMs in the biomedical domain.Furthermore, our datasets and evaluation tools are publicly available for future research.</p>
<p>Limitations</p>
<p>Considering the difficulty of evaluating natural language generation (NLG) tasks that usually require human evaluation, we only include natural language understanding (NLU) tasks or reformulate NLU tasks into NLU tasks.However, NLG tasks are also widely used in real-world applications.In the future, we consider introducing LLM-based evaluation methods to unlock an easy evaluation of NLG tasks, enriching our benchmark for a further comprehensive evaluation.</p>
<p>Due to the limitation of our budgets, we only translate several datasets of MCQA and NER.We only perform evaluation on models with 7B/8B model parameters.For a comprehensive evaluation, we should also perform comparison in a larger scale.We leave it as a future work to include more translated large-scale datasets in other tasks and evaluation results of larger models.Moreover, though we evaluate these models with four categories of prompt templates, each category only contains one template, which may introduce some fluctuation.To further improve the robustness of our benchmark, we consider including more diverse prompt templates in each prompt category in the future.</p>
<p>Evaluation results on Japanese general domains and biomedical domains in other languages are also valuable for comparison, providing some insights into developing Japanese biomedical LLMs.Such multilingual biomedical benchmark containing di-verse tasks is a promising research direction in the future.However, it is out of our scope in this paper.</p>
<p>Ethics Statement</p>
<p>We follow the licenses of the involved datasets, which are mainly MIT or CC-BY-4.08 .However, we should note that the NRNER and JCSTS datasets are distributed under the Non-Commercial CC-BY-NC-SA-4.0 license9 .In principle, the whole JMedBench should be distributed under a non-commercial license, whereas if it is used for the commercial scenario, these two datasets (i.e., NRNER and JCSTS) should be excluded.</p>
<p>Besides, considering the scale of the existing human-manufactured evaluation datasets, we adopt machine translation system (i.e., GPT-4) to translate some large-scale and high-quality English biomedical datasets into Japanese to fulfill a robust evaluation.However, machine translation system will inevitably generate unfaithful contents.Therefore, those who want to use our datasets to develop faithful biomedical LLMs or biomedical products for real-world application should be aware of this limitation.</p>
<p>Table 5 shows the statistics of involved datasets in the JMedBench.IgakuQA does not have an official training set, while its genre is similar to MedQA.Therefore, we share the training set of MedQA with IgakuQA for a few-shot evaluation.JMMLU-medical only contains the translated testing set, and we also share the training set of translated MMLU-medical-JP with JMMLU-medical.Considering our limited budgets, we only translated 1,000 training samples randomly selected from the original training set of the PubMedQA.As for the datasets derived from JMED-LLM, including EJMMT, MRNER-Medicine, MRNER-Disease, NRNER, CRADE, RRTNM, and SMDIS, we randomly split a small subset from the original dataset for few-shot evaluation.The size of the training set can be found in Table 5.As for the JCSTS, we also randomly split a small subset to be the training set.For the rest of the datasets, we strictly follow the origin setting of the split and use the training set or development set for few-show evaluation.</p>
<p>A.2 Prompt Templates for Data Augmentation</p>
<p>Table 6 shows the prompt template we used when using OpenAI's APIs for translating biomedical MCQA datasets.Besides, Table 7 is the prompt template for translating biomedical NER datasets.</p>
<p>A.3 Bad Cases during NER Dataset Translation</p>
<p>We summarized three main failure types during machine translation: (1) ambiguity of a single word, for example, 'depression' can be considered as a mental illness (うつ病) or pressing down (抑制);</p>
<p>(2) multiple possible expressions of a single word,  for example, 'glucose' can be translated into either グルコース or 血糖;</p>
<p>(3) differences in grammar between English and Japanese.Table 8 shows one bad case for each typical failure type during translating NER datasets.The parts underlined indicate an inconsistency between the entity and the text translation.Although there is a small number of failure cases during the machine translation phase, we still realize that the quality of the translation for both the entities and the text is very high during the manual modification process, which can prove the reliability and the scalability of our data augmentation method.</p>
<p>B Experimental Details B.1 Development in chronological order</p>
<p>We sorted the various models according to their release dates.In chronological order, they are: Llama2-7B (Jul.2023), SwallowLM-7B (Nov.2023), Meditron-7B (Dec.2023), Mistral-7B (May 2024), MMed-Llama3-8B (May 2024), Qwen2-7B (Jun.2024), Llama3-8B (Jul.2024), llm-jp-13B (Sep.2024).Figure 5 illustrates the relationship between model performance and release date.The color of the points represents the corresponding tasks, and the shape represents their models.Colored lines reflect the trend of model performance on each task over time.The figure shows that as time progresses, the performance of models on various tasks is consistently improving, especially for the STS task.Moreover, the improvement in the in-context learning (ICL) capabilities of the models is even more pronounced.</p>
<p>B.2 Ranking of Models</p>
<p>Figure 6 shows the zero-shot and few-shot performance rankings on JMedBench tasks among all involved LLMs.</p>
<p>B.3 Comparison Methods</p>
<p>Detailed information for involved comparison methods is listed in Table 9.</p>
<p>B.4 Prompts for Each Task</p>
<p>Detailed prompt templates for each task are shown in Table 10,11,12,13,and 14.</p>
<p>Figure 1 :
1
Figure 1: Overview of JMedBench</p>
<p>Figure 2 Figure 2 :
22
Figure2includes two radar charts that demonstrate models' zero-shot and few-shot performance on different tasks.Besides, we also rank the model</p>
<p>am a-3 Q w en 2 M is tr al M ed itr on llm -jp Sw al lo w LM M</p>
<p>Figure 3 :
3
Figure 3: Zero-shot performance under different prompt templates.</p>
<p>a-2 Ll am a-3 Q w en 2 M is tr al M ed itr on llm -jp Sw al lo w LM M M ed -L la m</p>
<p>Figure 4 :
4
Figure 4: Few-shot performance under different prompt templates.</p>
<p>Table 1 :
1
Comparison of existing benchmarks.
arXiv:2409.13317v1 [cs.CL] 20 Sep 2024</p>
<p>in the Appendix.Due to the computation resources, we only evaluate LLMs with around 7 ∼ 8B parameters.Llm-jp is a representative LLM that was pre-trained from scratch with Japanese and English texts.Although it does not
Accuracy (%)IGAJMMMedMUSMMedQMMLPubAver (Micro)Zero-shot EvaluationLlama2-7B22.6926.2030.3127.8123.1729.7763.5030.91Llama3-8B26.1935.0931.9432.2126.0036.7762.3034.51Qwen2-7B41.2544.0638.0338.4931.0349.0168.9042.58Mistral-7B25.1930.6830.6028.4423.5732.8268.8032.74Meditron-7B21.9425.6528.3126.3921.9225.6556.5028.56llm-jp-13B31.0036.5130.4631.6625.2935.5473.6035.17SwallowLM-7B27.8829.5029.2627.7322.3929.8870.7031.86MMed-Llama3-8B35.5637.4535.4336.9229.5438.8670.0038.64Few-shot Evaluation</p>
<p>Table 2 :
2
Benchmark results on Japanese biomedical MCQA tasks, including IgakuQA (IGA) and JMMLU-medical (JMM), as well as the translated versions of MedMCQA (MedM), USMLE-QA (USM), MedQA (MedQ), MMLUmedical (MML), and PubMedQA (Pub).We report the highest accuracy among four prompt templates as discussed in Section 3.3.The best and second-best performances are highlighted in bold and underlined, respectively.</p>
<p>Table 3 :
3
Benchmark results on Japanese biomedical NER tasks, including MRNER-Disease (MRD), MRNER-Medicine (MRM) and NRNER (NRN), as well as the translated versions of BC2GM (B2G), BC5Chem (B5C), BC5Disease (B5D), JNLPBA (JNL), and NCBI-Disease (NCB).We report the highest F1-entity score among four prompt templates as discussed in Section 3.3.The best and second-best performances are highlighted in bold and underlined, respectively.</p>
<p>Table 4 :
4
Benchmark results on the rest of other tasks in JMedBench, including Machine Translation (EJMMT), Document Classification (CRADE, RRTNM, SMDIS), and Semantic Text Similarity (JCSTS).The best and second-best performances are highlighted in bold and underlined, respectively.
Metric(en-&gt;ja) EJMMT(ja-&gt;en) EJMMTAverCRADERRTNMSMDIS(Micro) AverJCSTSBLUEAccuracy (%)PearsonZero-shot EvaluationLlama2-7B11.1314.1812.6527.1737.0854.7639.67-0.005Llama3-8B16.7923.6620.2325.0044.9451.1940.380.422Qwen2-7B15.2419.5917.4135.8759.5558.3351.250.636Mistral-7B10.9318.2414.5925.0048.3154.7642.690.110Meditron-7B8.397.227.8130.4352.8154.7646.000.072llm-jp-13B15.1423.1319.1328.2637.0851.1938.840.014SwallowLM-7B19.321.1510.2425.0041.5750.0038.860.056MMed-Llama3-8B23.0017.5020.2526.0955.0655.9545.700.553Few-shot EvaluationLlama2-7B12.8920.1816.5429.3544.9459.5244.610.099Llama3-8B20.2228.5024.3634.7853.9363.1050.600.483Qwen2-7B18.3325.4121.8744.5756.1886.9062.550.625Mistral-7B12.7623.0517.9130.4356.1866.6751.090.378Meditron-7B11.7921.6716.7326.0935.9654.7638.930.067llm-jp-13B27.9328.9628.4536.9646.0767.8650.290.144SwallowLM-7B23.2323.0723.1530.4344.9459.5244.970.039MMed-Llama3-8B25.5628.7327.1434.7857.3067.8653.310.515</p>
<p>Table 5 :
5
Statistics of involved datasets in JMedBench.</p>
<p>https://huggingface.co/datasets/Coldog2333/ JMedBench
https://github.com/nlp-waseda/JMMLU
https://openai.com/index/chatgpt/
We used gpt-4-0613 checkpoint.
We used gpt-3.5-turbo-1106 checkpoint.
https://github.com/nlp-waseda/JMMLU
https://github.com/sociocom/JMED-LLM
https://creativecommons.org/licenses/by/4.0/ deed.en
https://creativecommons.org/licenses/ by-nc-sa/4.0/deed.en
AcknowledgmentsThis work was supported by JST SPRING, Grant Number JPMJSP2108 and by Cross-ministerial Strategic Innovation Promotion Program (SIP) on "Integrated Health Care System" Grant Number JPJ012425.Prompt template for translating MCQA datasets #System Message You are an excellent machine translation system for the biomedical domain.Translate Japanese to English.Input and output should be in the sameJSONAmbiguity of wordsOriginal Text: Depression is a major clinical feature of Parkinson' s disease.Original Entity: depression Translated Text: うつ病はパーキンソン病の主要な臨床的特徴です。 Translated Entity: 抑制 Explanation: According to Cambridge English Dictionary, "depression" has multiple meanings: a mental illness (うつ病), or pressing down (抑制).Multiple Expressions of a Single WordOriginal Text: After recovery from hyperglycaemia, he remained polyuric despite normal blood glucose concentrations; water deprivation testing indicated nephrogenic diabetes insipidus, likely to be lithium-induced.Original Entity: glucose Translated Text: 高血糖からの回復後、彼は正常な血糖濃度にもかかわらず多尿であ り続けました。水制限テストは、リチウム誘発性である可能性のある尿崩症を示し ました Translated Entity: グルコース Explanation: "Glucose" can be translated into either "グルコース" or "血糖".Difference in GrammarOriginal Text: Molecular cloning and characterization of two genes encoding gp138, a cell surface glycoprotein involved in the sexual cell fusion of Dictyostelium discoideum.Original Entity: genes encoding gp138 Translated Text: "Dictyostelium discoideumの性的細胞融合に関与する細胞表面糖タン パク質であるgp138をコードする2つの遺伝子の分子クローニングと特性評価。 Translated Entity: gp138をコードする遺伝子 Explanation: Due to grammatical differences, the quantifier "2" is inserted between "genes" and "encoding gp138" when translating the text.
. Mistral-7b , </p>
<p>. Meditron-7b , </p>
<p>. Swallowlm-7b , </p>
<p>. Mistral-7b , </p>
<p>. Meditron-7b , </p>
<p>. Swallowlm-7b , </p>
<p>Josh References, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Akiko Aizawa, Eiji Aramaki, Bowen Chen, Fei Cheng, Hiroyuki Deguchi, Rintaro Enomoto, Kazuki Fujii, Kensuke Fukumoto, Takuya Fukushima, Namgi Han, arXiv:2407.03963Llm-jp: A cross-organizational project for the research and development of fully open japanese llms. 2024arXiv preprint</p>
<p>Large language models as computational linguistics tools: A comparative analysis of chatgpt and google machine translations. Mohammad Awad, Alafnan , Journal of Artificial Intelligence and Technology. 2024</p>
<p>Performance of large language models in numerical vs. Eden Avnat, Michal Levy, Daniel Herstain, Elia Yanko, Ben Daniel, Michal Tzuchman Joya, Dafna Katz, Sahar Eshel, Yael Laros, Shahar Dagan, Barami, arXiv:2406.03855semantic medical knowledge: Benchmarking on evidence-based q&amp;as. 2024arXiv preprint</p>
<p>Language models are few-shot learners. Tom B Brown, ArXiv:2005.141652020arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, arXiv:2311.16079Meditron-70b: Scaling medical pretraining for large language models. 2023arXiv preprint</p>
<p>Introduction to the bio-entity recognition task at jnlpba. Nigel Collier, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, Jin-Dong Kim, Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications. the International Joint Workshop on Natural Language Processing in Biomedicine and its ApplicationsNLPBA/BioNLP2004</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee2009</p>
<p>Ncbi disease corpus: a resource for disease name recognition and concept normalization. Rezarta Islamaj Dogan, Robert Leaman, Zhiyong Lu, Journal of biomedical informatics. 472014</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Investigating web corpus filtering methods for language model development in japanese. Rintaro Enomoto, Arseny Tolmachev, Takuro Niitsuma, Shuhei Kurita, Daisuke Kawahara, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20244Student Research Workshop)</p>
<p>Chengguang Gan and Tatsunori Mori. 2023. Sensitivity and robustness of large language models to prompt template in Japanese text classification tasks. Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Hiroki Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, Sakae Mizuki, Rio Yokota, Naoaki Okazaki, arXiv:2404.17790Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation. the 37th Pacific Asia Conference on Language, Information and ComputationHong Kong, ChinaAssociation for Computational Linguistics2024arXiv preprintContinual pre-training for cross-lingual llm adaptation: Enhancing japanese language capabilities</p>
<p>Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, Mcdonell, 10.5281/zenodo.12608602Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite. Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf; Ben Wang, Kevin Wangand Andy Zou. 2024. A framework for few-shot language model evaluation</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon, ACM Transactions on Computing for Healthcare. 312021</p>
<p>Medalpaca-an open-source collection of medical conversational ai models and training data. Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, Keno K Bressem, arXiv:2304.082472023arXiv preprint</p>
<p>Fine-grained error analysis on english-to-japanese machine translation in the medical domain. Takeshi Hayakawa, Yuki Arase, Proceedings of the 22nd Annual Conference of the European Association for Machine Translation. the 22nd Annual Conference of the European Association for Machine Translation2020</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Exploring humanlike translation strategy with large language models. Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, Xing Wang, 2024Transactions of the Association for Computational Linguistics12</p>
<p>Aligning ai with shared human values. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, Jacob Steinhardt, 2021aProceedings of the International Conference on Learning Representations (ICLR</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021b</p>
<p>How good are gpt models at machine translation? a comprehensive evaluation. Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young , Jin Kim, Mohamed Afify, Hany Hassan Awadalla, arXiv:2302.092102023arXiv preprint</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, Peter Szolovits, Applied Sciences. 111464212021</p>
<p>PubMedQA: A dataset for biomedical research question answering. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, Xinghua Lu, 10.18653/v1/D19-1259Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Jungo Kasai, Yuhei Kasai, Keisuke Sakaguchi, Yutaro Yamada, Dragomir Radev, arXiv:2303.18027Evaluating gpt-4 and chatgpt on japanese medical licensing examinations. 2023arXiv preprint</p>
<p>Imagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hin, Advances in neural information processing systems. 201225</p>
<p>Jglue: Japanese general language understanding evaluation. Kentaro Kurihara, Daisuke Kawahara, Tomohide Shibata, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation Conference2022</p>
<p>DrBenchmark: A large language understanding evaluation benchmark for French biomedical domain. Yanis Labrak, Adrien Bazoge, Oumaima El Khettari, Mickael Rouvier, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). Emmanuel Morin, Pierre-Antoine Gourraud, Richard Dufour, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024Pacome Constant Dit Beaufils, Natalia Grabar, Béatrice Daille, Solen Quiniou</p>
<p>Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, Zhiyong Lu, 2016. 2016Database</p>
<p>Can large language models reason about medical questions? Patterns. Valentin Liévin, Egeberg Christoffer, Andreas Geert Hother, Ole Motzfeldt, Winther, 20245</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/v1/2022.emnlp-main.759Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Semantic textual similarity in japanese clinical domain texts using bert. Faith Wavinya Mutinda, Shuntaro Yada, Shoko Wakamiya, Eiji Aramaki, Methods of Information in Medicine. 60S 012021</p>
<p>Can generalist foundation models outcompete special-purpose tuning? case study in medicine. Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolò Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, 2023CoRR</p>
<p>Medmcqa: A large-scale multisubject multi-choice dataset for medical domain question answering. Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu, Proceedings of the Conference on Health, Inference, and Learning. the Conference on Health, Inference, and LearningPMLR2022174</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, Weidi Xie, arXiv:2402.13963Towards building multilingual language model for medicine. 2024arXiv preprint</p>
<p>On context utilization in summarization with large language models. Aixin Mathieu Ravaut, Nancy Sun, Shafiq Chen, Joty, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Leveraging large language models for multiple choice question answering. Joshua Robinson, David Wingate, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Google translate vs. chatgpt: Can non-language professionals trust them for specialized translation?. Lucía Sanz, - Valdivieso, Belén López-Arroyo, Proceedings of the International Conference HiT-IT. the International Conference HiT-IT2023</p>
<p>Numeric magnitude comparison effects in large language models. Raj Shah, Vijay Marupudi, Reba Koenen, 10.18653/v1/2023.findings-acl.383Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023Khushi Bhardwaj, and Sashank Varma</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 62079722023</p>
<p>Overview of biocreative ii gene mention recognition. Larry Smith, Lorraine K Tanabe, Rie Johnson Nee Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger, Christoph M Friedrich, Kuzman Ganchev, Genome biology. 92008</p>
<p>Efficientnet: Rethinking model scaling for convolutional neural networks. Mingxing Tan, arXiv:1905.119462019arXiv preprint</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. Wang, arXiv:1804.074612018arXiv preprint</p>
<p>A comprehensive survey of continual learning: theory, method and application. Liyuan Wang, Xingxing Zhang, Hang Su, Jun Zhu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024</p>
<p>Pmc-llama: toward building open-source language models for medicine. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, Yanfeng Wang, Journal of the American Medical Informatics Association. e0452024</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, arXiv:2407.10671Qwen2 technical report. 2024aarXiv preprint</p>
<p>Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue. Songhua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, Hongying Zan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024b38</p>
<p>Multilingual sentence-t5: Scalable sentence encoders for multilingual applications. Chihiro Yano, Akihiko Fukuchi, Shoko Fukasawa, Hideyuki Tachibana, Yotaro Watanabe, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>
<p>Sentiment analysis in the era of large language models: A reality check. Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Pan, Lidong Bing, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>Context-faithful prompting for large language models. Wenxuan Zhou, Sheng Zhang, Hoifung Poon, Muhao Chen, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>A Benchmark Construction Details A.1 Further details of datasets in the JMedBench. </p>            </div>
        </div>

    </div>
</body>
</html>
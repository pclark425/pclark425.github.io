<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3977 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3977</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3977</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-3def68bd0f856886d34272840a7f81588f2bc082</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3def68bd0f856886d34272840a7f81588f2bc082" target="_blank">Survey of Hallucination in Natural Language Generation</a></p>
                <p><strong>Paper Venue:</strong> ACM Computing Surveys</p>
                <p><strong>Paper TL;DR:</strong> A broad overview of the research progress and challenges in the hallucination problem in NLG is provided, including task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation.</p>
                <p><strong>Paper Abstract:</strong> Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3977.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3977.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Statistical metrics (n-gram overlap)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Statistical lexical overlap metrics (e.g., ROUGE, BLEU, METEOR, PARENT, Knowledge F1, BVSS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Lexical-overlap based evaluation that measures n-gram precision/recall or bag-of-words similarity between generated text and one or more references (source and/or target); used as a proxy for faithfulness and adequacy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Lexical overlap/adequacy with reference text; proxy for faithfulness and information preservation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compute precision/recall/F1 or bag-of-words similarity between generated output and reference(s); some variants (e.g., PARENT) align n-grams to both source and target when target may contain extra information.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used broadly across summarization, data-to-text, translation tasks; examples mentioned include WIKIBIO for table-to-text issues and task-specific references.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>ROUGE/BLEU/METEOR scores; PARENT F1; BVSS sentence adequacy scores; reported statistics of correlation with human judgments (noted as low for hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Typically none for metric computation, but metrics are compared against human judgments to assess correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Insensitive to semantic/paraphrastic variation; poor correlation with human judgments for hallucination detection; cannot identify extrinsic hallucinations or locate hallucinatory substrings reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey reports conventional lexical metrics often fail to detect hallucination (e.g., SOTA summarizers can have ~25% hallucinated summaries while still scoring highly on ROUGE); PARENT-type metrics improve by referencing source but still limited to lexical matches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of Hallucination in Natural Language Generation', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3977.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3977.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IE-based metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Information Extraction (IE)-based factuality metrics (relation triple extraction and matching)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extract structured relation tuples (subject, relation, object) from generated text and from source/reference, then compare tuples to detect mismatches indicating hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Exact or entailed alignment of extracted facts/triples between generation and source/reference (factual consistency and faithfulness).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Apply IE/NRE/NLP pipeline to extract triples from generation and source; compute match/mismatch counts and derive precision/recall/F1 for verifiable facts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied in summarization, data-to-text, captioning; paper references entity-based variants and NER-based approaches for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Tuple-level precision/recall/F1; counts of mismatched relation triples.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Used to validate or create datasets; human annotators may check extracted triples or correct IE outputs for evaluation calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Error propagation from imperfect IE models; only captures extractable relational facts (misses implicit or complex claims); sensitive to IE coverage and schema.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>IE-based metrics provide finer-grained factual checks than raw n-gram overlap but suffer when IE extractors are inaccurate; entity-based NER metrics can be more robust in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of Hallucination in Natural Language Generation', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3977.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3977.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA-based metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question-Answering based factual consistency metrics (e.g., FEQA, QAGS, QuestEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate questions from the candidate generation, answer those questions using the source (and/or the generation), and compare answers to measure factual consistency between generation and source.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Consistency of answers produced from source vs answers from generated text (measures whether generated claims are supported by source).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>1) Use Question Generation (QG) on generated text to produce QA pairs; 2) run a QA model on source to answer generated questions; 3) compare answer similarity (exact match/soft similarity) to derive a faithfulness score.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied across summarization, dialogue, and data-to-text; referenced QA-metric implementations evaluated on summarization tasks and in meta-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Answer overlap/accuracy, aggregated QA-consistency scores; correlation with human judgments (reported higher than some baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>QA metrics are automatic but are meta-evaluated against human judgments; humans are used to create or validate question templates and assess correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality depends on QG and QA models (error propagation); cannot always localize specific hallucinatory spans; question generation quality strongly affects reliability; may miss unasked claims.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>QA-based metrics achieve substantially higher correlation with human judgments of faithfulness than traditional lexical baselines in summarization, but their reliability varies with the underlying QG/QA components and question selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of Hallucination in Natural Language Generation', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3977.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3977.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLI-based metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural Language Inference (NLI)/entailment-based metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use NLI models to estimate entailment/contradiction/neutral relations between source (premise) and generated text (hypothesis); entailment probability used as a faithfulness score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Degree to which generated claims are entailed by the source (faithfulness) as predicted by NLI models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Run sentence- or dependency-level NLI models on (source, generated sentence) pairs to compute entailment probabilities and aggregate to a faithfulness score.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied in summarization and dialogue; paper discusses MNLI/FEVER as NLI sources and efforts to adapt NLI to long-premise summarization contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Entailment/contradiction/neutral percentages; aggregated entailment probability; correlations with human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Humans annotate entailment/factuality data used to fine-tune or evaluate NLI models; human-labeled datasets created for faithfulness classification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Off-the-shelf NLI models transfer poorly to long-document summarization; sentence-level NLI cannot localize errors well; training data distribution mismatch (premise length) reduces effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>NLI-based metrics are more robust than token matching to lexical variability but suffer domain/length transfer issues; adaptations (long-premise NLI, sentence segmentation) improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of Hallucination in Natural Language Generation', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3977.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3977.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-based metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-model-based support detection (conditional vs unconditional LM losses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Compare token-level or span-level perplexity/loss from an unconditional LM (trained on targets) against a conditional LM (trained on source+target); lower loss under unconditional LM indicates token is unsupported by source and possibly hallucinatory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Whether tokens are better predicted without conditioning on the source (suggesting they are not supported by source).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Forced-path decoding computes per-token loss from both LMs and flags tokens as hallucinatory when unconditional LM loss is lower; aggregate ratio of flagged tokens used as hallucination score.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied primarily in data-to-text and summarization evaluations; depends on corpora used to train the conditional and unconditional LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Token-level hallucination ratio (flagged tokens / total tokens); per-token loss differences.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human judgments used to calibrate thresholds and validate flagged tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Sensitive to LM training data and domain; can mislabel common/unlikely tokens; relies on differences in distribution rather than explicit verification against facts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LM-based methods can identify unsupported tokens but are noisy and require careful calibration and validation against human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of Hallucination in Natural Language Generation', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3977.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3977.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Faithfulness classification / task-specific classifiers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised faithfulness classifiers and span-level detectors (e.g., FactCC, synthetic-hallucination-trained models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train classifiers on synthetic or annotated data to detect whether generated outputs (or spans within them) are faithful to the source; often multi-task with extraction of supporting/inconsistent spans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary or fine-grained labels of faithfulness at sentence/span level; ability to locate inconsistent spans and classify outputs as faithful/unfaithful.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Create synthetic negative examples (inserted hallucinations) or human-annotated corpora; train models to classify outputs or label inconsistent spans; evaluate via classification metrics and correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Task-specific synthetic corpora derived from summarization datasets; faithfulness-annotated dialogue corpora (e.g., annotated Wizard-of-Wikipedia responses) are cited.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>ROC AUC, F1, accuracy for detection; span-level localization metrics; correlation with human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human annotators create or validate training/validation labels; human evaluation used for meta-evaluation of classifier outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Synthetic negative generation may not cover real-world hallucinations; classifiers can overfit to synthetic patterns; datasets are task-specific, limiting generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Task-specific classifiers (e.g., FactCC-style) outperform general NLI in some settings for detecting factual inconsistency; span-level detectors improve explainability but depend on quality of training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of Hallucination in Natural Language Generation', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3977.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3977.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human annotation and judgment (scoring and pairwise comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human raters score generated outputs for hallucination/factual consistency (rating scales) or compare generated outputs against baselines/references to determine relative faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human judgment of faithfulness, factual consistency, correctness, informativeness, or hallucination level (sometimes split intrinsic vs extrinsic).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Two main paradigms: (1) scoring—annotators rate hallucination level on a scale; (2) comparing—annotators compare outputs to references or to each other. Fine-grained labels or separate intrinsic/extrinsic assessments may be used.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied across tasks; manual annotations are used to build faithfulness datasets and to meta-evaluate automatic metrics (e.g., FRANK-style evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Inter-annotator agreement, rating distributions, percentage of examples labeled hallucinated (e.g., ~25% summaries flagged), correlation with automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High—expert annotators or crowdworkers perform the primary judgments; experts sometimes needed for domain-specific verification (medical, numerical facts).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Expensive and time-consuming, especially for extrinsic (world-knowledge) fact-checking; annotator agreement can be low for ambiguous claims; fine-grained localization is laborious.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human evaluation remains a gold standard; many automatic metrics are benchmarked against human judgments and found to have varying correlation, motivating improved metric design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of Hallucination in Natural Language Generation', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3977.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3977.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks / meta-evaluation suites</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation benchmarks and meta-evaluation resources (e.g., FRANK, TRUE, synthetic faithfulness corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Datasets and meta-evaluation suites designed to assess and compare hallucination metrics and detectors via human annotations and diagnostic data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Metric-level ability to detect hallucinated examples (example-level accuracy), correlation with human judgments across tasks, and ROC AUC for detection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compile human-annotated datasets of faithful vs hallucinated outputs; evaluate automatic metrics by ROC AUC, correlation with human labels, and localization quality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>FRANK (faithfulness survey/meta-eval for summarization metrics), TRUE (reports Area Under ROC Curve for detection), WIKIBIO (dataset illustrating source-target divergence), Wizard-of-Wikipedia (dialogue faithfulness annotations), QMSum (dialogue summarization inconsistencies) referenced.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>ROC AUC, correlation coefficients with human judgments, detection accuracy, and dataset-level statistics (e.g., % of noisy/mismatched pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human annotations used to create the benchmark labels and to perform meta-evaluation of metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Benchmarks often task-specific, limited coverage of extrinsic hallucination (world knowledge), and creation of high-quality annotations is costly.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Meta-evaluations show many automatic metrics have limited or inconsistent correlation with humans; TRUE and FRANK indicate room for improvement and need for task-agnostic, fine-grained metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of Hallucination in Natural Language Generation', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3977.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3977.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reported limitations and open challenges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations in hallucination/factuality evaluation methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-cutting limitations described in the survey that affect the evaluation of generated outputs (including potential LLM-generated scientific theories): error propagation, poor transfer, inability to verify extrinsic claims, dataset divergence, and human evaluation costs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>N/A (this is a synthesis of limitations affecting evaluation criteria like faithfulness, factuality, and verifiability).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>N/A (describes weaknesses of existing methods: lexical, IE, QA, NLI, LM, supervised classifiers, and human eval).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>WIKIBIO (example of source-reference divergence); FRANK and TRUE cited as meta-evaluation resources highlighting metric shortcomings.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Examples: 25% of SOTA summarization outputs hallucinate despite high ROUGE; metrics report low correlation in some settings; synthetic-training helps but overfits to artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human validation is required to judge extrinsic factuality (world knowledge) and to create reliable labels; domain experts necessary for specialized scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Key challenges: (1) extrinsic hallucination requires evidence retrieval and fact-checking against world knowledge and is costly; (2) automatic metrics suffer from propagated errors (IE/QG/QA/NLI models); (3) transfer/generalization across tasks and long-document settings is weak; (4) fine-grained localization of hallucinatory substrings is hard; (5) numeral and small-edit sensitivity (negation, numbers) remains problematic.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey concludes current automatic metrics inadequately capture many forms of hallucination, especially extrinsic claims and subtle factual changes, so human-in-the-loop fact-checking and better evidence retrieval/verification pipelines are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of Hallucination in Natural Language Generation', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PARENT <em>(Rating: 2)</em></li>
                <li>FEQA <em>(Rating: 2)</em></li>
                <li>QAGS <em>(Rating: 2)</em></li>
                <li>QuestEval <em>(Rating: 2)</em></li>
                <li>FactCC <em>(Rating: 2)</em></li>
                <li>FRANK <em>(Rating: 2)</em></li>
                <li>TRUE <em>(Rating: 2)</em></li>
                <li>WIKIBIO <em>(Rating: 1)</em></li>
                <li>Wizard-of-Wikipedia <em>(Rating: 1)</em></li>
                <li>PersonaChat <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3977",
    "paper_id": "paper-3def68bd0f856886d34272840a7f81588f2bc082",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "Statistical metrics (n-gram overlap)",
            "name_full": "Statistical lexical overlap metrics (e.g., ROUGE, BLEU, METEOR, PARENT, Knowledge F1, BVSS)",
            "brief_description": "Lexical-overlap based evaluation that measures n-gram precision/recall or bag-of-words similarity between generated text and one or more references (source and/or target); used as a proxy for faithfulness and adequacy.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Lexical overlap/adequacy with reference text; proxy for faithfulness and information preservation.",
            "evaluation_methods": "Compute precision/recall/F1 or bag-of-words similarity between generated output and reference(s); some variants (e.g., PARENT) align n-grams to both source and target when target may contain extra information.",
            "benchmark_or_dataset": "Used broadly across summarization, data-to-text, translation tasks; examples mentioned include WIKIBIO for table-to-text issues and task-specific references.",
            "metrics_reported": "ROUGE/BLEU/METEOR scores; PARENT F1; BVSS sentence adequacy scores; reported statistics of correlation with human judgments (noted as low for hallucination).",
            "human_involvement": "Typically none for metric computation, but metrics are compared against human judgments to assess correlation.",
            "limitations_or_challenges": "Insensitive to semantic/paraphrastic variation; poor correlation with human judgments for hallucination detection; cannot identify extrinsic hallucinations or locate hallucinatory substrings reliably.",
            "llm_theory_example": null,
            "evaluation_results": "Survey reports conventional lexical metrics often fail to detect hallucination (e.g., SOTA summarizers can have ~25% hallucinated summaries while still scoring highly on ROUGE); PARENT-type metrics improve by referencing source but still limited to lexical matches.",
            "uuid": "e3977.0",
            "source_info": {
                "paper_title": "Survey of Hallucination in Natural Language Generation",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "IE-based metrics",
            "name_full": "Information Extraction (IE)-based factuality metrics (relation triple extraction and matching)",
            "brief_description": "Extract structured relation tuples (subject, relation, object) from generated text and from source/reference, then compare tuples to detect mismatches indicating hallucination.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Exact or entailed alignment of extracted facts/triples between generation and source/reference (factual consistency and faithfulness).",
            "evaluation_methods": "Apply IE/NRE/NLP pipeline to extract triples from generation and source; compute match/mismatch counts and derive precision/recall/F1 for verifiable facts.",
            "benchmark_or_dataset": "Applied in summarization, data-to-text, captioning; paper references entity-based variants and NER-based approaches for robustness.",
            "metrics_reported": "Tuple-level precision/recall/F1; counts of mismatched relation triples.",
            "human_involvement": "Used to validate or create datasets; human annotators may check extracted triples or correct IE outputs for evaluation calibration.",
            "limitations_or_challenges": "Error propagation from imperfect IE models; only captures extractable relational facts (misses implicit or complex claims); sensitive to IE coverage and schema.",
            "llm_theory_example": null,
            "evaluation_results": "IE-based metrics provide finer-grained factual checks than raw n-gram overlap but suffer when IE extractors are inaccurate; entity-based NER metrics can be more robust in practice.",
            "uuid": "e3977.1",
            "source_info": {
                "paper_title": "Survey of Hallucination in Natural Language Generation",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "QA-based metrics",
            "name_full": "Question-Answering based factual consistency metrics (e.g., FEQA, QAGS, QuestEval)",
            "brief_description": "Generate questions from the candidate generation, answer those questions using the source (and/or the generation), and compare answers to measure factual consistency between generation and source.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Consistency of answers produced from source vs answers from generated text (measures whether generated claims are supported by source).",
            "evaluation_methods": "1) Use Question Generation (QG) on generated text to produce QA pairs; 2) run a QA model on source to answer generated questions; 3) compare answer similarity (exact match/soft similarity) to derive a faithfulness score.",
            "benchmark_or_dataset": "Applied across summarization, dialogue, and data-to-text; referenced QA-metric implementations evaluated on summarization tasks and in meta-evaluation.",
            "metrics_reported": "Answer overlap/accuracy, aggregated QA-consistency scores; correlation with human judgments (reported higher than some baselines).",
            "human_involvement": "QA metrics are automatic but are meta-evaluated against human judgments; humans are used to create or validate question templates and assess correlation.",
            "limitations_or_challenges": "Quality depends on QG and QA models (error propagation); cannot always localize specific hallucinatory spans; question generation quality strongly affects reliability; may miss unasked claims.",
            "llm_theory_example": null,
            "evaluation_results": "QA-based metrics achieve substantially higher correlation with human judgments of faithfulness than traditional lexical baselines in summarization, but their reliability varies with the underlying QG/QA components and question selection.",
            "uuid": "e3977.2",
            "source_info": {
                "paper_title": "Survey of Hallucination in Natural Language Generation",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "NLI-based metrics",
            "name_full": "Natural Language Inference (NLI)/entailment-based metrics",
            "brief_description": "Use NLI models to estimate entailment/contradiction/neutral relations between source (premise) and generated text (hypothesis); entailment probability used as a faithfulness score.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Degree to which generated claims are entailed by the source (faithfulness) as predicted by NLI models.",
            "evaluation_methods": "Run sentence- or dependency-level NLI models on (source, generated sentence) pairs to compute entailment probabilities and aggregate to a faithfulness score.",
            "benchmark_or_dataset": "Applied in summarization and dialogue; paper discusses MNLI/FEVER as NLI sources and efforts to adapt NLI to long-premise summarization contexts.",
            "metrics_reported": "Entailment/contradiction/neutral percentages; aggregated entailment probability; correlations with human annotations.",
            "human_involvement": "Humans annotate entailment/factuality data used to fine-tune or evaluate NLI models; human-labeled datasets created for faithfulness classification.",
            "limitations_or_challenges": "Off-the-shelf NLI models transfer poorly to long-document summarization; sentence-level NLI cannot localize errors well; training data distribution mismatch (premise length) reduces effectiveness.",
            "llm_theory_example": null,
            "evaluation_results": "NLI-based metrics are more robust than token matching to lexical variability but suffer domain/length transfer issues; adaptations (long-premise NLI, sentence segmentation) improve performance.",
            "uuid": "e3977.3",
            "source_info": {
                "paper_title": "Survey of Hallucination in Natural Language Generation",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "LM-based metrics",
            "name_full": "Language-model-based support detection (conditional vs unconditional LM losses)",
            "brief_description": "Compare token-level or span-level perplexity/loss from an unconditional LM (trained on targets) against a conditional LM (trained on source+target); lower loss under unconditional LM indicates token is unsupported by source and possibly hallucinatory.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Whether tokens are better predicted without conditioning on the source (suggesting they are not supported by source).",
            "evaluation_methods": "Forced-path decoding computes per-token loss from both LMs and flags tokens as hallucinatory when unconditional LM loss is lower; aggregate ratio of flagged tokens used as hallucination score.",
            "benchmark_or_dataset": "Applied primarily in data-to-text and summarization evaluations; depends on corpora used to train the conditional and unconditional LMs.",
            "metrics_reported": "Token-level hallucination ratio (flagged tokens / total tokens); per-token loss differences.",
            "human_involvement": "Human judgments used to calibrate thresholds and validate flagged tokens.",
            "limitations_or_challenges": "Sensitive to LM training data and domain; can mislabel common/unlikely tokens; relies on differences in distribution rather than explicit verification against facts.",
            "llm_theory_example": null,
            "evaluation_results": "LM-based methods can identify unsupported tokens but are noisy and require careful calibration and validation against human judgments.",
            "uuid": "e3977.4",
            "source_info": {
                "paper_title": "Survey of Hallucination in Natural Language Generation",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Faithfulness classification / task-specific classifiers",
            "name_full": "Supervised faithfulness classifiers and span-level detectors (e.g., FactCC, synthetic-hallucination-trained models)",
            "brief_description": "Train classifiers on synthetic or annotated data to detect whether generated outputs (or spans within them) are faithful to the source; often multi-task with extraction of supporting/inconsistent spans.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Binary or fine-grained labels of faithfulness at sentence/span level; ability to locate inconsistent spans and classify outputs as faithful/unfaithful.",
            "evaluation_methods": "Create synthetic negative examples (inserted hallucinations) or human-annotated corpora; train models to classify outputs or label inconsistent spans; evaluate via classification metrics and correlation with human judgments.",
            "benchmark_or_dataset": "Task-specific synthetic corpora derived from summarization datasets; faithfulness-annotated dialogue corpora (e.g., annotated Wizard-of-Wikipedia responses) are cited.",
            "metrics_reported": "ROC AUC, F1, accuracy for detection; span-level localization metrics; correlation with human judgment.",
            "human_involvement": "Human annotators create or validate training/validation labels; human evaluation used for meta-evaluation of classifier outputs.",
            "limitations_or_challenges": "Synthetic negative generation may not cover real-world hallucinations; classifiers can overfit to synthetic patterns; datasets are task-specific, limiting generalization.",
            "llm_theory_example": null,
            "evaluation_results": "Task-specific classifiers (e.g., FactCC-style) outperform general NLI in some settings for detecting factual inconsistency; span-level detectors improve explainability but depend on quality of training data.",
            "uuid": "e3977.5",
            "source_info": {
                "paper_title": "Survey of Hallucination in Natural Language Generation",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Human evaluation",
            "name_full": "Human annotation and judgment (scoring and pairwise comparison)",
            "brief_description": "Human raters score generated outputs for hallucination/factual consistency (rating scales) or compare generated outputs against baselines/references to determine relative faithfulness.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Human judgment of faithfulness, factual consistency, correctness, informativeness, or hallucination level (sometimes split intrinsic vs extrinsic).",
            "evaluation_methods": "Two main paradigms: (1) scoring—annotators rate hallucination level on a scale; (2) comparing—annotators compare outputs to references or to each other. Fine-grained labels or separate intrinsic/extrinsic assessments may be used.",
            "benchmark_or_dataset": "Applied across tasks; manual annotations are used to build faithfulness datasets and to meta-evaluate automatic metrics (e.g., FRANK-style evaluations).",
            "metrics_reported": "Inter-annotator agreement, rating distributions, percentage of examples labeled hallucinated (e.g., ~25% summaries flagged), correlation with automatic metrics.",
            "human_involvement": "High—expert annotators or crowdworkers perform the primary judgments; experts sometimes needed for domain-specific verification (medical, numerical facts).",
            "limitations_or_challenges": "Expensive and time-consuming, especially for extrinsic (world-knowledge) fact-checking; annotator agreement can be low for ambiguous claims; fine-grained localization is laborious.",
            "llm_theory_example": null,
            "evaluation_results": "Human evaluation remains a gold standard; many automatic metrics are benchmarked against human judgments and found to have varying correlation, motivating improved metric design.",
            "uuid": "e3977.6",
            "source_info": {
                "paper_title": "Survey of Hallucination in Natural Language Generation",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Benchmarks / meta-evaluation suites",
            "name_full": "Evaluation benchmarks and meta-evaluation resources (e.g., FRANK, TRUE, synthetic faithfulness corpora)",
            "brief_description": "Datasets and meta-evaluation suites designed to assess and compare hallucination metrics and detectors via human annotations and diagnostic data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Metric-level ability to detect hallucinated examples (example-level accuracy), correlation with human judgments across tasks, and ROC AUC for detection.",
            "evaluation_methods": "Compile human-annotated datasets of faithful vs hallucinated outputs; evaluate automatic metrics by ROC AUC, correlation with human labels, and localization quality.",
            "benchmark_or_dataset": "FRANK (faithfulness survey/meta-eval for summarization metrics), TRUE (reports Area Under ROC Curve for detection), WIKIBIO (dataset illustrating source-target divergence), Wizard-of-Wikipedia (dialogue faithfulness annotations), QMSum (dialogue summarization inconsistencies) referenced.",
            "metrics_reported": "ROC AUC, correlation coefficients with human judgments, detection accuracy, and dataset-level statistics (e.g., % of noisy/mismatched pairs).",
            "human_involvement": "Human annotations used to create the benchmark labels and to perform meta-evaluation of metrics.",
            "limitations_or_challenges": "Benchmarks often task-specific, limited coverage of extrinsic hallucination (world knowledge), and creation of high-quality annotations is costly.",
            "llm_theory_example": null,
            "evaluation_results": "Meta-evaluations show many automatic metrics have limited or inconsistent correlation with humans; TRUE and FRANK indicate room for improvement and need for task-agnostic, fine-grained metrics.",
            "uuid": "e3977.7",
            "source_info": {
                "paper_title": "Survey of Hallucination in Natural Language Generation",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Reported limitations and open challenges",
            "name_full": "Limitations in hallucination/factuality evaluation methods",
            "brief_description": "Cross-cutting limitations described in the survey that affect the evaluation of generated outputs (including potential LLM-generated scientific theories): error propagation, poor transfer, inability to verify extrinsic claims, dataset divergence, and human evaluation costs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "N/A (this is a synthesis of limitations affecting evaluation criteria like faithfulness, factuality, and verifiability).",
            "evaluation_methods": "N/A (describes weaknesses of existing methods: lexical, IE, QA, NLI, LM, supervised classifiers, and human eval).",
            "benchmark_or_dataset": "WIKIBIO (example of source-reference divergence); FRANK and TRUE cited as meta-evaluation resources highlighting metric shortcomings.",
            "metrics_reported": "Examples: 25% of SOTA summarization outputs hallucinate despite high ROUGE; metrics report low correlation in some settings; synthetic-training helps but overfits to artifacts.",
            "human_involvement": "Human validation is required to judge extrinsic factuality (world knowledge) and to create reliable labels; domain experts necessary for specialized scientific claims.",
            "limitations_or_challenges": "Key challenges: (1) extrinsic hallucination requires evidence retrieval and fact-checking against world knowledge and is costly; (2) automatic metrics suffer from propagated errors (IE/QG/QA/NLI models); (3) transfer/generalization across tasks and long-document settings is weak; (4) fine-grained localization of hallucinatory substrings is hard; (5) numeral and small-edit sensitivity (negation, numbers) remains problematic.",
            "llm_theory_example": null,
            "evaluation_results": "Survey concludes current automatic metrics inadequately capture many forms of hallucination, especially extrinsic claims and subtle factual changes, so human-in-the-loop fact-checking and better evidence retrieval/verification pipelines are needed.",
            "uuid": "e3977.8",
            "source_info": {
                "paper_title": "Survey of Hallucination in Natural Language Generation",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PARENT",
            "rating": 2
        },
        {
            "paper_title": "FEQA",
            "rating": 2
        },
        {
            "paper_title": "QAGS",
            "rating": 2
        },
        {
            "paper_title": "QuestEval",
            "rating": 2
        },
        {
            "paper_title": "FactCC",
            "rating": 2
        },
        {
            "paper_title": "FRANK",
            "rating": 2
        },
        {
            "paper_title": "TRUE",
            "rating": 2
        },
        {
            "paper_title": "WIKIBIO",
            "rating": 1
        },
        {
            "paper_title": "Wizard-of-Wikipedia",
            "rating": 1
        },
        {
            "paper_title": "PersonaChat",
            "rating": 1
        }
    ],
    "cost": 0.014669,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Survey of Hallucination in Natural Language Generation</h1>
<p>ZIWEI JI, NAYEON LEE, RITA FRIESKE, TIEZHENG YU, DAN SU, YAN XU, ETSUKO ISHII, YEJIN BANG, DELONG CHEN, WENLIANG DAI, HO SHU CHAN, ANDREA MADOTTO, and PASCALE FUNG, Center for Artificial Intelligence Research (CAiRE), Hong Kong University of Science and Technology, Hong Kong</p>
<p>Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.</p>
<p>In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation; and (3) hallucinations in large language models (LLMs) ${ }^{1}$. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.
CCS Concepts: $\cdot$ Computing methodologies $\rightarrow$ Natural language generation; Neural networks.
Additional Key Words and Phrases: Hallucination, Intrinsic Hallucination, Extrinsic Hallucination, Faithfulness in NLG, Factuality in NLG, Consistency in NLG</p>
<h2>CONTENTS</h2>
<p>Abstract ..... 1
Contents ..... 1
1 Introduction ..... 3
2 Definitions ..... 4
2.1 Categorization ..... 4
2.2 Task Comparison ..... 5
2.3 Terminology Clarification ..... 5
3 Contributors to Hallucination in NLG ..... 5
3.1 Hallucination from Data ..... 5
3.2 Hallucination from Training and Inference ..... 8
4 Metrics Measuring Hallucination ..... 9
4.1 Statistical Metric ..... 9</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Authors' address: Ziwei Ji, zjiad@connect.ust.hk; Nayeon Lee, nyleeaa@connect.ust.hk; Rita Frieske, rita.frieske@ust. hk; Tiezheng Yu, tyuah@connect.ust.hk; Dan Su, dsu@connect.ust.hk; Yan Xu, yxucb@connect.ust.hk; Etsuko Ishii, eishii@connect.ust.hk; Yejin Bang, yjbang@connect.ust.hk; Delong Chen, delong.chen@connect.ust.hk; Wenliang Dai, wdaiai@connect.ust.hk; Ho Shu Chan, hschanav@connect.ust.hk; Andrea Madotto, amadotto@connect.ust.hk; Pascale Fung, pascale@ece.ust.hk, Center for Artificial Intelligence Research (CAiRE), Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>4.2 Model-based Metric ..... 10
4.3 Human Evaluation ..... 11
5 Hallucination Mitigation Methods ..... 11
5.1 Data-Related Methods ..... 12
5.2 Modeling and Inference Methods ..... 12
6 Future Directions ..... 15
6.1 Future Directions in Metrics Design ..... 15
6.2 Future Directions in Mitigation Methods ..... 16
7 Hallucination in Abstractive Summarization ..... 17
7.1 Hallucination Definition in Abstractive Summarization ..... 17
7.2 Hallucination Metrics in Abstractive Summarization ..... 17
7.3 Hallucination Mitigation in Abstractive Summarization ..... 19
7.4 Future Directions in Abstractive Summarization ..... 20
8 Hallucination in Dialogue Generation ..... 20
8.1 Hallucination Definition in Dialogue Generation ..... 20
8.2 Open-domain Dialogue Generation ..... 21
8.3 Task-oriented Dialogue Generation ..... 23
8.4 Future Directions in Dialogue Generation ..... 24
9 Hallucination in Generative Question Answering ..... 24
9.1 Hallucination Definition in GQA ..... 24
9.2 Hallucination-related Metrics in GQA ..... 25
9.3 Hallucination Mitigation in GQA ..... 25
9.4 Future Directions in GQA ..... 26
10 Hallucination in Data-to-Text Generation ..... 27
10.1 Hallucination Definition in Data-to-Text Generation ..... 27
10.2 Hallucination Metrics in Data-to-Text Generation ..... 27
10.3 Hallucination Mitigation in Data-to-Text Generation ..... 28
10.4 Future Directions in Data-to-Text Generation ..... 29
11 Hallucinations in Neural Machine Translation ..... 29
11.1 Hallucinations Definition and Categories in NMT ..... 29
11.2 Hallucination Metrics in NMT ..... 30
11.3 Hallucination Mitigation Methods in NMT ..... 32
11.4 Future Directions in NMT ..... 33
12 Hallucination in Vision-Language Generation ..... 33
12.1 Object Hallucination in Image Captioning ..... 34
12.2 Hallucination in Other VL Tasks ..... 35
12.3 Future Directions in VL ..... 35
13 Hallucination in Large Language Models ..... 35
13.1 Hallucination Definition in LLMs ..... 36
13.2 Hallucination Metrics for LLMs ..... 36
13.3 Hallucination Mitigation in LLMs ..... 38
13.4 Future Directions of Hallucination Mitigation in LLMs ..... 40
14 Conclusion ..... 42
References ..... 42</p>
<h1>1 INTRODUCTION</h1>
<p>Natural Language Generation (NLG) is one of the crucial yet challenging sub-fields of Natural Language Processing (NLP). NLG techniques are used in many downstream tasks such as summarization, dialogue generation, generative question answering (GQA), data-to-text generation, and machine translation. Recently, the rapid development of NLG has captured the imagination of many thanks to the advances in deep learning technologies, especially Transformer [262]-based models like BERT [44], BART [139], GPT-2 [209], and GPT-3 [20]. The conspicuous development of NLG tasks attracted the attention of many researchers, leading to an increased effort in the field.</p>
<p>Alongside the advancement of NLG models, attention towards their limitations and potential risks has also increased. Some early works [99, 277] focus on the potential pitfalls of utilizing the standard likelihood maximization-based objective in training and decoding of NLG models. They discovered that such likelihood maximization approaches could result in degeneration, which refers generated output that is bland, incoherent, or gets stuck in repetitive loops. Concurrently, it is discovered that NLG models often generate text that is nonsensical, or unfaithful to the provided source input [121, 215, 222, 263]. Researchers started referring to such undesirable generation as hallucination [177] ${ }^{2}$.</p>
<p>Hallucination in NLG is concerning because it hinders performance and raises safety concerns for real-world applications. For instance, in medical applications, a hallucinatory summary generated from a patient information form could pose a risk to the patient. It may provoke a life-threatening incident for a patient if the instructions of a medicine generated by machine translation are hallucinatory. Hallucination can also lead to potential privacy violations. Carlini et al. [26] demonstrate that language models can be prompted to recover and generate sensitive personal information from the training corpus (e.g., email address, phone/fax number, and physical address). Such memorization and recovery of the training corpus is considered a form of hallucination because the model is generating text that is not "faithful" to the source input content (i.e., such private information does not exist in the source input).</p>
<p>Currently, there are many active efforts to address hallucination for various NLG tasks. Analyzing hallucinatory content in different NLG tasks and investigating their relationship would strengthen our understanding of this phenomenon and encourage the unification of efforts from different NLG fields. However, to date, little has been done to understand hallucinations from a broader perspective that encompasses all major NLG tasks. To the best of our knowledge, existing surveys have only focused specific tasks like abstractive summarization [104, 177] and translation [133]. Thus, in this paper, we present a survey of the research progress and challenges in the hallucination problem in NLG. And offer a comprehensive analysis of existing research on the phenomenon of hallucination in different NLG tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. We mainly discussed hallucination of the unimodal NLG tasks that have textual input sources upon which the generated text can be assessed. We also briefly summarize hallucinations in multi-modal settings such as visuallanguage tasks [3, 17]. This survey can provide researchers with a high-level insight derived from the similarities and differences of different approaches. Furthermore, given the various stages of development in studying hallucination from different tasks, the survey can assist researchers in drawing inspiration on concepts, metrics, and mitigation methods.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Organization of this Survey. The remainder of this survey is organized as follows. Section 2 Section 6 provide an overview of the hallucination problem in NLG by discussing the definition and categorization, contributors, metrics, and mitigation methods of hallucinations, respectively. The second part of our survey discusses the hallucination problem associated with specific NLG tasks: abstractive summarization in Section 7, dialogue generation in Section 8, GQA in Section 9, data-to-text generation in Section 10, machine translation in Section 11, and VL generation in Section 12. The third part discusses this phenomenon in LLMs in Section 13. Finally, we conclude the whole survey in Section 14.</p>
<h1>2 DEFINITIONS</h1>
<p>In the general context outside of NLP, hallucination is a psychological term referring to a particular type of perception [73, 166]. Blom [18] define hallucination as "a percept, experienced by a waking individual, in the absence of an appropriate stimulus from the extracorporeal world". Simply put, a hallucination is an unreal perception that feels real. The undesired phenomenon of "NLG models generating unfaithful or nonsensical text" shares similar characteristics with such psychological hallucinations - explaining the choice of terminology. Hallucinated text gives the impression of being fluent and natural despite being unfaithful and nonsensical. It appears to be grounded in the real context provided, although it is actually hard to specify or verify the existence of such contexts. Similar to psychological hallucination, which is hard to tell apart from other "real" perceptions, hallucinated text is also hard to capture at first glance.</p>
<p>Within the context of NLP, the above definition of hallucination, the generated content that is nonsensical or unfaithful to the provided source content [72, 177, 198, 325], is the most inclusive and standard. However, there do exist variations in definition across NLG tasks, which will be further described in the later task-specific sections.</p>
<h3>2.1 Categorization</h3>
<p>Following the categorization from previous works [60, 104, 177], there are two main types of hallucinations, namely intrinsic hallucination and extrinsic hallucination. To explain the definition and categorization more intuitively, we give examples of each category of hallucinations for each NLG downstream task in Table 1.
(1) Intrinsic Hallucinations: The generated output that contradicts the source content. For instance, in the abstractive summarization task from Table 1, the generated summary "The first Ebola vaccine was approved in 2021" contradicts the source content "The first vaccine for Ebola was approved by the FDA in 2019.".
(2) Extrinsic Hallucinations: The generated output that cannot be verified from the source content (i.e., output that can neither be supported nor contradicted by the source). For example, in the abstractive summarization task from Table 1, the information "China has already started clinical trials of the COVID-19 vaccine." is not mentioned in source. We can neither find evidence for the generated output from the source nor assert that it is wrong. Notably, the extrinsic hallucination is not always erroneous because it could be from factually correct external information [177, 247]. Such factual hallucination can be helpful because it recalls additional background knowledge to improve the informativeness of the generated text. However, in most of the literature, extrinsic hallucination is still treated with caution because its unverifiable aspect of this additional information increases the risk from a factual safety perspective.</p>
<h1>2.2 Task Comparison</h1>
<p>The previous subsection is about the definition and categorization of hallucination commonly shared by many NLG tasks. Yet, there are some task-specific differences.</p>
<p>For the abstractive summarization, data-to-text, and dialogue tasks, the main difference is in what serves as the "source" and the level of tolerance towards hallucinations. The source in abstractive summarization is the input source text that is being summarized [228], while the source in data-totext is non-linguistic data [81, 219], and the source(s) in the dialogue system is dialogue history and/or the external knowledge sentences. Tolerance towards hallucinations is very low in both the summarization [197] and data-to-text tasks [198, 269, 274] because it is essential to provide faithful generation. In contrast, the tolerance is relatively higher in dialogue systems because the desired characteristics are not only faithfulness but also user engagement, especially in open-domain dialogue systems $[103,109]$.</p>
<p>For the generative question answering (GQA) task, the exploration of hallucination is at its early stage, so there is no standard definition or categorization of hallucination yet. However, we can see that the GQA literature mainly focuses on "intrinsic hallucination" where the source is the world knowledge [141]. Lastly, unlike the aforementioned tasks, the categorizations of hallucinations in machine translation vary within the task. Most relevant literature agrees that translated text is considered a hallucination when the source text is completely disconnected from the translated target [133, 187, 215]. For further details, please refer to Section 11.</p>
<h3>2.3 Terminology Clarification</h3>
<p>Multiple terminologies are associated with the concept of hallucination. We provide clarification of the commonly used terminologies hallucination, faithfulness, and factuality to resolve any confusion. Faithfulness is defined as staying consistent and truthful to the provided source - an antonym to "hallucination." Any work that tries to maximize faithfulness thus focuses on minimizing hallucination. For this reason, our survey includes all those works that address the faithfulness of machine-generated outputs. Factuality refers to the quality of being actual or based on fact. Depending on what serves as the "fact", "factuality" and "faithfulness" may or may not be the same. Maynez et al. [177] differentiate "factuality" from "faithfulness" by defining the "fact" to be the world knowledge. In contrast, Dong et al. [50] use the source input as the "fact" to determine the factual correctness, making "factuality" indistinguishable from "faithfulness". In this paper, we adopt the definition from Maynez et al. [177] because we believe having such a distinction between source knowledge and world knowledge provides a more clear understanding.</p>
<p>Note that the judging criteria for what is considered faithful or hallucinated (i.e., the definition of hallucination) can differ across tasks. For more details of these variation definitions, you can find in the later task-specific sections.</p>
<h2>3 CONTRIBUTORS TO HALLUCINATION IN NLG</h2>
<h3>3.1 Hallucination from Data</h3>
<p>The main cause of hallucination from data is source-reference divergence. This divergence happens 1) as an artifact of heuristic data collection or 2) due to the nature of some NLG tasks that inevitably contain such divergence. When a model is trained on data with source-reference(target) divergence, the model can be encouraged to generate text that is not necessarily grounded and not faithful to the provided source.</p>
<p>Heuristic data collection. When collecting large-scale datasets, some works heuristically select and pair real sentences or tables as the source and target [132, 283]. As a result, the target reference may contain information that cannot be supported by the source [198, 268]. For instance, when</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Sub-Task</th>
<th>Type</th>
<th>Source</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>Abstractive</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Summarization</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>[197]</td>
<td></td>
<td>Intrinsic</td>
<td>The first vaccine for Ebola was approved by the FDA in 2019</td>
<td></td>
</tr>
<tr>
<td>in the US, five years after the initial outbreak in 2014. To</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>produce the vaccine, scientists had to sequence the DNA</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>of Ebola, then identify possible vaccines, and finally show</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>successful clinical trials. Scientists say a vaccine for COVID-</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>19 is unlikely to be ready this year, although clinical trials</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>have already started.</td>
<td>The first Ebola vaccine was approved in 2021.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Extrinsic</td>
<td></td>
<td>China has already started clinical trials of the COVID-19</td>
</tr>
<tr>
<td>vaccine.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Task</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>oriented</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>[152]</td>
<td>Intrinsic</td>
<td>inform (NAME = pickwick hotel, PRICERANGE = moderate)</td>
<td>the hotel named pickwick hotel is in a high price range.</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Extrinsic</td>
<td></td>
<td>the pickwick hotel in san diego is a moderate price range</td>
</tr>
<tr>
<td>Dialogue</td>
<td></td>
<td></td>
<td>Dialog History:</td>
<td>Speaker 1: I like Roger Nadal the most!</td>
</tr>
<tr>
<td></td>
<td>Open</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>domain</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>[226]</td>
<td>Intrinsic</td>
<td>Speaker 1: What do you think about Murray?</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Speaker 2: I think Murray is a great player he just needs</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>to stay healthy in order to compete more. Who do you like</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>best?</td>
<td>Speaker 1: I like Roger Nadal the most!</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Extrinsic</td>
<td>Knowledge: In tennis, the term Big Four refers to the quartet</td>
<td></td>
</tr>
<tr>
<td>of men's singles players comprising Roger Federer, Rafael</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Nadal, Novak Djokovic, and Andy Murray.</td>
<td>Speaker 1: I like Djokovic. He has played in the top ten</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>singles players of the world.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Extrinsic</td>
<td>Question: dow jones industrial average please?</td>
<td>Answer: The Dow Jones Industrial Average (DJIA) is an index</td>
</tr>
<tr>
<td>of 30 major U.S. stock indexes. The DJIA was created in</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1896 by Charles Dow, a statistician and editor of The Wall</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Street Journal. The index was first published in the mid</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1880s, and was last updated on May 26, 1896.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Generative QA</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>[191]</td>
<td></td>
<td>Extrinsic</td>
<td>Question: definition of sadduction</td>
<td>Answer: Sadducees. noun. The definition of a sadducee is</td>
</tr>
<tr>
<td>a person who acts in a deceitful or duplicitous manner.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>An example of a Sadducee is a politician who acts</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>deceitfully in order to gain political power. 1 a member</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>of a Jewish sect that was active during the Second Temple.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Data2text</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>[283]</td>
<td></td>
<td>Intrinsic</td>
<td>TEAM</td>
<td></td>
</tr>
<tr>
<td>Rockets</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Nuggets</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Denver</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CITY</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>WIN</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>18</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>5</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>108</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>13</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>108</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>7</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>10</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>13</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>96</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>108</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>13</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>96</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>108</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>The Houston Rockets (18-4) defeated the Denver Nuggets</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(10-13) 108-96 on Saturday.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Houston has won two straight games and six of their</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>last seven.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Translation</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>[325]</td>
<td></td>
<td>Intrinsic</td>
<td>近见周四去书店。</td>
<td></td>
</tr>
<tr>
<td>(Michael went to the bookstore on</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Thursday.)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(Michael went to the bookstore on</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Thursday.)</td>
<td>Jerry didn't go to the bookstore.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Michael happily went to the bookstore with his friend.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Table 1.</strong> Examples of each category of hallucinations for each task. In the Data2Text task, H/A: home/away, MIN: minutes, PTS: points, REB: rebounds, AST: assists, BLK: blocks, FG_PCT: field goals percentage. The examples for VL tasks are shown in Figure 2 and Figure 3.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Works</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Automatic <br> Metrics</td>
<td style="text-align: center;">Statistical</td>
<td style="text-align: center;">Dialogue</td>
<td style="text-align: center;">Shuster et al. [233]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Data2Text</td>
<td style="text-align: center;">Dhingra et al. [45], Wang et al. [274]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Translation</td>
<td style="text-align: center;">Martindale et al. [175]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Captioning</td>
<td style="text-align: center;">Rohrbach et al. [222]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Modelbased</td>
<td style="text-align: center;">Abstractive Summarization</td>
<td style="text-align: center;">Durmus et al. [54], Kryscinski et al. [126], Nan et al. [190], Wang et al. [264] Gabriel et al. [75], Goodrich et al. [87], Pagnoni et al. [197], Zhou et al. [325] Falke et al. [66], Laban et al. [131], Mishra et al. [185], Scialom et al. [227]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dialogue</td>
<td style="text-align: center;">Balakrishnan et al. [10], Honovich et al. [101], Li et al. [152] Dziri et al. [61], Gupta et al. [94], Santhanam et al. [226]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generative QA</td>
<td style="text-align: center;">Sellam et al. [229]<em>,Zhang et al. [317]</em>,Durmus et al. [54]<em> Wang et al. [264]</em>, Su et al. [237]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Data2Text</td>
<td style="text-align: center;">Dušek and Kasner [57], Liu et al. [162], Wiseman et al. [283] Filippova [72], Rebuffel et al. [217], Tian et al. [251]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Translation</td>
<td style="text-align: center;">Kong et al. [124], Lee et al. [133], Tu et al. [257] Feng et al. [71], Garg et al. [80], Zhou et al. [325] Parthasarathi et al. [199], Raunak et al. [215]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Task-Agnostic</td>
<td style="text-align: center;">Goyal and Durrett [90], Liu et al. [160], Zhou et al. [325]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataRelated</td>
<td style="text-align: center;">Abstractive Summarization</td>
<td style="text-align: center;">Cao et al. [25], Nan et al. [190], Zhu et al. [328] Gunel et al. [92]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dialogue</td>
<td style="text-align: center;">Honovich et al. [101], Shen et al. [230], Wu et al. [285] Santhanam et al. [226], Shuster et al. [233]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generative QA</td>
<td style="text-align: center;">Bi et al. [15], Fan et al. [67], Yin et al. [297]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Data2Text</td>
<td style="text-align: center;">Liu et al. [162], Nie et al. [193], Parikh et al. [198], Wang [268] Nie et al. [192], Rebuffel et al. [216]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Translation</td>
<td style="text-align: center;">Lee et al. [133], Raunak et al. [215] <br> Briakou and Carpuat [19], Junczys-Dowmunt [111]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Captioning</td>
<td style="text-align: center;">Biten et al. [17]</td>
</tr>
<tr>
<td style="text-align: center;">Mitigation <br> Method</td>
<td style="text-align: center;">Modeling and Inference</td>
<td style="text-align: center;">Abstractive Summarization</td>
<td style="text-align: center;">Huang et al. [102], Li et al. [142], Song et al. [235] <br> Aralikatte et al. [6], Cao et al. [22], Cao and Wang [23] Albrecht and Hwa [4], Chen et al. [32], Zhao et al. [321]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dialogue</td>
<td style="text-align: center;">Balakrishnan et al. [10], Li et al. [152], Rashkin et al. [214] Dziri et al. [60]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generative QA</td>
<td style="text-align: center;">Fan et al. [67], Krishna et al. [125], Li et al. [141] Nakano et al. [189], Su et al. [237]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Data2Text</td>
<td style="text-align: center;">Liu et al. [162], Tian et al. [251], Wang et al. [269, 274], Xu et al. [291] Filippova [72], Rebuffel et al. [216], Su et al. [239], Xiao and Wang [286] Puduppully and Lapata [208]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Translation</td>
<td style="text-align: center;">Feng et al. [71], Lee et al. [133], Weng et al. [281] Li et al. [149], Raunak et al. [215], Wang and Sennrich [267] Bengio et al. [13], Zhou et al. [325] Goyal et al. [89], Xu et al. [290]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Captioning</td>
<td style="text-align: center;">Dai et al. [42], Xiao and Wang [286]</td>
</tr>
</tbody>
</table>
<p>Table 2. Evaluation metrics and mitigation methods for each task. *The hallucination metrics are not specifically proposed for generative question answering (GQA), but they can be adapted for that task.
constructing WIKIBIO [132], a dataset for generating biographical notes based on the infoboxes of Wikipedia, the authors took the Wikipedia infobox as the source and the first sentence of the Wikipedia page as the target ground-truth reference. However, the first sentence of the Wikipedia article is not necessarily equivalent to the infobox in terms of the information they contain. Indeed, Dhingra et al. [45] points out that $62 \%$ of the first sentences in WIKIBIO have additional information not stated in the corresponding infobox. Such mismatch between source and target in datasets can lead to hallucination.</p>
<p>Another problematic scenario is when duplicates from the dataset are not properly filtered out. It is almost impossible to check hundreds of gigabytes of text corpora manually. Lee et al. [134] show that duplicated examples from the pretraining corpus bias the model to favor generating repeats of the memorized phrases from the duplicated examples.</p>
<p>Innate divergence. Some NLG tasks by nature do not always have factual knowledge alignment between the source input text and the target reference, especially those that value diversity in generated output. For instance, it is acceptable for open-domain dialogue systems to respond in chit-chat style, subjective style [214], or with a relevant fact that is not necessarily present in the user input, history or provided knowledge source - this improves the engagingness and diversity of the dialogue generation. However, researchers have discovered that such dataset characteristic leads to inevitable extrinsic hallucinations.</p>
<h1>3.2 Hallucination from Training and Inference</h1>
<p>As discussed in the previous subsection, source-reference divergence existing in dataset is one of the contributors of hallucination. However, Parikh et al. [198] show that hallucination problem still occurs even when there is very little divergence in dataset. This is because there is another contributor of hallucinations - training and modeling choices of neural models [121, 215, 222, 263].</p>
<p>Imperfect representation learning. The encoder has the role of comprehending and encoding input text into meaningful representations. An encoder with a defective comprehension ability could influence the degree of hallucination [198]. When encoders learn wrong correlations between different parts of the training data, it could result in erroneous generation that diverges from the input $[6,71,142,251]$.</p>
<p>Erroneous decoding. The decoder takes the encoded input from the encoder and generates the final target sequence. Two aspects of decoding contribute to hallucinations. First, decoders can attend to the wrong part of the encoded input source, leading to erroneous generation [251]. Such wrong association results in generation with facts mixed up between two similar entities [60, 233]. Second, the design of the decoding strategy itself can contribute to hallucinations. Dziri et al. [60] illustrate that a decoding strategy that improves the generation diversity, such as top-k sampling, is positively correlated with increased hallucination. We conjecture that deliberately added "randomness" by sampling from the top-k samples instead of choosing the most probable token increase the unexpected nature of the generation, leading to a higher chance of containing hallucinated content.</p>
<p>Exposure Bias. Regardless of decoding strategy choices, the exposure bias problem [13, 213], defined as the discrepancy in decoding between training and inference time, can be another contributor to hallucination. It is common practice to train the decoder with teacher-forced maximum likelihood estimation (MLE) training, where the decoder is encouraged to predict the next token conditioned on the ground-truth prefix sequences. However, during the inference generation, the model generates the next token conditioned on the historical sequences previously generated by itself [97]. Such a discrepancy can lead to increasingly erroneous generation, especially when the target sequence gets longer.</p>
<p>Parametric knowledge bias. Pre-training of models on a large corpus is known to result in the model memorizing knowledge in its parameters [169, 201, 221]. This so-called parametric knowledge helps improve the performance of downstream tasks but also serves as another contributor to hallucinatory generation. Large pre-trained models used for downstream NLG tasks are powerful in providing generalizability and coverage, but Longpre et al. [163] have discovered that such</p>
<p>models prioritize parametric knowledge over the provided input. In other words, models that favor generating output with their parametric knowledge instead of the information from the input source can result in the hallucination of excess information in the output. On the other hand, current research works [21, 113, 174, 212, 298] highlight a discrepancy between surface realization and inherent knowledge of the model in NLG tasks. Models can realize they are generating something hallucinated in some way.</p>
<h1>4 METRICS MEASURING HALLUCINATION</h1>
<p>Recently, various studies have illustrated that most conventional metrics used to measure the quality of writing are not adequate for quantifying the level of hallucination [218]. It has been shown that state-of-the-art abstractive summarization systems, evaluated with metrics such as ROUGE, BLEU, and METEOR, have hallucinated content in $25 \%$ of their generated summaries [66]. A similar phenomenon has been shown in other NLG tasks, where it has been discovered that traditional metrics have a poor correlation with human judgment in terms of the hallucination problem [45, 54, 101, 125]. Therefore, there are active research efforts to define effective metrics for quantifying hallucination. FRANK [197] surveys the faithfulness metrics for summarization and compares these metrics' correlations with human judgments. To assess the example-level accuracy of metrics in diverse tasks, TRUE [100] reports their Area Under the ROC Curve (ROC AUC) in regard to hallucinated example detection.</p>
<h3>4.1 Statistical Metric</h3>
<p>One of the simplest approaches is to leverage lexical features (n-grams) to calculate the information overlap and contradictions between the generated and the reference texts - the higher the mismatch counts, the lower the faithfulness and thus the higher the hallucination score.</p>
<p>Given that many traditional metrics leverage the target text as the ground-truth reference (e.g., ROUGE, BLEU, etc.), Dhingra et al. [45] build upon this idea and propose PARENT (Precision And Recall of Entailed n-grams from the Table) ${ }^{3}$, a metric which can also measure hallucinations using both the source and target text as references. Particularly, PARENT n-gram lexical entailment matches generated text with both the source table and target text. The F1-score that combines the precision and recall of the entailment reflects the accuracy of the table-to-text task. The source text is additionally used because it is not guaranteed that the output target text contains the complete set of information available in the input source text.</p>
<p>It is common for NLG tasks to have multiple plausible outputs from the same input, which is known as one-to-many mapping [91, 238]. In practice, however, covering all the possible outputs is too expensive and almost impossible. Thus, many works simplify the hallucination evaluation setup by relying on the source text as the sole reference. Their metrics just focus on the information referred by input sources to measure hallucinations, especially intrinsic hallucinations. For instance, Wang et al. [274] propose PARENT-T, which simplifies PARENT by only using table content as the reference. Similarly, Knowledge F1 [233] - a variant of unigram F1 - has been proposed for knowledge-grounded dialogue tasks to measure the overlap between the model's generation and the knowledge used to ground the dialogue during dataset collection.</p>
<p>Furthermore, Martindale et al. [175] proposed a bag-of-vectors sentence similarity (BVSS) metric for measuring sentence adequacy in machine translation, that only refers to the target text. This statistical metric helps to determine whether the MT output has a different amount of information than the translation reference.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Although simple and effective, one potential limitation of lexical matching is that it can only handle lexical information. Thus, it fails to deal with syntactic or semantic variations [229].</p>
<h1>4.2 Model-based Metric</h1>
<p>Model-based metrics leverage neural models to measure the hallucination degree in the generated text. They are proposed to handle more complex syntactic and even semantic variations. The model-based metrics comprehend the source and generated texts and detect the knowledge/content mismatches. However, the neural models can be subject to errors that can propagate and adversely affect the accurate quantification of hallucination.
4.2.1 Information Extraction (IE)-based. It is not always easy to determine which part of the generated text contains the knowledge that requires verification. IE-based metrics use IE models to represent the knowledge in a simpler relational tuple format (e.g., subject, relation, object), then verify against relation tuples extracted from the source/reference. Here, the IE model is identifying and extracting the "facts" that require verification. In this way, words containing no verifiable information (e.g., stopwords, conjunctions, etc) are not included in the verification step.</p>
<p>For example, ground-truth reference text "Brad Pitt was born in 1963" and generated text "Brad Pitt was born in 1961" will be mapped to the relation triples (Brad Pitt, born-in, 1963) and (Brad Pitt, born-in, 1961) respectively ${ }^{4}$. The mismatch between the dates $(1963 \neq 1961)$ indicates that there is hallucination. One limitation associated with this approach is the potential error propagation from the IE model.
4.2.2 QA-based. This approach implicitly measures the knowledge overlap or consistency between the generation and the source reference. This is based on the intuition that similar answers will be generated from a same question if the generation is factually consistent with the source reference. It is already put in use to evaluate hallucinations in many tasks, such as summarization [54, 227, 264], dialogue [101], and data2text generation [217].</p>
<p>QA-based metric that measures the faithfulness of the generated text is consisted of three parts: First, given a generated text, a question generation (QG) model generates a set of question-answer pairs. Second, a question answering (QA) model answers the generated questions given a groundtruth source text as the reference (containing knowledge). Lastly, the hallucination score is computed based on the similarity of the corresponding answers.</p>
<p>Similar to the IE-based metrics, the limitation of this approach is the potential error that might arise and propagated from either the QG model or the QA model.
4.2.3 Natural Language Inference (NLI) Metrics. There are not many labelled datasets for hallucination detection tasks, especially at the early stage when the hallucination problem starts to gain attention. As an alternative, many works leverage the NLI dataset to tackle hallucinations. Note that NLI is a task that determines whether a "hypothesis" is true (entailment), false (contradiction), or undetermined (neutral) given a "premise". These metrics are based on the idea that only the source knowledge reference should entail the entirety of the information in faithful and hallucination-free generation [57, 61, 66, 101, 104, 126, 131, 185, 282]. More specifically, NLI-based metrics define the hallucination/faithfulness score to be the entailment probability between the source and its generated text, also known as the percentage of times generated text entails, neutral to, and contradicts the source.</p>
<p>According to Honovich et al. [101], NLI-based approaches are more robust to lexical variability than token matching approaches such as IE-based and QA-based metrics. Nevertheless, as illustrated by Falke et al. [66], off-the-shelf NLI models tend to transfer poorly to the abstractive summarization</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>task. Thus, there is a line of research in improving and extending the NLI paradigm specifically for hallucination evaluation purposes [61, 66]. Apart from generalizability, Goyal and Durrett [90] point out the potential limitation of using sentence-level entailment models, namely their incapability to pinpoint and locate which parts of the generation are erroneous. In response, the authors propose a new dependency-level entailment and attempt to identify factual inconsistencies in a more fine-grained manner.
4.2.4 Faithfulness Classification Metrics. To improve upon NLI-based metrics, task-specific datasets are constructed to improve from the NLI-based metrics. Liu et al. [160], Zhou et al. [325] constructed syntactic data by automatically inserting hallucinations into training instances. Santhanam et al. [226] and Honovich et al. [101] construct new corpora for faithfulness classification in dialogue responses. They manually annotate the Wizard-of-Wikipedia dataset [48], a knowledge grounded dialog dataset, by judging whether each response is hallucinated.</p>
<p>Faithfulness specific datasets can be better than NLI datasets because entailment or neutral labels of NLI datasets and faithfulness are not equivalent. For example, the hypothesis "Putin is U.S. president" can be considered to be either neutral to or entailed from the premise "Putin is president". However, from the faithfulness perspective, the hypothesis contains unsupported information "U.S.", which is deemed to be hallucination.
4.2.5 LM-based Metrics. These metrics leverage two language models (LMs) to determine if each token is supported or not: An unconditional LM is only trained on the targets (ground-truth references) in the dataset, while a conditional language model $L M_{\mathrm{x}}$ is trained on both source and target data. It is assumed that the next token is inconsistent with the input if unconditional LM gets a smaller loss than conditional $L M_{\mathrm{x}}$ during forced-path decoding [72, 251]. We classify the generated token as hallucinatory if the loss from LM is lower. The ratio of hallucinated tokens to the total number of target tokens $|y|$ can reflect the hallucination degree.</p>
<h1>4.3 Human Evaluation</h1>
<p>Due to the challenging and imperfect nature of the current automatic evaluation of hallucinations in NLG, human evaluation [226, 233] is still one of the most commonly used approaches. There are two main forms of human evaluation: (1) scoring, where human annotators rate the hallucination level in a range; and (2) comparing, where human annotators compare the output texts with baselines or ground-truth references [242].</p>
<p>Multiple terminologies, such as faithfulness [25, 32, 72, 177, 198, 214, 214, 239, 251, 286, 325], factual consistency [22, 23, 34, 226, 230, 285], fidelity [33], factualness ${ }^{5}$ [216], factuality ${ }^{4}$ [50], or on the other hand, hallucination [60, 102, 162, 226, 233], fact contradicting [192] are used in the human evaluation of hallucination to rate whether the generated text is in accord with the source input. Chen et al. [32], Nie et al. [193] use finer-grained metrics for intrinsic hallucination and extrinsic hallucination separately. Moreover, there are some broad metrics, such as Correctness [10, 15, 142, 269], Accuracy [141, 297], and Informativeness [152] considering both missing and additional contents (extrinsic hallucinations) compared to the input source.</p>
<h2>5 HALLUCINATION MITIGATION METHODS</h2>
<p>Common mitigation methods can be divided into two categories, in accordance with two main contributors of hallucinations: Data-Related Methods, and Modeling and Inference Methods.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5.1 Data-Related Methods</h1>
<p>5.1.1 Building a Faithful Dataset. Considering that noisy data encourage hallucinations, constructing faithful datasets manually is an intuitive method, and there are various ways to build such datasets: One way is employing annotators to write clean and faithful targets from scratch given the source [79, 280], which may lack diversity [95, 198, 203]. Another way is employing annotators to rewrite real sentences on the web [198], or targets in the existing dataset [268]. Basically, the revision strategy consists of three stages: (1) phrase trimming: removing phrases unsupported by the source in the exemplar sentence; (2) decontextualization: resolving co-references and deleting phrases dependent on context; (3) syntax modification: making the purified sentences flow smoothly. Meanwhile, other works [75, 101] leverage the model to generate data and instruct annotators to label whether these outputs contain hallucinations or not. While this approach is typically used to build diagnostic evaluation datasets, it has the potential to build faithful datasets.
5.1.2 Cleaning Data Automatically. In order to alleviate semantic noise issues, another approach is to find information that is irrelevant or contradictory to the input from the existing parallel corpus and then filter or correct the data. This approach is suitable for the case where there is a low or moderate level of noise in the original data [72, 193].</p>
<p>Some works [162, 215, 230] have dealt with the hallucination issue at the instance level by using a score for each source-reference pair and filtering out hallucinated ones. This corpus filtering method consists of several steps: (1) measuring the quality of the training samples in terms of hallucination utilizing the metrics described above; (2) ranking these hallucination scores in descending order; (3) selecting and filtering out the untrustworthy samples at the bottom. Instance-level scores can lead to a signal loss because divergences occur at the word level; i.e., parts of the target sentence are loyal to the source input, while others diverge [216].</p>
<p>Considering this issue, other works [55, 193] correct paired training samples, specifically the input data, according to the references. This method is mainly applied in the data-to-text task because structured data are easier to correcte than utterances. This method consists of two steps: (1) utilizing a model to parse the meaning representation (MR), such as attribute-value pairs, from original human textual references; (2) using the MR extracted from the reference to correct the input MR through slot matching. This method will enhance the semantic consistency between input and output without abandoning a part of the dataset.
5.1.3 Information Augmentation. It is intuitive that augmenting the inputs with external information will obtain a better representation of the source. Because the external knowledge, explicit alignment, extra training data, etc., can improve the correlation between the source and target and help the model learn better task-related features. Consequently, a better semantic understanding helps alleviate the divergence from the source issue. Examples of the augmented information include entity information [162], extracted relation triples from source document [25, 102] obtained by Fact Description Extraction, pre-executed operation results [192], synthetic data generated through replacement or perturbation [32, 133], retrieved external knowledge [15, 67, 92, 233, 328], and retrieved similar training samples [16].</p>
<p>These methods enforce a stronger alignment between inputs and outputs. However, they will bring challenges due to the gap between the original source and augmented information, such as the semantic gap between an ambiguous utterance and a distinct MR of structured data, and the format discrepancy between the structured knowledge graph and natural language.</p>
<h3>5.2 Modeling and Inference Methods</h3>
<h3>5.2.1 Architecture.</h3>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. The frameworks of training methods.
Encoder. The encoder learns to encode a variable-length sequence from input text into a fixedlength vector representation. As we mentioned above in Section 5.1.3, hallucination appears when the models lack semantic interpretation over the input. Some works have modified the encoder architecture in order to make it more compatible with input and learn a better representation. For example, Huang et al. [102] and Cao et al. [25] propose a dual encoder, consisting of a sequential document encoder and a structured graph encoder to deal with the additional knowledge.</p>
<p>Attention. The attention mechanism is an integral component in neural networks that selectively concentrates on some parts of sequences while ignoring others based on dependencies [262]. In order to encourage the generator to pay more attention to the source, Aralikatte et al. [6] introduce a short circuit from the input document to the vocabulary distribution via source-conditioned bias. Krishna et al. [125] employ sparse attention to improve the model's long-range dependencies in the hope of modeling more retrieved documents so as to mitigate the hallucination in the answer. Wu et al. [285] adopt inductive attention, which removes potentially uninformative attention links by injecting pre-established structural information to avoid hallucinations.</p>
<p>Decoder. The decoder is responsible for generating the final output in natural language given input representations [262]. Several work modified the decoder structures to mitigate hallucination, such as the multi-branch decoder [216], uncertainty-aware decoder [286], dual decoder, consisting of a sequential decoder and a tree-based decoder [235], and constrained decoder with lexical or structural limitations [10]. Based on the observation that the "randomness" from sampling-based decoding, especially near the end of sentences, can lead to hallucination, [136] propose to iteratively reduce the "randomness" through time. These decoders improve the possibility of faithful tokens while reducing the possibility of hallucinatory ones during inference by figuring out the implicit discrepancy and dependency between tokens or restricted by explicit constraints. Since such decoders may have more difficulty generating fluent or diverse text, there is a balance to be struck between them.</p>
<h1>5.2.2 Training.</h1>
<p>Planning/Sketching. Planning is a common method to control and restrict what the model generates by informing the content and its order [207]. Planning can be a separate step in a two-step generator [32, 162, 208, 239, 269], which is prone to progressive amplification of the hallucination problem. Or be injected into the end-to-end model during generation [291]. Sketching has a similar function to planning, and can also be adopted for handling hallucinations [269]. The difference is that the skeleton is treated as a part of the final generated text. While providing more controllability, such methods also need to strike a balance between faithfulness and diversity.</p>
<p>Reinforcement Learning (RL). As pointed out by Ranzato et al. [213], word-level maximum likelihood training leads to the problem of exposure bias. Some works [102, 124, 152, 181, 239] adopt RL to solve the hallucination problem, which utilizes different rewards to optimize the model. The</p>
<p>purpose of RL is for the agent to learn an optimal policy that maximizes the reward that accumulates from the environment [258].The reward function is critical to RL and, if properly designed, it can provide training signals that help the model accomplish its goal of hallucination reduction. For example, Li et al. [152] propose a slot consistency reward which is the cardinality of the difference between generated template and the slot-value pairs extracted from input dialogue act. Improving the slot consistency can help reduce the hallucination phenomenon of missing or misplacing slot values in generated templates. Mesgar et al. [181] attain persona consistency sub-reward via an NLI model to reduce the hallucinations in personal facts. Huang et al. [102] use a combination of ROUGE and the multiple-choice cloze score as the reward function to improve the faithfulness of summarization outputs. The cloze score is similar to the QA-based metric, measuring how well a QA model can address the questions by reading the generated summary (as context), where the questions are automatically constructed from the reference summary. As the above examples show, some RL reward functions for mitigating hallucination are inspired by existing automatic evaluation metrics. Although RL is challenging to learn and converge due to the extremely large search space, this method has the potential to obtain the best policy for the task without an oracle.</p>
<p>Multi-task Learning. Multi-task learning is also utilized for handling hallucinations in different NLG tasks. In this training paradigm, a shared model is trained on multiple tasks simultaneously to learn the commonalities of the tasks. The hallucination problem may be derived from the reliance of the training process on a single dataset, leading to the fact that the model fails to learn the actual task features. By adding proper additional tasks along with the target task during training, the model can suffer less from the hallucination problem. For example, Weng et al. [281] and Garg et al. [80] incorporate a word alignment task into the translation model to improve the alignment accuracy between the input and output, and thus faithfulness. Li et al. [142] combine an entailment task with abstractive summarization to encourage models to generate summaries entailed by and faithful to the source. Li et al. [141] incorporate rationale extraction and the answer generation, which allows more confident and correct answers and reduces the hallucination problem. The Multi-task approach has several advantages, such as data efficiency improvement, overfitting reduction, and fast learning. It is crucial to choose which tasks should be learned jointly, and learning multiple tasks simultaneously presents new challenges of design and optimization [38].</p>
<p>Controllable Generation. Current works treat the hallucination level as a controllable attribute in order to remain the hallucination in outputs at a low level. Controllable generation techniques such as controlled re-sampling [214], control codes that can be provided manually [72, 214, 285], or predicted automatically [285] are leveraged to improve faithfulness. This method may require some annotated datasets for training. Considering that hallucination is not necessarily harmful and may bring some benefits, controllable methods can be further adapted to change the degree of hallucination to meet the demands of different real-world applications.</p>
<p>Other general training methods such as regularization [117, 133, 187] and loss reconstruction [149, 267, 274] have also been proposed to tackle the hallucination problem.
5.2.3 Post-Processing. Post-processing methods can correct hallucinations in the output, and this standalone task requires less training data. Especially for noisy datasets where a large proportion of the ground truth references suffer from hallucinations, modeling correction is a competitive choice to handle the hallucination problem [32]. Cao et al. [22], Chen et al. [32], Dong et al. [50], and Dziri et al. [60] follow a generate-then-refine strategy. While the post-processing correction step tends to result in ungrammatical texts, this method allows researchers to utilise SOTA models which perform best in respect of other attributes, such as fluency, and then correct the results specifically for faithfulness by using small amounts of automatically generated training data.</p>
<h1>6 FUTURE DIRECTIONS</h1>
<p>Many studies have been conducted to tackle the hallucination problem in NLG and its downstream tasks. As mentioned above, we have discussed common metrics and mitigation methods to advance research in these fields. From a broader perspective, we wish to point out open challenges and potential directions in regard to metric and mitigation method.</p>
<h3>6.1 Future Directions in Metrics Design</h3>
<p>Fine-grained Metrics. Most of the existing hallucination metrics measure intrinsic and extrinsic hallucinations together as a unified metric. However, it is common for a single generation to have both types and a number of hallucinatory sub-strings. Fine-grained metrics that can distinguish between the two types of hallucinations will provide richer insight to researchers.</p>
<p>In order to implement a fine-graded metric, the first step would be to identify the exact location of the hallucinatory sub-strings correctly. However, some metrics such as those that are QA-based cannot identify the individual hallucinatory sub-strings. Improvements in this aspect would help improve the quality and explainability of the metrics. The next step would be to categorize the detected hallucinatory sub-strings. The hallucinatory sub-string will be intrinsic if it is wrong or nonsensical, and extrinsic if it is non-existing in the source context. Future work that explores an automatic method of categorization would be beneficial.</p>
<p>Fact-Checking. The factual verification of extrinsic hallucinations requires fact-checking against world knowledge, which can be time consuming and laborious. Leveraging an automatic factchecking system for extrinsic hallucination verification is, thus, other future work that requires attention. Fact-checking consists of the knowledge evidence selection and claim verification subtasks, and the following are the remaining challenges associated with each sub-task.</p>
<p>The main research problem associated with the evidence selection sub-task is how to retrieve evidence from the world knowledge. Most of the literature leverages Wikipedia as the knowledge source [135, 249, 300], which is only a small part of world knowledge. Other literature attempts to use the whole web as the knowledge source [65, 171]. However, this method leads to another research problem - "how to ensure the trustworthiness of the information we use from the web" [84]. Sourcelevel methods that leverages the meta-information of the web source (e.g., web traffic, PageRank or URL structure) have been proposed to deal with this trustworthiness issue [11, 204, 205]. Addressing the aforementioned issues to allow evidence selection against world knowledge will be an important future research direction.</p>
<p>For the verification subtask, verification models perform relatively well if given correct evidence [137]. However, it has been shown that verification models are prone to adversarial attacks and are not robust to negation, numerical or comparative words [250]. Improving this weakness of verification models would also be crucial because the factuality of a sentence can easily be changed by small word changes (i.e., changes in negations, numbers, and entities).</p>
<p>Generalization. Although we can see that the source and output text of different tasks are in various forms, investigating their relationship and common ground and proposing general metrics to evaluate hallucinations are worth exploring. Task-agnostic metrics with cross-domain robustness could help the research community to build a unified benchmark. It is also important and meaningful to build open-source platforms to collaborate and standardize the evaluation metrics for NLG tasks.</p>
<p>Incorporation of Human Cognitive Perspective. A good automatic metric should correlate with human evaluation. Humans are sensitive to different types of information. For instance, proper nouns are usually more important than pronouns in the generated text. Mistakes concerning named entities are striking to human users, but automatic metrics treat them equally if not properly</p>
<p>designed. In order to address this issue, new metrics should be designed from the human cognitive perspective. The human ability to recognize salient information and filter the rest is evident in scenarios where the most important facts need to be determined and assessed. For instance, when signing an agreement, a prospective employee naturally skims the document to look at the entries with numbers first. In this way, humans classify what they believe is crucial.</p>
<p>Automatic check-worthy detection has the potential to be applied to improve the correlation with human judgement. Implementing the automatic human-like judgment mentioned above can further mitigate hallucination and improve NLG systems.</p>
<h1>6.2 Future Directions in Mitigation Methods</h1>
<p>General and robust data pre-processing approaches. Since the data format varies between downstream tasks, there is still a gap for data processing methods between tasks, and currently, no universal method is effective for all NLG tasks [140]. Data pre-processing might result in grammatical errors or semantic transformation between the original and processed data, which can negatively affect the performance of generation. Therefore, we believe that general and robust data pre-processing methods can help mitigate the hallucinations in NLG.</p>
<p>Hallucinations in numerals. Most existing mitigation methods do not focus on the hallucination of numerals. However, the correctness of numerals in generated text, such as date, quantities and scalars are important for readers [246, 315, 321]. For example, given the source document "The optimal oxygen saturation $\left(\mathrm{SpO}<em 2="2">{2}\right)$ in adults with COVID-19 who are receiving supplemental oxygen is unknown. However, a target $\mathrm{SpO}</em>$ ", the summary "The target oxygen saturation range for patients with COVID-19 is $82-86 \%$." includes wrong numbers, which could be fatal. Currently, some works [193, 246, 315] point out that using commonsense knowledge can help to gain better numeral representation. And Zhao et al. [321] alleviate numeral hallucinations by re-ranking candidate-generated summaries based on the verification score of quantity entities. Therefore, we believe that explicitly modeling numerals to mitigate hallucinations is a potential direction.}$ of $92 \%$ to $96 \%$ seems logical, considering that indirect evidence from patients without COVID-19 suggests that an $\mathrm{SpO}_{2}$ of $&lt;92 \%$ or $&gt;96 \%$ may be harmful. ${ }^{6</p>
<p>Extrinsic Hallucination Mitigation. Though many works on mitigating hallucinations have been published, most do not distinguish between intrinsic and extrinsic hallucination. Moreover, the main research focus has been on dealing with intrinsic hallucination, while extrinsic hallucination has been somewhat overlooked as it is more challenging to reduce [104]. Therefore, we believe it is worth exploring different mitigation methods for intrinsic and extrinsic hallucinations, and relevant methods in fact-checking can be potentially used for this purpose.</p>
<p>Hallucination in long text. Many tasks in NLG require the model to process long input texts, such as multi-document summarization and generative question answering. We think adopting existing approaches to a Longformer [12]-based model could help encode long inputs. Meanwhile, part of dialogue systems need to generate long output text, in which the latter part may contradict history generation. Therefore, reducing self-contradiction is also an important future direction.</p>
<p>Reasoning. Misunderstanding facts in the source context will lead to intrinsic hallucination and errors. To help models understand the facts correctly requires reasoning over the input table or text. Moreover, if the generated text can be reasoned backwards to the source, we can assume it is faithful. There are some reasoning works in the area of dialogue [39, 83, 272], but few in reducing hallucinations. Moreover, tasks with quantities, such as logical table-to-text generation, require</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>numerical reasoning. Therefore, adding reasoning ability to the hallucination mitigation methods is also an interesting future direction.</p>
<p>Controllability. Controllability means the ability of models to control the level of hallucination and strike a balance between faithfulness and diversity [60, 222]. As mentioned in Section 3, it is acceptable for chit-chat models to generate a certain level of hallucinatory content as long as it is factual. Meanwhile, for the abstractive summarization task, there is no agreement in the research community about whether factual hallucinations are desirable or not [177]. Therefore, we believe controllability merits attention when exploring hallucination mitigation methods.</p>
<h1>7 HALLUCINATION IN ABSTRACTIVE SUMMARIZATION</h1>
<p>Abstractive summarization aims to extract essential information from source documents and to generate short, concise, and readable summaries [302]. Neural networks have achieved remarkable results on abstractive summarization. However, Maynez et al. [177] observe that neural abstractive summarization models are likely to generate hallucinatory content that is unfaithful to the source document. Falke et al. [66] analyze three recent abstractive summarization systems and show that $25 \%$ of the summaries generated from state-of-the-art models have hallucinated content. In addition, Zhou et al. [325] mention that even if a summary contains a large amount of hallucinatory content, it can achieve a high ROUGE [154] score. This has encouraged researchers to actively devise ways to improve the evaluation of abstractive summarization, especially from the hallucination perspective.</p>
<p>In this section, we review the current progress in automatic evaluation and the mitigation of hallucination, and list the remaining challenges for future work. In addition, it is worth mentioning that researchers have used various terms to describe the hallucination phenomenon, such as faithfulness, factual errors, and factual consistency, and we will use the original terms from their papers in the remainder of this section.</p>
<h3>7.1 Hallucination Definition in Abstractive Summarization</h3>
<p>The definition of hallucination in abstractive summarization follows that in Section 2. Specifically, we adopt the definition from [177]: given a document and its abstractive summary, a summary is hallucinated if it has any spans not supported by the input document. Once again, intrinsic hallucination refers to output content that contradicts the source, while extrinsic hallucination refers to output content that the source cannot verify. For instance, in Table 1, given the input article shown in the caption, an example of intrinsic hallucination is "The Ebola vaccine was rejected by the FDA in 2019," because this statement contradicts the given content "The first vaccine for Ebola was approved by the FDA in 2019 in the US". And an example of extrinsic hallucination is "China has already started clinical trials of the COVID-19 vaccine," because this statement is not mentioned in the given content. We can neither find evidence of it from the input article nor assert that it is wrong.</p>
<p>Pagnoni et al. [197] define fine-grained types of factual errors in summaries. As mentioned in 2.3, since the "fact" here refers to source knowledge, "factual error" can be treated as hallucination, and we can adopt this classification as a sub-type of hallucination. They establish three categories as semantic frame error, discourse error, and content verifiability error.</p>
<h3>7.2 Hallucination Metrics in Abstractive Summarization</h3>
<p>Existing metrics for hallucination in abstractive summarization are mainly model-based. Following [104], we divide the hallucination metrics into two categories: (1) unsupervised metrics and (2) semi-supervised metrics. Note that existing hallucination metrics evaluate both intrinsic and</p>
<p>extrinsic hallucinations together in one metric because it is difficult to automatically distinguish between them.
7.2.1 Unsupervised Metrics. Given that hallucination is a newly emerging problem, there are only a few hallucination-related datasets. Therefore, researchers have proposed to adopt other datasets to build unsupervised hallucination metrics. There are three types of such unsupervised metrics: (1) information extraction (IE)-based metrics, (2) natural language inferencing (NLI)-based metrics, (3) question answering (QA)-based metrics.</p>
<p>IE-based Metrics. As mentioned in Section 4, IE-based metrics leverage IE models to extract knowledge as relation tuples (subject, relation, object) from both the generation and knowledge source to analyze the factual accuracy of the generation [87]. However, IE models are not $100 \%$ reliable yet (making errors in the identification of the relation tuples). Therefore, Nan et al. [190] propose an entity-based metric relying on the Named-Entity Recognition model, which is relatively more robust. Their metric builds on the assumption that there will be a different set of named entities in the gold and generated summary if there exists hallucination.</p>
<p>NLI-based Metrics. As mentioned in Section 4, the NLI-model (textual entailment model) can be utilized to measure hallucination based on the assumption that a faithful summary will be entailed by the gold source. However, Falke et al. [66] discover that models trained on NLI datasets can not transfer well to abstractive summarization tasks, degrading the reliability of NLI-based hallucination metrics. To improve NLI models for hallucination evaluation, they release collected annotations as additional test data. Other efforts have also been made to further improve NLI models. Mishra et al. [185] find that the low performance of NLI-based metrics is mainly caused by the length of the premises in NLI datasets being shorter than the source documents in abstractive summarization. Thus, the authors propose to convert multiple-choice reading comprehension datasets into long premise NLI datasets automatically. The results indicate that long-premise NLI datasets help the model achieve a higher performance than the original NLI datasets. In addition, Laban et al. [131] introduce a simple but efficient method called SUMMAC $_{\text {Conv }}$ by applying NLI models to sentence units that are segmented from documents. The performance of their model is better than applying NLI models to the whole document.</p>
<p>QA-based Metrics. QA-based metrics measure the knowledge overlap or consistency between summaries and the source documents based on the intuition that QA models will achieve similar answers if the summaries are factually consistent with the source documents. QA-based metrics such as FEQA [54], QAGS [264], and QuestEval [227] follow three steps to obtain a final score: (1) a QG model generates questions from the summaries, (2) a QA model obtains answers from the source documents, and (3) calculate the score by comparing the set of answers from source documents and the set of answers from summaries. The results show that these reference-free metrics have substantially higher correlations with human judgments of faithfulness than the baseline metrics. Gabriel et al. [75] further analyze the FEQA and find that the effectiveness of QA-based metrics depends on the question. They also provide a meta-evaluation framework that includes QA metrics.
7.2.2 Semi-Supervised Metrics. Semi-supervised metrics are trained on the synthetic data generated from summarization datasets. Trained on these task-specific corpora, models can judge whether the generated summaries are hallucinatory. Kryscinski et al. [126] propose a weakly supervised model named FactCC for evaluating factual consistency. The model is trained jointly for three tasks: (1) checking whether the synthetic sentences remain factually consistent, (2) extracting supporting spans in the source documents, and (3) extracting inconsistent spans in the summaries, if any exist.</p>
<p>They transfer this model to check whether the summaries generated from summarization models are factually consistent. Results show that the performance of their FactCC model surpasses the classifiers trained on the MNLI or FEVER datasets. Zhou et al. [325] introduce a method to fine-tune a pre-trained language model on synthetic data with automatically inserted hallucinations in order to detect the hallucinatory content in summaries. The model can classify whether spans in the machine-generated summaries are faithful to the article. This method shows higher correlations with human factual consistency evaluation than the baselines.</p>
<h1>7.3 Hallucination Mitigation in Abstractive Summarization</h1>
<p>Recently, many approaches have been proposed to reduce the hallucination phenomenon in abstractive summarization.
7.3.1 Architecture Method. Seq-to-seq [244] models are widely used and achieve state-of-the-art performance in abstractive summarization. Researchers have made modifications to the architecture design of the seq-to-seq models to reduce hallucinated content in the summaries. We describe various efforts made to improve the encoder, decoder, or both the encoder and decoder of the seq-to-seq models.</p>
<p>Encoder. Zhu et al. [328] propose to use an explicit graph neural network (GNN) to encode the fact tuples extracted from source documents. In addition to an explicit graph encoder, Huang et al. [102] further design a multiple-choice cloze test reward to encourage the model to better understand entity interactions. Moreover, Gunel et al. [92] use external knowledge from Wikipedia to make knowledge embeddings, which the results show improve factual consistency.</p>
<p>Decoder. Song et al. [235] present the incorporation of a sequential decoder with a tree-based decoder to generate a summary sentence and its syntactic parse. This joint generation is performed improve faithfulness. Aralikatte et al. [6] introduce the Focus Attention Mechanism, which encourages decoders to generate tokens similar or topical to the source documents. The results on the BBC extreme summarization task show that models augmented with the Focus Attention Mechanism generate more faithful summaries.</p>
<p>Encoder-decoder. Cao et al. [25] extract fact descriptions from the source text and apply a dualattention seq-to-seq framework to force the summaries to be conditioned on both source documents and the extracted fact descriptions. Li et al. [142] propose an entailment-aware encoder and decoder with multi-task learning which incorporates the entailment knowledge into abstractive summarization models.
7.3.2 Training Method. Aside from architecture modification, some works improved the training approach to reduce hallucination. Cao and Wang [23] introduce a contrastive learning method to train summarization models. The positive training data are reference summaries, while the negative training data are automatically generated hallucinatory summaries, and the contrastive learning system is trained to distinguish between them. In the dialogue summarization field, Tang et al. [245] propose another contrastive fine-tuning strategy, named CONFIT, that can improve the factual consistency and overall quality of summaries.
7.3.3 Post-Processing Method. Some works carry out post-editing to reduce the hallucination of the model-generated summaries, which are viewed as draft summaries. Dong et al. [50] propose SpanFact, a pair of factual correction models that use knowledge learned from QA models to correct the spans in the generated summaries. Similar to SpanFact, Cao et al. [22] introduce a post-editing corrector module to identify and correct hallucinatory content in generated summaries. The corrector module is trained on synthetic data which are created by adding a series of heuristic</p>
<p>transformations to reference summaries. Zhao et al. [321] present HERMAN, a system that learns to recognize quantities (dates, amounts of money, etc.) in the generated summary and verify their factual consistency with the source text. According to the quantity hallucination score, the system chooses the most faithful summary where the source text supports its quantity terms from the candidate-generated summaries. Chen et al. [32] introduce a contrast candidate generation and selection system to do post-processing. The contrast candidate generation model replaces the named entities in the generated summaries with ones present in the source documents, and the contrast candidate selection model will select the best candidate as the final output summary.</p>
<h1>7.4 Future Directions in Abstractive Summarization</h1>
<p>Factual Hallucination Evaluation. Factual hallucinations contain information not found in source content, though it is factually correct. In the summarization task, this kind of hallucination could lead to better summaries. However, there is little work focused on evaluating factual hallucination explicitly. Fact-checking approaches could be potentially used in this regard.</p>
<p>Extrinsic Hallucination Mitigation. There has been little research on extrinsic hallucinations as it is more challenging to detect and mitigate content based on world knowledge. We believe it is worth exploring extrinsic hallucination in terms of evaluation metrics and mitigation methods.</p>
<p>Hallucination in Dialogue Summarization. In conversational data, the discourse relations between utterances and co-references between speakers are more complicated than from, say, news articles. For example, Zhong et al. [322] show that $74 \%$ of samples in the QMSum dataset consist of inconsistent facts. We believe exploring the hallucination issue in dialogue summarization is an important and special component of research into hallucination in abstractive summarization.</p>
<h2>8 HALLUCINATION IN DIALOGUE GENERATION</h2>
<p>Dialogue generation is an NLG task that automatically generates responses according to user utterances. The generated responses are required to be fluent, coherent, and consistent with the dialogue history. The dialogue generation task can be divided into two sub-tasks: (1) task-oriented dialogue generation; (2) open-domain dialogue generation. A task-oriented dialogue system aims to complete a certain task according to a user query in a specific domain, such as restaurant booking, hotel recommendation, and calendar checking. Meanwhile, an open-domain dialogue system aims to establish a multi-turn, long-term conversation with users while providing the users with an engaging experience.</p>
<h3>8.1 Hallucination Definition in Dialogue Generation</h3>
<p>The hallucination problem also exists in the dialogue generation task. It is important to note that a dialogue system is expected either to provide the user with the required information or to provide an engaging response without repeating utterances from the dialogue history. Thus, the tolerance for producing proper "hallucination" from the dialogue history is relatively higher.</p>
<p>The definition of hallucination in this task can be adopted from the general definition as follows: (1) Intrinsic hallucination: the generated response is contradictory to the dialogue history or the external knowledge sentences. In the examples of intrinsic hallucination shown in Table 1, we can verify that the output contradicts the inputs: In one example, the input is a "moderate" price range, but the model mistakenly generates a sentence with a "high" price range. In another case, the confusion of the names "Roger Federer" and "Rafael Nadal" causes the output generation of "Roger Nadal". (2) Extrinsic hallucination: the generated response is hard to verify with the dialogue history or the external knowledge sentences. Responses with extrinsic hallucination are impossible to verify with the given inputs. "Pickwick hotel" might be "in san diego", and Djokovic may have</p>
<p>been "in the top ten singles players of the world". However, we do not have enough information to check the truth of these statements.</p>
<p>In the following sections, the hallucination problem in open-domain and task-oriented dialogue generation tasks will be separately discussed according to the their natures.</p>
<h1>8.2 Open-domain Dialogue Generation</h1>
<p>While the term "hallucination" seems to have newly emerged in the NLP field, a related behavior, "inconsistency", of neural models has been widely discussed. This behavior has been pointed out as a shortcoming of generation-based approaches for open-domain chatbots [103, 165, 223]. Two possible types of inconsistency occur in open-domain dialogue generation: (1) inconsistency among the system utterances, such as when the system contradicts its previous utterance; (2) inconsistency with an external source, such as factually incorrect utterances. Whereas the first type is described using the term "inconsistency" [148, 278, 309] or "incoherence" [14, 58], some have recently started to call the second type "hallucination" [183, 224]. Self-inconsistency can be considered as an intrinsic hallucination problem, while the external inconsistency involves both intrinsic and extrinsic hallucinations, depending on the reference source.</p>
<p>As mentioned earlier, a certain level of hallucination may be acceptable in open-domain chit-chat as long as it does not involve severe factual issues. Moreoever, it is almost impossible to verify factual correctness since the system usually lacks a connection to external resources. With the introduction of knowledge-grounded dialogue tasks [48, 326], which provide an external reference, however, there has been more active discussion of hallucination in open-domain dialogue generation.
8.2.1 Self-Consistency. In end-to-end generative open-domain dialogue systems, the inconsistency among system utterances has been pointed out as the bottleneck to human-level performance [263]. We often observe an inconsistency in the answers to semantically similar yet not identical questions. For example, a system may answer the questions of "What is your name?" and "May I ask your name?" with different responses. Persona consistency has been the center of attention [144, 312] and it is one of the most obvious cases of self-contradiction regarding the character of the dialogue system. "Persona" is defined as the character that a dialogue system plays during a conversation, and can be composed of identity, language behavior, and an interaction style [144]. While some works has set their objective as teaching models to utilize speaker-level embeddings [144, 168], others condition generation with a set of descriptions about a persona, which we will discuss in detail in the next section.
8.2.2 External Consistency. Besides self-consistency, an open-domain dialogue system should also generate persona-consistent and informative responses corresponding so as to user utterances to further engage with the user during conversation. In this process, an external resource containing explicit persona information or world knowledge is introduced into the system to assist the model generation process.</p>
<p>The PersonaChat datasets [47, 312] have accelerated research into persona consistency [96, 128, 178, 284, 296, 307, 318]. In PersonaChat datasets, each conversation has persona descriptions such as "I like to ski" or "I am a high school teacher" attached. By conditioning the response generation on the persona description, a chit-chat model is expected to acquire an ability to generate a more persona-consistent response. Lately, the application of NLI methods [148, 234] or reinforcement learning frameworks [181] have been investigated. Although these methods conditioned on the PersonaChat datasets have been successful, further investigation of approaches that do not rely on a given set of persona descriptions is necessary because such descriptions are not always available, and covering every aspect of a persona with them is impossible.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://www.covid19treatmentguidelines.nih.gov/management/critical-care/oxygenation-and-ventilation/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>0360-0300/2022/2-ART \$15.00
https://doi.org/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
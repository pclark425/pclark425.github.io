<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7016 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7016</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7016</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-83fb274ca565544743c4cdc7abe58db88a163ae2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/83fb274ca565544743c4cdc7abe58db88a163ae2" target="_blank">Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text, demonstrating that dual encoding can significantly improve the quality of the generated text.</p>
                <p><strong>Paper Abstract:</strong> Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7016.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7016.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan serialization (role-delimited triple sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Serialized content plan: role-delimited, re-ordered triple sequence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicit content plan that serializes a re-ordered sequence of RDF triples using role delimiters (<S>, <P>, <O>) so that subjects, predicates and objects are concatenated into a linear sequence which is then encoded by an LSTM Plan Encoder. The plan is produced by a GCN-based neural planner that selects an ordering over triples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>role-delimited triple sequence (Plan)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each planned triple is linearized into tokens with special role delimiters inserted before each element, e.g. "<S> <subject-mention> <P> <predicate-id> <O> <object-mention>"; multiple triples are concatenated in the planned order to form a single sequence fed to an LSTM encoder. Entity mentions are by default tokenized normally (optionally joined with underscores in an ablation) and predicates use unique IDs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (lossy with respect to full graph connectivity)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Neural planner: a relational GCN (R-GCN) computes predicate/node embeddings with two extra indicator bits per predicate (visited flag and last-visited flag) and sequentially selects predicates via softmax over remaining predicates; selected predicate order yields the plan. The final plan sequence is encoded with a 2-layer bidirectional LSTM (Plan Encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (data-to-text from RDF triples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PlanENC (Plan Encoder) / used within DuALENC</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Plan creation: R-GCN (relational GCN) 2-layer node encoder (planner) producing predicate representations; Plan encoder: 2-layer bidirectional LSTM (256 hidden dims in generation experiments). Decoder: LSTM with attention and copy mechanism. For planner-only experiments GCN used 2 layers with 100-dim hidden states.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Planner evaluation: accuracy (exact match to human plan) and BLEU-2 for plan sequences; Generation evaluation: BLEU, METEOR, TER</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Planner (GCN planner on WebNLG): Accuracy SEEN=0.63, UnSEEN=0.61, ALL=0.62; BLEU-2 SEEN=80.8, UnSEEN=79.3, ALL=80.1. Generation (PlanENC): BLEU SEEN=64.42, UnSEEN=38.23, ALL=52.78; METEOR ALL=0.41; TER ALL=0.42.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Using explicit plans substantially improved generation metrics and stability: PlanENC outperformed single-encoder baselines (e.g., single GCN) and increased BLEU (PlanENC vs GCN single-encoder on SEEN by +8.52 BLEU). Planning reduced variability across random seeds (standard deviation decreased from 0.82 to 0.17 in ablation study).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Serialization is lossy: the linear plan cannot preserve all graph-structural information (authors note the serialized plan cannot preserve all original graph information). LSTM plan encoder sometimes struggles to capture semantic roles leading to omissions or incorrect argument assignment (examples shown). Average token lengths and explicit canonical ordering were not reported; delimiter use did not strongly affect BLEU in ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms simple sequential linearization baselines (GRU/Transformer) and random traversal baselines; improves over single GCN encoder on generation quality. Authors report better generalization than GRU/Transformer because plan retains structural organization, and better efficiency/accuracy than Step-By-Step pipeline planners.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7016.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7016.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reconstructed RDF graph (entity+predicate nodes) + R-GCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reconstructed RDF graph treating entities and predicates as nodes encoded with Relational Graph Convolutional Network (R-GCN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Input RDF triples are restructured so that subjects, predicates and objects are all nodes; each triple induces directed edges s->p, p->s, o->p, p->o and self-loops, and a relational GCN (R-GCN) produces node embeddings used both for planning and as a context memory for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>reconstructed RDF graph (entities and predicates as nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph construction: for each triple (s,p,o) create three nodes (s,p,o) where s and o are identified by entity mentions and p by a unique ID. Edges: four directed edges per triple (s→p, p→s, o→p, p→o) plus self-loops. Node input embeddings are average embeddings of node mentions. R-GCN performs iterative relational message passing to obtain node states.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-structured (lossless w.r.t. triple structure), encoded via graph neural network</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Relational GCN (R-GCN) iterative aggregation: h_v^t = ρ( Σ_{r∈R} Σ_{u∈N_v^r} (1/c_{v,r}) W_r h_u^{t-1} + b_r ). Used as graph encoder in single-encoder Graph2Seq baselines and as one of two encoders in DuALENC.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (data-to-text from RDF triples); also used to create representations for planning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>R-GCN (Relational Graph Convolutional Network) as Graph Encoder within GCN baseline and DuALENC</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>R-GCN with 2 layers used in planner experiments (100-dim hidden), increased to 256-dim hidden for generation experiments; relation-specific parameter matrices W_r and biases b_r; ReLU activation; initial node embeddings are 100-dim random vectors (planner) or pretrained/glove variants for some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used for both planner and generation; evaluation metrics reported include planner accuracy/BLEU-2 and generation BLEU/METEOR/TER and human measures (coverage, faithfulness, fluency).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>As graph-only encoder (GCN baseline) in planner: GCN planner achieved the reported high planner accuracy (see Plan entry) when used for planning. As generation-only GCN baseline, reported BLEU (from Table 2) GCN-EC BLEU=55.90 (SEEN, other splits not reported in that row); human eval coverage/faithfulness for GCN baseline: Coverage=79.8%, Faithfulness=76.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Graph encoding captures structural features that help generalization to UnSEEN domains (authors attribute small drop in UnSEEN accuracy to structural modeling vs linearized seq models). However, graph-only encoders create a larger 'structural gap' with sequential decoders and can make alignment with output sequence harder.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Graph encoder alone widens the structural mismatch between encoder and LSTM decoder (encoder produces graph-structured hidden states, decoder expects linear sequence), which authors argue harms alignment and can reduce coverage; GCN baseline has lower generation coverage even if faithfulness remains relatively stable. Graph encoders do not directly provide a linear plan for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to sequential linearizations (GRU/Transformer), R-GCN retains structural information and yields better planner generalization; but PlanENC or DuALENC (which combine plan serialization with graph encoder) outperform graph-only encoder on generation metrics and human eval overall.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7016.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7016.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sequential linearization (GRU/Transformer baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized RDF triple sequence used as input to Seq2Seq models (GRU or Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach where the input graph is linearized into a sequence (simple serialization) and fed to standard Seq2Seq encoders such as GRU or Transformer; these methods treat the graph as a flattened sequence and do not explicitly preserve graph structure beyond the chosen ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearized triple sequence (plain sequential serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph triples are converted to a flat token sequence (linearized) and input to sequence encoders (GRU or Transformer). The particular linearization used may be the enriched dataset plan or another string-based serialization; no special relational node encoding is applied beyond token order and optional delexicalization.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (lossy)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Plain sequential serialization (order supplied either by dataset-provided plans or heuristic ordering) and fed to standard sequence encoders (GRU-based or Transformer-based encoder-decoder).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GRU (seq2seq) and Transformer (seq2seq) baselines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard Seq2Seq architectures (GRU-based RNN encoder-decoder with attention, or Transformer encoder-decoder) trained on serialized triple inputs; Ferreira et al. (2019) variants used in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Generation: BLEU, METEOR, TER; Planner (if used for plan prediction) accuracy/BLEU-2</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported baseline generation BLEU (Table 2): GRU BLEU SEEN=56.09, Unseen=25.12, All=42.73; Transformer BLEU SEEN=56.28, Unseen=23.04, All=42.41. For plan prediction, these models had much lower UnSEEN planner accuracy: e.g., GRU/Transformer planner accuracy drops substantially on UnSEEN (from 0.56 SEEN to 0.10/0.09 UnSEEN in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Simple linearization is compatible with standard Seq2Seq training but leads to worse generalization to UnSEEN domains as it misses structural graph features; authors report larger accuracy drop on UnSEEN vs the GCN planner.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy with respect to graph structure (misses relational/topological features), poorer generalization to unseen domains, and significant accuracy drop in plan prediction on UnSEEN data. Authors attribute these deficiencies to linearization missing graph-level structural features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Worse generalization and lower planner accuracy than the proposed GCN planner and plan-based methods; PlanENC and DuALENC outperform these baselines on generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7016.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7016.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structure-random traversal (random walk / BFS / DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure-random graph traversal serializations (random walk, random BFS, random DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines that produce a serialized linear order of triples by performing randomized graph traversals (random walk, random breadth-first-search, random depth-first-search) to create a sequence representation of the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>structure-random traversal sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Create a permutation/sequence of triples by executing a traversal over the reconstructed RDF graph (random walk, random BFS, or random DFS) and outputting triples in the visited order; used as a baseline planner input.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, traversal-based (lossy, nondeterministic if randomness used)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph traversal algorithms (random walk, BFS, DFS) applied to RDF graph to produce an ordering of triples which is used as a plan sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>plan generation for graph-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Structure-Random baselines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Non-learned randomized traversal procedures producing candidate plan sequences; highest score among three random strategies reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Planner evaluation: accuracy and BLEU-2 (plan BLEU)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Planner baseline performance (Table 1): Structure-random Accuracy SEEN=0.32, UnSEEN=0.38, ALL=0.34; BLEU-2 SEEN=56.6, UnSEEN=62.9, ALL=59.5.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides a naive serialized input for training/evaluation; used as a lower-bound baseline but does not benefit learning beyond randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Non-deterministic and does not optimize for human-like plans; low accuracy relative to learned planner; ignores semantics of triples beyond graph adjacency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Significantly outperformed by the learned GCN planner (accuracy ~0.62) and by pipeline planners; demonstrates the value of learned planning over random traversals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7016.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7016.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-By-Step heuristic plan extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Step-By-Step: heuristic/statistical plan extraction from references (Moryossef et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline approach referenced in the paper that extracts plans from reference texts using heuristic string matching or transition-based ranking; used by some pipeline systems to obtain plans for the realization model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Step-By-Step: Separating planning from realization in neural data-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>heuristic-extracted plan (from reference sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Plans are derived from human references via heuristics (e.g., string matching) or transition-based statistical ranking to determine the order of triples that a reference realizes; these extracted linear orders are used as training signals for pipeline realizers.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (lossy, heuristic-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Heuristic string-matching and/or transition-based ranking to map references to an ordering over input triples; produced plans are fed to a realization model (e.g., OpenNMT) with copy mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (plans extracted from references)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>pipeline plan-and-realize graph-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Step-By-Step planner (pipeline) and realization with OpenNMT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline: plan extracted heuristically/statistically; realization uses standard seq2seq with copy. Reported in Moryossef et al.; compared in this paper as a competitive pipeline baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Generation BLEU, planner accuracy (as reported elsewhere), human eval coverage/faithfulness/fluency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>In this paper Step-By-Step reported high Coverage and Faithfulness in human eval (Coverage 96.1%, Faithfulness 89.3%) but worse fluency; planner accuracy/BLEU in Table 1: Step-By-Step Accuracy SEEN=0.49, UnSEEN=0.44, ALL=0.47; BLEU-2 SEEN=73.2, UnSEEN=68.0, ALL=70.8.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides pipeline training signal and high coverage/faithfulness at the cost of fluency when realizing triples separately; more computationally expensive in some variants (authors note Step-By-Step needed ~250s for one 7-triple instance in a cited report).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Heuristic extraction can be slow or brittle; pipeline separation can hurt fluency (Step-By-Step had worse fluency in human eval) and may be less efficient. Authors report their neural planner is more time-efficient and accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Authors report their GCN-based planner is both faster and more accurate than Step-By-Step variants on planner metrics, though Step-By-Step achieves very high coverage/faithfulness in human evaluation but at the expense of fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>step-by-step: Separating planning from realization in neural data-to-text generation <em>(Rating: 2)</em></li>
                <li>improving quality and efficiency in planbased neural data-to-text generation <em>(Rating: 2)</em></li>
                <li>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures <em>(Rating: 1)</em></li>
                <li>GTR-LSTM: A triple encoder for sentence generation from RDF data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7016",
    "paper_id": "paper-83fb274ca565544743c4cdc7abe58db88a163ae2",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Plan serialization (role-delimited triple sequence)",
            "name_full": "Serialized content plan: role-delimited, re-ordered triple sequence",
            "brief_description": "An explicit content plan that serializes a re-ordered sequence of RDF triples using role delimiters (&lt;S&gt;, &lt;P&gt;, &lt;O&gt;) so that subjects, predicates and objects are concatenated into a linear sequence which is then encoded by an LSTM Plan Encoder. The plan is produced by a GCN-based neural planner that selects an ordering over triples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "role-delimited triple sequence (Plan)",
            "representation_description": "Each planned triple is linearized into tokens with special role delimiters inserted before each element, e.g. \"&lt;S&gt; &lt;subject-mention&gt; &lt;P&gt; &lt;predicate-id&gt; &lt;O&gt; &lt;object-mention&gt;\"; multiple triples are concatenated in the planned order to form a single sequence fed to an LSTM encoder. Entity mentions are by default tokenized normally (optionally joined with underscores in an ablation) and predicates use unique IDs.",
            "representation_type": "sequential, token-based (lossy with respect to full graph connectivity)",
            "encoding_method": "Neural planner: a relational GCN (R-GCN) computes predicate/node embeddings with two extra indicator bits per predicate (visited flag and last-visited flag) and sequentially selects predicates via softmax over remaining predicates; selected predicate order yields the plan. The final plan sequence is encoded with a 2-layer bidirectional LSTM (Plan Encoder).",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "graph-to-text generation (data-to-text from RDF triples)",
            "model_name": "PlanENC (Plan Encoder) / used within DuALENC",
            "model_description": "Plan creation: R-GCN (relational GCN) 2-layer node encoder (planner) producing predicate representations; Plan encoder: 2-layer bidirectional LSTM (256 hidden dims in generation experiments). Decoder: LSTM with attention and copy mechanism. For planner-only experiments GCN used 2 layers with 100-dim hidden states.",
            "performance_metric": "Planner evaluation: accuracy (exact match to human plan) and BLEU-2 for plan sequences; Generation evaluation: BLEU, METEOR, TER",
            "performance_value": "Planner (GCN planner on WebNLG): Accuracy SEEN=0.63, UnSEEN=0.61, ALL=0.62; BLEU-2 SEEN=80.8, UnSEEN=79.3, ALL=80.1. Generation (PlanENC): BLEU SEEN=64.42, UnSEEN=38.23, ALL=52.78; METEOR ALL=0.41; TER ALL=0.42.",
            "impact_on_training": "Using explicit plans substantially improved generation metrics and stability: PlanENC outperformed single-encoder baselines (e.g., single GCN) and increased BLEU (PlanENC vs GCN single-encoder on SEEN by +8.52 BLEU). Planning reduced variability across random seeds (standard deviation decreased from 0.82 to 0.17 in ablation study).",
            "limitations": "Serialization is lossy: the linear plan cannot preserve all graph-structural information (authors note the serialized plan cannot preserve all original graph information). LSTM plan encoder sometimes struggles to capture semantic roles leading to omissions or incorrect argument assignment (examples shown). Average token lengths and explicit canonical ordering were not reported; delimiter use did not strongly affect BLEU in ablation.",
            "comparison_with_other": "Outperforms simple sequential linearization baselines (GRU/Transformer) and random traversal baselines; improves over single GCN encoder on generation quality. Authors report better generalization than GRU/Transformer because plan retains structural organization, and better efficiency/accuracy than Step-By-Step pipeline planners.",
            "uuid": "e7016.0",
            "source_info": {
                "paper_title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "Reconstructed RDF graph (entity+predicate nodes) + R-GCN",
            "name_full": "Reconstructed RDF graph treating entities and predicates as nodes encoded with Relational Graph Convolutional Network (R-GCN)",
            "brief_description": "Input RDF triples are restructured so that subjects, predicates and objects are all nodes; each triple induces directed edges s-&gt;p, p-&gt;s, o-&gt;p, p-&gt;o and self-loops, and a relational GCN (R-GCN) produces node embeddings used both for planning and as a context memory for decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "reconstructed RDF graph (entities and predicates as nodes)",
            "representation_description": "Graph construction: for each triple (s,p,o) create three nodes (s,p,o) where s and o are identified by entity mentions and p by a unique ID. Edges: four directed edges per triple (s→p, p→s, o→p, p→o) plus self-loops. Node input embeddings are average embeddings of node mentions. R-GCN performs iterative relational message passing to obtain node states.",
            "representation_type": "graph-structured (lossless w.r.t. triple structure), encoded via graph neural network",
            "encoding_method": "Relational GCN (R-GCN) iterative aggregation: h_v^t = ρ( Σ_{r∈R} Σ_{u∈N_v^r} (1/c_{v,r}) W_r h_u^{t-1} + b_r ). Used as graph encoder in single-encoder Graph2Seq baselines and as one of two encoders in DuALENC.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "graph-to-text generation (data-to-text from RDF triples); also used to create representations for planning",
            "model_name": "R-GCN (Relational Graph Convolutional Network) as Graph Encoder within GCN baseline and DuALENC",
            "model_description": "R-GCN with 2 layers used in planner experiments (100-dim hidden), increased to 256-dim hidden for generation experiments; relation-specific parameter matrices W_r and biases b_r; ReLU activation; initial node embeddings are 100-dim random vectors (planner) or pretrained/glove variants for some baselines.",
            "performance_metric": "Used for both planner and generation; evaluation metrics reported include planner accuracy/BLEU-2 and generation BLEU/METEOR/TER and human measures (coverage, faithfulness, fluency).",
            "performance_value": "As graph-only encoder (GCN baseline) in planner: GCN planner achieved the reported high planner accuracy (see Plan entry) when used for planning. As generation-only GCN baseline, reported BLEU (from Table 2) GCN-EC BLEU=55.90 (SEEN, other splits not reported in that row); human eval coverage/faithfulness for GCN baseline: Coverage=79.8%, Faithfulness=76.8%.",
            "impact_on_training": "Graph encoding captures structural features that help generalization to UnSEEN domains (authors attribute small drop in UnSEEN accuracy to structural modeling vs linearized seq models). However, graph-only encoders create a larger 'structural gap' with sequential decoders and can make alignment with output sequence harder.",
            "limitations": "Graph encoder alone widens the structural mismatch between encoder and LSTM decoder (encoder produces graph-structured hidden states, decoder expects linear sequence), which authors argue harms alignment and can reduce coverage; GCN baseline has lower generation coverage even if faithfulness remains relatively stable. Graph encoders do not directly provide a linear plan for decoding.",
            "comparison_with_other": "Compared to sequential linearizations (GRU/Transformer), R-GCN retains structural information and yields better planner generalization; but PlanENC or DuALENC (which combine plan serialization with graph encoder) outperform graph-only encoder on generation metrics and human eval overall.",
            "uuid": "e7016.1",
            "source_info": {
                "paper_title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "Sequential linearization (GRU/Transformer baselines)",
            "name_full": "Linearized RDF triple sequence used as input to Seq2Seq models (GRU or Transformer)",
            "brief_description": "Baseline approach where the input graph is linearized into a sequence (simple serialization) and fed to standard Seq2Seq encoders such as GRU or Transformer; these methods treat the graph as a flattened sequence and do not explicitly preserve graph structure beyond the chosen ordering.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "linearized triple sequence (plain sequential serialization)",
            "representation_description": "Graph triples are converted to a flat token sequence (linearized) and input to sequence encoders (GRU or Transformer). The particular linearization used may be the enriched dataset plan or another string-based serialization; no special relational node encoding is applied beyond token order and optional delexicalization.",
            "representation_type": "sequential, token-based (lossy)",
            "encoding_method": "Plain sequential serialization (order supplied either by dataset-provided plans or heuristic ordering) and fed to standard sequence encoders (GRU-based or Transformer-based encoder-decoder).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "graph-to-text generation",
            "model_name": "GRU (seq2seq) and Transformer (seq2seq) baselines",
            "model_description": "Standard Seq2Seq architectures (GRU-based RNN encoder-decoder with attention, or Transformer encoder-decoder) trained on serialized triple inputs; Ferreira et al. (2019) variants used in comparisons.",
            "performance_metric": "Generation: BLEU, METEOR, TER; Planner (if used for plan prediction) accuracy/BLEU-2",
            "performance_value": "Reported baseline generation BLEU (Table 2): GRU BLEU SEEN=56.09, Unseen=25.12, All=42.73; Transformer BLEU SEEN=56.28, Unseen=23.04, All=42.41. For plan prediction, these models had much lower UnSEEN planner accuracy: e.g., GRU/Transformer planner accuracy drops substantially on UnSEEN (from 0.56 SEEN to 0.10/0.09 UnSEEN in Table 1).",
            "impact_on_training": "Simple linearization is compatible with standard Seq2Seq training but leads to worse generalization to UnSEEN domains as it misses structural graph features; authors report larger accuracy drop on UnSEEN vs the GCN planner.",
            "limitations": "Lossy with respect to graph structure (misses relational/topological features), poorer generalization to unseen domains, and significant accuracy drop in plan prediction on UnSEEN data. Authors attribute these deficiencies to linearization missing graph-level structural features.",
            "comparison_with_other": "Worse generalization and lower planner accuracy than the proposed GCN planner and plan-based methods; PlanENC and DuALENC outperform these baselines on generation metrics.",
            "uuid": "e7016.2",
            "source_info": {
                "paper_title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "Structure-random traversal (random walk / BFS / DFS)",
            "name_full": "Structure-random graph traversal serializations (random walk, random BFS, random DFS)",
            "brief_description": "Baselines that produce a serialized linear order of triples by performing randomized graph traversals (random walk, random breadth-first-search, random depth-first-search) to create a sequence representation of the graph.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "structure-random traversal sequence",
            "representation_description": "Create a permutation/sequence of triples by executing a traversal over the reconstructed RDF graph (random walk, random BFS, or random DFS) and outputting triples in the visited order; used as a baseline planner input.",
            "representation_type": "sequential, traversal-based (lossy, nondeterministic if randomness used)",
            "encoding_method": "Graph traversal algorithms (random walk, BFS, DFS) applied to RDF graph to produce an ordering of triples which is used as a plan sequence.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "plan generation for graph-to-text",
            "model_name": "Structure-Random baselines",
            "model_description": "Non-learned randomized traversal procedures producing candidate plan sequences; highest score among three random strategies reported.",
            "performance_metric": "Planner evaluation: accuracy and BLEU-2 (plan BLEU)",
            "performance_value": "Planner baseline performance (Table 1): Structure-random Accuracy SEEN=0.32, UnSEEN=0.38, ALL=0.34; BLEU-2 SEEN=56.6, UnSEEN=62.9, ALL=59.5.",
            "impact_on_training": "Provides a naive serialized input for training/evaluation; used as a lower-bound baseline but does not benefit learning beyond randomization.",
            "limitations": "Non-deterministic and does not optimize for human-like plans; low accuracy relative to learned planner; ignores semantics of triples beyond graph adjacency.",
            "comparison_with_other": "Significantly outperformed by the learned GCN planner (accuracy ~0.62) and by pipeline planners; demonstrates the value of learned planning over random traversals.",
            "uuid": "e7016.3",
            "source_info": {
                "paper_title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "Step-By-Step heuristic plan extraction",
            "name_full": "Step-By-Step: heuristic/statistical plan extraction from references (Moryossef et al.)",
            "brief_description": "A pipeline approach referenced in the paper that extracts plans from reference texts using heuristic string matching or transition-based ranking; used by some pipeline systems to obtain plans for the realization model.",
            "citation_title": "Step-By-Step: Separating planning from realization in neural data-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "heuristic-extracted plan (from reference sentences)",
            "representation_description": "Plans are derived from human references via heuristics (e.g., string matching) or transition-based statistical ranking to determine the order of triples that a reference realizes; these extracted linear orders are used as training signals for pipeline realizers.",
            "representation_type": "sequential, token-based (lossy, heuristic-derived)",
            "encoding_method": "Heuristic string-matching and/or transition-based ranking to map references to an ordering over input triples; produced plans are fed to a realization model (e.g., OpenNMT) with copy mechanism.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (plans extracted from references)",
            "task_name": "pipeline plan-and-realize graph-to-text",
            "model_name": "Step-By-Step planner (pipeline) and realization with OpenNMT",
            "model_description": "Pipeline: plan extracted heuristically/statistically; realization uses standard seq2seq with copy. Reported in Moryossef et al.; compared in this paper as a competitive pipeline baseline.",
            "performance_metric": "Generation BLEU, planner accuracy (as reported elsewhere), human eval coverage/faithfulness/fluency",
            "performance_value": "In this paper Step-By-Step reported high Coverage and Faithfulness in human eval (Coverage 96.1%, Faithfulness 89.3%) but worse fluency; planner accuracy/BLEU in Table 1: Step-By-Step Accuracy SEEN=0.49, UnSEEN=0.44, ALL=0.47; BLEU-2 SEEN=73.2, UnSEEN=68.0, ALL=70.8.",
            "impact_on_training": "Provides pipeline training signal and high coverage/faithfulness at the cost of fluency when realizing triples separately; more computationally expensive in some variants (authors note Step-By-Step needed ~250s for one 7-triple instance in a cited report).",
            "limitations": "Heuristic extraction can be slow or brittle; pipeline separation can hurt fluency (Step-By-Step had worse fluency in human eval) and may be less efficient. Authors report their neural planner is more time-efficient and accurate.",
            "comparison_with_other": "Authors report their GCN-based planner is both faster and more accurate than Step-By-Step variants on planner metrics, though Step-By-Step achieves very high coverage/faithfulness in human evaluation but at the expense of fluency.",
            "uuid": "e7016.4",
            "source_info": {
                "paper_title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation",
                "publication_date_yy_mm": "2020-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        },
        {
            "paper_title": "step-by-step: Separating planning from realization in neural data-to-text generation",
            "rating": 2,
            "sanitized_title": "stepbystep_separating_planning_from_realization_in_neural_datatotext_generation"
        },
        {
            "paper_title": "improving quality and efficiency in planbased neural data-to-text generation",
            "rating": 2,
            "sanitized_title": "improving_quality_and_efficiency_in_planbased_neural_datatotext_generation"
        },
        {
            "paper_title": "Neural data-to-text generation: A comparison between pipeline and end-to-end architectures",
            "rating": 1,
            "sanitized_title": "neural_datatotext_generation_a_comparison_between_pipeline_and_endtoend_architectures"
        },
        {
            "paper_title": "GTR-LSTM: A triple encoder for sentence generation from RDF data",
            "rating": 1,
            "sanitized_title": "gtrlstm_a_triple_encoder_for_sentence_generation_from_rdf_data"
        }
    ],
    "cost": 0.01357625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation</h1>
<p>Chao Zhao ${ }^{\dagger}$, Marilyn Walker ${ }^{\ddagger}$ and Snigdha Chaturvedi ${ }^{\dagger}$<br>${ }^{\dagger}$ Department of Computer Science, University of North Carolina at Chapel Hill<br>${ }^{\ddagger}$ Natural Language and Dialog Systems Lab, University of California, Santa Cruz<br>{zhaochao, snigdha}@cs.unc.edu mawalker@ucsc.edu</p>
<h4>Abstract</h4>
<p>Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DuALENC, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.</p>
<h2>1 Introduction</h2>
<p>Data-to-text generation aims to create natural language text to describe the input data (Reiter and Dale, 2000). Here we focus on structured text input in a particular form such as a tree or a graph. Figure 1 shows an example where the input data is a mini knowledge graph, and the output text is its corresponding natural language description. Generating text from such data is helpful for many NLP tasks, such as question answering and dialogue (He et al., 2017; Liu et al., 2018; Moon et al., 2019).</p>
<p>During generation, the structure of the data as well as the content inside the structure jointly determine the generated text. For example, the direction of the edge "capital" in Figure 1 determines that "London is the capital of U.K." is an accurate description, but not vice versa. Current generation methods are based on sequence-to-sequence (Seq2Seq) encoder-decoder architecture (Sutskever et al., 2014), which requires the input data to be
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the WebNLG challenge: the source data is an RDF graph and the target output is a text description of the graph.
serialized as a sequence, resulting in a loss of structural information.</p>
<p>Recent research has shown the utility of incorporating structural information during generation. By replacing the sequential encoder with a structureaware graph encoder, such as a graph convolutional network (GCNs) (Kipf and Welling, 2017) or graph-state LSTMs (Song et al., 2018), the resulting graph-to-sequence (Graph2Seq) methods can encode the structural information of the input and thus outperform Seq2Seq models on certain tasks. However, these architectures broaden the structural gap between the encoder and decoder. That is, while the encoder receives the input data as a graph, the decoder has to create the output text as a linear chain structure.</p>
<p>This structural gap increases the difficulty of establishing alignments between source and target, which is believed to play a key role in text generation. For example, in machine translation, pre-reordering the source words into a word order that is close to that of the target sentence can yield significant improvements in translation quality (Bisazza and Federico, 2016). This suggests a need for an intermediate "planning" stage (Reiter</p>
<p>and Dale, 2000; Puduppully et al., 2019) to help with organizing the output.</p>
<p>In this work, we present a dual encoding model that is not only aware of the input graph structure but also incorporates a content planning stage. To encode the structural information in the input graph, we use a GCN based graph encoder. To narrow the ensuing structural gap, we use another GCN-based neural planner to create a sequential content plan of this graph, which is represented as a re-ordered sequence of its nodes. The plan is then encoded by an LSTM based sequential encoder. During generation, an LSTM based decoder simultaneously conditions on the two encoders, which helps it in capturing both the graph structure of the input data and the linear structure of the plan. We expect such a dual encoding (DualEnc) structure can integrate the advantages of both graph and sequential encoders while narrowing the structural gap present in single-encoder methods.</p>
<p>We evaluate the proposed planning and generation models on the WebNLG dataset (Colin et al., 2016; Gardent et al., 2017) - a widely used benchmark for data-to-text generation. Experimental results show that our neural planner achieves a $15 \%$ absolute improvement on accuracy compared to the previous best planning method. Furthermore, DualEnc significantly outperforms the previous start-of-the-art on the generation task. The human evaluation confirms that the texts generated by our model are preferred over strong baselines.</p>
<p>The contributions of this paper are three-fold:</p>
<ul>
<li>We propose a dual encoding method to narrow the structural gap between data encoder and text decoder for data-to-text generation;</li>
<li>We propose a neural planner, which is more efficient and effective than previous methods;</li>
<li>Experiments show that our method outperforms all baselines on a variety of measures.</li>
</ul>
<h2>2 Related Work</h2>
<p>This work is inspired by two lines of research: Seq2Seq generation and Graph2Seq generation.</p>
<h3>2.1 Seq2Seq Generation</h3>
<p>Traditional data-to-text generation follows a planning and realization pipeline (Reiter and Dale, 2000; Stent et al., 2004). More recent methods use Seq2Seq architecture (Sutskever et al., 2014) to combine planning and realization into an end-toend network and have achieved the state-of-the-art
on a variety of generation tasks (Lebret et al., 2016; Trisedya et al., 2018; Juraska et al., 2018; Reed et al., 2018). Despite the fair fluency and grammatical correctness, the generated text suffers from several problems such as repetition, omission, and unfaithfulness, which are less likely to happen in traditional planning-and-realization frameworks.</p>
<p>Recent work has shown that neural models can also benefit from an explicit planning step to alleviate the above-mentioned problems. The input of these planners ranges from unstructured keyphrases (Hua and Wang, 2019) to structured tables (Puduppully et al., 2019) and graphs (Ferreira et al., 2019; Moryossef et al., 2019a). Our work also focuses on planning from graph data. Compared with previous methods, we show that our neural planning method is more feasible and accurate. More importantly, rather than serializing the planning and realization stages in a pipeline, our dual encoding method simultaneously captures information from the original data and the corresponding plan.</p>
<h3>2.2 Graph2Seq Generation</h3>
<p>Graph neural networks (GNN) (Scarselli et al., 2009) aim to learn a latent state representation for each node in a graph by aggregating local information from its neighbors and the connected edges. Previous work has explored different ways of aggregating this local information, such as in GCNs (Kipf and Welling, 2017), gated graph neural networks (GGNNs) (Li et al., 2016), and Graph attention networks (GANs) (Veličković et al., 2018)</p>
<p>Several works have applied GNNs instead of Seq2Seq models for text generation (Beck et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Guo et al., 2019; Li et al., 2019), and some of them outperform Seq2Seq models. However, Damonte and Cohen (2019) use both types of encoders and show that GCN can help LSTM capture reentrant structures and long-range dependencies, albeit on a different problem than ours. Our method also uses the two types of encoders but instead of using one to assist the other, it combines them simultaneously to capture their complementary effects.</p>
<h2>3 Problem Statement</h2>
<p>In this work we focus on text generation from RDF data. ${ }^{1}$ The input for this task is a set of RDF triples, where each triple $(s, p, o)$ contains a subject, a predicate, and an object. For example, ("U.K.", "cap-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The architecture of the proposed DUALENC model. The input triples are converted as a graph and then fed to two GCN encoders for plan and text generation (Planner and Graph Encoder, top center). The plan is then encoded by an LSTM network (Plan Encoder, bottom center). Finally an LSTM decoder combines the hidden states from both the encoders to generate the text (Text Decoder, middle right).
ital", "London") is a RDF triple. The output is a natural language text with one or more sentences to describe the facts represented by this graph. Figure 1 shows an example of this task.</p>
<h2>4 Dual Encoding Model</h2>
<p>For a given input RDF graph, the aim of our method is not only to capture its structural information, but also to facilitate the information alignment between the input and output. The first goal can be achieved by employing a GCN encoder. To achieve the second goal, we first serialize and re-order the nodes of the graph as an intermediate plan using another GCN, and then feed the plan into an LSTM encoder. Finally, an LSTM decoder is used to generate the output by incorporating the context representations of both encoders. Notice that the graph and the plan are dual representations of the same input data. We encode them with two independent encoders, which can provide complementary information for decoding. The architecture of our dual encoding method is shown in Figure 2. We describe the two encoders and the decoder in the following three subsections.</p>
<h3>4.1 Graph Representation and Encoding</h3>
<p>To make it easier for GCNs to encode information from both entities and predicates, we reconstruct the input graph by regarding both entities and predicates as nodes, which is different from Figure 1.</p>
<p>Formally, for each RDF triple $(s, p, o)$, we regard the $s, p$, and $o$ as three kinds of nodes. $s$ and $o$ are identified by their entity mentions, and $p$ is identified by a unique ID. That is, two entities from different triples that have the same mentions will
be regarded as the same node. However, since we want to use predicates to distinguish between different triples, two predicates with the same mentions will be regarded as separate nodes. ${ }^{2}$
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The graph obtained from an RDF triple.</p>
<p>We use the same edge structure as Beck et al. (2018). As Figure 3 shows, a triple contains four directed edges to connect its nodes: $s \rightarrow p, p \rightarrow s$, $o \rightarrow p$, and $p \rightarrow o$. These edges help in information exchange between arbitrary neighbor pairs. There is also a special self-loop edge $n \rightarrow n$ for each node $n$ to enable information flow between adjacent iterations during feature aggregation.</p>
<p>After building the graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ from the RDF data, we use a relational GCN (R-GCN) (Schlichtkrull et al., 2018) to encode the graph and learn a state representation $\mathbf{h}_{v} \in \mathbb{R}^{d}$ for each node $v \in \mathcal{V}$ using the following iterative method:</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_R="\mathcal{R" r="r">{v}^{t}=\rho\left(\sum</em>}} \sum_{u \in \mathcal{N<em r="r" v_="v,">{v}^{r}} \frac{1}{c</em>}} \mathbf{W<em u="u">{r} \mathbf{h}</em>\right)
$$}^{(t-1)}+\mathbf{b}_{r</p>
<p>where $\mathbf{h}<em v="v">{v}^{0}=\mathbf{x}</em>}$ is the input embedding of the node $v$, and $\mathbf{h<em v="v">{v}^{t}$ is its hidden state at time-step $t$. We use the average embedding of the node mentions as $\mathbf{x}</em>$ is the set of in-neighbors of node $v$ with the edge} . \mathcal{R}$ is the set of all possible edge types, and $\mathcal{N}_{v}^{r</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The sequential decision-making process of the planning stage.
type as $r . \mathbf{W}<em r="r">{r}$ and $\mathbf{b}</em>\right|$ is a normalization term and $\rho()$ is an activation function.}$ are parameters for each edge type, which allow transformations of message to become relational-specific. $c_{v, r}=1 /\left|\mathcal{N}_{v}^{r</p>
<h3>4.2 Planning Creation and Encoding</h3>
<p>In the planning stage, we determine the content plan or order of triples (identified by their predicates) for text realization. For example, the content plan for the text in Figure 1 is: "assembly $\rightarrow$ capital $\rightarrow$ successor $\rightarrow$ manufacturer ". ${ }^{3}$</p>
<p>Learning a plan can be naturally regarded as a sequential decision-making process. That is, given a set of triples, we first determine which triple to mention/visit first, and then select the second triple from the remaining triples that have not been visited so far. This process continues until all the triples have been visited. During each decision step, the selection of the next triple can be regarded as a classification task, where the output space is all the remaining unvisited triples.</p>
<p>Figure 4 shows how our model implements this process. We first utilize the GCN encoder described in Section 4.1 to get the state representation of each node. However, while obtaining a predicate's representation, we concatenate two extra bits to the input feature $\mathbf{X}^{t}$. One is to indicate whether or not the predicate has been visited, the other to indicate the last predicate that has been visited. After the encoding, we get the final hidden state $\mathbf{h}<em i="i">{r</em>}}=\mathbf{h<em i="i">{r</em>$ as its representation, and calculate its probability of being selected as}}^{\langle T\rangle}$ for each predicate $r_{i} \in \mathcal{R</p>
<p>$$
P\left(r_{i}\right)=\operatorname{softmax}\left(\mathbf{h}<em i="i">{r</em>\right)
$$}}^{T} \mathbf{W} \overline{\mathbf{h}}_{\mathcal{R}</p>
<p>where $\overline{\mathbf{h}}_{\mathcal{R}}$ is the average pooling of all the predicate embeddings. For obtaining a plan, we select the predicate with the highest probability, append it onto the plan sequence, and then repeat the above process until all the predicates have been visited.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>After determining an order of input predicates, we complete the plan's triples by adding the corresponding subjects and objects. To better help the plan encoder (described below) capture the semantic roles of each entity and predicate, we add special tokens before Subjects, Predicates, and Objects as delimiters. For example, the plan of the example in Figure 1 will be:</p>
<div class="codehilite"><pre><span></span><code>&lt;S&gt; Aston Martin V8 &lt;P&gt; assembly &lt;O&gt; United King-
dom &lt;S&gt; United Kingdom &lt;P&gt; capital &lt;O&gt; London
&lt;S&gt; Aston Martin V8 &lt;P&gt; successor &lt;O&gt; Aston Mar-
tin Virage &lt;S&gt; Aston Martin Virage &lt;P&gt; manufacturer
&lt;O&gt; Aston Martin
</code></pre></div>

<p>Finally, we use an LSTM to encode the plan obtained above. We choose LSTM because it excels at capturing sequential information.</p>
<h3>4.3 Decoding</h3>
<p>During decoding, we adopt an LSTM-based decoder with an attention and copy mechanism. Since we have two representations of the input triple-set: the original graph and the serialized plan, we adopt two strategies for inputting context to the decoder.</p>
<p>The first strategy is to only use hidden states of the plan encoder as context. We refer to this strategy as PlanENc.</p>
<p>While the serialized plan may contain some structural information, it cannot preserve all the information of the original graph. We therefore propose a second strategy, DuALENC, to incorporate the information from both the graph and the plan. More concretely, when calculating the context state $\mathbf{m}<em t-1="t-1">{t}$ of the LSTM decoder at time step $t$, we concatenate the previous hidden state $\mathbf{z}</em>}$ and the two context vectors $\mathbf{c<em t="t">{t}^{1}$ and $\mathbf{c}</em>$ as:}^{2}$, and then update the current hidden state, $\mathbf{z}_{t</p>
<p>$$
\begin{array}{r}
\mathbf{m}<em t-1="t-1">{t}=\operatorname{MLP}\left(\left[\mathbf{z}</em>} ; \mathbf{c<em t="t">{t}^{1} ; \mathbf{c}</em>\right]\right) \
\mathbf{z}}^{2<em t-1="t-1">{t}=\operatorname{LSTM}\left(\mathbf{z}</em>},\left[\left(\mathbf{y<em t="t">{t-1} ; \mathbf{m}</em>\right]\right)\right.
\end{array}
$$</p>
<p>where $\mathbf{c}<em t="t">{t}^{1}$ and $\mathbf{c}</em>}^{2}$ are the attention-based weighted sum of the context memories from GCN and RNN encoders, respectively, and $\mathbf{y<em 0="0">{t-1}$ is the embedding of the previously generated token. The initial hidden state $\mathbf{z}</em>$ of LSTM as the context representation. For the graph encoder, we use an average of all the hidden states following a two-layer perceptron to produce the final state.}$ is the summation of the final states from the two encoders. For the plan encoder, we use the final state $\mathbf{H}^{T</p>
<h2>5 Experiments</h2>
<p>We conduct experiments to evaluate our Planner (Section 5.2) and the overall generation system (Section 5.3). ${ }^{4}$</p>
<h3>5.1 Dataset</h3>
<p>We conduct experiments on the WebNLG dataset (Gardent et al., 2017; Castro Ferreira et al., 2018) used in the WebNLG challenge. ${ }^{5}$ For each instance, the input is a set of up to 7 RDF triples from DBPedia, and the output is their text descriptions. Each triple-set is paired with a set of (up to three) humangenerated reference texts. Each reference is also paired with the order of triples it realized. We use them to train and evaluate our Planner. Overall, the dataset contains 9,674 unique triple-sets and 25,298 text references, and is divided into training, development, and test set. The test set contains two subsets, the SEEN part where the instances belong to one of the nine domains that are seen in the training and development set (such as Astronaut and Food), and the UnSEEN part where the instances are from the other five unseen domains. The UnSEEN part is designed to evaluate models' generalizability to out-of-domain instances.</p>
<h3>5.2 Experiments on Plan Generation</h3>
<p>As previous work suggests, planning plays a crucial role in text generation. We, therefore, first investigate the performance of our planner.</p>
<h3>5.2.1 Setup</h3>
<p>During the graph encoding, we initialize the node embeddings with 100-dimensional random vectors. Our GCN model has two layers, with the hidden size of each layer as 100 . The activation function is ReLU (Nair and Hinton, 2010). We optimize the training objective using Adam (Kingma and Ba, 2015) with a learning rate of 0.001 and an early stopping on the development set. The batch size is 100. We compare our results with the following six baseline planners:</p>
<ul>
<li>Random: returns a random permutation of the input triples as a plan;</li>
<li>Structure-Random: returns a random traversal over the input graph. We report the highest score among three random strategies: random walk, random BFS, and random DFS;</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>- Step-By-Step (Moryossef et al., 2019a): a transition-based statistical ranking method;
- Step-By-Step II (Moryossef et al., 2019b): a DFS-based method with a neural controller;
- GRU \&amp; Transformer (Ferreira et al., 2019): two neural Seq2Seq methods with attention;
We report the performance on three test sets: SEEN, UnSEEN, and All (SEEN \&amp; UnSEEN). We remove all one-triple instances for planner's evaluation since the planning for these instances is trivial. Results are evaluated with accuracy and BLEU-n (Papineni et al., 2002). For accuracy, we regard a plan as correct only if it exactly matches one of the human-generated plans. BLEU-n is more forgiving than accuracy. It is also adopted in Yao et al. (2019) for plan evaluation. Here we choose $n=2$.</p>
<h3>5.2.2 Results</h3>
<p>Table 1 shows results of the planning experiments. Our GCN method significantly outperforms all the baselines (approximate randomization (Noreen, 1989; Chinchor, 1992), $p&lt;0.05$ ) by a large margin on all the test sets and both measures, indicating the effectiveness of our planner. The most competitive baseline on All and UnSEEN sets is Step-By-Step, but our method is more time-efficient. For example, Step-By-Step needs 250 seconds to solve one 7-triple instance, but our method solves all 4928 instances in less than 10 seconds. For the SEEN set, the most competitive models are GRU and Transformer. However, while their accuracies drop by 0.46 on UnSEEN test set, our method drops only slightly by 0.02 , indicating our method's better generalization power.</p>
<p>We believe that this superior generalization capacity comes from the modeling of the graph structure. While the surface forms of triples in UnSEEN set do not overlap with those in the training data, the graph-level structural features are still shared, making it a key factor for generalization. GRU and Transformer linearize the graph as a sequential input, making them miss the structural information and resulting in poorer generalization capacity. Step-By-Step II also considers graph structure, but our model achieves better performance because we use GCN to encode the node representation, which can aggregate richer information from both the graph structure and the surface information.</p>
<p>We also investigated the effect of the graph size on the plan quality. In Figure 5, we separate the All test set into six subsets according to the size of input triple-sets, to reflect the model's capacity</p>
<table>
<thead>
<tr>
<th></th>
<th>Accuracy</th>
<th></th>
<th></th>
<th>BLEU-2</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>SEEN</td>
<td>UnSEEN</td>
<td>ALL</td>
<td>SEEN</td>
<td>UNSEEN</td>
<td>ALL</td>
</tr>
<tr>
<td>Random</td>
<td>0.28</td>
<td>0.34</td>
<td>0.31</td>
<td>54.1</td>
<td>62.1</td>
<td>57.9</td>
</tr>
<tr>
<td>Structure-random</td>
<td>0.32</td>
<td>0.38</td>
<td>0.34</td>
<td>56.6</td>
<td>62.9</td>
<td>59.5</td>
</tr>
<tr>
<td>Transformer (Ferreira et al., 2019)</td>
<td>0.56</td>
<td>0.09</td>
<td>0.34</td>
<td>74.3</td>
<td>20.9</td>
<td>49.3</td>
</tr>
<tr>
<td>GRU (Ferreira et al., 2019)</td>
<td>0.56</td>
<td>0.10</td>
<td>0.35</td>
<td>75.8</td>
<td>25.4</td>
<td>52.2</td>
</tr>
<tr>
<td>Step-By-Step II (Moryossef et al., 2019b)</td>
<td>0.45</td>
<td>0.44</td>
<td>0.44</td>
<td>67.7</td>
<td>67.3</td>
<td>67.5</td>
</tr>
<tr>
<td>Step-By-Step (Moryossef et al., 2019a)</td>
<td>0.49</td>
<td>0.44</td>
<td>0.47</td>
<td>73.2</td>
<td>68.0</td>
<td>70.8</td>
</tr>
<tr>
<td>GCN</td>
<td>$\mathbf{0 . 6 3}$</td>
<td>$\mathbf{0 . 6 1}$</td>
<td>$\mathbf{0 . 6 2}$</td>
<td>$\mathbf{8 0 . 8}$</td>
<td>$\mathbf{7 9 . 3}$</td>
<td>$\mathbf{8 0 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Planning results of three test sets evaluated by accuracy and BLEU-2.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Fine-grained planning results for the ALL test set. Our method outperforms all the baselines regardless of the triple size.
at a fine-grained level. Fewer input triples make the planning task easier, while the 7-triple case is the most difficult one. The accuracy of seven out of eight baselines drops to around 0 in this case, while our method achieves an accuracy of 0.19 . Besides this, our method consistently outperforms all the baselines for all the triple-set sizes.</p>
<h3>5.3 Experiments on Text Generation</h3>
<p>This section investigates the ability of our models to improve the generation quality.</p>
<h3>5.3.1 Setup</h3>
<p>We implement the generator based on the OpenNMT toolkit. ${ }^{6}$ For the graph encoder, we use a similar setting as above. Since the generation task is more complicated than planning, we increase the dimension of the input and the hidden states to 256 . The plan encoder is a 2-layer bidirectional LSTM with the same dimension setting of the GCN to ease the information fusion. During encoding, for UnSEEN test set, we adopt delexicalization (Gardent et al., 2017) to enhance the model's generalizability to unseen domains.</p>
<p>We use Adam with a batch size of 64 . The initial learning rate is set to 0.001 and is decayed with a rate of 0.7 after the eighth epoch. We continue the</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>training until the perplexity of the development set does not decrease. We also apply dropout on the decoding output layer with a rate of 0.3 .</p>
<p>The quality of the generated text (as well as those of the baselines) is evaluated through a variety of automatic measures, such as BLEU, METEOR, and TER, which are strictly the same as those applied in the official challenge. ${ }^{7}$ Following Marcheggiani and Perez-Beltrachini (2018), we report averaged performances over ten runs of the models.</p>
<p>We compare our method with the top systems of the WebNLG challenge and published state-of-theart systems. The WebNLG systems are:</p>
<ul>
<li>ADAPT: a neural system with sub-word representations to deal with rare words and sparsity.</li>
<li>TILB-SMT: a statistical machine translation method using Moses and delexicalization.</li>
<li>MELBOURNE: a Seq2Seq model with enriched delexicalization from DBPedia.
The published research models are:</li>
<li>GTR-LSTM (Trisedya et al., 2018): a graphbased triple encoder;</li>
<li>GCN-EC (Marcheggiani and PerezBeltrachini, 2018): a GCN-based triple encoder with glove embedding and copy;</li>
<li>GRU \&amp; Transformer (Ferreira et al., 2019): two pipeline methods with 5 sequential steps and GRU or Transformer as the encoder;</li>
<li>STEP-BY-STEP (Moryossef et al., 2019a): a pipeline method that generates the text from plans with OpenNMT and a copy mechanism.</li>
</ul>
<h3>5.3.2 Qualitative Results</h3>
<p>Table 2 shows the results of the automatic evaluation on the generation task. Our PlanENC achieves the best performance on BLEU and TER, while DuALENC performs best under METEOR. Both PlanENC and DuALENC significantly out-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">BLEU $(\uparrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">METEOR $(\uparrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TER $(\downarrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SEEN</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">SEEN</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">SEEN</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">All</td>
</tr>
<tr>
<td style="text-align: center;">TILB-SMT</td>
<td style="text-align: center;">54.29</td>
<td style="text-align: center;">29.88</td>
<td style="text-align: center;">44.28</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: center;">ADAPT</td>
<td style="text-align: center;">60.59</td>
<td style="text-align: center;">10.53</td>
<td style="text-align: center;">31.06</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;">MELBOURNE</td>
<td style="text-align: center;">54.52</td>
<td style="text-align: center;">33.27</td>
<td style="text-align: center;">45.13</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: center;">GTR-LSTM (2018)</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">29.20</td>
<td style="text-align: center;">37.10</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.55</td>
</tr>
<tr>
<td style="text-align: center;">GCN-EC (2018)</td>
<td style="text-align: center;">55.90</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GRU (2019)</td>
<td style="text-align: center;">56.09</td>
<td style="text-align: center;">25.12</td>
<td style="text-align: center;">42.73</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;">Transformer (2019)</td>
<td style="text-align: center;">56.28</td>
<td style="text-align: center;">23.04</td>
<td style="text-align: center;">42.41</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;">Step-By-Step (2019a)</td>
<td style="text-align: center;">53.30</td>
<td style="text-align: center;">34.41</td>
<td style="text-align: center;">47.24</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;">PlanENC</td>
<td style="text-align: center;">64.42</td>
<td style="text-align: center;">38.23</td>
<td style="text-align: center;">52.78</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;">DualENC</td>
<td style="text-align: center;">63.45</td>
<td style="text-align: center;">36.73</td>
<td style="text-align: center;">51.42</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.44</td>
</tr>
</tbody>
</table>
<p>Table 2: Generation results evaluated by BLEU, METEOR, and TER. We compare our methods with different generation systems (SMT, Sequential NMT, Graph NMT, Pipeline). Both of our methods outperform all the baselines on all three measures. We highlight both results if there is no significant difference.
perform the previous state-of-the-art (bootstrapping (Koehn and Monz, 2006), $p&lt;0.05$ ). For the SEEN part, while no existing published work performed better than ADAPT, our PlanENC achieves a 3.83 performance gain on BLEU. It also outperforms the single GCN encoder by 8.52 BLEU , which confirms the advantage of the planning stage for bridging the structural gap between the encoder and decoder. For the UnSEEN part, PlanENC and DualENC improve BLEU by 3.82 and 2.32 compared with the previous state-of-the-art. While it is difficult to distinguish the performance of DuALENC and PlanENC by automatic measures, our human experiments (see Section 5.3.4) show that dual encoding generates better text compared with PlanENC.</p>
<p>When comparing with the pipeline methods, one difference from the data perspective is how to obtain the plans of each instance to train the planner. While Step-By-Step uses heuristic string matching to extract plans from the referenced sentences, other methods (GRU and transformer), as well as ours, use plans provided in the enriched WebNLG dataset (Castro Ferreira et al., 2018). However, Step-By-Step reported worse BLEU results on these plans.</p>
<h3>5.3.3 Ablation Study</h3>
<p>To further analyze what factors contribute to the performance gain, we conduct an ablation study by removing the following components:</p>
<ul>
<li>Copy mechanism: The text is generated without copying from the source;</li>
<li>Triple planning: The input triples are shuffled before feeding into RNN, but the $(s, p, o)$</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">BLEU $(\uparrow)$</th>
<th style="text-align: center;">METEOR $(\uparrow)$</th>
<th style="text-align: center;">TER $(\downarrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PlanENC</td>
<td style="text-align: center;">$\mathbf{6 4 . 4 2} \pm 0.17$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5} \pm 0.00$</td>
<td style="text-align: center;">$\mathbf{0 . 3 3} \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">-plan</td>
<td style="text-align: center;">$57.81 \pm 0.82$</td>
<td style="text-align: center;">$0.40 \pm 0.00$</td>
<td style="text-align: center;">$0.40 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: left;">-copy</td>
<td style="text-align: center;">$61.64 \pm 0.53$</td>
<td style="text-align: center;">$0.43 \pm 0.01$</td>
<td style="text-align: center;">$0.36 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: left;">-mention</td>
<td style="text-align: center;">$61.49 \pm 0.35$</td>
<td style="text-align: center;">$0.43 \pm 0.00$</td>
<td style="text-align: center;">$0.36 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">-delimiter</td>
<td style="text-align: center;">$63.26 \pm 0.33$</td>
<td style="text-align: center;">$0.44 \pm 0.00$</td>
<td style="text-align: center;">$0.34 \pm 0.00$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of the ablation study.
inside a triple are not shuffled.</p>
<ul>
<li>Entity mentions: We join the words in a node mention with underlines (e.g., Aston-Martin instead of Aston Martin).</li>
<li>Plan delimiter: We concatenate the $(s, p, o)$ without separating them with role delimiters.
We conduct the ablation study on the SEEN testset using our PlanENC. Table 3 shows the average performance and standard deviations. Compared with PlanENC, replacing plans with a random sequence of triples hurts the BLEU score by 6.61 points, indicating that the accuracy of planning is essential for the quality of generation. Our planning also makes the model more stable to random seeds (by decreasing the standard deviation from 0.82 to 0.17 ). Removing the copy mechanism also decreases the BLEU score by 2.78 points. It demonstrates the effectiveness of copying words from the source triples rather than generating them from the vocabulary set. Removing the mention information, decreases the BLEU score by 2.93. It reflects two benefits of word mentions: to alleviate data sparsity and to coordinate with the copy mechanism. However, removing delimiters does not affect the BLEU much. Intuitively, we expected the delimiters to</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Absolute(\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Pairwise(\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CVGE FAITH</td>
<td style="text-align: center;">CVGE FAITH</td>
<td style="text-align: center;">FLCY</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MELBOURNE</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">-35.0</td>
<td style="text-align: center;">-42.5</td>
<td style="text-align: center;">-38.8</td>
</tr>
<tr>
<td style="text-align: left;">STEP</td>
<td style="text-align: center;">$\mathbf{9 6 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 9 . 3}$</td>
<td style="text-align: center;">$\mathbf{5 . 0}$</td>
<td style="text-align: center;">$\mathbf{- 3 . 7}$</td>
<td style="text-align: center;">-45.0</td>
</tr>
<tr>
<td style="text-align: left;">E2E-TRANS</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">-21.2</td>
<td style="text-align: center;">-32.5</td>
<td style="text-align: center;">-21.2</td>
</tr>
<tr>
<td style="text-align: left;">GCN</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">-48.7</td>
<td style="text-align: center;">-50.0</td>
<td style="text-align: center;">-26.3</td>
</tr>
<tr>
<td style="text-align: left;">PlanENC</td>
<td style="text-align: center;">$\mathbf{9 2 . 3}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{- 7 . 5}$</td>
<td style="text-align: center;">$\mathbf{- 1 2 . 5}$</td>
<td style="text-align: center;">$\mathbf{- 7 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">DUALENC</td>
<td style="text-align: center;">$\mathbf{9 4 . 5}$</td>
<td style="text-align: center;">$\mathbf{9 1 . 8}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of human evaluation. DuALENC outperforms most of the baselines on all measures.
help the LSTM capture the boundaries and semantic roles of each node, but the ablation study does not support it. We provide an example in Table 5 to show that the LSTM indeed has trouble learning such semantic roles.</p>
<h3>5.3.4 Human Evaluation</h3>
<p>Automatic measures are based on lexical similarities and are not good measures of text quality in general. We therefore further conduct a human evaluation on Amazon Mechanical Turk to better access the quality of the generated texts. We evaluate the results for MELBOURNE, Step-By-Step, Transformer, GCN, as well as our PlanENC and DuALENC. We randomly select 80 test instances (440 triples in total) with the size of tripleset between 4 to 7 , since they are more challenging than those with fewer triples. Then we evaluate the generation quality of each system with the following three measures:</p>
<ul>
<li>Coverage: the percentage of triples that are covered by the generated text (all $<s, p, o>$ values in the triples are realized);</li>
<li>Faithfulness: the percentage of triples that are faithfully described by the text (the text correctly expresses the predicate and also the subject and object as its arguments. No substitutions or hallucinations);</li>
<li>Fluency: a measure of the fluency or naturalness of the generated text.
For coverage and faithfulness, workers are asked to check each triple of an instance, and judge whether the triple is covered and faithfully described by the generated text. For fluency, we ask another group of workers to compare between two outputs of the same instance and identify which one is more fluent. Table 5 shows examples where these qualities are compromised.</li>
</ul>
<p>In Table 4, we report the absolute scores of
coverage and faithfulness, which range from 0 to $100 \%$. We also provide pairwise scores of all three measures by comparing the outputs of DuALENC with each of the other five systems. We report the percentage of instances that were judged to be worse/better/same than those of DuALENC, yielding a score ranging from $-100 \%$ (unanimously worse) to $100 \%$ (unanimously better). For example, MELBOURNE performs better/worse/same than DuALENC for $10 \% / 45 \% / 45 \%$ of the instances, yielding a pairwise score as $10 \%-45 \%=-0.35 \%$. We also report an overall pairwise score combining all three measures. For each instance, the overall score of one output is higher than the other iff it outperforms the other on at least one of the three measures and has a better or equal vote on the other two.</p>
<p>Our PlanENC and DuALENC outperform most of the baselines on all of the measures by a large margin (approximate randomization, $p&lt;0.05$. ), which is consistent with the automatic results. The only exception is Step-By-Step, which has high Coverage and Faithfulness (not significant). It first separates the input triples into smaller subsets and then realizes them separately. This greatly reduces the difficulty of long-term generation but at the expense of Fluency (worst among all the baselines). GCN does not perform well on Coverage, which demonstrates that the structural gap between encoding and decoding indeed makes generation more difficult. However, it has the smallest difference between Coverage and Faithfulness among all the baselines, indicating that the fidelity of generation can benefit from the encoding of graph-level structural information. By combining GCN and PLANENC, our DuALENC incorporates the advantages of both encoders while ameliorating their weaknesses, and therefore achieves the best OVERALL performance on human evaluation.</p>
<h3>5.4 Qualitative Analysis</h3>
<p>Table 5 shows examples of generated texts by various systems for an input of six triples. Colored fonts represent missing, unfaithful, and unfluent information. For example, PlanENC misses "Buzz Aldrin" and also wrongly expresses the subject of "retirement" as "Frank Borman", indicating that LSTM is less powerful at capturing the semantic roles of entities. This disadvantage can be well complemented by GCN, which is designed to capture the graph structure and the relations between entities. Hence, by incorporating information from</p>
<p>| Tripleset | (William Anders | birthPlace | British Hong Kong), (William Anders | was a crew member of | Apollo 8), <br> (Apollo 8 | crewMembers | Frank Borman), (Apollo 8 | backup pilot | Buzz Aldrin), (Apollo 8 | operator | <br> NASA), (William Anders | dateOfRetirement | 1969-09-01) |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- |</p>
<p>Table 5: Sample texts generated by our methods and baselines, compared with a human-provided reference. We highlight in different color the [missing], unfaithful, and unfluent parts of each text. Only the results of our DuALENC correctly mention all the input triples.
both GCN and LSTM, DuALENC correctly expresses the subject argument of "retirement".</p>
<h2>6 Conclusion</h2>
<p>This paper proposes DuALENC, a dual encoding method to bridge the structural gap between encoder and decoder for data-to-text generation. We use GCN encoders to capture the structural information of the data, which is essential for accurate planning and faithful generation. We also introduce an intermediate content planning stage to serialize the data and then encode it with an LSTM network. This serialized plan is more compatible with the output sequence, making the information alignment between the input and output easier. Experiments on WebNLG dataset demonstrate the effectiveness of our planner and generator by outperforming the previous state-of-the-art by a large margin. Future work will validate the effectiveness of this method on more varied data-to-text generation tasks.</p>
<h2>References</h2>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 273-283.</p>
<p>Arianna Bisazza and Marcello Federico. 2016. A survey of word reordering in statistical machine translation: Computational models and language phenomena. Computational Linguistics, 42(2):163-205.</p>
<p>Thiago Castro Ferreira, Diego Moussallem, Sander Wubben, and Emiel Krahmer. 2018. Enriching the
webnlg corpus. In Proceedings of the 11th International Conference on Natural Language Generation, INLG'18, Tilburg, The Netherlands. Association for Computational Linguistics.</p>
<p>Nancy Chinchor. 1992. The statistical significance of the muc-4 results. In Proceedings of the 4th conference on Message understanding, pages 30-50. Association for Computational Linguistics.</p>
<p>Emilie Colin, Claire Gardent, Yassine M’rabet, Shashi Narayan, and Laura Perez-Beltrachini. 2016. The webnlg challenge: Generating text from dbpedia data. In Proceedings of the 9th International Natural Language Generation conference, pages 163167.</p>
<p>Marco Damonte and Shay B Cohen. 2019. Structural neural encoders for amr-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3649-3658.</p>
<p>Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 552-562.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133.</p>
<p>Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transactions of the Association for Computational Linguistics, 7:297-312.</p>
<p>Shizhu He, Cao Liu, Kang Liu, and Jun Zhao. 2017. Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 199208.</p>
<p>Xinyu Hua and Lu Wang. 2019. Sentence-level content planning and style specification for neural text generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 591-602.</p>
<p>Juraj Juraska, Panagiotis Karagiannis, Kevin Bowden, and Marilyn Walker. 2018. A deep ensemble model with slot alignment for sequence-to-sequence natural language generation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages $152-162$.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Thomas N. Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.</p>
<p>Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proceedings on the Workshop on Statistical Machine Translation, pages 102121 .</p>
<p>Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213.</p>
<p>Wei Li, Jingjing Xu, Yancheng He, Shengli Yan, Yunfang Wu, et al. 2019. Coherent comment generation for chinese articles with a graph-to-sequence model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4843-4852.</p>
<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. Gated graph sequence neural networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Shuman Liu, Hongshen Chen, Zhaochun Ren, Yang Feng, Qun Liu, and Dawei Yin. 2018. Knowledge
diffusion for neural dialogue generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1489-1498.</p>
<p>Diego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for structured data to text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 1-9.</p>
<p>Seungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. 2019. Opendialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 845-854.</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019a. step-by-step: Separating planning from realization in neural data-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2267-2277.</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019b. improving quality and efficiency in planbased neural data-to-text generation. In Proceedings of the 12th International Conference on Natural Language Generation, pages 377-382.</p>
<p>Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807-814.</p>
<p>Eric W Noreen. 1989. Computer-intensive methods for testing hypotheses. Wiley New York.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311-318. Association for Computational Linguistics.</p>
<p>Ratish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with content selection and planning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6908-6915.</p>
<p>Lena Reed, Shereen Oraby, and Marilyn Walker. 2018. Can neural generators for dialogue learn sentence planning and discourse structuring? INLG 2018, page 284 .</p>
<p>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge university press.</p>
<p>Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2009. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80.</p>
<p>Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593-607. Springer.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for amr-to-text generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16161626 .</p>
<p>Amanda Stent, Rashmi Prassad, and Marilyn Walker. 2004. Trainable sentence planning for complex information presentations in spoken dialog systems. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL04), pages $79-86$.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112.</p>
<p>Bayu Distiawan Trisedya, Jianzhong Qi, Rui Zhang, and Wei Wang. 2018. Gtr-lstm: A triple encoder for sentence generation from rdf data. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1627-1637.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph Attention Networks. International Conference on Learning Representations. Accepted as poster.</p>
<p>Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-and-write: Towards better automatic storytelling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7378-7385.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://github.com/OpenNMT/OpenNMT-py&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ That is why some of the numbers in our table are not exactly the same as those in the cited works.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
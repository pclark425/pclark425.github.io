<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2687 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2687</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2687</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-8f7297454d7f44365b9bcda5ebb9439a43daf5e6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8f7297454d7f44365b9bcda5ebb9439a43daf5e6" target="_blank">Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This study defines a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency and believes it can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.</p>
                <p><strong>Paper Abstract:</strong> Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2687.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2687.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verbalized Confidence (Vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla Verbalized Confidence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting an LLM to output a numeric or verbal estimate of its confidence in its answer (e.g., 0%–100%) without accessing internal model states or fine-tuning; used as a direct, semantic uncertainty signal tied to the model's textual answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Verbalized Confidence (Vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A black-box method in which the LLM is prompted to produce its answer together with a confidence value (explicit percentage). Implemented with simple prompts ("Read the question, provide your answer, and your confidence in this answer") and evaluated across multiple LLMs and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based (black-box verbal reporting)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / general QA and reasoning tasks (commonsense, arithmetic, symbolic, ethics, professional knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Verbalized percentages tied to semantic answer (no internal-likelihood calibration), i.e., the model states plausibility as a numeric confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Expected Calibration Error (ECE) for calibration; AUROC, AUPRC-Positive, AUPRC-Negative for failure prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical benchmarking on multiple datasets and LLMs; comparison against ground-truth labels to compute ECE/AUROC metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Prompts and code released (paper repository referenced); standardized datasets and evaluation metrics used.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Used failure-prediction metrics (AUROC, AUPRC) and calibration (ECE) to identify overconfident/hallucinated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td>Not directly reported as a single rate; analyses show substantial overconfidence with many incorrect answers assigned 100% confidence (empirical distributions shown).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Direct verbalized confidence (percentages) and comparison with answer correctness; combined later with sampling/aggregation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>GSM8K, SVAMP, DateUnd, ObjectCou, SportUND, StrategyQA, Prf-Law, Biz-Ethics (as used in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Vanilla verbalized confidence is highly overconfident: confidence values concentrated in 80%–100% range; example aggregated results (average across tasks/models): GPT-4 average ECE ≈ 0.18 and AUROC ≈ 0.627; earlier models show substantially higher ECEs (e.g., GPT-3 average ECE ≈ 0.52). (See paper Table 2.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Tends to be highly overconfident and poorly discriminative for failure prediction; outputs often concentrated in multiples of 5 (human-like reporting), limiting granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2687.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-augmented prompting method that instructs the model to 'think step by step' before giving an answer, used to improve accuracy and (indirectly) calibration of verbalized confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Zero-shot Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A prompting strategy that asks models to generate intermediate reasoning steps (chain of thought) before the final answer and confidence; used in experiments to improve answer accuracy and calibration metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based prompting technique</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / reasoning benchmarks (math, commonsense, symbolic, ethics, professional knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Indirect: improved reasoning steps help model better judge its own answer plausibility, reflected in reduced ECE in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>ECE, AUROC, AUPRC used to evaluate confidence quality post-CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical evaluation comparing CoT to vanilla prompts across datasets and LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Prompts specified in appendices; experiments on public datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>CoT can reduce certain hallucinations by improving internal reasoning, but does not eliminate overconfidence.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Uses verbalized confidence produced after explicit reasoning; combined with sampling/aggregation for improved uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same as paper (GSM8K, SVAMP, DateUnd, ObjectCou, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CoT improved accuracy and reduced ECE in many settings (example: GPT-3.5 on GSM8K accuracy from 28% to 80.3% and ECE from 66 to 10 ; see Table 8); however, failure prediction (AUROC) may still be near random for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Typically outperforms vanilla prompting on calibration and accuracy; effect size diminishes with stronger models (GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Can lead to uniform high-confidence outputs (e.g., assigning 100% to all samples) which reduces ECE but harms discriminative failure prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2687.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Probing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Probing Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step black-box method: generate an answer in one session, then in a separate session ask the model to analyze that answer and provide a confidence estimate about its correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-Probing</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompting strategy that asks the model to assess a previously generated answer: generate answer in one chat session, then in another ask 'How likely is the above answer to be correct?' with concise reasoning and a confidence score.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based prompting technique (black-box)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / reasoning and QA</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Model inspects its own answer and provides a plausibility/confidence score based on its internal reasoning as expressed in the second session.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Evaluated using ECE and AUROC in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Benchmarked against other prompting strategies; shown to provide consistent advantage on GPT-4 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Detailed prompts provided in paper appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Self-Probing can surface when the model detects flaws in its own answer, reducing overconfidence in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Verbalized confidence produced after explicit critique/analysis of the generated answer.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Datasets used in paper (GSM8K, DateUnd, StrategyQA, Prf-Law, Biz-Ethics, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as among the more consistent prompting strategies; on GPT-4 Self-Probing maintained consistent advantage for calibration/failure prediction in aggregate (exact dataset-by-dataset numbers in paper figures/tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Improves over vanilla prompting in many settings; no single prompt consistently best across all datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Improvement diminishes with stronger models; still struggles on tasks requiring specialized/professional knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2687.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Step</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Step Verbalized Confidence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting the model to break reasoning into K steps and report a confidence for each step, then aggregate step confidences multiplicatively to produce an overall confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-Step Verbalized Confidence</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompt instructs the model to produce intermediate steps S_i and a confidence C_i for each step; the final confidence is computed as product over steps C_multi = ∏ C_i, aiming to reduce overconfidence by exposing uncertainty at sub-step level.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based prompting technique (black-box)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility judged via per-step confidences that are aggregated, allowing identification of uncertain sub-decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>ECE and AUROC used to evaluate aggregated per-step confidences.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical evaluation showing reduced overconfidence in some datasets; Table 9 reports mixed improvements across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Prompts and aggregation formula provided (product of per-step confidences).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>By revealing uncertain steps, incorrect reasoning chains can be detected more easily than single-step verbal confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Decomposed step-level confidences aggregated multiplicatively to quantify final uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same paper datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mixed results: in some tasks ECE reduced and AUROC improved; in others performance degraded relative to CoT (see Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Sometimes improves calibration/failure prediction relative to vanilla; not uniformly superior across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Aggregation by product assumes independence and can collapse confidences to very low values; sensitive to how steps are specified and counted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2687.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-K Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-K Verbalized Confidence Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting the model to output its K best candidate answers and a probability/confidence for each candidate, thereby normalizing confidence across multiple plausible outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Top-K Verbalized Confidence</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Top-K prompting asks the LLM to produce its top K guesses with associated percent confidences; later aggregation (Pair-Rank or Avg-Conf) converts candidate rankings/confidences into a calibrated probability distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based prompting and ranking aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / reasoning and QA</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates multiple candidate answers per query (explicit candidate enumeration).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Uses model-provided probabilities per candidate and cross-sample ranking information to infer plausibility of each candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>ECE, AUROC used to evaluate quality of aggregated Top-K confidences and ranking-derived distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical comparisons in paper; Pair-Rank aggregation (see below) applied to Top-K outputs improves calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Prompt templates and Pair-Rank formulation provided; Top-K K value varied experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>By enumerating alternative answers and their confidences, the method exposes uncertainty and reduces single-answer overconfidence.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Direct candidate confidences and ranking events converted into a categorical distribution via Pair-Rank MLE-inspired optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Paper datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Top-K combined with Pair-Rank yielded the lowest average ECE (as low as 0.028 in some settings); Top-K + Avg-Conf improved AUROC for failure prediction in multiple datasets (see Table 4 and Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Top-K outperforms vanilla verbalized confidence on average; especially effective when paired with aggregation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires multiple outputs per query (more API calls); relies on accurate self-reported candidate confidences which can still be miscalibrated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2687.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sampling Strategies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling Strategies (Self-Random, Prompt Paraphrasing, Misleading prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods to elicit multiple model responses to estimate uncertainty from response variance: use randomness (temperature), paraphrase prompts, or inject misleading hints to probe stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sampling Strategies for Black-box Uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three families: (1) Self-Random: re-query with randomness/temperature to sample from model output distribution; (2) Prompting: paraphrase the question to induce varied responses; (3) Misleading: give misleading cues (weak/strong/external) to test whether model sticks to initial answer, using variance as an uncertainty signal.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based ensemble / sampling methods (black-box)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates multiple candidate answers via repeated queries or perturbed prompts; not a hypothesis engine per se but used to assess answer uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility inferred from consistency (agreement) across sampled responses; high variance indicates lower plausibility/confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Measured via ECE and AUROC of aggregated confidences based on sampled responses.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical tests show sampling+consistency substantially improve failure prediction (e.g., Self-Random M=5 on GPT-3.5 achieved AUROC up to 0.927 on GSM8K; aggregate average AUROC improvements noted).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Sampling strategies and prompt variants documented; number of samples M varied and effects reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Using misleading prompts can reveal lack of robustness (a proxy for hallucination tendency); consistency-based aggregation reduces single-shot hallucination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>High disagreement across sampled responses flags potential hallucinations or uncertain claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Use consistency (fraction of samples matching chosen answer), Avg-Conf (weighted by verbalized confidences), and Pair-Rank over Top-K rankings as uncertainty estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Paper datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Sampling with M=5 and Consistency aggregation improved AUROC and ECE over M=1; eg. Self-Random (M=5) on GSM8K (GPT-3.5) ECE≈0.0628 (6.28 in table units) and AUROC≈0.927; overall average AUROC for sampling methods ≈0.73 in reported experiments (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Significantly outperforms single-shot verbalized confidence on failure prediction and often on calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Increased computational cost proportional to number of samples; diminishing returns as M grows; effectiveness varies with task type.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2687.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consistency Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregator computing confidence as the empirical agreement rate among M sampled responses: confidence = (1/M) * sum I{Y_i == Y_final}.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Consistency Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Measures agreement among sampled candidate answers and uses fraction matching the chosen answer as its confidence estimate; simple and effective for failure prediction when multiple samples are available.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Aggregation / ensemble-based uncertainty estimator</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>High agreement implies higher plausibility; low agreement implies uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Evaluated with ECE and AUROC in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical results show consistency often yields strong failure prediction (high AUROC) when paired with adequate sampling, e.g., large AUROC boosts for arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Formula and experimental settings provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Disagreement across samples used to flag likely hallucinations or uncertain claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Empirical agreement (consistency) among re-sampled answers.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Paper datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Consistency aggregation with sampling achieved high AUROC on arithmetic tasks (e.g., AUROC up to 0.927 on GSM8K with Self-Random M=5) and improved calibration in many settings (see Table 3 and Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms single-shot verbalized confidence in failure prediction and often improves ECE when sampling used.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires multiple queries; can be insensitive when model outputs identical but incorrect answers consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2687.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Avg-Conf</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Average-Confidence (Avg-Conf) Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregator that combines sampled responses and their verbalized confidences into a weighted measure: numerator sums confidences for samples equal to final answer; denominator sums confidences of all samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Avg-Conf Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Aggregates multiple sampled responses with their verbalized confidences to compute a confidence score using C_conf = sum_{i} I{Y_i==Y_final} * C_i / sum_i C_i, leveraging both consistency and verbalized confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Aggregation / ensemble-based uncertainty estimator</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Combines consistency and self-reported confidences to favor answers that are both frequent and assigned high per-sample confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>ECE and AUROC used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical results: Avg-Conf tends to boost failure prediction (AUROC) relative to consistency-only or verbalized-only methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Aggregation formula and experimental protocol provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Low weighted agreement indicates uncertain/hallucinated predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Weighted average of verbalized confidences conditioned on agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Paper datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Avg-Conf surpasses other aggregators in AUROC in five of six datasets in Table 4; mean AUROC reported ≈0.669 in those experiments (see Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Better failure-prediction than consistency-only and often better than Pair-Rank for AUROC.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on quality of per-sample verbalized confidences which can be overconfident or coarse-grained.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2687.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pair-Rank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pair-Rank Aggregation (Ranking MLE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregation method that uses pairwise ranking events among Top-K candidates to infer a categorical distribution P over answers by minimizing a ranking-based log-loss; suited for Top-K outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pair-Rank Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs pairwise ordering events from multiple Top-K outputs and estimates a categorical distribution P over answers by minimizing -sum I{u ≻ v} * log(P(u)/(P(u)+P(v))) subject to simplex constraint; solved via softmax parametrization and gradient optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Aggregation / ranking-based uncertainty estimator</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / uncertainty estimation and calibration</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Uses observed pairwise preference data to estimate answer probabilities, relying on ranking information which may be more reliable than self-reported confidences.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Evaluated with ECE and AUROC; Pair-Rank excels at reducing ECE (calibration).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical evaluation showed Pair-Rank achieves lowest average ECE in experiments (mean ECE ≈0.069 in one reported config, Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Derivation and optimization approach provided; implementation details discussed (softmax parametrization, gradient descent).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>By learning distribution from rankings, inconsistent/hallucinated answers can receive lower probability mass.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Estimated categorical distribution P derived from pairwise ranking likelihoods; probabilities used as confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Paper datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Pair-Rank produced the lowest average ECE in Table 4 (mean ECE ≈0.069) and competitive AUROC (mean ≈0.676 in that experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Better calibration (ECE) than Consistency and Avg-Conf on average; Avg-Conf may beat Pair-Rank on AUROC.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Optimization assumes Top-K draws without replacement from categorical distribution; computational overhead for fitting P with many candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2687.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq-Prob / Len-Norm-Prob / Token-Prob</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-probability-based White-box Methods (Sequence Probability, Length-Normalized Sequence Probability, Key Token Probability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>White-box methods that use token-output probabilities (logits) to estimate output likelihoods and derive confidence scores: aggregate token probabilities, length-normalize them, or focus on key output tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Token-probability-based White-box Methods</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three methods implemented by the authors for white-box settings: (1) Seq-Prob: product or sum of token probabilities for the output sequence; (2) Len-Norm-Prob: sequence probability normalized by length (e.g., seq-prob^{1/length}); (3) Token-Prob: use probability of result-specific token(s) as confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based white-box (logit-based) uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Higher token/sequence probabilities interpreted as higher plausibility of the produced answer.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>ECE, AUROC, AUPRC-P, AUPRC-N used to evaluate these scores.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Authors compared these white-box methods against black-box verbalized confidence across datasets; white-box generally outperforms black-box but gap is modest.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Implementation details described; applied where token probabilities are available (white-box/APIs providing logits).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Low token/sequence probability can indicate uncertain or hallucinated outputs, though token-likelihood focuses on next-token uncertainty and may miss semantic falsity.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Logit/token-probability aggregation with optional length normalization or key-token focus.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Paper datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>White-box methods typically achieved better AUROC/ECE than black-box verbalized confidence; example: token-prob AUROC up to ~0.606 in some datasets, and seq-prob often improved AUROC into 0.56–0.70 ranges depending on dataset (see Table 5 and Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>White-box methods outperform black-box verbalized confidence on average, but the gap in AUROC is modest (paper cites examples like 0.522 to 0.605).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Token-likelihood reflects next-token uncertainty rather than semantic correctness; can fail on semantically false but locally probable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2687.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCDropout</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Dropout (MCDropout)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A white-box approximation to Bayesian inference where dropout is applied at inference multiple times to obtain a distribution of outputs; referenced as an analogy for ensemble-based uncertainty estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dropout as a bayesian approximation: Representing model uncertainty in deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Monte Carlo Dropout</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A technique (cited) that performs multiple forward passes with dropout enabled at inference to obtain predictive distributions and estimate epistemic uncertainty; mentioned as conceptual inspiration for sampling-based black-box methods.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Neural Bayesian approximation / white-box method</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / uncertainty quantification</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Uncertainty inferred from variability across stochastic forward passes.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Ensemble of stochastic forward passes producing variance-based uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires white-box access and architectural support (dropout at inference).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2687.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Ensembles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An uncertainty estimation approach using an ensemble of independently trained models to capture predictive uncertainty by measuring disagreement across ensemble members.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simple and scalable predictive uncertainty estimation using deep ensembles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced classical ensemble approach where multiple models trained independently produce output distributions whose spread is used to estimate uncertainty; cited as conceptual antecedent to sampling+aggregation in black-box LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Neural ensemble-based uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / uncertainty quantification</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Disagreement across models indicates lower plausibility/confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Variance/disagreement across ensemble predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires training and maintaining multiple models; not directly applicable in closed-box API settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2687.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic Uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Uncertainty (Linguistic Invariances for Uncertainty Estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concept introduced by Kuhn et al. that aims to estimate uncertainty based on semantic invariances rather than token-likelihoods, addressing limitations of logit-based uncertainty for semantic correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Semantic Uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An approach (cited) that quantifies uncertainty using semantic-level invariances (e.g., paraphrase robustness) rather than surface token probabilities, proposed to better capture semantic correctness and reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based semantic-level uncertainty estimator / research concept</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / uncertainty estimation and hallucination detection</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Based on invariance of meaning across linguistic transformations; lack of invariance indicates uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Targets hallucination by focusing on semantic consistency rather than token-level likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Semantic invariance checks across paraphrases/transformations to quantify confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>May be computationally intensive; cited as promising but costly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e2687.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>External Calibrator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>External Calibrator (Mielke et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses external components (often relying on internal model representations) to post-hoc calibrate conversational agents and reduce overconfidence via linguistic calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reducing conversational agents’ overconfidence through linguistic calibration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>External Calibrator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced method that depends on accessing internal representations to calibrate model outputs linguistically; not applicable in fully black-box API-only settings according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Calibration / post-hoc adjustment (white-box or partial-access)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / conversational agents</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Post-hoc transformation of model outputs informed by internal signals or auxiliary models to match true accuracy distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Aims to reduce overconfident assertions by recalibrating verbal expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>External model or calibrator maps model outputs to calibrated confidence scores.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on internal model access or representations not available in closed-box APIs; less applicable in pure black-box settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e2687.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lin et al. (Verbalized Confidence Fine-tune)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teaching models to express their uncertainty in words</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that trained models (via fine-tuning) to output verbalized confidence; focused on settings where training data includes confidence labels rather than black-box zero-shot elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teaching models to express their uncertainty in words</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fine-tuned Verbalized Confidence (Lin et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced approach that fine-tunes models with datasets containing ground-truth confidence labels to produce calibrated verbalized confidences; differs from the black-box zero-shot focus of the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / calibration</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Supervised learning mapping to ground-truth confidences.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Supervised verbalized confidence as learned output.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Fine-tuned approaches can outperform zero-shot verbalized confidence but require labeled datasets and expensive fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires labeled confidence data and substantial compute to fine-tune models; less applicable for closed-box APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e2687.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tian et al. (Prompting for Calibration)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concurrent study focused on prompting strategies for eliciting calibrated confidence scores from models (mainly RLHF-fine-tuned LMs); cited and partially used as a baseline framework (Top-K prompt is motivated by this work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Prompt-based Calibration Strategies (Tian et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited work that studies various prompting approaches (including Top-K) to elicit calibrated confidences especially for models fine-tuned with human feedback; influenced the Top-K and prompting components in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM prompting / calibration strategies</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / calibration</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Prompt engineering to improve calibration of verbalized confidences.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Focused primarily on RLHF-fine-tuned LMs; not comprehensive for closed-box APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2687.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e2687.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hallucination & Uncertainty (Xiao & Wang)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>On hallucination and predictive uncertainty in conditional language generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work linking hallucination in generation to predictive uncertainty; cited to motivate need for reliable uncertainty estimates to reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On hallucination and predictive uncertainty in conditional language generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hallucination-Uncertainty Analyses</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prior analysis that studies the relationship between hallucination and model uncertainty in conditional generation and suggests that better uncertainty estimates can mitigate hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Analytical / empirical study</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / hallucination detection and uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Argues for uncertainty-aware generation and calibration to reduce hallucinated claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teaching models to express their uncertainty in words <em>(Rating: 2)</em></li>
                <li>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback <em>(Rating: 2)</em></li>
                <li>Dropout as a bayesian approximation: Representing model uncertainty in deep learning <em>(Rating: 1)</em></li>
                <li>Simple and scalable predictive uncertainty estimation using deep ensembles <em>(Rating: 1)</em></li>
                <li>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation <em>(Rating: 2)</em></li>
                <li>Reducing conversational agents’ overconfidence through linguistic calibration <em>(Rating: 1)</em></li>
                <li>On hallucination and predictive uncertainty in conditional language generation <em>(Rating: 1)</em></li>
                <li>Language models (mostly) know what they know <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2687",
    "paper_id": "paper-8f7297454d7f44365b9bcda5ebb9439a43daf5e6",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "Verbalized Confidence (Vanilla)",
            "name_full": "Vanilla Verbalized Confidence",
            "brief_description": "Prompting an LLM to output a numeric or verbal estimate of its confidence in its answer (e.g., 0%–100%) without accessing internal model states or fine-tuning; used as a direct, semantic uncertainty signal tied to the model's textual answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Verbalized Confidence (Vanilla)",
            "system_description": "A black-box method in which the LLM is prompted to produce its answer together with a confidence value (explicit percentage). Implemented with simple prompts (\"Read the question, provide your answer, and your confidence in this answer\") and evaluated across multiple LLMs and datasets.",
            "system_type": "LLM-based (black-box verbal reporting)",
            "scientific_domain": "NLP / general QA and reasoning tasks (commonsense, arithmetic, symbolic, ethics, professional knowledge)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Verbalized percentages tied to semantic answer (no internal-likelihood calibration), i.e., the model states plausibility as a numeric confidence.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Expected Calibration Error (ECE) for calibration; AUROC, AUPRC-Positive, AUPRC-Negative for failure prediction.",
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Empirical benchmarking on multiple datasets and LLMs; comparison against ground-truth labels to compute ECE/AUROC metrics.",
            "reproducibility_measures": "Prompts and code released (paper repository referenced); standardized datasets and evaluation metrics used.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Used failure-prediction metrics (AUROC, AUPRC) and calibration (ECE) to identify overconfident/hallucinated outputs.",
            "hallucination_rate": "Not directly reported as a single rate; analyses show substantial overconfidence with many incorrect answers assigned 100% confidence (empirical distributions shown).",
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Direct verbalized confidence (percentages) and comparison with answer correctness; combined later with sampling/aggregation methods.",
            "benchmark_dataset": "GSM8K, SVAMP, DateUnd, ObjectCou, SportUND, StrategyQA, Prf-Law, Biz-Ethics (as used in the paper)",
            "performance_metrics": "Vanilla verbalized confidence is highly overconfident: confidence values concentrated in 80%–100% range; example aggregated results (average across tasks/models): GPT-4 average ECE ≈ 0.18 and AUROC ≈ 0.627; earlier models show substantially higher ECEs (e.g., GPT-3 average ECE ≈ 0.52). (See paper Table 2.)",
            "comparison_with_baseline": null,
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Tends to be highly overconfident and poorly discriminative for failure prediction; outputs often concentrated in multiples of 5 (human-like reporting), limiting granularity.",
            "uuid": "e2687.0",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Zero-shot Chain-of-Thought (CoT) prompting",
            "brief_description": "A reasoning-augmented prompting method that instructs the model to 'think step by step' before giving an answer, used to improve accuracy and (indirectly) calibration of verbalized confidence.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "use",
            "system_name": "Zero-shot Chain-of-Thought (CoT)",
            "system_description": "A prompting strategy that asks models to generate intermediate reasoning steps (chain of thought) before the final answer and confidence; used in experiments to improve answer accuracy and calibration metrics.",
            "system_type": "LLM-based prompting technique",
            "scientific_domain": "NLP / reasoning benchmarks (math, commonsense, symbolic, ethics, professional knowledge)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Indirect: improved reasoning steps help model better judge its own answer plausibility, reflected in reduced ECE in experiments.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "ECE, AUROC, AUPRC used to evaluate confidence quality post-CoT prompting.",
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Empirical evaluation comparing CoT to vanilla prompts across datasets and LLMs.",
            "reproducibility_measures": "Prompts specified in appendices; experiments on public datasets.",
            "hallucination_prevention_method": "CoT can reduce certain hallucinations by improving internal reasoning, but does not eliminate overconfidence.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Uses verbalized confidence produced after explicit reasoning; combined with sampling/aggregation for improved uncertainty estimates.",
            "benchmark_dataset": "Same as paper (GSM8K, SVAMP, DateUnd, ObjectCou, etc.)",
            "performance_metrics": "CoT improved accuracy and reduced ECE in many settings (example: GPT-3.5 on GSM8K accuracy from 28% to 80.3% and ECE from 66 to 10 ; see Table 8); however, failure prediction (AUROC) may still be near random for some tasks.",
            "comparison_with_baseline": "Typically outperforms vanilla prompting on calibration and accuracy; effect size diminishes with stronger models (GPT-4).",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Can lead to uniform high-confidence outputs (e.g., assigning 100% to all samples) which reduces ECE but harms discriminative failure prediction.",
            "uuid": "e2687.1",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Self-Probing",
            "name_full": "Self-Probing Prompting",
            "brief_description": "A two-step black-box method: generate an answer in one session, then in a separate session ask the model to analyze that answer and provide a confidence estimate about its correctness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Self-Probing",
            "system_description": "Prompting strategy that asks the model to assess a previously generated answer: generate answer in one chat session, then in another ask 'How likely is the above answer to be correct?' with concise reasoning and a confidence score.",
            "system_type": "LLM-based prompting technique (black-box)",
            "scientific_domain": "NLP / reasoning and QA",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Model inspects its own answer and provides a plausibility/confidence score based on its internal reasoning as expressed in the second session.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Evaluated using ECE and AUROC in experiments.",
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Benchmarked against other prompting strategies; shown to provide consistent advantage on GPT-4 in experiments.",
            "reproducibility_measures": "Detailed prompts provided in paper appendix.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Self-Probing can surface when the model detects flaws in its own answer, reducing overconfidence in some settings.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Verbalized confidence produced after explicit critique/analysis of the generated answer.",
            "benchmark_dataset": "Datasets used in paper (GSM8K, DateUnd, StrategyQA, Prf-Law, Biz-Ethics, etc.)",
            "performance_metrics": "Reported as among the more consistent prompting strategies; on GPT-4 Self-Probing maintained consistent advantage for calibration/failure prediction in aggregate (exact dataset-by-dataset numbers in paper figures/tables).",
            "comparison_with_baseline": "Improves over vanilla prompting in many settings; no single prompt consistently best across all datasets.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Improvement diminishes with stronger models; still struggles on tasks requiring specialized/professional knowledge.",
            "uuid": "e2687.2",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Multi-Step",
            "name_full": "Multi-Step Verbalized Confidence",
            "brief_description": "Prompting the model to break reasoning into K steps and report a confidence for each step, then aggregate step confidences multiplicatively to produce an overall confidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Multi-Step Verbalized Confidence",
            "system_description": "Prompt instructs the model to produce intermediate steps S_i and a confidence C_i for each step; the final confidence is computed as product over steps C_multi = ∏ C_i, aiming to reduce overconfidence by exposing uncertainty at sub-step level.",
            "system_type": "LLM-based prompting technique (black-box)",
            "scientific_domain": "NLP / reasoning tasks",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility judged via per-step confidences that are aggregated, allowing identification of uncertain sub-decisions.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "ECE and AUROC used to evaluate aggregated per-step confidences.",
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Empirical evaluation showing reduced overconfidence in some datasets; Table 9 reports mixed improvements across tasks.",
            "reproducibility_measures": "Prompts and aggregation formula provided (product of per-step confidences).",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "By revealing uncertain steps, incorrect reasoning chains can be detected more easily than single-step verbal confidence.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Decomposed step-level confidences aggregated multiplicatively to quantify final uncertainty.",
            "benchmark_dataset": "Same paper datasets",
            "performance_metrics": "Mixed results: in some tasks ECE reduced and AUROC improved; in others performance degraded relative to CoT (see Table 9).",
            "comparison_with_baseline": "Sometimes improves calibration/failure prediction relative to vanilla; not uniformly superior across tasks.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Aggregation by product assumes independence and can collapse confidences to very low values; sensitive to how steps are specified and counted.",
            "uuid": "e2687.3",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Top-K Prompt",
            "name_full": "Top-K Verbalized Confidence Prompting",
            "brief_description": "Prompting the model to output its K best candidate answers and a probability/confidence for each candidate, thereby normalizing confidence across multiple plausible outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Top-K Verbalized Confidence",
            "system_description": "Top-K prompting asks the LLM to produce its top K guesses with associated percent confidences; later aggregation (Pair-Rank or Avg-Conf) converts candidate rankings/confidences into a calibrated probability distribution.",
            "system_type": "LLM-based prompting and ranking aggregation",
            "scientific_domain": "NLP / reasoning and QA",
            "hypothesis_generation_method": "Generates multiple candidate answers per query (explicit candidate enumeration).",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Uses model-provided probabilities per candidate and cross-sample ranking information to infer plausibility of each candidate.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "ECE, AUROC used to evaluate quality of aggregated Top-K confidences and ranking-derived distributions.",
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Empirical comparisons in paper; Pair-Rank aggregation (see below) applied to Top-K outputs improves calibration.",
            "reproducibility_measures": "Prompt templates and Pair-Rank formulation provided; Top-K K value varied experimentally.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "By enumerating alternative answers and their confidences, the method exposes uncertainty and reduces single-answer overconfidence.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Direct candidate confidences and ranking events converted into a categorical distribution via Pair-Rank MLE-inspired optimization.",
            "benchmark_dataset": "Paper datasets",
            "performance_metrics": "Top-K combined with Pair-Rank yielded the lowest average ECE (as low as 0.028 in some settings); Top-K + Avg-Conf improved AUROC for failure prediction in multiple datasets (see Table 4 and Table 10).",
            "comparison_with_baseline": "Top-K outperforms vanilla verbalized confidence on average; especially effective when paired with aggregation methods.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Requires multiple outputs per query (more API calls); relies on accurate self-reported candidate confidences which can still be miscalibrated.",
            "uuid": "e2687.4",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Sampling Strategies",
            "name_full": "Sampling Strategies (Self-Random, Prompt Paraphrasing, Misleading prompts)",
            "brief_description": "Methods to elicit multiple model responses to estimate uncertainty from response variance: use randomness (temperature), paraphrase prompts, or inject misleading hints to probe stability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Sampling Strategies for Black-box Uncertainty",
            "system_description": "Three families: (1) Self-Random: re-query with randomness/temperature to sample from model output distribution; (2) Prompting: paraphrase the question to induce varied responses; (3) Misleading: give misleading cues (weak/strong/external) to test whether model sticks to initial answer, using variance as an uncertainty signal.",
            "system_type": "LLM-based ensemble / sampling methods (black-box)",
            "scientific_domain": "NLP / uncertainty estimation",
            "hypothesis_generation_method": "Generates multiple candidate answers via repeated queries or perturbed prompts; not a hypothesis engine per se but used to assess answer uncertainty.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility inferred from consistency (agreement) across sampled responses; high variance indicates lower plausibility/confidence.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Measured via ECE and AUROC of aggregated confidences based on sampled responses.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Empirical tests show sampling+consistency substantially improve failure prediction (e.g., Self-Random M=5 on GPT-3.5 achieved AUROC up to 0.927 on GSM8K; aggregate average AUROC improvements noted).",
            "reproducibility_measures": "Sampling strategies and prompt variants documented; number of samples M varied and effects reported.",
            "hallucination_prevention_method": "Using misleading prompts can reveal lack of robustness (a proxy for hallucination tendency); consistency-based aggregation reduces single-shot hallucination risk.",
            "hallucination_detection_method": "High disagreement across sampled responses flags potential hallucinations or uncertain claims.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Use consistency (fraction of samples matching chosen answer), Avg-Conf (weighted by verbalized confidences), and Pair-Rank over Top-K rankings as uncertainty estimators.",
            "benchmark_dataset": "Paper datasets",
            "performance_metrics": "Sampling with M=5 and Consistency aggregation improved AUROC and ECE over M=1; eg. Self-Random (M=5) on GSM8K (GPT-3.5) ECE≈0.0628 (6.28 in table units) and AUROC≈0.927; overall average AUROC for sampling methods ≈0.73 in reported experiments (see Table 3).",
            "comparison_with_baseline": "Significantly outperforms single-shot verbalized confidence on failure prediction and often on calibration.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Increased computational cost proportional to number of samples; diminishing returns as M grows; effectiveness varies with task type.",
            "uuid": "e2687.5",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Consistency Aggregation",
            "name_full": "Consistency Aggregation",
            "brief_description": "Aggregator computing confidence as the empirical agreement rate among M sampled responses: confidence = (1/M) * sum I{Y_i == Y_final}.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Consistency Aggregation",
            "system_description": "Measures agreement among sampled candidate answers and uses fraction matching the chosen answer as its confidence estimate; simple and effective for failure prediction when multiple samples are available.",
            "system_type": "Aggregation / ensemble-based uncertainty estimator",
            "scientific_domain": "NLP / uncertainty estimation",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "High agreement implies higher plausibility; low agreement implies uncertainty.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Evaluated with ECE and AUROC in the paper.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Empirical results show consistency often yields strong failure prediction (high AUROC) when paired with adequate sampling, e.g., large AUROC boosts for arithmetic tasks.",
            "reproducibility_measures": "Formula and experimental settings provided.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Disagreement across samples used to flag likely hallucinations or uncertain claims.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Empirical agreement (consistency) among re-sampled answers.",
            "benchmark_dataset": "Paper datasets",
            "performance_metrics": "Consistency aggregation with sampling achieved high AUROC on arithmetic tasks (e.g., AUROC up to 0.927 on GSM8K with Self-Random M=5) and improved calibration in many settings (see Table 3 and Table 4).",
            "comparison_with_baseline": "Outperforms single-shot verbalized confidence in failure prediction and often improves ECE when sampling used.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Requires multiple queries; can be insensitive when model outputs identical but incorrect answers consistently.",
            "uuid": "e2687.6",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Avg-Conf",
            "name_full": "Average-Confidence (Avg-Conf) Aggregation",
            "brief_description": "Aggregator that combines sampled responses and their verbalized confidences into a weighted measure: numerator sums confidences for samples equal to final answer; denominator sums confidences of all samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Avg-Conf Aggregation",
            "system_description": "Aggregates multiple sampled responses with their verbalized confidences to compute a confidence score using C_conf = sum_{i} I{Y_i==Y_final} * C_i / sum_i C_i, leveraging both consistency and verbalized confidence.",
            "system_type": "Aggregation / ensemble-based uncertainty estimator",
            "scientific_domain": "NLP / uncertainty estimation",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Combines consistency and self-reported confidences to favor answers that are both frequent and assigned high per-sample confidence.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "ECE and AUROC used for evaluation.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Empirical results: Avg-Conf tends to boost failure prediction (AUROC) relative to consistency-only or verbalized-only methods.",
            "reproducibility_measures": "Aggregation formula and experimental protocol provided in paper.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Low weighted agreement indicates uncertain/hallucinated predictions.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Weighted average of verbalized confidences conditioned on agreement.",
            "benchmark_dataset": "Paper datasets",
            "performance_metrics": "Avg-Conf surpasses other aggregators in AUROC in five of six datasets in Table 4; mean AUROC reported ≈0.669 in those experiments (see Table 4).",
            "comparison_with_baseline": "Better failure-prediction than consistency-only and often better than Pair-Rank for AUROC.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Relies on quality of per-sample verbalized confidences which can be overconfident or coarse-grained.",
            "uuid": "e2687.7",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Pair-Rank",
            "name_full": "Pair-Rank Aggregation (Ranking MLE)",
            "brief_description": "Aggregation method that uses pairwise ranking events among Top-K candidates to infer a categorical distribution P over answers by minimizing a ranking-based log-loss; suited for Top-K outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Pair-Rank Aggregation",
            "system_description": "Constructs pairwise ordering events from multiple Top-K outputs and estimates a categorical distribution P over answers by minimizing -sum I{u ≻ v} * log(P(u)/(P(u)+P(v))) subject to simplex constraint; solved via softmax parametrization and gradient optimization.",
            "system_type": "Aggregation / ranking-based uncertainty estimator",
            "scientific_domain": "NLP / uncertainty estimation and calibration",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Uses observed pairwise preference data to estimate answer probabilities, relying on ranking information which may be more reliable than self-reported confidences.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Evaluated with ECE and AUROC; Pair-Rank excels at reducing ECE (calibration).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Empirical evaluation showed Pair-Rank achieves lowest average ECE in experiments (mean ECE ≈0.069 in one reported config, Table 4).",
            "reproducibility_measures": "Derivation and optimization approach provided; implementation details discussed (softmax parametrization, gradient descent).",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "By learning distribution from rankings, inconsistent/hallucinated answers can receive lower probability mass.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Estimated categorical distribution P derived from pairwise ranking likelihoods; probabilities used as confidence.",
            "benchmark_dataset": "Paper datasets",
            "performance_metrics": "Pair-Rank produced the lowest average ECE in Table 4 (mean ECE ≈0.069) and competitive AUROC (mean ≈0.676 in that experiment).",
            "comparison_with_baseline": "Better calibration (ECE) than Consistency and Avg-Conf on average; Avg-Conf may beat Pair-Rank on AUROC.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Optimization assumes Top-K draws without replacement from categorical distribution; computational overhead for fitting P with many candidates.",
            "uuid": "e2687.8",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Seq-Prob / Len-Norm-Prob / Token-Prob",
            "name_full": "Token-probability-based White-box Methods (Sequence Probability, Length-Normalized Sequence Probability, Key Token Probability)",
            "brief_description": "White-box methods that use token-output probabilities (logits) to estimate output likelihoods and derive confidence scores: aggregate token probabilities, length-normalize them, or focus on key output tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Token-probability-based White-box Methods",
            "system_description": "Three methods implemented by the authors for white-box settings: (1) Seq-Prob: product or sum of token probabilities for the output sequence; (2) Len-Norm-Prob: sequence probability normalized by length (e.g., seq-prob^{1/length}); (3) Token-Prob: use probability of result-specific token(s) as confidence.",
            "system_type": "LLM-based white-box (logit-based) uncertainty estimation",
            "scientific_domain": "NLP / uncertainty estimation",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Higher token/sequence probabilities interpreted as higher plausibility of the produced answer.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "ECE, AUROC, AUPRC-P, AUPRC-N used to evaluate these scores.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Authors compared these white-box methods against black-box verbalized confidence across datasets; white-box generally outperforms black-box but gap is modest.",
            "reproducibility_measures": "Implementation details described; applied where token probabilities are available (white-box/APIs providing logits).",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Low token/sequence probability can indicate uncertain or hallucinated outputs, though token-likelihood focuses on next-token uncertainty and may miss semantic falsity.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Logit/token-probability aggregation with optional length normalization or key-token focus.",
            "benchmark_dataset": "Paper datasets",
            "performance_metrics": "White-box methods typically achieved better AUROC/ECE than black-box verbalized confidence; example: token-prob AUROC up to ~0.606 in some datasets, and seq-prob often improved AUROC into 0.56–0.70 ranges depending on dataset (see Table 5 and Table 6).",
            "comparison_with_baseline": "White-box methods outperform black-box verbalized confidence on average, but the gap in AUROC is modest (paper cites examples like 0.522 to 0.605).",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Token-likelihood reflects next-token uncertainty rather than semantic correctness; can fail on semantically false but locally probable outputs.",
            "uuid": "e2687.9",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "MCDropout",
            "name_full": "Monte Carlo Dropout (MCDropout)",
            "brief_description": "A white-box approximation to Bayesian inference where dropout is applied at inference multiple times to obtain a distribution of outputs; referenced as an analogy for ensemble-based uncertainty estimation.",
            "citation_title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "mention_or_use": "mention",
            "system_name": "Monte Carlo Dropout",
            "system_description": "A technique (cited) that performs multiple forward passes with dropout enabled at inference to obtain predictive distributions and estimate epistemic uncertainty; mentioned as conceptual inspiration for sampling-based black-box methods.",
            "system_type": "Neural Bayesian approximation / white-box method",
            "scientific_domain": "Machine learning / uncertainty quantification",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Uncertainty inferred from variability across stochastic forward passes.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Ensemble of stochastic forward passes producing variance-based uncertainty estimates.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Requires white-box access and architectural support (dropout at inference).",
            "uuid": "e2687.10",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Deep Ensemble",
            "name_full": "Deep Ensembles",
            "brief_description": "An uncertainty estimation approach using an ensemble of independently trained models to capture predictive uncertainty by measuring disagreement across ensemble members.",
            "citation_title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "mention_or_use": "mention",
            "system_name": "Deep Ensemble",
            "system_description": "Referenced classical ensemble approach where multiple models trained independently produce output distributions whose spread is used to estimate uncertainty; cited as conceptual antecedent to sampling+aggregation in black-box LLMs.",
            "system_type": "Neural ensemble-based uncertainty estimation",
            "scientific_domain": "Machine learning / uncertainty quantification",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Disagreement across models indicates lower plausibility/confidence.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Variance/disagreement across ensemble predictions.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Requires training and maintaining multiple models; not directly applicable in closed-box API settings.",
            "uuid": "e2687.11",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Semantic Uncertainty",
            "name_full": "Semantic Uncertainty (Linguistic Invariances for Uncertainty Estimation)",
            "brief_description": "A concept introduced by Kuhn et al. that aims to estimate uncertainty based on semantic invariances rather than token-likelihoods, addressing limitations of logit-based uncertainty for semantic correctness.",
            "citation_title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
            "mention_or_use": "mention",
            "system_name": "Semantic Uncertainty",
            "system_description": "An approach (cited) that quantifies uncertainty using semantic-level invariances (e.g., paraphrase robustness) rather than surface token probabilities, proposed to better capture semantic correctness and reduce hallucinations.",
            "system_type": "LLM-based semantic-level uncertainty estimator / research concept",
            "scientific_domain": "NLP / uncertainty estimation and hallucination detection",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Based on invariance of meaning across linguistic transformations; lack of invariance indicates uncertainty.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Targets hallucination by focusing on semantic consistency rather than token-level likelihood.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Semantic invariance checks across paraphrases/transformations to quantify confidence.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "May be computationally intensive; cited as promising but costly.",
            "uuid": "e2687.12",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "External Calibrator",
            "name_full": "External Calibrator (Mielke et al.)",
            "brief_description": "A method that uses external components (often relying on internal model representations) to post-hoc calibrate conversational agents and reduce overconfidence via linguistic calibration.",
            "citation_title": "Reducing conversational agents’ overconfidence through linguistic calibration",
            "mention_or_use": "mention",
            "system_name": "External Calibrator",
            "system_description": "Referenced method that depends on accessing internal representations to calibrate model outputs linguistically; not applicable in fully black-box API-only settings according to the paper.",
            "system_type": "Calibration / post-hoc adjustment (white-box or partial-access)",
            "scientific_domain": "NLP / conversational agents",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Post-hoc transformation of model outputs informed by internal signals or auxiliary models to match true accuracy distributions.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Aims to reduce overconfident assertions by recalibrating verbal expressions.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "External model or calibrator maps model outputs to calibrated confidence scores.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Relies on internal model access or representations not available in closed-box APIs; less applicable in pure black-box settings.",
            "uuid": "e2687.13",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Lin et al. (Verbalized Confidence Fine-tune)",
            "name_full": "Teaching models to express their uncertainty in words",
            "brief_description": "Prior work that trained models (via fine-tuning) to output verbalized confidence; focused on settings where training data includes confidence labels rather than black-box zero-shot elicitation.",
            "citation_title": "Teaching models to express their uncertainty in words",
            "mention_or_use": "mention",
            "system_name": "Fine-tuned Verbalized Confidence (Lin et al.)",
            "system_description": "Referenced approach that fine-tunes models with datasets containing ground-truth confidence labels to produce calibrated verbalized confidences; differs from the black-box zero-shot focus of the present paper.",
            "system_type": "LLM-based fine-tuning",
            "scientific_domain": "NLP / calibration",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Supervised learning mapping to ground-truth confidences.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Supervised verbalized confidence as learned output.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": "Fine-tuned approaches can outperform zero-shot verbalized confidence but require labeled datasets and expensive fine-tuning.",
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Requires labeled confidence data and substantial compute to fine-tune models; less applicable for closed-box APIs.",
            "uuid": "e2687.14",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Tian et al. (Prompting for Calibration)",
            "name_full": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "brief_description": "A concurrent study focused on prompting strategies for eliciting calibrated confidence scores from models (mainly RLHF-fine-tuned LMs); cited and partially used as a baseline framework (Top-K prompt is motivated by this work).",
            "citation_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "mention_or_use": "mention",
            "system_name": "Prompt-based Calibration Strategies (Tian et al.)",
            "system_description": "Cited work that studies various prompting approaches (including Top-K) to elicit calibrated confidences especially for models fine-tuned with human feedback; influenced the Top-K and prompting components in this paper.",
            "system_type": "LLM prompting / calibration strategies",
            "scientific_domain": "NLP / calibration",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Prompt engineering to improve calibration of verbalized confidences.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Focused primarily on RLHF-fine-tuned LMs; not comprehensive for closed-box APIs.",
            "uuid": "e2687.15",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Hallucination & Uncertainty (Xiao & Wang)",
            "name_full": "On hallucination and predictive uncertainty in conditional language generation",
            "brief_description": "Work linking hallucination in generation to predictive uncertainty; cited to motivate need for reliable uncertainty estimates to reduce hallucinations.",
            "citation_title": "On hallucination and predictive uncertainty in conditional language generation",
            "mention_or_use": "mention",
            "system_name": "Hallucination-Uncertainty Analyses",
            "system_description": "Prior analysis that studies the relationship between hallucination and model uncertainty in conditional generation and suggests that better uncertainty estimates can mitigate hallucinations.",
            "system_type": "Analytical / empirical study",
            "scientific_domain": "NLP / hallucination detection and uncertainty",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Argues for uncertainty-aware generation and calibration to reduce hallucinated claims.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": null,
            "uuid": "e2687.16",
            "source_info": {
                "paper_title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teaching models to express their uncertainty in words",
            "rating": 2
        },
        {
            "paper_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "rating": 1
        },
        {
            "paper_title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "rating": 1
        },
        {
            "paper_title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
            "rating": 2
        },
        {
            "paper_title": "Reducing conversational agents’ overconfidence through linguistic calibration",
            "rating": 1
        },
        {
            "paper_title": "On hallucination and predictive uncertainty in conditional language generation",
            "rating": 1
        },
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 1
        }
    ],
    "cost": 0.027349,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs</h1>
<p>Miao Xiong ${ }^{1}$, Zhiyuan $\mathbf{H u}^{1}$, Xinyang $\mathbf{L u}^{1}$, Yifei $\mathbf{L i}^{3}$, Jie $\mathbf{F u}^{2}$, Junxian $\mathbf{H e}^{2}$, ${ }^{1}$ Bryan Hooi ${ }^{1 \dagger}$<br>${ }^{1}$ National University of Singapore ${ }^{2}$ The Hong Kong University of Science and Technology<br>${ }^{3}$ École Polytechnique Fédérale de Lausanne</p>
<h4>Abstract</h4>
<p>Empowering large language models (LLMs) to accurately express confidence in their answers is essential for reliable and trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve, yet still far from ideal performance. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs. The code is publicly available at https://github.com/MiaoXiong2320/llm-uncertainty.</p>
<h2>1 INTRODUCTION</h2>
<p>A key aspect of human intelligence lies in our capability to meaningfully express and communicate our uncertainty in a variety of ways (Cosmides \&amp; Tooby, 1996). Reliable uncertainty estimates are crucial for human-machine collaboration, enabling more rational and informed decision-making (Guo et al., 2017; Tomani \&amp; Buettner, 2021). Specifically, accurate confidence estimates of a model can provide valuable insights into the reliability of its responses, facilitating risk assessment and error mitigation (Kuleshov et al., 2018; Kuleshov \&amp; Deshpande, 2022), selective generation (Ren et al., 2022), and reducing hallucinations in natural language generation tasks (Xiao \&amp; Wang, 2021).</p>
<p>In the existing literature, eliciting confidence from machine learning models has predominantly relied on white-box access to internal model information, such as token-likelihoods (Malinin \&amp; Gales, 2020; Kadavath et al., 2022) and associated calibration techniques (Jiang et al., 2021), as well as model fine-tuning (Lin et al., 2022). However, with the prevalence of large language models, these</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>methods are becoming less suitable for several reasons: 1) The rise of closed-source LLMs with commercialized APIs, such as GPT-3.5 (OpenAI, 2021) and GPT-4 (OpenAI, 2023), which only allow textual inputs and outputs, lacking access to token-likelihoods or embeddings; 2) Token-likelihood primarily captures the model's uncertainty about the next token (Kuhn et al., 2023), rather than the semantic probability inherent in textual meanings. For example, in the phrase "Chocolate milk comes from brown cows", every word fits naturally based on its surrounding words, but high individual token likelihoods do not capture the falsity of the overall statement, which requires examining the statement semantically, in terms of its claims; 3) Model fine-tuning demands substantial computational resources, which may be prohibitive for researchers with lower computational resources. Given these constraints, there is a growing need to explore black-box approaches for eliciting the confidence of LLMs in their answers, a task we refer to as confidence elicitation.</p>
<p>Recognizing this research gap, our study aims to contribute to the existing knowledge from two perspectives: 1) explore black-box methods for confidence elicitation, and 2) conduct a comparative analysis to shed light on methods and directions for eliciting more accurate confidence. To achieve this, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling strategies for generating multiple responses, and aggregation strategies for computing the consistency. For each component, we devise a suite of methods. By integrating these components, we formulate a set of algorithms tailored for confidence elicitation. A comprehensive overview of the framework is depicted in Figure 1. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of tasks (Commonsense, Arithmetic, Symbolic, Ethics and Professional Knowledge) and five widely-used LLMs, i.e., GPT-3 (Brown et al., 2020), GPT-3.5 (OpenAI, 2021), GPT-4, Vicuna (Chiang et al., 2023) and LLaMA 2 (Touvron et al., 2023b).</p>
<p>Our investigation yields several observations: 1) LLMs tend to be highly overconfident when verbalizing their confidence, posing potential risks for the safe deployment of LLMs (§5.1). Intriguingly, the verbalized confidence values predominantly fall within the $80 \%$ to $100 \%$ range and are typically in multiples of 5 , similar to how humans talk about confidence. In addition, while scaling model capacity leads to performance improvement, the results remain suboptimal. 2) Prompting strategies, inspired by patterns observed in human dialogues, can mitigate this overconfidence, but the improvement also diminishes as the model capacity scales up (§5.2). Furthermore, while the calibration error (e.g. ECE) can be significantly reduced using suitable prompting strategies, failure prediction still remains a challenge. 3) Our study on sampling and aggregation strategies indicates their effectiveness in improving failure prediction performance (§5.3). 4) A detailed examination of aggregation strategies reveals that they cater to specific performance metrics, i.e., calibration and failure prediction, and can be selected based on desired outcomes (§5.4). 5) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC (§B.1). Despite these insights, it is worth noting that the methods introduced herein still face challenges in failure prediction, especially with tasks demanding specialized knowledge (§6). This emphasizes the ongoing need for further research and development in confidence elicitation for LLMs.</p>
<h1>2 Related Works</h1>
<p>Confidence Elicitation in LLMs. Confidence elicitation is the process of estimating LLM's confidence in their responses without model fine-tuning or accessing internal information. Within this scope, Lin et al. (2022) introduced the concept of verbalized confidence that prompts LLMs to express confidence directly. However, they mainly focus on fine-tuning on specific datasets where the confidence is provided, and its zero-shot verbalized confidence is unexplored. Other approaches, like the external calibrator from Mielke et al. (2022), depend on internal model representations, which are often inaccessible. While Zhou et al. (2023) examines the impact of confidence, it does not provide direct confidence scores to users. Our work aligns most closely with the concurrent study by Tian et al. (2023), which mainly focuses on the use of prompting strategies. Our approach diverges by aiming to explore a broader method space, and propose a comprehensive framework for systematically evaluating various strategies and their integration. We also consider a wider range of models beyond those RLHF-LMs examined in concurrent research, thus broadening the scope of confidence elicitation. Our results reveal persistent challenges across more complex tasks and contribute to a holistic understanding of confidence elicitation. For a more comprehensive discussion of the related works, kindly refer to Appendix C.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An Overview and example of Confidence Elicitation framework, which consists of three components: prompt, sampling and aggregator. By integrating distinct strategies from each component, we can devise different algorithms, e.g., Top-K (Tian et al., 2023) is formulated using Top-K prompt, self-random sampling with $M=1$, and Avg-Conf aggregation. Given an input question, we first choose a suitable prompt strategy, e.g., the vanilla prompt used here. Next, we determine the number of samples to generate ( $M=3$ here) and sampling strategy, and then choose an aggregator based on our preference (e.g. focus more on improving calibration or failure prediction) to compute confidences in its potential answers. The highest confident answer is selected as the final output.</p>
<h1>3 Exploring Black-box Framework for Confidence Elicitation</h1>
<p>In our pursuit to explore black-box approaches for eliciting confidence, we investigated a range of methods and discovered that they can be encapsulated within a unified framework. This framework, with its three pivotal components, offers a variety of algorithmic choices that combine to create diverse algorithms with different benefits for confidence elicitation. In our later experimental section (§5), we will analyze our proposed strategies within each component, aiming to shed light on the best practices for eliciting confidence in black-box LLMs.</p>
<h3>3.1 Motivation of The Framework</h3>
<p>Prompting strategy. The key question we aim to answer here is: in a black-box setting, what form of model inputs and outputs lead to the most accurate confidence estimates? This parallels the rich study in eliciting confidences from human experts: for example, patients often inquire of doctors about their confidence in the potential success of a surgery. We refer to this goal as verbalized confidence, and inspired by strategies for human elicitation, we design a series of human-inspired prompting strategies to elicit the model's verbalized confidence. We then unify these prompting strategies as a building block of our framework (§3.2). In addition, beyond its simplicity, this approach also offers an extra benefit over model's token-likelihood: the verbalized confidence is intrinsically tied to the semantic meaning of the answer instead of its syntactic or lexical form (Kuhn et al., 2023).
Sampling and Aggregation. In addition to the direct insights from model outputs, the variance observed among multiple responses for a given question offers another valuable perspective on model confidence. This line of thought aligns with the principle extensively explored in prior white-box access uncertainty estimation methodologies for classification (Gawlikowski et al., 2021), such as MCDropout (Gal \&amp; Ghahramani, 2016) and Deep Ensemble (Lakshminarayanan et al., 2017). The challenges in adapting ensemble-based methods lie in two critical components: 1) the sampling strategy, i.e., how to sample multiple responses from the model's answer distribution, and 2) the aggregation strategy, i.e., how to aggregate these responses to yield the final answer and its associated confidence. To optimally harness both textual output and response variance, we have integrated them within a unified framework.</p>
<h3>3.2 Prompting Strategy</h3>
<p>Drawing inspiration from patterns observed in human dialogues, we design a series of human-inspired prompting strategies to tackle challenges, e.g., overconfidence, that are inherent in the vanilla version of verbalized confidence. See Table 1 for an overview of these prompting strategies and Appendix F for complete prompts.</p>
<p>Table 1: Illustration of the prompting strategy (the complete prompt in Appendix F). To help models understand the concept of confidence, we also append the explanation "Note: The confidence indicates how likely you think your answer is true." to every prompt.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Vanilla</td>
<td style="text-align: left;">Read the question, provide your answer, and your confidence in this answer.</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: left;">Read the question, analyze step by step, provide your answer and your confidence in this <br> answer.</td>
</tr>
<tr>
<td style="text-align: left;">Self-Probing</td>
<td style="text-align: left;">Question: [...] Possible Answer: [...] Q: How likely is the above answer to be correct? Analyze <br> the possible answer, provide your reasoning concisely, and give your confidence in this answer.</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Step</td>
<td style="text-align: left;">Read the question, break down the problem into K steps, think step by step, give your <br> confidence in each step, and then derive your final answer and your confidence in this answer.</td>
</tr>
<tr>
<td style="text-align: left;">Top-K</td>
<td style="text-align: left;">Provide your $K$ best guesses and the probability that each is correct $(0 \%$ to $100 \%)$ for the <br> following question.</td>
</tr>
</tbody>
</table>
<p>CoT. Considering that a better comprehension of a problem can lead to a more accurate understanding of one's certainty, we adopt a reasoning-augmented prompting strategy. In this paper, we use zeroshot Chain-of-Thought, CoT (Kojima et al., 2022) for its proven efficacy in inducing reasoning processes and improving model accuracy across diverse datasets. Alternative strategies such as plan-and-solve (Wang et al., 2023) can also be used.</p>
<p>Self-Probing. A common observation of humans is that they often find it easier to identify errors in others' answers than in their own, as they can become fixated on a particular line of thinking, potentially overlooking mistakes. Building on this assumption, we investigate if a model's uncertainty estimation improves when given a question and its answer, then asked, "How likely is the above answer to be correct"? The procedure involves generating the answer in one chat session and obtaining its verbalized confidence in another independent chat session.</p>
<p>Multi-Step. Our preliminary study shows that LLMs tend to be overconfident when verbalizing their confidence (see Figure 2). To address this, we explore whether dissecting the reasoning process into steps and extracting the confidence of each step can alleviate the overconfidence. The rationale is that understanding each reasoning step's confidence could help the model identify potential inaccuracies and quantify their confidence more accurately. Specifically, for a given question, we prompt models to delineate their reasoning process into individual steps $S_{i}$ and evaluate their confidence in the correctness of this particular step, denoted as $C_{i}$. The overall verbalized confidence is then derived by aggregating the confidence of all steps: $C_{\text {multi-step }}=\prod_{i=1}^{n} C_{i}$, where $n$ represents the total number of reasoning steps.</p>
<p>Top-K. Another way to alleviate overconfidence is to realize the existence of multiple possible solutions or answers, which acts as a normalization for the confidence distribution. Motivated by this, Top-K (Tian et al., 2023) prompts LLMs to generate the top $K$ guesses and their corresponding confidence for a given question.</p>
<h1>3.3 SAMPLING STRATEGY</h1>
<p>Several methods can be employed to elicit multiple responses of the same question from the model: 1) Self-random, leveraging the model's inherent randomness by inputting the same prompt multiple times. The temperature, an adjustable parameter, can be used to calibrate the predicted token distribution, i.e., adjust the diversity of the sampled answers. An alternative choice is to introduce perturbations in the questions: 2) Prompting, by paraphrasing the questions in different ways to generate multiple responses. 3) Misleading, feeding misleading cues to the model, e.g.,"I think the answer might be ...". This method draws inspiration from human behaviors: when confident, individuals tend to stick to their initial answers despite contrary suggestions; conversely, when uncertain, they are more likely to waver or adjust their responses based on misleading hints. Building on this observation, we evaluate the model's response to misleading information to gauge its uncertainty. See Table 11 for the complete prompts.</p>
<h3>3.4 Aggregation Strategy</h3>
<p>Consistency. A natural idea of aggregating different answers is to measure the degree of agreement among the candidate outputs and integrate the inherent uncertainty in the model's output.</p>
<p>For any given question and an associated answer $\hat{Y}$, we sample a set of candidate answers $\hat{Y}_{i}$, where $i \in{1, \ldots, M}$. The agreement between these candidate responses and the original answer then serves as a measure of confidence, computed as follows:</p>
<p>$$
C_{\text {consistency }}=\frac{1}{M} \sum_{i=1}^{M} \mathbb{I}\left{\hat{Y}_{i}=\hat{Y}\right}
$$</p>
<p>Avg-Conf. The previous aggregation method does not utilize the available information of verbalized confidence. It is worth exploring the potential synergy between these uncertainty indicators, i.e., whether the verbalized confidence and the consistency between answers can complement one another. For any question and an associated answer $\hat{Y}$, we sample a candidate set $\left{\hat{Y}<em M="M">{1}, \ldots \hat{Y}</em>\right}$, and compute the confidence as follows:}\right}$ with their corresponding verbalized confidence $\left{C_{1}, \ldots C_{M</p>
<p>$$
C_{\text {conf }}=\frac{\sum_{i=1}^{M} \mathbb{I}\left{\hat{Y}<em i="i">{i}=\hat{Y}\right} \times C</em>
$$}}{\sum_{i=1}^{M} C_{i}</p>
<p>Pair-Rank. This aggregation strategy is tailored for responses generated using the Top-K prompt, as it mainly utilizes the ranking information of the model's Top-K guesses. The underlying assumption is that the model's ranking between two options may be more accurate than the verbalized confidence it provides, especially given our observation that the latter tends to exhibit overconfidence.
Given a question with $N$ candidate responses, the $i$-th response consists of $K$ sequentially ordered answers, denoted as $S_{K}^{(i)}=\left(S_{1}^{(i)}, S_{2}^{(i)}, \ldots, S_{K}^{(i)}\right)$. Let $\mathcal{A}$ represent the set of unique answers across all $N$ responses, where $M$ is the total number of distinct answers. The event where the model ranks answer $S_{u}$ above $S_{v}$ (i.e., $S_{u}$ appears before $S_{v}$ ) in its $i$-th generation is represented as $\left(S_{u} \succ S_{v}\right)$. In contexts where the generation is implicit, this is simply denoted as $\left(S_{u} \succ S_{v}\right)$. Let $E_{u v}^{(i)}$ be the event where at least one of $S_{u}$ and $S_{v}$ appears in the $i$-th generation. Then the probability of $\left(S_{u} \succ S_{v}\right)$, conditional on $E_{u v}^{(i)}$ and a categorical distribution $P$, is expressed as $\mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)}\right)$.
We then utilize a (conditional) maximum likelihood estimation (MLE) inspired approach to derive the categorical distribution $P$ that most accurately reflects these ranking events of all the $M$ responses:</p>
<p>$$
\min <em i="1">{P}-\sum</em>\right)=1
$$}^{N} \sum_{S_{u} \in \mathcal{A}} \sum_{S_{v} \in \mathcal{A}} \mathbb{I}\left{S_{u} \stackrel{(i)}{\succ} S_{v}\right} \cdot \log \mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)}\right) \quad \text { subject to } \sum_{S_{u} \in \mathcal{A}} P\left(S_{u</p>
<p>Proposition 3.1. Suppose the Top-K answers are drawn from a categorical distribution $P$ without replacement. Define the event $\left(S_{u} \succ S_{v}\right)$ to indicate that the realization $S_{u}$ is observed before $S_{v}$ in the $i$-th draw without replacement. Under this setting, the conditional probability is given by:</p>
<p>$$
\mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)}\right)=\frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)}
$$</p>
<p>The optimization objective to minimize the expected loss is then:</p>
<p>$$
\min <em i="1">{P}-\sum</em>\right)=1
$$}^{N} \sum_{S_{u} \in \mathcal{A}} \sum_{S_{v} \in \mathcal{A}} \mathbb{I}\left{S_{u} \stackrel{(i)}{\succ} S_{v}\right} \cdot \log \frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)} \quad \text { s.t. } \sum_{S_{u} \in \mathcal{A}} P\left(S_{u</p>
<p>To address this constrained optimization problem, we first introduce a change of variables by applying the softmax function to the unbounded domain. This transformation inherently satisfies the simplex constraints, converting our problem into an unconstrained optimization setting. Subsequently, optimization techniques such as gradient descent can be used to obtain the categorical distribution.</p>
<h1>4 EXPERIMENT SETUP</h1>
<p>Datasets. We evaluate the quality of confidence estimates across five types of reasoning tasks: 1) Commonsense Reasoning on two benchmarks, Sports Understanding (SportUND) (Kim, 2021) and StrategyQA (Geva et al., 2021) from BigBench (Ghazal et al., 2013); 2) Arithmetic Reasoning on two math problems, GSM8K (Cobbe et al., 2021) and SVAMP (Patel et al., 2021); 3) Symbolic Reasoning</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Empirical distribution (First row) and reliability diagram (Second row) of vanilla verbalized confidence across four models on GSM8K. The prompt used is in Table 14. From this figure, we can observe that 1) the confidence levels primarily range between $80 \%$ and $100 \%$, often in multiples of $5 ; 2$ ) the accuracy within each bin is much lower than its corresponding confidence, indicating significant overconfidence.
on two benchmarks, Date Understanding (DateUnd) (Wu \&amp; Wang, 2021) and Object Counting (ObjectCou) (Wang et al., 2019) from BigBench; 4) tasks requiring Professional Knowledge, such as Professional Law (Prf-Law) from MMLU (Hendrycks et al., 2021); 5) tasks that require Ethical Knowledge, e.g., Business Ethics (Biz-Ethics) from MMLU (Hendrycks et al., 2021).</p>
<p>Models We incorporate a range of widely used LLMs of different scales, including Vicuna 13B (Chiang et al., 2023), GPT-3 175B (Brown et al., 2020), GPT-3.5-turbo (OpenAI, 2021), GPT-4 (OpenAI, 2023) and LLaMA 2 70B (Touvron et al., 2023b).</p>
<p>Evaluation Metrics. To evaluate the quality of confidence outputs, two orthogonal tasks are typically employed: calibration and failure prediction (Naeini et al., 2015; Yuan et al., 2021; Xiong et al., 2022). Calibration evaluates how well a model's expressed confidence aligns with its actual accuracy: ideally, samples with an $80 \%$ confidence should have an accuracy of $80 \%$. Such well-calibrated scores are crucial for applications including risk assessment. On the other hand, failure prediction gauges the model's capacity to assign higher confidence to correct predictions and lower to incorrect ones, aiming to determine if confidence scores can effectively distinguish between correct and incorrect predictions. In our study, we employ Expected Calibration Error (ECE) for calibration evaluation and Area Under the Receiver Operating Characteristic Curve (AUROC) for gauging failure prediction. Given the potential imbalance from varying accuracy levels, we also introduce AUPRC-Positive (PR-P) and AUPRC-Negative (PR-N) metrics to emphasize whether the model can identify incorrect and correct samples, respectively.</p>
<p>Further details on datasets, models, metrics, and implementation can be found in Appendix E.</p>
<h1>5 Evaluation and Analysis</h1>
<p>To provide insights on the best practice for eliciting confidence, we systematically examine each component (see Figure 1) of the confidence elicitation framework (§3). We test the performance on eight datasets of five different reasoning types and five commonly used models (see §4), and yield the following key findings.</p>
<h3>5.1 LLMs TEND TO BE OVERCONFIDENT WHEN VERBALIZING THEIR CONFIDENCE</h3>
<p>The distribution of verbalized confidences mimics how humans talk about confidence. To examine model's capacity to express verbalized confidence, we first visualize the distribution of confidence in Figure 2. Detailed results on other datasets and models are provided in Appendix Figure 5. Notably, the models tend to have high confidence for all samples, appearing as multiples of 5 and with most values ranging between the $80 \%$ to $100 \%$ range, which is similar to the patterns identified in the training corpus for GPT-like models as discussed by Zhou et al. (2023). Such behavior suggests that models might be imitating human expressions when verbalizing confidence.</p>
<p>Table 2: Vanilla Verbalized Confidence of 4 models and 8 datasets (metrics are given by $\times 10^{2}$ ). Abbreviations are used: Date (Date Understanding), Count (Object Counting), Sport (Sport Understanding), Law (Professional Law), Ethics (Business Ethics). ECE $&gt;0.25$, AUROC, AUPRC-Positive, AUPRC-Negative $&lt;0.6$ denote significant deviation from ideal performance. Significant deviations in averages are highlighted in red. The prompt used is in Table 14.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Count</th>
<th style="text-align: center;">Strategy</th>
<th style="text-align: center;">Sport</th>
<th style="text-align: center;">Law</th>
<th style="text-align: center;">Ethics</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ECE $\downarrow$</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vicuna</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">46.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">43.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">37.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">18.0</td>
</tr>
<tr>
<td style="text-align: center;">ROC $\uparrow$</td>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vicuna</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">52.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">56.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">55.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">62.7</td>
</tr>
<tr>
<td style="text-align: center;">PR-N $\uparrow$</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">54.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vicuna</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">67.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">62.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">48.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">34.2</td>
</tr>
<tr>
<td style="text-align: center;">PR-P $\uparrow$</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">46.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vicuna</td>
<td style="text-align: center;">4.10</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">36.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">47.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">60.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">78.4</td>
</tr>
</tbody>
</table>
<p>Calibration and failure prediction performance improve as model capacity scales. The comparison of the performance of various models (Table 2) reveals a trend: as we move from GPT-3, Vicuna, GPT-3.5 to GPT-4, with the increase of model accuracy, there is also a noticeable decrease in ECE and increase in AUROC, e.g., approximate $22.2 \%$ improvement in AUROC from GPT-3 to GPT-4.</p>
<p>Vanilla verbalized confidence exhibits significant overconfidence and poor failure prediction, casting doubts on its reliability. Table 2 presents the performance of vanilla verbalized confidence across five models and eight tasks. According to the criteria given in Srivastava et al. (2023), GPT-3, GPT-3.5, and Vicuna exhibit notably high ECE values, e.g., the average ECE exceeding 0.377 , suggesting that the verbalized confidence of these LLMs are poorly calibrated. While GPT-4 displays lower ECE, its AUROC and AUPRC-Negative scores remain suboptimal, with an average AUROC of merely $62.7 \%$-close to the $50 \%$ random guess threshold-highlighting challenges in distinguishing correct from incorrect predictions.</p>
<h1>5.2 Human-inspired Prompting Strategies Partially Reduce Overconfidence</h1>
<p>Human-inspired prompting strategies improve model accuracy and calibration, albeit with diminishing returns in advanced models like GPT-4. As illustrated in Figure 3, we compare the performance of five prompting strategies across five datasets on GPT-3.5 and GPT-4. Analyzing the average ECE, AUROC, and their respective performances within each dataset, human-inspired strategies offer consistent improvements in accuracy and calibration over the vanilla baseline, with modest advancements in failure prediction.</p>
<p>No single prompting strategy consistently outperforms the others. Figure 3 suggests that there is no single strategy that can consistently outperform the others across all the datasets and models. By evaluating the average rank and performance enhancement for each method over five task types, we find that Self-Probing maintains the most consistent advantage over the baseline on GPT-4, while Top- $K$ emerges as the top performer on GPT-3.5.</p>
<p>While ECE can be effectively reduced using suitable prompting strategies, failure prediction still remains a challenge. Comparing the average calibration performance across datasets ('mean</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparative analysis of 5 prompting strategies over 5 datasets for 2 models (GPT-3.5 and GPT-4). The 'average' bar represents the mean ECE for a given prompting strategy across datasets. The 'mean ECE' line is the average across all strategies and datasets. AUROC is calculated in a similar manner. The accuracy comparison is shown in Appendix B.4.
ece' lines) and the average failure prediction performance ('mean auroc'), we find that while we can reduce ECE with the right prompting strategy, the model's failure prediction capability is still limited, i.e., close to the performance of random guess (AUROC=0.5). A closer look at individual dataset performances reveals that the proposed prompt strategies such as CoT have significantly increased the accuracy (see Table 8), while the confidence output distribution still remains at the range of $80 \%-100 \%$, suggesting that a reduction in overconfidence is due to the diminished gap between average confidence and accuracy, not necessarily indicating a substantial increase in the model's ability to judge the correctness of its responses. For example, with the CoT prompting on the GSM8K dataset, GPT-4 with $93.6 \%$ accuracy achieves a near-optimal ECE 0.064 by assigning $100 \%$ confidence to all samples. However, since all samples receive the same confidence, it is challenging to distinguish between correct and incorrect samples based on the verbalized confidence.</p>
<h1>5.3 Variance Among Multiple Responses Improves Failure Prediction</h1>
<p>Table 3: Comparison of sampling strategies with the number of responses $M=5$ on GPT-3.5. The prompt and aggregation strategies are fixed as CoT and Consistency when $M&gt;1$. To compare the effect of $M$, we also provide the baseline with $M=1$ from Figure 3. Metrics are given by $\times 10^{2}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prf-Law</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DateUnd</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">StrategyQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Biz-Ethics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ECE</td>
<td style="text-align: center;">AUROC</td>
<td style="text-align: center;">ECE</td>
<td style="text-align: center;">AUROC</td>
<td style="text-align: center;">ECE</td>
<td style="text-align: center;">AUROC</td>
<td style="text-align: center;">ECE</td>
<td style="text-align: center;">AUROC</td>
<td style="text-align: center;">ECE</td>
<td style="text-align: center;">AUROC</td>
<td style="text-align: center;">ECE</td>
<td style="text-align: center;">AUROC</td>
</tr>
<tr>
<td style="text-align: center;">Misleading (M=5)</td>
<td style="text-align: center;">8.03</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">69.6</td>
</tr>
<tr>
<td style="text-align: center;">Self-Random (M=5)</td>
<td style="text-align: center;">6.28</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">73.0</td>
</tr>
<tr>
<td style="text-align: center;">Prompt (M=5)</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">69.2</td>
</tr>
<tr>
<td style="text-align: center;">CoT (M=1)</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">56.4</td>
</tr>
<tr>
<td style="text-align: center;">Top-K (M=1)</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">65.2</td>
</tr>
</tbody>
</table>
<p>Consistency among multiple responses is more effective in improving failure prediction and calibration compared to verbalized confidence $(M=1)$, with particularly notable improvements on the arithmetic task. Table 3 demonstrates that the sampling strategy with 5 sampled responses paired with consistency aggregation consistently outperform verbalized confidence in calibration and failure prediction, particularly on arithmetic tasks, e.g., GSM8K showcases a remarkable improvement in AUROC from $54.8 \%$ (akin to random guessing) to $92.7 \%$, effectively distinguishing between incorrect and correct answers. The average performance in the last two columns also indicates improved ECE and AUROC scores, suggesting that obtaining the variance among multiple responses can be a good indicator of uncertainty.</p>
<p>As the number of sampled responses increases, model performance improves significantly and then converges. Figure 7 exhibits the performance of various number of sampled responses $M$ from $M=1$ to $M=13$. The result suggests that the ECE and AUROC could be improved by sampling more responses, but the improvement becomes marginal as the number gets larger. Additionally, as the computational time and resources required for $M$ responses go linearly with the baseline $(M=1)$, $M$ thus presents a trade-off between efficiency and effectiveness. Detailed experiments investigating the impact of the number of responses can be found in Appendix B. 6 and B.7.</p>
<h3>5.4 Introducing Verbalized Confidence Into The Aggregation Outperforms Consistency-only Aggregation</h3>
<p>Pair-Rank achieves better performance in calibration while Avg-Conf boosts more in failure prediction. On the average scale, we find that Pair-Rank emerges as the superior choice for calibration that can reduce ECE to as low as 0.028 , while Avg-Conf stands out for its efficacy in failure prediction.</p>
<p>Table 4: Performance comparison of aggregation strategies on GPT-4 using Top-K Prompt and Self-Random sampling. Pair-Rank aggregation achieves the lowest ECE in half of the datasets and maintains the lowest average ECE in calibration; Avg-Conf surpasses other methods in terms of AUROC in five out of the six datasets in failure prediction. Metrics are given by $\times 10^{2}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Aggregator</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">Law</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Sport</th>
<th style="text-align: center;">Strategy</th>
<th style="text-align: center;">Ethics</th>
<th style="text-align: center;">Mean \&amp; Var</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ECE $\downarrow$</td>
<td style="text-align: center;">Consistency</td>
<td style="text-align: center;">4.80</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">$12.0 \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg-Conf</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">7.70</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">5.90</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">$14.8 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pair-Rank</td>
<td style="text-align: center;">7.40</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">8.50</td>
<td style="text-align: center;">2.80</td>
<td style="text-align: center;">3.50</td>
<td style="text-align: center;">3.80</td>
<td style="text-align: center;">$6.90 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">AUROC $\uparrow$</td>
<td style="text-align: center;">Consistency</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">$66.9 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg-Conf</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">$66.9 \pm 1.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pair-Rank</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">$67.6 \pm 0.4$</td>
</tr>
</tbody>
</table>
<p>This observation agrees with the underlying principle that Pair-Rank learns the categorical distribution of potential answers through our $K$ observations, which aligns well with the notion of calibration and is therefore more likely to lead to a lower ECE. In contrast, Avg-Conf leverages the consistency, using verbalized confidence as a weighting factor for each answer. This approach is grounded in the observation that accurate samples often produce consistent outcomes, while incorrect ones yield various responses, leading to a low consistency. This assumption matches well with failure prediction, and is confirmed by the results in Table 4. In addition, our comparative analysis of various aggregation strategies reveals that introducing verbalized confidence into the aggregation (e.g., Pair-Rank and Avg-Conf) is more effective compared to consistency-only aggregation (e.g., Consistency), especially when LLM queries are costly, and we are limited in sampling frequency (set to $M=5$ queries in our experiment). Verbalized confidence, albeit imprecise, reflects the model's uncertainty tendency and can enhance results when combined with ensemble methods.</p>
<h1>6 DISCUSSIONS</h1>
<p>In this study, we focus on confidence elicitation, i.e., empowering Large Language Models (LLMs) to accurately express the confidence in their responses. Recognizing the scarcity of existing literature on this topic, we define a systematic framework with three components: prompting, sampling and aggregation to explore confidence elicitation algorithms and then benchmark these algorithms on two tasks across eight datasets and five models. Our findings reveal that LLMs tend to exhibit overconfidence when verbalizing their confidence. This overconfidence can be mitigated to some extent by using proposed prompting strategies such as CoT and Self-Probing. Furthermore, sampling strategies paired with specific aggregators can improve failure prediction, especially in arithmetic datasets. We hope this work could serve as a foundation for future research in these directions.</p>
<p>Comparative analysis of white-box and black-box methods. While our method is centered on black-box settings, comparing it with white-box methods helps us understand the progress in the field. We conducted comparisons on five datasets with three white-box methods (see §B.1) and observed that although white-box methods indeed perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. This finding underscores that the field remains challenging and unresolved.</p>
<p>Are current algorithms satisfactory? Not quite. Our findings (Table 4) reveals that while the best-performing algorithms can reduce ECE to a quite low value like 0.028 , they still face challenges in predicting incorrect predictions, especially in those tasks requiring professional knowledge, such as professional law. This underscores the need for ongoing research in confidence elicitation.</p>
<p>What is the recommendation for practitioners? Balancing between efficiency, simplicity, and effectiveness, and based on our empirical results, we recommend a stable-performing method for practitioners: Top-K prompt + Self-Random sampling + Avg-Conf or Pair-Rank aggregation. Please refer to Appendix D for the reasoning and detailed discussions, including the considerations when using black-box confidence elicitation algorithms and why these methods fail in certain cases.</p>
<p>Limitations and Future Work: 1) Scope of Datasets. We mainly focuses on fixed-form and freeform question-answering QA tasks where the ground truth answer is unique, while leaving tasks such as summarization and open-ended QA to the future work. 2) Black-box Setting. Our findings indicate black-box approaches remain suboptimal, while the white-box setting, with its richer information access, may be a more promising avenue. Integrating black-box methods with limited white-box access data, such as model logits provided by GPT-3, could be a promising direction.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>This research is supported by the Ministry of Education, Singapore, under the Academic Research Fund Tier 1 (FY2023).</p>
<h2>REFERENCES</h2>
<p>Kendrick Boyd, Kevin H. Eng, and C. David Page. Area under the precision-recall curve: Point estimates and confidence intervals. In Hendrik Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip Železný (eds.), Machine Learning and Knowledge Discovery in Databases, pp. 451-466, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg. ISBN 978-3-642-40994-3.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.</p>
<p>Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. A close look into the calibration of pre-trained language models. arXiv preprint arXiv:2211.00151, 2022.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Leda Cosmides and John Tooby. Are humans good intuitive statisticians after all? rethinking some conclusions from the literature on judgment under uncertainty. cognition, 58(1):1-73, 1996.</p>
<p>Ailin Deng, Miao Xiong, and Bryan Hooi. Great models think alike: Improving model reliability via inter-model latent agreement. arXiv preprint arXiv:2305.01481, 2023.</p>
<p>Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059. PMLR, 2016.</p>
<p>Paul H Garthwaite, Joseph B Kadane, and Anthony O’Hagan. Statistical methods for eliciting probability distributions. Journal of the American statistical Association, 100(470):680-701, 2005a.</p>
<p>Paul H Garthwaite, Joseph B Kadane, and Anthony O’Hagan. Statistical methods for eliciting probability distributions. Journal of the American statistical Association, 100(470):680-701, 2005b.</p>
<p>Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies, 2021.</p>
<p>Ahmad Ghazal, Tilmann Rabl, Minqing Hu, Francois Raab, Meikel Poess, Alain Crolotte, and Hans-Arno Jacobsen. Bigbench: Towards an industry standard benchmark for big data analytics. In Proceedings of the 2013 ACM SIGMOD international conference on Management of data, pp. $1197-1208,2013$.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pp. 1321-1330. PMLR, 2017.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962-977, 2021.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.</p>
<p>Ethan Kim. Sports understanding in bigbench, 2021.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022. URL https://api . semanticscholar.org/CorpusID:249017743.</p>
<p>Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023.</p>
<p>Volodymyr Kuleshov and Shachi Deshpande. Calibrated and sharp uncertainties in deep learning via density estimation. In International Conference on Machine Learning, pp. 11683-11693. PMLR, 2022.</p>
<p>Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning using calibrated regression. In International conference on machine learning, pp. 2796-2804. PMLR, 2018.</p>
<p>Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022.</p>
<p>Andrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction. arXiv preprint arXiv:2002.07650, 2020.</p>
<p>Sabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. Reducing conversational agents’ overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857-872, 2022.</p>
<p>Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. In Advances in Neural Information Processing Systems, volume 34, pp. 15682-15694, 2021.</p>
<p>Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence, volume 29, 2015.</p>
<p>OpenAI. ChatGPT. https://www.openai.com/gpt-3/, 2021. Accessed: April 21, 2023.
OpenAI. Gpt-4 technical report, 2023.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080-2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 168. URL https://aclanthology.org/2021.naacl-main. 168.</p>
<p>Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter J Liu. Out-of-distribution detection and selective generation for conditional language models. arXiv preprint arXiv:2209.15558, 2022.</p>
<p>Quintin P. Solano, Laura Hayward, Zoey Chopra, Kathryn Quanstrom, Daniel Kendrick, Kenneth L. Abbott, Marcus Kunzmann, Samantha Ahle, Mary Schuller, Erkin Ötleş, and Brian C. George. Natural language processing and assessment of resident feedback quality. Journal of Surgical Education, 78(6):e72-e77, 2021. ISSN 1931-7204. doi: https://doi.org/10.1016/ j.jsurg.2021.05.012. URL https://www.sciencedirect.com/science/article/ pii/S1931720421001537.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/ forum?id=uyTLSBvos j.</p>
<p>Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023.</p>
<p>Christian Tomani and Florian Buettner. Towards trustworthy predictions from deep neural networks with fast adversarial calibration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 9886-9896, 2021.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Jianfeng Wang, Rong Xiao, Yandong Guo, and Lei Zhang. Learning to count objects with few exemplar annotations. arXiv preprint arXiv:1905.07898, 2019.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Annual Meeting of the Association for Computational Linguistics, 2023. URL https://api.semanticscholar.org/CorpusID:258558102.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Xinyi Wu and Zijian Wang. Data understanding in bigbench, 2021.
Yijun Xiao and William Yang Wang. On hallucination and predictive uncertainty in conditional language generation. arXiv preprint arXiv:2103.15025, 2021.</p>
<p>Miao Xiong, Shen Li, Wenjie Feng, Ailin Deng, Jihai Zhang, and Bryan Hooi. Birds of a feather trust together: Knowing when to trust a classifier via adaptive neighborhood aggregation. arXiv preprint arXiv:2211.16466, 2022.</p>
<p>Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, and Bryan Hooi. Proximity-informed calibration for deep neural networks. arXiv preprint arXiv:2306.04590, 2023.</p>
<p>Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang. Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3040-3049, 2021.</p>
<p>Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In Icml, volume 1, pp. 609-616, 2001.</p>
<p>Jize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning. In International conference on machine learning, pp. 11117-11128. PMLR, 2020.</p>
<p>Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey area: Expressions of overconfidence and uncertainty in language models. arXiv preprint arXiv:2302.13439, 2023.</p>
<h1>A Proof of Proposition 3.1</h1>
<p>Notation. Given a question with $N$ candidate responses, the $i$-th response consists of $K$ sequentially ordered answers, denoted as $\mathcal{S}<em 1="1">{K}^{(i)}=\left(S</em>\right)$.
Proposition A.1. Suppose the Top-K answers are drawn from a categorical distribution $P$ without replacement. Define the event $\left(S_{u} \succ S_{v}\right)$ to indicate that the realization $S_{u}$ is observed before $S_{v}$ in the $i$-th draw without replacement. Under this setting, the conditional probability is given by:}^{(i)}, S_{2}^{(i)}, \ldots, S_{K}^{(i)}\right)$. Let $\mathcal{A}=\left{S_{1}, S_{2}, \ldots, S_{M}\right}$ represent the set of unique answers across all $N$ responses, where $M$ is the total number of distinct answers. The event where the model ranks answer $S_{u}$ above $S_{v}$ in its $i$-th generation is represented as $\left(S_{u} \stackrel{(i)}{\succ} S_{v}\right)$. In contexts where the generation is implicit, this is simply denoted as $\left(S_{u} \succ S_{v}\right)$. Let $E_{u v}^{(i)}$ be the event where at least one of $S_{u}$ and $S_{v}$ appears in the $i$-th generation. The probability of $\left(S_{u} \succ S_{v}\right)$, given $E_{u v}^{(i)}$ and a categorical distribution $P$, is expressed as $\mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)</p>
<p>$$
\mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)}\right)=\frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)}
$$</p>
<p>The optimization objective to minimize the expected loss is then:</p>
<p>$$
\min <em i="1">{P}-\sum</em>\right)=1
$$}^{N} \sum_{S_{u} \in \mathcal{A}} \sum_{S_{v} \in \mathcal{A}} \mathbb{I}\left{S_{u} \stackrel{(i)}{\succ} S_{v}\right} \cdot \log \frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)} \quad \text { s.t. } \sum_{S_{u} \in \mathcal{A}} P\left(S_{u</p>
<p>Proof. Let us begin by examining the position $j$ in the response sequence $\mathcal{S}<em u="u">{K}^{(i)}$ where either $S</em>$ :}$ or $S_{v}$ is first sampled, and the other has not yet been sampled. We denote this event as $F_{j}^{(i)}\left(S_{u}, S_{v}\right)$, and for simplicity, we refer to it as $F_{j</p>
<p>$$
\begin{aligned}
F_{j}=F_{j}^{(i)}\left(S_{u}, S_{v}\right) &amp; ={\text { the earliest position in } \mathcal{S}<em u="u">{K}^{(i)} \text { where either } S</em> j} \
&amp; ={\forall m, n \in{1,2, \ldots, N} \mid S_{m}^{(i)}=S_{u}, S_{n}^{(i)}=S_{v}, j=\min (m, n)}
\end{aligned}
$$} \text { or } S_{v} \text { appears is </p>
<p>Given this event, the probability that $S_{u}$ is sampled before $S_{v}$ across all possible positions $j$ is:</p>
<p>$$
\mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)}\right)=\sum_{j=1}^{N} \mathbb{P}\left(F_{j} \mid P, E_{u v}^{(i)}\right) \times \underbrace{\mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)}, F_{j}\right)}_{\text {(a) }}
$$</p>
<p>To further elucidate (1), which is conditioned on $F_{j}$, we note that the first sampled answer between $S_{u}$ and $S_{v}$ appears at position $j$. We then consider all potential answers sampled prior to $j$. For this, we introduce a permutation set $\mathcal{H}<em j-1="j-1">{j-1}$ to encapsulate all feasible combinations of answers for the initial $j-1$ samplings. A representative sampling sequence is given by: $\mathcal{S}</em>\right}\right}$.
Consequently, (a) can be articulated as:}=\left{S_{(1)} \succ S_{(2)} \succ\right.$ $\left.\cdots \succ S_{(j-1)} \mid \forall l \in{1,2, \ldots, j-1}, S_{(l)} \in \mathcal{A} \backslash\left{S_{u}, S_{v</p>
<p>$$
\mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)}, F_{j}\right)=\sum_{\mathcal{S}<em j-1="j-1">{j-1} \in \mathcal{H}</em>}} \mathbb{P}\left(\mathcal{S<em u="u" v="v">{j-1} \mid P, E</em>}^{(i)}, F_{j}\right) \times \underbrace{\mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)}, \mathcal{S<em j="j">{j-1}, F</em>
$$}\right)}_{\text {(b) }</p>
<p>Consider the term (b), which signifies the probability that, given the first $j-1$ samplings and the restriction that the $j$-th sampling can only be $S_{u}$ or $S_{v}, S_{u}$ is sampled prior to $S_{v}$. This probability is articulated as:</p>
<p>$$
\begin{aligned}
\mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)}, F_{j}, \mathcal{S}<em j="j">{j-1}\right) &amp; =\frac{\mathbb{P}\left(S</em>}^{(i)}=S_{u} \mid P, E_{u v}^{(i)}, F_{j}, \mathcal{S<em j="j">{j-1}\right)}{\mathbb{P}\left(S</em>}^{(i)}=S_{u} \mid P, E_{u v}^{(i)}, F_{j}, \mathcal{S<em j="j">{j-1}\right)+\mathbb{P}\left(S</em>}^{(i)}=S_{v} \mid P, E_{u v}^{(i)}, F_{j}, \mathcal{S<em u="u">{j-1}\right)} \
&amp; =\frac{\frac{P\left(S</em>}\right)}{1-\sum_{S_{m} \in \mathcal{S<em m="m">{j-1}} P\left(S</em>}\right)}}{\frac{P\left(S_{v}\right)}{1-\sum_{S_{m} \in \mathcal{S<em m="m">{j-1}} P\left(S</em>}\right)}+\frac{P\left(S_{u}\right)}{1-\sum_{S_{m} \in \mathcal{S<em m="m">{j-1}} P\left(S</em> \
&amp; =\frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)}
\end{aligned}
$$}\right)}</p>
<p>Integrating equation (9) into equation (8), we obtain:</p>
<p>$$
\begin{aligned}
\mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)}, F_{j}\right) &amp; =\sum_{\mathcal{S}<em j-1="j-1">{j-1} \in \mathcal{H}</em>}} \mathbb{P}\left(\mathcal{S<em j="j">{j-1} \mid P, F</em> \
&amp; =\frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)} \times \sum_{\mathcal{S}}\right) \times \frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)<em j-1="j-1">{j-1} \in \mathcal{H}</em>}} \mathbb{P}\left(\mathcal{S<em u="u" v="v">{j-1} \mid P, E</em>\right) \
&amp; \stackrel{(c)}{=} \frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)}
\end{aligned}
$$}^{(i)}, F_{j</p>
<p>Subsequently, incorporating equation (10) into equation (7), we deduce:</p>
<p>$$
\begin{aligned}
\mathbb{P}\left(S_{u} \succ S_{v} \mid P, E_{u v}^{(i)}\right) &amp; =\sum_{j=1}^{K} \mathbb{P}\left(F_{j} \mid P, E_{u v}^{(i)}\right) \times \frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)} \
&amp; =\frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)} \times \sum_{j=1}^{K} \mathbb{P}\left(F_{j} \mid P, E_{u v}^{(i)}\right) \
&amp; \stackrel{(d)}{=} \frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)}
\end{aligned}
$$</p>
<p>The derivations in (c) and (d) employ the Law of Total Probability.
Incorporating Equation 11 into Equation 3, the minimization objective is formulated as:</p>
<p>$$
\min <em i="1">{P}-\sum</em>\right)=1
$$}^{N} \sum_{S_{u} \in \mathcal{A}} \sum_{S_{v} \in \mathcal{A}} \mathbb{I}\left{S_{u} \stackrel{(i)}{\succ} S_{v}\right} \times \log \frac{P\left(S_{u}\right)}{P\left(S_{u}\right)+P\left(S_{v}\right)} \quad \text { s.t. } \sum_{S_{u} \in \mathcal{A}} P\left(S_{u</p>
<h1>B Detailed Experiment Results</h1>
<h2>B. 1 White-Box Methods OUTPERFORM BLACK-BOX METHODS, BUT THE GAP IS NARROW.</h2>
<p>Comparative Analysis of White-Box and Black-Box Methods: Which performs better - white-box or black-box methods? Do white-box methods, with their access to more internal information, outperform their black-box counterparts? If so, how large is the performance gap? To address these questions, we conduct a comparative analysis of white-box methods based on token probability against black-box models utilizing verbalized confidence.</p>
<p>Implementation details: We utilize the probabilities of each output token to develop three token-probability-based white-box methods: 1) Sequence Probability (seq-prob), which aggregates the probabilities of all tokens; 2) Length-Normalized Sequence Probability (len-norm-prob), which normalizes the sequence probability based on sequence length, i.e., seq-prob ${ }^{1 / \text { length }}$; 3) Key Token Probability (token-prob), designed to focus on the result-specific tokens, e.g., "35" in the output sequence "Explanation: ....; Answer: 35; ...", thereby minimizing the influence of irrelevant output tokens. For our implementation, we use the Chain-of-Thought and Top-K Verbalized Confidence prompt to acquire verbalized confidence and select GPT3 as the backbone model.</p>
<p>Findings: Our comparative analysis, detailed in Table 5 and Table 6, yields several key insights: 1) Generally, white-box methods exhibit better performance, with length-normalized sequence probability and key token probability emerging as the most effective methods across five datasets and four evaluation metrics. 2) The gap between white-box and black-box methods is relatively modest. Moreover, even the best-performing white-box methods fall short of achieving satisfactory results. This is particularly apparent in the AUROC metric, where the performance of nearly all methods across various datasets ranges between $0.5-0.6$, signifying a limited capability in distinguishing between correct and incorrect responses. 3) These experimental results suggest that uncertainty estimation in LLMs remains a challenging and unresolved issue. As mentioned in our introduction, the logit-based methods, which predominantly capture the model's uncertainty regarding the next token, are less effective in capturing the semantic uncertainty inherent in their textual meanings. Although several alternative approaches like semantic uncertainty (Kuhn et al., 2023) have been proposed, they come with significant computational demands. This scenario underscores the need for future research on both white-box and black-box methods to discover more efficient and effective methods for uncertainty estimation in LLMs.</p>
<h2>B. 2 HOW MUCH DOES THE ROLE-PLAY PROMPT AFFECT THE PERFORMANCE?</h2>
<p>To explore how the verbalized confidence elicitation performance varies when LLMs are asked to play different personalities such as "confident" and "cautious", we conduct the experiment in Figure 4 and in Table 7. The results are derived when adding "You are a confident GPT" (Left) and "You are a cautious GPT" (Right) to the beginning of the Chain of Thought (CoT) prompt (Table 15). The experimental results show that the difference between their confidence distribution seems minimal, suggesting that assuming different personalities does not significantly affect performance metrics such as accuracy, ECE, and AUROC.</p>
<h2>B. 3 How is the distribution of Vanilla Verbalized Confidence Across Models AND DATASETS?</h2>
<p>Figure 5 presents the empirical distribution of vanilla verbalized confidence across 4 models and 5 datasets. Notably, all the models output confidence as the multiples of 5 , with most values ranging between the $80 \%$ to $100 \%$ range. This behavior resembles the patterns identified in the training corpus for GPT-like models as discussed by Zhou et al. (2023). Such behavior suggests that models might be imitating human expressions when verbalizing confidence.</p>
<h2>B. 4 Detailed Performance of Different Prompting Strategies</h2>
<p>Multi-step and Top-K prompting strategies demonstrate promising results in reducing ECE and improving AUROC, with Top-K being relatively more effective. Figure 6 presents a comparison of various prompting strategies (CoT, Multi-Step, Top-K) against vanilla verbalized confidence.</p>
<p>Table 5: Performance comparison (metrics are given by $\times 10^{2}$ ) of token-probability-based whitebox methods including the baseline sequence probability ("seq-prob"), length-normalized sequence probability ("len-norm-prob") and key token probability ("token-prob"), and black-box verbalized confidence ("Verbalized") on GPT-3 using Top-K Prompt.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Acc</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">ECE</th>
<th style="text-align: center;">AUROC</th>
<th style="text-align: center;">AUPRC-P</th>
<th style="text-align: center;">AUPRC-N</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">59.90</td>
<td style="text-align: center;">Verbalized</td>
<td style="text-align: center;">39.04</td>
<td style="text-align: center;">50.34</td>
<td style="text-align: center;">60.06</td>
<td style="text-align: center;">40.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seq-prob</td>
<td style="text-align: center;">7.14</td>
<td style="text-align: center;">55.50</td>
<td style="text-align: center;">62.99</td>
<td style="text-align: center;">45.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">len-norm-prob</td>
<td style="text-align: center;">37.65</td>
<td style="text-align: center;">55.50</td>
<td style="text-align: center;">62.99</td>
<td style="text-align: center;">45.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">token-prob</td>
<td style="text-align: center;">32.43</td>
<td style="text-align: center;">60.61</td>
<td style="text-align: center;">69.90</td>
<td style="text-align: center;">47.10</td>
</tr>
<tr>
<td style="text-align: center;">Biz-Ethics</td>
<td style="text-align: center;">61.00</td>
<td style="text-align: center;">Verbalized</td>
<td style="text-align: center;">18.20</td>
<td style="text-align: center;">66.27</td>
<td style="text-align: center;">71.95</td>
<td style="text-align: center;">50.59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seq-prob</td>
<td style="text-align: center;">48.49</td>
<td style="text-align: center;">62.30</td>
<td style="text-align: center;">71.07</td>
<td style="text-align: center;">52.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">len-norm-prob</td>
<td style="text-align: center;">33.70</td>
<td style="text-align: center;">62.30</td>
<td style="text-align: center;">71.07</td>
<td style="text-align: center;">52.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">token-prob</td>
<td style="text-align: center;">27.65</td>
<td style="text-align: center;">67.00</td>
<td style="text-align: center;">74.89</td>
<td style="text-align: center;">55.01</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">11.52</td>
<td style="text-align: center;">Verbalized</td>
<td style="text-align: center;">77.40</td>
<td style="text-align: center;">54.05</td>
<td style="text-align: center;">12.70</td>
<td style="text-align: center;">89.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seq-prob</td>
<td style="text-align: center;">7.73</td>
<td style="text-align: center;">69.80</td>
<td style="text-align: center;">20.40</td>
<td style="text-align: center;">94.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">len-norm-prob</td>
<td style="text-align: center;">72.41</td>
<td style="text-align: center;">70.61</td>
<td style="text-align: center;">21.23</td>
<td style="text-align: center;">94.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">token-prob</td>
<td style="text-align: center;">35.60</td>
<td style="text-align: center;">69.29</td>
<td style="text-align: center;">20.63</td>
<td style="text-align: center;">94.27</td>
</tr>
<tr>
<td style="text-align: center;">DateUND</td>
<td style="text-align: center;">15.72</td>
<td style="text-align: center;">Verbalized</td>
<td style="text-align: center;">83.47</td>
<td style="text-align: center;">50.80</td>
<td style="text-align: center;">15.93</td>
<td style="text-align: center;">84.54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seq-prob</td>
<td style="text-align: center;">16.10</td>
<td style="text-align: center;">62.93</td>
<td style="text-align: center;">22.39</td>
<td style="text-align: center;">90.61</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">len-norm-prob</td>
<td style="text-align: center;">81.27</td>
<td style="text-align: center;">62.93</td>
<td style="text-align: center;">22.39</td>
<td style="text-align: center;">90.61</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">token-prob</td>
<td style="text-align: center;">74.19</td>
<td style="text-align: center;">54.25</td>
<td style="text-align: center;">19.28</td>
<td style="text-align: center;">83.85</td>
</tr>
<tr>
<td style="text-align: center;">Prf-Law</td>
<td style="text-align: center;">44.92</td>
<td style="text-align: center;">Verbalized</td>
<td style="text-align: center;">41.55</td>
<td style="text-align: center;">49.54</td>
<td style="text-align: center;">44.43</td>
<td style="text-align: center;">55.78</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seq-prob</td>
<td style="text-align: center;">32.31</td>
<td style="text-align: center;">51.07</td>
<td style="text-align: center;">45.75</td>
<td style="text-align: center;">56.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">len-norm-prob</td>
<td style="text-align: center;">49.66</td>
<td style="text-align: center;">51.06</td>
<td style="text-align: center;">45.75</td>
<td style="text-align: center;">56.79</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">token-prob</td>
<td style="text-align: center;">43.26</td>
<td style="text-align: center;">61.24</td>
<td style="text-align: center;">53.84</td>
<td style="text-align: center;">64.69</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Distribution of the verbalized confidence with different specified role descriptions in prompts. The results are derived when adding "You are a confident GPT" (Left) and "You are a cautious GPT" (Right) to the beginning of the Chain of Thought (CoT) prompt (Table 15). All other aspects of the prompts remain identical to the standard CoT format.</p>
<p>The detailed performance of CoT, Multi-Step, and Top-K prompt can be found in Table 8, Table 9 and Table 10, respectively. Judging from the 'average' bar, which computes the mean value across five datasets, both Multi-step and Top-K prompting strategies effectively reduce ECE and enhance AUROC. Moreover, Top-K shows relatively better performance improvements. The intuition behind this improvement is that this prompting strategy, requesting the model to generate multiple guesses along with their corresponding confidences, naturally nudges the model to be aware of the existence of</p>
<p>Table 6: Performance comparison (metrics are given by $\times 10^{2}$ ) of token-probability-based whitebox methods including the baseline sequence probability ("seq-prob"), length-normalized sequence probability ("len-norm-prob") and key token probability ("token-prob"), and black-box verbalized confidence ("Verbalized") on GPT-3 using CoT Prompt.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Acc</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">ECE</th>
<th style="text-align: center;">AUROC</th>
<th style="text-align: center;">AUPRC-P</th>
<th style="text-align: center;">AUPRC-N</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DateUND</td>
<td style="text-align: center;">62.33</td>
<td style="text-align: center;">Verbalized</td>
<td style="text-align: center;">37.40</td>
<td style="text-align: center;">50.36</td>
<td style="text-align: center;">62.50</td>
<td style="text-align: center;">38.12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seq-prob</td>
<td style="text-align: center;">62.30</td>
<td style="text-align: center;">56.37</td>
<td style="text-align: center;">65.14</td>
<td style="text-align: center;">43.21</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">len-norm-prob</td>
<td style="text-align: center;">15.78</td>
<td style="text-align: center;">58.70</td>
<td style="text-align: center;">66.57</td>
<td style="text-align: center;">47.24</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">token-prob</td>
<td style="text-align: center;">27.32</td>
<td style="text-align: center;">40.27</td>
<td style="text-align: center;">55.20</td>
<td style="text-align: center;">35.69</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">67.57</td>
<td style="text-align: center;">Verbalized</td>
<td style="text-align: center;">29.74</td>
<td style="text-align: center;">51.37</td>
<td style="text-align: center;">68.16</td>
<td style="text-align: center;">34.54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seq-prob</td>
<td style="text-align: center;">67.56</td>
<td style="text-align: center;">52.04</td>
<td style="text-align: center;">69.58</td>
<td style="text-align: center;">33.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">len-norm-prob</td>
<td style="text-align: center;">6.79</td>
<td style="text-align: center;">52.11</td>
<td style="text-align: center;">70.41</td>
<td style="text-align: center;">33.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">token-prob</td>
<td style="text-align: center;">30.59</td>
<td style="text-align: center;">53.00</td>
<td style="text-align: center;">68.80</td>
<td style="text-align: center;">36.89</td>
</tr>
<tr>
<td style="text-align: center;">Biz-Ethics</td>
<td style="text-align: center;">59.00</td>
<td style="text-align: center;">Verbalized</td>
<td style="text-align: center;">40.90</td>
<td style="text-align: center;">49.15</td>
<td style="text-align: center;">58.59</td>
<td style="text-align: center;">41.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seq-prob</td>
<td style="text-align: center;">26.50</td>
<td style="text-align: center;">58.99</td>
<td style="text-align: center;">64.30</td>
<td style="text-align: center;">47.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">len-norm-prob</td>
<td style="text-align: center;">39.43</td>
<td style="text-align: center;">58.99</td>
<td style="text-align: center;">64.30</td>
<td style="text-align: center;">47.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">token-prob</td>
<td style="text-align: center;">36.31</td>
<td style="text-align: center;">67.38</td>
<td style="text-align: center;">75.33</td>
<td style="text-align: center;">54.89</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">52.31</td>
<td style="text-align: center;">Verbalized</td>
<td style="text-align: center;">47.49</td>
<td style="text-align: center;">50.32</td>
<td style="text-align: center;">52.47</td>
<td style="text-align: center;">48.02</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seq-prob</td>
<td style="text-align: center;">52.30</td>
<td style="text-align: center;">57.47</td>
<td style="text-align: center;">56.75</td>
<td style="text-align: center;">54.39</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">len-norm-prob</td>
<td style="text-align: center;">29.80</td>
<td style="text-align: center;">57.92</td>
<td style="text-align: center;">58.84</td>
<td style="text-align: center;">55.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">token-prob</td>
<td style="text-align: center;">44.94</td>
<td style="text-align: center;">58.44</td>
<td style="text-align: center;">57.54</td>
<td style="text-align: center;">60.43</td>
</tr>
<tr>
<td style="text-align: center;">Prf-Law</td>
<td style="text-align: center;">44.85</td>
<td style="text-align: center;">Verbalized</td>
<td style="text-align: center;">53.43</td>
<td style="text-align: center;">50.13</td>
<td style="text-align: center;">44.90</td>
<td style="text-align: center;">55.91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seq-prob</td>
<td style="text-align: center;">44.85</td>
<td style="text-align: center;">51.88</td>
<td style="text-align: center;">46.62</td>
<td style="text-align: center;">56.09</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">len-norm-prob</td>
<td style="text-align: center;">31.00</td>
<td style="text-align: center;">50.10</td>
<td style="text-align: center;">45.34</td>
<td style="text-align: center;">55.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">token-prob</td>
<td style="text-align: center;">51.75</td>
<td style="text-align: center;">57.83</td>
<td style="text-align: center;">50.53</td>
<td style="text-align: center;">62.52</td>
</tr>
<tr>
<td style="text-align: center;">Role</td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">ACC</td>
<td style="text-align: center;">ECE</td>
<td style="text-align: center;">AUROC</td>
<td style="text-align: center;">AUPRC-P</td>
<td style="text-align: center;">AUPRC-N</td>
</tr>
<tr>
<td style="text-align: center;">Confident</td>
<td style="text-align: center;">chatgpt-0613</td>
<td style="text-align: center;">0.7103</td>
<td style="text-align: center;">0.2741</td>
<td style="text-align: center;">0.5679</td>
<td style="text-align: center;">0.7398</td>
<td style="text-align: center;">0.3635</td>
</tr>
<tr>
<td style="text-align: center;">Cautious</td>
<td style="text-align: center;">chatgpt-0613</td>
<td style="text-align: center;">0.6983</td>
<td style="text-align: center;">0.2812</td>
<td style="text-align: center;">0.5946</td>
<td style="text-align: center;">0.7415</td>
<td style="text-align: center;">0.4009</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance Comparison of Verbalized Confidence Elicitation with two types of prompt: "You are a confident GPT" and "You are a cautious GPT". The difference between these two prompts seems minimal, suggesting that asking LLMs to take on different personae does not significantly affect the performance.
various possible answers, preventing overconfidence in a single response and promoting re-evaluation of given answers.</p>
<h1>B. 5 Top-K Verbalized Confidence Performance</h1>
<p>The detailed experiments performance of Top-K verbalized confidence can be found in Table 10.</p>
<h2>B. 6 Impact of Misleading Prompts in Misleading Sampling Strategy</h2>
<p>To study the effect of different misleading prompts used in misleading sampling strategy, we categorize the prompts into three types: Weak Claim, Strong Claim, and External Source. The specific prompts are shown in Table 11. The Weak Claim category represents prompts that exhibit uncertainty, such as "I vaguely remember the answer is" or "I think the answer should be". These prompts suggest that the user is providing misleading information to the Large Language Model (LLM) but lacks confidence in the information provided. The Strong Claim category includes prompts like "I am pretty sure that this is" or "I am very confident that", which express a high degree of certainty. The External Source</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Empirical distribution of vanilla verbalized confidence across 4 models and 5 datasets. The prompt used is in Table 14. From this figure, we can observe that 1) the confidence levels primarily range between $80 \%$ and $100 \%$, often in multiples of 5 ; 2) a large portion of incorrect predictions (red) has been observed even in the $100 \%$ confidence bar, indicating significant overconfidence.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Performance Comparison of four verbalized confidence methods: vanilla, CoT, Multi-Step, Top-K in terms of ECE and AUROC for five types of datasets on GPT-3.5. Refer to Table 10 for detailed results.
category represents prompts that cite external sources as their evidence, such as "Wikipedia says" or "the latest research shows that".</p>
<p>Our experimental results (Table 11) indicate that the Weak Claim category performs better. A possible explanation is that on one hand even providing weak misleading information, the model will analyze and reassess their answers. On the other hand, since the misleading answers are generated randomly, confidently providing this information can sometimes lead to negative effects. For example, the model provides a correct answer with moderate confidence. However, if a misleading hint is provided</p>
<p>Table 8: Improvement of verbalized confidence with Chain-of-Thought Prompts</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">CoT</th>
<th style="text-align: center;">GPT3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ACC(\%)</td>
<td style="text-align: center;">ECE</td>
<td style="text-align: center;">AUROC</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">55</td>
</tr>
<tr>
<td style="text-align: center;">DateUnd</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">57</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">60</td>
</tr>
<tr>
<td style="text-align: center;">Prf-Law</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">49</td>
</tr>
<tr>
<td style="text-align: center;">Biz-Ethics</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">56</td>
</tr>
</tbody>
</table>
<p>Table 9: Evaluation of multistep verbalized confidence for GPT-3.5 Models</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">SA</th>
<th style="text-align: center;">GPT3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ACC(\%)</td>
<td style="text-align: center;">ECE</td>
<td style="text-align: center;">AUROC</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">60</td>
</tr>
<tr>
<td style="text-align: center;">DateUnd</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">57</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">72</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">59</td>
</tr>
<tr>
<td style="text-align: center;">Prf-Law</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">49</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">49</td>
</tr>
<tr>
<td style="text-align: center;">Biz-Ethics</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">60</td>
</tr>
</tbody>
</table>
<p>with high confidence or is supported by an external source, the model may be inclined to believe the prompt and alter its predictions.</p>
<h1>B. 7 IMPACT OF THE NUMBER OF CANDIDATE ANSWERS</h1>
<p>We investigate the impact of the number of candidate answers, denoted as $K$, utilized in the sampling strategy. Specifically, $K$ represents the number of queries used to construct the set of candidate answers for consistency calculation. We illustrate its calibration performance (ECE) and failure prediction performance (AUROC) in relation to varying numbers of $K$ (ranging from $K=1$ to $K=13$ ) in Figure 7.</p>
<p>The results indicate that, in terms of AUROC, a higher candidate set size $K$ contributes to superior performance and reduced variance. However, the optimal candidate size $K$ for ECE varies across different datasets. For instance, the StrategyQA dataset exhibits improved performance with a larger $K$, whereas the Business Ethics dataset generally performs better with a moderate number of candidate answers (e.g., $K=4$ ). This observation can be attributed to the limited variability of misleading information (restricted to 4 types) used in our experiments for the Business Ethics dataset, implying that the introduction of a large number of more queries does not significantly enhance the information pool. Therefore, to strike a balance between computational efficiency and performance, we set the candidate set to be 4 in our study.</p>
<p>Table 10: Evaluation of Top-K verbalized confidence on GPT-3.5.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">GPT3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ACC(\%)</td>
<td style="text-align: center;">ECE</td>
<td style="text-align: center;">AUROC</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">58.5</td>
</tr>
<tr>
<td style="text-align: center;">DateUnd</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">74.2</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">61.3</td>
</tr>
<tr>
<td style="text-align: center;">Prf-Law</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">58.9</td>
</tr>
<tr>
<td style="text-align: center;">Biz-Ethics</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">73.3</td>
</tr>
</tbody>
</table>
<p>Table 11: Different Prompts used for misleading sampling strategy.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Group</th>
<th style="text-align: left;">Consistency Confidence Misleading Prompts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Weak Claim</td>
<td style="text-align: left;">Hint: I think the answer should be</td>
</tr>
<tr>
<td style="text-align: left;">Weak Claim</td>
<td style="text-align: left;">Hint: I read online the answer is</td>
</tr>
<tr>
<td style="text-align: left;">Weak Claim</td>
<td style="text-align: left;">Hint: I vaguely remember the answer is</td>
</tr>
<tr>
<td style="text-align: left;">External Source</td>
<td style="text-align: left;">Hint: My teacher tells me the answer is</td>
</tr>
<tr>
<td style="text-align: left;">External Source</td>
<td style="text-align: left;">Hint: The textbook shows that answer is</td>
</tr>
<tr>
<td style="text-align: left;">External Source</td>
<td style="text-align: left;">Hint: An expert says that the answer is</td>
</tr>
<tr>
<td style="text-align: left;">External Source</td>
<td style="text-align: left;">Hint: Latest research shows that the answer is</td>
</tr>
<tr>
<td style="text-align: left;">External Source</td>
<td style="text-align: left;">Hint: Wikipedia claims that the answer is</td>
</tr>
<tr>
<td style="text-align: left;">Strong Claim</td>
<td style="text-align: left;">Hint: I am 80\% sure the answer is</td>
</tr>
<tr>
<td style="text-align: left;">Strong Claim</td>
<td style="text-align: left;">Hint: I am pretty sure the answer is</td>
</tr>
<tr>
<td style="text-align: left;">Strong Claim</td>
<td style="text-align: left;">Hint: Undoubtedly the answer is</td>
</tr>
<tr>
<td style="text-align: left;">Strong Claim</td>
<td style="text-align: left;">Hint: With my 100\% confidence the answer is</td>
</tr>
</tbody>
</table>
<h1>B. 8 PERFORMANCE OF DIFFERENT CONFIDENCE ELICITATION METHODS</h1>
<h2>C Related Works</h2>
<p>Confidence Elicitation in LLMs. Confidence elicitation refers to the process of estimating LLM's confidence in their responses, without relying on model fine-tuning or accessing the proprietary information of LLMs. Within this scope, Lin et al. (2022) proposes the concept of verbalized confidence that elicits the model to output confidence directly. However, the evaluation is tailored
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Impact of the number of responses responses on GPT-3.5. The sampling strategy is fixed as misleading. For every given number of misleading hints, we randomly sample the specified number of queries for 5 times and calculate the mean ECE and AUROC, and compute its variance(plotted as error bar). Note that the number of hints plus 1 is the number of responses sampled during experiment.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding to: Miao Xiong (miao.xiong@u.nus.edu).
${ }^{\dagger}$ Equal advising: bhooi@comp.nus.edu.sg, junxianh@cse.ust.hk&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7433 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7433</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7433</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-267406682</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.00905v4.pdf" target="_blank">Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation</a></p>
                <p><strong>Paper Abstract:</strong> Context: The rapid evolution of Large Language Models (LLMs) has sparked significant interest in leveraging their capabilities for automating code review processes. Prior studies often focus on developing LLMs for code review automation, yet require expensive resources, which is infeasible for organizations with limited budgets and resources. Thus, fine-tuning and prompt engineering are the two common approaches to leveraging LLMs for code review automation. Objective: We aim to investigate the performance of LLMs-based code review automation based on two contexts, i.e., when LLMs are leveraged by fine-tuning and prompting. Fine-tuning involves training the model on a specific code review dataset, while prompting involves providing explicit instructions to guide the model's generation process without requiring a specific code review dataset. Method: We leverage model fine-tuning and inference techniques (i.e., zero-shot learning, few-shot learning and persona) on LLMs-based code review automation. In total, we investigate 12 variations of two LLMs-based code review automation (i.e., GPT- 3.5 and Magicoder), and compare them with the Guo et al.'s approach and three existing code review automation approaches. Results: The fine-tuning of GPT 3.5 with zero-shot learning helps GPT-3.5 to achieve 73.17% -74.23% higher EM than the Guo et al.'s approach. In addition, when GPT-3.5 is not fine-tuned, GPT-3.5 with few-shot learning achieves 46.38% - 659.09% higher EM than GPT-3.5 with zero-shot learning. Conclusions: Based on our results, we recommend that (1) LLMs for code review automation should be fine-tuned to achieve the highest performance; and (2) when data is not sufficient for model fine-tuning (e.g., a cold-start problem), few-shot learning without a persona should be used for LLMs for code review automation.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7433.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7433.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tune vs Zero-shot GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned GPT-3.5 (zero-shot inference) versus non-fine-tuned GPT-3.5 (Guo et al. zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison showing that fine-tuning GPT-3.5 on a small subset of task-specific examples substantially improves Exact Match (EM) and CodeBLEU over using GPT-3.5 in zero-shot prompting without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial large autoregressive transformer pretrained on large corpora (text+code); further adapted via OpenAI fine-tuning API in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review automation (generate revised code from submitted code + reviewer comment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a submitted code snippet and (optionally) a reviewer comment, produce the corrected/improved revised code (sequence generation).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language instruction + code snippet input (zero-shot prompt; no demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot prompt template with an instruction and input (submitted code and reviewer comment). Fine-tuned on ≈6% of training set (~20k examples) prior to inference; temperature=0.0, top_p=1.0, max_length=512.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) and CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Fine-tuned GPT-3.5 EM example: 37.93% (CodeReviewer dataset); CodeBLEU example: 49.00% (CodeReviewer).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Guo et al. zero-shot GPT-3.5 EM (CodeReviewer): 21.77%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+73.17% to +74.23% higher EM (relative) reported for fine-tuned GPT-3.5 vs Guo et al.'s zero-shot GPT-3.5 (range across datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Fine-tuned with ~6% of training examples (trial-and-error selection), OpenAI fine-tune API; inference temp=0.0, top_p=1.0, max_length=512; zero-shot prompt template (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7433.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7433.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuning benefit (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of model fine-tuning on GPT-3.5 for code review automation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantifies improvement from fine-tuning GPT-3.5 (trained on a small subset of in-domain examples) relative to the same model used without fine-tuning, showing large EM and CodeBLEU gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer adapted via supervised fine-tuning on target code-review pairs (prompted at inference in zero-shot style).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review automation (revised code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate task-specific revised code from submitted code (and reviewer comment) by leveraging learned mapping from fine-tuning examples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Fine-tuned model used with zero-shot prompt at inference (instruction + input).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>model adaptation / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Fine-tuning used ~6% of training data (randomly sampled); inference uses zero-shot prompt templates. Hyperparams per OpenAI defaults for fine-tune; inference temp=0.0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) and CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Fine-tuned GPT-3.5 EM (examples across datasets) reported substantially higher than non-fine-tuned; CodeBLEU also increased.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Non-fine-tuned GPT-3.5 zero-shot (as evaluated in this study / Guo et al. baseline values in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Fine-tuning produced +63.91% to +1,100% higher EM (relative) and +5.91% to +63.9% higher CodeBLEU in reported comparisons (range across datasets and cases).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Fine-tune examples ≈6% of training; inference zero-shot prompts; temperature=0.0, top_p=1.0, max_length=512.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7433.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7433.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot vs Zero-shot (GPT-3.5, non‑fine‑tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of in‑context few‑shot demonstrations versus zero‑shot prompts for non‑fine‑tuned GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding a small number of task demonstrations in the prompt (in‑context learning) substantially improves EM and CodeBLEU compared to zero‑shot prompting for non‑fine‑tuned GPT‑3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (non-fine-tuned at inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer used via in-context learning with demonstration examples appended to prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review automation (revised code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate revised code using prompt that contains N demonstration (input,comment -> revised code) examples plus the test input.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot in‑context learning: instruction + N demonstration examples (N=3) + test input (code + comment).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / in‑context demonstration</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Three demonstrations selected per test sample using BM25 retrieval (gensim); prompt template per Figure 3b; temperature=0.0, top_p=1.0, max_length=512; N=3 chosen per Gao et al. guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) and CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Few-shot GPT-3.5 (non-fine-tuned) obtained substantially higher EM than zero-shot; CodeBLEU also increased (examples in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot non-fine-tuned GPT-3.5 (same datasets; see Table 4 for per-dataset numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Few-shot achieved +46.38% to +659.09% higher EM than zero-shot (range across datasets and persona conditions); CodeBLEU improved by 3.97%–33.36% (no persona) and 5.55%–60.27% (with persona) in reported ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>3 demonstrations per prompt selected by BM25; demonstration selection chosen by BM25 (gensim); temperature=0.0; max_length=512.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7433.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7433.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Persona inclusion effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of adding a 'persona' instruction (act as expert developer) to prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including an explicit persona (instructing the model to act as an expert developer) in prompts reduced EM and CodeBLEU compared to identical prompts without the persona, and tended to cause extra incorrect changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer prompted to adopt a role/persona prior to generating revised code.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review automation (revised code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate improved code when the prompt includes an explicit persona statement (e.g., 'You are an expert software developer in <lang>').</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instruction + persona statement + (optionally) demonstrations + input code + reviewer comment.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / instruction framing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Persona text instructs model to 'act as an expert software developer' and to prefer high-quality code; tested with both zero-shot and few-shot prompts. Examples in Figure 3 and Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) and CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Including persona produced lower EM and CodeBLEU; e.g., EM decreased by small to large amounts depending on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Identical prompts without persona (zero-shot or few-shot) as reported in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Persona inclusion caused EM to be 1.02% to 54.17% lower (relative) and CodeBLEU to be 0.15% to 19.13% lower depending on prompt and dataset (ranges reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Persona phrasing: 'You are an expert software developer in <lang>. You always want to improve your code ...'; evaluated with/without persona for zero-shot and few-shot; temp=0.0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7433.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7433.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt design: simple vs broken vs detailed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of prompt instruction style (simple instruction, instruction broken into smaller steps, detailed instruction) on GPT-3.5 performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different prompt instruction formulations materially affected EM: a simple single instruction prompt outperformed prompts where the instruction was broken into smaller steps or where a longer, more detailed instruction was provided.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer prompted with alternative instruction formulations to test sensitivity to prompt wording and structure.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review automation (revised code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce revised code from input code + reviewer comment; compare alternative instruction wordings and structures in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot or few-shot prompts with three alternative instruction styles: P1 simple instruction (Figure 3), P2 instruction broken into smaller steps (Figure 7), P3 detailed instruction (Figure 8).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / instruction framing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>P1: concise single instruction. P2: the task broken into smaller numbered steps. P3: more detailed constraints and wording. Evaluated both zero-shot and few-shot (no persona for these experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) and CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Simple instruction (P1) achieved the highest EM across prompt designs (per-dataset EMs in Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompts with instruction broken into smaller steps (P2) and detailed instruction (P3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>For zero-shot: P1 produced +16.44% to +45.45% EM (relative) over P2, and +37.12% to +880.00% EM (relative) over P3; for few-shot: P1 produced +5.15% to +290.00% EM over P2 and +5.61% to +515.79% EM over P3 (ranges across datasets reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Persona excluded for these prompt-design experiments; temperature=0.0; few-shot used 3 examples when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7433.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7433.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Demonstration selection & count (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Choice of demonstration examples for few-shot (BM25-selected, 3 examples) and its effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study used BM25 retrieval to select three in‑context demonstration examples per test input; the authors cite prior work showing three examples can reach ~90% of maximum in-context performance and observed sizable few-shot gains using this selection/count.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (non-fine-tuned when few-shot used)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer performing in-context learning with retrieved demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review automation (revised code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate revised code using an in-context prompt containing retrieved demonstrations (input, comment -> revised code).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot in-context prompt with exactly 3 demonstrations selected via BM25 per test example.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / example selection</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>BM25 (gensim) used to select 3 nearest demonstration examples; choice based on prior findings (Gao et al.) and trial; demonstrations appended before instruction+test input. N=3 chosen because prior work indicated ~90% of highest EM can be reached with 3 vs many more examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) and CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Using 3 BM25-selected demonstrations produced the reported few-shot EM improvements vs zero-shot (see few-shot vs zero-shot entry); concrete per-dataset numbers appear in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot prompts without demonstrations; alternative selection/counts not tried in this study (threat noted).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Enabling few-shot with 3 BM25-selected demos yielded +46.38% to +659.09% higher EM (relative) vs zero-shot (ranges reported across datasets and conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>BM25 retrieval via gensim; 3 demonstrations per prompt; temperature=0.0; max_length=512.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7433.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7433.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tune size effect (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of fine-tuning dataset size (6% vs 10% vs 20%) on fine-tuned GPT-3.5 performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Larger fine-tuning set sizes produced modest but consistent improvements in EM and CodeBLEU for GPT-3.5; 20% fine-tune gave the highest performance among tested sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer further fine-tuned on 6%, 10% or 20% subsamples of the training data and evaluated with identical zero-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review automation (revised code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate how the portion of training data used for fine-tuning affects downstream EM and CodeBLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Model fine-tuned on varying-size datasets; inference using the same zero-shot prompt template.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>model adaptation / data size</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Fine-tune sizes used: ~6%, 10%, 20% of original training set. Persona not used. Inference temp=0.0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) and CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples from Table 6: CodeReviewer EM: 6%->37.93%, 10%->37.72%, 20%->38.80%; CodeBLEU changed modestly across sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Fine-tune with 6% of training set (baseline for the study's initial choice).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Fine-tuning with 20% vs 6% produced +2.29% to +12.07% higher EM and +0.54% to +2.55% higher CodeBLEU depending on dataset (reported ranges).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Fine-tune performed with selected fraction of training set; persona excluded; inference temp=0.0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring the potential of chatgpt in automated code refinement: An empirical study <em>(Rating: 2)</em></li>
                <li>What makes good in-context demonstrations for code intelligence tasks with llms? <em>(Rating: 2)</em></li>
                <li>Constructing effective in-context demonstration for code intelligence tasks: An empirical study <em>(Rating: 1)</em></li>
                <li>A prompt pattern catalog to enhance prompt engineering with chatgpt <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7433",
    "paper_id": "paper-267406682",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Fine-tune vs Zero-shot GPT-3.5",
            "name_full": "Fine-tuned GPT-3.5 (zero-shot inference) versus non-fine-tuned GPT-3.5 (Guo et al. zero-shot)",
            "brief_description": "Comparison showing that fine-tuning GPT-3.5 on a small subset of task-specific examples substantially improves Exact Match (EM) and CodeBLEU over using GPT-3.5 in zero-shot prompting without fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Commercial large autoregressive transformer pretrained on large corpora (text+code); further adapted via OpenAI fine-tuning API in this study.",
            "model_size": "175B",
            "task_name": "Code review automation (generate revised code from submitted code + reviewer comment)",
            "task_description": "Given a submitted code snippet and (optionally) a reviewer comment, produce the corrected/improved revised code (sequence generation).",
            "problem_format": "Natural-language instruction + code snippet input (zero-shot prompt; no demonstrations).",
            "format_category": "prompt style",
            "format_details": "Zero-shot prompt template with an instruction and input (submitted code and reviewer comment). Fine-tuned on ≈6% of training set (~20k examples) prior to inference; temperature=0.0, top_p=1.0, max_length=512.",
            "performance_metric": "Exact Match (EM) and CodeBLEU",
            "performance_value": "Fine-tuned GPT-3.5 EM example: 37.93% (CodeReviewer dataset); CodeBLEU example: 49.00% (CodeReviewer).",
            "baseline_performance": "Guo et al. zero-shot GPT-3.5 EM (CodeReviewer): 21.77%",
            "performance_change": "+73.17% to +74.23% higher EM (relative) reported for fine-tuned GPT-3.5 vs Guo et al.'s zero-shot GPT-3.5 (range across datasets).",
            "experimental_setting": "Fine-tuned with ~6% of training examples (trial-and-error selection), OpenAI fine-tune API; inference temp=0.0, top_p=1.0, max_length=512; zero-shot prompt template (Figure 3).",
            "statistical_significance": null,
            "uuid": "e7433.0",
            "source_info": {
                "paper_title": "Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Fine-tuning benefit (GPT-3.5)",
            "name_full": "Effect of model fine-tuning on GPT-3.5 for code review automation",
            "brief_description": "Quantifies improvement from fine-tuning GPT-3.5 (trained on a small subset of in-domain examples) relative to the same model used without fine-tuning, showing large EM and CodeBLEU gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Pretrained transformer adapted via supervised fine-tuning on target code-review pairs (prompted at inference in zero-shot style).",
            "model_size": "175B",
            "task_name": "Code review automation (revised code generation)",
            "task_description": "Generate task-specific revised code from submitted code (and reviewer comment) by leveraging learned mapping from fine-tuning examples.",
            "problem_format": "Fine-tuned model used with zero-shot prompt at inference (instruction + input).",
            "format_category": "model adaptation / prompt style",
            "format_details": "Fine-tuning used ~6% of training data (randomly sampled); inference uses zero-shot prompt templates. Hyperparams per OpenAI defaults for fine-tune; inference temp=0.0.",
            "performance_metric": "Exact Match (EM) and CodeBLEU",
            "performance_value": "Fine-tuned GPT-3.5 EM (examples across datasets) reported substantially higher than non-fine-tuned; CodeBLEU also increased.",
            "baseline_performance": "Non-fine-tuned GPT-3.5 zero-shot (as evaluated in this study / Guo et al. baseline values in Table 4).",
            "performance_change": "Fine-tuning produced +63.91% to +1,100% higher EM (relative) and +5.91% to +63.9% higher CodeBLEU in reported comparisons (range across datasets and cases).",
            "experimental_setting": "Fine-tune examples ≈6% of training; inference zero-shot prompts; temperature=0.0, top_p=1.0, max_length=512.",
            "statistical_significance": null,
            "uuid": "e7433.1",
            "source_info": {
                "paper_title": "Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Few-shot vs Zero-shot (GPT-3.5, non‑fine‑tuned)",
            "name_full": "Effect of in‑context few‑shot demonstrations versus zero‑shot prompts for non‑fine‑tuned GPT-3.5",
            "brief_description": "Adding a small number of task demonstrations in the prompt (in‑context learning) substantially improves EM and CodeBLEU compared to zero‑shot prompting for non‑fine‑tuned GPT‑3.5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (non-fine-tuned at inference)",
            "model_description": "Pretrained transformer used via in-context learning with demonstration examples appended to prompt.",
            "model_size": "175B",
            "task_name": "Code review automation (revised code generation)",
            "task_description": "Generate revised code using prompt that contains N demonstration (input,comment -&gt; revised code) examples plus the test input.",
            "problem_format": "Few-shot in‑context learning: instruction + N demonstration examples (N=3) + test input (code + comment).",
            "format_category": "prompt style / in‑context demonstration",
            "format_details": "Three demonstrations selected per test sample using BM25 retrieval (gensim); prompt template per Figure 3b; temperature=0.0, top_p=1.0, max_length=512; N=3 chosen per Gao et al. guidance.",
            "performance_metric": "Exact Match (EM) and CodeBLEU",
            "performance_value": "Few-shot GPT-3.5 (non-fine-tuned) obtained substantially higher EM than zero-shot; CodeBLEU also increased (examples in Table 4).",
            "baseline_performance": "Zero-shot non-fine-tuned GPT-3.5 (same datasets; see Table 4 for per-dataset numbers).",
            "performance_change": "Few-shot achieved +46.38% to +659.09% higher EM than zero-shot (range across datasets and persona conditions); CodeBLEU improved by 3.97%–33.36% (no persona) and 5.55%–60.27% (with persona) in reported ranges.",
            "experimental_setting": "3 demonstrations per prompt selected by BM25; demonstration selection chosen by BM25 (gensim); temperature=0.0; max_length=512.",
            "statistical_significance": null,
            "uuid": "e7433.2",
            "source_info": {
                "paper_title": "Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Persona inclusion effect",
            "name_full": "Impact of adding a 'persona' instruction (act as expert developer) to prompts",
            "brief_description": "Including an explicit persona (instructing the model to act as an expert developer) in prompts reduced EM and CodeBLEU compared to identical prompts without the persona, and tended to cause extra incorrect changes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Pretrained transformer prompted to adopt a role/persona prior to generating revised code.",
            "model_size": "175B",
            "task_name": "Code review automation (revised code generation)",
            "task_description": "Generate improved code when the prompt includes an explicit persona statement (e.g., 'You are an expert software developer in &lt;lang&gt;').",
            "problem_format": "Instruction + persona statement + (optionally) demonstrations + input code + reviewer comment.",
            "format_category": "prompt style / instruction framing",
            "format_details": "Persona text instructs model to 'act as an expert software developer' and to prefer high-quality code; tested with both zero-shot and few-shot prompts. Examples in Figure 3 and Table 4.",
            "performance_metric": "Exact Match (EM) and CodeBLEU",
            "performance_value": "Including persona produced lower EM and CodeBLEU; e.g., EM decreased by small to large amounts depending on dataset.",
            "baseline_performance": "Identical prompts without persona (zero-shot or few-shot) as reported in Table 4.",
            "performance_change": "Persona inclusion caused EM to be 1.02% to 54.17% lower (relative) and CodeBLEU to be 0.15% to 19.13% lower depending on prompt and dataset (ranges reported in paper).",
            "experimental_setting": "Persona phrasing: 'You are an expert software developer in &lt;lang&gt;. You always want to improve your code ...'; evaluated with/without persona for zero-shot and few-shot; temp=0.0.",
            "statistical_significance": null,
            "uuid": "e7433.3",
            "source_info": {
                "paper_title": "Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Prompt design: simple vs broken vs detailed",
            "name_full": "Effect of prompt instruction style (simple instruction, instruction broken into smaller steps, detailed instruction) on GPT-3.5 performance",
            "brief_description": "Different prompt instruction formulations materially affected EM: a simple single instruction prompt outperformed prompts where the instruction was broken into smaller steps or where a longer, more detailed instruction was provided.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Pretrained transformer prompted with alternative instruction formulations to test sensitivity to prompt wording and structure.",
            "model_size": "175B",
            "task_name": "Code review automation (revised code generation)",
            "task_description": "Produce revised code from input code + reviewer comment; compare alternative instruction wordings and structures in prompts.",
            "problem_format": "Zero-shot or few-shot prompts with three alternative instruction styles: P1 simple instruction (Figure 3), P2 instruction broken into smaller steps (Figure 7), P3 detailed instruction (Figure 8).",
            "format_category": "prompt style / instruction framing",
            "format_details": "P1: concise single instruction. P2: the task broken into smaller numbered steps. P3: more detailed constraints and wording. Evaluated both zero-shot and few-shot (no persona for these experiments).",
            "performance_metric": "Exact Match (EM) and CodeBLEU",
            "performance_value": "Simple instruction (P1) achieved the highest EM across prompt designs (per-dataset EMs in Table 7).",
            "baseline_performance": "Prompts with instruction broken into smaller steps (P2) and detailed instruction (P3).",
            "performance_change": "For zero-shot: P1 produced +16.44% to +45.45% EM (relative) over P2, and +37.12% to +880.00% EM (relative) over P3; for few-shot: P1 produced +5.15% to +290.00% EM over P2 and +5.61% to +515.79% EM over P3 (ranges across datasets reported in the paper).",
            "experimental_setting": "Persona excluded for these prompt-design experiments; temperature=0.0; few-shot used 3 examples when applicable.",
            "statistical_significance": null,
            "uuid": "e7433.4",
            "source_info": {
                "paper_title": "Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Demonstration selection & count (few-shot)",
            "name_full": "Choice of demonstration examples for few-shot (BM25-selected, 3 examples) and its effect",
            "brief_description": "The study used BM25 retrieval to select three in‑context demonstration examples per test input; the authors cite prior work showing three examples can reach ~90% of maximum in-context performance and observed sizable few-shot gains using this selection/count.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (non-fine-tuned when few-shot used)",
            "model_description": "Pretrained transformer performing in-context learning with retrieved demonstrations.",
            "model_size": "175B",
            "task_name": "Code review automation (revised code generation)",
            "task_description": "Generate revised code using an in-context prompt containing retrieved demonstrations (input, comment -&gt; revised code).",
            "problem_format": "Few-shot in-context prompt with exactly 3 demonstrations selected via BM25 per test example.",
            "format_category": "prompt style / example selection",
            "format_details": "BM25 (gensim) used to select 3 nearest demonstration examples; choice based on prior findings (Gao et al.) and trial; demonstrations appended before instruction+test input. N=3 chosen because prior work indicated ~90% of highest EM can be reached with 3 vs many more examples.",
            "performance_metric": "Exact Match (EM) and CodeBLEU",
            "performance_value": "Using 3 BM25-selected demonstrations produced the reported few-shot EM improvements vs zero-shot (see few-shot vs zero-shot entry); concrete per-dataset numbers appear in Table 4.",
            "baseline_performance": "Zero-shot prompts without demonstrations; alternative selection/counts not tried in this study (threat noted).",
            "performance_change": "Enabling few-shot with 3 BM25-selected demos yielded +46.38% to +659.09% higher EM (relative) vs zero-shot (ranges reported across datasets and conditions).",
            "experimental_setting": "BM25 retrieval via gensim; 3 demonstrations per prompt; temperature=0.0; max_length=512.",
            "statistical_significance": null,
            "uuid": "e7433.5",
            "source_info": {
                "paper_title": "Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Fine-tune size effect (GPT-3.5)",
            "name_full": "Effect of fine-tuning dataset size (6% vs 10% vs 20%) on fine-tuned GPT-3.5 performance",
            "brief_description": "Larger fine-tuning set sizes produced modest but consistent improvements in EM and CodeBLEU for GPT-3.5; 20% fine-tune gave the highest performance among tested sizes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (fine-tuned)",
            "model_description": "Pretrained transformer further fine-tuned on 6%, 10% or 20% subsamples of the training data and evaluated with identical zero-shot prompts.",
            "model_size": "175B",
            "task_name": "Code review automation (revised code generation)",
            "task_description": "Evaluate how the portion of training data used for fine-tuning affects downstream EM and CodeBLEU.",
            "problem_format": "Model fine-tuned on varying-size datasets; inference using the same zero-shot prompt template.",
            "format_category": "model adaptation / data size",
            "format_details": "Fine-tune sizes used: ~6%, 10%, 20% of original training set. Persona not used. Inference temp=0.0.",
            "performance_metric": "Exact Match (EM) and CodeBLEU",
            "performance_value": "Examples from Table 6: CodeReviewer EM: 6%-&gt;37.93%, 10%-&gt;37.72%, 20%-&gt;38.80%; CodeBLEU changed modestly across sizes.",
            "baseline_performance": "Fine-tune with 6% of training set (baseline for the study's initial choice).",
            "performance_change": "Fine-tuning with 20% vs 6% produced +2.29% to +12.07% higher EM and +0.54% to +2.55% higher CodeBLEU depending on dataset (reported ranges).",
            "experimental_setting": "Fine-tune performed with selected fraction of training set; persona excluded; inference temp=0.0.",
            "statistical_significance": null,
            "uuid": "e7433.6",
            "source_info": {
                "paper_title": "Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring the potential of chatgpt in automated code refinement: An empirical study",
            "rating": 2,
            "sanitized_title": "exploring_the_potential_of_chatgpt_in_automated_code_refinement_an_empirical_study"
        },
        {
            "paper_title": "What makes good in-context demonstrations for code intelligence tasks with llms?",
            "rating": 2,
            "sanitized_title": "what_makes_good_incontext_demonstrations_for_code_intelligence_tasks_with_llms"
        },
        {
            "paper_title": "Constructing effective in-context demonstration for code intelligence tasks: An empirical study",
            "rating": 1,
            "sanitized_title": "constructing_effective_incontext_demonstration_for_code_intelligence_tasks_an_empirical_study"
        },
        {
            "paper_title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "rating": 1,
            "sanitized_title": "a_prompt_pattern_catalog_to_enhance_prompt_engineering_with_chatgpt"
        }
    ],
    "cost": 0.017922499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation
June 18, 2024</p>
<p>Chanathip Pornprasit chanathip.pornprasit@monash.edu 
Monash University
Australia</p>
<p>Chakkrit Tantithamthavorn chakkrit@monash.edu 
Monash University
Australia</p>
<p>Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation
June 18, 2024586211534679AD5DCE5D1AF188824299arXiv:2402.00905v4[cs.SE]Preprint submitted to ElsevierModern Code ReviewCode Review AutomationLarge Language ModelsGPT-3.5Few-Shot LearningPersona
Context:The rapid evolution of Large Language Models (LLMs) has sparked significant interest in leveraging their capabilities for automating code review processes.Prior studies often focus on developing LLMs for code review automation, yet require expensive resources, which is infeasible for organizations with limited budgets and resources.Thus, fine-tuning and prompt engineering are the two common approaches to leveraging LLMs for code review automation.Objective: We aim to investigate the performance of LLMs-based code review automation based on two contexts, i.e., when LLMs are leveraged by fine-tuning and prompting.Fine-tuning involves training the model on a specific code review dataset, while prompting involves providing explicit instructions to guide the model's generation process without requiring a specific code review dataset.Method: We leverage model fine-tuning and inference techniques (i.e., zero-shot learning, few-shot learning and persona) on LLMs-based code review automation.In total, we investigate 12 variations of two LLMs-based code review automation (i.e., GPT-3.5 and Magicoder), and compare them with the Guo et al.'s approach and three existing code review automation approaches (i.e., CodeReviewer, TufanoT5 and D-ACT).Results: The fine-tuning of GPT 3.5 with zero-shot learning helps GPT-3.5 to achieve 73.17% -74.23% higher EM than the Guo et al.'s approach.In addition, when GPT-3.5 is not fine-tuned, GPT-3.5 with few-shot learning achieves 46.38% -659.09% higher EM than GPT-3.5 with zero-shot learning.Conclusions: Based on our results, we recommend that (1) LLMs for code review automation should be fine-tuned to achieve the highest performance.;and (2) when data is not sufficient for model fine-tuning (e.g., a cold-start problem), few-shot learning without a persona should be used for LLMs for code review automation.Our findings contribute valuable insights into the practical recommendations and trade-offs associated with deploying LLMs for code review automation.</p>
<p>Introduction</p>
<p>Code review is a software quality assurance practice where developers other than an author (aka.reviewers) review a code change that the author creates to ensure the quality of the code change before being integrated into a codebase.While code review can ensure high software quality, code review is still time-consuming and expensive.Thus, neural machine translation (NMT)-based code review automation approaches were proposed [1,2,3] to facilitate and expedite the code review process.However, prior studies [4,5] found that such approaches are still not perfect due to limited knowledge of the NMT-based code review automation models that are trained on a small code review dataset.</p>
<p>To address the aforementioned challenge of the NMT-based code review automation approaches, recent work proposed large language model (LLM)-based approaches for the code review automation task [5,6].A large language model is a large Since pre-training LLMs for code review automation can be expensive, fine-tuning and prompt engineering are the two common approaches to leveraging LLMs for code review automation.In particular, fine-tuning involves further training LLMs that are already pre-trained on a specific code review dataset.For example, Lu et al. [9] proposed LLaMa-Reviewer, which is the LLM-based code review automation approach that is being fine-tuned on a base LLaMa model [10].On the other hand, prompting [11,12,13] involves providing explicit instructions to guide the model's generation process without requiring a spe-cific code review dataset.For instance, Guo et al. [14] conducted an empirical study to investigate the potential of GPT-3.5 for code review automation by using zero-shot learning with GPT-3.5.</p>
<p>While Guo et al. [14] demonstrate the potential of using GPT-3.5 for code review automation, their study still has the following limitations.First, the results of Guo et al. [14] are limited to zero-shot GPT-3.5.However, there are other approaches to leverage GPT-3.5 (i.e., fine-tuning and few-shot learning) that are not included in their study.Thus, it is difficult for practitioners to conclude which approach is the best for leveraging LLMs for code review automation.Second, although prior studies [15,16,17] found that model fine-tuning can improve the performance of pre-trained LLMs, Guo et al. [14] did not evaluate the performance of LLMs when being fine-tuned.Thus, it is difficult for practitioners to conclude whether LLMs for code review automation should be fine-tuned to achieve the most effective results.Third, Guo et al. [14] did not investigate the impact of few-shot learning, which can improve the performance of LLMs over zero-shot learning [18,19,20].Hence, it is difficult for practitioners to conclude which prompting strategy (i.e., zero-shot learning, few-shot learning, and a persona) is the most effective for code review automation.</p>
<p>In this work, we aim to investigate the performance of LLMsbased code review automation based on two contexts, i.e., when LLMs are leveraged by fine-tuning and prompting.In particular, we evaluate two LLMs (i.e., GPT-3.5 and Magicoder [21]) and the existing LLM-based code review automation approaches [4,5,6] with respect to the following evaluation measures: Exact Match (EM) [1,4] and CodeBLEU [22].Through the experimental study of the three code review automation datasets (i.e., CodeReviewer data [5], Tufano data [6], and D-ACT data [4]), we answer the following three research questions:</p>
<p>(RQ1) What is the most effective approach to leverage LLMs for code review automation?</p>
<p>Result.The fine-tuning of GPT 3.5 with zero-shot learning achieves 73.17% -74.23% higher EM than the Guo et al.'s approach [14] (i.e., GPT 3-5 without fine-tuning).The results imply that GPT-3.5 should be fine-tuned to achieve the highest performance.</p>
<p>(RQ2) What is the benefit of model fine-tuning on GPT-3.5 for code review automation?</p>
<p>Result.The fine-tuning of GPT 3.5 with few-shot learning achieves 63.91% -1,100% higher Exact Match than those that are not fine-tuned.The results indicate that fine-tuned GPT-3.5 can generate more correct revised code than GPT-3.5 without fine-tuning.</p>
<p>(RQ3) What is the most effective prompting strategy on GPT-3.5 for code review automation?</p>
<p>Result.GPT-3.5 with few-shot learning achieves 46.38% -659.09% higher Exact Match than GPT-3.5 with zero-shot learning.On the other hand, when a persona is included in input prompts, GPT-3.5 achieves 1.02% -54.17% lower Exact Match than when the persona is not included in input prompts.The results indicate that the best prompting strategy when using  GPT-3.5 without fine-tuning is using few-shot learning without a persona.</p>
<p>Recommendation.Based on our results, we recommend that (1) LLMs for code review automation should be fine-tuned to achieve the highest performance; and (2) when data is not sufficient for model fine-tuning (e.g., a cold-start problem), few-shot learning without a persona should be used for LLMs for code review automation.</p>
<p>Contributions.In summary, the main contributions of our work are as follows:</p>
<p>• We are the first to investigate the performance of LLMsbased code review automation when using model finetuning and inference techniques (i.e., zero-shot learning, few-shot learning and persona).</p>
<p>• We provide recommendations for adopting LLMs for code review automation to practitioners.</p>
<p>Open Science.Our fine-tuned models, script and results are made available online [23].</p>
<p>Paper Organization.Section 2 describes the related work and formulates research questions.Section 3 describes the study design of our study.Section 4 presents the experiment results.Section 5 discusses our experiment results.Section 6 describes possible threats to the validity.Section 7 draws the conclusions of our work.</p>
<p>Related Work and Research Questions</p>
<p>In this section, we provide the background knowledge of code review automation, discuss the existing large-language model-based code review automation approaches, and formulate the research questions.</p>
<p>Code Review Automation</p>
<p>Code review is a software quality assurance practice where developers other than an author (aka.reviewers) provide feedback for a code change created by the author to ensure that the code change has sufficient quality to meet quality standards.While code review can ensure high software quality, code review is still time-consuming and expensive.Thus, developers still face challenges in receiving timely feedback from reviewers [24,25].Therefore, code review automation approaches [1,4,5,6,26] were proposed to facilitate and expedite the code review process.</p>
<p>Code review automation is generally formulated as a sequence generation task, where a language model is trained to learn the relationship between the submitted code and the revised code.Then, during the inference phase, the model aims to generate a revised version of a code change.Recently, neural machine translation (NMT)-based code review automation approaches were proposed [1,2,3].Typically, NMT-based code review automation approaches are trained on a specific code review dataset.However, prior studies [4,5] found that NMTbased code review automation approaches can perform well, but is still not perfect.This imperfect performance has to do with the limited knowledge of the NMT-based code review automation approaches that are being trained on a small code review dataset.</p>
<p>LLMs-based Code Review Automation Approaches</p>
<p>Large language models (LLMs) for code review automation refer to large language models specifically designed to support code review automation tasks, aiming to understand and generate source code written by developers and natural languages written by reviewers.Since source code and comments often have their own semantic and syntactical structures, recent work proposed various LLMs-based code review automation approaches [5,6,9].For example, Li et al. [5] proposed CodeReviewer, a pre-trained LLM that is based on the CodeT5 model [8].Prior studies found that LLMs-based code review automation approaches often outperform NMT-based ones [4,5,6].For example, Li et al. [5] found that their proposed approach outperforms a transformer-based NMT model by 11.76%.Below, we briefly discuss the general modelling pipeline of LLMs for code review automation presented in Figure 1.</p>
<p>Model Pre-Training refers to the initial phase of training a large language model, where the model is exposed to a large amount of unlabeled data to learn general language representations.This phase aims to initialize the model's parameters and learn generic features that can be further fine-tuned for specific downstream tasks.Recently, there have been many large language models for code (i.e., LLMs that are specifically trained on source code and related natural languages).For example, the open-source community-developed large language models such as Code-LLaMa [27], StarCoder [28], and Magicoder [21]; and the commercial large language models such as GPT-3.5.</p>
<p>However, the development of large language models for code requires expensive GPU resources and budget.For example, GPT-3.5 requires 10,000 NVIDIA V-100 GPUs for model pretraining. 1LLaMa2 [29] requires Meta's Research Super Cluster (RSC) as well as internal production clusters, which consists of 1 https://gaming.lenovo.com/emea/threads/17314-The-hardware-behind-ChatGPTapproximately 2,000 NVIDIA A-100 GPUs in total.Therefore, many software organizations with limited resources and budgets may not be able to develop their large language models.Thus, fine-tuning and prompt engineering are the two common approaches to leverage the existing LLMs for code review automation when expensive GPU resources are not available for pre-training a large language model from scratch, where these techniques are more desirable for many organizations to quickly adopt new technologies.</p>
<p>Model Fine-Tuning is a common practice, particularly in transfer learning scenarios, where a model pre-trained on a large dataset (source domain, e.g., source code understanding) is adapted to a related but different task or dataset (target domain, e.g., code review automation).Recently, researchers have leveraged model fine-tuning techniques for LLMs to improve the performance of code review automation approaches.For example, Lu et al. [9] proposed LLaMa-Reviewer, which is an LLM-based code review automation approach that is being finetuned on a base LLaMa model [10] using three code review automation tasks, i.e., a review necessity prediction task to check if diff hunks need a review, a code review comment generation task to generates pertinent comments for a given code snippet, and a code refinement task to generate minor adjustment to the existing code.Lu et al. [9] found that the fine-tuning step on LLMs can greatly improve the performance of the existing code review automation approaches.</p>
<p>Inference refers to the process of using a pre-trained language model to generate source code based on a given natural language prompt instruction.Therefore, prompt engineering plays an important role in leveraging LLMs for code review automation to guide LLMs to generate the desired output.Different prompting strategies have been proposed. 2For example, zero-shot learning, few-shot learning [18,30,31], chain-ofthought [32,33], tree-of-thought [32,33], self-consistency [34], and persona [13].Nevertheless, not all prompting strategies are relevant to code review automation.For example, chainof-thought, self-consistency and tree-of-thought promptings are not applicable to the code review automation task since they are designed for arithmetic and logical reasoning problems.Thus, we exclude them from our study.</p>
<p>In contrast, zero-shot learning, few-shot learning, and persona prompting are the instruction-based prompting strategies, which are more suitable for software engineering (including code review automation) tasks [35,36,37,38].In particular, zero-shot learning involves prompting LLMs to generate an output from a given instruction and an input.On the other hand, few-shot learning [18,30,31] involves prompting LLMs to generate an output from N demonstration examples {(x 1 , y 1 ), (x 2 , y 2 ), ..., (x N , y N )} and an actual input in a testing set, where x i and y i are the inputs and outputs obtained from a training set, respectively.Persona [13] involves prompting LLMs to act as a specific role or persona to ensure that LLMs will generate output that is similar to the output generated by a specified persona.Recently, Guo et al. [14] conducted an empirical study to investigate the potential of GPT-3.5 for code review automation.However, their study still has the following limitations.</p>
<p>First, the results of Guo et al. [14] are limited to zero-shot GPT-3.5.In particular, Guo et al. [14] conducted experiments to find the best prompt for leveraging zero-shot learning with GPT-3.5.However, there are other approaches to leverage GPT-3.5 (i.e., fine-tuning and few-shot learning) that are not included in their study.The lack of a systematic evaluation of the use of fine-tuning and few-shot learning on GPT-3.5 makes it difficult for practitioners to conclude which approach is the best for leveraging LLMs for code review automation.To address this challenge, we formulate the following research question.RQ1: What is the most effective approach to leverage LLMs for code review automation?</p>
<p>Second, the performance of LLMs when being fine-tuned is still unknown.In particular, Guo et al. [14] did not evaluate the performance of LLMs when being fine-tuned.However, prior studies [15,16,17] found that model fine-tuning can improve the performance of pre-trained LLMs.The lack of experiments with model fine-tuning makes it difficult for practitioners to conclude whether LLMs for code review automation should be fine-tuned to achieve the most effective results.To address this challenge, we formulate the following research question.RQ2: What is the benefit of model fine-tuning on GPT-3.5 for code review automation?</p>
<p>Third, the performance of LLMs for code review automation when using few-shot learning is still unknown.In particular, Guo et al. [14] did not investigate the impact of fewshot learning on LLMs for code review automation.However, recent work [18,19,20] found that few-shot learning could improve the performance of LLMs over zero-shot learning.The lack of experiments with few-shot learning on LLMs for code review automation makes it difficult for practitioners to conclude which prompting strategy (i.e., zero-shot learning, fewshot learning, and persona) is the most effective for code review automation.To address this challenge, we formulate the following research question.RQ3: What is the most effective prompting strategy on GPT-3.5 for code review automation?Table 2: Experimental settings in our study.We do not include experimental settings #3 and #4 since LLMs already learn the relationship between input (i.e., code submitted for review) and output (i.e., revised code).</p>
<p>Experimental setting Fine-Tuning Inference Technique Prompting Use
Persona #1 ✓ Zero-shot ✗ #2 ✓ #3 Few-shot ✗ #4 ✓ #5 ✗ Zero-shot ✗ #6 ✓ #7
Few-shot ✗ #8 ✓</p>
<p>Experimental Design</p>
<p>In this section, we provide an overview and details of our experimental design.</p>
<p>Overview</p>
<p>The goal of this work is to investigate which LLMs perform best when using model fine-tuning and inference techniques (i.e., zero-shot learning, few-shot learning [18,30,31], and persona [13]).To achieve this goal, we conduct experiments with two LLMs (i.e., GPT-3.5 and Magicoder [21]) on the following datasets that are widely studied in the code review automation literature [4,9,14,39]: CodeReviewer data [5], Tufano data [6] and D-ACT data [4].We use Magicoder [21] in our experiment since it is further trained on high-quality synthetic instructions and solutions.</p>
<p>In this study, we conduct experiments under six settings as presented in Table 2.According to the table, when the LLMs are fine-tuned, we use zero-shot learning with and without a persona.We do not use few-shot learning with the fine-tuned LLMs since the LLMs already learn the relationship between an input (i.e., code submitted for review) and an output (i.e., improved code).On the other hand, when the LLMs are not finetuned, we use zero-shot learning and few-shot learning, where each inference technique is used with and without a persona.Finally, we conduct 36 experiments in total (2 LLMs × 6 settings × 3 datasets).</p>
<p>Figure 2 provides an overview of our experimental design.To begin, the studied code review datasets are split into training and testing sets.The training set consists of the code submitted for review and reviewers' comments as input; and revised code as output.On the other hand, the testing set consists of only code submitted for review and reviewers' com  Then, we create prompts that look similar to the prompt templates.Finally, we use the studied LLMs to generate revised code from given prompts.We explain the details of the studied datasets, model fine-tuning, inference via prompting, evaluation measures, and hyper-parameter settings below.</p>
<p>The Studied Datasets</p>
<p>Recently, Tufano et al. [1,2] collected datasets with the constraint that revised code must not contain the code tokens (e.g., identifiers) that do not appear in code submitted for review.Thus, such datasets do not align with the real code review practice since developers may add new code tokens when they revise their submitted code.Therefore, in this study, we use the CodeReviewer [5], TufanoT5 [6], and D-ACT [4] datasets, which do not have the above constraint in data collection instead.The details of the studied datasets are as follows (the statistic of the studied datasets is presented in Table 3).</p>
<p>• CodeReviewer data : Li et al. [5] collected this dataset from the GitHub projects across nine programming languages (i.e., C, C++, C#, Java, Python, Ruby, php, Go, and Javascript).The dataset contains triplets of the code submitted for review (diff hunk granularity), a reviewer's comment, and the revised version of the code submitted for review (diff hunk granularity).</p>
<p>• Tufano data : Tufano et al. [6] collected this dataset from Java projects in GitHub, and 6,388 Java projects hosted in Gerrit.Each record in the dataset contains a triplet of code submitted for review (function granularity), a reviewer's comment, and code after being revised (function granularity).Tufano et al. [6] created two types of this dataset (i.e., Tufano data (with comment) and Tufano data (without comment)).</p>
<p>• D-ACT data : Pornprasit et al. [4] collected this dataset from the three Java projects hosted on Gerrit (i.e., Android, Google and Ovirt).Each record in the dataset contains a triplet of codebase (function granularity), code of the first version of a patch (function granularity), and code of the approved version of a patch (function granularity).</p>
<p>Model Fine-Tuning</p>
<p>To fine-tune the studied LLMs, as suggested by OpenAI, we first select a few training examples to fine-tune an LLM to see if the performance improves.Thus, we randomly select a set of examples from the whole training set by using the random function in Python to reduce bias in the data selection.However, there is no existing rule or principle to determine the number of examples that should be selected from a training set.Thus, we use the trial-and-error approach to determine the suitable number of training examples.To do so, we start by using approximately 6% training examples from the whole training set to fine-tune GPT-3.5.We find that GPT-3.5 that is fine-tuned with such training examples outperforms the existing code review automation approaches [4,5,6].Therefore, based on the above finding, we use 6% training examples for the whole experiment.</p>
<p>After that, the selected training examples is used to finetune the studied LLMs.In particular, we fine-tune GPT-3.5 by using the API provided by OpenAI5 .On the other hand, to fine-tune Magicoder [21], we leverage the state-of-the-art parameter-efficient fine-tuning technique called DoRA [40]. Figure 3: Prompt templates for zero-shot learning and few-shot learning that contain simple instructions (lang refers to a programming language).The text in blue is omitted when reviewers' comments are not used in experiments.</p>
<p>Inference via Prompting</p>
<p>In this work, we conduct experiments with the following prompting techniques: zero-shot learning, few-shot learning and a persona.We explain each prompting technique below.</p>
<p>For zero-shot learning, we first design the prompt template as presented in Figure 3a by following the guidelines from Ope-nAI 3,4 to ensure that the structure of the prompt is suitable for GPT-3.5.The prompt template consists of the following components: an instruction and an input (i.e., code submitted for review and a reviewer's comment).</p>
<p>Then, we create prompts by using the prompt template in Figure 3a and the code submitted for review with a reviewer's comment in a testing set.Finally, we use the LLMs to generate revised code from the created prompts.</p>
<p>For few-shot learning [18,30,31], we first design the prompt template as presented in Figure 3b.Similar to zero-shot learning, we follow the guidelines from OpenAI when designing the prompt template.The prompt template consists of the following components: demonstration examples, an instruction and an input (i.e., code submitted for review and a reviewer's comment).</p>
<p>In few-shot learning, demonstration examples are required to create a prompt.Thus, we select three demonstration examples, where each example consists of two inputs (i.e., code submitted for review and a reviewer's comment) and an output (i.e., revised code), by using BM25 [41].We use BM25 [41] since prior work [12,42] shows that BM25 [41] outperforms other sample selection approaches for software engineering tasks.In this work, we use BM25 [41] provided by the gensim6 package.We select three demonstration examples for each testing sample since Gao et al. [11] showed that GPT-3.5 using three demonstration examples achieves comparable performance (i.e, 90% of the highest Exact Match) when compared to GPT-3.5 that achieves the highest performance by using 16 or more demonstration examples.Then, we create prompts from the prompt template in Figure 3b; the code submitted for review and a reviewer's comment in the testing set; and the demonstration examples of the code submitted for review.Finally, we use LLMs to generate revised code from the prompts.</p>
<p>For persona [13], we include a persona in the prompt templates in Figure 3 to instruct GPT-3.5 to act as a software developer.We do so to ensure that the revised code generated by GPT-3.5 looks like the source code written by a software developer.</p>
<p>The Evaluation Measures</p>
<p>We use the following measures to evaluate the performance of the studied LLMs (i.e., GPT-3.5 and Magicoder [21]) and code review automation approaches (i.e., CodeReviewer [5], TufanoT5 [6], and D-ACT [4]).</p>
<ol>
<li>Exact Match (EM) [4,5,6] is the number of the generated revised code that is the same as the actual revised code in the testing dataset.We use this measure since it is widely used for evaluating code review automation approaches [1,4,6].To compare the generated revised code with the actual revised code, we first tokenize both revised code to sequences of tokens.Then, we compared the sequence of tokens of the generated revised code with the sequence of tokens of the actual revised code.A high value of EM indicates that a model can generate revised code that is the same as the actual revised code in the testing dataset.2. CodeBLEU [22] is the extended version of BLEU (i.e., an n-gram overlap between the translation generated by a deep learning model and the translation in ground truth) [43] for automatic evaluation of the generated code.</li>
</ol>
<p>We do not measure BLEU like in prior work [5,6] since Ren et al. [22] found that this measure ignores syntactic and semantic correctness of the generated code.In addition to BLEU, CodeBLEU considers the weighted n-gram match, matched syntactic information (i.e., abstract syntax tree: AST) and matched semantic information (i.e., data flow: DF) when computing the similarity between the generated revised code and the actual revised code.A high value of CodeBLEU indicates that a model can generate revised code that is syntactically and semantically similar to the actual revised code in the testing dataset.</p>
<p>The Hyper-Parameter Settings</p>
<p>In this study, we use the following hyper-parameter settings when using GPT-3.5 to generate revised code: temperature of 0.0 (as suggested by Guo et al. [14]), top p of 1.0 (default value), and max length of 512.To fine-tune GPT-3.5, we use hyper-parameters (e.g., number of epochs and learning rate) that are automatically provided by OpenAI API.</p>
<p>For Magicoder [21], we use the same hyper-parameter as GPT-3.5 to generate revised code.To fine-tune Magicoder, we use the following hyper-parameters for DoRA [40]: attention dimension (r) of 16, alpha (α) of 8, and dropout of 0.1 .</p>
<p>Model</p>
<h1>parameters GPT-3.5 175 B Magicoder [21] 6.7 B TufanoT5 [6] 60.5 M CodeReviewer [5] 222.8 M D-ACT [4] 222.8 M</h1>
<p>Result</p>
<p>In this section, we present the results of the following three research questions.(RQ1) What is the most effective approach to leverage LLMs for code review automation?Approach.To address this RQ, we leverage fine-tuning and inference techniques (i.e., zero-shot learning, few-shot learning, and persona) on GPT-3.5 and Magicoder (The details of GPT-3.5 and Magicoder are presented in Table 5) .Then, we measure EM of the results obtained from GPT-3.5, Magicoder and Guo et al.'s approach [14].</p>
<p>Result.The fine-tuning of GPT 3.5 with zero-shot learning helps GPT-3.5 to achieve 73.17% -74.23% higher EM than the Guo et al. [14]'s approach.Table 4 shows the results of EM achieved by GPT-3.5, Magicoder and Guo et al.'s approach [14].The table shows that when GPT-3.5 and Magicoder are fine-tuned, such models achieve 73.17% -74.23% and 26.00% -28.53% higher EM than the Guo et al.'s approach [14], respectively.</p>
<p>The results indicate that model fine-tuning could help GPT-3.5 to achieve higher EM when compared to the Guo et al.'s approach [14].The higher EM has to do with model finetuning.When GPT-3.5 or Magicoder is fine-tuned, such models learn the relationship between inputs (i.e., code submitted for review and a reviewer's comment) and an output (i.e., revised code) from a number of examples in a training set.On the contrary, Guo et al.'s approach [14] only relies on the instruction and given input to generate revised code, which GPT-3.5 never learned during model pre-training.(RQ2) What is the benefit of model fine-tuning on GPT-3.5 for code review automation?Approach.To address this RQ, we fine-tune GPT-3.5 as explained in Section 3.Then, we measure EM and CodeBLEU of the results obtained from the fine-tuned GPT-3.5 and the non fine-tuned GPT-3.5 with zero-shot learning.Result.The fine-tuning of GPT 3.5 with zero-shot learning helps GPT-3.5 to achieve 63.91% -1,100% higher EM than those that are not fine-tuned.Table 4 shows that in terms of EM, the fine-tuning of GPT 3.5 with zero-shot learning helps GPT-3.5 to achieve 63.91% -1,100% higher than those that are not fine-tuned.In terms of CodeBLEU, the fine-tuning of GPT 3.5 with zero-shot learning helps GPT-3.5 to achieve 5.91% -63.9% higher than those that are not fine-tuned.</p>
<p>The results indicate that fine-tuned GPT-3.5 achieve higher EM and CodeBLEU than those that are not fine-tuned.During the model fine-tuning process, GPT-3.5 adapt to the code review automation task by directly learning the relationship between inputs (i.e., code submitted for review and a reviewer's executor.addArgs("--args","--sample_name=" + sample_name, "--targets_file=" + tnFile, "--pre_tn_file=" + preTnFile, "--seg_file=" + segFile, "--output_dir=" + outputDir, "--log2_input=" + logArg); executor.exec();} (a) The difference between code submitted for review and revised code that GPT-3.5 with zero-shot learning (no persona) correctly generates.</p>
<ul>
<li>executor.addArgs("--args","--sample_name=" + sampleName, "--targets_file=" + tnFile, "--pre_tn_file=" + preTnFile, "--seg_file=" + segFile, "--output_dir=" + outputDir, "--log2_input=" + logArg); + executor.addArgs("--args","--sample_name=" + sample_name, "--targets_file=" + tnFile, "--pre_tn_file=" + preTnFile, "--seg_file=" + segFile, "--output_dir=" + outputDir, "--log2_input=" + logArg); executor.exec();</li>
</ul>
<p>(b) The difference between code submitted for review and revised code that GPT-3.5 with zero-shot learning (use persona) incorrectly generates.(c) The difference between code submitted for review and revised code that GPT-3.5 with few-shot learning (no persona) correctly generates.
+ } + } }) + } (d)
The difference between code submitted for review and revised code that GPT-3.5 with few-shot learning (use persona) incorrectly generates.comment) and an output (i.e., revised code) from a number of examples in a training set.In contrast, non fine-tuned GPT-3.5 is given only an instruction and inputs which are not presented during model pre-training.Therefore, fine-tuned GPT-3.5 can better adapt to the code review automation task than those that are not fine-tuned.</p>
<p>(RQ3) What is the most effective prompting strategy on GPT-3.5 for code review automation?Approach.To address this RQ, we use zero-shot learning and few-shot learning with non fine-tuned GPT-3.5, where each inference technique is used with and without a persona, to generate revised code as explained in Section 3.Then, similar to RQ2, we measure EM and CodeBLEU of the results obtained from GPT-3.5.</p>
<p>Result.GPT-3.5 with few-shot learning achieves 46.38% -659.09% higher EM than GPT-3.5 with zero-shot learning.</p>
<p>Table 4 shows that in terms of EM, the use of few-shot learning on GPT-3.5 helps GPT-3.5 to achieve 46.38% -241.98% and 53.95% -659.09% higher than the use of zero-shot learning on GPT-3.5 without a persona and with a persona, respectively.In terms of CodeBLEU, the use of few-shot learning on GPT-3.5 helps GPT-3.5 to achieve 3.97% -33.36% and 5.55% -60.27% higher than the use of zero-shot learning on GPT-3.5 without a persona and with a persona, respectively.</p>
<p>The results indicate that GPT-3.5 with few-shot learning can achieve higher EM and CodeBLEU than GPT-3.5 with zeroshot learning.When few-shot learning is used to generate revised code from GPT-3.5, GPT-3.5 has more information from given demonstration examples in a prompt to guide the generation of revised code from given code submitted for review and a reviewer's comment.In other words, such demonstration examples could help GPT-3.5 to correctly generate revised code from a given code submitted for review and a reviewer's comment.</p>
<p>When a persona is included in input prompts, GPT-3.5 achieves 1.02% -54.17% lower EM than when the persona is not included in input prompts.Table 4 also shows that when a persona is included in prompts, GPT-3.5 with zeroshot and few-shot learning achieves 3.67% -54.17% and 1.02% -30.77% lower EM compared to when a persona is not included in prompts, respectively.Similarly, when a persona is included in prompts, GPT-3.5 with zero-shot and few-shot learning achieves 1.33% -19.13% and 0.15% -0.90% lower Code-BLEU compared to when a persona is not included in prompts, respectively.</p>
<p>The results indicate that when a persona is included in prompts, GPT-3.5 with zero-shot and few-shot learning achieves lower EM and CodeBLEU.To illustrate the impact of the persona, we present an example of the revised code that Figure 5: Example of the code changes of each type GPT-3.5 generates when zero-shot learning with and without a persona is used, and an example of the revised code that GPT-3.5 generates when few-shot learning with and without a persona is used in Figure 4. Figure 4a presents the revised code that GPT-3.5 with zeroshot learning (no persona) correctly generates.In this figure, GPT-3.5 suggests an alternative way to initialize variable log-Arg.In contrast, Figure 4b presents the revised code that GPT-3.5 with zero-shot learning (use persona) incorrectly generates.In this figure, GPT-3.5 suggests changing the variable type from Boolean to boolean and changing the variable name from sample name to sampleName in addition to suggesting an alternative way to initialize variable logArg.Figure 4c presents another example of the revised code that GPT-3.5 with few-shot learning (no persona) correctly generates.In this figure, GPT-3.5 suggests a new if condition.On the contrary, Figure 4d presents the revised code that GPT-3.5 with few-shot learning (use persona) incorrectly generates.In this figure, GPT-3.5 suggests an additional if statement and an additional else block.</p>
<p>The above examples imply that when a persona is included in prompts, GPT-3.5 tends to suggest additional incorrect changes to the submitted code compared to when a persona is not included in prompts.</p>
<p>The above results indicate that the best prompting strategy when using GPT-3.5 without fine-tuning is few-shot learning without a persona.</p>
<p>Discussion</p>
<p>In this section, we discuss the implications of our findings, the additional results of GPT-3.5, and the cost and benefits of using GPT-3.5.</p>
<p>Implications of Our Findings</p>
<p>GPT-3.5 does not require a lot of training data for model fine-tuning to adapt to the code review automation task since Table 4 shows that GPT-3.5 that is fine-tuned on a subset of a training set outperforms the studied code review automation approaches [4,5,6].The results imply that GPT-3.5 can adapt to the code review automation task by learning from a small set of training examples (approximately 20k training examples in this study), unlike the studied code review automation approaches that require the whole training set to adapt to the code review automation task.</p>
<p>Recommendations to practitioners.LLMs for code review automation should be fine-tuned to achieve the highest performance.The reason for this recommendation is the results of RQ2 show that fine-tuned GPT-3.5 outperforms those that are not fine-tuned.In contrast, when data is not sufficient for model fine-tuning (e.g., a cold-start problem), few-shot learning without a persona should be used for LLMs for code review automation.The reason for this recommendation is the results of RQ3 show that GPT-3.5 with few-shot learning outperforms GPT-3.5 with zero-shot learning, and GPT-3.5 without a persona outperforms GPT-3.5 with persona.</p>
<p>The Characteristics of the Revised Code that are Correctly</p>
<p>Generated by GPT-3.5</p>
<p>The results of RQ2 and RQ3 demonstrate the benefits of model fine-tuning and few-shot learning on GPT-3.5 for the code review automation task, respectively.However, practitioners still do not clearly understand the characteristics of the code changes of the revised code that GPT-3.5 correctly generates.To address this challenge, we aim to qualitatively investigate the revised code that GPT-3.5 can correctly generate.To do so, The results of GPT-3.5 (persona is not included in prompts)</p>
<p>Figure 6: The EM achieved by GPT-3.5 Zero-shot , GPT-3.5 Few-shot and GPT-3.5 Fine-tuned categorized by the types of code change.Here, GPT-3.5 Zero-shot and GPT-3.5 Few-shot refer to non fine-tuned GPT-3.5 with zero-shot learning and few-shot learning, respectively.On the other hand, GPT-3.5 Fine-tuned refers to fine-tuned GPT-3.5 with zero-shot learning.</p>
<p>we randomly select the revised code that is only correctly generated by a particular model (e.g., we randomly obtain the revised code that only fine-tuned GPT-3.5 correctly generates while the others do not.)by using a confidence level of 95% and a confidence interval of 5%.Then, we classify the code changes of the selected revised code into the following categories based on the taxonomy of code change created by Tufano et al. [1] (the examples of the code changes in each category are depicted in Figure 5):</p>
<p>• fixing bug: The code changes in this category involve fixing bugs in the past.The following sub-categories are related to this category: exception handling, conditional statement, lock mechanism, method return value, and method invocation.</p>
<p>• refactoring: The code changes in this category involve making changes to code structure without changing the behavior of the changed code.The following sub-categories are related to this category: inheritance; encapsulation; methods interaction; readability; and renaming parameter, method and variable.</p>
<p>• other: The code changes that cannot be classified as neither fixing bug nor refactoring will fall into this category.</p>
<p>Figure 6 presents the characteristics of the code changes (i.e., Fixing Bug, Refactoring and Other) of the revised code that GPT-3.5 correctly generates.According to the figure, for the Tufano data (with comment), CodeReviewer data and D-ACT data dataset, the fine-tuning of GPT 3.5 with zero-shot learning helps GPT-3.5 achieves the highest EM for the code changes of Refactoring and Other.In contrast, for the Tufano data (without comment) dataset, non fine-tuned GPT-3.5 with few-shot learning achieves the highest EM for the code changes of all categories.According to Figure 6a and Figure 6b, we also find that fine-tuned GPT-3.5 and non fine-tuned GPT-3.5 that zeroshot and few-shot learning are used achieve the highest EM for the code changes of type other across all studied datasets.The reason for this result is that we do not specify the characteristics of the revised code (i.e., fixing bug and refactoring) in prompts.Thus, such models possibly generate revised code that is not specific to fixing bug or refactoring.Therefore, the majority of the code changes of the generated revised code are categorized as other.</p>
<p>The Impact of the Size of Training Dataset on Fine-Tuned</p>
<p>GPT-3.5</p>
<p>The results of RQ2 show that model fine-tuning can help increase the performance of GPT-3.5.However, little is known whether fine-tuned GPT-3.5 can achieve higher performance when being fine-tuned with larger training sets.Therefore, we conduct experiments by using 10% and 20% of training sets to fine-tune GPT-3.5 (we do not use persona in these experiments).</p>
<p>Table 6 shows the results of EM and CodeBLEU that finetuned GPT-3.5 achieves across different sizes of training sets.The table shows that GPT-3.5 that is fine-tuned with 20% of a training set achieves 2.29% -12.07%higher EM and 0.54% -2.55% higher CodeBLEU than GPT-3.5 that is fine-tuned with 6% of a training set.In addition, GPT-3.5 that is fine-tuned with 20% of a training set achieves 2.38% -25.75% higher EM and 0.01% -1.25% higher CodeBLEU than GPT-3.5 that is fine- tuned with 10% of a training set, respectively.The results indicate that fine-tuned GPT-3.5 achieves higher performance when being fine-tuned with a larger training set.</p>
<p>(Instruction) Follow the steps below to improve the given submitted code step 1 -read the given submitted code and a reviewer comment step 2 -identify lines that need to be modified, added or deleted step 3 -generate the improved code without your explanation.</p>
<p>(Input) <input code> (Input) <input comment> (a) A prompt template for zero-shot learning.</p>
<p>(Instruction and examples) You are given 3 examples.Each example begins with "##Example" and ends with "---".Each example contains the submitted code, the developer comment, and the improved code.The submitted code and improved code is written in <lang>.</p>
<p>Follow the steps below to improve the given submitted code step 1 -read the given submitted code and a reviewer comment in the above examples step 2 -identify lines that need to be modified, added or deleted in the examples step 3 -read the given submitted code and a reviewer comment step 4 -identify lines that need to be modified, added or deleted step 5 -generate the improved code without your explanation.Figure 7: Prompt templates for zero-shot learning and few-shot learning that instructions are broken into smaller tasks (lang refers to a programming language).The text in blue is omitted when reviewers' comments are not used in experiments.</p>
<p>The Impact of Prompt Design on GPT-3.5</p>
<p>In RQ3, we use the prompt templates in Figure 3 that contain simple instructions to conduct experiments.However, prior work [44,45] found that the design of prompts has an impact on the performance of LLMs.Thus, we further investigate the impact of prompt design on GPT-3.5 for code review automation.To do so, we conduct experiments by using the following two new prompt designs (we do not include a persona in these prompt designs since the results of RQ3 show that GPT-3.5 without a persona outperforms GPT-3.5 with a persona).First, we use the prompt design that a single instruction is broken into smaller steps 7 , as depicted in Figure 7. Second, we use (Instruction) A developer asks you to help him improve his submitted code based on the given reviewer comment.He emphasizes that the improved code must have higher quality, conforms to coding convention or standard, and works correctly.He tells you to refrain from putting the submitted code in a class or method, and providing global variables or an implementation of methods that appear in the submitted code.He asks you to recommend the improved code without your explanation.(Input) <input code> (Input) <input comment> (a) A prompt template for zero-shot learning.</p>
<p>(Instruction and examples) You are given 3 examples.Each example begins with "##Example" and ends with "---".Each example contains the submitted code, the developer comment, and the improved code.The submitted code and improved code is written in <lang>.</p>
<p>A developer asks you to help him improve his submitted code based on the given reviewer comment.He emphasizes that the improved code must have higher quality, conforms to coding convention or standard, and works correctly.He tells you to refrain from putting the submitted code in a class or method, and providing global variables or an implementation of methods that appear in the submitted code.He asks you to recommend the improved code without your explanation.the prompt design that contains more detailed instructions 8 , as depicted in Figure 8.</p>
<p>Table 7 shows the results of EM and CodeBLEU that GPT-3.5 achieves across different prompt designs.The table shows that for zero-shot learning, GPT-3.5 that is prompted by the prompt with a simple instruction achieves 16.44% -45.45% higher EM than GPT-3.5 that is prompted by the prompt with an instruction being broken down into smaller steps.In addition, GPT-3.5 that is prompted by the prompt with a simple instruction achieves 37.12% -880.00%higher EM than GPT-3.5 that is prompted by the prompt with a detailed instruction.</p>
<p>The table also shows that for few-shot learning, GPT-3.5 that is prompted by the prompt with a simple instruction achieves 5.15% -290.00%higher EM than GPT-3.5 that is prompted by the prompt with an instruction being broken down into smaller steps.Furthermore, GPT-3.5 that is prompted by the prompt with a simple instruction achieves 5.61% -515.79%higher EM than GPT-3.5 that is prompted by the prompt with detailed in- The evaluation results of GPT-3.5 for different prompt templates.P1 refers to the prompt templates with simple instructions (Figure 3).P2 refers to the prompt templates with instructions being broken down into smaller steps.(Figure7).P3 refers to the prompt templates with detailed instructions (Figure 8).</p>
<p>CodeReviewer</p>
<p>Tufano (with comment) struction.</p>
<p>The results indicate that GPT-3.5 that is prompted by the prompts with a simple instruction achieves the highest EM when compared to other prompt designs.Thus, the results imply that the prompt with a simple instruction is the most suitable for GPT-3.5 for code review automation.</p>
<p>Cost and Benefits of Using GPT-3.5 for Code Review Automation</p>
<p>Cost is one of the factors that determine the choices of AI services for practitioners.In the case of GPT-3.5 provided by OpenAI, the cost of using GPT-3.5 varies depending on usage 9 .In particular, the cost of using zero-shot learning and few-shot learning with GPT-3.5 is approximately 0.002 USD per query (for 1k input tokens and 1k output tokens) and 0.0035 USD per query (for 4k input tokens and 1k output tokens), respectively.On the other hand, the cost for fine-tuning one GPT-3.5 is approximately 40 USD (we use approximately 8k examples from a training set), and the cost for using fine-tuned GPT-3.5 is approximately 0.009 USD per query (for 1k input tokens and 1k output tokens).</p>
<p>Assume that a software developer uses GPT-3.5 to help him/her review code submitted for review 1,000 times per day on average and he/she works for 25 days per month, the total GPT-3.5 usage is 1,000 × 25 × 12 = 300,000 times per year.Such GPT-3.5 usage accounts for $600 per year (300,000 × 0.002) when using zero-shot learning with GPT-3.5, $1,050 per year (300,000 × 0.0035) when using few-shot learning with GPT-3.5, and $2,740 per year (40 + (300,000 × 0.009)) when using fine-tuned GPT-3.5.However, when compared to the average yearly salary of software engineers around the world 10 , the usage cost of GPT-3.5 is approximately 62.23% -91.73% and 97.51% -99.46% less than the lowest average salary (i.e., $7,255 in Nigeria) and the highest average salary (i.e., $110,140 in the United States), respectively.Nevertheless, the results of RQ1 show that fine-tuned GPT-3.5 achieves the highest EM.In addition, the results of RQ3 show that the use of few-shot learning without persona on GPT-3.5 helps GPT-3.5 achieve the highest EM and CodeBLEU.Thus, we recommend that GPT-3.5 for code review automation should be fine-tuned.Otherwise, leveraging GPT-3.5 by using few-shot learning without persona can be considered as an alternative. 9https://openai.com/pricing 10https://codesubmit.io/blog/software-engineer-salary-by-country/</p>
<p>Threats to Validity</p>
<p>We describe the threats to the validity of our study below.</p>
<p>Threats to Construct Validity</p>
<p>Threats to construct validity relate to the example selection techniques that we use to select examples for few-shot learning, and the design choices of the persona.We explain each threat below.</p>
<p>In this study, we only use the BM25 [41] technique to select three demonstration examples for few-shot learning.However, using more demonstration examples or different techniques to select demonstration examples may lead to results that differ from the reported results.Thus, more demonstration examples and other techniques for selecting demonstration examples can be explored in future work.</p>
<p>Since the code review automation task is related to revising the patches written by software developers, we use the persona (i.e., an expert software developer) to ensure that the revised code generated by GPT-3.5 looks like the source code written by a software developer.However, there are other similar personas (e.g., a senior software engineer, or a front-end software developer) that we do not explore.Thus, future work can explore other design choices of prompt and persona to find the optimal prompt and persona for code review automation tasks.</p>
<p>Threats to Internal Validity</p>
<p>Threats to internal validity relate to the randomness of GPT-3.5 and Magicoder, and the hyper-parameter settings that we use to fine-tune GPT-3.5 and Magicoder.The results that we obtain from GPT-3.5 and Magicoder may vary due to the randomness of GPT-3.5 and Magicoder.However, doing the same experiments multiple rounds can be expensive due to large testing datasets.Finally, we do not explore all possible combinations of hyper-parameter settings (e.g., the number of epoch or learning rate) when fine-tuning GPT-3.5 and Magicoder.We do not do so since the search space of hyper-parameter settings is large, which can be expensive.Nonetheless, the main goal of this study is not to find the best hyper-parameter settings for code review automation, but to investigate the performance of GPT-3.5 and Magicoder on code review automation tasks when using model fine-tuning.</p>
<p>Threats to External Validity</p>
<p>Threats to external validity relate to the generalizability of our findings in other software projects.In this study, we conduct the experiment with the dataset obtained from recent work [4,5,6].However, the results of our experiment may not be generalized to other software projects.Thus, other software projects can be explored in future work.</p>
<p>Another threat relates to the updates to GPT-3.5 made by OpenAI in future.Due to the updates, reproduced experiment results may differ from those reported in this paper.</p>
<p>Conclusion</p>
<p>In this work, we investigate the performance of LLMs (i.e., GPT-3.5 and Magicoder) for code review automation when using model fine-tuning and inference techniques (i.e., zero-shot learning, few-shot learning, and persona).We also compare the performance of the LLMs with the existing code review automation approaches [4,5,6].Our results show that (1) finetuned GPT-3.5 performs best for code review automation and (2) the best prompting strategy when using GPT-3.5 without fine-tuning is few-shot learning without a persona.Based on the results, we recommend that (1) LLMs for code review automation should be fine-tuned to achieve the highest performance; and (2) when data is not sufficient for model fine-tuning, fewshot learning without a persona should be used for LLMs for code review automation.</p>
<p>Figure 1 :
1
Figure 1: An overview of the modelling pipeline of LLMs for code review automation.</p>
<p>ments.Next, to fine-tune the studied LLMs, we first randomly obtain a set of training examples from the training set since using the whole training set is prohibitively expensive.Then, we use the selected training examples to fine-tune the studied LLMs.On the other hand, to use the inference techniques (i.e., zero-shot learning, few-shot learning and a persona), we first design prompt templates for each inference technique based on</p>
<p>Figure 2 :
2
Figure 2: An overview of our experimental design (A persona is a part of zero-shot and few-shot learning).</p>
<p>+</p>
<p>public static void writeSegmentedCopyRatioPlot(final String sample_name, final String tnFile, final String preTnFile, final String segFile, final String outputDir, final Boolean log) String logArg = log ?"TRUE" : "FALSE"; final RScriptExecutor executor = new RScriptExecutor(); executor.addScript(newResource(R_SCRIPT, CopyRatioSegmentedPlotter.class));</p>
<p>Figure 4 :
4
Figure 4: (RQ3) Examples of the difference between code submitted for review and revised code generated by GPT-3.5 with zero-shot learning and few-shot learning.</p>
<p>The results of GPT-3.5 (persona is included in prompts)CodeReviewer data Tufano data (with comment) Tufano data (without comment) D − ACT data Fixing bug Refactoring Other Fixing bug Refactoring Other Fixing bug Refactoring Other Fixing bug</p>
<p>Submitted code: <input code> (Input) Developer comment: <input comment> (b) A prompt template for few-shot learning.</p>
<p>Submitted code: <input code> (Input) Developer comment: <input comment> (b) A prompt template for few-shot learning.</p>
<p>Figure 8 :
8
Figure 8: Prompt templates for zero-shot learning and few-shot learning that more details are added to the instructions (lang refers to a programming language).The text in blue is omitted when reviewers' comments are not used in experiments.</p>
<p>Table 1 :
1
[14]differences between our work and Guo et al.'s work[14].
Guo et al. [14]Our workLLMs/approachesGPT-3.5, CodeRe-GPT-3.5, Magicoder [21],viewer [5]CodeReviewer [5], Tu-fanoT5 [6], D-ACT [4]Include fine-tuningNoYesLLMs?Prompting tech-Zero-shot learning, Per-Zero-shot learning, Few-niquessonashot learning, Persona2.3. GPT-3.5 for Code Review Automation</p>
<p>Table 3 :
3
[4]tatistic of the studied datasets (the dataset of Android, Google and Ovirt are from the D-ACT data dataset[4]).Your task is to improve the given submitted code based on the given reviewer comment.Please only generate the improved code without your explanation.
Dataset# Train # Validation # Test # Language Granularity Has CommentCodeReviewer data [5] 150,40513,10213,1049Diff Hunk✓Tufano data [6]134,23816,77916,7791Function✓/✗Android [4]14,6901,8361,8351Function✗Google [4]9,8991,2371,2351Function✗Ovirt [4]21,5092,6862,6881Function✗(Persona) You are an expert software developer in <lang>. You always want to improve yourcode to have higher quality.(Instruction) (Input) <input code>(Input) <input comment>(a) A prompt template for zero-shot learning.## ExampleSubmitted code: <code>Developer comment: <comment>Improved code: <code>--<other examples>--
(Persona) You are an expert software developer in <lang>.You always want to improve your code to have higher quality.You have to generate an output that follows the given examples.(Instruction and examples) You are given 3 examples.Each example begins with "##Example" and ends with "---".Each example contains the submitted code, the developer comment, and the improved code.The submitted code and improved code is written in <lang>.Your task is to improve your submitted code based on the comment that another developer gave you.(Input) Submitted code: <input code> (Input) Developer comment: <input comment> (b) A prompt template for few-shot learning.</p>
<p>Table 4 :
4
The evaluation results of GPT-3.5, Magicoder and the existing code review automation approaches.
ApproachFine-TuningInference Technique Prompting Use PersonaCodeReviewer EM CodeBLEUTufano (with comment) Tufano (without comment) EM CodeBLEU EM CodeBLEUEMAndroid CodeBLEUEMGoogle CodeBLEUEMOvirt CodeBLEUGPT-3.5✓ ✗Zero-shot Few-shot✗ ✓ ✗ ✓ ✗ ✓37.93% 37.70% 17.72% 17.07% 26.55% 26.28%49.00% 49.20% 44.17% 43.11% 47.50% 47.43%22.16% 21.98% 13.52% 12.49% 19.79% 20.03%82.99% 83.04% 78.36% 77.32% 81.47% 81.61%6.02% 6.04% 2.62% 2.29% 8.96% 9.18%79.81% 79.76% 74.92% 73.21% 79.21% 78.98%2.34% 2.29% 0.49% 0.57% 2.34% 1.62%74.15% 74.74% 61.85% 55.88% 75.33% 74.65%6.71% 6.14% 0.16% 0.00% 2.89% 2.45%81.08% 81.02% 61.04% 50.65% 81.40% 81.07%3.05% 2.64% 0.48% 0.22% 1.64% 1.67%74.67% 74.95% 56.55% 45.73% 73.83% 73.29%Magicoder✓ ✗Zero-shot Few-shot✗ ✓ ✗ ✓ ✗ ✓27.43% 27.98% 9.75% 9.93% 15.89% 17.80%44.86% 45.36% 39.45% 39.48% 36.24% 38.93%11.14% 11.06% 8.65% 8.71% 2.93% 2.89%69.77% 69.60% 73.90% 73.57% 4.36% 3.70%1.97% 2.12% 0.81% 1.51% 1.99% 1.84%69.25% 68.84% 59.49% 67.65% 7.49% 6.96%0.27% 0.65% 0.16% 0.27% 0.22% 0.27%65.39% 65.41% 47.37% 47.46% 37.61% 16.59%0.57% 1.13% 0.08% 0.08% 0.49% 0.65%69.30% 69.30% 48.82% 48.58% 42.50% 18.83%0.30% 1.00% 0.04% 0.11% 0.74% 0.82%64.19% 64.16% 44.38% 43.65% 40.33% 19.55%Guo et al. [14]✗Zero-shot✓21.77%59.85%----------CodeReviewer [5]33.23%55.43%15.17%80.83%4.14%78.76%0.54%75.24%0.81%80.10%1.23%75.32%TufanoT5 [6]---11.90%43.39%14.26%79.48%5.40%77.26%0.27%75.88%1.37%82.25%0.19%73.53%D-ACT [4]------0.65%75.99%5.98%81.85%1.79%79.77%</p>
<p>Table 5 :
5
The statistical details of GPT-3.5, Magicoder and the existing code review automation approaches.</p>
<p>public static void writeSegmentedCopyRatioPlot(final String sample_name, final String tnFile, final String preTnFile, final String segFile, final String outputDir, final Boolean log) { + public static void writeSegmentedCopyRatioPlot(final String sampleName, final String tnFile, final String preTnFile, final String segFile, final String outputDir, final boolean log) {
-String logArg = "FALSE";-if (log) {-logArg = "TRUE";-}+ String logArg = log ? "TRUE" : "FALSE";final RScriptExecutor executor = new RScriptExecutor();executor.addScript(new Resource(R_SCRIPT, CopyRatioSegmentedPlotter.class));-</p>
<p>Example of the code change for bug fixing (modify if condition) Example of the code change for bug fixing (remove synchronized keyword) Example of the code change for other (remove method call)
} // Show the loading indicator until data has been fetched. -if (!totalPagesFromData &amp;&amp; totalPagesFromData !== 0) { + if (totalPagesFromData === null) { return fullPageLoadingIndicator; } … (a) -protected synchronized void closeLedgerManagerFactory() { + protected void closeLedgerManagerFactory() { LedgerManagerFactory lmToClose; synchronized(this) { ... (b) "userscripts", cmd) public EventDefinition(IEventDeclaration declaration, StreamInputReader streamInputReader) {log.misc.debug("Userscript to run {}".format(cmd_path))-this.fDeclaration = declaration;-runner.run(cmd, <em>args, env=env, verbose=verbose).-this.fStreamInputReader = streamInputReader;+runner.run(cmd_path, </em>args, env=env, verbose=verbose)+fDeclaration = declaration;runner.finished.connect(commandrunner.deleteLater)+fStreamInputReader = streamInputReader;runner.finished.connect(runner.deleteLater)(c) Example of the code change for refactoring (change variable name)(d) Example of the code change for refactoring (remove this qualifier)...public void move() {return false;...if (! ServerDB::serverExists(srvnum))if (!newX.equals(spriteBase.getX()) || !return false;newY.equals(spriteBase.getY())) {-if (! ServerDB::getConf(srvnum, "autostart",MetaLogger.log(String.format("Monster moved from (%f,::mp.bAutoStart).toBool())%f) to (%f, %f)", spriteBase.getX(), spriteBase.getY(),-return false;newX, newY));Server *s = new Server(srvnum, this);}if (! s-&gt;bValid) {-this.setChanged();delete s;-this.notifyObservers();...}(e) Example of the code change for other (remove if condition)(f)</p>
<p>Table 6 :
6
The evaluation results of GPT-3.5 when being fine-tuned with different sizes of training sets.
CodeReviewerTufano (with comment) Tufano (without comment)AndroidGoogleOvirtSize of Training SetEMCodeBLEUEMCodeBLEUEMCodeBLEUEMCodeBLEUEMCodeBLEUEMCodeBLEU6%37.93%49.00%22.16%82.99%6.02%79.81%2.34%74.15%6.71%81.08%3.05%74.67%10%37.72%48.83%22.31%83.43%5.37%80.68%2.51%75.10%5.98%80.65%2.71%75.13%20%38.80%49.33%22.84%83.44%5.65%80.42%2.34%76.04%7.52%81.40%2.83%75.46%</p>
<p>Table 7 :
7
https://www.promptingguide.ai/
https://help.openai.com/en/articles/6654000-best-practices-for-promptengineering-with-openai-api
4 https://platform.openai.com/docs/guides/prompt-engineering/strategywrite-clear-instructions
https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tunedmodel
https://github.com/piskvorky/gensim
https://learn.microsoft.com/en-us/azure/aiservices/openai/concepts/advanced-prompt-engineering?pivots=programminglanguage-chat-completions#break-the-task-down
https://www.promptingguide.ai/introduction/tips
Code Submitted for Review with Reviewers' Comment , Training Set Revised Code Code Submitted for Review with Reviewers' Comment , Sample Training Set Revised Code Code Submitted for Review with Reviewers' Comment , Sample Training Set Code Submitted for Review with
On learning meaningful code changes via neural machine translation. Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, Denys Poshyvanyk, Proceedings of ICSE. ICSE2019</p>
<p>Towards automating code review activities. Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, Gabriele Bavota, Proceedings of ICSE. ICSE2021</p>
<p>Autotransform: Automated code transformation to support modern code review process. Patanamon Thongtanunam, Chanathip Pornprasit, Chakkrit Tantithamthavorn, Proceedings of ICSE. ICSE2022</p>
<p>D-ACT: Towards diff-aware code transformation for code review under a time-wise evaluation. Chanathip Pornprasit, Chakkrit Tantithamthavorn, Patanamon Thongtanunam, and Chunyang Chen. 2023Proceedings of SANER</p>
<p>Automating code review activities by large-scale pre-training. Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, Proceedings of ESEC/FSE. ESEC/FSE2022</p>
<p>Using pre-trained models to boost code review automation. Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, Gabriele Bavota, Proceedings of ICSE. ICSE2022</p>
<p>Attention is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Proceedings of NIPS. NIPS2017</p>
<p>Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. Yue Wang, Weishi Wang, Shafiq Joty, Steven Ch Hoi, Proceedings of EMNLP. EMNLP2021</p>
<p>Llama-reviewer: Advancing code review automation with large language models through parameterefficient fine-tuning. J Lu, L Yu, X Li, L Yang, C Zuo, 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE). 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>What makes good in-context demonstrations for code intelligence tasks with llms?. Shuzheng Gao, Xin-Cheng Wen, Cuiyun Gao, Wenxuan Wang, Hongyu Zhang, Michael R Lyu, 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE Computer Society2023</p>
<p>Constructing effective in-context demonstration for code intelligence tasks: An empirical study. Shuzheng Gao, Xin-Cheng Wen, Cuiyun Gao, Wenxuan Wang, Michael R Lyu, Proceedings of ASE. ASE2023</p>
<p>Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C Schmidt, arXiv:2302.11382A prompt pattern catalog to enhance prompt engineering with chatgpt. 2023arXiv preprint</p>
<p>Exploring the potential of chatgpt in automated code refinement: An empirical study. Qi Guo, Junming Cao, Xiaofei Xie, Shangqing Liu, Xiaohong Li, Bihuan Chen, Xin Peng, arXiv:2309.082212023arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 2023</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Proceedings of NeurIPS. NeurIPS2020</p>
<p>Large language models are few-shot testers: Exploring llm-based general bug reproduction. Sungmin Kang, Juyeon Yoon, Shin Yoo, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Large language models are few-shot summarizers: Multi-intent comment generation via in-context learning. Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi Jin, Xiaoguang Mao, Xiangke Liao, 2024</p>
<p>Magicoder: Source code is all you need. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang, arXiv:2312.021202023arXiv preprint</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, Neel Tang, Ming Sundaresan, Ambrosio Zhou, Shuai Blanco, Ma, arXiv:2009.102972020arXiv preprint</p>
<p>Code reviewing in the trenches: Challenges and best practices. Laura Macleod, Michaela Greiler, Margaret-Anne Storey, Christian Bird, Jacek Czerwonka, IEEE Software. 2017</p>
<p>Convergent Contemporary Software Peer Review Practices. C Peter, Christian Rigby, Bird, Proceedings of ESEC/FSE. ESEC/FSE2013</p>
<p>Towards automating code review activities. Rosalia Tufan, Luca Pascarella, Michele Tufanoy, Denys Poshyvanykz, Gabriele Bavota, Proceedings of ICSE. ICSE2021</p>
<p>Code llama: Open foundation models for code. Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Rapin, arXiv:2308.129502023arXiv preprint</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, arXiv:2305.06161may the source be with you!. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What makes good in-context examples for gpt-3?. 2021arXiv preprint</p>
<p>Larger language models do in-context learning differently. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, arXiv:2303.038462023arXiv preprint</p>
<p>The cot collection: Improving zeroshot and few-shot learning of language models via chain-of-thought finetuning. Seungone Kim, June Se, Doyoung Joo, Joel Kim, Seonghyeon Jang, Jamin Ye, Minjoon Shin, Seo, arXiv:2305.140452023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Proceedings of NeurIPS. NeurIPS2022</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Prompting for automatic log template extraction. Junjielong Xu, Ruichun Yang, Yintong Huo, Chengyu Zhang, Pinjia He, arXiv:2307.099502023arXiv preprint</p>
<p>Unprecedented code change automation: The fusion of llms and transformation by example. Malinda Dilhara, Abhiram Bellur, Timofey Bryksin, Danny Dig, arXiv:2402.071382024arXiv preprint</p>
<p>Md Rakib, Hossain Misu, Cristina V Lopes, Iris Ma, James Noble, arXiv:2402.00247Towards ai-assisted synthesis of verified dafny methods. 2024arXiv preprint</p>
<p>Zhihan Jiang, Jinyang Liu, Zhuangbin Chen, Yichen Li, Junjie Huang, Yintong Huo, Pinjia He, Jiazhen Gu, Michael R Lyu, arXiv:2310.01796Llmparser: A llm-based log parsing framework. 2023arXiv preprint</p>
<p>Xin Zhou, Kisub Kim, Bowen Xu, Donggyun Han, Junda He, David Lo, arXiv:2303.07221Generation-based code review automation: How far are we?. 2023arXiv preprint</p>
<p>Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Min-Hung Chen, arXiv:2402.09353Weight-decomposed low-rank adaptation. Dora2024arXiv preprint</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval. Stephen Robertson, Hugo Zaragoza, 2009</p>
<p>Evaluating instruction-tuned large language models on code comprehension and generation. Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, Yiling Lou, arXiv:2308.012402023arXiv preprint</p>
<p>Bleu: A method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of ACL. ACL2002</p>
<p>Are prompt engineering and todo comments friends or foes? an evaluation on github copilot. David Obrien, Sumon Biswas, Sayem Mohammad Imtiaz, Rabe Abdalkareem, Emad Shihab, Hridesh Rajan, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024</p>
<p>Soneya Binta Hossain, Nan Jiang, Qiang Zhou, Xiaopeng Li, Wen-Hao Chiang, Yingjun Lyu, Hoan Nguyen, Omer Tripp, arXiv:2404.11595A deep dive into large language models for automated bug localization and repair. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-272 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-272</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-272</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-270067866</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.17893v1.pdf" target="_blank">Arithmetic Reasoning with LLM: Prolog Generation & Permutation</a></p>
                <p><strong>Paper Abstract:</strong> Instructing large language models (LLMs) to solve elementary school math problems has shown great success using Chain of Thought (CoT). However, the CoT approach relies on an LLM to generate a sequence of arithmetic calculations which can be prone to cascaded calculation errors. We hypothesize that an LLM should focus on extracting predicates and generating symbolic formulas from the math problem description so that the underlying calculation can be done via an external code interpreter. We investigate using LLM to generate Prolog programs to solve mathematical questions. Experimental results show that our Prolog-based arithmetic problem-solving outperforms CoT generation in the GSM8K benchmark across three distinct LLMs. In addition, given the insensitive ordering of predicates and symbolic formulas in Prolog, we propose to permute the ground truth predicates for more robust LLM training via data augmentation.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e272.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e272.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PrologGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prolog code generation with external Prolog interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finetuning LLMs to translate math word problems into Prolog predicates/rules; an external Prolog interpreter (PySwip / SWI-Prolog) deterministically executes the logic and arithmetic to produce the numerical answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (Llama-2, CodeLlama, Mistral)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Multi-step grade-school arithmetic (addition, subtraction, multiplication, division, unit conversion and combined multi-step arithmetic reasoning as in GSM8K problems).</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>GSM8K standard (diverse grade-school numbers); evaluation also on GSM-HARD which replaces numbers with larger values; only integer-answer problems considered due to PySwip limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised finetuning (LoRA, 8-bit quant) to generate Prolog programs; decode with beam search and execute with PySwip; dataset GSM8K-Prolog created semi-automatically (GPT-4 + manual correction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Across 7B LLMs, Prolog generation improves accuracy over CoT: Llama-2: 41.5% (GSM8K) vs CoT 33.8%; CodeLlama: 55.0% vs CoT 37.5%; Mistral: 66.3% vs CoT 58.9%. On GSM-HARD Prolog also improves over CoT (e.g., Mistral 50.6% vs CoT 30.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Hypothesis and empirical support that LLMs are better used to extract predicates and symbolic formulas (translation task) while delegating deterministic calculation to a symbolic engine; this avoids cascaded calculation errors common in sequential CoT numeric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Evaluated only at 7B models; relative improvements hold across three different 7B models; no experiments at larger model scales reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Generated Prolog programs often contain semantic logic errors (manual sample: 100% of problematic samples had semantic errors), occasional syntax errors (~7% of sampled problematic cases), and domain mistakes (e.g., unit conversion bugs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Chain-of-Thought (CoT) finetuning baseline and a Python-code-generation baseline; also compared across models and permutation-augmentation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Translating problems into symbolic Prolog programs and using a deterministic Prolog interpreter yields substantially higher arithmetic reasoning accuracy than asking LLMs to perform sequential numeric calculation (CoT) on GSM8K-style problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Reasoning with LLM: Prolog Generation & Permutation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e272.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e272.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROPER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predicate Permutation (PROPER) data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-augmentation method that samples permutations of Prolog predicates (facts and goals) so finetuned LLMs learn Prolog's non-sequential predicate ordering and become robust to ordering variations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (Llama-2, CodeLlama, Mistral)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Same multi-step arithmetic tasks as PrologGen (GSM8K / GSM-HARD).</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>GSM8K standard; GSM-HARD includes larger numbers; only integer-answer subset used.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Augment training set by sampling permutations of goals and facts per Prolog program (up to 100 permutations per sample, practical cap taken), then finetune with LoRA; slight prompt modifications for permuted samples to avoid confusing the model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>PROPER further improved accuracy when using a suitable permutation ratio: eg. for Llama-2 accuracy rose from Prolog 41.5% to PROPER 51.0% on GSM8K (≈ +9.5% reported improvement when adding two permuted samples per original); CodeLlama improved from 55.0% to 59.0% (≈ +4.0%). For Mistral improvements were smaller or inconsistent (Mistral PROPER 70.2% vs Prolog 66.3% in final table but some permutation ratios hurt).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Learning many predicate orderings teaches the model the non-sequential nature of Prolog, focusing learning on predicate extraction rather than order-sensitive sequence generation; improves robustness of symbolic translation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Accuracy depends on permutation ratio; authors report lowered validation loss with more permutations but note leakage issues; best practical permutation ratio found empirically (1:2 in some settings) though model-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If applied improperly (or with data leakage), permutations reduce generalization; improvements are model-dependent — some models (Mistral) show worsened performance at certain permutation ratios. Also introduces soft data leakage risk if permutations of validation samples end up in training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared PROPER vs non-permuted Prolog finetuning and vs CoT; also evaluated multiple permutation ratios and checkpoint selection strategies (validation loss vs validation accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Augmenting Prolog program outputs with predicate-order permutations helps LLMs learn the non-sequential structure of symbolic solutions and can substantially increase arithmetic solving accuracy, but gains depend on permutation ratio and model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Reasoning with LLM: Prolog Generation & Permutation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e272.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e272.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting / CoT supervised finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Asking LLMs to produce step-by-step natural language reasoning (arithmetic steps) and then the final answer; used here as the supervised baseline for arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2, CodeLlama, Mistral (7B versions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Multi-step arithmetic within GSM8K (word problems requiring arithmetic sequences of operations).</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>GSM8K standard; GSM-HARD for larger numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised finetuning using CoT ground-truth labels from GSM8K with LoRA and 8-bit quantization; used as direct baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported baseline accuracies: Llama-2 CoT 33.8% (GSM8K) / 12.0% (GSM-HARD); CodeLlama CoT 37.5% / 13.9%; Mistral CoT 58.9% / 30.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Paper suggests sequential natural-language CoT causes cascaded calculation errors because LLMs must both reason and compute numeric steps in sequence; no internal mechanistic probe provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>CoT is shown to be an emergent ability in prior work, but in this paper CoT finetuned 7B models underperform program-generation methods; no larger-scale CoT experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Cascaded arithmetic/calculation errors in sequential step generation; sensitivity to multi-step numeric computation leading to incorrect final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to Prolog and PROPER code-generation methods and to Python code generation baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>On GSM8K, prompting LLMs to write natural-language step-by-step arithmetic (CoT) yields substantially lower final-answer accuracy than translating into symbolic programs executed by a deterministic interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Reasoning with LLM: Prolog Generation & Permutation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e272.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e272.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PythonGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Python code generation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alternative program-aided baseline where LLMs generate Python code to compute answers and a Python interpreter executes the code; dataset prepared similarly to Prolog but with Python solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Same GSM8K multi-step arithmetic problems.</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>GSM8K standard; only integer-answer subset considered.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Finetune Llama-2 to generate Python code; execute code to produce numeric answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Python generation achieved 55.12% accuracy on GSM8K with Llama-2 — higher than both Prolog and PROPER results for Llama-2 in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Authors attribute Python advantage possibly to pretraining prevalence of Python code in model pretraining corpora (models are better at generating popular structured languages); suggests with more Prolog training data Prolog could match Python.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Not extensively explored beyond this single comparison; implies model pretraining composition affects ability to generate executable code.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not detailed; implicit risks include semantic errors in generated code and typical issues of code generation (variable misuse, unit errors) similar to Prolog case.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Prolog and CoT finetuning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Generating Python programs and executing them produced higher accuracy than Prolog generation for Llama-2 in this study, suggesting pretraining-language prevalence influences program-aided arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Reasoning with LLM: Prolog Generation & Permutation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e272.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e272.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 7B (finetuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter open foundation LLM (Llama-2) finetuned with LoRA and 8-bit quantization to generate CoT, Prolog, or permuted-Prolog solutions for GSM8K arithmetic problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>GSM8K multi-step arithmetic (diverse grade-school operations and unit conversions).</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>GSM8K / GSM-HARD (integers only due to interpreter limitation).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Finetuned with LoRA (r=32, alpha=64 best config), 8-bit quantization; training: 6 epochs, batch size 128, LR 3e-4; inference via beam search (beam=4) and PySwip interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>CoT: 33.8% (GSM8K) / 12.0% (GSM-HARD); Prolog: 41.5% / 32.4%; PROPER: 51.0% / 37.4%; Python generation (separate experiment): 55.12% (GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Finetuned Llama-2 is better at predicate extraction and symbolic translation than direct numeric computation via CoT; performance improves when models are trained to output executable symbolic code and when permutation augmentations are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Only 7B scale evaluated; improvements from PROPER and Prolog vs CoT are observed at this scale. The authors did not test larger models, so scaling effects beyond 7B are unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Semantic errors dominate generated Prolog outputs; unit-conversion and variable-definition mistakes in examples; inability to handle decimal answers due to interpreter limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared internal performance across CoT, Prolog, PROPER, Python generation; also compared checkpoint selection by validation accuracy vs validation loss.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>For Llama-2 (7B), generating Prolog and leveraging a Prolog interpreter substantially outperforms CoT finetuning; further gains achieved with PROPER and Python generation outperformed Prolog in one experiment, highlighting influence of pretraining code distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Reasoning with LLM: Prolog Generation & Permutation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e272.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e272.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLlama (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLlama 7B (finetuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-pretrained 7B LLM specialized for code generation; finetuned in the paper to emit Prolog programs for arithmetic problems and compared to CoT and PROPER variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Multi-step arithmetic aligned with GSM8K tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>GSM8K and GSM-HARD integer problems.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>LoRA finetuning with 8-bit quant; Prolog/PROPER training and evaluation with PySwip interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>CoT: 37.5% (GSM8K) / 13.9% (GSM-HARD); Prolog: 55.0% / 41.6%; PROPER: 59.0% / 45.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>CodeLlama's pretraining on code corpora likely makes it better at generating structured, correct program outputs which translates into higher Prolog-generation arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Evaluated at 7B only; shows larger relative gains from Prolog generation compared to CoT than Llama-2 did, consistent with code-pretraining advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same categories as other models: semantic logic errors in generated Prolog and occasional syntax/variable mistakes; improvements with PROPER but still not perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across CoT, Prolog, and PROPER methods; compared to Llama-2 and Mistral models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Code-pretrained models like CodeLlama are particularly effective at generating executable symbolic programs for arithmetic problems and benefit from PROPER augmentation, outperforming CoT by a large margin.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Reasoning with LLM: Prolog Generation & Permutation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e272.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e272.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B (finetuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performing 7B LLM finetuned to output Prolog solutions and compared with CoT and PROPER; typically achieves the highest absolute accuracy among evaluated 7B models in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>GSM8K multi-step arithmetic problems and GSM-HARD variations.</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>GSM8K; GSM-HARD uses larger numbers; integer answers only.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>LoRA finetuning (same configuration as others), Prolog/PROPER training and execution via PySwip.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>CoT: 58.9% (GSM8K) / 30.8% (GSM-HARD); Prolog: 66.3% / 50.6%; PROPER: 70.2% / 54.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Mistral shows strong baseline arithmetic capability (CoT higher than other CoT baselines) and gains from program generation, but PROPER's benefit is model-dependent (some permutation ratios reduced performance before tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Only evaluated at 7B; achieved the highest absolute accuracies among the three 7B models tested; PROPER sometimes yields diminishing returns if model already strong at Prolog generation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Error analysis on finetuned Mistral found that in a manual sample of 70 problematic Prolog outputs, 100% had semantic logic mistakes and ~7% had syntax errors; common semantic mistakes include incorrect logical formulations and unit/variable misuse.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared CoT vs Prolog vs PROPER and across permutation ratios; compared against Llama-2 and CodeLlama.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Even with a strong baseline, Mistral benefits from translating problems into symbolic Prolog and executing them, but PROPER augmentation must be tuned per model as gains are not monotonic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Reasoning with LLM: Prolog Generation & Permutation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>GSM8K: Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided Language models <em>(Rating: 2)</em></li>
                <li>Tora: A tool-integrated reasoning agent for mathematical problem solving <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-272",
    "paper_id": "paper-270067866",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "PrologGen",
            "name_full": "Prolog code generation with external Prolog interpreter",
            "brief_description": "Finetuning LLMs to translate math word problems into Prolog predicates/rules; an external Prolog interpreter (PySwip / SWI-Prolog) deterministically executes the logic and arithmetic to produce the numerical answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs (Llama-2, CodeLlama, Mistral)",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "Multi-step grade-school arithmetic (addition, subtraction, multiplication, division, unit conversion and combined multi-step arithmetic reasoning as in GSM8K problems).",
            "number_range_or_complexity": "GSM8K standard (diverse grade-school numbers); evaluation also on GSM-HARD which replaces numbers with larger values; only integer-answer problems considered due to PySwip limitation.",
            "method_or_intervention": "Supervised finetuning (LoRA, 8-bit quant) to generate Prolog programs; decode with beam search and execute with PySwip; dataset GSM8K-Prolog created semi-automatically (GPT-4 + manual correction).",
            "performance_result": "Across 7B LLMs, Prolog generation improves accuracy over CoT: Llama-2: 41.5% (GSM8K) vs CoT 33.8%; CodeLlama: 55.0% vs CoT 37.5%; Mistral: 66.3% vs CoT 58.9%. On GSM-HARD Prolog also improves over CoT (e.g., Mistral 50.6% vs CoT 30.8%).",
            "mechanistic_insight": "Hypothesis and empirical support that LLMs are better used to extract predicates and symbolic formulas (translation task) while delegating deterministic calculation to a symbolic engine; this avoids cascaded calculation errors common in sequential CoT numeric computation.",
            "performance_scaling": "Evaluated only at 7B models; relative improvements hold across three different 7B models; no experiments at larger model scales reported.",
            "failure_modes": "Generated Prolog programs often contain semantic logic errors (manual sample: 100% of problematic samples had semantic errors), occasional syntax errors (~7% of sampled problematic cases), and domain mistakes (e.g., unit conversion bugs).",
            "comparison_baseline": "Compared against Chain-of-Thought (CoT) finetuning baseline and a Python-code-generation baseline; also compared across models and permutation-augmentation variants.",
            "key_finding": "Translating problems into symbolic Prolog programs and using a deterministic Prolog interpreter yields substantially higher arithmetic reasoning accuracy than asking LLMs to perform sequential numeric calculation (CoT) on GSM8K-style problems.",
            "uuid": "e272.0",
            "source_info": {
                "paper_title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "PROPER",
            "name_full": "Predicate Permutation (PROPER) data augmentation",
            "brief_description": "A data-augmentation method that samples permutations of Prolog predicates (facts and goals) so finetuned LLMs learn Prolog's non-sequential predicate ordering and become robust to ordering variations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs (Llama-2, CodeLlama, Mistral)",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "Same multi-step arithmetic tasks as PrologGen (GSM8K / GSM-HARD).",
            "number_range_or_complexity": "GSM8K standard; GSM-HARD includes larger numbers; only integer-answer subset used.",
            "method_or_intervention": "Augment training set by sampling permutations of goals and facts per Prolog program (up to 100 permutations per sample, practical cap taken), then finetune with LoRA; slight prompt modifications for permuted samples to avoid confusing the model.",
            "performance_result": "PROPER further improved accuracy when using a suitable permutation ratio: eg. for Llama-2 accuracy rose from Prolog 41.5% to PROPER 51.0% on GSM8K (≈ +9.5% reported improvement when adding two permuted samples per original); CodeLlama improved from 55.0% to 59.0% (≈ +4.0%). For Mistral improvements were smaller or inconsistent (Mistral PROPER 70.2% vs Prolog 66.3% in final table but some permutation ratios hurt).",
            "mechanistic_insight": "Learning many predicate orderings teaches the model the non-sequential nature of Prolog, focusing learning on predicate extraction rather than order-sensitive sequence generation; improves robustness of symbolic translation.",
            "performance_scaling": "Accuracy depends on permutation ratio; authors report lowered validation loss with more permutations but note leakage issues; best practical permutation ratio found empirically (1:2 in some settings) though model-dependent.",
            "failure_modes": "If applied improperly (or with data leakage), permutations reduce generalization; improvements are model-dependent — some models (Mistral) show worsened performance at certain permutation ratios. Also introduces soft data leakage risk if permutations of validation samples end up in training.",
            "comparison_baseline": "Compared PROPER vs non-permuted Prolog finetuning and vs CoT; also evaluated multiple permutation ratios and checkpoint selection strategies (validation loss vs validation accuracy).",
            "key_finding": "Augmenting Prolog program outputs with predicate-order permutations helps LLMs learn the non-sequential structure of symbolic solutions and can substantially increase arithmetic solving accuracy, but gains depend on permutation ratio and model.",
            "uuid": "e272.1",
            "source_info": {
                "paper_title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting / CoT supervised finetuning",
            "brief_description": "Asking LLMs to produce step-by-step natural language reasoning (arithmetic steps) and then the final answer; used here as the supervised baseline for arithmetic reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2, CodeLlama, Mistral (7B versions)",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "Multi-step arithmetic within GSM8K (word problems requiring arithmetic sequences of operations).",
            "number_range_or_complexity": "GSM8K standard; GSM-HARD for larger numbers.",
            "method_or_intervention": "Supervised finetuning using CoT ground-truth labels from GSM8K with LoRA and 8-bit quantization; used as direct baseline.",
            "performance_result": "Reported baseline accuracies: Llama-2 CoT 33.8% (GSM8K) / 12.0% (GSM-HARD); CodeLlama CoT 37.5% / 13.9%; Mistral CoT 58.9% / 30.8%.",
            "mechanistic_insight": "Paper suggests sequential natural-language CoT causes cascaded calculation errors because LLMs must both reason and compute numeric steps in sequence; no internal mechanistic probe provided.",
            "performance_scaling": "CoT is shown to be an emergent ability in prior work, but in this paper CoT finetuned 7B models underperform program-generation methods; no larger-scale CoT experiments reported.",
            "failure_modes": "Cascaded arithmetic/calculation errors in sequential step generation; sensitivity to multi-step numeric computation leading to incorrect final answers.",
            "comparison_baseline": "Compared directly to Prolog and PROPER code-generation methods and to Python code generation baseline.",
            "key_finding": "On GSM8K, prompting LLMs to write natural-language step-by-step arithmetic (CoT) yields substantially lower final-answer accuracy than translating into symbolic programs executed by a deterministic interpreter.",
            "uuid": "e272.2",
            "source_info": {
                "paper_title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "PythonGen",
            "name_full": "Python code generation baseline",
            "brief_description": "Alternative program-aided baseline where LLMs generate Python code to compute answers and a Python interpreter executes the code; dataset prepared similarly to Prolog but with Python solutions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2 (7B)",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "Same GSM8K multi-step arithmetic problems.",
            "number_range_or_complexity": "GSM8K standard; only integer-answer subset considered.",
            "method_or_intervention": "Finetune Llama-2 to generate Python code; execute code to produce numeric answer.",
            "performance_result": "Python generation achieved 55.12% accuracy on GSM8K with Llama-2 — higher than both Prolog and PROPER results for Llama-2 in the paper.",
            "mechanistic_insight": "Authors attribute Python advantage possibly to pretraining prevalence of Python code in model pretraining corpora (models are better at generating popular structured languages); suggests with more Prolog training data Prolog could match Python.",
            "performance_scaling": "Not extensively explored beyond this single comparison; implies model pretraining composition affects ability to generate executable code.",
            "failure_modes": "Not detailed; implicit risks include semantic errors in generated code and typical issues of code generation (variable misuse, unit errors) similar to Prolog case.",
            "comparison_baseline": "Compared against Prolog and CoT finetuning baselines.",
            "key_finding": "Generating Python programs and executing them produced higher accuracy than Prolog generation for Llama-2 in this study, suggesting pretraining-language prevalence influences program-aided arithmetic performance.",
            "uuid": "e272.3",
            "source_info": {
                "paper_title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Llama-2 (7B)",
            "name_full": "Llama-2 7B (finetuned variants)",
            "brief_description": "A 7B-parameter open foundation LLM (Llama-2) finetuned with LoRA and 8-bit quantization to generate CoT, Prolog, or permuted-Prolog solutions for GSM8K arithmetic problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "GSM8K multi-step arithmetic (diverse grade-school operations and unit conversions).",
            "number_range_or_complexity": "GSM8K / GSM-HARD (integers only due to interpreter limitation).",
            "method_or_intervention": "Finetuned with LoRA (r=32, alpha=64 best config), 8-bit quantization; training: 6 epochs, batch size 128, LR 3e-4; inference via beam search (beam=4) and PySwip interpreter.",
            "performance_result": "CoT: 33.8% (GSM8K) / 12.0% (GSM-HARD); Prolog: 41.5% / 32.4%; PROPER: 51.0% / 37.4%; Python generation (separate experiment): 55.12% (GSM8K).",
            "mechanistic_insight": "Finetuned Llama-2 is better at predicate extraction and symbolic translation than direct numeric computation via CoT; performance improves when models are trained to output executable symbolic code and when permutation augmentations are applied.",
            "performance_scaling": "Only 7B scale evaluated; improvements from PROPER and Prolog vs CoT are observed at this scale. The authors did not test larger models, so scaling effects beyond 7B are unknown.",
            "failure_modes": "Semantic errors dominate generated Prolog outputs; unit-conversion and variable-definition mistakes in examples; inability to handle decimal answers due to interpreter limits.",
            "comparison_baseline": "Compared internal performance across CoT, Prolog, PROPER, Python generation; also compared checkpoint selection by validation accuracy vs validation loss.",
            "key_finding": "For Llama-2 (7B), generating Prolog and leveraging a Prolog interpreter substantially outperforms CoT finetuning; further gains achieved with PROPER and Python generation outperformed Prolog in one experiment, highlighting influence of pretraining code distribution.",
            "uuid": "e272.4",
            "source_info": {
                "paper_title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "CodeLlama (7B)",
            "name_full": "CodeLlama 7B (finetuned variants)",
            "brief_description": "A code-pretrained 7B LLM specialized for code generation; finetuned in the paper to emit Prolog programs for arithmetic problems and compared to CoT and PROPER variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CodeLlama",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "Multi-step arithmetic aligned with GSM8K tasks.",
            "number_range_or_complexity": "GSM8K and GSM-HARD integer problems.",
            "method_or_intervention": "LoRA finetuning with 8-bit quant; Prolog/PROPER training and evaluation with PySwip interpreter.",
            "performance_result": "CoT: 37.5% (GSM8K) / 13.9% (GSM-HARD); Prolog: 55.0% / 41.6%; PROPER: 59.0% / 45.9%.",
            "mechanistic_insight": "CodeLlama's pretraining on code corpora likely makes it better at generating structured, correct program outputs which translates into higher Prolog-generation arithmetic performance.",
            "performance_scaling": "Evaluated at 7B only; shows larger relative gains from Prolog generation compared to CoT than Llama-2 did, consistent with code-pretraining advantage.",
            "failure_modes": "Same categories as other models: semantic logic errors in generated Prolog and occasional syntax/variable mistakes; improvements with PROPER but still not perfect.",
            "comparison_baseline": "Compared across CoT, Prolog, and PROPER methods; compared to Llama-2 and Mistral models.",
            "key_finding": "Code-pretrained models like CodeLlama are particularly effective at generating executable symbolic programs for arithmetic problems and benefit from PROPER augmentation, outperforming CoT by a large margin.",
            "uuid": "e272.5",
            "source_info": {
                "paper_title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Mistral (7B)",
            "name_full": "Mistral 7B (finetuned variants)",
            "brief_description": "A high-performing 7B LLM finetuned to output Prolog solutions and compared with CoT and PROPER; typically achieves the highest absolute accuracy among evaluated 7B models in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "GSM8K multi-step arithmetic problems and GSM-HARD variations.",
            "number_range_or_complexity": "GSM8K; GSM-HARD uses larger numbers; integer answers only.",
            "method_or_intervention": "LoRA finetuning (same configuration as others), Prolog/PROPER training and execution via PySwip.",
            "performance_result": "CoT: 58.9% (GSM8K) / 30.8% (GSM-HARD); Prolog: 66.3% / 50.6%; PROPER: 70.2% / 54.4%.",
            "mechanistic_insight": "Mistral shows strong baseline arithmetic capability (CoT higher than other CoT baselines) and gains from program generation, but PROPER's benefit is model-dependent (some permutation ratios reduced performance before tuning).",
            "performance_scaling": "Only evaluated at 7B; achieved the highest absolute accuracies among the three 7B models tested; PROPER sometimes yields diminishing returns if model already strong at Prolog generation.",
            "failure_modes": "Error analysis on finetuned Mistral found that in a manual sample of 70 problematic Prolog outputs, 100% had semantic logic mistakes and ~7% had syntax errors; common semantic mistakes include incorrect logical formulations and unit/variable misuse.",
            "comparison_baseline": "Compared CoT vs Prolog vs PROPER and across permutation ratios; compared against Llama-2 and CodeLlama.",
            "key_finding": "Even with a strong baseline, Mistral benefits from translating problems into symbolic Prolog and executing them, but PROPER augmentation must be tuned per model as gains are not monotonic.",
            "uuid": "e272.6",
            "source_info": {
                "paper_title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "GSM8K: Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "gsm8k_training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "PAL: Program-aided Language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Tora: A tool-integrated reasoning agent for mathematical problem solving",
            "rating": 2,
            "sanitized_title": "tora_a_toolintegrated_reasoning_agent_for_mathematical_problem_solving"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        }
    ],
    "cost": 0.015537249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Arithmetic Reasoning with LLM: Prolog Generation &amp; Permutation</p>
<p>Xiaocheng Yang 
Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning
New York University Shanghai</p>
<p>Bingsen Chen 
Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning
New York University Shanghai</p>
<p>Yik-Cheung Tam 
Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning
New York University Shanghai</p>
<p>Arithmetic Reasoning with LLM: Prolog Generation &amp; Permutation
60D1249A90402B7F63882BC9572C522A
Instructing large language models (LLMs) to solve elementary school math problems has shown great success using Chain of Thought (CoT).However, the CoT approach relies on an LLM to generate a sequence of arithmetic calculations which can be prone to cascaded calculation errors.We hypothesize that an LLM should focus on extracting predicates and generating symbolic formulas from the math problem description so that the underlying calculation can be done via an external code interpreter.We investigate using LLM to generate Prolog programs to solve mathematical questions.Experimental results show that our Prolog-based arithmetic problem-solving outperforms CoT generation in the GSM8K benchmark across three distinct LLMs.In addition, given the insensitive ordering of predicates and symbolic formulas in Prolog, we propose to permute the ground truth predicates for more robust LLM training via data augmentation.</p>
<p>Introduction</p>
<p>Large language models (LLMs), with their scaling of model size and data size, have demonstrated impressive performance across various understanding and generation tasks (Brown et al., 2020;Chowdhery et al., 2022;Rae et al., 2021;Thoppilan et al., 2022;Touvron et al., 2023;Almazrouei et al., 2023;Jiang et al., 2023).Nevertheless, such LLMs fall short in addressing mathematical problems that involves arithmetic, commonsense, and symbolic reasoning -topics that may appear deceptively simple to humans (Rae et al., 2021).Existing works leveraged Chain-of-Thought (CoT) reasoning that asks language models to generate both the answer and the step-by-step reasoning chain, which helps break down a complex reasoning task into a sequential thought process (Wei et al., 2022b).Particularly, arithmetic reasoning with CoT is shown to be an emergent ability that language models acquired during the scaling process (Wei et al., 2022a).Yet, natural language reasoning is not native to mathematical operations and symbolic manipulations.A line of work has focused on augmenting language models with deterministic computation resources like a calculator (Schick et al., 2023) or program-based tools (Gao et al., 2023;Gou et al., 2023).However, all such methods require a sequential reasoning trajectory, where models need to translate the natural language questions into sequential mathematical or logical operations.Our research probes into the application of Prolog, a logic programming language, in solving the arithmetic reasoning task.Prolog solves arithmetic reasoning tasks by defining an unordered set of predicates and running queries over them.We further explain the unique properties of Prolog in Section 2. In Prolog code generation for arithmetic reasoning, LLMs extract facts and rules in mathematical ques- Our research has the following contributions: 1) We curate and open-source the GSM8K-Prolog dataset with a semi-automatic approach, which contains arithmetic reasoning problems and their corresponding Prolog code solutions.2) Our experiments show that Prolog code generation is consistently better than CoT on the arithmetic reasoning task, indicating that LLM can focus on predicate extractions and rely on an external tool to calculate and perform the logical induction to address mathematical problems.3) Given the non-sequential nature of predicates in Prolog code, we propose predicate permutation as a data augmentation method and demonstrate its efficacy in robust LLM training.</p>
<p>Question</p>
<p>:-use_module(library(clpq)).</p>
<p>earn (weng, 12).work (weng, 50).solve(Total_salary) :earn(weng, Salary_per_hour), work(weng, Working_minutes), {Total_salary = Salary_per_hour * Working_minutes / 60}.</p>
<p>Original Ground Truth</p>
<p>:-use_module(library(clpq)).</p>
<p>solve(Total_salary) :work(weng, Working_minutes), {Total_salary = Salary_per_hour * Working_minutes / 60}, earn(weng, Salary_per_hour).</p>
<p>work (weng, 50).earn (weng, 12).</p>
<p>Permuted Ground Truth</p>
<p>Facts</p>
<p>Goals</p>
<p>Rule</p>
<p>Figure 2: Prolog and permuted Prolog code samples.</p>
<p>Preliminaries: Prolog Language</p>
<p>Prolog is a logic programming language, which was initially designed for artificial intelligence and computational linguistics (Clocksin and Mellish, 2003;Bratko, 2012;Covington, 2002).As shown in the upper graph of Figure 2, a Prolog program defines a set of predicates that contains facts and goals.In the example, facts include earn(weng, 12) that declares the hourly salary of Weng, and work(weng, 50) that defines the working minutes of Weng; the goals constitute a rule in the form of solve<answer>:-<goal_1>,<goal_2>, ....A rule is true when all the goals are satisfied.Having all the facts and goals defined in the program, users can make a query to obtain the solutions that make the rule true given all the facts.Moreover, Prolog codes are not sequential like Python, meaning that the order of facts and rules does not alter the result of the program.The lower graph in Figure 2, shows an equivalent sample that permutes the order of the predicates, which produces the same result as the original program.</p>
<p>3 Method</p>
<p>GSM8K-Prolog Dataset</p>
<p>To our knowledge, there has not been a dataset for solving mathematical questions with Prolog.We hence curated a dataset based on GSM8K (Cobbe et al., 2021), a popular benchmark of diverse grade school math word problems, in a semi-automatic manner with OpenAI's Text Completion API1 .In particular, we used the same dataset splits and questions in GSM8K and prompted GPT-4 to generate the Prolog programs to solve the questions.We then manually corrected some malfunctioning samples.In this manner, we obtained a high-quality corpus with 100% accuracy in terms of the code results.Algorithm 1 describes the detailed pseudocode for creating this dataset.We open-sourced this dataset to the research community with the MIT license.2 .</p>
<p>PROPER: Prolog Permutation</p>
<p>Since Prolog predicates are permutable, inspired by XLNet (Yang et al., 2020) that performs a tokenwise permutation via attention masking, we decided to also use the permutation technique.The XLNet, via the permutation, can attend to tokens on both sides during training and thus can partially obtain the property of autoencoding while maintaining the property of autoregressive modeling.</p>
<p>Similarly, PROPER takes advantage of the permutative property of facts and goals in the Prolog programs as indicated in Figure 2.For each original program, we sample n of its permutations and mix them into the dataset.In this way, models can learn to extract predicates in the mathematical questions based on any other predicates regardless of the ordering, which more precisely reflects the nature of the Prolog language.We describe the practical details of permutation in Appendix A.2.</p>
<p>Experiments</p>
<p>Setup</p>
<p>Dataset We used the GSM8K-Prolog described in Section 3.1.We denote the corpus as D. The input format follows the instruction prompt used in Stanford Alpaca (Taori et al., 2023) (See sample prompts in Appendix A.3).We discarded samples that exceeded 512 tokens.Notably, when we used PROPER to augment the dataset, we used slightly altered input prompts for permuted samples because we found that using the same instruction for both the original ground truth codes and the permuted ones degraded the performance of the model.A likely reason is that having multiple correct output tokens for the same input instruction confuses the model.In addition, besides the GSM8K's test set, GSM-HARD (Gao et al., 2023), which replaces the numbers in the GSM8K test set with large numbers and thus makes questions hard for language models, was also used for evaluation.</p>
<p>Training We experimented with different LLMs' 7B versions, including Llama2 (Touvron et al., 2023), CodeLlama (Rozière et al., 2023) and Mistral (Jiang et al., 2023).We adopted 8-bit quantization and LoRA (Hu et al., 2021) to finetune models efficiently at a reasonable performance cost.We applied LoRA to finetune query and value weight matrices in the transformer blocks.We experimented with different LoRA rank and alpha settings, including (r, α) = (8, 16), (16, 32), and (32, 64).With more trainable parameters, r = 32, α = 64 yielded significantly better results, which we thereby adopted as the configuration for all the experiments.Note that this setting resulted in training only 0.248% of the 7 billion parameters for Llama2 and CodeLlama, and 0.188% of the 7 billion for Mistral.We document our training details and GPU usage in Appendix A.5.</p>
<p>Evaluation At inference time, we used beam search with a beam size of 4 to generate the Prolog code.We then used the PySwip library3 , a foreign interface of Prolog in Python, as the Prolog interpreter to produce the final answer.We used accuracy as the metric for evaluation.It is defined as
Acc = |Dtest| i=1 1 P(a pred i )=P(a true i ) |D test | × 100%
where P denotes the Prolog interpreter.Notably, since we noticed that the PySwip library cannot handle decimal answers, we only considered the samples with an integer answer.</p>
<p>Results</p>
<p>Prolog generation performs consistently better than CoT across three models.Table 2: Accuracy(%) results on GSM8K with different permutation ratios.We report both the best and average accuracy of 1:1 and 1:2 over three trials with different randomly permuted data in the form of max (avg).Note that the 1:0 case essentially means not applying PROPER.from CoT to Prolog generation, which is potentially attributed to its pretraining on the code-related corpus.In other words, CodeLlama is specifically trained to generate structured programs better than natural language reasoning.</p>
<p>With a proper permutation ratio, PROPER further enhances LLM's arithmetic reasoning with Prolog generation.Permutation ratio refers to the ratio between original samples and permuted samples.As shown in Table 2, by adding two permuted samples for each original sample, we observed an increased accuracy of 9.5% and 4.0% of Llama-2 and CodeLlama respectively on the test set.This improvement indicates that learning the non-sequential structure of Prolog predicates is helpful for LLMs to generate correct Prolog programs to solve arithmetic problems.On the other hand, the lowered accuracy of Mistral, compared with its case of one permutation per sample, suggests that PROPER might be limited for models already with high Prolog generation capacity.</p>
<p>Lowered validation loss from PROPER does not lead to higher accuracy.As is shown in Figure 3, increasing the permutation ratio results in significantly lowered validation loss.This is because we first added in permutations and then split a validation set from the training set.Consequently, the permutations of validation samples were included in the training set and the generalization ability of the language models enabled the models to utilize the permutations to improve the performance on the validation set, causing a soft data leakage.Therefore, according to Table 2, the permutation ratio of 1:2 yielded a weakened performance on Mistral although the validation loss was the lowest.Increased validation loss from PROPER does not lead to decreased validation accuracy.Excluding the permutations of validation samples from the training set, we report both the cross-entropy loss and the accuracy on the validation set for Llama2 using different methods in Figure 4.A mismatch between the loss and accuracy is observed.As a loss curve decreases to the minimum and bounces back, the corresponding accuracy curve keeps increasing and then maintains a high level.As is shown in Table 3, by choosing checkpoints based on validation accuracy instead of validation loss, the performance can be improved across all methods.Moreover, the improvement for the Prolog and PROPER method is significantly greater than that of CoT, suggesting a larger divergence between the objective of cross entropy loss and the ultimate accuracy of Prolog generation.Therefore, it is suggested to choose the best checkpoint based on the  validation accuracy.Nevertheless, the new performance is similar to the initial results where leakage is involved.We notice that late checkpoints yield better performance according to the validation accuracy and the validation loss keeps decreasing in the initial setting.Therefore, both settings happen to pick late checkpoints, resulting in similar performance.</p>
<p>We have also tested Python generation, for which the corpus was generated by the same procedure as Algorithms 1 except that we prepare Python codes instead of Prolog codes.It gives an accuracy of 55.12% on GSM8K using Llama2 as the base model, better than both Prolog and PROPER.One possible reason is that Python now is the prevalent programming language and Llama2 might have been pretrained on a large amount of Python codes.We believe if sufficient Prolog codes are used for training, Prolog generation can at least match up with Python generation due to its essence of symbolic reasoning.</p>
<p>We present some representative error cases of Mistral (1:1) in Appendix A.4.</p>
<p>Related Work</p>
<p>Arithmetic Reasoning The Chain-of-Thought (CoT) prompting approach (Wei et al., 2022b) first proposes to prompt the model to generate the reasoning chain step-by-step to reach the final answer.Afterwards, advancements have been made in LLMs' reasoning capacity via step-by-step methods (Zhou et al., 2023;Zhu et al., 2023;Huang et al., 2022;Liang et al., 2023).However, the natural language generation still performs poorly on complex or multi-step reasoning.Therefore, one trajectory of efforts has been made to leverage reasoning structures like trees (Yao et al., 2023;Long, 2023) and graphs (Besta et al., 2023;Zhang et al., 2023).Another trajectory is to render the reasoning task based on external tools (Cobbe et al., 2021;Mishra et al., 2023;Gou et al., 2023;Gao et al., 2023;Shao et al., 2023;Chen et al., 2023), which is the one that we are following.Besides, Yuan et al.'s (2023) RFT method shares the idea of dataset augmentation, but they compile rejection samples from multiple models to form an augmented training set, which is different from PROPER's automatic permutation.</p>
<p>Neural Symbolic Reasoning Neural symbolic reasoning (Andreas et al., 2016;Neelakantan et al., 2017;Hudson and Manning, 2019;Gupta et al., 2020;Nye et al., 2021) aims to leverage both neural networks and symbolic reasoning to obtain better reasoning abilities and transparency.Those methods suffer from low scalability of learning and reasoning components.LLMs are hence adopted to generate symbolic representations from natural language (Lyu et al., 2023;Pan et al., 2023;Yang et al., 2023), where deterministic symbolic solvers will process the query and symbolic representations generated by LLMs to conduct reasoning or proofs.Prolog has been a popular candidate for the format of symbolic representations.We are posited on this trajectory and in the specific field of arithmetic reasoning.</p>
<p>Conclusion</p>
<p>In conclusion, we aim to enhance the reasoning performance of LLMs.We adopt the pipeline that the model generates Prolog predicates from a mathematical question in natural language and an external Prolog interpreter processes the query for a final result.We contribute an open-sourced corpus named GSM8K-Prolog, which is a high-quality Prolog-annotated version of GSM8K.We show that Prolog generation substantially outperformed CoT generation across all three 7B models for solving arithmetic reasoning problems.We also propose PROPER, a data augmentation method designed specifically for Prolog code generation, which enables the finetuned models to learn the nonsequential nature of Prolog predicates.PROPER further improves the model's accuracy on GSM8K-Prolog and mitigates early convergence during training.Lastly, due to the gap between crossentropy loss objective and accuracy, we suggest using validation accuracy instead of validation loss to pick the best checkpoint.</p>
<p>Limitations</p>
<p>Although we have experimentally conducted fullparameter finetuning, the result was not satisfying.We believe it is because of the limited size of the original corpus.Therefore, at the current stage, we cannot have a comparison with other methods like ToRA (Gou et al., 2023) or RFT (Yuan et al., 2023).Future research can look into preparing a larger and more diverse corpus adapted to Prolog code generation.Besides, We did not try scaling the base model to more than 7B parameters.So we do not know the impact of model scaling on the performance of Prolog code generation for arithmetic reasoning.Furthermore, due to the limitation of the PySwip library, solvable questions are restricted to the ones with an integer answer.Future work can expand the domain by using other interpreting tools.</p>
<p>A Appendix</p>
<p>A.1 Generation Procedure of GSM8K-Prolog Below is the detailed pseudo-code for the GSM8K-Prolog dataset generation.</p>
<p>Algorithm 1 Procedure of GSM8K-Prolog Generation Input: The original GSM8K dataset, denoted as set X = {(q i , a CoT i )} N i=1 , where each sample consists of one question q i and one Chain-of-Thought answer a CoT i ; A Prolog interpreter P that returns the output of a Prolog program; A Chain-of-Thought answer retriever C that parses out the final answer of a natural language reasoning chain.
Output: GSM8K-Prolog dataset D = {(q i , a Prolog i )} N i=1
Initialize a set of indices I ← {1, • • • , N }, a static instruction prompt in the new dataset p ins , and an initial question for querying OpenAI API q gen .Manually craft 10 correct Prolog codes {a
Prolog i } 10 i=1 that correctly solve {q i } 10 i=1 in X to initialize D for i ∈ I do Retrieve a sample (q i , a CoT i ) ∈ X Prompt GPT-4 with {q gen } ∪ {(q k , a CoT k , a Prolog k ) 10 k=1 } ∪ {q i , a CoT i } to obtain a Prolog i if P(a Prolog i ) = C(a CoT i ) then D ← D ∪ {(p ins , q i , aPrologfixed = {(q k , a CoT k , a Prolog k ) k / ∈I }, |Q fixed | = 10. for j = 1, . . . , M do // M trial attempts for i ∈ I do Retrieve a sample (q i , a CoT i ) ∈ X Sample Q random ← {(q k , a CoT k , aProlog} ∪ Q f ixed ∪ Q random ∪ {q i , a CoT i } to obtain a Prolog i if P(a Prolog i ) = C(a CoT i ) then D ← D ∪ {(p ins , q i , a Prolog i )} I ← I \ {i} end if end for end for if I ̸ = ∅ then Manually correct Prolog codes {a Prolog i } i∈I that solve {q i } i∈I D ← D ∪ {(p ins , q i , a Prolog i ) i∈I } end if A.</p>
<p>Permutation procedures</p>
<p>Permutations can be performed both on the level of facts or rules and on the level of goals in a rule.In practice, for each piece of code, we first permute the goals in the solve<answer>:-<goal_1>,<goal_2>, ... predicate.Since the total number of permutations is sensitive to the number of goals and can easily grow to a large magnitude, thus running out of memory, we used the permutation method in the itertools library to yield an iterator over the permutations.Then, we took up to 10 goal permutations from the iterator.If there were less than 10 goal permutations in total because the code was concise and there were not many goals, we took as many goal permutations as possible.Then, in the same manner, we took up to 10 fact and rule permutations.In principle, there would be at most 100 permuted samples generated for one original sample.Then, for each sample, while conducting an experiment that required a certain number of permutations, we randomly sampled permutations from the set of permutations of size up to 100.For some sample, if the target number of permutations exceeded the total permutations it had, we took all its permutations instead.</p>
<p>A.3 Instruction Prompt Samples</p>
<p>Below are the instruction prompts we used for different training settings (CoT, Prolog, and Permuted Prolog).</p>
<p>A.4 Error Analysis</p>
<p>In this section, we present some typical errors made by our best-performing model to understand the behavior and bottleneck of LLM generation of prolog programs to solve arithmetic reasoning tasks.We sampled 70 problematic Prolog codes generated by finetuned Mistral using the 1:1 permutation setting, which had yielded the highest accuracy, that had errors and manually checked the type of errors using the Swi-Prolog interpreter.100% of them had semantic errors, meaning their logic of solution was wrong.7% of them had syntax errors, meaning the Swi-Prolog yields syntax errors on those codes.Some examples of typical syntax errors are listed below.The lines causing errors are in bold.The details of the errors are explained in the comments.</p>
<p>Figure 1 :
1
Figure 1: Overview of Prolog generation for arithmetic reasoning with large language models.</p>
<p>Figure 3 :
3
Figure 3: Validation loss curves for training Llama2, CodeLlama, and Mistral with different permutation ratios (We only report the first trial when we use permuted data since the loss curves are very similar across trials).</p>
<p>Figure 4 :
4
Figure 4: Validation loss curves and validation accuracy curves for training Llama2 with different methods (We only report the first trial when we use permuted data since the loss and accuracy curves are very similar across trials).</p>
<p>Manually select the top 10 clean and logical Prolog code from the current D to form a new few-shot sample set Q</p>
<p>k ) k / ∈I }, |Q random | = 10 from D // Adding 10 dynamic samples and 10 fixed samples into the 20-shot prompt.Prompt GPT-4 with {q gen</p>
<p>arXiv:2405.17893v1 [cs.CL] 28 May 2024 tions and formulate them into Prolog code.If the facts and rules are accurately captured, a Prolog interpreter can precisely solve for a correct answer in a deterministic way.</p>
<p>Table 1 :
1
Accuracy results on the GSM8K and GSM-HARD datasets.We compare regular Prolog generation (Prolog) and PROPER Prolog generation with the CoT baseline (supervised finetuning with LoRA using CoT ground truth labels in the original GSM8K dataset).
MethodGSM8KLlama-2 GSM-HARDCodeLlama GSM8K GSM-HARDGSM8KMistral GSM-HARDCoT33.8%12.0%37.5%13.9%58.9%30.8%Prolog41.5%32.4%55.0%41.6%66.3%50.6%PROPER51.0%37.4%59.0%45.9%70.2%54.4%</p>
<p>Table 3 :
3
Accuracy(%) results of training Llama2 on the GSM8K dataset.We compare the results of avoiding validation sample leakage in the training set and picking the optimal checkpoint based on validation loss and accuracy with the initial results with leakage.The best and average accuracy of 1:1 and 1:2 are in the form of max (avg).</p>
<p>instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.### Instruction: Please generate an explanatory answer to solve the given math problem.instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.### Instruction: Please generate a piece of Prolog code to solve the given math problem.
SettingPrompt TemplateCoTBelow is an ### Input:<Question>### Output:<CoT Reasoning>Prolog GenerationBelow is an ### Input:<Question>### Output:<Prolog Code>Permuted Prolog GenerationBelow is an instruction that describes a task, paired with an input that provides furthercontext. Write a response that appropriately completes the request.### Instruction:Please generate a piece of Prolog code in non-sequential order to solve the given mathproblem.### Input:<Question>### Output:<Prolog Code>
https://platform.openai.com/docs/guides/ text-generation/chat-completions-api
https://huggingface.co/datasets/ Thomas-X-Yang/gsm8k-prolog
Prolog version 9.0.4. PySwip version 0.2.11. https: //github.com/yuce/pyswip
Question Prolog CodeJulia's boat sprang a leak while she was out on the lake.The boat was taking on two liters of water for every ten feet she rowed back towards shore.It took her sixteen seconds to row twenty feet.The shore was 64 seconds away.How much water had the boat taken on by the time Julia reached shore?:-use_module(library(clpq)).liters_per_10_feet(julia, 2).time_per_20_feet(julia, 16).distance_to_shore(julia, 64).solve(Water_taken_on) :-liters_per_10_feet(julia, Liters_per_10_feet), time_per_20_feet(julia, Time_per_20_feet), distance_to_shore(julia, Distance_to_shore), {Seconds_per_10_feet = Time_per_20_feet / 2}, {Total_seconds = Distance_to_shore / 10 * Seconds_per_10_feet}, {Total_feet_rowed = Total_seconds / Seconds_per_20_feet * 20}, % The variable has never been defined.{Total_liters_taken_on = Total_feet_rowed * Liters_per_10_feet}, {Water_taken_on = Total_liters_taken_on}.Bill bakes 300 rolls, 120 chocolate croissants, and 60 baguettes every day.Each roll is 4 inches long, each croissant is 6 inches long, and each baguette is two feet long.If Bill puts all the baked goods end to end, how long will they be in feet?:-use_module(library(clpq)). bake(bill, rolls, 300).bake(bill, chocolate_croissants, 120).bake(bill, baguettes, 60).length(roll, 4).# The predicate has already been reserved byProlog. length(chocolate_croissant,6).length(baguette, 2).solve(Total_length) :bake(bill, rolls, Num_rolls), bake(bill, chocolate_croissants, Num_chocolate_croissants), bake(bill, baguettes, Num_baguettes), length(roll, Roll_length), length(chocolate_croissant, Chocolate_croissant_length), length(baguette, Baguette_length), {Roll_total_length = Num_rolls * Roll_length}, {Chocolate_croissant_total_length = Num_chocolate_croissants * Chocolate_croissant_length}, {Baguette_total_length = Num_baguettes * Baguette_length}, {Total_length = Roll_total_length + Chocolate_croissant_total_length + Baguette_total_length}. % This code also contains a semantic error because it fails to convert the unit of inch to feet.In a race with 240 Asians, 80 were Japanese, and the rest were Chinese.If the number of boys on the Chinese team was 60, how many girls were on the Chinese team?:-use_module(library(clpq)).total_participants(240). japanese_participants(80). chinese_boys(60).num_tests(mr_bert, 6).lowest_score_removal(mr_bert, 1).target_average(mr_bert, 93).scores(brinley, [89, 71, 92, 100, 86]).solve(Test_score) :-num_tests(mr_bert, Num_tests), lowest_score_removal(mr_bert, Lowest_score_removal), target_average(mr_bert, Target_average), scores(brinley, Scores), Length is Num_tests -Lowest_score_removal, {Total_score = sum(Scores)}, % The built-in predicate is misused.{Average_score = Total_score / Length}, {Test_score = (Target_average * Length) -Total_score}.A.5 Training Details and Computational BudgetDuring finetuning, we controlled the number of epochs to be 6, batch size to be 128, and learning rate to be 3 × 10 −4 .For a single training run, we used 2 NVIDIA RTX 4090 GPUs to finetune Llama2 and CodeLlama and 2 NVIDIA RTX 8000 GPUs to finetune Mistral.We adopted Distributed Data Parallelism to speed up training.Training on the original CoT data in GSM8K or the non-permuted Prolog code data took around 2 hours on 2 NVIDIA RTX 4090 GPUs and around 10 hours on 2 NVIDIA RTX 8000 GPUs.When we added in permuted samples, the training time grew proportionally with the dataset size since we controlled the number of epochs and batch size.During inference on the test set, we used a batch size of 2 on an RTX 4090 GPU, which took around 6 hours to finish a full inference round, and a batch size of 3 on one RTX 8000 GPU, which took around 7 hours to finish a full inference round.
Daniel Heslow. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model. Julien Launay, Quentin Malarticwith state-of-the-art performance</p>
<p>Neural module networks. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, 10.1109/CVPR.2016.122016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, 2023Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler</p>
<p>Prolog programming for Artificial Intelligence. Ivan Bratko, 2012Addison-Wesley</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 2023</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Mark Agrawal, Omernick, M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas EckJeff Dean, Slav Petrovand Noah Fiedel. 2022. Palm: Scaling language modeling with pathways</p>
<p>Programming in Prolog. W F Clocksin, C S Mellish, 2003Springer-Verlag</p>
<p>. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021Training verifiers to solve math word problems</p>
<p>Natural language processing for Prolog programmers. A Michael, Covington, 2002Prentice Hall</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, Pal: Program-aided language models. 2023</p>
<p>Tora: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, 2023</p>
<p>Neural module networks for reasoning over text. Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, Matt Gardner, International Conference on Learning Representations. 2020</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, Lora: Low-rank adaptation of large language models. 2021</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, 2022</p>
<p>Learning by abstraction: The neural state machine. Drew Hudson, Christopher D Manning, Advances in Neural Information Processing Systems. Curran Associates, Inc201932</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, 2023</p>
<p>Large language model guided tree-ofthought. Jieyi Long, 2023</p>
<p>Faithful chain-ofthought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, 2023</p>
<p>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan, Lila: A unified benchmark for mathematical reasoning. 2023</p>
<p>Learning a natural language interface neural programmer. Arvind Neelakantan, Quoc V Le, Martin Abadi, Andrew Mccallum, Dario Amodei, International Conference on Learning Representations. 2017</p>
<p>Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Maxwell Nye, Michael Henry Tessler, Joshua B Tenenbaum, Brenden M Lake, 2021</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , 2023</p>
<p>Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. 2021arXiv preprint</p>
<p>. Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Canton Bhatt, Aaron Ferrer, Wenhan Grattafiori, Alexandre Xiong, Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialomand Gabriel Synnaeve. 2023. Code llama: Open foundation models for code</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Synthetic prompting: Generating chain-of-thought demonstrations for large language models. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLR2023202</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, 2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Neuro-symbolic integration brings causal and reliable reasoning proofs. Sen Yang, Xin Li, Leyang Cui, Lidong Bing, Wai Lam, 2023</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, V Quoc, Le, 2020</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, Chang Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. 2023arXiv preprint</p>
<p>Cumulative reasoning with large language models. Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao, 2023</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least-to-most prompting enables complex reasoning in large language models. </p>
<p>Solving math word problems via cooperative reasoning induced language models. Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang, 10.18653/v1/2023.acl-long.245Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20231</p>            </div>
        </div>

    </div>
</body>
</html>
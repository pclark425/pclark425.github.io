<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1475 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1475</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1475</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-1460b2073fc915c496021cc40613a87d9b46fa51</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1460b2073fc915c496021cc40613a87d9b46fa51" target="_blank">Virtual vs. real: Trading off simulations and physical experiments in reinforcement learning with Bayesian optimization</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> Entropy Search is extended, a Bayesian optimization algorithm that maximizes information gain from each experiment, to the case of multiple information sources, and the result is a principled way to automatically combine cheap, but inaccurate information from simulations with expensive and accurate physical experiments in a cost-effective manner.</p>
                <p><strong>Paper Abstract:</strong> In practice, the parameters of control policies are often tuned manually. This is time-consuming and frustrating. Reinforcement learning is a promising alternative that aims to automate this process, yet often requires too many experiments to be practical. In this paper, we propose a solution to this problem by exploiting prior knowledge from simulations, which are readily available for most robotic platforms. Specifically, we extend Entropy Search, a Bayesian optimization algorithm that maximizes information gain from each experiment, to the case of multiple information sources. The result is a principled way to automatically combine cheap, but inaccurate information from simulations with expensive and accurate physical experiments in a cost-effective manner. We apply the resulting method to a cart-pole system, which confirms that the algorithm can find good control policies with fewer experiments than standard Bayesian optimization on the physical system only.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1475.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1475.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulink (Quanser nonlinear model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nonlinear Simulink model of the Quanser Linear Inverted Pendulum (manufacturer provided)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manufacturer-provided nonlinear Simulink model of the Quanser self-erecting single inverted pendulum used as a simulator to provide cheap, approximate evaluations of controller performance for Bayesian optimization (MF-ES). It is used together with a GP-based error model to trade simulation information against real experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Virtual vs. Real: Trading Off Simulations and Physical Experiments in Reinforcement Learning with Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Nonlinear Simulink model (manufacturer provided) for the Quanser Linear Inverted Pendulum</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A nonlinear dynamics model implemented in Matlab/Simulink, provided by the hardware manufacturer, used to run 30 s simulated roll-outs of a parameterized LQR controller to compute a simulated cost J_sim(θ). It serves as a fast, approximate predictor of the real system behavior and is explicitly modeled as biased/noisy relative to the real platform.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / control systems / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity approximate dynamics model (nonlinear Simulink implementation): realistic dynamical equations provided by manufacturer but not identical to the real hardware; treated as an imperfect model with systematic discrepancy to the real system.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Simulates discrete-time state transitions of the cart-pole (states s, ψ, ˙s, ˙ψ) over 30 s roll-outs; noise level set substantially lower than real experiments (η_sim = 1e-5 vs η_exp = 2.08e-4); systematic bias observed (simulated costs generally lower than real costs), modeled via an additive GP error term J_err(θ); prior means set pessimistically (m_sim = 0.04, m_err = 0.02); uses rational-quadratic kernels for both simulation and error components; no explicit claim of modeling e.g. sensor/actuator delay fidelity or high-frequency effects, timestep not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LQR-parameterized controller optimized by Multifidelity Entropy Search (MF-ES)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Controller parameterization: static state-feedback controller u_k = F x_k where F is obtained from a discrete LQR design with two tunable weight parameters θ ∈ ℝ^2; optimization method is a GP-based Bayesian optimization (MF-ES) that chooses evaluations in simulation or on the real system.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Optimize controller parameters (LQR weights) to minimize a time-averaged stabilizing cost for the cart-pole (balancing) task; i.e., model-based policy search / reinforcement learning for control tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world physical Quanser Linear Inverted Pendulum (physical experiments / hardware)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Final selected controller evaluated on real hardware achieved cost J(θ_bg) = 0.0194; across 10 runs MF-ES (sim+exp) produced controllers that were on average 33.23% better than ES (exp-only). MF-ES required on average 2.7 physical experiments (±) and 11.9 simulations, versus ES requiring 3.5 physical experiments on average; ES failed to find stabilizing controller in 4/10 runs while MF-ES consistently found stabilizing controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Paper compares two fidelity levels: the simulator (approximate Simulink model) and the real system. Key result: leveraging the (biased but cheaper) simulator via MF-ES reduces the required number of physical experiments (≈22.86% fewer on average) and yields better final real-world controllers (≈33.23% improvement on average) compared to Bayesian optimization using only physical experiments (ES). The MF-ES GP explicitly models simulator error (J_err) so that simulator information is used proportionally to its modeled reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit quantitative minimal-fidelity threshold is given. The paper emphasizes that simulator quality is modeled (via separate GP kernel for J_sim and J_err) and that MF-ES automatically adapts: if the simulator is reliable the optimizer exploits it heavily (many simulations), otherwise it favors real experiments. It notes that hyperparameters of the kernels may be optimized and that the method can revert to experiments if simulation is not informative.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit reported case where transfer failed due to low simulator fidelity; however the authors observe a systematic bias (simulated costs lower than real) which is addressed by the additive error GP. They also note that inappropriate setting of simulation 'effort' (cost) could lead to suboptimal trade-offs (e.g., setting effort too high may cause unnecessary real experiments), but no catastrophic transfer failures from low fidelity are documented in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Virtual vs. real: Trading off simulations and physical experiments in reinforcement learning with Bayesian optimization', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient reinforcement learning for robots using informative simulated priors <em>(Rating: 2)</em></li>
                <li>Real-world reinforcement learning via multifidelity simulators <em>(Rating: 2)</em></li>
                <li>Multi-Information Source Optimization with General Model Discrepancies <em>(Rating: 2)</em></li>
                <li>Predicting the output from a complex computer code when fast approximations are available <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1475",
    "paper_id": "paper-1460b2073fc915c496021cc40613a87d9b46fa51",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Simulink (Quanser nonlinear model)",
            "name_full": "Nonlinear Simulink model of the Quanser Linear Inverted Pendulum (manufacturer provided)",
            "brief_description": "A manufacturer-provided nonlinear Simulink model of the Quanser self-erecting single inverted pendulum used as a simulator to provide cheap, approximate evaluations of controller performance for Bayesian optimization (MF-ES). It is used together with a GP-based error model to trade simulation information against real experiments.",
            "citation_title": "Virtual vs. Real: Trading Off Simulations and Physical Experiments in Reinforcement Learning with Bayesian Optimization",
            "mention_or_use": "use",
            "simulator_name": "Nonlinear Simulink model (manufacturer provided) for the Quanser Linear Inverted Pendulum",
            "simulator_description": "A nonlinear dynamics model implemented in Matlab/Simulink, provided by the hardware manufacturer, used to run 30 s simulated roll-outs of a parameterized LQR controller to compute a simulated cost J_sim(θ). It serves as a fast, approximate predictor of the real system behavior and is explicitly modeled as biased/noisy relative to the real platform.",
            "scientific_domain": "mechanics / control systems / robotics",
            "fidelity_level": "medium-fidelity approximate dynamics model (nonlinear Simulink implementation): realistic dynamical equations provided by manufacturer but not identical to the real hardware; treated as an imperfect model with systematic discrepancy to the real system.",
            "fidelity_characteristics": "Simulates discrete-time state transitions of the cart-pole (states s, ψ, ˙s, ˙ψ) over 30 s roll-outs; noise level set substantially lower than real experiments (η_sim = 1e-5 vs η_exp = 2.08e-4); systematic bias observed (simulated costs generally lower than real costs), modeled via an additive GP error term J_err(θ); prior means set pessimistically (m_sim = 0.04, m_err = 0.02); uses rational-quadratic kernels for both simulation and error components; no explicit claim of modeling e.g. sensor/actuator delay fidelity or high-frequency effects, timestep not specified.",
            "model_or_agent_name": "LQR-parameterized controller optimized by Multifidelity Entropy Search (MF-ES)",
            "model_description": "Controller parameterization: static state-feedback controller u_k = F x_k where F is obtained from a discrete LQR design with two tunable weight parameters θ ∈ ℝ^2; optimization method is a GP-based Bayesian optimization (MF-ES) that chooses evaluations in simulation or on the real system.",
            "reasoning_task": "Optimize controller parameters (LQR weights) to minimize a time-averaged stabilizing cost for the cart-pole (balancing) task; i.e., model-based policy search / reinforcement learning for control tuning.",
            "training_performance": null,
            "transfer_target": "Real-world physical Quanser Linear Inverted Pendulum (physical experiments / hardware)",
            "transfer_performance": "Final selected controller evaluated on real hardware achieved cost J(θ_bg) = 0.0194; across 10 runs MF-ES (sim+exp) produced controllers that were on average 33.23% better than ES (exp-only). MF-ES required on average 2.7 physical experiments (±) and 11.9 simulations, versus ES requiring 3.5 physical experiments on average; ES failed to find stabilizing controller in 4/10 runs while MF-ES consistently found stabilizing controllers.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Paper compares two fidelity levels: the simulator (approximate Simulink model) and the real system. Key result: leveraging the (biased but cheaper) simulator via MF-ES reduces the required number of physical experiments (≈22.86% fewer on average) and yields better final real-world controllers (≈33.23% improvement on average) compared to Bayesian optimization using only physical experiments (ES). The MF-ES GP explicitly models simulator error (J_err) so that simulator information is used proportionally to its modeled reliability.",
            "minimal_fidelity_discussion": "No explicit quantitative minimal-fidelity threshold is given. The paper emphasizes that simulator quality is modeled (via separate GP kernel for J_sim and J_err) and that MF-ES automatically adapts: if the simulator is reliable the optimizer exploits it heavily (many simulations), otherwise it favors real experiments. It notes that hyperparameters of the kernels may be optimized and that the method can revert to experiments if simulation is not informative.",
            "failure_cases": "No explicit reported case where transfer failed due to low simulator fidelity; however the authors observe a systematic bias (simulated costs lower than real) which is addressed by the additive error GP. They also note that inappropriate setting of simulation 'effort' (cost) could lead to suboptimal trade-offs (e.g., setting effort too high may cause unnecessary real experiments), but no catastrophic transfer failures from low fidelity are documented in the experiments.",
            "uuid": "e1475.0",
            "source_info": {
                "paper_title": "Virtual vs. real: Trading off simulations and physical experiments in reinforcement learning with Bayesian optimization",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient reinforcement learning for robots using informative simulated priors",
            "rating": 2
        },
        {
            "paper_title": "Real-world reinforcement learning via multifidelity simulators",
            "rating": 2
        },
        {
            "paper_title": "Multi-Information Source Optimization with General Model Discrepancies",
            "rating": 2
        },
        {
            "paper_title": "Predicting the output from a complex computer code when fast approximations are available",
            "rating": 2
        }
    ],
    "cost": 0.009289249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Virtual vs. Real: Trading Off Simulations and Physical Experiments in Reinforcement Learning with Bayesian Optimization</h1>
<p>Alonso Marco ${ }^{1,5}$, Felix Berkenkamp ${ }^{2,5}$, Philipp Hennig ${ }^{1,5}$, Angela P. Schoellig ${ }^{3}$, Andreas Krause ${ }^{2,5}$, Stefan Schaal ${ }^{1,4,5}$, and Sebastian Trimpe ${ }^{1,5}$</p>
<h4>Abstract</h4>
<p>In practice, the parameters of control policies are often tuned manually. This is time-consuming and frustrating. Reinforcement learning is a promising alternative that aims to automate this process, yet often requires too many experiments to be practical. In this paper, we propose a solution to this problem by exploiting prior knowledge from simulations, which are readily available for most robotic platforms. Specifically, we extend Entropy Search, a Bayesian optimization algorithm that maximizes information gain from each experiment, to the case of multiple information sources. The result is a principled way to automatically combine cheap, but inaccurate information from simulations with expensive and accurate physical experiments in a cost-effective manner. We apply the resulting method to a cart-pole system, which confirms that the algorithm can find good control policies with fewer experiments than standard Bayesian optimization on the physical system only.</p>
<h2>I. INTRODUCTION</h2>
<p>Typically, the control policies that are used in robotics depend on a small set of tuning parameters. To achieve the best performance on the real system, these parameters are usually tuned manually in experiments on the physical platform. Policy search methods in reinforcement learning aim to automate this process [1]. However, without prior knowledge, these methods can require significant amounts of experimental time before determining optimal, or even only reasonable parameters. In robotics, simulation models of the robotic system are usually available, e.g., as a by-product of the design process. While exploiting knowledge from simulation models has been considered before, no principled way to trade off between the relative costs and accuracies of simulations and experiments exists [2]. As a result, state-of-the-art reinforcement learning methods require more experimental time on the real system than necessary.</p>
<p>In this paper, we propose a new reinforcement learning method that can automatically optimize the parameters of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. The proposed algorithm optimizes the parameters $\boldsymbol{\theta}$ of a control policy based on data of a cheap, but inaccurate simulation and expensive data from the real system. By actively trading off between the information that can be gained from each system relative to their costs, the algorithm requires significantly fewer evaluations on the physical system.
control policies based on data from different information sources, such as simulations and experiments. Specifically, we use an extension of Entropy Search [3], [4], a Bayesian optimization framework for information-efficient global optimization. The resulting method automatically trades off the amount of information gained from different sources with their respective costs and requires fewer physical experiments to determine optimal parameters (see Fig. 1).</p>
<p>Related work: Improving the performance of reinforcement learning with prior model information from a simulator has been considered before. A typical approach is twostage learning, where algorithms are trained for a certain amount of time in simulation in order to warm-start the learning on the real robot [2]. For example, [5] reports performance improvements when using model information from simulation as a prior for real experiments. Transfer learning is a similar approach that aims to generalize between different tasks, rather than from a simulated model to the real system [6]. The work in [7] learns an optimal policy and value function of a finite Markov decision process based on models with different accuracies. They rely on hierarchical models and switch to higher accuracy models once a threshold accuracy has been reached at a lower level. A commonly used reinforcement learning method is policy gradients [8], where policy parameters are improved locally along the gradient. In this setting, simulation knowledge can be used to estimate the gradient of real experiments [9]. However, policy gradient methods only converge to locally optimal parameters. None of the above methods explicitly considers the cost of experiments on a robot. In this paper, we actively trade off the different costs and information gains associated with simulation and real experiments and obtain</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>globally optimal parameter estimates.</p>
<p>A method that has been particularly successful for parameter optimization in robotics is Bayesian optimization [10]. In particular, methods based on Gaussian process (GP, [11]) models are widely used because of their ability to determine globally optimal parameters within few evaluations. Examples include gait optimization in legged robots [12] and controller optimization for a snake-like robot [13]. In [14], the controller parameters of a linear state-feedback controller were optimized using the LQR framework as a low-dimensional representation of controller policies, while in [15] the control policy itself was defined by Bayesian optimization with a specifically chosen kernel. Safety constraints on the robot during the optimization process were considered in [16]. A comparison of different Bayesian and nonBayesian global optimization methods can be found in [17]. All the previous methods use Bayesian optimization directly on the real system. In contrast, we consider an extension of Bayesian optimization that can extract additional information from a simulator and speed up the optimization process.</p>
<p>The methodology herein is related to multi-task Bayesian optimization, where one aims to transfer knowledge about two related tasks [18], [19]. A GP model with multiple information sources was first considered in [20]. Since then, optimization with multiple information sources has been considered under strict requirements, such as models forming a hierarchy of increasing accuracy and without considering different costs [21], [22]. More recently, [23] used a myopic policy, called the ‘knowledge gradient’ by [24], in order determine, which parameters to evaluate.</p>
<p>Our contribution: In this paper, we present a Bayesian optimization algorithm for multiple information sources. We use entropy [3], [4] to measure the information content of simulations and experiments. Since this is an appropriate unit of measure for the utility of both sources, our algorithm is able to compare physically meaningful quantities in the same units on either side, and trade off accuracy for cost. As a result, the algorithm can automatically decide whether to evaluate cheap, but inaccurate simulations or perform expensive and precise real experiments. We apply the method to optimize the policy of a cart-pole system and show that this approach can speed up the optimization process significantly compared to standard Bayesian optimization [14]. The main contributions of the paper are (i) a novel Bayesian optimization algorithm that can trade off between costs of multiple information sources and (ii) the first application of such a framework to the problem of reinforcement learning and optimization of controller parameters.</p>
<p>For convenience within the next sections, we rename the concepts accuracy and cost: We now refer to the lack of accuracy of a controller as cost. Furthermore, we now denominate the cost of retrieving an evaluation from a specific information source as effort.</p>
<h2>II. Problem Statement</h2>
<p>We consider a reinforcement learning setting, where we aim to find an optimal policy to complete a certain task on a dynamic system. While we do not have access to a perfect model of the system, we assume that a control policy is available, which is parameterized by parameters $\boldsymbol{\theta}$ within some domain $\mathcal{D}$. The goal is to determine the optimal parameters $\boldsymbol{\theta}_{\min}$ that globally minimize the cost of a task,</p>
<p>$$
\boldsymbol{\theta}_{\min } \in \underset{\boldsymbol{\theta} \in \mathcal{D}}{\operatorname{argmin}} J(\boldsymbol{\theta})
$$</p>
<p>The cost $J(\boldsymbol{\theta})$ measures the performance of the policy on a certain task on the real system. For example, one evaluation of the cost function could consist of controlling a robot with the parameterized policy and measuring an error signal over a fixed time horizon. To solve the optimization problem in (1), we can query a parameter vector $\boldsymbol{\theta}<em n="n">{n}$ at each iteration $n$ and observe the resulting performance $J\left(\boldsymbol{\theta}</em>\right)$. Since these experiments cause wear in the robot and take time, the goal is to minimize the number of iterations before the optimal parameters in (1) are determined.</p>
<p>We assume that a simulation of the system is available, which we want to exploit to solve (1) more efficiently with fewer evaluations on the physical system. While simulations are only an approximation of the real world and cannot be used to determine the optimal parameters in (1) directly, they can be used to provide an estimate $J_{\text {sim }}(\boldsymbol{\theta})$ of the true cost. We use this estimate to obtain information about the location of the optimal parameters on the real system. As a result, at each iteration $n$, we do not only choose the next parameters $\boldsymbol{\theta}_{n}$ to evaluate, but also whether to perform a simulation or an experiment.</p>
<p>Both experiments, in the real world and in simulation, have physically meaningful evaluation efforts associated to them. For example, the effort may account for the amount of time required to complete an experiment/simulation and for monetary costs such as wear in the system. The overall goal is to minimize the total effort incurred in the experiments and simulations until the optimal parameters (1) on the real system are found.</p>
<h2>III. Preliminaries</h2>
<p>We start by introducing the necessary background information on GPs and Bayesian optimization.</p>
<h2>A. Gaussian Processes (GPs)</h2>
<p>While the cost $J(\boldsymbol{\theta})$ in (1) can easily be evaluated in an experiment for a given parameter $\boldsymbol{\theta}$, the functional relationship between parameters and the cost is unknown a priori. We use GPs as a nonparametric model to approximate the unknown function. The goal is to find an approximation of the nonlinear map, $J(\boldsymbol{\theta}): \mathcal{D} \mapsto \mathbb{R}$, from an input vector $\boldsymbol{\theta} \in \mathcal{D}$ to the function value $J(\boldsymbol{\theta})$. This is accomplished by modeling function values $J(\boldsymbol{\theta})$, associated with different values of $\boldsymbol{\theta}$, as random variables so that any finite number of these random variables have a joint Gaussian distribution [11].</p>
<p>For the nonparametric regression, we define a prior mean function $m(\boldsymbol{\theta})$, which encodes prior knowledge about the function $J(\cdot)$, and a covariance function $k\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime}\right)$, which defines the covariance of any two function values, $J(\boldsymbol{\theta})$ and $J\left(\boldsymbol{\theta}^{\prime}\right), \boldsymbol{\theta}, \boldsymbol{\theta}^{\prime} \in \mathcal{D}$, and is used to model uncertainty about</p>
<p>the mean estimate. The latter is also known as the kernel. The choice of kernel is problem-dependent and encodes assumptions about smoothness and rate of change of the unknown function, $J(\cdot)$.</p>
<p>The GP framework can be used to predict the function value $J\left(\boldsymbol{\theta}^{<em>}\right)$ at an arbitrary input $\boldsymbol{\theta}^{</em>}\in\mathcal{D}$, based on a set of $n$ past observations $\mathcal{D}<em i="i">{n}=\left{\boldsymbol{\theta}</em>},{\hat{J}}\left(\boldsymbol{\theta<em i="1">{i}\right)\right}</em>$. We assume that observations are noisy measurements of the true function; that is,}^{n</p>
<p>$\hat{J}(\boldsymbol{\theta})=J(\boldsymbol{\theta})+\omega(\boldsymbol{\theta}),$ (2)</p>
<p>where the noise $\omega(\boldsymbol{\theta}) \sim \mathcal{N}\left(0, \eta^{2}(\boldsymbol{\theta})\right)$ depends on the input. Conditioned on the previous observations, the mean and variance of the posterior normal distribution are</p>
<p>$$
\begin{aligned}
&amp; \mu_{n}\left(\boldsymbol{\theta}^{<em>}\right)=m\left(\boldsymbol{\theta}^{</em>}\right)+\mathbf{k}<em n="n">{n}\left(\boldsymbol{\theta}^{<em>}\right) \mathbf{K}<em n="n">{n}^{-1} \hat{\mathbf{y}}</em> \
&amp; \sigma_{n}^{2}\left(\boldsymbol{\theta}^{</em>}\right)=k\left(\boldsymbol{\theta}^{<em>}, \boldsymbol{\theta}^{</em>}\right)-\mathbf{k}</em>^{}\left(\boldsymbol{\theta<em>}\right) \mathbf{K}<em n="n">{n}^{-1} \mathbf{k}</em>^{}^{\mathrm{T}}\left(\boldsymbol{\theta</em>}\right)
\end{aligned}
$$</p>
<p>where $\hat{\mathbf{y}}<em 1="1">{n}=\left[\hat{J}\left(\boldsymbol{\theta}</em>}\right)-m\left(\boldsymbol{\theta<em n="n">{1}\right), \ldots, \hat{J}\left(\boldsymbol{\theta}</em>}\right)-m\left(\boldsymbol{\theta<em n="n">{n}\right)\right]^{\mathrm{T}}$ is the vector of observed, noisy deviations from the mean, the vector $\mathbf{k}</em>^{}\left(\mathbf{a<em>}\right)=\left[k\left(\boldsymbol{\theta}^{</em>}, \boldsymbol{\theta}<em n="n">{1}\right), \ldots, k\left(\boldsymbol{\theta}^{<em>}, \boldsymbol{\theta}_{n}\right)\right]$ contains the covariances between the new input $\boldsymbol{\theta}^{</em>}$ and the observed data points in $\mathcal{D}</em>}$, and the symmetric matrix $\mathbf{K<em n="n">{n} \in \mathbb{R}^{n \times n}$ has entries $\left[\mathbf{K}</em>\right]<em i="i">{(i, j)}=k\left(\boldsymbol{\theta}</em>}, \boldsymbol{\theta<em i="i" j="j">{j}\right)+\delta</em>\right), i, j \in{1, \ldots, n}$.} \eta^{2}\left(\boldsymbol{\theta}_{i</p>
<h2>B. Bayesian Optimization</h2>
<p>We want to use the GP model of the cost function for parameter optimization. Using statistical models of an objective function for optimization is known as Bayesian optimization [10] in the literature. It comprises a class of data-efficient optimization methods that aim to determine the global optimum of cost functions that are expensive to evaluate. In our case, each evaluation of the cost with certain controller parameters on the robot cause wear in the system and may take a long time to perform. In the case of GP models, the mean, (3), and variance, (4) can be used to determine new parameters to evaluate that are promising candidates for the global optimum. For example, [25] uses upper confidence bounds that allow for provable convergence guarantees.</p>
<p>In this paper, we build upon the Entropy Search (ES, [3]) algorithm, which selects parameters in order to maximally reduce the uncertainty about the location of the minimum of $J(\boldsymbol{\theta})$ in each step. It quantifies this uncertainty through the entropy of the distribution over the location of the minimum,</p>
<p>$$
p_{\min }(\boldsymbol{\theta})=\mathbb{P}\left(\boldsymbol{\theta} \in \underset{\boldsymbol{\theta} \in \mathcal{D}}{\operatorname{argmin}} J(\boldsymbol{\theta})\right)
$$</p>
<p>The approach becomes tractable by approximating $p_{\min }$ on a non-uniform grid, with higher resolution in areas where it is more likely to find the minimum. The key idea is that, upon convergence, we expect $p_{\min }$ to be peaked around the minima, thus to have low entropy. The rate of change in the entropy of $p_{\min }$ determines how much information about the location of the global minimum we obtain with each evaluation of the cost function. Given this metric, the optimal
parameter at which to evaluate the cost function in the next iteration, is the one that is most informative:</p>
<p>$$
\boldsymbol{\theta}_{n+1}=\underset{\boldsymbol{\theta} \in \mathcal{D}}{\operatorname{argmax}} \mathrm{E}[\Delta H(\boldsymbol{\theta})]
$$</p>
<p>where $\Delta H(\boldsymbol{\theta})$ is the change in entropy of $p_{\min }$ caused by retrieving a new cost value at location $\boldsymbol{\theta}$. Intuitively, by collecting cost values at the most informative locations (6), we keep decreasing the amount of entropy in $p_{\min }$ until eventually $p_{\min }$ is peaked around the optima. At iteration $n$, we compute the best guess $\boldsymbol{\theta}_{\mathrm{bg}}$ about the optimal parameters by minimizing the current GP posterior (3):</p>
<p>$$
\boldsymbol{\theta}<em n="n">{\mathrm{bg}}=\underset{\boldsymbol{\theta} \in \mathcal{D}}{\operatorname{argmin}} \mu</em>)
$$}(\boldsymbol{\theta</p>
<p>The computation of $\Delta H$ given the GP model of $J(\cdot)$ requires several approximations. A full derivation is beyond the scope of the paper, but all details can be found in [3].</p>
<h2>IV. REINFORCEMENT LEARNING WITH SIMULATIONS</h2>
<p>In this section, we show how the ES algorithm can be extended to multiple sources of information, such as simulations and physical experiments. The two main challenges are modeling the errors of the simulator in a principled way and trading off evaluation effort and information gain. In the following, we focus on the case where only one simulation is available for ease of exposition. However, the approach can easily be extended to an arbitrary number of information sources.</p>
<h2>A. GP Model for Multiple Information Sources</h2>
<p>To model the choice between simulation and phisical experiment, we use a specific kernel structure that is similar to the one used in [23]. The key idea is to model the cost on the real system as being partly explained through the simulator plus some error term. That is, $J(\boldsymbol{\theta})=J_{\text {sim }}(\boldsymbol{\theta})+J_{\text {err }}(\boldsymbol{\theta})$, where the true cost consists of the estimated cost of the simulation, $J_{\text {sim }}(\boldsymbol{\theta})$, and a systematic error term, $J_{\text {err }}(\boldsymbol{\theta})$. To incorporate this in the GP framework of Sec. III-A, we extend the parameter vector by an additional binary variable $\delta$, which indicates whether the cost is evaluated in simulation $(\delta=0)$ or on the physical system $(\delta=1)$. Based on the extended parameter $\mathbf{a}=(\boldsymbol{\theta}, \delta)$, we can model the cost by adapting the GP kernel to</p>
<p>$$
k\left(\mathbf{a}, \mathbf{a}^{\prime}\right)=k_{\text {sim }}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime}\right)+k_{\delta}\left(\delta, \delta^{\prime}\right) k_{\text {err }}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime}\right)
$$</p>
<p>The kernels $k_{\text {sim }}(\cdot, \cdot)$ and $k_{\text {err }}(\cdot, \cdot)$ model the cost function on the simulator and its difference to the cost on the physical system, respectively. The kernel $k_{\delta}\left(\delta, \delta^{\prime}\right)=\delta \delta^{\prime}$ is equal to one if both parameters indicate a physical experiment and zero otherwise.</p>
<p>From Sec. III-A, we know that the kernel (8) models the covariances for different parameters. Intuitively, the kernel (8) encodes that two experiments on the physical system covary strongly. However, if one of the $\delta$-variables is zero (i.e., a simulation), then the only covariances between the two values is captured by $k_{\text {sim }}$. Effectively, the error covariance is switched off in simulations in order to model that simulations</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Synthetic example of how simulations and physical experiments can be combined by trading off information and evaluation effort. In (a), top, it is shown the GP posterior conditioned on one simulation (blue dot) and one physical experiment (red dot). The GP model from Sec. IV-A encodes that a portion of the uncertainty in the cost of the real system (red shaded) can be explained through the simulator (blue shaded). The red dashed line represents the cost function of the physical system. The cost function of the simulator is omitted for simplicity. In (a), bottom, it is shown the expected information gain per unit of effort of the simulator (blue line), and of the physical system (red line). The most informative point (blue dot) is selected among the two sources by the proposed method as next evaluation (in this case, a simulation). In (b), top, it is shown the GP posterior after nine iterations. The global minimum (orange dot) is found close to the true minimum.
cannot provide all the information about $J$. By choosing the kernels $k_{\text {sim }}$ and $k_{\text {err }}$, we can model to what extend $J$ can be explained by the simulator and thereby its quality. This is illustrated with a synthetic example in Fig. 2. The total variance of the cost on the physical system is shown in red. Before any data is observed, it is equal to the uncertainty about the simulation plus the uncertainty about the error. As shown in Fig. 2a, the blue shaded region highlights the variance of the simulator. Evaluations in simulation (blue dots) reduce the uncertainty of this blue shaded region, but reduce only partially the uncertainty about the true cost (red). In contrast, an evaluation on the real system (red dot) allows one to learn the true cost $J$ directly, thus reducing the total uncertainty (red), while some uncertainty about the variance of $J_{\text {sim }}$ remains (blue). Having uncertainty about the simulation is by itself irrelevant for the proposed method, because we solely aim to minimize the performance on the physical system.</p>
<p>Next to the kernel, we account for different amounts of noise in simulation (typically noise-free) and on the real system. That is, the noise variance of measurements, $\eta^{2}(\mathbf{a})$ in (2), takes different values, $\eta_{\text {exp }}^{2}$ and $\eta_{\text {sim }}^{2}$, depending on whether an experiment or a simulation is chosen. With this kernel and noise structure, the two information sources can be modeled by a single GP, and predictions can be made according to (3) and (4).</p>
<h2>B. Optimization</h2>
<p>With the GP model defined, we now consider how it can be used to trade off accuracy for evaluation effort. As a first step, we quantify the goal. As before, we want to minimize the cost (1) on the real system. This means, the distribution over the minimum is defined in terms of the same cost (5) as in standard ES. In order to approximate $p_{\min }$, we need to use the GP kernel with the additional $\delta$ factor fixed to one,</p>
<p>$$
p_{\min }(\boldsymbol{\theta})=\mathbb{P}\left(\boldsymbol{\theta} \in \underset{\boldsymbol{\theta} \in \mathcal{D}, \delta=1}{\operatorname{argmin}} J(\boldsymbol{\theta}, \delta)\right)
$$</p>
<p>As in ES, the goal is to arrive at a distribution $p_{\min }$ that has low entropy (i.e., very peaked on a certain location). The expected change in entropy is an appropriate measure for this. However, this quantity additionally depends on the variable $\delta$, so that the algorithm has an additional degree of freedom in the parameters to optimize. If one were to use the same optimization problem as in (6), the algorithm would always choose to evaluate parameters with $\delta=1$. This is because the experiments with $\delta=1$ provide information about the cost function $J$ directly, while an evaluation with $\delta=0$ only provides information about part of the cost, $J_{\text {sim }}$.</p>
<p>To trade off between the two choices more appropriately, we associate an effort measure with both kinds of evaluations; $t_{\text {sim }}$ for the simulation and $t_{\text {exp }}$ for physical experiments. While simulations are less informative about $p_{\min }$, they are significantly cheaper than experiments on a physical platform so that $t_{\text {sim }}&lt;t_{\text {exp }}$. These effort measures can have physically meaningful units, such as the amount of time taken by a simulation relative to a physical experiment. While the effort measures are important to trade off the relative gains in information, they do not require tuning. For example, setting the effort of the simulator too high may lead to more experiments on the physical system than necessary, but the optimal parameters on the real system are found regardless.</p>
<p>A key advantage of using entropy to determine progress towards the goal is that it is a consistent unit of measurement for both information sources, even in the case of different noise variances. As a result, we can compare the gain in information about the location of the minimum (i.e., $p_{\min }$ ) in simulation and physical experiments relative to their efforts. Thus, we select the next parameters, $\boldsymbol{\theta}_{n+1}$, and where to evaluate them, $\delta$, according to</p>
<p>$$
\underset{\boldsymbol{\theta} \in \mathcal{D}, i \in{\text { sim, exp }}}{\operatorname{argmax}} \mathrm{E}\left[\Delta H_{i}(\boldsymbol{\theta})\right] / t_{i}
$$</p>
<p>The expected gain in entropy, $\mathrm{E}\left[\Delta H_{i}\right]$, depends on whether we evaluate in simulation or physical experiment. By selecting the best gain per unit of effort, the algorithm automatically decides which kind of evaluation decreases the uncertainty about the location of the minimum the most, relative to effort. Importantly, since the GP model in (8) is adaptive to the quality of the simulator, the acquisition function (10) leads to informed decisions about whether the simulator is reliable enough to lead to additional information.</p>
<p>We illustrate a typical run of the algorithm in Fig. 2. The algorithm was initialized with one physical experiment (red dot in Fig. 2a) for the purpose of illustration. The evaluation effort of the simulator was set to $40\%$ less of that of the real system. As a result, it is advantageous to exploit initially the low effort that takes to do simulations. The algorithm automatically decides to do so, as can be seen in Fig. 2a. The simulation (blue dot) decreases the amount of uncertainty about the simulation model, but provides only partial information about the true cost of the system. As a result, the method eventually starts to evaluate parameters on the real system. Notice that this is not the same as two stage learning, because the algorithm can decide to switch back to simulations if this is beneficial. This is especially important in situations where the quality of the simulation is not known in advance and the hyperparameters of the kernels in (8) are optimized. Eventually, the algorithm converges to a distribution $p_{\min }$ that is peaked around the minima of the cost function. Since the model can exploit cheap information from simulation, fewer physical experiments are needed to determine the minimum than if only physical experiments were used.</p>
<p>Because the proposed method extends Entropy Search (ES) to multiple information sources, we refer to it as Multifidelity Entropy Search (MF-ES).</p>
<h2>V. EXPERIMENTAL RESULTS</h2>
<p>In this section, we evaluate MF-ES for optimizing the feedback controller of an unstable cart-pole system, as illustrated in Fig. 1.</p>
<h2>A. Experimental Setup</h2>
<p>As experimental setup, we use the Quanser Linear Inverted Pendulum, [26]. The dynamics of the system are described by</p>
<p>$$
\mathbf{x}<em k="k">{k+1}=\mathbf{f}\left(\mathbf{x}</em>\right)
$$}, u_{k</p>
<p>where $\mathbf{x}<em k="k">{k}=\left[s</em>}, \psi_{k}, \dot{s<em k="k">{k}, \dot{\psi}</em>(\cdot)$ is the transition function (see [26] for details).}\right]^{\mathrm{T}}$ is the state at discrete time step $k$, which is comprised of pendulum angle $\psi$, cart position $s$, and their time derivatives; $u_{k}$ is the commanded motor voltage driving the cart; and $\mathbf{f</p>
<p>The cart-pole setup is connected through dedicated hardware to a standard Laptop and can be controlled via Matlab/Simulink. A nonlinear Simulink model of the system dynamics (11) is provided by the manufacturer and used as the simulator in our setting.</p>
<h2>B. Controller Tuning Problem</h2>
<p>To stabilize the pendulum about its upright equilibrium, we use a static state-feedback controller,</p>
<p>$$
u_{k}=\mathbf{F} \mathbf{x}_{k}
$$</p>
<p>with gain matrix $\mathbf{F} \in \mathbb{R}^{1 \times 4}$. We seek optimal gains $\boldsymbol{F}$ that minimize the cost function</p>
<p>$$
J=\frac{1}{K} \sum_{k=0}^{K-1} s_{k}^{2}+\psi_{k}^{2}+\dot{s}<em k="k">{k}^{2}+0.1 \dot{\psi}</em>
$$}^{2}+10^{-1.5} u_{k}^{2</p>
<p>over a sufficiently long time horizon $K$. The cost (13) penalizes deviations from the equilibrium $\mathbf{x}=0$ and control effort $\left(u_{k}^{2}\right)$.</p>
<p>Instead of tuning the controllers gains $\mathbf{F}$ directly (i.e. setting $\boldsymbol{\theta}=\mathbf{F}$ ), we follow the approach from [14], [27], and pre-structure suitable controllers gains by means of a Linear Quadratic Regulator (LQR, [28]) design using a nominal, linearized version, $(\mathbf{A}, \mathbf{B})$, of the dynamics in (11), around the aforementioned equilibrium, which can be obtained from the simulator, for example. That is, the controller gain is computed from a discrete-time LQR design,</p>
<p>$$
\mathbf{F}=\operatorname{dlqr}\left(\mathbf{A}, \mathbf{B}, \mathbf{W}<em _mathrm_u="\mathrm{u">{\mathrm{x}}(\boldsymbol{\theta}), \mathbf{W}</em>)\right)
$$}}(\boldsymbol{\theta</p>
<p>where $\mathbf{W}<em _mathrm_u="\mathrm{u">{\mathrm{x}}(\boldsymbol{\theta})$ and $\mathbf{W}</em>)$ are suitable parameterizations of LQR weights (see [14], [27] for details). Here, we selected}}(\boldsymbol{\theta</p>
<p>$$
\begin{array}{ll}
\mathbf{W}<em 1="1">{\mathrm{x}}(\boldsymbol{\theta})=\operatorname{diag}\left(10^{\theta</em> \in[-3,2] \
\mathbf{W}}}, 1,1,0.1\right), &amp; \theta_{1<em 2="2">{\mathrm{u}}(\boldsymbol{\theta})=10^{-\theta</em> \in[1,5]
\end{array}
$$}}, &amp; \theta_{2</p>
<p>Hence, we are left with tuning two parameters, $\boldsymbol{\theta} \in \mathbb{R}^{2}$.
Advantages of the LQR parameterization in (14) are the possibility of dimensionality reduction, exploitation of prior knowledge in form of a linear dynamics model, and guarantees of stability and certain robustness properties with respect to the nominal system (see [14] for a discussion). However, we emphasize that LQR-weights are only one possible way to parameterize feedback controllers; alternative parameterizations [29] or direct tuning of the gains $\mathbf{F}$ [16] is also possible. The method proposed herein is independent of the specific parameterization used.</p>
<p>With the above definitions, the cost function $J(\boldsymbol{\theta})$, which we seek to minimize (1), is defined by equations (13)-(16). An evaluation of the cost $J_{\exp }\left(\boldsymbol{\theta}^{<em>}\right)$ is obtained by computing the controller gain (14) based on the weight matrices (15), (16), performing a 30 s balancing experiment on the physical system, and computing the cost according to (13) from the experimental data $\left{\mathbf{x}<em k="k">{k}, u</em>\right}<em _sim="{sim" _text="\text">{k=0, \ldots, K-1}$. A simulation sample $J</em>^{}}\left(\boldsymbol{\theta</em>}\right)$ is obtained in the same manner by running a 30 s simulation instead.</p>
<p>If a candidate controller violates safety limits on the states and inputs, it is determined as unstable, and we assign a fixed penalty of $J_{\exp }=0.06$ and $J_{\text {sim }}=0.04$ for physical experiment and simulation, respectively. These numbers are chosen conservatively larger than the cost of the worse stabilizing controller observed after some a priori initial evaluations. Thus, evaluations during the learning procedure shall not result in higher costs than these.</p>
<p>The controller is automatically tuned over roll-outs without human intervention. To this end, a nominal ${ }^{1}$ controller $\boldsymbol{\theta}_{\text {nom }}=[0,1.5]$ is balancing the pole when no tuning experiment is being performed. The optimizer triggers new experiments, when an evaluation on the real system is required. As soon as the experiment is finished, or instability is</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. GP posterior after termination of the exploration with MF-ES. The evaluations on the simulator (light blue) are systematically below the evaluations on the real system (dark blue). This bias is captured by the GP model assuming a lower prior mean for the simulator data, as mentioned in Sec. V-C. The posterior mean (green surface) and ±2 std (grey surface) predict the underlying cost function of the real system, conditioned on the observed data from both simulator and experiments. The best guess location for the global minimum, θbg, is represented by the orange dot.</p>
<p>detected, the system switches back to the nominal controller. The nominal controller shows very poor performance, which shall be improved with the proposed RL method.</p>
<h3><em>C. Bayesian Optimization Settings</em></h3>
<p>We apply the method of Sec. IV, MF-ES, to optimize the experimental cost (13) by querying simulations and experiments. The efforts in (10) correspond to the approximate times we need to wait until a simulation is computed and a physical experiment is performed, tsim = 1 s and texp = 30 s, i.e., simulations require 30 times less effort than physical experiments.</p>
<p>For the GP model, we choose the rational quadratic kernel with α = 1/4 (see [11]) for both ksim and kerr in (8). Hyperparameters, such as length scales and output variances, were chosen from some initial experiments and then held fixed during optimization. As prior mean functions, we use msim(θ) ≡ 0.04 and merr(θ) ≡ 0.02, respectively, for the simulation and error GP. These choices correspond to the penalties Jsim and Jexp given for unstable controllers (adding msim and merr for the experiment). Hence, the prior mean is pessimistic in the sense that we believe a controller to be unstable before seeing any data. The prior variance of ksim and kerr are chosen as σsim = 1.6 × 10<sup>−5</sup> and σerr = 3.84 × 10<sup>−4</sup> respectively.</p>
<p>The noise standard deviation of an evaluation on the real system, as defined in (2), has been estimated to ηexp = 2.08 × 10<sup>−4</sup>, while the noise of the simulator has been set to ηsim = 10<sup>−5</sup>, roughly twenty times lower.</p>
<p>We stop the exploration when the GP posterior mean at the best guess θbg (i.e., the current estimate of the global minimum) has not changed significantly (within a range of σerr/4 over the last 3 iterations), and we are sufficiently certain about its value (posterior standard deviation at θbg</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. (Top) Cost obtained at each iteration with the proposed approach during one exploration run. When the exploration terminates, the best guess is evaluated on the physical system (violet dot). (Bottom) Evolution of the GP posterior mean at the best guess µv(θbg) and std ±σv(θbg).</p>
<p>less than σerr/2). Once the exploration has terminated, we evaluate the final best guess controller on a physical experiment and take its cost as the outcome of the learning procedure.</p>
<h3><em>D. Results</em></h3>
<p>We run MF-ES on the LQR problem described in Sec. V-B. Fig. 3 shows the final GP cost function landscape after the learning procedure, highlighting simulations (in light blue) and experiments (in dark blue). For the same learning run, Fig. 4 (top) illustrates how MF-ES alternates between simulations and physical experiments over iterations. As can be seen, the algorithm first performs multiple cheap simulations, which allow to identify regions of unstable controllers (i.e., regions of high predicted cost in Fig. 3) without any real experiment. At iterations 10 and 14, the algorithm demands two expensive physical experiments. The reason is that a time unit spent in simulation is expected to be less informative than on a physical experiment. Thereby, experiment time should be better spent on the physical system. Fig. 4 (bottom) shows the GP posterior mean and standard deviation of the best guess at each iteration. The stopping criterion terminates the exploration after 14 iterations because the GP posterior mean of the last three best guesses were steady enough. Finally, the algorithm selects the last global minimum, θbg = [0.212, 2.42] (orange dot in Fig. 3), as the final controller, which was evaluated on the physical system retrieving a low cost J(θbg) = 0.0194.</p>
<p>As a remark, we observe that the algorithm alternates between simulations and experiments in a non-trivial way, which cannot be reproduced with a simple two-stage learning process, where simulations are used to seed experimental reinforcement learning. Furthermore, in Fig. 3, we can see that the posterior mean around θ = [−2, 4] falls back to the prior in the absence of evaluations. As pointed out in Sec. V-C, the prior mean is pessimistic in the sense that predicts instability in unforeseen areas, which is a reasonable assumption in controller tuning of real systems.</p>
<p>In order to illustrate the benefit of trading off data from <em>experiments and simulations</em>, we compare MF-ES to ES [3], which uses <em>only physical experiments</em>. The latter corresponds to the automatic controller tuning setting in [14]. We run each of these methods ten times on the controller tuning problem. The results are discussed in Fig. 5 and Fig. 6.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Comparison of the final controller cost at each run between the proposed approach (MF-ES) and ES. The cost of the nominal controller (beige) with ± 2 std is shown for reference.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. Number of physical experiments at each run for MF-ES (dark blue) and standard ES (dark red), as well as simulations for MF-ES (light blue).</p>
<p>In Fig. 5, we show the cost of the final controller at each run, for both methods. The cost of the nominal controller (green) is shown as a reference. MF-ES finds controllers that are 33.23% better, on average. Moreover, it consistently finds stabilizing controllers, while ES fails to find a stabilizing solution in 4 out of 10 cases (cost of 0.06).</p>
<p>Fig. 6 compares the number of physical experiments performed with MF-ES (dark blue) and with ES (dark red). While ES needs on average 3.5 physical experiments, MF-ES needs 2.7 (22.86% less) plus 11.9 simulations. These results demonstrate that MF-ES can find, on average, better controllers with a lower number of real experiments by also leveraging information from simulations.</p>
<h2>VI. CONCLUSION</h2>
<p>We have shown a generic Bayesian optimization that can adaptively select between multiple information sources with different accuracies and evaluation efforts, such as experiments on a real robot and simulations. We applied this method to a policy optimization task on a cart-pole system. The experimental results confirm that using prior model information from a simulator can reduce the amount of data required to globally find good control policies.</p>
<h2>REFERENCES</h2>
<ul>
<li>[1] R. S. Sutton and A. G. Barto, <em>Reinforcement Learning: An Introduction</em>. MIT press, 1998.</li>
<li>[2] J. Kober, J. A. Bagnell, and J. Peters, "Reinforcement learning in robotics: A survey," <em>The International Journal of Robotics Research</em>, vol. 32, no. 11, pp. 1238–1274, 2013.</li>
<li>[3] P. Hennig and C. J. Schuler, "Entropy search for information-efficient global optimization," <em>Journal of Machine Learning Research</em>, vol. 13, no. 1, pp. 1809–1837, 2012.</li>
<li>[4] J. Villemonteix, E. Vazquez, and E. Walter, "An informational approach to the global optimization of expensive-to-evaluate functions," <em>Journal of Global Optimization</em>, vol. 44, no. 4, pp. 509–540, 2008.</li>
<li>[5] M. Cutler and J. P. How, "Efficient reinforcement learning for robots using informative simulated priors," in <em>IEEE International Conference on Robotics and Automation</em>, 2015, pp. 2605–2612.</li>
<li>[6] M. E. Taylor and P. Stone, "Transfer learning for reinforcement learning domains: A survey," <em>Journal of Machine Learning Research</em>, vol. 10, pp. 1633–1685, 2009.</li>
<li>[7] M. Cutler, T. J. Walsh, and J. P. How, "Real-world reinforcement learning via multifidelity simulators," <em>IEEE Transactions on Robotics</em>, vol. 31, no. 3, pp. 655–671, 2015.</li>
<li>[8] J. Peters and S. Schaal, "Policy gradient methods for robotics," in <em>IEEE International Conference on Intelligent Robots and Systems</em>, 2006, pp. 2219–2225.</li>
<li>[9] P. Abbeel, M. Quigley, and A. Y. Ng, "Using Inaccurate Models in Reinforcement Learning," in <em>ACM International Conference on Machine Learning</em>, 2006, pp. 1–8.</li>
<li>[10] J. Mockus, <em>Bayesian Approach to Global Optimization</em>, ser. Mathematics and Its Applications, M. Hazewinkel, Ed. Springer, 1989, vol. 37.</li>
<li>[11] C. E. Rasmussen and C. K. Williams, <em>Gaussian Processes for Machine Learning</em>. MIT Press, 2006.</li>
<li>[12] D. J. Lizotte, T. Wang, M. H. Bowling, and D. Schuurmans, "Automatic gait optimization with Gaussian process regression." in <em>International Joint Conference on Artificial Intelligence</em>, vol. 7, 2007, pp. 944–949.</li>
<li>[13] M. Tesch, J. Schneider, and H. Choset, "Using response surfaces and expected improvement to optimize snake robot gait parameters," in <em>IEEE International Conference on Intelligent Robots and Systems</em>, 2011, pp. 1069–1074.</li>
<li>[14] A. Marco, P. Hennig, J. Bohg, S. Schaal, and S. Trimpe, "Automatic LQR tuning based on Gaussian process global optimization," in <em>IEEE International Conference on Robotics and Automation</em>, 2016, pp. 270–277.</li>
<li>[15] H. Abdelrahman, F. Berkenkamp, J. Poland, and A. Krause, "Bayesian optimization for maximum power point tracking in photovoltaic power plants," in <em>European Control Conference</em>, 2016, pp. 2078–2083.</li>
<li>[16] F. Berkenkamp, A. Krause, and Angela P. Schoellig, "Bayesian optimization with safety constraints: Safe and automatic parameter tuning in robotics." arXiv, 2016, arXiv:1602.04450 [cs.RO].</li>
<li>[17] R. Calandra, A. Seyfarth, J. Peters, and M. P. Deisenroth, "An experimental comparison of Bayesian optimization for bipedal locomotion," in <em>IEEE International Conference on Robotics and Automation</em>, 2014, pp. 1951–1958.</li>
<li>[18] A. Krause and C. S. Ong, "Contextual Gaussian process bandit optimization," in <em>Neural Information Processing Systems</em>, 2011, pp. 2447–2455.</li>
<li>[19] K. Swersky, J. Snoek, and R. P. Adams, "Multi-Task Bayesian Optimization," in <em>Advances in Neural Information Processing Systems</em>, 2013, pp. 2004–2012.</li>
<li>[20] M. C. Kennedy and A. O'Hagan, "Predicting the output from a complex computer code when fast approximations are available," <em>Biometrika</em>, vol. 87, no. 1, pp. 1–13, 2000.</li>
<li>[21] A. I. J. Forrester, A. Söbester, and A. J. Keane, "Multi-fidelity optimization via surrogate modelling," <em>Royal Society of London A: Mathematical, Physical and Engineering Sciences</em>, vol. 463, no. 2088, pp. 3251–3269, 2007.</li>
<li>[22] K. Kandasamy, G. Dasarathy, J. B. Oliva, J. Schneider, and B. Poczos, "Multi-fidelity Gaussian process bandit optimisation." arXiv, 2016, arXiv:1603.06288 [cs, stat].</li>
<li>[23] M. Poloczek, J. Wang, and P. I. Frazier, "Multi-Information Source Optimization with General Model Discrepancies." arXiv, 2016, arXiv:1603.00389v2 [stat.ML].</li>
<li>[24] P. Frazier, W. Powell, and S. Dayanik, "The knowledge-gradient policy for correlated normal beliefs," <em>INFORMS Journal on Computing</em>, vol. 21, no. 4, pp. 599–613, 2009.</li>
<li>[25] N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger, "Gaussian process optimization in the bandit setting: No regret and experimental design," in <em>International Conference on Machine Learning</em>, 2010, pp. 1015–1022.</li>
<li>[26] Quanser, "Self-erecting single inverted pendulum – Instructor manual"," Tech. Rep. 516, rev. 4.1.</li>
<li>[27] S. Trimpe, A. Millane, S. Doessegger, and R. D'Andrea, "A self-tuning LQR approach demonstrated on an inverted pendulum," in <em>19th IFAC World Congress</em>, 2014, pp. 11 281–11 287.</li>
<li>[28] B. D. O. Anderson and J. B. Moore, <em>Optimal Control: Linear Quadratic Methods</em>. Mineola, New York: Dover Publications, 2007.</li>
<li>[29] J. Roberts, I. Manchester, and R. Tedrake, "Feedback controller parameterizations for reinforcement learning," in <em>IEEE Symposium on Adaptive Dynamic Programming And Reinforcement Learning</em>, 2011, pp. 310–317.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The nominal controller is the optimal controller if the true dynamics was linear according to the nominal model $(\mathbf{A}, \mathbf{B})$. Then, choosing $\mathbf{W}<em _mathrm_u="\mathrm{u">{\mathrm{x}}(\boldsymbol{\theta})$ and $\mathbf{W}</em>$, see [14]. This is a typical choice when neglecting the nonlinear dynamics.&#160;}}(\boldsymbol{\theta})$ corresponding to the cost (13) yields the optimal controller $\mathbf{F<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>Accepted final version. To appear in 2017 IEEE International Conference on Robotics and Automation.
(c)2017 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/nepublishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
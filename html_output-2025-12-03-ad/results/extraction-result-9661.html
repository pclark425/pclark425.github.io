<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9661 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9661</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9661</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-278769611</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.14599v2.pdf" target="_blank">Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful scientific hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9661.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9661.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthHypo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Truthful Hypothesis Generation Benchmark (TruthHypo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical benchmark introduced in this paper to evaluate LLMs' ability to generate truthful scientific hypotheses using a temporally-split PubTator-derived knowledge graph, task templates, negative examples, and link- and relation-level metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o, GPT-4o-mini, Llama-3.1-8B, Llama-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models evaluated in the benchmark: two Llama-3.1 sizes (8B, 70B) and two GPT-4o family sizes (GPT-4o-mini, GPT-4o); all trained on data available before 2024 and used to generate hypotheses under multiple knowledge-augmentation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (biomedical hypothesis generation: Chemical–Gene, Disease–Gene, Gene–Gene)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated comparison to temporally-held-out ground-truth relations from a PubTator-derived knowledge graph (link prediction/classification) plus human expert review on open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Link-level precision/recall/F1 (existence of a connection), relation-level accuracy (correct relation label), and groundedness scores from KnowHD; also human selection ratios in open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>TruthHypo: dataset built from PubTator 3.0 with temporal split into 'seen' (PMID ≤ 36600000) and 'unseen' (PMID ≥ 38200000) edges, three relation types, and controlled negative samples (1209 Chemical–Gene, 268 Disease–Gene, 547 Gene–Gene instances).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLMs generally struggle to generate truthful hypotheses; only GPT-4o exceeded mean accuracies >60% across tasks. Link-level F1 > relation-level accuracy, indicating models find candidate links but mislabel relations. External knowledge (KG and literature) improves some models, especially larger ones; combined KG+literature yields the highest average accuracy (e.g., GPT-4o average Acc ≈ 66.95% in KG+Lit).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Temporal data split and ground-truth restricted to relations published after 2024; verifying true novelty requires expensive wet-lab validation. Smaller models may suffer performance degradation when augmented with external knowledge due to integration difficulties. Benchmark focuses on biomedical relation types and may not generalize to all scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>TruthHypo uses automated ground-truth from literature (PubTator) rather than experimental validation; human expert review was used for open-ended tasks and corroborated that groundedness correlates with perceived truthfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use temporal splits to avoid data leakage; include negative/no-relation examples; evaluate both link-level (precision/recall/F1) and relation-level (accuracy) metrics; test multiple knowledge-augmentation settings (parametric, KG, literature, combined).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9661.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9661.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KnowHD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-based Hallucination Detection (KnowHD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework introduced in this paper that decomposes an LLM-generated hypothesis and its rationale into atomic claims and evaluates each claim's groundedness against literature, knowledge graphs, or both to detect hallucinated/unfounded reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o-mini (analysis presented), GPT-4o and Llama-3.1 series used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>KnowHD uses LLM prompting to (1) decompose rationales into atomic claims and (2) judge entailment between context and claims; experiments used GPT-4o-mini for in-depth groundedness reporting and multiple LLMs for hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (claim-level groundedness evaluation using PubMed literature and PubTator knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic claim verification: retrieve supporting context (BM25 over PubMed or graph-extracted edges) for each atomic claim, then prompt an LLM to judge whether the context entails the claim; aggregate per-claim binary judgments to a hypothesis groundedness score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Groundedness score = fraction of atomic claims judged as entailed by the retrieved context (Equation 3). Context sources: context_D (top-k BM25 literature), context_G (graph edges with involved entities), or union.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Uses the TruthHypo dataset for candidate hypotheses; evidence retrieval uses PubMed (articles with PMID ≤ 36600000) and PubTator 3.0 knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>KnowHD correlates with truthfulness: higher groundedness groups have higher mean accuracy (e.g., GPT-4o-mini had mean accuracy 72.77% for hypotheses with groundedness >80% vs 60.96% overall for Chemical–Gene in combined setting). KnowHD verified 76.30% of claims for literature-augmented Chemical–Gene rationales but only 51.08% when relying solely on KG for the same.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality depends on retrieval (BM25 thresholds, k) and on LLM judgments of entailment; some atomic claims may be underspecified or require experimental evidence not present in literature/graph. Computational cost grows with number of claims and retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human expert annotations in open-ended tasks align with KnowHD: hypotheses with higher groundedness were more often judged truthful by experts and by GPT-4o. KnowHD provides automated approximation of what a literature/graph-grounded human check would do.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Decompose rationales into atomic claims, retrieve from both literature and KG where possible, set BM25 k (claim verification used k=8) and τ thresholds explicitly, and use groundedness to rank/select among multiple candidate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9661.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9661.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Groundedness score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis groundedness score (KnowHD aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A numeric score computed as the fraction of atomic claims in a hypothesis whose retrieved context (literature and/or graph) entails those claims, used as a proxy for hypothesis truthfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o-mini (analysis), GPT-4o, Llama-3.1 series</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Computed by prompting LLMs to judge entailment for each claim against retrieved context; used to rank candidate hypotheses generated by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Atomic-claim decomposition + retrieval (BM25 or KG) + LLM entailment judgment; hypothesis groundedness = mean of binary entailment indicators across claims (Equation 3).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Thresholded groundedness groups were used to correlate with accuracy; higher groundedness (e.g., >80%) showed clearly higher hypothesis accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to TruthHypo outputs and to an open-ended hypothesis generation dataset from Qi et al. (2024) for human study.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Groundedness is predictive: hypotheses with higher groundedness had substantially higher measured accuracy and were favored by human experts; e.g., combined KG+Lit groundedness >80% corresponded to sizable accuracy gains (GPT-4o-mini Chemical & Gene: 72.77% vs baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on completeness of retrieval sources; binary entailment judgments by LLMs can be noisy; setting of k and τ affects coverage and precision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Serves as an automated surrogate for manual literature/graph checking; correlates with expert judgments but not a replacement for experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use groundedness to filter and rank multiple generated hypotheses, prefer combined KG+literature contexts, and validate automated entailment judgments with human review on critical cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9661.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9661.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation metrics (TruthHypo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Link-level and relation-level evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-tiered evaluation: link-level metrics (precision, recall, F1) measure whether LLMs identify potential connections, while relation-level accuracy measures correctness of predicted relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o, GPT-4o-mini, Llama-3.1-8B, Llama-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Evaluated under four knowledge settings; metrics computed on TruthHypo test instances across three relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (relation prediction/link discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated scoring against held-out 'unseen' relations from PubTator (temporal split). Link-level: precision/recall/F1; relation-level: accuracy of predicted relation label (including 'no relation').</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision emphasizes reducing false positives (important for costly validation), recall emphasizes coverage, F1 balances both; accuracy measures exact label correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>TruthHypo (temporal holdout from PubTator 3.0) with added negative 'no relation' samples balanced across labels.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Observed pattern: link-level F1 scores are generally higher than relation-level accuracy — LLMs find candidate links but mislabel or overgeneralize relations. Small models tend to have higher recall and lower precision; larger models (e.g., GPT-4o) achieve higher precision at cost of recall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Metrics require reliable ground-truth relations; 'no relation' negative design choices influence scores; relation taxonomy limited to three relation types in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automated metrics are fast and reproducible but do not substitute for lab validation; human expert review used for open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report both link-level and relation-level metrics, and consider precision-first evaluation when downstream validation is costly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9661.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9661.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge-augmentation settings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parametric / Parametric + KG / Parametric + Lit. / Parametric + KG + Lit.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four experimental conditions evaluating LLM hypothesis generation: using only model parameters, augmenting with structured KG context, augmenting with retrieved literature context (RAG), or combining both KG and literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o, GPT-4o-mini, Llama-3.1-8B, Llama-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models' ability to incorporate external knowledge assessed under each setting; KG contexts were multi-hop link chains turned into text, literature retrieved via BM25 from PubMed (PMID ≤ 36600000).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare model outputs and metrics (F1, accuracy, groundedness) across the four settings to determine benefits of external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Improvement in link-level F1, relation-level accuracy, and groundedness verification rates when external information is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to TruthHypo dataset; KG built from PubTator 3.0 (seen subset) and PubMed corpus for literature retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Larger models typically benefit from KG and literature (combined setting gives highest groundedness and accuracy); e.g., GPT-4o showed ~5.14% accuracy gain when augmented; smaller models sometimes decreased performance when given external context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Smaller LLMs may struggle to integrate external context coherently, causing degraded performance; KG-only context sometimes insufficient to verify claims compared to literature; retrieval and context formatting matter.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Augmented contexts approximate what a researcher would consult (papers and curated KG), but still differ from experimental verification.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use combined KG+literature where possible, format KG multi-hop chains into concise textual context, and evaluate LLM ability to use external context per-size of model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9661.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9661.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Groundedness-based selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Candidate-hypothesis selection by highest groundedness score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure that generates multiple candidate hypotheses per input with an LLM, computes KnowHD groundedness for each, and selects the hypothesis with the highest groundedness as final output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o-mini (primary reported), GPT-4o, Llama-3.1 variants</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>In experiments, LLMs generated 5 candidates per input; groundedness selection compared to greedy decoding and self-consistency (majority voting).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Empirical comparison of selection strategies (groundedness-max, greedy, self-consistency) by measuring resulting accuracy on TruthHypo.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Final hypothesis relation-level accuracy; also comparison versus baselines across knowledge settings.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to TruthHypo; candidate count = 5; baselines: greedy next-token selection and self-consistency majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Groundedness-based selection typically outperforms greedy and majority-voting when external knowledge is available (e.g., GPT-4o-mini average accuracy 63.44% in combined setting using groundedness selection). In parametric-only setting, majority-voting slightly outperformed groundedness selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Computational overhead (generate multiple candidates + claim verification for each); effectiveness depends on accurate retrieval and claim-judgment steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides an automated way to mimic human preference for well-supported hypotheses; human validation still necessary for high-stakes decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Generate multiple hypotheses and pick by groundedness when external knowledge is provided; reserve majority-voting for pure parametric settings where groundedness evidence is unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9661.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9661.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25 + PubMed retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 retriever over PubMed corpus for evidence retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information retrieval pipeline used in the paper to fetch supporting literature for claims using BM25 ranking over PubMed chunks; used both for hypothesis generation context (k=32) and claim verification (k=8).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o-mini (used for claim verification), GPT-4o, Llama-3.1</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Retrieval component (not an LLM) paired with LLMs to provide external unstructured evidence; τ threshold set to 0.0, k=32 for generation, k=8 for claim verification.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (literature retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>BM25 ranks document chunks; top-k documents concatenated as context for LLM generation or verification; used to ensure temporal integrity by restricting PMIDs ≤ 36600000.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relevance via BM25 score and rank; impact measured indirectly by groundedness verification rates and downstream hypothesis accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>PubMed corpus (pre-2023 articles for retrieval); MedRAG-processed chunks used to build BM25 index.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BM25 retrieval provided substantial evidence coverage for claim verification and improved groundedness when combined with KG; literature-supported rationales had higher verification rates (e.g., 76.30% verified claims for literature-augmented Chemical & Gene rationales).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>BM25 relies on lexical matching and can miss semantically relevant passages; setting k and τ trades off precision and recall; dense retrievers may perform differently in biomedical domain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Emulates manual literature search; faster but limited by index design and matching heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use BM25 on carefully chunked biomedical text (MedRAG), set τ and k explicitly (τ=0.0 used here), and combine literature retrieval with KG evidence for best groundedness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models as biomedical hypothesis generators: A comprehensive evaluation <em>(Rating: 2)</em></li>
                <li>Can chatgpt be used to generate scientific hypotheses <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>PubTator 3.0: an ai-powered literature resource for unlocking biomedical knowledge <em>(Rating: 2)</em></li>
                <li>The probabilistic relevance framework: BM25 and beyond <em>(Rating: 2)</em></li>
                <li>Improving scientific hypothesis generation with knowledge grounded large language models <em>(Rating: 2)</em></li>
                <li>Truth-Hypo: (if present in related works) — related benchmarking work on hypothesis generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9661",
    "paper_id": "paper-278769611",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "TruthHypo",
            "name_full": "Truthful Hypothesis Generation Benchmark (TruthHypo)",
            "brief_description": "A biomedical benchmark introduced in this paper to evaluate LLMs' ability to generate truthful scientific hypotheses using a temporally-split PubTator-derived knowledge graph, task templates, negative examples, and link- and relation-level metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o, GPT-4o-mini, Llama-3.1-8B, Llama-3.1-70B",
            "llm_description": "Models evaluated in the benchmark: two Llama-3.1 sizes (8B, 70B) and two GPT-4o family sizes (GPT-4o-mini, GPT-4o); all trained on data available before 2024 and used to generate hypotheses under multiple knowledge-augmentation settings.",
            "scientific_domain": "Biomedicine (biomedical hypothesis generation: Chemical–Gene, Disease–Gene, Gene–Gene)",
            "evaluation_method": "Automated comparison to temporally-held-out ground-truth relations from a PubTator-derived knowledge graph (link prediction/classification) plus human expert review on open-ended tasks.",
            "evaluation_criteria": "Link-level precision/recall/F1 (existence of a connection), relation-level accuracy (correct relation label), and groundedness scores from KnowHD; also human selection ratios in open-ended tasks.",
            "benchmark_or_dataset": "TruthHypo: dataset built from PubTator 3.0 with temporal split into 'seen' (PMID ≤ 36600000) and 'unseen' (PMID ≥ 38200000) edges, three relation types, and controlled negative samples (1209 Chemical–Gene, 268 Disease–Gene, 547 Gene–Gene instances).",
            "results_summary": "LLMs generally struggle to generate truthful hypotheses; only GPT-4o exceeded mean accuracies &gt;60% across tasks. Link-level F1 &gt; relation-level accuracy, indicating models find candidate links but mislabel relations. External knowledge (KG and literature) improves some models, especially larger ones; combined KG+literature yields the highest average accuracy (e.g., GPT-4o average Acc ≈ 66.95% in KG+Lit).",
            "limitations_or_challenges": "Temporal data split and ground-truth restricted to relations published after 2024; verifying true novelty requires expensive wet-lab validation. Smaller models may suffer performance degradation when augmented with external knowledge due to integration difficulties. Benchmark focuses on biomedical relation types and may not generalize to all scientific domains.",
            "comparison_to_human_or_traditional": "TruthHypo uses automated ground-truth from literature (PubTator) rather than experimental validation; human expert review was used for open-ended tasks and corroborated that groundedness correlates with perceived truthfulness.",
            "recommendations_or_best_practices": "Use temporal splits to avoid data leakage; include negative/no-relation examples; evaluate both link-level (precision/recall/F1) and relation-level (accuracy) metrics; test multiple knowledge-augmentation settings (parametric, KG, literature, combined).",
            "uuid": "e9661.0",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "KnowHD",
            "name_full": "Knowledge-based Hallucination Detection (KnowHD)",
            "brief_description": "A framework introduced in this paper that decomposes an LLM-generated hypothesis and its rationale into atomic claims and evaluates each claim's groundedness against literature, knowledge graphs, or both to detect hallucinated/unfounded reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o-mini (analysis presented), GPT-4o and Llama-3.1 series used in experiments",
            "llm_description": "KnowHD uses LLM prompting to (1) decompose rationales into atomic claims and (2) judge entailment between context and claims; experiments used GPT-4o-mini for in-depth groundedness reporting and multiple LLMs for hypothesis generation.",
            "scientific_domain": "Biomedicine (claim-level groundedness evaluation using PubMed literature and PubTator knowledge graph)",
            "evaluation_method": "Automatic claim verification: retrieve supporting context (BM25 over PubMed or graph-extracted edges) for each atomic claim, then prompt an LLM to judge whether the context entails the claim; aggregate per-claim binary judgments to a hypothesis groundedness score.",
            "evaluation_criteria": "Groundedness score = fraction of atomic claims judged as entailed by the retrieved context (Equation 3). Context sources: context_D (top-k BM25 literature), context_G (graph edges with involved entities), or union.",
            "benchmark_or_dataset": "Uses the TruthHypo dataset for candidate hypotheses; evidence retrieval uses PubMed (articles with PMID ≤ 36600000) and PubTator 3.0 knowledge graph.",
            "results_summary": "KnowHD correlates with truthfulness: higher groundedness groups have higher mean accuracy (e.g., GPT-4o-mini had mean accuracy 72.77% for hypotheses with groundedness &gt;80% vs 60.96% overall for Chemical–Gene in combined setting). KnowHD verified 76.30% of claims for literature-augmented Chemical–Gene rationales but only 51.08% when relying solely on KG for the same.",
            "limitations_or_challenges": "Quality depends on retrieval (BM25 thresholds, k) and on LLM judgments of entailment; some atomic claims may be underspecified or require experimental evidence not present in literature/graph. Computational cost grows with number of claims and retrieved documents.",
            "comparison_to_human_or_traditional": "Human expert annotations in open-ended tasks align with KnowHD: hypotheses with higher groundedness were more often judged truthful by experts and by GPT-4o. KnowHD provides automated approximation of what a literature/graph-grounded human check would do.",
            "recommendations_or_best_practices": "Decompose rationales into atomic claims, retrieve from both literature and KG where possible, set BM25 k (claim verification used k=8) and τ thresholds explicitly, and use groundedness to rank/select among multiple candidate hypotheses.",
            "uuid": "e9661.1",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Groundedness score",
            "name_full": "Hypothesis groundedness score (KnowHD aggregated)",
            "brief_description": "A numeric score computed as the fraction of atomic claims in a hypothesis whose retrieved context (literature and/or graph) entails those claims, used as a proxy for hypothesis truthfulness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o-mini (analysis), GPT-4o, Llama-3.1 series",
            "llm_description": "Computed by prompting LLMs to judge entailment for each claim against retrieved context; used to rank candidate hypotheses generated by LLMs.",
            "scientific_domain": "Biomedicine",
            "evaluation_method": "Atomic-claim decomposition + retrieval (BM25 or KG) + LLM entailment judgment; hypothesis groundedness = mean of binary entailment indicators across claims (Equation 3).",
            "evaluation_criteria": "Thresholded groundedness groups were used to correlate with accuracy; higher groundedness (e.g., &gt;80%) showed clearly higher hypothesis accuracy.",
            "benchmark_or_dataset": "Applied to TruthHypo outputs and to an open-ended hypothesis generation dataset from Qi et al. (2024) for human study.",
            "results_summary": "Groundedness is predictive: hypotheses with higher groundedness had substantially higher measured accuracy and were favored by human experts; e.g., combined KG+Lit groundedness &gt;80% corresponded to sizable accuracy gains (GPT-4o-mini Chemical & Gene: 72.77% vs baseline).",
            "limitations_or_challenges": "Depends on completeness of retrieval sources; binary entailment judgments by LLMs can be noisy; setting of k and τ affects coverage and precision.",
            "comparison_to_human_or_traditional": "Serves as an automated surrogate for manual literature/graph checking; correlates with expert judgments but not a replacement for experimental validation.",
            "recommendations_or_best_practices": "Use groundedness to filter and rank multiple generated hypotheses, prefer combined KG+literature contexts, and validate automated entailment judgments with human review on critical cases.",
            "uuid": "e9661.2",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Evaluation metrics (TruthHypo)",
            "name_full": "Link-level and relation-level evaluation metrics",
            "brief_description": "A two-tiered evaluation: link-level metrics (precision, recall, F1) measure whether LLMs identify potential connections, while relation-level accuracy measures correctness of predicted relation types.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o, GPT-4o-mini, Llama-3.1-8B, Llama-3.1-70B",
            "llm_description": "Evaluated under four knowledge settings; metrics computed on TruthHypo test instances across three relation types.",
            "scientific_domain": "Biomedicine (relation prediction/link discovery)",
            "evaluation_method": "Automated scoring against held-out 'unseen' relations from PubTator (temporal split). Link-level: precision/recall/F1; relation-level: accuracy of predicted relation label (including 'no relation').",
            "evaluation_criteria": "Precision emphasizes reducing false positives (important for costly validation), recall emphasizes coverage, F1 balances both; accuracy measures exact label correctness.",
            "benchmark_or_dataset": "TruthHypo (temporal holdout from PubTator 3.0) with added negative 'no relation' samples balanced across labels.",
            "results_summary": "Observed pattern: link-level F1 scores are generally higher than relation-level accuracy — LLMs find candidate links but mislabel or overgeneralize relations. Small models tend to have higher recall and lower precision; larger models (e.g., GPT-4o) achieve higher precision at cost of recall.",
            "limitations_or_challenges": "Metrics require reliable ground-truth relations; 'no relation' negative design choices influence scores; relation taxonomy limited to three relation types in the benchmark.",
            "comparison_to_human_or_traditional": "Automated metrics are fast and reproducible but do not substitute for lab validation; human expert review used for open-ended tasks.",
            "recommendations_or_best_practices": "Report both link-level and relation-level metrics, and consider precision-first evaluation when downstream validation is costly.",
            "uuid": "e9661.3",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Knowledge-augmentation settings",
            "name_full": "Parametric / Parametric + KG / Parametric + Lit. / Parametric + KG + Lit.",
            "brief_description": "Four experimental conditions evaluating LLM hypothesis generation: using only model parameters, augmenting with structured KG context, augmenting with retrieved literature context (RAG), or combining both KG and literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o, GPT-4o-mini, Llama-3.1-8B, Llama-3.1-70B",
            "llm_description": "Models' ability to incorporate external knowledge assessed under each setting; KG contexts were multi-hop link chains turned into text, literature retrieved via BM25 from PubMed (PMID ≤ 36600000).",
            "scientific_domain": "Biomedicine",
            "evaluation_method": "Compare model outputs and metrics (F1, accuracy, groundedness) across the four settings to determine benefits of external knowledge.",
            "evaluation_criteria": "Improvement in link-level F1, relation-level accuracy, and groundedness verification rates when external information is provided.",
            "benchmark_or_dataset": "Applied to TruthHypo dataset; KG built from PubTator 3.0 (seen subset) and PubMed corpus for literature retrieval.",
            "results_summary": "Larger models typically benefit from KG and literature (combined setting gives highest groundedness and accuracy); e.g., GPT-4o showed ~5.14% accuracy gain when augmented; smaller models sometimes decreased performance when given external context.",
            "limitations_or_challenges": "Smaller LLMs may struggle to integrate external context coherently, causing degraded performance; KG-only context sometimes insufficient to verify claims compared to literature; retrieval and context formatting matter.",
            "comparison_to_human_or_traditional": "Augmented contexts approximate what a researcher would consult (papers and curated KG), but still differ from experimental verification.",
            "recommendations_or_best_practices": "Use combined KG+literature where possible, format KG multi-hop chains into concise textual context, and evaluate LLM ability to use external context per-size of model.",
            "uuid": "e9661.4",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Groundedness-based selection",
            "name_full": "Candidate-hypothesis selection by highest groundedness score",
            "brief_description": "A procedure that generates multiple candidate hypotheses per input with an LLM, computes KnowHD groundedness for each, and selects the hypothesis with the highest groundedness as final output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o-mini (primary reported), GPT-4o, Llama-3.1 variants",
            "llm_description": "In experiments, LLMs generated 5 candidates per input; groundedness selection compared to greedy decoding and self-consistency (majority voting).",
            "scientific_domain": "Biomedicine",
            "evaluation_method": "Empirical comparison of selection strategies (groundedness-max, greedy, self-consistency) by measuring resulting accuracy on TruthHypo.",
            "evaluation_criteria": "Final hypothesis relation-level accuracy; also comparison versus baselines across knowledge settings.",
            "benchmark_or_dataset": "Applied to TruthHypo; candidate count = 5; baselines: greedy next-token selection and self-consistency majority voting.",
            "results_summary": "Groundedness-based selection typically outperforms greedy and majority-voting when external knowledge is available (e.g., GPT-4o-mini average accuracy 63.44% in combined setting using groundedness selection). In parametric-only setting, majority-voting slightly outperformed groundedness selection.",
            "limitations_or_challenges": "Computational overhead (generate multiple candidates + claim verification for each); effectiveness depends on accurate retrieval and claim-judgment steps.",
            "comparison_to_human_or_traditional": "Provides an automated way to mimic human preference for well-supported hypotheses; human validation still necessary for high-stakes decisions.",
            "recommendations_or_best_practices": "Generate multiple hypotheses and pick by groundedness when external knowledge is provided; reserve majority-voting for pure parametric settings where groundedness evidence is unavailable.",
            "uuid": "e9661.5",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "BM25 + PubMed retrieval",
            "name_full": "BM25 retriever over PubMed corpus for evidence retrieval",
            "brief_description": "An information retrieval pipeline used in the paper to fetch supporting literature for claims using BM25 ranking over PubMed chunks; used both for hypothesis generation context (k=32) and claim verification (k=8).",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "GPT-4o-mini (used for claim verification), GPT-4o, Llama-3.1",
            "llm_description": "Retrieval component (not an LLM) paired with LLMs to provide external unstructured evidence; τ threshold set to 0.0, k=32 for generation, k=8 for claim verification.",
            "scientific_domain": "Biomedicine (literature retrieval)",
            "evaluation_method": "BM25 ranks document chunks; top-k documents concatenated as context for LLM generation or verification; used to ensure temporal integrity by restricting PMIDs ≤ 36600000.",
            "evaluation_criteria": "Relevance via BM25 score and rank; impact measured indirectly by groundedness verification rates and downstream hypothesis accuracy.",
            "benchmark_or_dataset": "PubMed corpus (pre-2023 articles for retrieval); MedRAG-processed chunks used to build BM25 index.",
            "results_summary": "BM25 retrieval provided substantial evidence coverage for claim verification and improved groundedness when combined with KG; literature-supported rationales had higher verification rates (e.g., 76.30% verified claims for literature-augmented Chemical & Gene rationales).",
            "limitations_or_challenges": "BM25 relies on lexical matching and can miss semantically relevant passages; setting k and τ trades off precision and recall; dense retrievers may perform differently in biomedical domain.",
            "comparison_to_human_or_traditional": "Emulates manual literature search; faster but limited by index design and matching heuristics.",
            "recommendations_or_best_practices": "Use BM25 on carefully chunked biomedical text (MedRAG), set τ and k explicitly (τ=0.0 used here), and combine literature retrieval with KG evidence for best groundedness.",
            "uuid": "e9661.6",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models as biomedical hypothesis generators: A comprehensive evaluation",
            "rating": 2,
            "sanitized_title": "large_language_models_as_biomedical_hypothesis_generators_a_comprehensive_evaluation"
        },
        {
            "paper_title": "Can chatgpt be used to generate scientific hypotheses",
            "rating": 2,
            "sanitized_title": "can_chatgpt_be_used_to_generate_scientific_hypotheses"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "PubTator 3.0: an ai-powered literature resource for unlocking biomedical knowledge",
            "rating": 2,
            "sanitized_title": "pubtator_30_an_aipowered_literature_resource_for_unlocking_biomedical_knowledge"
        },
        {
            "paper_title": "The probabilistic relevance framework: BM25 and beyond",
            "rating": 2,
            "sanitized_title": "the_probabilistic_relevance_framework_bm25_and_beyond"
        },
        {
            "paper_title": "Improving scientific hypothesis generation with knowledge grounded large language models",
            "rating": 2,
            "sanitized_title": "improving_scientific_hypothesis_generation_with_knowledge_grounded_large_language_models"
        },
        {
            "paper_title": "Truth-Hypo: (if present in related works) — related benchmarking work on hypothesis generation",
            "rating": 1,
            "sanitized_title": "truthhypo_if_present_in_related_works_related_benchmarking_work_on_hypothesis_generation"
        }
    ],
    "cost": 0.01417275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models
8 Jun 2025</p>
<p>Guangzhi Xiong aidong@virginia.edu 
University of Virginia</p>
<p>Eric Xie 
University of Virginia</p>
<p>Corey Williams 
University of Virginia</p>
<p>Myles Kim 
University of Virginia</p>
<p>Amir Hassan Shariatmadari 
University of Virginia</p>
<p>Sikun Guo 
University of Virginia</p>
<p>Stefan Bekiranov 
University of Virginia</p>
<p>Aidong Zhang 
University of Virginia</p>
<p>Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models
8 Jun 202527206A855BB799669A959D59A0E3FCB5arXiv:2505.14599v2[cs.CL]
Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions.However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources.Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability.To facilitate the systematic study of these challenges, we introduce Truth-Hypo, a benchmark for assessing the capabilities of LLMs in generating truthful scientific hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge.Our results show that LLMs struggle to generate truthful hypotheses.By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs.Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery.Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have transformed the landscape of artificial intelligence, demonstrating remarkable capabilities across diverse applications, from natural language understanding to creative content generation [Karanikolas et al., 2023;Franceschelli and Musolesi, 2024;Raiaan et al., 2024].These models, trained on extensive corpora of text, demonstrate an ability to analyze, summarize, and generate human-like text, enabling advancements across diverse domains.Recently, there has been a growing interest in leveraging LLMs for scientific discovery [Zhong et al., 2023;Yang et al., 2023;Kumar et al., 2023;Liu et al., 2024;Baek et al., 2024;Guo et al., 2024].Their capacity to process and synthesize vast amounts of scientific literature positions them as valuable tools in aiding researchers, particularly for tasks such as literature reviews, summarization, and even generating new hypotheses [Qi et al., 2023;Zhou et al., 2024;M. Bran et al., 2024;Wright et al., 2022;Zeng et al., 2023;D'Arcy et al., 2024;Ifargan et al., 2025;Yang et al., 2025].</p>
<p>One particularly promising application of LLMs is their use in scientific hypothesis generation, where they can assist in identifying promising research directions [Park et al., 2024;Si et al., 2024;Guo et al., 2025].By analyzing extensive scientific literature, LLMs can uncover gaps in existing knowledge and propose novel hypotheses that may not be immediately apparent to human researchers.For instance, LLMs have been successfully applied to propose novel drug combinations for breast cancer treatment, some of which were later validated in laboratory experiments, showcasing their potential to accelerate biomedical discoveries [Abdel-Rehim et al., 2024].</p>
<p>Despite these advancements, there are substantial challenges that limit the practical utility of LLMs in scientific hypothesis generation.A critical concern is the inability to evaluate the truthfulness of generated hypotheses.While LLMs can generate hypotheses that seem plausible, it remains uncertain whether these hypotheses are valid and grounded in existing knowledge or merely hallucinated and scientifically invalid.This issue is further exacerbated by the welldocumented "hallucination" problem, where LLMs confidently produce information that is factually inaccurate or unsupported, posing challenges to their reliability in scientific contexts [Jin et al., 2024].While current research has largely focused on improving the novelty and diversity of LLM-generated hypotheses, their truthfulness and grounding in established knowledge remain underexplored [Baek et al., 2024;Hu et al., 2024;Si et al., 2024].</p>
<p>To address these challenges, we introduce TruthHypo, a comprehensive benchmark for evaluating the ability of LLMs to generate truthful scientific hypotheses, and KnowHD, a knowledge-based hallucination detection framework designed to assess the groundedness of these hypotheses.Truth-Hypo, built on a biomedical knowledge graph along with a domain-specific corpus, provides a controlled environment to evaluate how well LLM-generated hypotheses align with established scientific knowledge.KnowHD focuses on analyzing the reasoning processes of LLMs to identify hypotheses that are likely hallucinated or untruthful.Our findings reveal that LLMs face significant challenges in generating truthful hypotheses.By analyzing hallucinations in the reasoning processes behind generated hypotheses, we demonstrate that groundedness scores from KnowHD serve as an effective signal for identifying truthful hypotheses from the diverse outputs of LLMs.</p>
<p>Human evaluations on open-ended hypothesis generation tasks further confirm the utility of KnowHD in identifying scientifically valid hypotheses.</p>
<p>Our main contributions are summarized as follows:</p>
<p>• We introduce TruthHypo, a comprehensive benchmark designed to evaluate the ability of LLMs to generate truthful scientific hypotheses.</p>
<p>• We propose KnowHD, a knowledge-based hallucination detection framework that assesses the groundedness of LLM-generated hypotheses and identifies hallucinated claims by analyzing the rationale behind the hypothesis generation.</p>
<p>• We provide an extensive analysis of existing LLMs on TruthHypo, highlighting their limitations and challenges in generating truthful hypotheses.</p>
<p>• Our evaluation further reveals the connection between hallucination and truthfulness of generated hypotheses, showing the effectiveness of using KnowHD to select truthful and grounded hypotheses.</p>
<p>Truthful Hypothesis Generation Benchmark</p>
<p>To systematically evaluate the ability of large language models (LLMs) to generate truthful scientific hypotheses, we introduce TruthHypo, a benchmark tailored for biomedical hypothesis generation.TruthHypo is designed to simulate realworld conditions by employing rigorous dataset construction, task formulation, and truthfulness evaluation metrics.An overview of the dataset construction, task formulation, and evaluation framework is depicted in Figure 1.</p>
<p>Dataset Construction</p>
<p>The dataset for TruthHypo is derived from PubTator 3.0 [Wei et al., 2024], a comprehensive biomedical knowledge graph that includes annotated relations (also called edges) extracted from scientific articles.To simulate the temporal progression of scientific discovery, we partitioned the graph into "seen" and "unseen" subsets based on the publication years of the corresponding articles.Relations in the "seen" subset were extracted from papers published before 2023, identified by PMIDs ≤ 366000001 .The "unseen" subset, designed to represent new discoveries, comprises relations extracted from papers published after 2024, identified by PMIDs ≥ 38200000.</p>
<p>To ensure no overlap between the two subsets, we removed the edges in the unseen subset that shared head and tail entities with those in the seen subset.In addition, to maintain quality and validity, only relations discovered by multiple articles in the test data were retained.This filtering process guarantees that the unseen subset exclusively contains knowledge unavailable before 2024, simulating the conditions of future scientific research.</p>
<p>In building the dataset, we focused on three key relation types: "Chemical &amp; Gene", "Disease &amp; Gene", and "Gene &amp; Gene".These relation types were chosen for their complementary nature, detailed annotations, and potential for objective evaluation.To construct comprehensive classification tasks for evaluating different LLMs, we augment the dataset with negative test cases to assess whether LLMs tend to make false-positive predictions on entity pairs that lack a direct relationship in the existing knowledge base.The number of negative samples (labeled as "no relation") for each relation type is controlled to align with the average number of instances across other labels of the same relation type.The final dataset has 1209 instances for the "Chemical &amp; Gene" task, 268 instances for the "Disease &amp; Gene" task, and 547 instances for the "Gene &amp; Gene" task.A summary of the dataset statistics is presented in Table 1</p>
<p>Task Formulation</p>
<p>The TruthHypo benchmark includes three tasks, corresponding to the selected relation types: "Chemical &amp; Gene", "Disease &amp; Gene", and "Gene &amp; Gene".For each task, the input is a hypothesis generation query with two entities (see Figure 5 in Appendix D for the template), and the LLM is required to hypothesize the potential relationship between them based on available knowledge and reasoning.</p>
<p>To comprehensively assess LLM performance, we evaluate their ability to generate hypotheses under various knowledge augmentation settings.In the first setting, LLMs rely solely on their parametric knowledge -information encoded in their parameters during pretraining on large corpora.This evaluates the model's intrinsic understanding and reasoning capabilities.</p>
<p>To enhance hypothesis generation, we introduce a second setting in which LLMs are augmented with structured knowledge from the "seen" knowledge graph.In this approach, key entities from the input are mapped to nodes in the graph, and multi-hop link chains connecting these nodes are explored.These chains, representing relevant relationships, are transformed into textual descriptions and provided as context for the model to use during hypothesis generation.</p>
<p>Another setting leverages information from biomedical literature using a retrieval-augmented generation (RAG) pipeline.Relevant documents are retrieved from the PubMed corpus 2 using BM25 [Robertson et al., 2009].To maintain consistency with the knowledge graph's temporal split, only articles with PMIDs ≤ 36600000 are included in the retrieval.This simulates the process of generating hypotheses based on literature available at a given point in time.</p>
<p>Finally, we consider a combined setting, where both structured knowledge from the graph and unstructured information from retrieved literature are used to support hypothesis generation.This comprehensive approach provides a more holistic context, enabling the model to reason across both sources.The LLM prompt templates we used to combine the external information with the original user instructions can be found in Figures 6, 8, 7, and 9 (Appendix D).</p>
<p>2 https://pubmed.ncbi.nlm.nih.gov/</p>
<p>Evaluation Metrics</p>
<p>To evaluate the quality of generated scientific hypotheses, we employ a set of complementary metrics tailored to different aspects of hypothesis generation.These metrics assess the performance of LLMs in identifying valid connections between entities (link-level evaluation) and predicting specific relations (relation-level evaluation).</p>
<p>For link-level evaluation, we focus on precision, recall, and F1 score.Precision measures the proportion of correctly identified connections among all hypothesized connections, emphasizing the reduction of false positives.Recall evaluates the model's ability to comprehensively identify all valid connections, capturing its sensitivity to true positives.The F1 score, as the harmonic mean of precision and recall, provides a balanced measure of performance, combining both the accuracy of predictions and the coverage of valid connections.These link-level metrics are critical for assessing the LLM's ability to hypothesize plausible relationships between entities, regardless of the specific relation type.</p>
<p>For relation-level evaluation, we employ accuracy to measure how often the generated hypotheses match the correct relation labels in the ground truth.Accuracy captures the overall correctness of hypotheses by considering both the existence of a connection and the predicted relation type.While precision, recall, and F1 focus on identifying potential connections, accuracy provides a finer-grained assessment of the model's capability to generate accurate relation labels.</p>
<p>By combining link-level and relation-level evaluations, the TruthHypo benchmark comprehensively measures the truthfulness of LLM-generated hypotheses, assessing the ability of LLMs to produce scientifically valid outputs.</p>
<p>Knowledge-based Hallucination Detection</p>
<p>As discussed earlier, a critical concern regarding the truthfulness of LLM-generated hypotheses is the occurrence of hallucinations, where models generate plausible-sounding but unsupported claims.To address this, we introduce KnowHD, a knowledge-based hallucination detection framework that evaluates the groundedness of LLM-generated hypotheses by analyzing the rationale behind their generation.KnowHD operates using scientific literature, knowledge graphs, or a combination of both as the knowledge base.An overview of the framework is presented in Figure 2.</p>
<p>To evaluate groundedness, each hypothesis and its reasoning chain are first decomposed into a set of atomic claims.This step is critical because hypotheses often consist of compound reasoning steps, some of which may be supported by existing knowledge while others may not.Parsing these into atomic claims allows a more granular evaluation of groundedness and isolates unsupported components.This step is implemented by prompting LLMs with the template shown in Figure 10 (Appendix D).</p>
<p>When using scientific literature as the knowledge base, relevant documents for each atomic claim are retrieved from the PubMed corpus, limited to articles published before 2023 (PMID ≤ 36600000).BM25 is employed to rank documents based on their relevance to the claim.To ensure computational efficiency and focus on the most relevant information, only the top-k documents are retained.The context retrieved from the literature corpus D for a claim p is defined as:</p>
<p>Rationale of Generated Hypothesis
context D (p) = {d 1 , d 2 , • • • , d k | d i ∈ D, BM25(p; d i ) ≥ τ, rank(d i ) ≤ k},(1)
where
d i represents a document in the corpus, BM25(p; d i )
is the relevance score assigned to the document for the claim p. τ is a threshold ensuring relevance, and rank(d i ) denotes the rank of d i in the BM25-retrieved list.When using a knowledge graph G as the knowledge base, the context for a claim is derived from the graph structure.For a claim p, relevant knowledge is extracted as:
context G (p) = {(e h , r, e t ) ∈ G |{e h , e t } ⊆ V(p) } , (2)
where (e h , r, e t ) represents an edge in the knowledge graph with head entity e h , tail entity e t , and relation r.The set V(p) contains all entities mentioned in the claim p.</p>
<p>The groundedness of a claim is determined based on whether the given context information (context D , context G , or context D ∪ context G ) can fully support the claim, which is implemented by prompting LLMs to provide a judgment using the template in Figure 12 (Appendix D).If the concatenated context collectively entails the claim, it is considered grounded.The overall groundedness of a hypothesis h is computed as:
groudedness(h) = 1 |C(h)| p∈C(h) 1[context(p) |= p], (3)
where C(h) represents the set of atomic claims for hypothesis h, and 1[x |= y] returns 1 if x entails y and 0 otherwise.The
context(p) can be context D (p), context G (p), or context D (p)∪ context G (p).
By offering both literature-based and graph-based contexts, KnowHD provides a robust framework for hallucination detection, offering flexibility to adapt to the available knowledge sources.This systematic evaluation of atomic claims enables a detailed assessment of the groundedness of hypotheses, identifying unsupported components and improving the reliability of LLM-generated outputs.</p>
<p>Benchmark Analysis on TruthHypo</p>
<p>Experiment Settings</p>
<p>To assess the ability of existing LLMs to generate truthful scientific hypotheses, we selected a diverse range of models varying in type and size.The Llama-3 family [Dubey et al., 2024] represents open-source LLMs, while the GPT-4 family [Achiam et al., 2023] exemplifies proprietary models.From each family, we evaluated two LLMs of different sizes (Llama-3.1-8B&amp; Llama-3.1-70B,GPT-4o-mini &amp; GPT-4o) to investigate size-related differences in performance.All LLMs were trained on the knowledge available before 2024, preventing recall of the exact knowledge for hypothesis generation.More implementation details are in Appendix A.</p>
<p>The TruthHypo benchmark evaluates LLMs across four distinct settings: (1) parametric knowledge only, (2) parametric knowledge with knowledge graphs (KG), (3) parametric knowledge with literature (Lit.), and (4) parametric knowledge with both KG and literature.These settings allow us to explore the impact of external knowledge sources on hypothesis generation.The F1 and accuracy scores of different models are reported in this section.More detailed results on the precision and recall can be found in Appendix C.</p>
<p>Comparison of LLMs in Truthful Hypothesis Generation</p>
<p>Table 2 presents the evaluation results for different LLMs and knowledge settings on TruthHypo.Across all tasks, the results indicate that most LLMs struggle to generate truthful scientific hypotheses, with only GPT-4o achieving mean accuracies exceeding 60%.Additionally, we can observe that link-level F1 scores are higher than relation-level accuracy scores, which indicates that LLMs can identify potential connections between entities but often fail to accurately predict the specific relationships.</p>
<p>For models from the same family with different sizes, larger LLMs tend to generate scientific hypotheses more likely to be truthful.This can be attributed to two main factors.First, larger LLMs generally perform better because they can store and leverage more knowledge in their parameters, as shown by the results of parametric knowledge-only setting.Second, LLMs of different sizes have diverse capabilities to process external knowledge for hypothesis generation.For example, GPT-4o-mini shows a modest 1.14% accuracy improvement when augmented with KG and literature, whereas GPT-4o achieves a more substantial 5.14% increase under the same conditions.This suggests that larger LLMs can better utilize additional context to reason about truthful scientific hypotheses.Similar trends are observed when comparing Llama-3.1-8B and Llama-3.1-70B.Interestingly, smaller models, such as Llama-3.1-8B,sometimes experience decreased performance when information from KG and literature is introduced.This degradation may stem from challenges in effectively integrating internal and external information, which can disrupt the model's reasoning processes.Performance differences are also observed across the three relation types: "Chemical &amp; Gene", "Disease &amp; Gene" and "Gene &amp; Gene".Notably, all larger models, including GPT-4o, GPT-4o-mini, and Llama-3.1-70B,tend to perform better on "Chemical &amp; Gene" tasks than on the other two types.This trend suggests that the "Chemical &amp; Gene" task may be more aligned with the pre-trained knowledge or reasoning capabilities of these models.In contrast, the smaller Llama-3.1-8Bshows a more inconsistent pattern, with performance varying across tasks and settings, likely reflecting its more limited parametric capacity and reasoning abilities.These variations in performance across relation types may be attributed to differences in training data distributions or the complexity of the relation types themselves.The relatively stronger performance on the "Chemical &amp; Gene" task highlights potential domain-specific biases or strengths in the LLMs, offering insights into their suitability for targeted applications in real-world scientific discovery.</p>
<p>Hallucination Detection on LLM-generated Hypotheses</p>
<p>To assess the groundedness of the generated hypotheses, we evaluated their rationales using KnowHD under various knowledge settings.KnowHD measures how well a hypothesis is supported by structured knowledge (KG), unstructured knowledge (literature), or both combined.The groundedness evaluation results for hypotheses generated by GPT-4o-mini are presented in Table 3.</p>
<p>The results demonstrate distinct contributions of KG and literature to grounding hypotheses.For example, KnowHD with the literature as the support knowledge base can verify 76.30% claims in the rationales of literature-augmented 'Chemical &amp; Gene" hypotheses.However, the hallucination detector can hardly verify the rationale generated based on adding KG information to parametric knowledge with only 51.08% of the claims being grounded.Combining KG and literature yields the highest groundedness scores, effectively leveraging the complementary strengths of both sources to identify grounded claims and detect hallucinations.</p>
<p>To further explore the relationship between hallucination and truthfulness, Figure 3 examines mean accuracy as a function of groundedness scores.Hypotheses were grouped based on their groundedness scores, and the average accuracy for each group was calculated.The figure reveals a positive correlation between groundedness scores and hypothesis truthfulness.As groundedness scores increase, the likelihood  of the hypothesis being truthful also increases.For example, GPT-4o-mini achieves a mean accuracy of 60.96% on "Chemical &amp; Gene" tasks under the combined KG + Literature setting, but this rises to 72.77% for hypotheses with groundedness scores above 80%.These findings underscore the potential of KnowHD to identify hypotheses with a higher probability of being truthful, particularly in contexts enriched with external knowledge.</p>
<p>Improving Generation of Truthful Hypotheses with KnowHD</p>
<p>To validate the utility of KnowHD on enhancing hypothesis generation, we prompted LLMs to generate five candidate hypotheses for each input and selected the one with the highest groundedness score as the final output.This approach was compared to two baselines: the greedy search method, where the hypothesis is generated using greedy next-token selection by the LLM, and the self-consistency method [Wang et al., 2022], which selects hypotheses based on majority voting across multiple predictions.As shown in Figure 4, groundedness-based hypothesis selection generally outperforms both the greedy search and majority-voting methods across most knowledge settings.In the parametric knowledge-only setting, the majority-voting method achieves slightly higher accuracy (61.86%) compared to groundedness-based selection (59.83%).However, as external knowledge is introduced, groundedness-based selection demonstrates consistent improvements over both baselines.For example, in the combined parametric + KG + Literature setting, GPT-4o-mini achieves an average accuracy of 63.44% when groundedness-based selection is used, approaching the performance of the larger GPT-4o model.</p>
<p>These results highlight the effectiveness of groundedness scores in scenarios where external knowledge is incorporated, as they help identify hypotheses that are more likely to be truthful.By detecting hallucinations in reasoning steps and focusing on grounded hypotheses, KnowHD provides a robust mechanism for enhancing the reliability and truthfulness of LLM-generated scientific hypotheses.</p>
<p>Human Study on Open-ended Tasks</p>
<p>To further assess the generalizability of KnowHD's effectiveness in selecting truthful hypotheses, we conducted experiments on open-ended hypothesis generation tasks.These tasks were designed to evaluate whether KnowHD could reliably identify hypotheses with a higher likelihood of truthfulness across broader and less structured generation scenarios.</p>
<p>For this analysis, we utilized the publicly available hypothesis generation dataset introduced by Qi et al. [2024], which involves generating free-form hypotheses based on given background information.We selected GPT-4o-mini as the tested LLM and enhanced its hypothesis generation process by incorporating external knowledge from scientific literature and knowledge graphs (KG).The model was prompted to generate five distinct scientific hypotheses for each input.These hypotheses were then evaluated by KnowHD, which assessed their groundedness based on their alignment with both structured (KG) and unstructured (literature) knowledge sources.</p>
<p>To analyze the relationship between groundedness scores and hypothesis truthfulness, we filtered generated hypotheses to create pairs with contrasting groundedness levels.For each input, we identified one hypothesis with the highest ground-  edness score and another with the lowest.We retained pairs where the higher groundedness score was at 30% greater than the lower score.This filtering resulted in 54 pairs of hypotheses with significant differences in groundedness levels.To validate KnowHD's effectiveness, we involved two domain experts to annotate each pair (80% agreement), selecting the hypothesis they deemed more likely truthful based on the given information.Additionally, GPT-4o was prompted to analyze the same pairs and provide its judgment.Results of this annotation study, summarized in Table 4, report the selection ratio for each group, defined as the proportion of hypotheses in each group identified as more truthful.</p>
<p>The results demonstrate a significant relationship between groundedness scores and the perceived truthfulness of hypotheses.Hypotheses with higher groundedness scores were consistently more likely to be selected as truthful by both human experts and GPT-4o, as indicated by the substantial differences in selection ratios.These findings highlight the utility of KnowHD in distinguishing truthful hypotheses, even in unstructured, open-ended generation tasks.By effectively leveraging groundedness as a criterion, KnowHD provides a robust mechanism for improving the reliability of LLMgenerated hypotheses, reinforcing its potential for facilitating real-world scientific discovery processes.</p>
<p>6 Related Work</p>
<p>Scientific Hypothesis Generation</p>
<p>The use of LLMs for scientific hypothesis generation is a rapidly growing field, leveraging the ability of these models to process and synthesize vast amounts of scientific literature [Qi et al., 2023;Yang et al., 2023;Zhou et al., 2024;Ciucȃ et al., 2023;Skarlinski et al., 2024;Radensky et al., 2024;Xiong et al., 2024c;Guo et al., 2024].LLMs have been applied in identifying research gaps and generating novel hypotheses, with notable successes in areas such as drug discovery, where generated hypotheses have led to experimentally validated drug combinations [Abdel-Rehim et al., 2024].Despite these advancements, most existing studies emphasize the novelty and diversity of hypotheses without addressing the critical aspect of truthfulness [Qi et al., 2024;Baek et al., 2024;Wang et al., 2023;Hu et al., 2024;Li et al., 2024].The prevalent hallucination problem exacerbates this issue, as LLMs often generate hypotheses that appear plausible but lack factual support [Huang et al., 2023].This gap motivates the development of TruthHypo, a benchmark explicitly designed to assess the ability of LLMs to generate truthful and grounded scientific hypotheses.</p>
<p>Knowledge Graph Reasoning</p>
<p>Knowledge graph reasoning involves inferring missing facts or relationships within a knowledge graph, with tasks such as link prediction, entity classification, and relation extraction being extensively studied [Nickel et al., 2015;Lin et al., 2015;Ji et al., 2021;Shu et al., 2024].Traditional link prediction focuses on predicting edges between entities based on graph structure.These tasks primarily target structured graph completion, emphasizing pattern detection rather than creative reasoning [Zhang and Chen, 2018;Krenn et al., 2023;Liu et al., 2023;Wu et al., 2023;Gu and Krenn, 2024].TruthHypo introduces a novel benchmark that centers on LLM-driven scientific hypothesis generation, leveraging LLMs' ability to flexibly integrate external knowledge through contextual inputs.Unlike static graph reasoning, TruthHypo evaluates how well LLMs generate grounded and truthful hypotheses.This shift highlights the growing role of LLMs in scientific discovery and bridges the gap between symbolic graph reasoning and natural language creativity.</p>
<p>Retrieval-augmented Generation</p>
<p>Retrieval-augmented generation (RAG) has emerged as a powerful approach for improving the factual accuracy and relevance of LLM outputs by integrating external knowledge during the generation process.This technique has been applied with literature retrieval, as demonstrated by [Lewis et al., 2020], to dynamically incorporate up-to-date information into model outputs.Retrieval-augmented generation methods enhance the ability of LLMs to ground their outputs in external knowledge, making them particularly valuable in tasks requiring factual accuracy, such as scientific text generation [Lála et al., 2023;Munikoti et al., 2023].In addition to literature retrieval, retrieval-augmented generation using knowledge graphs has gained attention for its potential to provide structured, domain-specific knowledge during text generation [Peng et al., 2024;Ma et al., 2024;Wang et al., 2025].Truth-Hypo builds on this paradigm by integrating both literature and knowledge graph retrieval to provide a robust evaluation of LLMs' ability to generate truthful scientific hypotheses.This dual approach enables a comprehensive analysis of the role of external knowledge in mitigating hallucinations and ensuring the groundedness of generated hypotheses.</p>
<p>Conclusion</p>
<p>We presented TruthHypo, a benchmark for evaluating the ability of LLMs to generate truthful scientific hypotheses, and KnowHD, a framework for detecting hallucinations by assessing groundedness in reasoning.Through extensive evaluation, we highlighted the limitations of existing LLMs and demonstrated that selecting highly grounded hypotheses improves truthfulness.These contributions offer valuable insights for improving the reliability and utility of LLMs in scientific discovery.</p>
<p>A Implementation Details</p>
<p>For the retrieval of external knowledge from scientific literature, we implemented the information retrieval system by adopting the BM25 retriever [Robertson et al., 2009] for processed PubMed chunks provided by the MedRAG toolkit [Xiong et al., 2024a,b].BM25 (Best Matching 25) is a probabilistic retrieval model that ranks documents based on term frequency, document length normalization, and the specificity of terms through inverse document frequency (IDF).We selected BM25 as our text retriever because it is particularly effective for the biomedical domain, where dense retrievers often struggle to encode the nuanced semantics of biomedical terms such as gene names [Luo et al., 2022].BM25's reliance on exact term matching with statistical weighting makes it well-suited for capturing term-specific relevance in structured biomedical text.In our experiments, τ in Equation ( 1) is set as 0.0.The number of retrieved documents is set as k = 32 for hypothesis generation, and k = 8 for claim verification.</p>
<p>To identify biomedical entities in a given claim, we used a two-step process.In the first step, we prompted LLMs to extract the entity mentions directly from the claim (Figure 11).This step focused on identifying relevant biomedical terms, such as gene names, proteins, or diseases, without additional processing or complex workflows.The extracted entity mentions were then used in the second step, where each mention was matched to its unified representation in the PubTator 3.0 knowledge graph.This matching was implemented using a BM25 retriever.For constructing the BM25 index, each piece of text, or "chunk", was designed by concatenating all possible mentions of a given entity stored in PubTator 3.0.By leveraging BM25's ranking capabilities, we retrieved the most relevant chunk corresponding to each entity mention, ensuring accurate alignment with PubTator's unified entities.</p>
<p>B Computational Cost</p>
<p>Table 5 shows the number of all tokens used in experiments for Table 2.It shows that the additional knowledge from either the knowledge graph (KG) or literature (Lit.) will significantly increase the number of input tokens.In particular, the literature brings more tokens than KG, as the knowledge in KG is always structured and summarized.While input lengths vary across different settings, output lengths are relatively stable, a consistent pattern shown in different LLMs.</p>
<p>C Additional Quantitative Results on TruthHypo</p>
<p>Table 2 presents the F1 score of various LLMs on the Truth-Hypo benchmark, evaluating their ability to identify the existence of a new relation given current knowledge.To provide a more granular analysis, Table 6 breaks down the results into precision and recall for different tasks, offering insights into the strengths and weaknesses of each model and knowledge augmentation setting.</p>
<p>From the results in Table 6, we observe that smaller LLMs, such as Llama-3.1-8B,tend to achieve higher recall scores across all tasks, along with relatively lower precision.This indicates that while these models can generate a comprehensive set of hypotheses, they are prone to a high false positive rate, which could pose challenges in real-world applications, such as scientific hypothesis generation, where precision is often critical.High false positive rates could result in wasted time and resources when pursuing hypotheses that are unlikely to hold upon experimental validation.</p>
<p>Given that validating new biomedical hypotheses often requires months or even years of research, ensuring high precision in hypothesis generation is of paramount importance.Among the tested models, GPT-4o with external knowledge from the literature achieved the highest precision across all tasks, demonstrating its ability to generate hypotheses with fewer false positives.However, this precision came at the expense of lower recall, especially when compared to GPT-4o with knowledge augmentations from both literature and knowledge graphs (KG).This trade-off highlights the importance of balancing precision and recall based on the specific requirements of a given application.</p>
<p>When comparing different knowledge settings, we found that the improvements provided by external knowledge sources varied across tasks and models.For example, knowledge graph (KG) information significantly enhanced the precision of all LLMs on tasks involving "Disease &amp; Gene" and "Gene &amp; Gene" relations, but it did not notably improve the precision of GPT-4o on the "Chemical &amp; Gene" task.In contrast, the literature knowledge augmentation slightly improved the precision of all LLMs except GPT-4o-mini.Interestingly, the setting that combined both knowledge sources provided a more balanced precision improvement, offering a middle ground between the individual benefits of KG and literature-based augmentations.</p>
<p>Additionally, Table 6 reveals that larger models such as GPT-4o consistently outperformed smaller models in precision, regardless of the knowledge setting, reflecting their ability to integrate complex external information effectively.This highlights the potential of larger models to better utilize structured and unstructured knowledge sources for hypothesis generation.However, smaller models, with their higher recall, may still serve as useful tools for exploratory or broad hypothesis generation tasks where exhaustive coverage is prioritized over precision.</p>
<p>Overall, the analysis demonstrates that the choice of LLM and knowledge augmentation strategy should be guided by the specific trade-offs between precision and recall that align with the requirements of the downstream task.For biomedi-cal applications, where precision is often paramount, leveraging models like GPT-4o with literature-based augmentations appears to be the most effective approach.</p>
<p>To further understand the limitations of hypothesis generation with high groundedness scores, we conducted an indepth analysis of the error patterns.We identified two representative types of errors: (1) cases where the LLM incorrectly infers that there is no association between the given entities, despite supporting evidence; and (2) cases where the model simply echoes or paraphrases the provided context without engaging in substantive reasoning or hypothesis formation.These findings highlight the need for more robust mechanisms to ensure both accurate association detection and genuine reasoning in hypothesis generation, enhancing the interpretability and trustworthiness of the overall system [Doshi-Velez and Kim, 2017;Loh et al., 2022;Miller, 2023;Sinha et al., 2024a,b].</p>
<p>D Prompt Templates for LLMs in Experiments</p>
<p>Figure 5 shows the template we used to construct a hypothesis generation query given two different entities.The prompt templates for the use of LLMs in the "Parametric", "Parametric + KG", "Parametric + Lit.", and "Parametric + KG + Lit." settings are presented in Figures 6, 7, 8, 9, respectively.These templates were designed to guide the LLMs in effectively leveraging various sources of knowledge while maintaining a pre-determined structure in the model output to facilitate consistent parsing and downstream analysis.</p>
<p>Figure 10 shows the template for LLMs to extract scientific claims from the entire rationale.For the identification of biomedical entities and the use of LLMs for claim verification, we employed the templates shown in Figures 11 and 12, respectively.The entity identification templates (Figure 11) were crafted to enable the LLMs to extract precise mentions of biomedical entities such as genes or diseases from textual claims.These prompts were carefully designed to minimize ambiguity, ensuring that entities sharing the same mention could be properly distinguished using their unique IDs.</p>
<p>Knowledge</p>
<p>Figure 1 :
1
Figure 1: Overview of the TruthHypo benchmark, including dataset construction, task formulation, and truthfulness evaluation.</p>
<p>Figure 2 :
2
Figure 2: Overview of the KnowHD hallucination detection framework.Hypotheses are parsed into atomic claims, which are then evaluated for groundedness using a knowledge graph, scientific literature or both as knowledge sources.</p>
<p>Figure 3 :
3
Figure 3: Mean accuracy corresponding to different levels of groundedness.Hypotheses are grouped based on their groundedness scores provided by KnowHD (KG + Literature).Only groups with no less than 10 hypotheses are shown in the plots.The dot size reflects the number of samples in each level of groundedness.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Prompt template for constructing user input with given entities.</p>
<p>Table 1 :
1
. Statistics of various tasks in the TruthHypo benchmark.
TaskLabel# Instancepositive correlate328Chemical &amp; Genenegative correlate478no relation403stimulate104Disease &amp; Geneinhibit75no relation89positive correlate247Gene &amp; Genenegative correlate118no relation182</p>
<p>Table 2 :
2
Performance comparison of different LLMs on the TruthHypo benchmark across various knowledge settings.The metrics reported are link-level F1 and relation-level accuracy (Acc) for each task (Chemical &amp; Gene, Disease &amp; Gene, Gene &amp; Gene), as well as their averages."Param."denotes parametric knowledge, while "KG" and "Lit."refer to knowledge graphs and literature, respectively.All scores are percentages (%).
KnowledgeLLMChemical &amp; Gene Disease &amp; Gene Gene &amp; Gene F1 Acc F1 Acc F1 AccAverage F1 AccLlama-3.1-8B80.1642.4379.3741.0479.19 46.07 66.90 43.23ParametricLlama-3.1-70B 81.3652.4483.2954.4876.66 49.91 71.54 52.03[Wei et al., 2022]GPT-4o-mini83.3161.2981.8459.3379.32 53.02 75.49 58.79GPT-4o80.7466.1775.3854.8571.56 55.58 73.17 61.81Llama-3.1-8B81.3740.6179.5948.1379.61 48.45 70.65 43.73Parametric + KGLlama-3.1-70B 87.8562.8667.6252.2478.29 58.14 79.10 60.18[Baek et al., 2024]GPT-4o-mini86.4257.6574.1755.6081.65 62.34 79.40 58.65GPT-4o88.6663.8579.5056.7282.73 61.06 81.62 62.15Llama-3.1-8B80.7846.0780.4643.2879.91 42.60 68.58 44.76Parametric + Lit.Llama-3.1-70B 82.5656.7484.1652.9979.18 51.55 73.37 54.84[Lewis et al., 2020]GPT-4o-mini85.2859.8085.7153.7381.50 51.19 77.08 56.67GPT-4o79.5265.9275.8455.9764.69 51.92 71.84 60.82Llama-3.1-8B75.9836.4877.5841.4279.19 45.70 65.37 39.62Parametric + KGLlama-3.1-70B 84.8059.3177.6456.3481.24 55.76 77.37 57.95+ LiteratureGPT-4o-mini88.3460.9684.4758.2184.17 58.50 81.42 59.93GPT-4o89.7169.3182.8662.3185.91 63.99 83.55 66.95</p>
<p>Table 4 :
4
Resultsof analysis on open-ended hypothesis generation tasks."GPT" and "Human" denote the selection ratios by GPT-4o and human experts, respectively.All scores are percentages (%).pvalues were calculated using Wilcoxon signed-rank test and Z-test.</p>
<p>Table 5 :
5
Summary of #tokens used for all experiments in Table 2.
LLMTypeParam.+KGSetting +Lit.+KG+Lit.LlamaInput295.8k1.7M25.8M27.2M-3.1-8BOutput 811.1k1.0M782.5k1.2MLlamaInput295.8k1.7M25.8M27.2M-3.1-70BOutput 813.7k 881.2k 777.6k767.8kGPT-4oInput295.8k1.7M25.8M27.2M-miniOutput 751.6k 684.0k 787.2k707.1kGPT-4oInput Output 909.9k 839.1k 891.5k 295.8k 1.7M 25.8M27.2M 875.3k</p>
<p>Table 6 :
6
Performance comparison of different LLMs on the TruthHypo benchmark across various knowledge settings, with precision and recall as the evaluation metrics."Prec"denotes the link-level precision, while "Recall" represents the link-level recall.Prompt template for constructing user input with given entitiesCan we hypothesize the potential relation between {{entity type 1}} {{entity name 1}} ({{entity ID 1}}) and {{entity type 2}} {{entity name 2}} ({{entity ID 2}})?The final hypothesis can be one of [{{relation label 1}}, {{relation label 2}}, 'no relation'].
LLMChemical &amp; Gene Disease &amp; Gene Gene &amp; Gene Prec Recall Prec Recall Prec Recall Prec Recall AverageLlama-3.1-8B 67.5798.5166.7997.7766.92 96.99 67.29 98.00ParametricLlama-3.1-70B 74.6989.3375.2393.3073.13 80.55 74.37 87.48Wei et al. [2022]GPT-4o-mini83.0083.6279.4784.3675.50 83.56 80.37 83.70GPT-4o90.5972.8382.6769.2783.27 62.74 87.60 69.63Llama-3.1-8B 71.4294.5474.0486.0372.16 88.77 71.93 91.85Parametric + KGLlama-3.1-70B 90.6385.2493.1453.0788.58 70.14 90.34 76.89Baek et al. [2024]GPT-4o-mini86.3786.4891.0662.5792.39 73.15 88.27 79.70GPT-4o86.2791.1991.3070.3987.96 78.08 87.21 84.89Llama-3.1-8B 68.8297.7768.3697.7768.49 95.89 68.67 97.26Parametric + Lit.Llama-3.1-70B 74.9291.9475.5694.9774.58 84.38 74.92 90.30Lewis et al. [2020]GPT-4o-mini78.1893.8080.1092.1874.94 89.32 77.55 92.37GPT-4o92.7369.6083.7869.2789.37 50.68 90.62 64.44Llama-3.1-8B 68.2185.7370.6486.0369.96 91.23 69.01 87.26Parametric + KGLlama-3.1-70B 84.1385.4887.4169.8380.05 82.47 83.33 82.59+ LiteratureGPT-4o-mini82.6194.9182.4586.5983.16 85.21 82.73 91.19GPT-4o86.6193.0584.8081.0187.50 84.38 86.61 89.11
PMID is the unique identifier of the paper where the edge was extracted.
AcknowledgementsThis work is supported in part by the US National Science Foundation under grants 2217071, 2213700, 2106913,  2008208, and NIH grant 1R01LM014012.Prompt template for hypothesis generation with knowledge from parameters and KG You are a scientist.Your task is to generate a scientific hypothesis following given instructions.Prompt template for claim identification ### Statement {{statement}}Summarize the statement as a list of claims which will be further verified by external resources.Output the summarized claims in the JSON format: '''json{"claims": ["claim1", ...]}'''Prompt template for entity recognition ### Background {{background}}Extract key entities from the background statement that will be used to search for relevant information in an external knowledge graph.Each entity should be extracted as "entity type (e.g., Disease/Chemical/-Gene/Mutation) entity name (entity id if presented)".Output the extracted entities in the JSON format: '''json{"entities": ["entity1", ...]}'''
Scientific hypothesis generation by a large language model: Laboratory validation in breast cancer treatment. Abbi Abdel-Rehim, Hector Zenil, Oghenejokpeme Orhobor, Marie Fisher, Ross J Collins, Elizabeth Bourne, Gareth W Fearnley, Emma Tate, Holly X Smith, Larisa N Soldatova, arXiv:2405.122582024arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy. Ioana Ciucȃ, Yuan-Sen, Sandor Ting, Kartheik Kruk, Iyer, arXiv:2306.116482023arXiv preprint</p>
<p>Marg: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024arXiv preprint</p>
<p>Towards a rigorous science of interpretable machine learning. Finale Doshi, - Velez, Been Kim, arXiv:1702.086082017arXiv preprint</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>On the creativity of large language models. Giorgio Franceschelli, Mirco Musolesi, AI &amp; SOCIETY. 2024</p>
<p>Forecasting high-impact research topics via machine learning on evolving knowledge graphs. Xuemei Gu, Mario Krenn, arXiv:2402.086402024arXiv preprint</p>
<p>Embracing foundation models for advancing scientific discovery. Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Aidong Xiong, Zhang, 2024 IEEE International Conference on Big Data (BigData). IEEE2024</p>
<p>Ideabench: Benchmarking large language models for research idea generation. Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Albert Xiong, Myles Huang, Corey M Kim, Stefan Williams, Aidong Bekiranov, Zhang, 31st SIGKDD Conference on Knowledge Discovery and Data Mining -Datasets and Benchmarks Track. 2025</p>
<p>Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.142552024arXiv preprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, arXiv:2311.052322023arXiv preprint</p>
<p>Autonomous llm-driven research-from data to human-verifiable research papers. Tal Ifargan, Lukas Hafner, Maor Kern, Ori Alcalay, Roy Kishony, NEJM AI. 21AIoa2400555, 2025</p>
<p>A survey on knowledge graphs: Representation, acquisition, and applications. Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, Philip Yu, IEEE transactions on neural networks and learning systems. 202133</p>
<p>Demystifying large language models for medicine: A primer. Qiao Jin, Nicholas Wan, Robert Leaman, Shubo Tian, Zhizheng Wang, Yifan Yang, Zifeng Wang, Guangzhi Xiong, Po-Ting Lai, Qingqing Zhu, arXiv:2410.188562024arXiv preprint</p>
<p>Large language models versus natural language understanding and generation. Nikitas Karanikolas, Eirini Manga, Nikoletta Samaridi, Eleni Tousidou, Michael Vassilakopoulos, Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics. the 27th Pan-Hellenic Conference on Progress in Computing and Informatics2023</p>
<p>Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. Mario Krenn, Lorenzo Buffoni, Bruno Coutinho, Sagi Eppel, Jacob Gates Foster, Andrew Gritsevskiy, Harlin Lee, Yichao Lu, Nima João P Moutinho, Sanjabi, Nature Machine Intelligence. 5112023</p>
<p>Mycrunchgpt: A chatgpt assisted framework for scientific machine learning. Varun Kumar, Leonard Gleyzer, Adar Kahana, Khemraj Shukla, George Em Karniadakis, arXiv:2306.155512023arXiv preprint</p>
<p>Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, arXiv:2312.07559Paperqa: Retrieval-augmented generative agent for scientific research. 2023arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Chain of ideas: Revolutionizing research via novel idea development with llm agents. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, arXiv:2410.131852024arXiv preprint</p>
<p>Learning entity and relation embeddings for knowledge graph completion. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, Xuan Zhu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201529</p>
<p>A survey on graph classification and link prediction based on gnn. Xingyu Liu, Juan Chen, Quan Wen, arXiv:2307.008652023arXiv preprint</p>
<p>Conversational drug editing using retrieval and domain feedback. Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Application of explainable artificial intelligence for healthcare: A systematic review of the last decade. Wen Hui, Loh, Ping Chui, Silvia Ooi, Prabal Seoni, Filippo Datta Barua, Molinari, Acharya Rajendra, Computer methods and programs in biomedicine. 2011-2022. 2022226107161</p>
<p>Improving biomedical information retrieval with neural retrievers. Man Luo, Arindam Mitra, Tejas Gokhale, Chitta Baral, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, Nature Machine Intelligence. 2024</p>
<p>Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo, Think-on-graph 2.0: Deep and interpretable large language model reasoning with knowledge graph-guided retrieval. arXiv e-prints. 20242407</p>
<p>Explainable ai is dead, long live explainable ai! hypothesis-driven decision support using evaluative ai. Tim Miller, Proceedings of the 2023 ACM conference on fairness, accountability, and transparency. the 2023 ACM conference on fairness, accountability, and transparency2023</p>
<p>Evaluating the effectiveness of retrievalaugmented large language models in scientific document reasoning. Sai Munikoti, Anurag Acharya, Sridevi Wagle, Sameera Horawalavithana, arXiv:2311.043482023arXiv preprint</p>
<p>A review of relational machine learning for knowledge graphs. Maximilian Nickel, Kevin Murphy, Evgeniy Volker Tresp, Gabrilovich, Proceedings of the IEEE. 10412015</p>
<p>Can chatgpt be used to generate scientific hypotheses. Yang Jeong, Park , Daniel Kaplan, Zhichu Ren, Chia-Wei Hsu, Changhao Li, Haowei Xu, Sipei Li, Ju Li, Journal of Materiomics. 1032024</p>
<p>Graph retrieval-augmented generation: A survey. Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang, arXiv:2408.089212024arXiv preprint</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.059652023arXiv preprint</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Jinfang Hu, Bowen Zhou, First Conference on Language Modeling. 2024</p>
<p>Scideator: Humanllm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>Most Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami Azam. A review on large language models: Architectures, applications, taxonomies, open issues and challenges. Mohaimenul Azam Khan Raiaan, Md Saddam Hossain, Kaniz Mukta, Nur Fatema, Sadman Mohammad Fahad, Sakib, 2024IEEE Access</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Knowledge graph large language model (kg-llm) for link prediction. Dong Shu, Tianle Chen, Mingyu Jin, Chong Zhang, Mengnan Du, Yongfeng Zhang, arXiv:2403.073112024arXiv preprint</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Colidr: Concept learning using aggregated disentangled representations. Sanchit Sinha, Guangzhi Xiong, Aidong Zhang, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>A selfexplaining neural architecture for generalizable concept learning. Sanchit Sinha, Guangzhi Xiong, Aidong Zhang, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence2024</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. Sam Michael D Skarlinski, Jon M Cox, James D Laurent, Michaela Braza, Hinks, J Michael, Manvitha Hammerling, Ponnapati, Andrew D Samuel G Rodriques, White, arXiv:2409.137402024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.142592023arXiv preprint</p>
<p>Knowledge graph retrievalaugmented generation for llm-based recommendation. Shijie Wang, Wenqi Fan, Yue Feng, Xinyu Ma, Shuaiqiang Wang, Dawei Yin, arXiv:2501.022262025arXiv preprint</p>
<p>Pubtator 3.0: an ai-powered literature resource for unlocking biomedical knowledge. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Nucleic Acids Research. 35e2352022. 2024Chain-ofthought prompting elicits reasoning in large language models</p>
<p>Generating scientific claims for zero-shot scientific fact checking. Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, Lucy Lu, Wang , arXiv:2203.129902022arXiv preprint</p>
<p>Dynamic link prediction using graph representation learning with enhanced structure and temporal information. Chaokai Wu, Yansong Wang, Tao Jia, 2023 26th International Conference on Computer Supported Cooperative Work in Design (CSCWD). IEEE2023</p>
<p>Benchmarking retrieval-augmented generation for medicine. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang, Findings of the Association for Computational Linguistics: ACL 2024. 2024</p>
<p>Improving retrievalaugmented generation in medicine with iterative follow-up questions. Guangzhi Xiong, Qiao Jin, Xiao Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang, Biocomputing 2025: Proceedings of the Pacific Symposium. World Scientific2024</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2411.023822024arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, arXiv:2309.027262023arXiv preprint</p>
<p>Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, 2nd AI4Research Workshop: Towards a Knowledge-grounded Scientific Research Lifecycle. 2025</p>
<p>Meta-review generation with checklist-guided iterative introspection. Qi Zeng, Mankeerat Sidhu, Pong Hou, Lu Chan, Heng Wang, Ji, arXiv:2305.146472023arXiv preprint</p>
<p>Link prediction based on graph neural networks. Muhan Zhang, Yixin Chen, Advances in neural information processing systems. 201831</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, Advances in Neural Information Processing Systems. 202336</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7704 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7704</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7704</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-275789786</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.11840v1.pdf" target="_blank">Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction</a></p>
                <p><strong>Paper Abstract:</strong> Systematic reviews are time-consuming endeavors. Historically speaking, knowledgeable humans have had to screen and extract data from studies before it can be analyzed. However, large language models (LLMs) hold promise to greatly accelerate this process. After a pilot study which showed great promise, we investigated the use of freely available LLMs for extracting data for systematic reviews. Using three different LLMs, we extracted 24 types of data, 9 explicitly stated variables and 15 derived categorical variables, from 112 studies that were included in a published scoping review. Overall we found that Gemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably well, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with human coding, respectively. While promising, these results highlight the dire need for a human-in-the-loop (HIL) process for AI-assisted data extraction. As a result, we present a free, open-source program we developed (AIDE) to facilitate user-friendly, HIL data extraction with LLMs.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7704.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7704.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIDE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-assisted Data Extraction (AIDE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A free, open-source R GUI that coordinates LLM API calls to extract structured data from full-text scholarly PDFs and presents extracted items to a human reviewer for mandatory validation and recording into a coding form.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Noah L. Schroeder, Chris Davis Jaldi, Shan Zhang</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AIDE (Human-in-the-loop LLM-assisted data extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Sends a single API request per paper containing the structured full text (or extracted text) plus a coding-form-style set of prompts; the LLM returns values for 24 predefined variables which are auto-populated into a GUI where a human inspects, corrects if needed, and records each data point.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Full-text PDFs (sent directly to Gemini models) and extracted plain text (sent to Mistral); coding-form prompts supplied as row 1 of the uploaded CSV</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured coding form rows (24 explicit and derived variables per study), with provenance links to source pages when available</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Single-request prompting: all 24 prompts sent together in one API call; intentionally generic prompts (no extensive prompt engineering) to reflect average researcher usage</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Google Gemini 1.5 Flash, Google Gemini 1.5 Pro, Mistral Large 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>112 studies from a published scoping review (public OSF repository referenced in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact Match and 'Accurate Match' agreement against human coding, plus Cohen's Kappa</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Overall Exact Match across 24 variables: Gemini 1.5 Pro 72.14% (κ=0.31), Gemini 1.5 Flash 71.17% (κ=0.30), Mistral Large 2 62.43% (κ=0.20). Accurate Match: Gemini 1.5 Pro 75.82% (κ=0.34), Gemini 1.5 Flash 73.40% (κ=0.32), Mistral Large 2 62.02% (κ=0.19). Better performance on explicitly stated variables (~81–85% accurate) than on derived categorical variables (~56–70% accurate).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>LLMs were less effective on derived/categorized variables; performance sensitive to prompt design and input formatting (PDF vs plain text); Mistral API could not accept PDFs directly which introduced variability; risks of hallucination and inconsistent output formatting; authors stress the need for mandatory human-in-the-loop validation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7704.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7704.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wang & Luo (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model to the Rescue of Automated Data Extraction for Educational Systematic Reviews and Meta-analyses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced preprint/report that appears to describe using LLMs to automate data extraction for educational systematic reviews and meta-analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Model to the Rescue of Automated Data Extraction for Educational Systematic Reviews and Meta-analyses</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Model to the Rescue of Automated Data Extraction for Educational Systematic Reviews and Meta-analyses</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>X. Wang, G. Luo</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7704.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7704.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cao et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting is all you need: LLMs for systematic review screening</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study examining LLM-based approaches for screening in systematic reviews (as suggested by the title).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompting is all you need: LLMs for systematic review screening</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Prompting is all you need: LLMs for systematic review screening</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>C. Cao, J. Sang, R. Arora</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7704.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7704.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sami et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced arXiv work proposing and empirically evaluating a multi-agent AI system to support systematic literature review workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>A. M. Sami, Z. Rasheed, K. K. Kemell</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7704.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7704.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Susnjak et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced arXiv paper that appears to explore domain-specific fine-tuning of LLMs to support research synthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>T. Susnjak, P. Hwang, N. H. Reyes, Alc Barczak, T. R. Mcintosh, S. Ranathunga</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7704.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7704.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scherbakov et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced arXiv work that itself is an LLM-automated systematic review, indicating use of LLMs to perform literature review automation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>D. Scherbakov, N. Hubig, V. Jansari, A. Bakumenko, L. A. Lenert</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7704.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7704.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kartchner et al. (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited ACL paper describing zero-shot information extraction methods using LLMs for clinical meta-analysis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>D. Kartchner, S. Ramalingam, I. Al-Hussaini, O. Kronick, C. Mitchell</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7704.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7704.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gartlehner et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data extraction for evidence synthesis using a large language model: A proof-of-concept study</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study in Research Synthesis Methods presenting a proof-of-concept for using an LLM to extract data for evidence synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data extraction for evidence synthesis using a large language model: A proof-of-concept study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Data extraction for evidence synthesis using a large language model: A proof-of-concept study</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>G. Gartlehner, L. Kahwati, R. Hilscher</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Model to the Rescue of Automated Data Extraction for Educational Systematic Reviews and Meta-analyses <em>(Rating: 2)</em></li>
                <li>Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning <em>(Rating: 2)</em></li>
                <li>The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review <em>(Rating: 2)</em></li>
                <li>Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models <em>(Rating: 2)</em></li>
                <li>Prompting is all you need: LLMs for systematic review screening <em>(Rating: 2)</em></li>
                <li>Data extraction for evidence synthesis using a large language model: A proof-of-concept study <em>(Rating: 2)</em></li>
                <li>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7704",
    "paper_id": "paper-275789786",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "AIDE",
            "name_full": "AI-assisted Data Extraction (AIDE)",
            "brief_description": "A free, open-source R GUI that coordinates LLM API calls to extract structured data from full-text scholarly PDFs and presents extracted items to a human reviewer for mandatory validation and recording into a coding form.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction",
            "authors": "Noah L. Schroeder, Chris Davis Jaldi, Shan Zhang",
            "year": null,
            "method_name": "AIDE (Human-in-the-loop LLM-assisted data extraction)",
            "method_description": "Sends a single API request per paper containing the structured full text (or extracted text) plus a coding-form-style set of prompts; the LLM returns values for 24 predefined variables which are auto-populated into a GUI where a human inspects, corrects if needed, and records each data point.",
            "input_type": "Full-text PDFs (sent directly to Gemini models) and extracted plain text (sent to Mistral); coding-form prompts supplied as row 1 of the uploaded CSV",
            "output_type": "Structured coding form rows (24 explicit and derived variables per study), with provenance links to source pages when available",
            "prompting_technique": "Single-request prompting: all 24 prompts sent together in one API call; intentionally generic prompts (no extensive prompt engineering) to reflect average researcher usage",
            "model_name": "Google Gemini 1.5 Flash, Google Gemini 1.5 Pro, Mistral Large 2",
            "model_size": null,
            "datasets_used": "112 studies from a published scoping review (public OSF repository referenced in paper)",
            "evaluation_metric": "Exact Match and 'Accurate Match' agreement against human coding, plus Cohen's Kappa",
            "reported_results": "Overall Exact Match across 24 variables: Gemini 1.5 Pro 72.14% (κ=0.31), Gemini 1.5 Flash 71.17% (κ=0.30), Mistral Large 2 62.43% (κ=0.20). Accurate Match: Gemini 1.5 Pro 75.82% (κ=0.34), Gemini 1.5 Flash 73.40% (κ=0.32), Mistral Large 2 62.02% (κ=0.19). Better performance on explicitly stated variables (~81–85% accurate) than on derived categorical variables (~56–70% accurate).",
            "limitations": "LLMs were less effective on derived/categorized variables; performance sensitive to prompt design and input formatting (PDF vs plain text); Mistral API could not accept PDFs directly which introduced variability; risks of hallucination and inconsistent output formatting; authors stress the need for mandatory human-in-the-loop validation.",
            "counterpoint": true,
            "uuid": "e7704.0",
            "source_info": {
                "paper_title": "Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Wang & Luo (2024)",
            "name_full": "Large Language Model to the Rescue of Automated Data Extraction for Educational Systematic Reviews and Meta-analyses",
            "brief_description": "A referenced preprint/report that appears to describe using LLMs to automate data extraction for educational systematic reviews and meta-analyses.",
            "citation_title": "Large Language Model to the Rescue of Automated Data Extraction for Educational Systematic Reviews and Meta-analyses",
            "mention_or_use": "mention",
            "paper_title": "Large Language Model to the Rescue of Automated Data Extraction for Educational Systematic Reviews and Meta-analyses",
            "authors": "X. Wang, G. Luo",
            "year": 2024,
            "method_name": null,
            "method_description": null,
            "input_type": null,
            "output_type": null,
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7704.1",
            "source_info": {
                "paper_title": "Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Cao et al. (2024)",
            "name_full": "Prompting is all you need: LLMs for systematic review screening",
            "brief_description": "A referenced study examining LLM-based approaches for screening in systematic reviews (as suggested by the title).",
            "citation_title": "Prompting is all you need: LLMs for systematic review screening",
            "mention_or_use": "mention",
            "paper_title": "Prompting is all you need: LLMs for systematic review screening",
            "authors": "C. Cao, J. Sang, R. Arora",
            "year": 2024,
            "method_name": null,
            "method_description": null,
            "input_type": null,
            "output_type": null,
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7704.2",
            "source_info": {
                "paper_title": "Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Sami et al. (2024)",
            "name_full": "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation",
            "brief_description": "A referenced arXiv work proposing and empirically evaluating a multi-agent AI system to support systematic literature review workflows.",
            "citation_title": "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation",
            "mention_or_use": "mention",
            "paper_title": "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation",
            "authors": "A. M. Sami, Z. Rasheed, K. K. Kemell",
            "year": 2024,
            "method_name": null,
            "method_description": null,
            "input_type": null,
            "output_type": null,
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7704.3",
            "source_info": {
                "paper_title": "Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Susnjak et al. (2024)",
            "name_full": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "brief_description": "A referenced arXiv paper that appears to explore domain-specific fine-tuning of LLMs to support research synthesis tasks.",
            "citation_title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "mention_or_use": "mention",
            "paper_title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "authors": "T. Susnjak, P. Hwang, N. H. Reyes, Alc Barczak, T. R. Mcintosh, S. Ranathunga",
            "year": 2024,
            "method_name": null,
            "method_description": null,
            "input_type": null,
            "output_type": null,
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7704.4",
            "source_info": {
                "paper_title": "Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Scherbakov et al. (2024)",
            "name_full": "The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review",
            "brief_description": "A referenced arXiv work that itself is an LLM-automated systematic review, indicating use of LLMs to perform literature review automation.",
            "citation_title": "The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review",
            "mention_or_use": "mention",
            "paper_title": "The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review",
            "authors": "D. Scherbakov, N. Hubig, V. Jansari, A. Bakumenko, L. A. Lenert",
            "year": 2024,
            "method_name": null,
            "method_description": null,
            "input_type": null,
            "output_type": null,
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7704.5",
            "source_info": {
                "paper_title": "Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Kartchner et al. (2023)",
            "name_full": "Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models",
            "brief_description": "A cited ACL paper describing zero-shot information extraction methods using LLMs for clinical meta-analysis tasks.",
            "citation_title": "Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models",
            "mention_or_use": "mention",
            "paper_title": "Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models",
            "authors": "D. Kartchner, S. Ramalingam, I. Al-Hussaini, O. Kronick, C. Mitchell",
            "year": 2023,
            "method_name": null,
            "method_description": null,
            "input_type": null,
            "output_type": null,
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7704.6",
            "source_info": {
                "paper_title": "Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Gartlehner et al. (2024)",
            "name_full": "Data extraction for evidence synthesis using a large language model: A proof-of-concept study",
            "brief_description": "A cited study in Research Synthesis Methods presenting a proof-of-concept for using an LLM to extract data for evidence synthesis.",
            "citation_title": "Data extraction for evidence synthesis using a large language model: A proof-of-concept study",
            "mention_or_use": "mention",
            "paper_title": "Data extraction for evidence synthesis using a large language model: A proof-of-concept study",
            "authors": "G. Gartlehner, L. Kahwati, R. Hilscher",
            "year": 2024,
            "method_name": null,
            "method_description": null,
            "input_type": null,
            "output_type": null,
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7704.7",
            "source_info": {
                "paper_title": "Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Model to the Rescue of Automated Data Extraction for Educational Systematic Reviews and Meta-analyses",
            "rating": 2,
            "sanitized_title": "large_language_model_to_the_rescue_of_automated_data_extraction_for_educational_systematic_reviews_and_metaanalyses"
        },
        {
            "paper_title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "rating": 2,
            "sanitized_title": "automating_research_synthesis_with_domainspecific_large_language_model_finetuning"
        },
        {
            "paper_title": "The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review",
            "rating": 2,
            "sanitized_title": "the_emergence_of_large_language_models_llm_as_a_tool_in_literature_reviews_an_llm_automated_systematic_review"
        },
        {
            "paper_title": "Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models",
            "rating": 2,
            "sanitized_title": "zeroshot_information_extraction_for_clinical_metaanalysis_using_large_language_models"
        },
        {
            "paper_title": "Prompting is all you need: LLMs for systematic review screening",
            "rating": 2,
            "sanitized_title": "prompting_is_all_you_need_llms_for_systematic_review_screening"
        },
        {
            "paper_title": "Data extraction for evidence synthesis using a large language model: A proof-of-concept study",
            "rating": 2,
            "sanitized_title": "data_extraction_for_evidence_synthesis_using_a_large_language_model_a_proofofconcept_study"
        },
        {
            "paper_title": "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation",
            "rating": 2,
            "sanitized_title": "system_for_systematic_literature_review_using_multiple_ai_agents_concept_and_an_empirical_evaluation"
        }
    ],
    "cost": 0.0144135,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction</p>
<p>Noah L Schroeder schroedern@ufl.edu 
Department of Computer &amp; Information Science &amp; Engineering
University of Florida
GainesvilleFlorida</p>
<p>Chris Davis Jaldi 
Department of Computer Science &amp; Engineering
Wright State University
DaytonOhio</p>
<p>Shan Zhang 
School of Teaching and Learning
University of Florida
GainesvilleFlorida</p>
<p>Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction
EE27E59428C2585A1731EDA9A1284EA8
Systematic reviews are time-consuming endeavors.Historically speaking, knowledgeable humans have had to screen and extract data from studies before it can be analyzed.However, large language models (LLMs) hold promise to greatly accelerate this process.After a pilot study which showed great promise, we investigated the use of freely available LLMs for extracting data for systematic reviews.Using three different LLMs, we extracted 24 types of data, 9 explicitly stated variables and 15 derived categorical variables, from 112 studies that were included in a published scoping review.Overall we found that Gemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably well, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with human coding, respectively.While promising, these results highlight the dire need for a human-in-the-loop (HIL) process for AI-assisted data extraction.As a result, we present a free, open-source program we developed (AIDE) to facilitate user-friendly, HIL data extraction with LLMs.</p>
<p>Introduction</p>
<p>Large language models (LLMs) are revolutionizing the way we think about work.They are transforming the way we synthesize large bodies of knowledge, create reports, or get feedback on our writing.For example, in educational contexts, researchers have begun investigating if LLMs can be used to generate learning content 1 or if they can be used as temporary teachers 2 .Overall, LLMs are rapidly evolving, improving at different tasks, and technically expanding in ways we need not dive into in detail here.</p>
<p>As research synthesists, we recognize that ignoring the advancements in LLMs could be to our own detriment.After all, systematic review methods can be intensely time-consuming, requiring human effort and expertise throughout the entire process 3,4 .It is then critical that we examine for what, and to what extent, we can leverage LLMs in research synthesis workflows.</p>
<p>Our team is not alone in exploring how LLMs can be used in research synthesis.LLMs have been used as second screeners during abstract screening [5][6][7] , for data extraction [8][9][10] , and throughout the systematic review process more generally 11,12 .Meanwhile, other lines of work have focused on technical aspects such as how to fine-tune LLMs for research synthesis tasks (Silva et al., 2024, Susnjak et al.,  2024).</p>
<p>In this paper, we focus on the data extraction step of the systematic review process.Reviewing relevant research in the area shows very promising, although arguably preliminary results.For example, researchers have found promising results for extracting data in medical contexts 8,9,14 , with some studies showing over 90% accuracy of the data extracted 8 .While this sounds promising, there is reason to believe that LLMs are not at a point where we can blindly trust their data extraction, especially considering the risk of hallucination 15 .Much of the existing work on extracting data with LLMs has used only a small number of PDFs 8,9 , extracted data for relatively few variables 8,14 , and explored a similar group of LLMs (generally, ChatGPT or Claude).Notably, LLMs have been rapidly advancing with new models introduced frequently, and there are also many other leading LLMs that may perform differently for data extraction tasks.Moreover, some studies have found the data returned by the LLM was noisy 14 and it is well-known that LLMs can be sensitive to the prompt 16,17 and other LLM settings used, such as the temperature 18 .Together, these factors indicate that more in-depth research is needed to explore the efficacy of LLMs for systematic review data extraction.</p>
<p>The Present Study</p>
<p>Given the time consuming nature of systematic reviews and the limited, yet promising data around using LLMs for data extraction, we sought to examine the efficacy of data extraction using LLMs.We separate this article into three parts.</p>
<p>In Part I, we describe a pilot study exploring the following research questions with a small sample (n = 8) of studies across three LLMs (Claude, GPT 4, and ChatPDF):</p>
<p>RQ1: How accurately can an LLM extract explicitly stated data?RQ2: How accurately can an LLM extract and categorize data based on predefined categories?RQ3: What is the overall accuracy of an LLM's data extraction compared to a human coder?</p>
<p>In Part II, we describe a more formalized, in-depth exploration of using freely available LLMs accessible via API (Gemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2) for data extraction tasks with a substantially larger sample of studies (n = 112).We address the same three research questions as the pilot study.</p>
<p>In Part III, we describe a free, open-source, user-friendly graphic-user interface program we developed to streamline the data extraction process with LLMs.The software focuses on including a human-in-the-loop (HIL) where the LLM and human work together to extract and confirm each data point extracted.</p>
<p>Part I -Pilot Study Overview and Research Questions</p>
<p>Our initial exploration of using LLMs for data extraction took place in the Summer of 2023.At that point, LLMs were still a relatively new technology and were just beginning to become more widely adopted.Our initial exploration focused on the research questions stated in the present study section.</p>
<p>Methods</p>
<p>LLMs Used.We used what were three leading LLMs at the time: Claude (https://claude.ai/),ChatPDF (https://www.chatpdf.com/),and GPT-4 (https://openai.com/).All were used via their web-based chat interface.</p>
<p>Data Set.We extracted data from four journal articles and four conference proceedings related to the use of virtual characters in K-12 education.</p>
<p>Variables Extracted.We asked each LLM to extract 24 variables, including explicit data (e.g., duration of study, learning topic, etc.) and derived data based on our own categorization scheme (e.g., agent role, agent type, study media based on our categorization scheme, etc.).We chose to include both types of data because both are typically extracted during systematic reviews and meta-analyses.In total, we had nine explicit variables and 15 derived variables extracted from each study.Table 1 shows the prompt used to extract each variable.Notably, we did not try to perfect each prompt.Rather, our goal was to write prompts in a way that an average researcher approaching an LLM may write them.</p>
<p>Data Analysis.We calculated simple inter-rater agreement between the human extracted data for each study and the LLM-extracted data for each study.</p>
<p>Table 1.The prompt we used for all three LLMs to extract 24 data points from each study.</p>
<p>Prompt Used in LLMs</p>
<p>Analyze the article for scientific review and answer the following questions with answers less than a line while mentioning the page number of the pdf in which you found the answer to a particular question if it's mentioned in the study, otherwise, the result should be Not Reported.</p>
<ol>
<li>What age group can the agent be classified into?(Classify into any one of Kid, Adult, Irrelevant-non-human agent, Multiple agents &amp;&amp;/|| multiple ages, or Unknown)</li>
</ol>
<p>23.What role can the agent be classified into?(Classify into any one of Demonstrating/Modeling, Coaching/Scaffolding, Information Source, Testing, Multiple roles, or Other) 24.What defined outcomes are measured as results in the study?(Learning -<for example, retention, transfer, achievement, performance, errors>, Motivation-<for example motivation, goals, self-efficacy, interest>, Perception-<for example, feelings, emotions, satisfaction, ratings of the system>, Cognitive Load/Mental Effort -<for example mental effort, cognitive load, intrinsic load, extraneous load, germane load>)</p>
<p>Results</p>
<p>RQ1: How accurately can an LLM extract explicitly stated data?</p>
<p>We asked each LLM to extract nine explicit variables from each of the eight data sources.Compared to the human coder, we found that Claude was the most accurate LLM, showing 93.06% agreement with the human coder.Meanwhile, GPT-4 was also quite accurate (91.67% agreement), while ChatPDF trailed a distant third, while still being reasonably accurate (75% agreement).</p>
<p>RQ2: How accurately can an LLM extract and categorize data based on predefined categories?</p>
<p>We also asked each LLM to extract 15 derived variables from each of the eight data sources.We found that Claude was again the most accurate, with 80.00% agreement with the human coder.Meanwhile, GPT-4 had 64.17% agreement with the human coder, and ChatPDF had 60.00% agreement with the human coder.</p>
<p>RQ3: What is the overall accuracy of an LLM's data extraction compared to a human coder?</p>
<p>Finally, we examined the overall accuracy of the LLM's data extraction compared to that of a human coder.Again, Claude led the LLMs with 84.90% agreement with the human coder.GPT-4 placed second, with 74.48% agreement, and ChatPDF came in third with 65.63% agreement.</p>
<p>Discussion</p>
<p>We found that the LLMs' ability to extract data was greatly dependent on the type of data being extracted and the prompt used.We had intentionally designed our prompts "generically", having not conducted a thorough prompt engineering exercise to perfect them.This was an intentional choice, meant to mimic how an "average" researcher may interact with a LLM.While the accuracy of our data extraction was not as high as that seen in other studies 8 , we still found extremely promising results from leading LLMs.However, we found using the web-based chat interface tedious if many studies were going to be screened and thus started exploring the use of APIs.</p>
<p>Part II -Main Study</p>
<p>Overview and Research Questions</p>
<p>Having seen promising results from our pilot study in 2023, in late 2024 we re-initiated our investigation with a more thorough main study.Moreover, we decided to focus solely on LLMs that provide free API access, as API requests are much more efficient ways of extracting large amounts of data than using a web-based chat interface on the respective LLM's website and then transcribing it to a .csvfile.We investigated the same research questions as in the pilot study.</p>
<p>Methods</p>
<p>LLMs Used.The LLMs used in the main study were Google's Gemini 1.5 Flash (https://aistudio.google.com/),Gemini 1.5 Pro (https://aistudio.google.com/),and Mistral Large 2 (https://mistral.ai/).Notably, all three of these LLMs are available via API free of charge (rate limits may apply).</p>
<p>Data Extraction.For this main study, we used an LLM to help generate Python code for using the LLM APIs to extract the data from the studies in our sample.The Python files are available on OSF: https://osf.io/nraxf/?view_only=879654690be249cc9237525ed7f86ec2.As shown in the Python scripts, we sent one request to the LLM containing all of the prompts.</p>
<p>We sent the full PDF file to the Gemini models.However, at the time of our study, the Mistral Large 2 API could not process PDFs directly.As such, we used PyPDF2 to extract the text data from the PDF and then sent the text data to Mistral.As with the Gemini API requests, all prompts were sent in one request for each study in our sample.</p>
<p>After receiving the response from the API, the Python script then assembled the responses into a coding form, with one row for each study evaluated by the LLM.</p>
<p>Data Set.The dataset consisted of 112 studies included in a published scoping review 19 .</p>
<p>Variables Extracted.The full coding forms with the data extracted from each study are publicly available in an OSF repository: https://osf.io/nraxf/?view_only=879654690be249cc9237525ed7f86ec2.</p>
<p>Data Analysis.The data analysis used the same methods as in the pilot study.In addition, we also classified responses as an "Accurate Match", but not identical to human coding, and reported that data separately.We report the simple agreement statistic as well as Cohen's Kappa.</p>
<p>Results</p>
<p>Preface</p>
<p>We found that the Gemini 1.5 Pro and Gemini 1.5 Flash models were able to analyze all 112 studies in our sample.Moreover, the results were presented relatively consistently (see raw data extraction in the OSF repository).However, this was not the case for Mistral Large 2. First, we found that Mistral only returned results for 103 of the studies, and of those, 3 were incomplete, and one had somewhat irrelevant results.Consequently, our results for Gemini 1.5 models are for 112 studies.For Mistral Large 2, we counted the studies that were not returned or analyzed as not matching, meaning we still calculated the results as if we had received results for the full sample.As a consequence, the Mistral Large 2 scores are lower because there were a number of studies not reported, meaning for each study not reported, there are 24 variables not reported, or in this context, not matching human coding.</p>
<p>In addition, the response from the Mistral API was not as consistent as the Gemini 1.5 models (as shown in the raw data forms on OSF).We had to manually clean the data before analysis.We recognize that this likely could have been resolved through prompt changes for the Mistral model, however we felt it important to keep the same prompt for all three LLMs.</p>
<p>RQ1: How accurately can an LLM extract explicitly stated data?</p>
<p>Similar to the pilot study, we asked each LLM to extract nine explicit variables from all 112 data sources.We further divided the comparison metrics into two categories: Exact Match and Accurate Match.Compared to the human coder in the Exact Match analysis for explicitly stated data, Google Gemini 1.5 Pro was the most accurate LLM (83.33% agreement, κ = 0.40).Google Gemini 1.5 Flash was also quite accurate (81.75% agreement, κ = 0.39), while Mistral Large 2 followed in third place but remained reasonably accurate (71.73% agreement, κ = 0.30).</p>
<p>Next, we looked at the Accurate Match metrics.Google Gemini 1.5 Pro again demonstrated the highest accuracy (85.12% agreement, κ = 0.41), followed by Google Gemini 1.5 Flash (82.84% agreement, κ = 0.40).Mistral Large 2 was third, though still reasonably accurate (74.40% agreement, κ = 0.33).</p>
<p>RQ2: How accurately can an LLM extract and categorize data based on predefined categories?</p>
<p>We also asked each LLM to extract the remaining (n = 15) derived variables from all 112 data sources and analyzed the responses using the same two accuracy metrics.For Exact Match, Google Gemini 1.5 Pro was again the most accurate, with 65.42% agreement (κ = 0.24), followed by Google Gemini 1.5 Flash with 64.82% agreement (κ = 0.23) and Mistral Large 2 (56.85% agreement, κ = 0.12).</p>
<p>For the Accurate Match analysis, we found that Google Gemini 1.5 Pro again led the LLMs (70.24% agreement, κ = 0.29), followed by Google Gemini 1.5 Flash (67.74% agreement, κ = 0.26) and Mistral Large 2 (62.02% agreement, κ = 0.19).</p>
<p>RQ3: What is the overall accuracy of an LLM's data extraction compared to a human coder?</p>
<p>Finally, we examined the overall accuracy of each LLM's data extraction.For the Exact Match analysis, Google Gemini 1.5 Pro led the group (72.14% agreement, κ = 0.31), Google Gemini 1.5 Flash placed second (71.17% agreement, κ = 0.30), and Mistral Large 2 was third (62.43% agreement, κ = 0.20).</p>
<p>When examining the Accurate Match analysis, Google Gemini 1.5 Pro again led the LLMs (75.82% agreement, κ = 0.34), followed by Google Gemini 1.5 Flash (73.40% agreement, κ = 0.32) and Mistral Large 2 (62.02% agreement, κ = 0.19).</p>
<p>Discussion</p>
<p>As shown above, the LLMs tested here were quite accurate when extracting explicitly stated data and variables from studies.However, they were substantially less effective at categorizing data based on predefined categories.Importantly, we found that all LLMs tested in our main study performed similarly.Our purpose is not to say one model is more effective than others, because we believe the LLMs are too context-dependant (e.g., individual PDF or study, type(s) of data being extracted, prompt used, temperature of LLM, etc.) to be able to make such a claim.Rather, our purpose was to explore if LLMs can be accurate enough to play a role in the research synthesis process.</p>
<p>Based on prior studies in the area 8 and our pilot study, readers may be surprised by the lower accuracy we found when the free-to-use LLM APIs were used at the scale of a full scoping review rather than a small subset of studies.There are a number of reasons why these results may have occurred and it is reasonable to assume that the LLMs tested here have the capability to be more effective than seen here.We have no doubt this is the case given our experiences working with LLMs.For example, it is widely known that LLMs can be highly sensitive to the prompt used and other settings of the LLM [16][17][18] .As noted, we made the intentional decision not to spend much time optimizing our prompt.Rather, we focused on coming up with a prompt we felt was consistent with one a researcher may use if they lacked in-depth knowledge of LLMs, as we felt this was a more realistic use-case than if we had spent time optimizing each aspect of the prompt and LLM settings.It is highly plausible that if one took enough time fine-tuning each individual question to include in the prompt a higher accuracy could be achieved.However, this could be highly time-consuming, which would defeat the purpose of using a LLM to aid in data extraction.</p>
<p>In addition, it is important to highlight differences in how the LLMs were used and the capabilities of each.For example, with Gemini models we were able to send the PDF files directly to the model.As such, we do not know what types of processing may have occurred.Meanwhile, Mistral only allowed text-only inputs.This means we had to extract text to send to be processed by the LLM.This introduces further inconsistency because there are a variety of ways one could extract text.For example, you can lose formatting, you can extract structured text, you can convert text to markdown, etc.We chose a relatively simple text format, however this could, in part, explain why Mistral did not perform as well as the Gemini models in our testing here.</p>
<p>In summary, there are many factors that can influence the accuracy of data extraction by LLMs in a research synthesis context.Even if we had found over 90% accuracy, we would still not advise researchers to adopt LLM outputs without human oversight.For example, just because some data was extracted accurately with one specific prompt does not mean that accuracy will translate to different contexts.As such, we feel that a human-in-the-loop solution is needed.Consequently, we envisioned and built (see Part III) a platform that allows research synthesists to use LLMs to extract data, but a human can double-check the LLMs extracted data to ensure it is accurate.We posit this will save time compared to only human-based data extraction.</p>
<p>Part III -Introducing a Software Platform for LLM-Assisted Data Extraction</p>
<p>AI-Assisted Data Extraction: A Graphic User Interface for LLM-based Data Extraction</p>
<p>A key takeaway from our studies is that LLMs are not a point where we can blindly trust them to extract data for systematic reviews or meta-analyses across various contexts.Rather, it is essential to have a HIL approach where an experienced and knowledgeable researcher can review, approve, or change the extracted data as needed.However, doing so when extracting data via API calls or chat interfaces common on many LLM websites is not time-efficient, as both would require the researcher to read the entire paper to find each data point.Accordingly, we have developed a free, open-source, user-friendly software to help facilitate this process.</p>
<p>Overview of AIDE.We present a free, open-source, graphic user interface package for R called AI-assisted Data Extraction (AIDE).AIDE allows you to use LLMs for extracting data with mandatory human oversight.At the present time, AIDE is designed to allow you to use free API keys from Google AI Studio, Local models via Ollama, Mistral AI, or Open Router to access LLMs.AIDE was designed from the ground up to be free software, so only free APIs have been integrated at this point1 .Notably, there are free models that perform well for data extraction tasks (as shown in our results above) and at this point at least, researchers need not pay for LLM models unless they choose to do so.</p>
<p>As shown in Figure 1, the user interface for setting up your LLM is relatively straightforward; a user selects their LLM provider, enters their API key, chooses their model, and uploads their coding form.The first row of the coding form will be read as prompts to the LLM and should be written as such.A critically important feature of AIDE is the analysis page.The user uploads a PDF, which they can see in a PDF viewer.They are automatically presented with an estimate of the number of tokens the API request will use, which allows one to make an informed decision as to if the LLM they plan to use is appropriate.Moreover, this allows for one to estimate the cost of their API request if they choose to use a paid LLM.If using local models via Ollama, the user can also select the context window size they want to use (Figure 2).The right side of the analysis page is auto-populated with the data from the coding form.When "Analyze PDF" is pressed, a single request is made to the LLM API that sends the extracted structured full text, a brief prompt, and the prompts from the coding form.When a response is received, the data is parsed and auto-populated into the corresponding boxes in the user interface.If the Source button is pressed, the LLM's reasoning for the answer is shown when available (see Figure 3) and the PDF reader scrolls to the appropriate page of the PDF automatically.If the answer is correct, pressing the Record button records it to the appropriate location on the coding form on your local computer.If the answer needs to be modified, the user can edit the text before clicking Record.When the user is ready to move to the next paper for analysis, they simply upload a new file, and AIDE will clear the responses in the interface and move to the next row in the coding form file on the local computer.After Analyze PDF is pressed, a request is sent to the API, and the process is identical to the one previously described.</p>
<p>Conclusion</p>
<p>LLMs offer promise for conserving resources during the data extraction process in research synthesis.Our pilot study and large-scale main study showed promise for LLMs being able to accurately extract various types of data, significantly reducing labor time and enhancing the efficiency of qualitative research.However, it became explicitly clear that they cannot be relied on alone.Rather, a HIL process 20 is needed to ensure accuracy while still conserving time.As a result, we developed AIDE, a free, open-source graphic user interface program for R that helps research synthesists use LLMs to extract and record data in a time-efficient manner.Research is needed to quantify the resources saved by using software like AIDE.</p>
<p>Figure 1 .
1
Figure 1.A screenshot of the AIDE setup page.</p>
<p>Figure 2 .
2
Figure 2. Context size analysis of the PDF that was uploaded into AIDE.</p>
<p>Figure 3 .
3
Figure 3.The analysis panel of AIDE.Note the source panel showing where information was found.</p>
<p>Script 1 :
1
The scripts to install, load, and run AIDE.#Installpackage from Github Repo devtools::install_github("noah-schroeder/AIDE", dependencies = TRUE)</p>
<p>Open Router does allow one to use paid services, such as Claude or ChatGPT, if they choose to.</p>
<p>Who is the author? (in the format of <Author Last Name> (<Year of publication>) for single author, <Author 1 Last Name> &amp; <Author 2 Last Name> (<Year of publication>) for two authors, <Author 1 Last Name> et al. (<Year of publication>) for multiple authors) 2. What is the Year of publication? 3. What is the publication type?. Classify into Journal article, Conference proceeding. Dissertationor Other accordingly) 4. What is the publication name</p>
<p>In which country was the data collected/study conducted? Do not assume the author's country is where the data were collected. ( Country name if the area is specified else Not Reported) 6. What are the grades of the participants (US grade equivalents</p>
<p>Are there any special populations? ( ASD, disability, or Disorders) 10. what biological genders are involved in the study? ( only Male, only Female, Mixed or Not Reported) 11. What is the intervention duration, meaning how long did the participant interact with the system? 12. What is the Learning Topic in the Experiment for the participants? 13. How did the participants interact with the system?. What is the average age of the participants? 8. How many participants are in the study? 9. If not stated. record as Not Reported</p>
<p>Quantitative, Mixed, Not Reported? 17. What is the study Purpose type? ( Classify into any one of Feasibility/Usability/Perception if there's only 1 group involved in the experiment, Experiment if there are 2 or more groups involved in the experiment, Study protocol if the experiment is a study plan, and Not reported if it is not specified ) 18. What is the type of agent? (Classify into any one of Pedagogical, Conversational, Motivational, Teachable, Multiple roles or Not reported) 19. How many on-screen agents are there in the system? 20. Is the agent's physical appearance human-like? (Classify into Human-inspired if the physical appearance is human-like, Non-human if not, Mixed-multiple agents if both types were used, or Not reported) 21. What does the agent/agent's biological gender look like?. What is the total Duration of the study? 16. What is the study measures type i.e. Qualitative. Classify into any one of Female, Male, Gender neutral, Multiple agents, non-human, or Not Reported) References</p>
<p>Generative AI for Learning: Investigating the Potential of Learning Videos with Synthetic Virtual Instructors. D Leiker, A R Gyllen, I Eldesouky, M ; Cukurova, N Wang, G Rebolledo-Mendez, V Dimitrova, N Matsuda, O C Santos, 10.1007/978-3-031-36336-8_81Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners, Doctoral Consortium and Blue Sky. Nature SwitzerlandSpringer2023</p>
<p>Exploring the potential of ChatGPT as a Substitute teacher: A Case Study. Al Ghazali, S Zaki, N Ali, L Harous, S , Int J Inf Educ Technol. 1422024</p>
<p>Using machine learning for continuous updating of meta-analysis in educational context. O Chernikova, M Stadler, I Melev, F Fischer, 10.1016/j.chb.2024.108215Comput Hum Behav. 1561082152024</p>
<p>Large Language Model to the Rescue of Automated Data Extraction for Educational Systematic Reviews and Meta-analyses. X Wang, G Luo, Metamate, 10.35542/osf.io/wn3cdMay 2, 2024Published online</p>
<p>Prompting is all you need: LLMs for systematic review screening. C Cao, J Sang, R Arora, 10.1101/2024.06.01.24308323Published online June 3, 2024:2024.06.01.24308323</p>
<p>The Promise and Challenges of Using LLMs to Accelerate the Screening Process of Systematic Reviews. A Huotala, M Kuutila, P Ralph, M Mäntylä, 10.48550/arXiv.2404.1566720248Published online May</p>
<p>Methodological insights into ChatGPT's screening performance in systematic reviews. M Issaiy, H Ghanaati, S Kolahi, 10.1186/s12874-024-02203-8BMC Med Res Methodol. 241782024</p>
<p>Data extraction for evidence synthesis using a large language model: A proof-of-concept study. G Gartlehner, L Kahwati, R Hilscher, 10.1002/jrsm.1710Res Synth Methods. 1542024</p>
<p>Performance of two large language models for data extraction in evidence synthesis. A Konet, I Thomas, G Gartlehner, 10.1002/jrsm.1732Res Synth Methods. 1552024</p>
<p>Exploring the use of a Large Language Model for data extraction in systematic reviews: a rapid feasibility study. L Schmidt, K Hair, S Graziozi, 10.48550/arXiv.2405.14445202423Published online May</p>
<p>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation. A M Sami, Z Rasheed, K K Kemell, 10.48550/arXiv.2403.08399202413Published online March</p>
<p>The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review. D Scherbakov, N Hubig, V Jansari, A Bakumenko, L A Lenert, 10.48550/arXiv.2409.04600September 6, 2024Published online</p>
<p>Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning. T Susnjak, P Hwang, N H Reyes, Alc Barczak, T R Mcintosh, S Ranathunga, 10.48550/arXiv.2404.08680April 8, 2024Published online</p>
<p>Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models. D Kartchner, S Ramalingam, I Al-Hussaini, O Kronick, C Mitchell, 2023Association for Computational Linguistics</p>
<p>A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. Smti Tonmoy, Smm Zaman, V Jain, 10.48550/arXiv.2401.01313January 8, 2024Published online</p>
<p>How Susceptible are LLMs to Influence in Prompts?. S Anagnostidis, J Bulian, 10.48550/arXiv.2408.11865August 172024Published online</p>
<p>What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering. F Errica, G Siracusano, D Sanvito, R Bifulco, 10.48550/arXiv.2406.12334September 5, 2024Published online</p>
<p>Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters. M Loya, D A Sinha, R Futrell, 10.18653/v1/2023.findings-emnlp.241Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Pedagogical agents in K-12 education: a scoping review. S Zhang, C D Jaldi, N L Schroeder, J R Gladstone, 10.1080/15391523.2024.2381229J Res Technol Educ. Published online. July 30, 2024. December 19, 2024</p>
<p>Human-in-the-loop Artificial Intelligence. F M Zanzotto, Viewpoint, 10.1613/jair.1.11345J Artif Intell Res. 642019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2645 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2645</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2645</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-a6aed0c4e0f39a55edb407f492e41f178a62907f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a6aed0c4e0f39a55edb407f492e41f178a62907f" target="_blank">PaperRobot: Incremental Draft Generation of Scientific Ideas</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A PaperRobot who performs as an automatic research assistant by conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs is presented.</p>
                <p><strong>Paper Abstract:</strong> We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30%, 24% and 12% of the time, respectively.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2645.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2645.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperRobot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperRobot: Incremental Draft Generation of Scientific Ideas</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end research assistant that (1) extracts background knowledge graphs (KGs) from large collections of papers, (2) generates new scientific ideas by predicting links (new edges) in those KGs, and (3) incrementally drafts parts of papers (title → abstract → conclusion/future work → follow-on title) using memory-attention generation networks grounded in extracted entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperRobot</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PaperRobot is a multi-component system combining knowledge extraction, knowledge-graph (KG) link prediction, and knowledge-grounded neural generation. Components: (a) Background Knowledge Extraction — entity mention extraction/linking (Wei et al., 2013) to MeSH/CTD, building a KG of entities (Disease, Chemical, Gene) and relations; (b) Link Prediction — rich entity representations combining multi-head graph attention over one-hop neighbors and a contextual text encoder (bi-LSTM) over sentences containing the entity, a gated combination of graph/text representations, trained with a TransE-style translation objective and marginal loss with negative sampling to score candidate triplets; (c) New-idea / Hypothesis Generation — predict new edges (e_i, r, e_j) with an associated confidence score y and rank/select top related entities; (d) Paper Writing — memory-attention networks (multihop memory attention) plus a pointer-generator copy mechanism (hybrid Mem2seq + pointer-generator) to generate abstracts, conclusions/future work and follow-on titles conditioned on an input title and the predicted related entities; (e) Training & Safety components — coverage loss and a simple beam-search masking repetition-removal method at test time to reduce repetition; human Turing tests and human post-editing used for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid: knowledge-graph-based + knowledge-grounded neural generation (memory-attention + pointer-generator)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical / life sciences (applied to PMC Open Access biomedical papers); also experimented on NLP-domain corpus</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates hypotheses as link-prediction outputs on a constructed biomedical KG: compute entity embeddings combining multi-head graph-attention (attention over one-hop neighbors) and contextual text embeddings (bi-LSTM with bilinear attention); train embeddings with a TransE translation objective (minimize F(h,r,t)=||e_h + r - e_t||_2^2) using marginal ranking loss and negative sampling; for each indirectly connected pair and relation type, compute score y (from trained embeddings / distance) indicating probability the triple holds; rank by y and select top related entities (up to 10) as candidate hypotheses/ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Uses the learned link-confidence score y (derived from embedding distance F and subsequent scoring/ranking) to indicate plausibility; also uses attention-based softmax probabilities and gating when incorporating entities into generated text to favor higher-confidence items.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Confidence score y for predicted triples (derived from embedding distance F); distance score F(e_h,r,e_t)=||e_h + r - e_t||_2^2 used during training; ranking by y to select top-k related entities. No explicit novelty/uniqueness metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Computational and human evaluation: (1) automatic evaluation of generated text (perplexity measured on an external language model, METEOR for topical relevance), (2) human Turing tests where experts/non-experts compare system vs human text (reported percent selections), (3) human post-editing (domain expert edits system abstracts) and plagiarism checks, (4) ablation studies to assess component contributions. Predicted KG links themselves are NOT experimentally (wet-lab) validated and are noted to require human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Code and data resources stated as publicly available on GitHub (link in paper). Model architectures, training objectives (TransE-style loss, marginal loss, coverage loss), and dataset sizes (training/dev/test counts) are reported; hyperparameters noted to be in Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Grounding generation in: (a) KG-predicted entities (copying mechanism / memory attention) to reduce free hallucination, (b) coverage losses for both reference attention and memory distribution to avoid repeated attention (and thus repeated unsupported claims), and (c) a beam-search masking method at test time that forbids re-generating non-stop words already produced in the same output.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Post-generation human expert review (post-editing) and Turing tests used to surface incorrect or implausible outputs; ablation analysis and manual inspection identify incorrect abbreviations, numbers and pronouns. No automated hallucination-detector (e.g., fact-checker) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Uses model-derived scalar confidence/probability scores: link prediction confidence score y (from embedding distance); softmax attention weights and generator probabilities P_gen, P_tau (copy-from-title), P_e (copy-from-entity); gates (g_p, σ-based) produce probabilistic mixing ratios, which serve as measures of model uncertainty about source of tokens. No Bayesian or explicit calibrated uncertainty framework reported.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>PMC Open Access Subset (1,687,060 papers used to construct KGs). Training/dev/test splits for writing tasks: Training ~22,811 papers (titles→abstract etc.); Development 2,095; Test 2,095. External language model trained on 500k titles, 50k abstracts, 50k conclusions/future-work from PubMed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper-writing: Title→Abstract perplexity 11.5 and METEOR 13.0 (our full approach). Comparisons: Seq2seq PPL 19.6 / METEOR 9.1; Editing Network PPL 18.8 / METEOR 9.2; Pointer Network PPL 146.7 / METEOR 8.5. Abstract→Conclusion PPL 18.3 / METEOR 11.2 (our approach). Conclusion→Title PPL 14.8 / METEOR 8.9 (our approach). Human evaluation (Turing tests): system-generated abstracts selected over human-authored ones by domain expert up to 30% of the time; conclusions/future-work up to 24%; new titles up to 12% (Table 4). Human post-editing metrics: BLEU1 59.6, BLEU4 55.4, ROUGE 73.3, TER 35.2 for 50 abstracts after expert edits (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>The paper compares its generation module to Seq2seq, an Editing Network, and a Pointer Network: their full approach outperforms baselines in perplexity and METEOR (see numbers above). Human judges sometimes prefer PaperRobot outputs (up to 30% for abstracts by expert). Link prediction module is qualitatively validated (examples and ablations) but no numeric link-prediction baseline scores are presented in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors note key limitations: reliance on language model and extracted facts (limited reasoning), occasional generation of incorrect abbreviations, numbers, pronouns and rare-context entities; predicted new links require human verification (no wet-lab validation); lower performance in smaller domains (NLP corpus) due to fewer examples and coarse-grained entity/relation types; no explicit novelty-detection or formal plausibility-certification beyond confidence scores and human review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaperRobot: Incremental Draft Generation of Scientific Ideas', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2645.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2645.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG Link Predictor (TransE + Graph/Text Encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Link Prediction module combining multi-head graph attention, textual context encoder, gated combination and TransE objective</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A KG link-prediction method that forms entity embeddings by fusing multi-head graph-attention over one-hop neighbors with contextual textual encodings (bi-LSTM + bilinear attention), combined via an element-wise gate, and trained with a TransE translation loss and marginal ranking loss to score candidate triples and produce confidence y for hypothesized relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KG Link Predictor (GraphAttention + Context + TransE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Entity representation: graph-structure encoder uses self-attention / multi-head graph attention (inspired by GAT) over one-hop neighbors to produce structure-based context representations; contextual text encoder uses a bi-LSTM over a context sentence with bilinear attention to produce text-based context representation; a learned sigmoid gate combines graph/text vectors into a final entity embedding. Training: TransE-style translational assumption (e_h + r ≈ e_t), distance F = ||e_h + r - e_t||_2^2 and marginal ranking loss with negative samples (replace head or tail randomly). Prediction: compute score y for each candidate (e_i, r, e_j) from embeddings and rank/threshold to enrich KG with predicted edges (top-k selected).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>knowledge graph-based (graph attention + text-enhanced embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical (entities: Disease, Chemical, Gene) constructed from PMC; general KG completion methodology</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates candidate hypotheses by scoring and ranking candidate triples (e_i, r, e_j) using learned embeddings and translational distance; propagation of neighbors from semantically similar entities (if Calcium and Zinc similar, propagate neighbors), with top-ranked predictions forming new edges/hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Uses the embedding-derived score y to indicate likelihood/plausibility and ranks predictions by y; no external novelty/plausibility filter beyond ranking and human review.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Distance score F(e_h,r,e_t)=||e_h + r - e_t||_2^2 used in training; confidence score y computed for predictions and used for ranking and selection. No additional quantitative hypothesis-quality metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Qualitative examples and ablation studies demonstrating that link prediction introduces topically related entities (Table 7). Authors explicitly state predicted links need to be verified by humans; no wet-lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Training objective, negative sampling strategy, and dataset size for KG construction are reported; code/data reportedly available on GitHub.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Grounding predictions within the constructed KG and combining graph-structure and contextual text reduces arbitrary generation; gating reduces over-emphasis on noisy source. However, no formal hallucination-proofing mechanism for link prediction is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Manual inspection and qualitative analysis in ablations detected implausible outputs; predicted links are flagged as needing human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Confidence score y for each predicted triple (probabilistic ranking); softmax attention distributions over neighbors and text tokens provide additional internal probability weights.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>KG constructed from PMC Open Access Subset: 1,687,060 papers, 30,483 entities, 875,698 relations. Training/dev/test splits provided for downstream generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric link-prediction performance metrics (e.g., Hits@k, MRR) are reported in the paper; evaluation is qualitative (examples) and downstream effect measured via writing metrics and ablation impact.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Authors situate approach relative to translation-based KG embeddings (TransE) and prior text-enhanced KG embeddings, and claim novelty by incorporating multi-head graph attention; no direct numeric baseline comparison for link-prediction metrics is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No explicit calibration or formal novelty detection; predictions require human verification; no reported numeric link-prediction benchmarks (Hits@k/MRR) to quantify accuracy; rare entities with limited contexts produce unreliable contextual embeddings leading to incorrect predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaperRobot: Incremental Draft Generation of Scientific Ideas', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2645.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2645.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory-Attention Generator (Mem2seq + Pointer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-attention based generation combining Mem2seq-style multihop memory networks and pointer-generator copy mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A knowledge-grounded text generation module that initializes decoder state via multihop memory attention over retrieved related entities, uses memory-network attention (multihop, coverage-aware) and reference (title) attention during decoding, and combines generation and copy distributions with gating to produce abstracts/conclusions and follow-on titles grounded in KG-predicted entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Memory-Attention Generator (Mem2seq + pointer-generator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reference encoder: bi-directional GRU encodes the input title. Initialization: multihop memory-attention (ϕ hops, authors use ϕ=3) over related-entity memory vectors to form initial decoder hidden state. Decoder: at each step compute memory attention (multihop with entity coverage vectors), reference (title) attention with coverage to reduce repetition, and generate token via a hybrid of (a) vocabulary generation P_gen (softmax over language model conditioned on decoder state, reference and memory contexts), (b) copying from title P_τ, (c) copying from related entities P_e. Two gates (g_p for generate vs copy; ~tilde{g}_p for copy-from-title vs copy-from-entities) mix distributions. Loss combines negative log-likelihood plus coverage loss for both reference and memory attention; repetition-removal masking applied at test time (beam-search masking).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>knowledge-grounded neural generation / retrieval-augmented generation (memory networks + pointer-generator)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical text generation (paper abstracts, conclusions/future work, titles)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not directly a hypothesis generator; used to verbalize hypotheses/ideas (predicted entities/links) into readable abstracts and sections, thereby surfacing hypotheses generated by the KG link predictor to human readers.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is indirectly assessed by grounding generated text in high-confidence KG entities and by human evaluation (Turing tests and post-editing). No automatic logical/causal plausibility verifier is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Downstream textual quality metrics used as proxies for usefulness: perplexity (measured on an external LM), METEOR for topical relevance, BLEU/ROUGE/TER for post-edit comparisons. No direct hypothesis-quality metrics beyond ranking of entities by link-confidence y.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Automatic metrics (perplexity, METEOR) and human Turing tests; human post-editing study (50 abstracts) measures edit effort and textual similarity scores; ablation studies demonstrate impact of removing memory network (topic drift) or link-prediction (loss of related entities).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Architecture details (number of hops, gates, coverage loss formulation), training/truncation heuristics and dataset splits provided; code and resources stated as available on GitHub.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Grounding via copy mechanisms to KG-predicted entities and title tokens, use of coverage loss (to reduce repeated ungrounded tokens), and repetition-removal beam masking at test time to reduce repetitive hallucinated phrases.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Human evaluation and post-editing identify problematic hallucinations; ablation analyses reveal topical drift when memory networks are removed. No automated factual verification module reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Token-level generation probabilities: P_gen (vocabulary), P_τ (title copy), P_e (entity copy) and gating outputs (g_p, ~tilde{g}_p) indicate the model's preference and can be interpreted as internal confidence measures; attention distributions and coverage vectors reflect uncertainty about focus over memory/reference.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same PMC-derived datasets and training/dev/test splits as PaperRobot; generation evaluations use held-out test set of 2,095 examples per task and an external LM trained on PubMed subsets for perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Generation metrics (see PaperRobot): Title→Abstract PPL 11.5 / METEOR 13.0 (full approach). Human-selection rates in Turing tests: abstracts chosen up to 30% by domain expert. Ablation: removing memory networks increases topic drift and degrades outputs (qualitative and reflected in higher perplexity/METEOR when evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms baseline Seq2seq, Editing Network and Pointer Network on perplexity and METEOR (see numeric comparisons in main paper Table 3). Pointer Network can be viewed as removing memory network from the full approach and performs worse (much higher perplexity).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not perform factual reasoning beyond copying/patterns; susceptible to generating incorrect factual details (abbreviations, numbers, pronouns) particularly for rare tokens; success depends on quality and granularity of extracted entities/relations and size of domain corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaperRobot: Incremental Draft Generation of Scientific Ideas', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Translating embeddings for modeling multi-relational data <em>(Rating: 2)</em></li>
                <li>Graph attention networks <em>(Rating: 2)</em></li>
                <li>Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems <em>(Rating: 2)</em></li>
                <li>Get to the point: Summarization with pointer-generator networks <em>(Rating: 2)</em></li>
                <li>Mining strong relevance between heterogeneous entities from unstructured biomedical data <em>(Rating: 2)</em></li>
                <li>Modeling coverage for neural machine translation <em>(Rating: 1)</em></li>
                <li>End-to-end memory networks <em>(Rating: 1)</em></li>
                <li>Paper abstract writing through editing mechanism <em>(Rating: 1)</em></li>
                <li>Learning to generate one-sentence biographies from Wikidata <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2645",
    "paper_id": "paper-a6aed0c4e0f39a55edb407f492e41f178a62907f",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "PaperRobot",
            "name_full": "PaperRobot: Incremental Draft Generation of Scientific Ideas",
            "brief_description": "An end-to-end research assistant that (1) extracts background knowledge graphs (KGs) from large collections of papers, (2) generates new scientific ideas by predicting links (new edges) in those KGs, and (3) incrementally drafts parts of papers (title → abstract → conclusion/future work → follow-on title) using memory-attention generation networks grounded in extracted entities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PaperRobot",
            "system_description": "PaperRobot is a multi-component system combining knowledge extraction, knowledge-graph (KG) link prediction, and knowledge-grounded neural generation. Components: (a) Background Knowledge Extraction — entity mention extraction/linking (Wei et al., 2013) to MeSH/CTD, building a KG of entities (Disease, Chemical, Gene) and relations; (b) Link Prediction — rich entity representations combining multi-head graph attention over one-hop neighbors and a contextual text encoder (bi-LSTM) over sentences containing the entity, a gated combination of graph/text representations, trained with a TransE-style translation objective and marginal loss with negative sampling to score candidate triplets; (c) New-idea / Hypothesis Generation — predict new edges (e_i, r, e_j) with an associated confidence score y and rank/select top related entities; (d) Paper Writing — memory-attention networks (multihop memory attention) plus a pointer-generator copy mechanism (hybrid Mem2seq + pointer-generator) to generate abstracts, conclusions/future work and follow-on titles conditioned on an input title and the predicted related entities; (e) Training & Safety components — coverage loss and a simple beam-search masking repetition-removal method at test time to reduce repetition; human Turing tests and human post-editing used for validation.",
            "system_type": "hybrid: knowledge-graph-based + knowledge-grounded neural generation (memory-attention + pointer-generator)",
            "scientific_domain": "biomedical / life sciences (applied to PMC Open Access biomedical papers); also experimented on NLP-domain corpus",
            "hypothesis_generation_method": "Generates hypotheses as link-prediction outputs on a constructed biomedical KG: compute entity embeddings combining multi-head graph-attention (attention over one-hop neighbors) and contextual text embeddings (bi-LSTM with bilinear attention); train embeddings with a TransE translation objective (minimize F(h,r,t)=||e_h + r - e_t||_2^2) using marginal ranking loss and negative sampling; for each indirectly connected pair and relation type, compute score y (from trained embeddings / distance) indicating probability the triple holds; rank by y and select top related entities (up to 10) as candidate hypotheses/ideas.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Uses the learned link-confidence score y (derived from embedding distance F and subsequent scoring/ranking) to indicate plausibility; also uses attention-based softmax probabilities and gating when incorporating entities into generated text to favor higher-confidence items.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Confidence score y for predicted triples (derived from embedding distance F); distance score F(e_h,r,e_t)=||e_h + r - e_t||_2^2 used during training; ranking by y to select top-k related entities. No explicit novelty/uniqueness metrics reported.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Computational and human evaluation: (1) automatic evaluation of generated text (perplexity measured on an external language model, METEOR for topical relevance), (2) human Turing tests where experts/non-experts compare system vs human text (reported percent selections), (3) human post-editing (domain expert edits system abstracts) and plagiarism checks, (4) ablation studies to assess component contributions. Predicted KG links themselves are NOT experimentally (wet-lab) validated and are noted to require human verification.",
            "reproducibility_measures": "Code and data resources stated as publicly available on GitHub (link in paper). Model architectures, training objectives (TransE-style loss, marginal loss, coverage loss), and dataset sizes (training/dev/test counts) are reported; hyperparameters noted to be in Appendix.",
            "hallucination_prevention_method": "Grounding generation in: (a) KG-predicted entities (copying mechanism / memory attention) to reduce free hallucination, (b) coverage losses for both reference attention and memory distribution to avoid repeated attention (and thus repeated unsupported claims), and (c) a beam-search masking method at test time that forbids re-generating non-stop words already produced in the same output.",
            "hallucination_detection_method": "Post-generation human expert review (post-editing) and Turing tests used to surface incorrect or implausible outputs; ablation analysis and manual inspection identify incorrect abbreviations, numbers and pronouns. No automated hallucination-detector (e.g., fact-checker) reported.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Uses model-derived scalar confidence/probability scores: link prediction confidence score y (from embedding distance); softmax attention weights and generator probabilities P_gen, P_tau (copy-from-title), P_e (copy-from-entity); gates (g_p, σ-based) produce probabilistic mixing ratios, which serve as measures of model uncertainty about source of tokens. No Bayesian or explicit calibrated uncertainty framework reported.",
            "benchmark_dataset": "PMC Open Access Subset (1,687,060 papers used to construct KGs). Training/dev/test splits for writing tasks: Training ~22,811 papers (titles→abstract etc.); Development 2,095; Test 2,095. External language model trained on 500k titles, 50k abstracts, 50k conclusions/future-work from PubMed.",
            "performance_metrics": "Paper-writing: Title→Abstract perplexity 11.5 and METEOR 13.0 (our full approach). Comparisons: Seq2seq PPL 19.6 / METEOR 9.1; Editing Network PPL 18.8 / METEOR 9.2; Pointer Network PPL 146.7 / METEOR 8.5. Abstract→Conclusion PPL 18.3 / METEOR 11.2 (our approach). Conclusion→Title PPL 14.8 / METEOR 8.9 (our approach). Human evaluation (Turing tests): system-generated abstracts selected over human-authored ones by domain expert up to 30% of the time; conclusions/future-work up to 24%; new titles up to 12% (Table 4). Human post-editing metrics: BLEU1 59.6, BLEU4 55.4, ROUGE 73.3, TER 35.2 for 50 abstracts after expert edits (Table 5).",
            "comparison_with_baseline": "The paper compares its generation module to Seq2seq, an Editing Network, and a Pointer Network: their full approach outperforms baselines in perplexity and METEOR (see numbers above). Human judges sometimes prefer PaperRobot outputs (up to 30% for abstracts by expert). Link prediction module is qualitatively validated (examples and ablations) but no numeric link-prediction baseline scores are presented in the paper.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Authors note key limitations: reliance on language model and extracted facts (limited reasoning), occasional generation of incorrect abbreviations, numbers, pronouns and rare-context entities; predicted new links require human verification (no wet-lab validation); lower performance in smaller domains (NLP corpus) due to fewer examples and coarse-grained entity/relation types; no explicit novelty-detection or formal plausibility-certification beyond confidence scores and human review.",
            "uuid": "e2645.0",
            "source_info": {
                "paper_title": "PaperRobot: Incremental Draft Generation of Scientific Ideas",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "KG Link Predictor (TransE + Graph/Text Encoder)",
            "name_full": "Knowledge Graph Link Prediction module combining multi-head graph attention, textual context encoder, gated combination and TransE objective",
            "brief_description": "A KG link-prediction method that forms entity embeddings by fusing multi-head graph-attention over one-hop neighbors with contextual textual encodings (bi-LSTM + bilinear attention), combined via an element-wise gate, and trained with a TransE translation loss and marginal ranking loss to score candidate triples and produce confidence y for hypothesized relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "KG Link Predictor (GraphAttention + Context + TransE)",
            "system_description": "Entity representation: graph-structure encoder uses self-attention / multi-head graph attention (inspired by GAT) over one-hop neighbors to produce structure-based context representations; contextual text encoder uses a bi-LSTM over a context sentence with bilinear attention to produce text-based context representation; a learned sigmoid gate combines graph/text vectors into a final entity embedding. Training: TransE-style translational assumption (e_h + r ≈ e_t), distance F = ||e_h + r - e_t||_2^2 and marginal ranking loss with negative samples (replace head or tail randomly). Prediction: compute score y for each candidate (e_i, r, e_j) from embeddings and rank/threshold to enrich KG with predicted edges (top-k selected).",
            "system_type": "knowledge graph-based (graph attention + text-enhanced embeddings)",
            "scientific_domain": "biomedical (entities: Disease, Chemical, Gene) constructed from PMC; general KG completion methodology",
            "hypothesis_generation_method": "Generates candidate hypotheses by scoring and ranking candidate triples (e_i, r, e_j) using learned embeddings and translational distance; propagation of neighbors from semantically similar entities (if Calcium and Zinc similar, propagate neighbors), with top-ranked predictions forming new edges/hypotheses.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Uses the embedding-derived score y to indicate likelihood/plausibility and ranks predictions by y; no external novelty/plausibility filter beyond ranking and human review.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Distance score F(e_h,r,e_t)=||e_h + r - e_t||_2^2 used in training; confidence score y computed for predictions and used for ranking and selection. No additional quantitative hypothesis-quality metrics reported.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Qualitative examples and ablation studies demonstrating that link prediction introduces topically related entities (Table 7). Authors explicitly state predicted links need to be verified by humans; no wet-lab validation.",
            "reproducibility_measures": "Training objective, negative sampling strategy, and dataset size for KG construction are reported; code/data reportedly available on GitHub.",
            "hallucination_prevention_method": "Grounding predictions within the constructed KG and combining graph-structure and contextual text reduces arbitrary generation; gating reduces over-emphasis on noisy source. However, no formal hallucination-proofing mechanism for link prediction is reported.",
            "hallucination_detection_method": "Manual inspection and qualitative analysis in ablations detected implausible outputs; predicted links are flagged as needing human verification.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Confidence score y for each predicted triple (probabilistic ranking); softmax attention distributions over neighbors and text tokens provide additional internal probability weights.",
            "benchmark_dataset": "KG constructed from PMC Open Access Subset: 1,687,060 papers, 30,483 entities, 875,698 relations. Training/dev/test splits provided for downstream generation tasks.",
            "performance_metrics": "No numeric link-prediction performance metrics (e.g., Hits@k, MRR) are reported in the paper; evaluation is qualitative (examples) and downstream effect measured via writing metrics and ablation impact.",
            "comparison_with_baseline": "Authors situate approach relative to translation-based KG embeddings (TransE) and prior text-enhanced KG embeddings, and claim novelty by incorporating multi-head graph attention; no direct numeric baseline comparison for link-prediction metrics is provided.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "No explicit calibration or formal novelty detection; predictions require human verification; no reported numeric link-prediction benchmarks (Hits@k/MRR) to quantify accuracy; rare entities with limited contexts produce unreliable contextual embeddings leading to incorrect predictions.",
            "uuid": "e2645.1",
            "source_info": {
                "paper_title": "PaperRobot: Incremental Draft Generation of Scientific Ideas",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Memory-Attention Generator (Mem2seq + Pointer)",
            "name_full": "Memory-attention based generation combining Mem2seq-style multihop memory networks and pointer-generator copy mechanism",
            "brief_description": "A knowledge-grounded text generation module that initializes decoder state via multihop memory attention over retrieved related entities, uses memory-network attention (multihop, coverage-aware) and reference (title) attention during decoding, and combines generation and copy distributions with gating to produce abstracts/conclusions and follow-on titles grounded in KG-predicted entities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Memory-Attention Generator (Mem2seq + pointer-generator)",
            "system_description": "Reference encoder: bi-directional GRU encodes the input title. Initialization: multihop memory-attention (ϕ hops, authors use ϕ=3) over related-entity memory vectors to form initial decoder hidden state. Decoder: at each step compute memory attention (multihop with entity coverage vectors), reference (title) attention with coverage to reduce repetition, and generate token via a hybrid of (a) vocabulary generation P_gen (softmax over language model conditioned on decoder state, reference and memory contexts), (b) copying from title P_τ, (c) copying from related entities P_e. Two gates (g_p for generate vs copy; ~tilde{g}_p for copy-from-title vs copy-from-entities) mix distributions. Loss combines negative log-likelihood plus coverage loss for both reference and memory attention; repetition-removal masking applied at test time (beam-search masking).",
            "system_type": "knowledge-grounded neural generation / retrieval-augmented generation (memory networks + pointer-generator)",
            "scientific_domain": "biomedical text generation (paper abstracts, conclusions/future work, titles)",
            "hypothesis_generation_method": "Not directly a hypothesis generator; used to verbalize hypotheses/ideas (predicted entities/links) into readable abstracts and sections, thereby surfacing hypotheses generated by the KG link predictor to human readers.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility is indirectly assessed by grounding generated text in high-confidence KG entities and by human evaluation (Turing tests and post-editing). No automatic logical/causal plausibility verifier is reported.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Downstream textual quality metrics used as proxies for usefulness: perplexity (measured on an external LM), METEOR for topical relevance, BLEU/ROUGE/TER for post-edit comparisons. No direct hypothesis-quality metrics beyond ranking of entities by link-confidence y.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Automatic metrics (perplexity, METEOR) and human Turing tests; human post-editing study (50 abstracts) measures edit effort and textual similarity scores; ablation studies demonstrate impact of removing memory network (topic drift) or link-prediction (loss of related entities).",
            "reproducibility_measures": "Architecture details (number of hops, gates, coverage loss formulation), training/truncation heuristics and dataset splits provided; code and resources stated as available on GitHub.",
            "hallucination_prevention_method": "Grounding via copy mechanisms to KG-predicted entities and title tokens, use of coverage loss (to reduce repeated ungrounded tokens), and repetition-removal beam masking at test time to reduce repetitive hallucinated phrases.",
            "hallucination_detection_method": "Human evaluation and post-editing identify problematic hallucinations; ablation analyses reveal topical drift when memory networks are removed. No automated factual verification module reported.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Token-level generation probabilities: P_gen (vocabulary), P_τ (title copy), P_e (entity copy) and gating outputs (g_p, ~tilde{g}_p) indicate the model's preference and can be interpreted as internal confidence measures; attention distributions and coverage vectors reflect uncertainty about focus over memory/reference.",
            "benchmark_dataset": "Same PMC-derived datasets and training/dev/test splits as PaperRobot; generation evaluations use held-out test set of 2,095 examples per task and an external LM trained on PubMed subsets for perplexity.",
            "performance_metrics": "Generation metrics (see PaperRobot): Title→Abstract PPL 11.5 / METEOR 13.0 (full approach). Human-selection rates in Turing tests: abstracts chosen up to 30% by domain expert. Ablation: removing memory networks increases topic drift and degrades outputs (qualitative and reflected in higher perplexity/METEOR when evaluated).",
            "comparison_with_baseline": "Outperforms baseline Seq2seq, Editing Network and Pointer Network on perplexity and METEOR (see numeric comparisons in main paper Table 3). Pointer Network can be viewed as removing memory network from the full approach and performs worse (much higher perplexity).",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Does not perform factual reasoning beyond copying/patterns; susceptible to generating incorrect factual details (abbreviations, numbers, pronouns) particularly for rare tokens; success depends on quality and granularity of extracted entities/relations and size of domain corpus.",
            "uuid": "e2645.2",
            "source_info": {
                "paper_title": "PaperRobot: Incremental Draft Generation of Scientific Ideas",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Translating embeddings for modeling multi-relational data",
            "rating": 2
        },
        {
            "paper_title": "Graph attention networks",
            "rating": 2
        },
        {
            "paper_title": "Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems",
            "rating": 2
        },
        {
            "paper_title": "Get to the point: Summarization with pointer-generator networks",
            "rating": 2
        },
        {
            "paper_title": "Mining strong relevance between heterogeneous entities from unstructured biomedical data",
            "rating": 2
        },
        {
            "paper_title": "Modeling coverage for neural machine translation",
            "rating": 1
        },
        {
            "paper_title": "End-to-end memory networks",
            "rating": 1
        },
        {
            "paper_title": "Paper abstract writing through editing mechanism",
            "rating": 1
        },
        {
            "paper_title": "Learning to generate one-sentence biographies from Wikidata",
            "rating": 1
        }
    ],
    "cost": 0.01564825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PaperRobot: Incremental Draft Generation of Scientific Ideas</h1>
<p>Qingyun Wang ${ }^{1}$, Lifu Huang ${ }^{1}$, Zhiying Jiang ${ }^{1}$, Kevin Knight ${ }^{2}$, Heng Ji ${ }^{1,3}$, Mohit Bansal ${ }^{4}$, Yi Luan ${ }^{5}$<br>${ }^{1}$ Rensselaer Polytechnic Institute ${ }^{2}$ DiDi Labs ${ }^{3}$ University of Illinois at Urbana-Champaign<br>${ }^{4}$ University of North Carolina at Chapel Hill ${ }^{5}$ University of Washington<br>kevinknight@didiglobal.com, hengji@illinois.edu</p>
<h4>Abstract</h4>
<p>We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to $30 \%, 24 \%$ and $12 \%$ of the time, respectively. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Our ambitious goal is to speed up scientific discovery and production by building a PaperRobot, who addresses three main tasks as follows.</p>
<p>Read Existing Papers. Scientists now find it difficult to keep up with the overwhelming amount of papers. For example, in the biomedical domain, on average more than 500 K papers are published every year ${ }^{2}$, and more than 1.2 million new papers are published in 2016 alone, bringing the total number of papers to over 26 million (Van Noorden, 2014). However, human's reading ability</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>keeps almost the same across years. In 2012, US scientists estimated that they read, on average, only 264 papers per year ( 1 out of 5000 available papers), which is, statistically, not different from what they reported in an identical survey last conducted in 2005. PaperRobot automatically reads existing papers to build background knowledge graphs (KGs), in which nodes are entities/concepts and edges are the relations between these entities (Section 2.2).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: PaperRobot Incremental Writing
Create New Ideas. Scientific discovery can be considered as creating new nodes or links in the knowledge graphs. Creating new nodes usually means discovering new entities (e.g., new proteins) through a series of real laboratory experiments, which is probably too difficult for PaperRobot. In contrast, creating new edges is easier to automate using the background knowledge graph as the starting point. Foster et al. (2015) shows that more than $60 \%$ of 6.4 million papers in biomedicine and chemistry are about incremental work. This inspires us to automate the incremental creation of new ideas and hypotheses by predicting new links in background KGs. In fact, when there is more data available, we can construct larger and richer background KGs for more reliable link prediction. Recent work (Ji et al., 2015b) successfully mines strong relevance between drugs and diseases from biomedical pa-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: PaperRobot Architecture Overview
pers based on KGs constructed from weighted cooccurrence. We propose a new entity representation that combines KG structure and unstructured contextual text for link prediction (Section 2.3).</p>
<p>Write a New Paper about New Ideas. The final step is to communicate the new ideas to the reader clearly, which is a very difficult thing to do; many scientists are, in fact, bad writers (Pinker, 2014). Using a novel memory-attention network architecture, PaperRobot automatically writes a new paper abstract about an input title along with predicted related entities, then further writes conclusion and future work based on the abstract, and finally predicts a new title for a future follow-on paper, as shown in Figure 1 (Section 2.4).</p>
<p>We choose biomedical science as our target domain due to the sheer volume of available papers. Turing tests show that PaperRobot-generated output strings are sometimes chosen over humanwritten ones; and most paper abstracts only require minimal edits from domain experts to become highly informative and coherent.</p>
<h2>2 Approach</h2>
<h3>2.1 Overview</h3>
<p>The overall framework of PaperRobot is illustrated in Figure 2. A walk-through example produced from this whole process is shown in Table 1. In the following subsections, we will elaborate on the algorithms for each step.</p>
<h3>2.2 Background Knowledge Extraction</h3>
<p>From a massive collection of existing biomedical papers, we extract entities and their relations
to construct background knowledge graphs (KGs). We apply an entity mention extraction and linking system (Wei et al., 2013) to extract mentions of three entity types (Disease, Chemical and Gene) which are the core data categories in the Comparative Toxicogenomics Database (CTD) (Davis et al., 2016), and obtain a Medical Subject Headings (MeSH) Unique ID for each mention. Based on the MeSH Unique IDs, we further link all entities to the CTD and extract 133 subtypes of relations such as Marker/Mechanism, Therapeutic, and Increase Expression. Figure 3 shows an example.</p>
<h3>2.3 Link Prediction</h3>
<p>After constructing the initial KGs from existing papers, we perform link prediction to enrich them. Both contextual text information and graph structure are important to represent an entity, thus we combine them to generate a rich representation for each entity. Based on the entity representations, we determine whether any two entities are semantically similar, and if so, we propagate the neighbors of one entity to the other. For example, in Figure 3, because Calcium and Zinc are similar in terms of contextual text information and graph structure, we predict two new neighbors for Calcium: CD14 molecule and neuropilin 2 which are neighbors of Zinc in the initial KGs.</p>
<p>We formulate the initial KGs as a list of tuples numbered from 0 to $\kappa$. Each tuple $\left(e_{i}^{h}, r_{i}, e_{i}^{t}\right)$ is composed of a head entity $e_{i}^{h}$, a tail entity $e_{i}^{t}$, and their relation $r_{i}$. Each entity $e_{i}$ may be involved in multiple tuples and its one-hop connected neighbors are denoted as $N_{e_{i}}=\left[n_{i 1}, n_{i 2}, \ldots\right] . \quad e_{i}$ is</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Biomedical Knowledge Extraction and Link Prediction Example (dash lines are predicted links)
also associated with a context description $s_{i}$ which is randomly selected from the sentences where $e_{i}$ occurs. We randomly initialize vector representations $\boldsymbol{e}<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}$ and $\boldsymbol{r}</em>$ respectively.
Graph Structure Encoder To capture the importance of each neighbor's feature to $e_{i}$, we perform self-attention (Veličković et al., 2018) and compute a weight distribution over $N_{e_{i}}$ :}}$ for $e_{i}$ and $r_{i</p>
<p>$$
\begin{aligned}
\boldsymbol{e}<em e="e">{i}^{\prime} &amp; =\boldsymbol{W}</em>} \boldsymbol{e<em i="i" j="j">{i}, \quad \boldsymbol{n}</em>}^{\prime}=\boldsymbol{W<em i="i" j="j">{e} \boldsymbol{n}</em> \
c_{i j} &amp; =\operatorname{LeakyReLU}\left(\boldsymbol{W}<em i="i">{f}\left(\boldsymbol{e}</em>}^{\prime} \oplus \boldsymbol{n<em i="i">{i j}^{\prime}\right)\right) \
\boldsymbol{c}</em>\right)
\end{aligned}
$$}^{\prime} &amp; =\operatorname{Softmax}\left(\boldsymbol{c}_{i</p>
<p>where $\boldsymbol{W}<em f="f">{e}$ is a linear transformation matrix applied to each entity. $\boldsymbol{W}</em>}$ is the parameter for a single layer feedforward network. $\oplus$ denotes the concatenation operation between two matrices. Then we use $\boldsymbol{c<em e__i="e_{i">{i}^{\prime}$ and $N</em>}}$ to compute a structure based context representation of $\boldsymbol{\epsilon<em i="i" j="j">{i}=\sigma\left(\sum c</em>}^{\prime} \boldsymbol{n<em i="i" j="j">{i j}^{\prime}\right)$, where $n</em>$ and $\sigma$ is Sigmoid function.} \in N_{e_{i}</p>
<p>In order to capture various types of relations between $e_{i}$ and its neighbors, we further perform multi-head attention on each entity, based on multiple linear transformation matrices. Finally, we get a structure based context representation $\tilde{\boldsymbol{e}}<em i="i">{i}=$ $\left[\boldsymbol{\epsilon}</em>}^{0} \oplus \ldots \oplus \boldsymbol{\epsilon<em i="i">{i}^{M}\right]$, where $\boldsymbol{\epsilon}</em>}^{m}$ refers to the context representation obtained with the $m$-th head, and $\tilde{\boldsymbol{e}<em 1="1">{i}$ is the concatenated representation based on the attention of all $M$ heads.
Contextual Text Encoder Each entity $e$ is also associated with a context sentence $\left[w</em>}, \ldots, w_{l}\right]$. To incorporate the local context information, we first apply a bi-directional long short-term memory (LSTM) (Graves and Schmidhuber, 2005) network to get the encoder hidden states $\boldsymbol{H<em 1="1">{s}=$
$\left[\boldsymbol{h}</em>}, \ldots, \boldsymbol{h<em i="i">{l}\right]$, where $\boldsymbol{h}</em>}$ represents the hidden state of $w_{i}$. Then we compute a bilinear attention weight for each word $w_{i}: \mu_{i}=\boldsymbol{e}^{\top} \boldsymbol{W<em i="i">{s} \boldsymbol{h}</em>}, \boldsymbol{\mu}^{\prime}=$ $\operatorname{Softmax}(\boldsymbol{\mu})$, where $\boldsymbol{W<em i="i">{s}$ is a bilinear term. We finally get the context representation $\hat{\boldsymbol{e}}=\boldsymbol{\mu}^{\top} \boldsymbol{h}</em>$, we design a gate function to balance these two types of information:}$. Gated Combination To combine the graph-based representation $\hat{\boldsymbol{e}}$ and local context based representations $\hat{\boldsymbol{e}</p>
<p>$$
\boldsymbol{g}<em e="e">{e}=\sigma\left(\hat{\boldsymbol{g}}</em>}\right), \quad \boldsymbol{e}=\boldsymbol{g<em e="e">{e} \odot \hat{\boldsymbol{e}}+\left(1-\boldsymbol{g}</em>
$$}\right) \odot \hat{\boldsymbol{e}</p>
<p>where $\boldsymbol{g}<em e="e">{e}$ is an entity-dependent gate function of which each element is in $[0,1], \hat{\boldsymbol{g}}</em>$ is a learnable parameter for each entity $e, \sigma$ is a Sigmoid function, and $\odot$ is an element-wise multiplication.
Training and Prediction To optimize both entity and relation representations, following TransE (Bordes et al., 2013), we assume the relation between two entities can be interpreted as translations operated on the entity representations, namely $\boldsymbol{h}+\boldsymbol{r} \approx \boldsymbol{t}$ if $(h, r, t)$ holds. Therefore, for each tuple $\left(e_{i}^{h}, r_{i}, e_{i}^{t}\right)$, we can compute their distance score: $F\left(e_{i}^{h}, r_{i}, e_{i}^{t}\right)=\left|\boldsymbol{e}<em i="i">{i}^{h}+\boldsymbol{r}</em>}-\boldsymbol{e<em 2="2">{i}^{t}\right|</em>$. We use marginal loss to train the model:}^{2</p>
<p>$$
\begin{aligned}
&amp; \text { Loss }=\sum_{\left(e_{i}^{h}, r_{i}, e_{i}^{t}\right) \in K} \sum_{\left(\bar{e}<em i="i">{i}^{h}, \bar{r}</em>}, \bar{e<em i="i">{i}^{t}\right) \in K} \max (0 \
&amp; \gamma+F\left(e</em>}^{h}, r_{i}, e_{i}^{t}\right)-F\left(\bar{e<em i="i">{i}^{h}, \bar{r}</em>\right))
\end{aligned}
$$}, \bar{e}_{i}^{t</p>
<p>where $\left(e^{h}, r, t^{h}\right)$ is a positive tuple and $\left(\bar{e}^{h}, \bar{r}^{h}, \bar{t}^{h}\right)$ is a negative tuple, and $\gamma$ is a margin. The negative tuples are generated by either replacing the head or the tail entity of positive tuples with a randomly chosen different entity.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Title</th>
<th style="text-align: center;">Snail transcription factor negatively regulates maspin tumor suppressor in human prostate cancer cells</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Entities</td>
<td style="text-align: center;">Related: nasopharyngeal carcinoma; diallyl disulfide</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Output</td>
<td style="text-align: center;">Human (Neal et al., 2012)</td>
<td style="text-align: center;">System</td>
<td style="text-align: center;">Post-edited by Human</td>
</tr>
<tr>
<td style="text-align: center;">Abstract</td>
<td style="text-align: center;">Background: Maspin, a putative tumor suppressor that is down-regulated in breast and prostate cancer, has been associated with decreased cell motility. Snail transcription factor is a zinc finger protein that is increased in breast cancer and is associated with increased tumor motility and invasion by induction of epithelial-mesenchymal transition (EMT). We investigated the molecular mechanisms by which Snail increases tumor motility and invasion utilizing prostate cancer cells. Methods: Expression levels were analyzed by RT-PCR and western blot analyses. Cell motility and invasion assays were performed, while Snail regulation and binding to maspin promoter was analyzed by luciferase reporter and chromatin immunoprecipitation (ChIP) assays. Results: Snail protein expression was higher in different prostate cancer cells lines as compared to normal prostate epithelial cells.</td>
<td style="text-align: center;">Background: Snail is a multifunctional protein that plays an important role in the pathogenesis of prostate cancer. However, it has been shown to be associated with poor prognosis. The purpose of this study was to investigate the effect of negatively on the expression of maspin in human nasopharyngeal carcinoma cell lines. Methods: Quantitative real-time PCR and western blot analysis were used to determine whether the demethylating agent was investigated by quantitative RT-PCR (qRT-PCR) and Western blotting. Results showed that the binding protein plays a significant role in the regulation of tumor growth and progression.</td>
<td style="text-align: center;">Background: Snail is a multifunctional protein that plays an important role in the pathogenesis of prostate cancer. It has been shown associated with poor prognosis. The purpose of this study is to investigate the negative effect of on the expression of Maspin in human nasopharyngeal carcinoma cell lines. Methods: Quantitative RT-PCR (qRT-PCR) and western blot analyses were used to determine correlation of the two proteins expressions. Results showed that the binding protein plays a significant role in the regulation of tumor growth and progression.</td>
</tr>
<tr>
<td style="text-align: center;">Conclusion and Future work</td>
<td style="text-align: center;">Collectively, our results indicate for the first time that Snail can negatively regulate maspin through direct promoter repression resulting in increased migration and invasion in prostate cancer cells. This study reveals a novel mechanism of how Snail may function and show the importance of therapeutic targeting of Snail signaling in future.</td>
<td style="text-align: center;">In summary, our study demonstrates that Snail negatively inhibited the expression of Maspin in human nasopharyngeal carcinoma cell lines and in vitro. Our results indicate that the combination of the demethylating agent might be a potential therapeutic target for the treatment of prostate cancer.</td>
<td style="text-align: center;">In summary, our study in vitro demonstrates that Snail negatively inhibits the expression of Maspin in human nasopharyngeal carcinoma cell lines. Our results further indicate that Maspin might be a potential therapeutic target for the treatment of prostate cancer.</td>
</tr>
<tr>
<td style="text-align: center;">New Title</td>
<td style="text-align: center;">Role of maspin in cancer (Berardi et al., 2013)</td>
<td style="text-align: center;">The role of nasopharyngeal carcinoma in the rat model of prostate cancer cells</td>
<td style="text-align: center;">The role of Maspin in the rat model of nasopharyngeal carcinoma cells</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of Human and System Written Paper Elements (bold words are topically related entities; italic words show human edits)</p>
<p>After training, for each pair of indirectly connected entities $e_{i}, e_{j}$ and a relation type $r$, we compute a score $y$ to indicate the probability that $\left(e_{i}, r, e_{j}\right)$ holds, and obtain an enriched knowledge graph $\widetilde{K}=\left[\left(e_{n+1}^{k}, r_{n+1}, e_{n+1}^{t}, y_{n+1}\right) \ldots\right]$.</p>
<h3>2.4 New Paper Writing</h3>
<p>In this section, we use title-to-abstract generation as a case study to describe the details of our paper writing approach. Other tasks (abstract-toconclusion and future work, and conclusion and future work-to-title) follow the same architecture.</p>
<p>Given a reference title $\tau=\left[w_{1}, \ldots, w_{l}\right]$, we apply the knowledge extractor (Section 2.2) to extract entities from $\tau$. For each entity, we retrieve a set of related entities from the enriched knowledge graph $\widetilde{K}$ after link prediction. We rank all the related entities by confidence scores and select up to</p>
<p>10 most related entities $E_{\tau}=\left[e_{1}^{\tau}, \ldots, e_{v}^{\tau}\right]$. Then we feed $\tau$ and $E_{\tau}$ together into the paper generation framework as shown in Figure 2. The framework is based on a hybrid approach of a Mem2seq model (Madotto et al., 2018) and a pointer generator (Gu et al., 2016; See et al., 2017). It allows us to balance three types of sources for each time step during decoding: the probability of generating a token from the entire word vocabulary based on language model, the probability of copying a word from the reference title, such as regulates in Table 1, and the probability of incorporating a related entity, such as Snail in Table 1. The output is a paragraph $Y=\left[y_{1}, \ldots, y_{o}\right] .^{3}$
Reference Encoder For each word in the refer-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ence title, we randomly embed it into a vector and obtain $\boldsymbol{\tau}=\left[\boldsymbol{w}<em l="l">{1}, \ldots, \boldsymbol{w}</em>}\right]$. Then, we apply a bi-directional Gated Recurrent Unit (GRU) encoder (Cho et al., 2014) on $\boldsymbol{\tau}$ to produce the encoder hidden states $\boldsymbol{H}=\left[\boldsymbol{h<em l="l">{1}, \ldots, \boldsymbol{h}</em>\right]$.
Decoder Hidden State Initialization Not all predicted entities are equally relevant to the title. For example, for the title in Table 2, we predict multiple related entities including nasopharyngeal carcinoma and diallyl disulfide, but nasopharyngeal carcinoma is more related because nasopharyngeal carcinoma is also a cancer related to snail transcription factor, while diallyl disulfide is less related because diallyl disulfide's anticancer mechanism is not closely related to maspin tumor suppressor. We propose to apply memoryattention networks to further filter the irrelevant ones. Recent approaches (Sukhbaatar et al., 2015; Madotto et al., 2018) show that compared with soft-attention, memory-based multihop attention is able to refine the attention weight of each memory cell to the query multiple times, drawing better correlations. Therefore, we apply a multihop attention mechanism to generate the initial decoder hidden state.</p>
<p>Given the set of related entities $E=\left[e_{1}, \ldots, e_{v}\right]$, we randomly initialize their vector representation $\boldsymbol{E}=\left[\boldsymbol{e}<em v="v">{1}, \ldots, \boldsymbol{e}</em>}\right]$ and store them in memories. Then we use the last hidden state of reference encoder $\boldsymbol{h<em 0="0">{l}$ as the first query vector $\boldsymbol{q}</em>$, and iteratively compute the attention distribution over all memories and update the query vector:</p>
<p>$$
\begin{aligned}
p_{k i} &amp; =\boldsymbol{\nu}<em q="q">{k}^{\top} \tanh \left(\boldsymbol{W}</em>}^{k} \boldsymbol{q<em e="e">{k-1}+\boldsymbol{U}</em>}^{k} \boldsymbol{e<em k="k">{i}+\boldsymbol{b}</em>\right) \
\boldsymbol{q}<em k="k">{k} &amp; =\boldsymbol{p}</em>
\end{aligned}
$$}^{\top} \boldsymbol{e}+\boldsymbol{q}_{k-1</p>
<p>where $k$ denotes the $k$-th hop among $\varphi$ hops in total. ${ }^{4}$ After $\varphi$ hops, we obtain $\boldsymbol{q}<em j="j">{\varphi}$ and take it as the initial hidden state of the GRU decoder.
Memory Network To better capture the contribution of each entity $e</em>}$ to each decoding output, at each decoding step $i$, we compute an attention weight for each entity and apply a memory network to refine the weights multiple times. We take the hidden state $\tilde{\boldsymbol{h}<em 0="0">{i}$ as the initial query $\tilde{\boldsymbol{q}}</em>$ and iteratively update it:}=\tilde{\boldsymbol{h}}_{i</p>
<p>$$
\begin{aligned}
&amp; \tilde{p}<em k="k">{k j}=\nu</em>}^{\top} \tanh \left(\widetilde{\boldsymbol{W}<em k-1="k-1">{\tilde{q}}^{k} \tilde{\boldsymbol{q}}</em>}+\widetilde{\boldsymbol{U}<em j="j">{e}^{k} \boldsymbol{e}</em>}+\boldsymbol{W<em i="i" j="j">{\tilde{e}} \tilde{c}</em>\right) \
&amp; \boldsymbol{u}}+b_{k<em k="k">{i k}=\widetilde{\boldsymbol{p}}</em>}^{\top} \boldsymbol{e<em k="k">{j}, \quad \tilde{\boldsymbol{q}}</em>}=\boldsymbol{u<em k-1="k-1">{i k}+\tilde{\boldsymbol{q}}</em>
\end{aligned}
$$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>where $\tilde{\boldsymbol{c}}<em m="0">{i j}=\sum</em>}^{i-1} \boldsymbol{\beta<em i="i">{m j}$ is an entity coverage vector and $\boldsymbol{\beta}</em>}$ is the attention distribution of last hop $\boldsymbol{\beta<em _psi="\psi">{i}=\widetilde{\boldsymbol{p}}</em>}$, and $\psi$ is the total number of hops. We then obtain a final memory based context vector for the set of related entities $\boldsymbol{\chi<em _psi="\psi" i="i">{i}=\boldsymbol{u}</em>$.
Reference Attention Our reference attention is similar to (Bahdanau et al., 2015; See et al., 2017), which aims to capture the contribution of each word in the reference title to the decoding output. At each time step $i$, the decoder receives the previous word embedding and generate decoder state $\tilde{\boldsymbol{h}}_{i}$, the attention weight of each reference token is computed as:</p>
<p>$$
\begin{aligned}
&amp; \alpha_{i j}=\boldsymbol{\varsigma}^{\top} \tanh \left(\boldsymbol{W}<em i="i">{h} \tilde{\boldsymbol{h}}</em>}+\boldsymbol{W<em j="j">{\tau} \boldsymbol{h}</em>}+\boldsymbol{W<em i="i" j="j">{\tilde{e}} \tilde{\boldsymbol{c}}</em>}+\boldsymbol{b<em i="i">{\tau}\right) \
&amp; \boldsymbol{\alpha}</em>}^{\prime}=\operatorname{Softmax}\left(\boldsymbol{\alpha<em i="i">{i}\right) ; \quad \boldsymbol{\phi}</em>}=\boldsymbol{\alpha<em j="j">{i}^{\prime \top} \boldsymbol{h}</em>
\end{aligned}
$$</p>
<p>$\tilde{\boldsymbol{c}}<em m="0">{i j}=\sum</em>}^{i-1} \alpha_{m j}$ is a reference coverage vector, which is the sum of attention distributions over all previous decoder time steps to reduce repetition (See et al., 2017). $\boldsymbol{\phi<em _tau="\tau">{i}$ is the reference context vector.
Generator For a particular word $w$, it may occur multiple times in the reference title or in multiple related entities. Therefore, at each decoding step $i$, for each word $w$, we aggregate its attention weights from the reference attention and memory attention distributions: $P</em>}^{i}=\sum_{m \mid w_{m}=w} \boldsymbol{\alpha<em e="e">{i m}^{\prime}$ and $P</em>}^{i}=\sum_{m \mid w \in e_{m}} \boldsymbol{\beta<em i="i">{i m}$ respectively. In addition, at each decoding step $i$, each word in the vocabulary may also be generated with a probability according to the language model. The probability is computed from the decoder state $\tilde{\boldsymbol{h}}</em>}$, the reference context vector $\boldsymbol{\phi<em i="i">{i}$, and the memory context vector $\boldsymbol{\chi}</em>}: P_{\text {gen }}=\operatorname{Softmax}\left(\boldsymbol{W<em i="i">{\text {gen }}\left[\tilde{\boldsymbol{h}}</em>} ; \boldsymbol{\phi<em i="i">{i} ; \boldsymbol{\chi}</em>}\right]+\boldsymbol{b<em _gen="{gen" _text="\text">{\text {gen }}\right)$, where $\boldsymbol{W}</em>}}$ and $b_{\text {gen }}$ are learnable parameters. To combine $P_{\tau}, P_{e}$ and $P_{\text {gen }}$, we compute a gate $\boldsymbol{g<em p="p">{\tau}$ as a soft switch between generating a word from the vocabulary and copying words from the reference title $\tau$ or the related entities $E: \boldsymbol{g}</em>}=$ $\sigma\left(\boldsymbol{W<em i="i">{\boldsymbol{p}}^{\top} \tilde{\boldsymbol{h}}</em>}+\boldsymbol{W<em i-1="i-1">{\boldsymbol{\varsigma}}^{\top} \boldsymbol{z}</em>}+\boldsymbol{b<em i-1="i-1">{p}\right)$, where $\boldsymbol{z}</em>}$ is the embedding of the previous generated token at step $i-1 . \quad \boldsymbol{W<em z="z">{p}, \boldsymbol{W}</em>}$, and $\boldsymbol{b<em p="p">{p}$ are learnable parameters, and $\sigma$ is a Sigmoid function. We also compute a gate $\tilde{\boldsymbol{g}}</em>}$ as a soft switch between copying words from reference text and the related entities: $\tilde{\boldsymbol{g}<em _phi="\phi">{p}=\sigma\left(\boldsymbol{W}</em>}^{\top} \boldsymbol{\phi<em _chi="\chi">{i}+\boldsymbol{W}</em>}^{\top} \boldsymbol{\chi<em p="p">{i}+\tilde{\boldsymbol{b}}</em>}\right)$, where $\boldsymbol{W<em _chi="\chi">{\phi}, \boldsymbol{W}</em>$ are learnable parameters.}$, and $\tilde{\boldsymbol{b}}_{p</p>
<p>The final probability of generating a token $z$ at decoding step $i$ can be computed by:</p>
<p>$$
P\left(z_{i}\right)=\boldsymbol{g}<em e="e" g="g" n="n">{p} P</em>}+\left(1-\boldsymbol{g<em p="p">{p}\right)\left(\tilde{\boldsymbol{g}}</em>} P_{\tau}+\left(1-\tilde{\boldsymbol{g}<em e="e">{p}\right) P</em>\right)
$$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;"># papers</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"># avg entities</th>
<th style="text-align: center;"># avg predicted</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Title-to-</td>
<td style="text-align: center;">Abstract-to-Conclusion</td>
<td style="text-align: center;">Conclusion and</td>
<td style="text-align: center;">in Title /</td>
<td style="text-align: center;">related entities /</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Abstract</td>
<td style="text-align: center;">and Future work</td>
<td style="text-align: center;">Future work-to-Title</td>
<td style="text-align: center;">paper</td>
<td style="text-align: center;">paper</td>
</tr>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: center;">22,811</td>
<td style="text-align: center;">22,811</td>
<td style="text-align: center;">15,902</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Development</td>
<td style="text-align: center;">2,095</td>
<td style="text-align: center;">2,095</td>
<td style="text-align: center;">2,095</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">6.1</td>
</tr>
<tr>
<td style="text-align: left;">Test</td>
<td style="text-align: center;">2,095</td>
<td style="text-align: center;">2,095</td>
<td style="text-align: center;">2,095</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">8.5</td>
</tr>
</tbody>
</table>
<p>Table 2: Paper Writing Statistics</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Title-to-Abstract</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Abstract-to-Conclusion <br> and Future Work</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Conclusion and <br> Future Work-to-Title</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Perplexity</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">Perplexity</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">Perplexity</td>
<td style="text-align: center;">METEOR</td>
</tr>
<tr>
<td style="text-align: left;">Seq2seq (Bahdanau et al., 2015)</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">6.0</td>
</tr>
<tr>
<td style="text-align: left;">Editing Network (Wang et al., 2018b)</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">5.5</td>
</tr>
<tr>
<td style="text-align: left;">Pointer Network (See et al., 2017)</td>
<td style="text-align: center;">146.7</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">6.6</td>
</tr>
<tr>
<td style="text-align: left;">Our Approach (-Repetition Removal)</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">$\mathbf{1 2 . 3}$</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">7.4</td>
</tr>
<tr>
<td style="text-align: left;">Our Approach</td>
<td style="text-align: center;">$\mathbf{1 1 . 5}$</td>
<td style="text-align: center;">$\mathbf{1 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{1 8 . 3}$</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">$\mathbf{1 4 . 8}$</td>
<td style="text-align: center;">$\mathbf{8 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Automatic Evaluation on Paper Writing for Diagnostic Tasks (\%). The Pointer Network can be viewed as removing memory network part from our approach without repetition removal.</p>
<p>The loss function, combined with the coverage loss (See et al., 2017) for both reference attention and memory distribution, is presented as:</p>
<p>$$
\begin{aligned}
\text { Loss }= &amp; \sum_{i}-\log P\left(z_{i}\right)+\lambda \sum_{i}\left(\min \left(\alpha_{i j}, \tilde{\boldsymbol{c}}<em i="i" j="j">{i j}\right)\right. \
&amp; \left.+\min \left(\beta</em>\right)\right)
\end{aligned}
$$}, \hat{\boldsymbol{c}}_{i j</p>
<p>where $P\left(z_{i}\right)$ is the prediction probability of the ground truth token $z_{i}$, and $\lambda$ is a hyperparameter.
Repetition Removal Similar to many other long text generation tasks (Suzuki and Nagata, 2017), repetition remains a major challenge (Foster and White, 2007; Xie, 2017). In fact, $11 \%$ sentences in human written abstracts include repeated entities, which may mislead the language model. Following the coverage mechanism proposed by (Tu et al., 2016; See et al., 2017), we use a coverage loss to avoid any entity in reference input text or related entity receiving attention multiple times. We further design a new and simple masking method to remove repetition during the test time. We apply beam search with beam size 4 to generate each output, if a word is not a stop word or punctuation and it is already generated in the previous context, we will not choose it again in the same output.</p>
<h2>3 Experiment</h2>
<h3>3.1 Data</h3>
<p>We collect biomedical papers from the PMC Open Access Subset. ${ }^{5}$ To construct ground truth for new title prediction, if a human written paper $A$</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>cites a paper $B$, we assume the title of $A$ is generated from $B$ 's conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix.</p>
<h3>3.2 Automatic Evaluation</h3>
<p>Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model ${ }^{6}$ learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment. ${ }^{7}$ The results are shown in Table 3. We can see that our framework outperforms all previous approaches.</p>
<h3>3.3 Turing Test</h3>
<p>Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert (non-native speaker) and a non-expert (native speaker). Each human judge is asked to compare a system output and a human-authored string, and select the better one.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Input</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Output</th>
<th style="text-align: center;">Domain Expert</th>
<th style="text-align: center;">Non-expert</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">End-to-End</td>
<td style="text-align: center;">Human Title</td>
<td style="text-align: center;">Different</td>
<td style="text-align: center;">Abstract (1st)</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Same</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">System Abstract</td>
<td style="text-align: center;">Different</td>
<td style="text-align: center;">Conclusion and Future work</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Same</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">System Conclusion and Future work</td>
<td style="text-align: center;">Different</td>
<td style="text-align: center;">Title</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Same</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">System Title</td>
<td style="text-align: center;">Different</td>
<td style="text-align: center;">Abstract (2nd)</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Diagnostic</td>
<td style="text-align: center;">Human Abstract</td>
<td style="text-align: center;">Different</td>
<td style="text-align: center;">Conclusion and Future work</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Same</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human Conclusion and Future work</td>
<td style="text-align: center;">Different</td>
<td style="text-align: center;">Title</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Same</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p>Table 4: Turing Test Human Subject Passing Rates (\%). Percentages show how often a human judge chooses our system's output over human's when it is mixed with a human-authored string. If the output strings (e.g., abstracts) are based on the same input string (e.g., title), the Input condition is marked "Same", otherwise "Different".</p>
<table>
<thead>
<tr>
<th style="text-align: left;">BLEU1</th>
<th style="text-align: center;">BLEU2</th>
<th style="text-align: center;">BLEU3</th>
<th style="text-align: center;">BLEU4</th>
<th style="text-align: center;">ROUGE</th>
<th style="text-align: center;">TER</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">59.6</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">35.2</td>
</tr>
</tbody>
</table>
<p>Table 5: Evaluation on Human Post-Editing(\%)</p>
<p>Table 4 shows the results on 50 pairs in each setting. We can see that PaperRobot generated abstracts are chosen over human-written ones by the expert up to $30 \%$ times, conclusion and future work up to $24 \%$ times, and new titles up to $12 \%$ times. We don't observe the domain expert performs significantly better than the non-expert, because they tend to focus on different aspects the expert focuses on content (entities, topics, etc.) while the non-expert focuses on the language.</p>
<h3>3.4 Human Post-Editing</h3>
<p>In order to measure the effectiveness of PaperRobot acting as a wring assistant, we randomly select 50 paper abstracts generated by the system during the first iteration and ask the domain expert to edit them until he thinks they are informative and coherent. The BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and TER (Snover et al., 2006) scores by comparing the abstracts before and after human editing are presented in Table 5. It took about 40 minutes for the expert to finish editing 50 abstracts. Table 1 includes the post-edited example. We can see that most edits are stylist changes.</p>
<h3>3.5 Analysis and Discussions</h3>
<p>To better justify the function of each component, we conduct ablation studies by removing memory networks, link prediction, and repetition removal respectively. The results are shown in Table 6. We can see that the approach without memory networks tends to diverge from the main topic, especially for generating long texts such as
abstracts (the detailed length statistics are shown in Table 8). From Table 6 we can see the later parts of the abstract (Methods and Results) include topically irrelevant entities such as "imipramine" which is used to treat depression instead of human prostate cancer.</p>
<p>Link prediction successfully introduces new and topically related ideas, such as " $R T-P C R$ " and "western blot" which are two methods for analyzing the expression level of Snail protein, as also mentioned in the human written abstract in Table 1. Table 7 shows more examples of entities which are related to the entities in input titles based on link prediction. We can see that the predicted entities are often genes or proteins which cause the disease mentioned in a given title, or other diseases from the same family.</p>
<p>Our simple beam search based masking method successfully removes some repeated words and phrases and thus produces more informative output. The plagiarism check in Table 9 shows our model is creative, because it's not simply copying from the human input.</p>
<h3>3.6 Remaining Challenges</h3>
<p>Our generation model is still largely dependent on language model and extracted facts, and thus it lacks of knowledge reasoning. It generates a few incorrect abbreviations such as "Organophosphates(BA)", "chronic kidney disease(UC)" and "Fibrosis(DC)") because they appear rarely in the training data and thus their contextual representations are not reliable. It also generates some incorrect numbers (e.g., "The patients were divided into four groups : Group 1, Group B...") and pronouns (e.g., "A 63-year-old man was referred to our hospital ... she was treated with the use of the descending coronary artery").</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Output</th>
<th style="text-align: center;">Without Memory Networks</th>
<th style="text-align: center;">Without Link Prediction</th>
<th style="text-align: center;">Without Repetition Removal</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Abstract</td>
<td style="text-align: center;">Background: Snail has been reported to exhibit a variety of biological functions. In this study, we investigated the effect of negatively on maspin demethylation in human prostate cancer cells. Methods: Quantitative real-time PCR and western blot analysis were used to investigate the effects of the demethylating agent on the expression of the protein kinase (TF) gene promoter. Results: The results showed that the presence of a single dose of $50 \mu M$ in a dose-dependent manner, whereas the level of the BMP imipramine was significantly higher than that of the control group.</td>
<td style="text-align: center;">Background: Snail has been shown to be associated with poor prognosis. In this study, we investigated the effect of negatively on the expression of maspin in human prostate cancer cells. Methods: Cells were treated with a single dose of radiotherapy for 24 h , and was used to investigate the significance of a quantitative factor for the treatment of the disease. Results: The remaining controls showed a significant increase in the G2/M phase of the tumor suppressor protein ( $\mathrm{p}&lt;0.05$ ).</td>
<td style="text-align: center;">Background: Snail is a major health problem in human malignancies. However, the role of Snail on the expression of maspin in human prostate cancer cells is not well understood. The aim of this study was to investigate the effect of Snail on the expression of maspin in human prostate cancer cells. Methods: The expression of the expression of Snail and maspin was investigated using quantitative RT-PCR and western blot analysis. Results: The remaining overall survival (OS) and overall survival (OS) were analyzed.</td>
</tr>
<tr>
<td style="text-align: center;">Conclusion and Future work</td>
<td style="text-align: center;">In summary, our study demonstrated that negatively inhibited the expression of the BMP imipramine in human prostate cancer cells. Our findings suggest that the inhibition of maspin may be a promising therapeutic strategy for the treatment.</td>
<td style="text-align: center;">In summary, our results demonstrate that negatively inhibited the expression of maspin in human prostate cancer cells. Our findings suggest that the combination of radiotherapy may be a potential therapeutic target for the treatment of disease.</td>
<td style="text-align: center;">In summary, our results demonstrate that snail inhibited the expression of maspin in human prostatic cells. The expression of snail in PC-3 cells by snail, and the expression of maspin was observed in the presence of the expression of maspin.</td>
</tr>
<tr>
<td style="text-align: center;">New Title</td>
<td style="text-align: center;">Protective effects of homolog on human breast cancer cells by inhibiting the Endoplasmic Reticulum Stress</td>
<td style="text-align: center;">The role of prostate cancer in human breast cancer cells</td>
<td style="text-align: center;">The role of maspin and maspin in human breast cancer cells</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation Test Results on the Same Title in Table 1</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Titles</th>
<th style="text-align: left;">Predicted Related Entities</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pseudoachondroplasia/COMP translating from the bench to the <br> bedside</td>
<td style="text-align: left;">osteoarthritis; skeletal dysplasia; thrombospondin-5</td>
</tr>
<tr>
<td style="text-align: left;">Role of ceramide in diabetes mellitus: evidence and mechanisms</td>
<td style="text-align: left;">diabetes insulin ceramide; metabolic disease</td>
</tr>
<tr>
<td style="text-align: left;">Exuberant clinical picture of Buschke-Fischer-Brauer palmo- <br> plantar keratoderma in bedridden patient</td>
<td style="text-align: left;">neoplasms; retinoids; autosomal dominant disease</td>
</tr>
<tr>
<td style="text-align: left;">Relationship between serum adipokine levels and radiographic <br> progression in patients with ankylosing spondylitis</td>
<td style="text-align: left;">leptin; rheumatic diseases; adiponectin; necrosis; <br> DKK-1; IL-6-RFP</td>
</tr>
</tbody>
</table>
<p>Table 7: More Link Prediction Examples (bold words are entities detected from titles)</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Abstract</th>
<th style="text-align: center;">Conclusion and <br> Future Work</th>
<th style="text-align: center;">Title</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">System</td>
<td style="text-align: center;">112.4</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">16.5</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">106.5</td>
<td style="text-align: center;">105.5</td>
<td style="text-align: center;">13.0</td>
</tr>
</tbody>
</table>
<p>Table 8: The Average Number of Words of System and Human Output</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Output</th>
<th style="text-align: center;">$\mathbf{1}$</th>
<th style="text-align: center;">$\mathbf{2}$</th>
<th style="text-align: center;">$\mathbf{3}$</th>
<th style="text-align: center;">$\mathbf{4}$</th>
<th style="text-align: center;">$\mathbf{5}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Abstracts</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">8.03</td>
<td style="text-align: center;">3.60</td>
<td style="text-align: center;">1.46</td>
</tr>
<tr>
<td style="text-align: left;">Conclusions</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">5.52</td>
<td style="text-align: center;">2.58</td>
<td style="text-align: center;">1.28</td>
</tr>
<tr>
<td style="text-align: left;">Titles</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">1.31</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.00</td>
</tr>
</tbody>
</table>
<p>Table 9: Plagiarism Check: Percentage (\%) of $n$-grams in human input which appear in system generated output for test data.</p>
<p>All of the system generated titles are declarative sentences while human generated titles are often more engaging (e.g., "Does HPV play any role in the initiation or prognosis of endometrial
adenocarcinomas?"). Human generated titles often include more concrete and detailed ideas such as "etumorType, An Algorithm of Discriminating Cancer Types for Circulating Tumor Cells or Cellfree DNAs in Blood", and even create new entity abbreviations such as etumorType in this example.</p>
<h3>3.7 Requirements to Make PaperRobot Work: Case Study on NLP Domain</h3>
<p>When a cool Natural Language Processing (NLP) system like PaperRobot is built, it's natural to ask whether she can benefit the NLP community itself. We re-build the system based on 23,594 NLP papers from the new ACL Anthology Network (Radev et al., 2013). For knowledge extraction we apply our previous system trained for the NLP domain (Luan et al., 2018). But the results are much less satisfactory compared to the</p>
<p>biomedical domain. Due to the small size of data, the language model is not able to effectively copy out-of-vocabulary words and thus the output is often too generic. For example, given a title "Statistics based hybrid approach to Chinese base phrase identification", PaperRobot generates a fluent but uninformative abstract "This paper describes a novel approach to the task of Chinese-base-phrase identification. We first utilize the solid foundation for the Chinese parser, and we show that our tool can be easily extended to meet the needs of the sentence structure.".</p>
<p>Moreover, compared to the biomedical domain, the types of entities and relations in the NLP domain are rather coarse-grained, which often leads to inaccurate prediction of related entities. For example, for an NLP paper title "Extracting molecular binding relationships from biomedical text", PaperRobot mistakenly extracts "prolog" as a related entity and generates an abstract "In this paper, we present a novel approach to the problem of extracting relationships among the prolog program. We present a system that uses a macromolecular binding relationships to extract the relationships between the abstracts of the entry. The results show that the system is able to extract the most important concepts in the prolog program.".</p>
<h2>4 Related Work</h2>
<p>Link Prediction. Translation-based approaches (Nickel et al., 2011; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015a) have been widely exploited for link prediction. Compared with these studies, we are the first to incorporate multi-head graph attention (Sukhbaatar et al., 2015; Madotto et al., 2018; Veličković et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing.</p>
<p>Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al.,</p>
<p>2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published paper. This is the first work on automatic writing of key paper elements for the biomedical domain, especially conclusion and future work, and follow-on paper titles.</p>
<h2>5 Conclusions and Future Work</h2>
<p>We build a PaperRobot who can predict related entities for an input title and write some key elements of a new paper (abstract, conclusion and future work) and predict a new title. Automatic evaluations and human Turing tests both demonstrate her promising performance. PaperRobot is merely an assistant to help scientists speed up scientific discovery and production. Conducting experiments is beyond her scope, and each of her current components still requires human intervention: constructed knowledge graphs cannot cover all technical details, predicted new links need to be verified, and paper drafts need further editing. In the future, we plan to develop techniques for extracting entities of more fine-grained entity types, and extend PaperRobot to write related work, predict authors, their affiliations and publication venues.</p>
<h2>Acknowledgments</h2>
<p>The knowledge extraction and prediction components were supported by the U.S. NSF No. 1741634 and Tencent AI Lab Rhino-Bird Gift Fund. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the 5th International Conference on Learning Representations.</p>
<p>Rossana Berardi, Francesca Morgese, Azzurra Onofri, Paola Mazzanti, Mirco Pistelli, Zelmira Ballatore, Agnese Savini, Mariagrazia De Lisa, Miriam Caramanti, Silvia Rinaldi, et al. 2013. Role of maspin in cancer. Clinical and translational medicine.</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In Advances in neural information processing systems.</p>
<p>Andrew Chisholm, Will Radford, and Ben Hachey. 2017. Learning to generate one-sentence biographies from Wikidata. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics.</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Allan Peter Davis, Cynthia J Grondin, Robin J Johnson, Daniela Sciaky, Benjamin L King, Roy McMorran, Jolene Wiegers, Thomas C Wiegers, and Carolyn J Mattingly. 2016. The comparative toxicogenomics database: update 2017. Nucleic acids research.</p>
<p>Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the 9th Workshop on Statistical Machine Translation.</p>
<p>Daniel Duma and Ewan Klein. 2013. Generating natural language from linked data: Unsupervised template extraction. In Proceedings of the 10th International Conference on Computational Semantics.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. Generation from abstract meaning representation using tree transducers. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Jacob G. Foster, Andrey Rzhetsky, and James A. Evans. 2015. Tradition and innovation in scientists research strategies. American Sociological Review.</p>
<p>Mary Ellen Foster and Michael White. 2007. Avoiding repetition in generated text. In Proceedings of the 11th European Workshop on Natural Language Generation.</p>
<p>Alex Graves and Jürgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional lstm and other neural network architectures. In Proceedings of the 2015 IEEE International Joint Conference on Neural Networks.</p>
<p>Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Hardy Hardy and Andreas Vlachos. 2018. Guided neural language generation for abstractive summarization using Abstract Meaning Representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. 2015a. Knowledge graph embedding via dynamic mapping matrix. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing.</p>
<p>Ming Ji, Qi He, Jiawei Han, and Scott Spangler. 2015b. Mining strong relevance between heterogeneous entities from unstructured biomedical data. Data Mining and Knowledge Discovery, 29:976998.</p>
<p>Lucie-Aimée Kaffee, Hady Elsahar, Pavlos Vougiouklis, Christophe Gravier, Frederique Laforest, Jonathon Hare, and Elena Simperl. 2018. Learning to generate Wikipedia summaries for underserved languages from Wikidata. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Ioannis Konstas and Mirella Lapata. 2013. A global model for concept-to-text generation. Journal of Artificial Intelligence Research.</p>
<p>Niveda Krishnamoorthy, Girish Malkarnenkar, Raymond J Mooney, Kate Saenko, and Sergio Guadarrama. 2013. Generating natural-language video descriptions using text-mined knowledge. In Proceedings of the 27th AAAI Conference on Artificial Intelligence.</p>
<p>Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Georgios Spithourakis, Jianfeng Gao, and Bill Dolan. 2016. A persona-based neural conversation model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Proceedings of Text Summarization Branches Out.</p>
<p>Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the 39th AAAI Conference on Artificial Intelligence.</p>
<p>Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, and Zhifang Sui. 2018. Table-to-text generation by structure-aware seq2seq learning. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence.</p>
<p>Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</p>
<p>Di Lu, Spencer Whitehead, Lifu Huang, Heng Ji, and Shih-Fu Chang. 2018. Entity-aware image caption generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. 2018. Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Corey L. Neal, Veronica Henderson, Bethany N. Smith, Danielle McKeithen, Tisheeka Graham, Baohan T. Vo, and Valerie A. Odero-Marah. 2012. Snail transcription factor negatively regulates maspin tumor suppressor in human prostate cancer cells. BMC Cancer.</p>
<p>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on Machine Learning.</p>
<p>Feng Nie, Jinpeng Wang, Jin-Ge Yao, Rong Pan, and Chin-Yew Lin. 2018. Operation-guided neural networks for high fidelity data-to-text generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Steven Pinker. 2014. Why academics stink at writing. The Chronicle of Higher Education.</p>
<p>Nima Pourdamghani, Kevin Knight, and Ulf Hermjakob. 2016. Generating English from Abstract Meaning Representations. In Proceedings of the 9th International Natural Language Generation conference.</p>
<p>Yuting Qiang, Yanwei Fu, Yanwen Guo, Zhi-Hua Zhou, and Leonid Sigal. 2016. Learning to generate posters of scientific papers. In Proceedings of the 30th AAAI Conference on Artificial Intelligence.</p>
<p>Dragomir R. Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. 2013. The acl anthology network corpus. Language Resources and Evaluation, pages 1-26.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Sujian Li, Baobao Chang, and Zhifang Sui. 2018. Orderplanning neural text generation from structured data. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence.</p>
<p>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the Association for Machine Translation in the Americas.</p>
<p>Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. In Advances in Neural Information Processing Systems.</p>
<p>Jun Suzuki and Masaaki Nagata. 2017. Cutting-off redundant repeating generations for neural abstractive summarization. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics.</p>
<p>Bayu Distiawan Trisedya, Jianzhong Qi, Rui Zhang, and Wei Wang. 2018. GTR-LSTM: A triple encoder for sentence generation from RDF data. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016. Modeling coverage for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Raghuram Vadapalli, Bakhtiyar Syed, Nishant Prabhu, Balaji Vasan Srinivasan, and Vasudeva Varma. 2018. When science journalism meets artificial intelligence: An interactive demonstration. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Richard Van Noorden. 2014. Scientists may be reaching a peak in reading habits. Nature.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. Proceedings of the 8th International Conference on Learning Representations.</p>
<p>Qingyun Wang, Xiaoman Pan, Lifu Huang, Boliang Zhang, Zhiying Jiang, Heng Ji, and Kevin Knight. 2018a. Describing a knowledge base. In Proceedings of the 11th International Conference on Natural Language Generation.</p>
<p>Qingyun Wang, Zhihao Zhou, Lifu Huang, Spencer Whitehead, Boliang Zhang, Heng Ji, and Kevin Knight. 2018b. Paper abstract writing through editing mechanism. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the 28th AAAI Conference on Artificial Intelligence.</p>
<p>Zhigang Wang and Juan-Zi Li. 2016. Text-enhanced representation learning for knowledge graph. In Proceedings of the 25th International Joint Conference on Artificial Intelligence.</p>
<p>Chih-Hsuan Wei, Hung-Yu Kao, and Zhiyong Lu. 2013. PubTator: a web-based text mining tool for assisting biocuration. Nucleic acids research.</p>
<p>Spencer Whitehead, Heng Ji, Mohit Bansal, Shih-Fu Chang, and Clare Voss. 2018. Incorporating background knowledge into video description generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Sam Wiseman, Stuart Shieber, and Alexander Rush. 2018. Learning neural templates for text generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Qi Wu, Chunhua Shen, Peng Wang, Anthony Dick, and Anton van den Hengel. 2018. Image captioning and visual question answering based on attributes and external knowledge. In Proceedings of the 2018 IEEE transactions on pattern analysis and machine intelligence.</p>
<p>Ziang Xie. 2017. Neural text generation: A practical guide. arXiv preprint arXiv:1711.09534.</p>
<p>Jiacheng Xu, Kan Chen, Xipeng Qiu, and Xuanjing Huang. 2017. Knowledge graph representation with jointly structural and textual encoding. In Proceedings of the 26th International Joint Conference on Artificial Intelligence.</p>
<p>Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, and Vadim Sheinin. 2018. SQL-to-text generation with graph-to-sequence model. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/ oa_package/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ https://github.com/pytorch/examples/ tree/master/word_language_model
${ }^{7}$ The perplexity scores of the language model are in the Appendix.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8328 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8328</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8328</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-271329041</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.14790v2.pdf" target="_blank">Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?</a></p>
                <p><strong>Paper Abstract:</strong> Solving grid puzzles involves a significant amount of logical reasoning. Hence, it is a good domain to evaluate reasoning capability of a model which can then guide us to improve the reasoning ability of models. However, most existing works evaluate only the final predicted answer of a puzzle, without delving into an in-depth analysis of the LLMs’ reasoning chains (such as where they falter) or providing any finer metrics to evaluate them. Since LLMs may rely on simple heuristics or artifacts to predict the final answer, it is crucial to evaluate the generated reasoning chain beyond overall correctness measures, for accurately evaluating the reasoning abilities of LLMs. To this end, we first develop GridPuzzle, an evaluation dataset comprising of 274 grid-based puzzles with different complexities. Second, we propose a new error taxonomy derived from manual analysis of reasoning chains from LLMs including GPT-4, Claude-3, Gemini, Mistral, and Llama-2. Then, we develop a LLM-based framework for large-scale subjective evaluation (i.e., identifying errors) and an objective metric, PuzzleEval, to evaluate the correctness of reasoning chains. Evaluating reasoning chains from LLMs leads to several interesting findings. We further show that existing prompting methods used for enhancing models’ reasoning abilities do not improve performance on GridPuzzle. This highlights the importance of understanding fine-grained errors and presents a challenge for future research to enhance LLMs’ puzzle-solving abilities by developing methods that address these errors.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8328.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8328.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 / GPT-4-Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source large language model from OpenAI evaluated in this study for step-by-step reasoning on logic grid puzzles using zero-shot chain-of-thought prompting; used as both a solver (GPT-4-Turbo) and as the basis for the PuzzleEval/auto-evaluator (GPT-4o was used as evaluator).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source transformer-based LLM from OpenAI; used in April 2024 model releases for deterministic, zero-temperature inference in this study. The paper does not provide architecture or training-data specifics beyond being a closed-source high-capability LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GridPuzzle (logic grid puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzle (2D assignment grid; relational matching across rows/columns)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot Chain-of-Thought (Zero-shot-CoT) prompting: natural language instruction plus puzzle text; model prompted to produce a step-by-step reasoning chain and final answer table; temperature set to 0 for deterministic output; final answers compared via exact-match table EM against gold solution.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot chain-of-thought step-by-step reasoning; produced multi-step textual reasoning chains. The authors also used GPT-4o as an auto-evaluator to label steps and compute PuzzleEval; various prompting mitigation strategies (Plan-and-Solve, Self-correct, Self-discover, Feedback-Learning, Program-of-Thought) were evaluated with GPT-4-Turbo but provided little improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy (exact-match of final answer table): GPT-4 achieved the highest accuracy in the study at 5.11% (14 / 274 puzzles). PuzzleEval Average Correctness Score (ACS): 0.59 average across grid sizes (per Table 4: 3x4=0.61, 3x5=0.62, 4x4=0.56, 4x5=0.54, 4x6=0.60).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No direct evidence or probing of geometric/3D spatial reasoning was performed. The paper focuses on logical relational reasoning in a 2D grid (assignment relations); analyses evaluate correctness and error types of reasoning steps rather than diagnosing spatial representations. Thus there is no explicit evidence that GPT-4 is using spatial reasoning beyond managing relational/grid assignments in text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against Claude-3, Gemini-Pro, Llama-2-13B, Mistral-7B and Llama-2-70B in the same Zero-shot-CoT setup; GPT-4 had the highest accuracy but PuzzleEval scores were similar to Gemini. Authors also compared behavior when applying mitigation prompting strategies and found limited improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite relatively higher PuzzleEval ACS, GPT-4 still produced low final-answer accuracy (5.11%); errors frequently occur late in reasoning chains (error propagation, wrong elimination, wrong reasoning). Many steps are classified as reiteration (NC). Prompting-based mitigation strategies did not substantially improve performance. No specific geometric/spatial failure analyses were performed because the task is primarily relational grid logic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8328.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8328.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3 (Anthropic) - family includes Opus, Sonnet, Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source LLM (Anthropic Claude-3 family) evaluated on GridPuzzle using zero-shot chain-of-thought prompting to produce stepwise reasoning and final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3 (Opus variant referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source LLM family from Anthropic; treated in the paper as a high-capability model used in April 2024 releases for deterministic inference (temperature 0) in zero-shot-CoT prompts. No architecture/parameter details provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GridPuzzle (logic grid puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzle (2D relational assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot-CoT: prompt with instruction + puzzle, request step-by-step reasoning and final answer table; temperature 0, deterministic inference; outputs evaluated by Exact Match accuracy and PuzzleEval ACS.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot chain-of-thought stepwise textual reasoning. Also evaluated via PuzzleEval (GPT-4o) and via the Auto-evaluator taxonomy for step-wise error categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PuzzleEval Average Correctness Score (ACS): 0.54 average across grid sizes (per Table 4: 3x4=0.60, 3x5=0.56, 4x4=0.52, 4x5=0.55, 4x6=0.46). Exact-match accuracy value is not explicitly enumerated in the text for Claude-3, but closed-source models performed marginally better than open-source ones.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No explicit analyses probing spatial reasoning (geometric or 3D) were performed. All evaluation concerns logical relational reasoning in grid puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to GPT-4, Gemini-Pro, Llama-2, Mistral; shows intermediate PuzzleEval performance (worse than GPT-4/Gemini in ACS but better than small open-source models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Exhibits the same broad failure patterns: many steps are NC (reiteration), errors cluster in later steps (error propagation, wrong elimination). Prompt-based mitigation strategies did not significantly improve final accuracy on GridPuzzle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8328.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8328.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-Pro (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source high-capability LLM from Google evaluated on GridPuzzle under zero-shot-CoT; produced relatively high PuzzleEval ACS but lower final exact-match accuracy comparable to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source LLM from Google used in April 2024 model releases for deterministic inference in this study; the paper does not provide architecture or parameter counts for Gemini-Pro.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GridPuzzle (logic grid puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzle (2D relational assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot chain-of-thought prompting: instruction + puzzle text; model asked to produce step-by-step reasoning chain and final answer table; temperature=0. Outputs evaluated by Exact Match and PuzzleEval.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot-CoT; produced shorter reasoning chains on average relative to GPT-4 (average steps reported ~14.91 for Gemini vs ~20.66 for GPT-4), which authors note may reduce opportunities for errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PuzzleEval ACS: 0.58 average across grid sizes (per Table 4: 3x4=0.60, 3x5=0.64, 4x4=0.54, 4x5=0.52, 4x6=0.62). Exact-match accuracy is not explicitly listed numerically in the main text (only that GPT-4 had the highest at 5.11%); Gemini's accuracy is described as marginally lower than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No direct spatial-reasoning probes; analysis focuses on logical correctness of stepwise deductions rather than spatial/geometric representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to GPT-4 and Claude-3; similar PuzzleEval ACS to GPT-4 though with notably fewer reasoning steps. Closed-source models (including Gemini) outperform open-source ones.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Shorter chains may hide errors (fewer steps); still low final-answer accuracy due to late-chain errors (wrong elimination, error propagation). Prompt mitigation strategies yielded only marginal gains (Self-Discover gave small improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8328.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8328.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-13B (Meta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LLM (Llama-2 family, 13B parameter variant) evaluated on GridPuzzle; performance was poor, solving very few puzzles and showing low PuzzleEval ACS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer-based LLM (13B parameters) from Meta used in the study for deterministic zero-shot-CoT inference; authors ran inference on NVIDIA A100 GPUs. Paper does not describe training data beyond model name.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GridPuzzle (logic grid puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzle (2D relational assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot-CoT prompting: instruction + puzzle text; model asked to output step-by-step reasoning and final answer table; temperature 0. Inference on A100 GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot chain-of-thought; no external symbolic solvers used. Authors also ran a scaling experiment to Llama-2-70B for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact-match accuracy: Llama-2-13B solved only 1 puzzle correctly out of 274 (1 / 274). PuzzleEval ACS: average 0.41 across grid sizes (Table 4: 3x4=0.45, 3x5=0.46, 4x4=0.46, 4x5=0.42, 4x6=0.28).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No explicit evidence of spatial reasoning; performance indicates limited reasoning skill on relational grid tasks. Authors note Llama-2 and other open-source models lack reasoning skills compared to closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to closed-source GPT-4, Claude-3, Gemini and open-source Mistral; clear gap with closed-source models. Scaling to the 70B Llama improved performance slightly (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Consistently poor accuracy and lower PuzzleEval ACS that degrades with increased grid size; low instruction-following capabilities and parameter size cited as contributors to poor performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8328.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8328.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-70B (Meta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger open-source variant of Llama-2 (70B parameters) evaluated in a scaling experiment that showed only marginal improvement over the 13B variant on GridPuzzle.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer-based LLM (70B parameters) from Meta; evaluated by the authors in a zero-shot-CoT setting to measure scaling effects relative to the 13B variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GridPuzzle (logic grid puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzle (2D relational assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot-CoT prompting identical to other models; scaling experiment used same dataset and evaluation (Exact Match & PuzzleEval).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot chain-of-thought; no external symbolic solvers. PuzzleEval and error taxonomy used to analyze reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact-match accuracy: Llama-2-70B solved 2 puzzles out of 274 (2 / 274) versus 1 / 274 for 13B. PuzzleEval ACS improved by ~11% on average relative to the 13B variant (exact ACS numbers for 70B in Table 6 of the appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No specific probing of spatial reasoning; only marginal gains from scaling suggest increased parameters alone do not confer the needed structured reasoning to solve grid puzzles reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to Llama-2-13B (scaling experiment) and to closed-source models; still substantially behind GPT-4/Gemini/Claude in final accuracy and PuzzleEval performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Marginal improvement despite large parameter increase; still far lower performance than best closed-source models; indicates scaling alone insufficient to solve these logical grid puzzles reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8328.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8328.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B-parameter LLM (Mistral family) evaluated on GridPuzzle and found to perform worst among the evaluated models in PuzzleEval and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM (7B parameters) used in April 2024 experiments for zero-shot-CoT evaluation on the dataset; treated as a smaller, instruction-tuned open model in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GridPuzzle (logic grid puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzle (2D relational assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot-CoT prompting with instruction + puzzle; model asked to generate step-by-step reasoning and final answer table; temperature 0; inference on A100 GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot chain-of-thought; evaluated using exact-match accuracy and PuzzleEval ACS; reasoning chain error analysis done via manual annotation and the GPT-4o Auto-evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PuzzleEval ACS: 0.27 average across grid sizes (Table 4: 3x4=0.29, 3x5=0.26, 4x4=0.27, 4x5=0.26, 4x6=0.27). Exact-match accuracy not enumerated explicitly but described as worst-performing with essentially failing to solve puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No evidence of explicit spatial reasoning; very low PuzzleEval and accuracy indicate weak relational reasoning on grid tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to larger open-source (Llama-2-13B/70B) and closed-source models; performed worst in both PuzzleEval and accuracy metrics among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Very low ACS and near-zero accuracy; authors attribute poor performance to small model size and weak instruction-following capability; prompting mitigation strategies did not rescue performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to automatically solve logic grid puzzles <em>(Rating: 2)</em></li>
                <li>Puzzle solving using reasoning of large language models: A survey <em>(Rating: 2)</em></li>
                <li>Puzzle solving without search or human knowledge: An unnatural language approach <em>(Rating: 2)</em></li>
                <li>Leveraging large language models to generate answer set programs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8328",
    "paper_id": "paper-271329041",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 / GPT-4-Turbo (OpenAI)",
            "brief_description": "A closed-source large language model from OpenAI evaluated in this study for step-by-step reasoning on logic grid puzzles using zero-shot chain-of-thought prompting; used as both a solver (GPT-4-Turbo) and as the basis for the PuzzleEval/auto-evaluator (GPT-4o was used as evaluator).",
            "citation_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
            "mention_or_use": "use",
            "model_name": "GPT-4 / GPT-4-Turbo",
            "model_description": "Closed-source transformer-based LLM from OpenAI; used in April 2024 model releases for deterministic, zero-temperature inference in this study. The paper does not provide architecture or training-data specifics beyond being a closed-source high-capability LLM.",
            "model_size": null,
            "puzzle_name": "GridPuzzle (logic grid puzzles)",
            "puzzle_type": "Grid-based logic puzzle (2D assignment grid; relational matching across rows/columns)",
            "task_setup": "Zero-shot Chain-of-Thought (Zero-shot-CoT) prompting: natural language instruction plus puzzle text; model prompted to produce a step-by-step reasoning chain and final answer table; temperature set to 0 for deterministic output; final answers compared via exact-match table EM against gold solution.",
            "mechanisms_or_strategies": "Zero-shot chain-of-thought step-by-step reasoning; produced multi-step textual reasoning chains. The authors also used GPT-4o as an auto-evaluator to label steps and compute PuzzleEval; various prompting mitigation strategies (Plan-and-Solve, Self-correct, Self-discover, Feedback-Learning, Program-of-Thought) were evaluated with GPT-4-Turbo but provided little improvement.",
            "performance_metrics": "Accuracy (exact-match of final answer table): GPT-4 achieved the highest accuracy in the study at 5.11% (14 / 274 puzzles). PuzzleEval Average Correctness Score (ACS): 0.59 average across grid sizes (per Table 4: 3x4=0.61, 3x5=0.62, 4x4=0.56, 4x5=0.54, 4x6=0.60).",
            "evidence_of_spatial_reasoning": "No direct evidence or probing of geometric/3D spatial reasoning was performed. The paper focuses on logical relational reasoning in a 2D grid (assignment relations); analyses evaluate correctness and error types of reasoning steps rather than diagnosing spatial representations. Thus there is no explicit evidence that GPT-4 is using spatial reasoning beyond managing relational/grid assignments in text.",
            "comparisons": "Compared against Claude-3, Gemini-Pro, Llama-2-13B, Mistral-7B and Llama-2-70B in the same Zero-shot-CoT setup; GPT-4 had the highest accuracy but PuzzleEval scores were similar to Gemini. Authors also compared behavior when applying mitigation prompting strategies and found limited improvements.",
            "limitations_or_failure_cases": "Despite relatively higher PuzzleEval ACS, GPT-4 still produced low final-answer accuracy (5.11%); errors frequently occur late in reasoning chains (error propagation, wrong elimination, wrong reasoning). Many steps are classified as reiteration (NC). Prompting-based mitigation strategies did not substantially improve performance. No specific geometric/spatial failure analyses were performed because the task is primarily relational grid logic.",
            "uuid": "e8328.0",
            "source_info": {
                "paper_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Claude-3",
            "name_full": "Claude-3 (Anthropic) - family includes Opus, Sonnet, Haiku",
            "brief_description": "A closed-source LLM (Anthropic Claude-3 family) evaluated on GridPuzzle using zero-shot chain-of-thought prompting to produce stepwise reasoning and final answers.",
            "citation_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
            "mention_or_use": "use",
            "model_name": "Claude-3 (Opus variant referenced)",
            "model_description": "Closed-source LLM family from Anthropic; treated in the paper as a high-capability model used in April 2024 releases for deterministic inference (temperature 0) in zero-shot-CoT prompts. No architecture/parameter details provided in the paper.",
            "model_size": null,
            "puzzle_name": "GridPuzzle (logic grid puzzles)",
            "puzzle_type": "Grid-based logic puzzle (2D relational assignment)",
            "task_setup": "Zero-shot-CoT: prompt with instruction + puzzle, request step-by-step reasoning and final answer table; temperature 0, deterministic inference; outputs evaluated by Exact Match accuracy and PuzzleEval ACS.",
            "mechanisms_or_strategies": "Zero-shot chain-of-thought stepwise textual reasoning. Also evaluated via PuzzleEval (GPT-4o) and via the Auto-evaluator taxonomy for step-wise error categorization.",
            "performance_metrics": "PuzzleEval Average Correctness Score (ACS): 0.54 average across grid sizes (per Table 4: 3x4=0.60, 3x5=0.56, 4x4=0.52, 4x5=0.55, 4x6=0.46). Exact-match accuracy value is not explicitly enumerated in the text for Claude-3, but closed-source models performed marginally better than open-source ones.",
            "evidence_of_spatial_reasoning": "No explicit analyses probing spatial reasoning (geometric or 3D) were performed. All evaluation concerns logical relational reasoning in grid puzzles.",
            "comparisons": "Compared to GPT-4, Gemini-Pro, Llama-2, Mistral; shows intermediate PuzzleEval performance (worse than GPT-4/Gemini in ACS but better than small open-source models).",
            "limitations_or_failure_cases": "Exhibits the same broad failure patterns: many steps are NC (reiteration), errors cluster in later steps (error propagation, wrong elimination). Prompt-based mitigation strategies did not significantly improve final accuracy on GridPuzzle.",
            "uuid": "e8328.1",
            "source_info": {
                "paper_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Gemini-Pro",
            "name_full": "Gemini-Pro (Google)",
            "brief_description": "A closed-source high-capability LLM from Google evaluated on GridPuzzle under zero-shot-CoT; produced relatively high PuzzleEval ACS but lower final exact-match accuracy comparable to GPT-4.",
            "citation_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
            "mention_or_use": "use",
            "model_name": "Gemini-Pro",
            "model_description": "Closed-source LLM from Google used in April 2024 model releases for deterministic inference in this study; the paper does not provide architecture or parameter counts for Gemini-Pro.",
            "model_size": null,
            "puzzle_name": "GridPuzzle (logic grid puzzles)",
            "puzzle_type": "Grid-based logic puzzle (2D relational assignment)",
            "task_setup": "Zero-shot chain-of-thought prompting: instruction + puzzle text; model asked to produce step-by-step reasoning chain and final answer table; temperature=0. Outputs evaluated by Exact Match and PuzzleEval.",
            "mechanisms_or_strategies": "Zero-shot-CoT; produced shorter reasoning chains on average relative to GPT-4 (average steps reported ~14.91 for Gemini vs ~20.66 for GPT-4), which authors note may reduce opportunities for errors.",
            "performance_metrics": "PuzzleEval ACS: 0.58 average across grid sizes (per Table 4: 3x4=0.60, 3x5=0.64, 4x4=0.54, 4x5=0.52, 4x6=0.62). Exact-match accuracy is not explicitly listed numerically in the main text (only that GPT-4 had the highest at 5.11%); Gemini's accuracy is described as marginally lower than GPT-4.",
            "evidence_of_spatial_reasoning": "No direct spatial-reasoning probes; analysis focuses on logical correctness of stepwise deductions rather than spatial/geometric representations.",
            "comparisons": "Compared to GPT-4 and Claude-3; similar PuzzleEval ACS to GPT-4 though with notably fewer reasoning steps. Closed-source models (including Gemini) outperform open-source ones.",
            "limitations_or_failure_cases": "Shorter chains may hide errors (fewer steps); still low final-answer accuracy due to late-chain errors (wrong elimination, error propagation). Prompt mitigation strategies yielded only marginal gains (Self-Discover gave small improvement).",
            "uuid": "e8328.2",
            "source_info": {
                "paper_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Llama-2-13B",
            "name_full": "Llama-2-13B (Meta)",
            "brief_description": "An open-source LLM (Llama-2 family, 13B parameter variant) evaluated on GridPuzzle; performance was poor, solving very few puzzles and showing low PuzzleEval ACS.",
            "citation_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
            "mention_or_use": "use",
            "model_name": "Llama-2-13B",
            "model_description": "Open-source transformer-based LLM (13B parameters) from Meta used in the study for deterministic zero-shot-CoT inference; authors ran inference on NVIDIA A100 GPUs. Paper does not describe training data beyond model name.",
            "model_size": "13B",
            "puzzle_name": "GridPuzzle (logic grid puzzles)",
            "puzzle_type": "Grid-based logic puzzle (2D relational assignment)",
            "task_setup": "Zero-shot-CoT prompting: instruction + puzzle text; model asked to output step-by-step reasoning and final answer table; temperature 0. Inference on A100 GPUs.",
            "mechanisms_or_strategies": "Zero-shot chain-of-thought; no external symbolic solvers used. Authors also ran a scaling experiment to Llama-2-70B for comparison.",
            "performance_metrics": "Exact-match accuracy: Llama-2-13B solved only 1 puzzle correctly out of 274 (1 / 274). PuzzleEval ACS: average 0.41 across grid sizes (Table 4: 3x4=0.45, 3x5=0.46, 4x4=0.46, 4x5=0.42, 4x6=0.28).",
            "evidence_of_spatial_reasoning": "No explicit evidence of spatial reasoning; performance indicates limited reasoning skill on relational grid tasks. Authors note Llama-2 and other open-source models lack reasoning skills compared to closed-source models.",
            "comparisons": "Directly compared to closed-source GPT-4, Claude-3, Gemini and open-source Mistral; clear gap with closed-source models. Scaling to the 70B Llama improved performance slightly (see separate entry).",
            "limitations_or_failure_cases": "Consistently poor accuracy and lower PuzzleEval ACS that degrades with increased grid size; low instruction-following capabilities and parameter size cited as contributors to poor performance.",
            "uuid": "e8328.3",
            "source_info": {
                "paper_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Llama-2-70B",
            "name_full": "Llama-2-70B (Meta)",
            "brief_description": "A larger open-source variant of Llama-2 (70B parameters) evaluated in a scaling experiment that showed only marginal improvement over the 13B variant on GridPuzzle.",
            "citation_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
            "mention_or_use": "use",
            "model_name": "Llama-2-70B",
            "model_description": "Open-source transformer-based LLM (70B parameters) from Meta; evaluated by the authors in a zero-shot-CoT setting to measure scaling effects relative to the 13B variant.",
            "model_size": "70B",
            "puzzle_name": "GridPuzzle (logic grid puzzles)",
            "puzzle_type": "Grid-based logic puzzle (2D relational assignment)",
            "task_setup": "Zero-shot-CoT prompting identical to other models; scaling experiment used same dataset and evaluation (Exact Match & PuzzleEval).",
            "mechanisms_or_strategies": "Zero-shot chain-of-thought; no external symbolic solvers. PuzzleEval and error taxonomy used to analyze reasoning chains.",
            "performance_metrics": "Exact-match accuracy: Llama-2-70B solved 2 puzzles out of 274 (2 / 274) versus 1 / 274 for 13B. PuzzleEval ACS improved by ~11% on average relative to the 13B variant (exact ACS numbers for 70B in Table 6 of the appendix).",
            "evidence_of_spatial_reasoning": "No specific probing of spatial reasoning; only marginal gains from scaling suggest increased parameters alone do not confer the needed structured reasoning to solve grid puzzles reliably.",
            "comparisons": "Compared directly to Llama-2-13B (scaling experiment) and to closed-source models; still substantially behind GPT-4/Gemini/Claude in final accuracy and PuzzleEval performance.",
            "limitations_or_failure_cases": "Marginal improvement despite large parameter increase; still far lower performance than best closed-source models; indicates scaling alone insufficient to solve these logical grid puzzles reliably.",
            "uuid": "e8328.4",
            "source_info": {
                "paper_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral-7B-Instruct",
            "brief_description": "An open-source 7B-parameter LLM (Mistral family) evaluated on GridPuzzle and found to perform worst among the evaluated models in PuzzleEval and accuracy.",
            "citation_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct",
            "model_description": "Open-source LLM (7B parameters) used in April 2024 experiments for zero-shot-CoT evaluation on the dataset; treated as a smaller, instruction-tuned open model in the study.",
            "model_size": "7B",
            "puzzle_name": "GridPuzzle (logic grid puzzles)",
            "puzzle_type": "Grid-based logic puzzle (2D relational assignment)",
            "task_setup": "Zero-shot-CoT prompting with instruction + puzzle; model asked to generate step-by-step reasoning and final answer table; temperature 0; inference on A100 GPUs.",
            "mechanisms_or_strategies": "Zero-shot chain-of-thought; evaluated using exact-match accuracy and PuzzleEval ACS; reasoning chain error analysis done via manual annotation and the GPT-4o Auto-evaluator.",
            "performance_metrics": "PuzzleEval ACS: 0.27 average across grid sizes (Table 4: 3x4=0.29, 3x5=0.26, 4x4=0.27, 4x5=0.26, 4x6=0.27). Exact-match accuracy not enumerated explicitly but described as worst-performing with essentially failing to solve puzzles.",
            "evidence_of_spatial_reasoning": "No evidence of explicit spatial reasoning; very low PuzzleEval and accuracy indicate weak relational reasoning on grid tasks.",
            "comparisons": "Compared to larger open-source (Llama-2-13B/70B) and closed-source models; performed worst in both PuzzleEval and accuracy metrics among evaluated models.",
            "limitations_or_failure_cases": "Very low ACS and near-zero accuracy; authors attribute poor performance to small model size and weak instruction-following capability; prompting mitigation strategies did not rescue performance.",
            "uuid": "e8328.5",
            "source_info": {
                "paper_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to automatically solve logic grid puzzles",
            "rating": 2,
            "sanitized_title": "learning_to_automatically_solve_logic_grid_puzzles"
        },
        {
            "paper_title": "Puzzle solving using reasoning of large language models: A survey",
            "rating": 2,
            "sanitized_title": "puzzle_solving_using_reasoning_of_large_language_models_a_survey"
        },
        {
            "paper_title": "Puzzle solving without search or human knowledge: An unnatural language approach",
            "rating": 2,
            "sanitized_title": "puzzle_solving_without_search_or_human_knowledge_an_unnatural_language_approach"
        },
        {
            "paper_title": "Leveraging large language models to generate answer set programs",
            "rating": 1,
            "sanitized_title": "leveraging_large_language_models_to_generate_answer_set_programs"
        }
    ],
    "cost": 0.016312999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?
4 Oct 2024</p>
<p>Nemika Tyagi ntyagi8@asu.edu 
Arizona State University</p>
<p>Mihir Parmar mparmar3@asu.edu 
Arizona State University</p>
<p>Mohith Kulkarni 
Arizona State University</p>
<p>Aswin Rrv 
Arizona State University</p>
<p>Nisarg Patel 
Arizona State University</p>
<p>Mutsumi Nakamura 
Arizona State University</p>
<p>Arindam Mitra 
Arizona State University</p>
<p>Chitta Baral chitta@asu.edu 
Arizona State University</p>
<p>Raymond 
Arizona State University</p>
<p>Stuart 
Arizona State University</p>
<p>Wesley Tom 
Arizona State University</p>
<p>Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?
4 Oct 202478C09A2091C87337B7984DC7C8B0AF1BarXiv:2407.14790v2[cs.CL]cavemandairy-freegluten-freevegan
Solving grid puzzles involves a significant amount of logical reasoning.Hence, it is a good domain to evaluate the reasoning capability of a model which can then guide us to improve the reasoning ability of models.However, most existing works evaluate only the final predicted answer of a puzzle, without delving into an indepth analysis of the LLMs' reasoning chains (such as where they falter) or providing any finer metrics to evaluate them.Since LLMs may rely on simple heuristics or artifacts to predict the final answer, it is crucial to evaluate the generated reasoning chain beyond overall correctness measures, for accurately evaluating the reasoning abilities of LLMs.To this end, we first develop GridPuzzle, an evaluation dataset comprising 274 grid-based puzzles with different complexities.Second, we propose a new error taxonomy derived from manual analysis of reasoning chains from LLMs including GPT-4, Claude-3, Gemini, Mistral, and Llama-2.Then, we develop an LLM-based framework for large-scale subjective evaluation (i.e., identifying errors) and an objective metric, PuzzleEval, to evaluate the correctness of reasoning chains.Evaluating reasoning chains from LLMs leads to several interesting findings.We further show that existing prompting methods used for enhancing models' reasoning abilities do not improve performance on Grid-Puzzle.This highlights the importance of understanding fine-grained errors and presents a challenge for future research to enhance LLMs' puzzle-solving abilities by developing methods that address these errors 1 .</p>
<p>Introduction</p>
<p>Recent advancements in LLMs such as GPT-4, Gemini, Claude-3 (Anthropic, 2024), Llama-2 (Touvron et al., 2023), andMistral (Jiang et al., 2023) have achieved remarkable performance on a wide range of Natural Language Understanding (NLU) tasks previously thought to be exclusive to humans.Beyond NLU, exploring LLMs' logical reasoning abilities (Liu et al., 2021;Saparov and He, 2022;Parmar et al., 2024;Patel et al., 2024) on complex tasks such as puzzle-solving is underexplored.Past attempts have been made to evaluate models on logic-intensive grid-based puzzlesolving.However, they either do not focus on evaluating LLMs (Mitra and Baral, 2015;Jabrayilzade and Tekir, 2020) or do not evaluate LLMs independently, but rather use neuro-symbolic approaches (Ishay et al., 2023) that use external specialized solvers on LLM outputs.Here, we aim to evaluate the puzzle-solving abilities of LLMs by themselves, without the use of any external logic solvers.</p>
<p>To understand the reasoning capabilities of LLMs, it is important to evaluate reasoning chains, rather than the final predicted answer.There have been works that evaluate reasoning chains using objective metrics such as ROSCOE (Golovneva et al., 2022), CTC (Deng et al., 2021), and BARTScore (Yuan et al., 2021), however, they do not focus specifically on evaluating reasoning.Some prior works propose metrics for specific reasoning tasks, such as FOLIO (Han et al., 2022) and ProntoQA (Saparov and He, 2022).However, these methods rely on reference-based evaluation, do not focus on puzzle-solving, and do not aim to identify finegrained errors in reasoning chains.To address these limitations, we propose a reference-free manual and automated subjective evaluation of reasoning chains to understand various fine-grained errors in reasoning chains for grid-based puzzle-solving.</p>
<p>Motivated by Mitra and Baral (2015), we first develop GridPuzzle (Figure 1), a comprehensive evaluation dataset consisting of grid-based puzzles with grid-size of 3 × 4, 3 × 5, 4 × 4, 4 × 5, and 4 × 6 with three levels of difficulty (easy, medium, and hard).Then, we evaluate LLMs including GPT-1.Stuart lost 4 more pounds than the person who used the vegan diet.2. The dieter who used the gluten-free diet lost 2 more pounds than Raymond.3. Stuart lost 7 lbs.4. The dieter who lost 7 lbs is either the person who used the caveman diet or Raymond. 5. Wesley lost 5 lbs.</p>
<p>Step-by-step solution: [Your steps show how you are solving the puzzle] Inst.</p>
<p>Auto-Evaluated Reasoning Chains { ""Sentence"": ""Clue 3 says Stuart lost 7 lbs."",""Premise"": ""Clue 3"", ""Conclusion"": ""Stuart lost 7 lbs."",""Explanation"": ""Reiteration of the clue."",""Error Category"": ""NC"", ""Sub Category"": ""-"" }, { ""Sentence"": ""Clue 4 specifies that the dieter who lost 7 lbs is either the person who used the caveman diet or Raymond."",""Premise"": ""Clue 4"", ""Conclusion"": ""Stuart must be on the caveman diet or be Raymond."",""Explanation"": ""That is the right reasoning from the given clue."",""Error Category"": ""RR"", ""Sub Category"": ""-"" },...... 4, Gemini-Pro, Claude-3, Llama-2, and Mistral on GridPuzzle in zero-shot-CoT setting (Figure 1).Experimental results show that LLMs do not fare well and achieve a maximum of 5.1% accuracy.</p>
<p>Puzzle</p>
<p>To investigate the reasoning chains, we manually analyze them (Figure 1) to find fine-grained errors (further details in section 3.3).Based on this, we propose a new error taxonomy comprising five broad categories, and nine fine-grained subcategories (Tables 1 and 2), providing deeper insights into the primary causes of the LLMs' reasoning failures.However, scaling manual analysis to a larger set is time-consuming and laborious.Hence, we propose to leverage LLMs as auto-evaluators by creating prompts that utilize error taxonomy to automate the analysis of reasoning chains and help in identifying errors (Figure 1).While evaluating w.r.t.manual annotation, our auto-evaluator model achieves ∼ 86% agreement, hence providing quality error categorization.</p>
<p>Beyond identifying errors and the accuracy of the final answer, we propose PuzzleEval, an LLMbased framework to evaluate reasoning chains for grid-based puzzles.PuzzleEval involves a multi-stage evaluation using GPT-4o.First, we identify key logical conclusions from the reasoning chain; second, we extract key logical concepts from these conclusions; and finally, we measure the presence of these logical concepts in the final gold answer to assess the correctness of the reasoning chain.Evaluating reasoning chains based on error categorization and PuzzleEval reveals interesting findings such as LLMs show lower accuracy despite having more error-free reasoning steps, open-source models lack reasoning skills compared to closed-source models, and the most dominant error categories are wrong reasoning and elimination.Additionally, we employ existing prompting methods such as Plan-and-Solve and Self-discover, demonstrating that these methods do not improve performance on GridPuzzle.We believe that our findings will inspire future work in the automated in-depth evaluation of reasoning chains for broader reasoning tasks and enhance the reasoning abilities of models.</p>
<p>Related Work</p>
<p>Puzzle-solving Task Puzzle-solving task provides insights into LLMs' logical reasoning.Gi-adikiaroglou et al. (2024) categorize puzzles into (1) rule-based and (2) rule-less puzzles.Rule-less puzzles include riddles (Lin et al., 2021), MCQs (Zhao and Anderson, 2023), programming puzzles (Schuster et al., 2021), and commonsense reasoning puzzles (Gu et al., 2023); however, in our work we focus on rule-based puzzles.In rule-based puzzles, past attempts have explored Sudoku (Noever and Burdick, 2021), Rubik's Cube, 8-puzzle, Game of 24 (Yao et al., 2024), crosswords (Yao et al., 2024), chess puzzles (Feng et al., 2024), card games (Gupta, 2023), BoardgameQA (Kazemi et al., 2024), and Lateral Thinking Puzzles (Huang et al., 2024).However, grid-based puzzle solving is under-explored.Mitra and Baral (2015) proposed a grid-based puzzle dataset and Dziri et al. (2023) studied compositionality in LLMs using Grid Puzzle, but these works do not provide any insights into the performance of recent LLMs.Motivated by this, we propose a systematically curated gridbased puzzle dataset, GridPuzzle, and provide an evaluation of various LLMs in puzzle-solving.</p>
<p>Automatic Evaluation of Reasoning Chains</p>
<p>Previous works (Dalvi et al., 2021;Saparov and He, 2022;Han et al., 2022) have focused on referencefree evaluation, which is not reliant on goldreasoning chains.Recently, ROSCOE (Golovneva et al., 2022) proposed a suite of metrics to measure the semantic consistency, logicality, informativeness, fluency, and factuality of reasoning chains, while the ReCEval framework (Prasad et al., 2023) evaluates reasoning chains based on two key properties: correctness and informativeness.Recent evaluation methods such as LLM evaluation (Chiang and Lee, 2023) and G-Eval (Liu et al., 2023) leverage LLMs to measure the quality of reasoning chains.LLM evaluation involves presenting task instructions and a text sample to LLMs, asking them to rate the sample's quality on a 5-point Likert scale, whereas the latter incorporates automatic chain-of-thought generated by the LLM describing the detailed evaluation steps.Additionally, Tyen et al. (2023)'s attempt to use GPT-4 as an evaluator in a few-shot setting, shows that evaluating reasoning chains remains a challenge.Furthermore, Au-toRace (Automatic Reasoning Chain Evaluation) (Hao et al., 2024) proposed a fully automated approach for evaluating reasoning chains that adapt to different tasks without human effort.However, these methods do not evaluate reasoning chains at the level of fine-grained error types and do not pro-vide detailed task-specific insights.To address this, we propose LLM-based reference-free evaluation methods that identify fine-grained errors and assess the correctness of generated reasoning chains.</p>
<p>Evaluation of Reasoning Chains</p>
<p>GridPuzzle</p>
<p>To develop this dataset, we extract logic grid puzzles of various grid sizes from Puzzle Baron's Logic Puzzles2 .Specifically, we compile logic grid puzzles of size 3 × 4, 3 × 5, 4 × 4, 4 × 5, and 4 × 6.Each grid size has three levels of difficulty (easy, medium, and hard) except 4 × 6.This particular grid size has only two difficulty levels (Easy and Medium).Statistics corresponding to each grid size are presented in Figure 1</p>
<p>Manual Evaluation</p>
<p>To explore where exactly these LLMs falter in performing reasoning, we conduct a detailed manual analysis of the reasoning chains generated by them while solving grid-based puzzles.Details of the annotation guidelines provided to the human evaluators are given in the Appendix D. Our manual analysis process consists of three steps.First, we begin by segmenting the reasoning chains into individual sentences, allowing us to categorize errors more precisely.Second, we identify the premise and conclusion for each sentence and determine their respective correctness.We refrain from subdividing sentences into multiple premises or conclusions to maintain simplicity for finding errors.At last, each sentence is categorized as either containing a single premise and conclusion or being a declarative statement without a conclusion.Then, we begin assessing potential issues or errors in the reasoning chains.Now, we follow an exhaustive (2) Incomplete Information Lacks necessary information to make a particular conclusion.</p>
<p>(3) Assumptions Statements not derived from clues directly; might include assumed information relevant to the clue.</p>
<p>Derived Conclusions using clues given in puzzle which was not inherently given in the clues.</p>
<p>(4) Error Propagation Premise derived from a previous incorrect conclusion.</p>
<p>(5) Incomplete Information Lacks necessary information to make a particular conclusion.</p>
<p>(6) Wrong Assumption The derived assumption is incorrect.</p>
<p>Wrong Conclusion</p>
<p>Derived using the premise (which itself is either taken directly from the clues or derived)</p>
<p>(a) Wrong Reasoning The reasoning is incorrect, regardless of the premise's accuracy.</p>
<p>(b) Error propagation Conclusion is incorrect due to an erroneous premise.</p>
<p>(c) Wrong Elimination All premises are present, but not all conclusions are correctly derived.</p>
<p>Table 2: Proposed error taxonomy for sub-categories based on manual analysis.These sub-categories are defined for cases where either the conclusion or premise is incorrect ("RW" or "WR") or both are incorrect ("WW").For "WW", the error sub-categories might appear in any combinations between (1)(2)(3)(4)(5)(6) and (a-c) such as '1a', '4b', or '6c'.</p>
<p>approach to create fine-grained error categories.</p>
<p>We begin with 30 reasoning chains (6 puzzles x 5 reasoning chains from LLMs) to manually identify potential errors.Next, we categorize these errors in a structured format.We then add another 30 reasoning chains to see if any new types of errors emerge.If new errors are identified, we refine our categories accordingly.This process is repeated until we evaluate a total of 150 reasoning chains and no new types of errors are found.Based on this method, we have carefully filtered and categorized several errors made by LLMs, presenting them as five broad categories and nine sub-categories.</p>
<p>Proposed Error Taxonomy</p>
<p>Broad Categories As shown in For instance, the category "WW" comprises sentences where the sentence consists of a wrong premise as well as a wrong conclusion.Interestingly, we also find the "WR" category consists of instances where a wrong premise still leads to a correct conclusion.Additionally, sentences containing only information from clues or premises from previous steps fall under "NC".We conduct further investigation as to why the premises and conclusions become incorrect.</p>
<p>Sub-categories: Wrong Premise As shown in Table 2, we identified the source of the premise to determine the origin of errors: (i) 'From Clues'where the premise is directly borrowed from one of the clues without any further reasoning, and (ii) 'Derived' -where the premise is inferred from either the clues or the previous conclusions.From Table 2, there are six possible reasons associated with two different sources for the wrong premise.</p>
<p>When the premise originates from the source (i), we find three types of errors: Hallucination -When some factual information from the clues is distorted or completely made up; Incomplete information -When the information is correctly borrowed from the clues but it is not sufficient to make a particular conclusion; Assumptions -This is a special category where the premise is not derived but also not given exactly in the clues.It is often related to one of the clues and is of the form, "Let's assume" or "Assuming that."When source is derived, we find three different errors: Error Propagation -This occurs when a previously incorrect conclusion becomes the basis for a flawed premise, thereby extending the error from one conclusion to the next; Incomplete information -When the derived premise is not sufficient to make a particular conclusion; and lastly, Wrong Assumption -When the LLM reasoner clearly states that a premise was an assumption but it was incorrectly derived.</p>
<p>Sub-categories: Wrong Conclusion</p>
<p>As shown in Table 2 (source), conclusions are always logically derived from a fixed set of premises.For having a wrong conclusion in any reasoning step, we find 3 errors responsible: Error Propagation -When a conclusion is wrong strictly due to some error in the preceding premise; Wrong Elimination -When the conclusion is wrong because the LLM reasoner failed to eliminate all the unfit choices correctly.This case is specific to the grid-based puzzle task but is inherently an erroneous deduction on the</p>
<p>Examples of reasoning chain evaluated by GPT-4o</p>
<p>Sentence:"Therefore, Zeno must be 69%, and UCLA must be 62%.",Premise: "If Zeno were 55%, there would be no score 7% lower than 55% for UCLA.",Conclusion: "Zeno must be 69%, and UCLA must be 62%", Explanation: "The conclusion is incorrect as UCLA is already known to be 62% from clue 2.", Error Category: "RW.",Premise: "A", Sentence:"Since the third performer used flashlights, it must be either Lora or Carmen.",Premise: "The performer who used flashlights was either Lora or Carmen.",Conclusion: "The third performer must be either Lora or Carmen.",Explanation: "The conclusion is based on the incorrect premise that the third performer used flashlights.",Error Category: "WW", Sub Category "4B", Table 3: Examples of reasoning chain evaluated by Auto-evaluator (GPT-4o).</p>
<p>LLM's end; Wrong reasoning -The remaining incorrect conclusions that did not fit in the above categories are classified under this label.</p>
<p>Automated Evaluation</p>
<p>Manual analysis of reasoning chains provides a detailed categorization of errors; however, it is tedious and, therefore, challenging to scale for the entire dataset.However, analyzing the distribution of errors from our proposed taxonomy on the whole dataset is also crucial in understanding the shortcomings of LLMs' reasoning ability.Thus we develop an LLM-based auto-evaluator to automate the process of error evaluation.To this end, we prompt the GPT-4o model to identify and categorize errors in the given reasoning chain.Our prompt consists of a system instruction followed by a user prompt containing the reasoning chain to be evaluated along with the original puzzle and its gold solution.The system prompt can be further dissected into 3 key components: the instructions, the knowledge, and an exemplar.The instruction contains all the rules that the GPT-4o needs to follow to conduct accurate evaluation and error categorization of the reasoning chains.It incorporates similar sequential steps used during the manual evaluation of reasoning chains along with the required output format.The knowledge has a detailed description of our error taxonomy including the broad and sub-categories.We also provide a preference order for selecting categories along with the description to minimize any ambiguity in the evaluation process.Lastly, the exemplar consists of a puzzle, its correct solution, the original model-produced reasoning chain, and the manually evaluated reasoning chain with our error categories.We termed this LLM-based evaluator as "Auto-evaluator".Appendix B provides the structure of the Auto-evaluator prompt.</p>
<p>Using the Auto-evaluator, we evaluated a total of 1,370 reasoning chains generated by five different LLMs for solving 274 puzzles.The application of our Auto-evaluator to this large dataset allowed us to analyze the distribution of error categories on a broader scale.To validate the accuracy of the evaluations performed by the Auto-evaluator, we randomly sampled 20 reasoning chains from the manually evaluated set.The authors then compared their error category assignments to those given by the Auto-evaluator.The agreement score for the total number of reasoning steps between the manual evaluation and the GPT-4o evaluation is ∼ 86%.Table 3 shows the example of reasoning steps evaluated by GPT-4o.</p>
<p>Experimental Setup</p>
<p>Experiments</p>
<p>We evaluate a range of closed-source LLMs including GPT-4-Turbo, Claude-3-Opus, and Gemini-Pro, and open-source models Llama-2-13B, and Mistral-7B-Instruct on GridPuzzle in the Zero-shot-CoT setting (Kojima et al., 2022).We also conducted a scaling experiment on Llama-2-70B and the results are given in the Appendix F. Our GridPuzzle dataset consists of a set of instances denoted as P = &lt; p i×j n , a n &gt;, where p i×j n is n th puzzle instance with grid size of i × j and a n as a gold answer.We prompt each LLM to generate a reasoning chain before predicting answer â.To evaluate each model in the Zero-shot-CoT setting, we provide &lt; I, p i×j n &gt; as input to the model and predict an answer â where I is a natural language instruction.The evaluation is conducted on the OpenAI, Google, and Anthropic model versions released in April 2024 with temperature setting 0 for deterministic predictions.NVIDIA A100 GPUs are used for conducting the inference of open-source models with a batch size of 4. The example prompts used for these experiments are provided in Appendix A.</p>
<p>Reasoning Chain Sequential Labeling</p>
<p>Step 1. From clue 1, we... Step 2. Since Stuart is the... . .</p>
<p>Step n.So the remaining...</p>
<p>Final Conclusion</p>
<p>Step 1.The vegan is assig...</p>
<p>Step 2. Tom is the dairy... . .</p>
<p>Step n.The last diet is...</p>
<p>Average Correctness</p>
<p>The Average Correctness = 0.235</p>
<p>Gold solution</p>
<p>Figure 2: The process of calculating PuzzleEval metrics is described above.The reasoning chains are produced by our five LLMs and the gold solution is taken from our GridPuzzle dataset.</p>
<p>Metrics</p>
<p>Accuracy We use accuracy to demonstrate the capability of LLMs in solving grid-based puzzles based on their ability to predict the final answer.To calculate this metric, we use the LLM-generated final answers and compare them with the available gold solution.The predicted answers and the gold solution are in the form of tables with the number of rows and columns equal to the grid size of the puzzle.We perform an Exact Match (EM) to compare the two tables and mark them as correct only when all the entries of the tables match.See the example of the final answer table in Appendix C.</p>
<p>PuzzleEval We developed this LLM-based metric to assess step-by-step reasoning chains and provide a correctness score for each step, as well as the Average Correctness Score (ACS) for the entire chain.PuzzleEval is a reference-free metric specifically designed for assessing reasoning chains generated for grid-based puzzle tasks.It evaluates the correctness of each step in the reasoning chain and reports the score using only the final answer table provided as the gold solution, without requiring any comparison to a gold reasoning chain.As shown in Figure 2, PuzzleEval consists of a three-stage pipeline to evaluate any reasoning chain.First, we prompt GPT-4o to label all the steps sequentially to account for any discrepancies in the different formats of reasoning chains produced by various models, and to extract only the final conclusions from each step.This stage is crucial as it removes the portion of a step where the models just reiterated clues or previous conclusions.Second, we instruct the model to extract the pair-wise relation of elements from the puzzle that have been either accepted or rejected in the extracted final conclusions.If the extracted conclusion is "Sam is assigned to the year 2015 but not 2014.",these pairs are of the form "Sam -2015" or "Sam -not 2014".Third, we provide the gold solution table and ask the model to check if these accepted or rejected pairs match the given information.As per the validation, the pairs extracted from every step are marked as correct or incorrect.After obtaining this information for each step the correctness score is calculated by adding up all the correct and incorrect steps (correct pairs are marked 1 and incorrect pairs are marked 0) divided by the total number of pairs in each step.Finally, the ACS is determined by adding up all the correctness scores from each step and dividing by the number of steps to capture the overall quality of the reasoning chain.Hence, PuzzleEval provides ACS for each reasoning chain in range of 0 to 1.</p>
<p>Results and Analysis</p>
<p>Objective Evaluation</p>
<p>To evaluate the performance of LLMs when solving grid-based puzzles, we assess the outputs of 5 LLMs using the accuracy and PuzzleEval.As shown in Figure 3, we found that all the models have low performance on the GridPuzzle dataset in terms of accuracy.The smaller open-source LLMs completely failed at the puzzle-solving task, with Llama-2 solving only one puzzle correctly.Closesource models with significantly larger parameter sizes also exhibited poor performance.GPT-4 had the highest accuracy at only 5.11% (14 puzzles out of 274).Despite the overall low performance of all LLMs, the closed-source models perform marginally better.We evaluate the quality of the reasoning chains using PuzzleEval.Table 4 provides the ACS for each grid size available in the GridPuzzle.Surprisingly, compared to the accuracy, the performance of the models with PuzzleEval was significantly better as shown in Table 4.</p>
<p>The ACS lie in the range of 0.26 to 0.64 across all grid sizes.This higher score can be attributed to the partial correctness of reasoning chains when solving the grid-puzzle task.The disparity between metrics shows that evaluating only final answers doesn't fully capture LLMs' effectiveness in complex logical tasks like grid puzzles.</p>
<p>With the increase in the sizes of the grids, the complexity of the puzzles also rises, leading to a depreciating performance by the LLMs with larger grids.Overall the performance of larger LLMs was much better than the small open-source models.Mistral-7B performed the worst in PuzzleE- val which is in accordance with its low accuracy score.GPT-4 and Gemini models surprisingly have similar PuzzleEval scores (0.59 and 0.58 respectively) despite their large difference in accuracy.This difference in PuzzleEval could be attributed to the relatively shorter reasoning chains (fewer reasoning steps) produced by Gemini (an average of 14.91 steps) compared to GPT-4 (an average of 20.66 steps).Shorter reasoning chains may reduce the number of errors that occur while solving the puzzle.It is interesting to note that the smaller LLMs have consistently low performance with the increase in the grid size of the puzzles but the larger LLMs have mixed performance.</p>
<p>Reasoning Chain Evaluation</p>
<p>The relative distribution of the broad error categories over the collective reasoning steps for each model is given in Figure 4.It is important to note that, despite using the same zero-shot-CoT setting, the GPT-4 and Llama-2 used significantly more reasoning steps (&gt; 5.5k steps) to solve the 274 puzzles compared to the other three models (∼ 4k steps).The distribution of error sub-categories for each model is presented as heatmaps in the first five sub-figures in Figure 5. Here, we present several findings based on the evaluation of different error category distributions across GridPuzzle.</p>
<p>Majority of reasoning steps are error-free.Figure 4 shows that most reasoning steps for each model fall into the "NC" error category, indicating that many steps reiterate the facts or clues from the initial puzzle rather than focusing on reasoning.Over 55% of Gemini-Pro's reasoning steps fall into this category, the most among all models, suggesting that Gemini spends the fewest steps on actual reasoning.The "RR" category comprises over 46% of GPT-4's reasoning steps, highlighting its strong reasoning ability.This higher number of correct reasoning steps correlates with GPT-4's higher Puz-zleEval score, reflecting its overall effectiveness.</p>
<p>The accuracy is low despite the reasoning chains being mostly error-free.The disparity between accuracy and PuzzleEval arises from the relative location of errors within the reasoning chains.It has been observed that "RR" category reasoning steps mainly occur in the initial half of the chain, leading to a high overall PuzzleEval score.Conversely, errors in the "RW", "WR", and "WW" categories typically occur in the latter half, resulting in incorrect final answers and lower accuracy scores.Based on error taxonomy, "RW", "WR", and "WW" broad error categories have been further dissected into 6 × 3 error sub-categories, with their distribution across reasoning steps shown in Figure 5.</p>
<p>Dominant broad categories: RW and WW.</p>
<p>The most common error sub-category across all heatmaps appears to be the "-", the absence of errors.All the reasoning steps with "NC" and "RR" classifications fall in this category.To observe the actual overall trend across all 5 LLMs, the top 10 most common error sub-categories have been listed in the last sub-figure of Figure 5.The top categories 'a' and 'c' refer to the Wrong Reasoning and the Wrong Elimination sub-categories under the "RW" category.These errors arise when the premise is correct but the LLMs fail to make accurate deductions from it.A number of the top 10 sub-error categories ('1a', '4a', '4b', '4c', '5a', '5c', and '6a') emerge from the "WW" category.</p>
<p>For the categories, '4a', '4b', and, '4c' the errors in the premise are propagated from errors in previous reasoning steps showing how initially occurring errors in the chain can lead to more dependent errors.The '4b' error category is the one where this behavior is maximized as here both the premise and conclusions were wrong because of previously propagated errors.The '5a' and '5c' errors occurred due to the incompleteness or lack of information in the premise and wrong reasoning or elimination in the corresponding conclusions.The '1a' kind of error occurred when the premise consisted of hallucinated information.The only sub-category from the "WR" category making it in the top 10 is the '3' category which is caused due to wrong assumptions in the premise.It can be noted here that the reasoning steps of the "WR" category do not deteriorate either of the evaluation metrics, as the conclusions ended up being correct, but rather indicate the inconsistency of the LLMs in reasoning over puzzle-solving.Proprietary LLMs are way better at GridPuzzle than Open-Source LLMs.From the results of objective and subjective metrics, it is evident that the open-source models have lower performance on the grid-puzzle-solving task than the proprietary models.The Llama-2 and Mistral models have the lowest accuracy values and their low performance on the PuzzleEval consistently degrades with the increase in the size and complexity of the grids.The Claude-3, Gemini, and GPT-4 models have higher values of accuracy but their performance across the grid sizes in the PuzzleEval is inconsistent.The disparity in the performance of both kinds of models can be attributed to the difference in their parameter sizes and the low instruction following capabilities of small open-source models.</p>
<p>Popular reasoning error mitigation strategies do not improve LLMs on GridPuzzle.We conduct a case study on a subset of GridPuzzle, focusing on a 3x4 grid size, utilizing commonly employed prompting techniques to enhance the reasoning capabilities of LLMs.In particular, we use five strategies: (1) Plan-and-Solve (Wang et al., 2023), (2) Self-correct (Zhang et al., 2024), (3) Self-discover (Zhou et al., 2024), (4) Feedback-Learning, and (5) Program of Thought prompting (Chen et al., 2023).</p>
<p>We updated the prompts corresponding to these techniques to include some of our major findings from the reasoning chain evaluations and error categorization analysis as precautionary instructions.</p>
<p>The first strategy Plan-and-Solve prompts the model to first generate a plan to solve the given problem and then follow those steps.The second strategy is inspired by the Self-correct method which uses a combination of self-verification and self-refine to improve reasoning.Next, we used the Self-discover technique which is a 2-step structured reasoning process.We created our prompting technique called "Feedback-Learning" by providing specific feedback system instructions to the LLM based on our error taxonomy.Lastly, we also implemented a code-style prompting technique that implements a code to solve the puzzle but does not give a reasoning chain.The detailed prompt structure is described in Appendix E and the results of these strategies are in Table 5.It is evident from the results that prompting-based strategies are not sufficient to significantly improve the LLM reasoning on the grid-puzzle-solving task.Compared to the rest of the strategies, Self-Discover marginally improves the performance on both accuracy and Puz-zleEval.These results indicate the need to develop techniques beyond prompting by having deeper insights from LLMs' reasoning chains.</p>
<p>Conclusion</p>
<p>In this work, we evaluated the logical reasoning abilities of LLMs through the lens of a grid-based puzzle-solving task.We introduced GridPuzzle, an evaluation dataset of 274 puzzles with various grid sizes.From a manual evaluation of reasoning chains generated by five different LLMs on GridPuzzle, we developed a fine-grained error taxonomy with five broad categories and nine subcategories.We then created an Auto-evaluator to automate the identification of error categories, providing broader insights into error distributions across the dataset.Additionally, we proposed Puz-zleEval, a reference-free metric to objectively evaluate the correctness of reasoning chains for gridbased puzzles.Our analysis of error distributions in GridPuzzle revealed several interesting findings and insights into the logical reasoning abilities of different LLMs.We further evaluated existing reasoning-specific prompting methods, such as self-discover and self-correct, finding that they do not improve results on GridPuzzle.We believe our work offers a challenging dataset, highlights where these LLMs make mistakes, and provides insights to develop better logical reasoning systems for complex tasks such as grid puzzle-solving.</p>
<p>Limitations</p>
<p>While GridPuzzle facilitates the evaluation of LLMs' logical reasoning abilities, the complexity of the puzzles can be enhanced by incorporating further complex grid sizes beyond 4x6.Additionally, this study can be extended to different types of puzzles, such as Sudoku, Game of 24, and commonsense puzzles.Though our study provides finegrained error categories, it can be further refined by mapping to formal logic to identify more detailed and atomic errors, offering a deeper understanding of LLMs' reasoning failures.Although we propose an effective automatic method for error identification to reduce manual analysis, exploring other automated methods using smaller-scale supervised learning could be a promising future research direction.We also note that this research is currently limited to the English language and can be extended to multilingual scenarios to evaluate the logical reasoning abilities of LLMs.</p>
<p>Ethics Statement</p>
<p>The dataset, GridPuzzle, used for this study is based on 274 puzzles from the open-source platform (more details in section 3.1).No personal information from data creators has been collected during the creation of the dataset.The data collection process strictly adheres to the terms of use and privacy policies of the platform.Furthermore, the use of proprietary LLMs such as GPT-4, Gemini, and Claude-3 in this study adheres to their policies of usage.We have used AI assistants (Grammarly and ChatGPT) to address the grammatical errors and rephrase the sentences.</p>
<p>Task Definition:</p>
<p>Analyze and evaluate the given reasoning chains into specific reasoning error categories.</p>
<p>Figure 7: The prompt structure of auto-evaluator.The prompt consists of instructions to evaluate reasoning chains, Knowledge of the error category descriptions, and a human-evaluated reasoning chain example.This system prompt is followed by a user prompt with the puzzle, its gold solution, and the reasoning chain to be evaluated.</p>
<p>Since we have the pounds lost as 3, 5, 7, and 9, and Stuart already lost 7 lbs, the only way for someone to lose 2 more pounds than another (excluding the 7 lbs already accounted for) is for the gluten-free dieter to have lost 9 lbs and Raymond to have lost 7 lbs.</p>
<p>RW-C</p>
<p>However, since Stuart lost 7 lbs and is not Raymond, this clue helps us deduce the sequence of weight loss but needs to be revisited for exact matching once we have more information.RR 5. From clue 5, Wesley lost 5 lbs.RR 6.Now, knowing Wesley lost 5 lbs, Stuart lost 7 lbs, and the vegan diet resulted in a 3 lbs loss, we can deduce that Tom, being the only person left, lost 9 lbs.</p>
<p>WR-2 7. Since Tom lost 9 lbs and the gluten-free diet is associated with losing 2 more pounds than Raymond, and the only diet left for Wesley (who lost 5 lbs) is dairy-free, it fits that Tom is on the gluten-free diet.</p>
<p>RW-C</p>
<ol>
<li>This leaves the vegan diet for the person who lost 3 lbs, which hasn't been directly matched to a name yet.</li>
</ol>
<p>RR</p>
<p>Since Stuart is on the caveman diet and lost 7 lbs, Wesley is left with dairy-free and lost 5 lbs, and Tom is on the gluten-free diet and lost 9 lbs, Raymond must be the one who lost 3 lbs and is on the vegan diet.</p>
<p>WW-4B</p>
<p>C Evaluation of Reasoning Chains</p>
<p>In order to identify the error categories from the erroneous reasoning chains we conducted manual and auto-evaluation of the reasoning chains.The process of manual evaluation has been described in figure 8 and the process of auto-evaluation using GPT-4o has been described in Figure 9.</p>
<p>D Annotation Guideline</p>
<p>To conduct the manual analysis of the reasoning chains, the annotators were provided the guidelines described in figure 10.The same guideline was also used to create the system prompt for the GPT-4o Auto-evaluator.The annotation process was conducted by 5 annotators and the annotations were also cross-examined to resolve any discrepancies.</p>
<ol>
<li>
<p>Take your time to refer to the annotation example provided.</p>
</li>
<li>
<p>Make sure to break every step into individual sentences before doing the annotations and categorizations.</p>
</li>
<li>
<p>Analyze every sentence separated by a full stop and divide it into premise and conclusion (the conclusion is the derived information).</p>
</li>
</ol>
<p>If a sentence starts with words like "so" or "therefore", the premise will be the previous conclusion.4. Take your time to refer to the error categories described in the Error Categories knowledge section below and Classify the derived information into specific error categories. 5.When classifying into the main Error Categories, the premise and conclusion correctness are determined independently from each other and it is only based on the answer given in the correct solution.</p>
<p>Compare the premise and conclusions with the correct solution first to determine if they are wrong or right.7. Do not use the sub-category explanations to justify your main category (WW, WR, RR, RW, NC) classifications.Only after the error categories have been assigned you will look into the sub categories.8.The premise is declared wrong only when it violates the data given in the correct solution.The conclusion is declared wrong only when it violates the data given in the correct solution.9.If the category has a wrong premise, classify it further into a subcategory for the wrong premise.10.If the category has a wrong conclusion, classify it further into a subcategory for the wrong conclusion.11.Check your work and see if the chosen category is indeed the most appropriate category.12.You are not required to solve the entire puzzle or correct any mistakes.You can take the help from the correct solution to conduct your analysis.</p>
<p>12-step Annotation Guideline</p>
<p>Figure 10: The 12-step guideline provided to the annotators to conduct manual evaluation of the reasoning chains.</p>
<p>E Mitigation Strategy Prompts</p>
<p>We conducted a study on the 60, 3x4 puzzles present in GridPuzzle dataset to try and improve the reasoning abilities of LLMs when solving the gridpuzzle task.We used prompt-based methods, such as the Plan-and-Solve technique, which divides puzzle-solving into planning and solving steps.We also enhanced the solver with insights from our error taxonomy.The prompt structure for this technique is given in figure 11.Next, we devised our own strategy to improve LLM reasoning by using the top error categories from our findings and teaching the LLM to rectify those mistakes.This strategy termed as Feedbacklearning makes use of a detailed system prompt that acts as a feedback-providing unit followed by a basic user prompt to solve the puzzle.The prompt structure for this strategy is shown in figure 12.</p>
<p>We also implemented a code-based technique to sole GridPuzzle.We borrowed the PoT prompt from the original implementation to create a solver function to solve the puzzles.Next, we asked an LLM to implement this code and produce the Final Answer.Since the codes produced by the LLM may contain some errors we utilized the LLM's compiler to implement the code instead of a rigid Python compiler.The prompt structure is provided in figure 13.Next is the Self-correct strategy which merges Self-verify and Self-refine qualities to minimize LLM reasoning errors.It starts with solving You are an excellent puzzle solver.However, we have analyzed how you solve the puzzle before and we found that you make the following mistakes more often while reasoning: Based on your evaluations, refine the answer and provide the final solution.</p>
<p>System Prompt</p>
<p>User Prompt</p>
<p>Figure 12: The prompt structure for the Feedbacklearning strategy.The system prompts consist of instructions regarding the major errors as well as ways to rectify those errors.</p>
<p>the puzzle using a Zero-shot-CoT prompt, followed by prompting the LLM to verify and refine the solution.Finally, it integrates the model's suggestions with insights from our error taxonomy to enhance the puzzle-solving response.The prompt structure for this strategy is shown in figure 14.Lastly, the Self-Discover strategy, depicted in figure 15, proved most effective in reducing LLM reasoning errors in puzzle-solving.This approach begins by having the model analyze the problem and potential errors, follows with a list of prescribed reasoning modules, prompts the LLM to select and apply the most suitable module, and concludes by using a structured prompt to solve the puzzle.</p>
<p>F Model Scaling Effect: Llama-70B</p>
<p>We conducted a case study on the Llama models to analyze their performance on GridPuzzle with increasing model parameter size.We repeated the same experiment in the Zeo-shot-CoT setting with the Llama-70B model.We found that the perfor- mance of the bigger model was marginally higher than the 13B model.The Accuracy went up from 1 correct final answer in the 13B model to 2 in the 70B model.The scores on PuzzleEval also went up 11% on average.However, despite the slight improvement, the Llama model's performance was still inferior to GPT-4, Gemini, and Claude.The experimental findings are presented in Table 6.We infer that even with the increasing model parameter size, the LLMs lack the intrinsic reasoning capabilities required to solve complex logic problems such as GridPuzzle.In the first part of this prompt the model is prompted to assess the problem and select the appropriate reasoning module to solve it.Then the module is modified to give a structured plan to solve the puzzle.In the second part, the model uses this structured plan along with instructions from our error taxonomy to solve the puzzle.</p>
<p>Fill</p>
<p>the following table to show your final answer.3 | correct option from names | correct option from diets 5 | correct option from names | correct option from diets 7 | correct option from names | correct option from diets caveman diet or Raymond.3. Since Stuart lost 7 lbs (from Clue 3), he either used the caveman diet or is named Raymond.4. Clue 5 states Wesley lost 5 lbs. 5. Therefore, Wesley cannot have used the caveman diet or lost 7 lbs.6.We don't know his diet yet. . . .20.That leaves 9 lbs to be the only unassigned number, and since it can't be Wesley (5 lbs) or Stuart (7 lbs), Tom must have lost 9 lbs.21.The only remaining diet for Tom is gluten-free.</p>
<p>Figure 1 :
1
Figure 1: Schematic representation of proposed pipeline.Begins with the data collection of GridPuzzle dataset (top left) and evaluating various LLMs in zero-shot CoT setting (bottom left), then analyzing reasoning chains of LLMs manually to find various error types (top right) and automate this analysis process using LLM to check the correctness of reasoning chain by finding errors (bottom right).</p>
<p>Figure 3 :
3
Figure 3: Performance of five different LLMs in terms of accuracy on the GridPuzzle dataset.</p>
<p>Figure 5 :
5
Figure 5: The first five sub-figures in the above section show the error Sub-category distribution over five LLMS.The last sub-figure denotes the top 10 error Sub category distribution across all model reasoning steps.</p>
<p>Figure 8 :
8
Figure 8: The top left section of the figure consists of a 3x4 sample puzzle from the GridPuzzle dataset along with the Zero-shot-CoT prompt.Right below the prompt, we have the Gold solution for the corresponding puzzle.In the top right section of the figure, we have the Model-generated Reasoning chain to solve this puzzle along with the Final Answer.In this particular instance, the reasoning chain was generated by the GPT-4 model.In the bottom half of the figure, we have the manually evaluated reasoning chain, split into individual sentences.The corresponding error categories found in each step is given the bottom right column called 'Error Categories.'</p>
<p>Figure 11 :
11
Figure 11: The prompt structure for the Plan-and-Solve strategy which is split into two prompts one for planning and the other for solving the puzzle.</p>
<p>Read the Puzzle given below and follow the instructions: <Puzzle> Final answer:</p>
<p>Figure 13 :
13
Figure13: The prompt structure for the Program of Thought prompting technique.In the first part, we asked the LLM to generate a code to solve the given puzzle.In the second part, an LLM is prompted to implement the code produced in the first part to get the Final Answer table.</p>
<p>Figure 14 :
14
Figure14: The prompt structure for the Self-Correct strategy is split into 3 parts.The first prompt solves the puzzle, the second prompt verifies the solution and gives suggestions to improve the solution, and the third prompt uses these suggestions along with error taxonomy-based instructions to refine the final solution.</p>
<p>ForFigure 15 :
15
Figure15:The prompt structure for the Self-Discover strategy.In the first part of this prompt the model is prompted to assess the problem and select the appropriate reasoning module to solve it.Then the module is modified to give a structured plan to solve the puzzle.In the second part, the model uses this structured plan along with instructions from our error taxonomy to solve the puzzle.</p>
<p>(top left).
Error CategoryDescriptionWWWrong Premise and Wrong ConclusionWRWrong Premise and Right ConclusionRWRight Premise and Wrong ConclusionRRRight Premise and Right ConclusionNCNo Conclusion statement or no reasoning involvedTable 1: Proposed error taxonomy for broad categoriesbased on manual analysis. If a statement starts with "so,therefore, hence, this means, this implies, etc." and/or isnot followed by any premise, consider the previous state-ment's conclusion or the previous NC as the premise.</p>
<p>Table 1
1
, we present five main categories: "WW" -Wrong Premise Wrong Conclusion, "WR" -Wrong Premise Right Conclusion, "RW" -Right Premise Wrong Conclusion, "RR" -Right Premise Right Conclusion, and "NC" -No Conclusion.These acronyms of broad categories are self-explanatory.</p>
<p>Table 4 :
4
The results for PuzzleEval on the different grid sizes available in GridPuzzle dataset in terms of ACS.
Model3 x 4 3 x 5 4 x 4 4 x 5 4 x 6 AvgLlama0.450.460.460.420.280.41Mistral0.290.260.270.260.270.27Claude0.600.560.520.550.460.54Gemini0.600.640.540.520.620.58GPT-40.610.620.560.540.600.59% of Step-wise Error Categories41.31 33.50 17.21 7.61 0.37 NC36.94 28.38 19.83 14.00 0.85 RR35.82 16.63 23.82 22.91 0.82 RW35.62 46.67 11.02 6.46 0.23 WW55.17 14.13 15.40 14.96 0.34 WRClaude-3 (4295)Llama-2 (6491)Mistral (4287)GPT-4 (5662)Gemini (4084)
Figure 4: The percentage distribution of the broad error categories across the combined reasoning steps of all five LLMs.The total number of steps generated by each model is provided inside the round brackets below the model names.</p>
<p>Table 5 :
5
The results for accuracy and PuzzleEval using GPT-4-Turbo, with and without mitigation strategies for the 60 samples of 3 × 4 grid-size.</p>
<ol>
<li>Wrong Reasoning........ 2. Error Propagation......... Now, you understand where the problem is.To solve this problem, I am giving you some guidelines that you can use.1.To solve the wrong reasoning problem,...... 2. To solve the error propagation problem,..... Now you have some hints as to how to remove such errors.Using all the above knowledge, please solve the puzzle step-by-step.However, make sure to verify each step again to see if you are making any above errors.Then, re-generate those particular steps if you think they are erroneous.</li>
</ol>
<p>Table 6 :
6
The results for PuzzleEval on the different grid sizes available in GridPuzzle dataset in terms of ACS for Llama-70B.The Accuracy of Llama-70B was 2/274 puzzles.
First PromptSystem Prompt:You are a grid-puzzle solving assistant.User Prompt:<Zero-shot-CoT Puzzle Prompt>Final Answer:Second PromptSystem Prompt:You are a grid-puzzle-solving assistantUser Prompt:Let's verify the solution of the given puzzle below andsuggest some corrections:<Puzzle>Solution:<Previous Response>Suggestion:Third PromptSystem Prompt:You are a grid-puzzle-solving assistantUser Prompt:Now let's use the suggestions to solve the puzzle again:<Previous Suggestion Response><Instructions based on Error Taxonomy><Puzzle>Solution:
Data and source code are available at https://github. com/Mihir3009/GridPuzzle * Equal Contribution
https://logic.puzzlebaron.com/
AcknowledgementWe thank the anonymous reviewers for their constructive suggestions.We extend our gratitude to the Research Computing (RC), and Enterprise Technology at ASU for providing computing resources, and access to the ChatGPT enterprise version for experiments.We acknowledge support by a 2023 Spring Amazon Research Award (ARA).This material is also based upon work supported by the Engineering Research and Development Center -Information Technology Laboratory (ERDC-ITL) under Contract No. W912HZ24C0022.A GridPuzzle Dataset -Sample PuzzleThe GridPuzzle dataset contains 274 puzzles of various grid sizes and complexity.A sample puzzle from the dataset along with the Zero-shot-CoT prompt is described in Figure6.All the puzzles in the dataset have a similar structure with varying numbers of clues.Attached Instruction PromptWhile answering use the following format:Step-by-step solution: [Your steps show how you are solving the puzzle] Final Answer: Fill the following table to show your final answer.2. The April 17 outing was in Gilmore City.3. The outing where they saw 16 shooting stars was in Gilmore City.4. The April 10 outing logged 7 more shooting stars than the April 22 outing.PuzzleJamie never misses an opportunity to watch the night sky for falling stars.Using only the clues below, match the shooting stars to the options from dates and locations.Remember, as with all grid-based logic puzzles, no option in any category will ever be used more than once.B Auto Evaluator: GPT-4oTo expand the reasoning chain evaluation process we prompt the GPT-4o model with a detailed system prompt.The structure of this system prompt is elaborated in Figure7.The 3 main components of this system prompt are the Instruction -similar to the ones given to human evaluators, the Knowledge -obtained from the error taxonomy, and an Exemplar -consisting of a Puzzle, its Gold Solution, the LLM-generated Reasoning chain, and the evaluated Reasoning Chain.Instruction Instructions:Analyze each sentence by dividing it into a premise and conclusion, evaluate errors based on the correct solution, and classify according to specific error categories and subcategories.Formatting Instructions:Follow a JSON format for the output, detailing each sentence with its premise, conclusion, explanation, error category, and subcategory.Knowledge Error Taxonomy Description:Detailed description of each of the error categories and their subcategories.Main Categories:Wrong Premise Wrong Conclusion (WW), Wrong Premise Right Conclusion (WR), Right Premise Wrong Conclusion (RW), Right Premise Right Conclusion (RR), and No Conclusion (NC).Subcategories:Hallucination, Incomplete Information, Assumptions, Error Propagation, Incomplete Information, Wrong Assumption, Wrong reasoning, wrong elimination.Evaluation ExamplePuzzle:Nicholas and Edith are having a small dinner party...Categories:vintages: 1984, 1988, 1992, 1996.wines : Annata Branco, Bianca Flaux, Ece Suss, Vendemmia.types : gewurztraminer, merlot, pinot noir, riesling.Clues:Prompt Model ResponsePuzzle:A group of friends has decided to try several different weight-loss diets and exercises to see who amongst them can lose the most weight in 3 months.Using only the clues below, match the pounds lost to the options from names and diets.Remember, as with all grid-based logic puzzles, no option in any category will ever be used more than once.pounds lost : 3, 5, 7, 9. names : Raymond, Stuart, Tom, Wesley.diets : caveman, dairy-free, gluten-free, vegan.Clues:1. Stuart lost 4 more pounds than the person who used the vegan diet.2. The dieter who used the gluten-free diet lost 2 more pounds than Raymond.3. Stuart lost 7 lbs.4. The dieter who lost 7 lbs is either the person who used the caveman diet or Raymond. 5. Wesley lost 5 lbs.While answering use the following format:Step-by-step solution: [Your steps show how you are solving the puzzle] Final Answer:Fill the following Step-by-step solution:1. From clue 3, we know Stuart lost 7 lbs.2. From clue 1, since Stuart lost 7 lbs, the person on the vegan diet lost 3 lbs (7 -4 = 3).3. From clue 4, since Stuart lost 7 lbs, he is either on the caveman diet or he is Raymond.But we know Stuart's name, so he must be on the caveman diet.4. From clue 2, the person on the gluten-free diet lost 2 more pounds than Raymond.Since we have the pounds lost as 3, 5, 7, and 9, and Stuart already lost 7 lbs, the only way for someone to lose 2 more pounds than another (excluding the 7 lbs already accounted for) is for the gluten-free dieter to have lost 9 lbs and Raymond to have lost 7 lbs.However, since Stuart lost 7 lbs and is not Raymond, this clue helps us deduce the sequence of weight loss but needs to be revisited for exact matching once we have more information.5. From clue 5, Wesley lost 5 lbs.6.Now, knowing Wesley lost 5 lbs, Stuart lost 7 lbs, and the vegan diet resulted in a 3 lbs loss, we can deduce that Tom, being the only person left, lost 9 lbs.7. Since Tom lost 9 lbs and the gluten-free diet is associated with losing 2 more pounds than Raymond, and the only diet left for Wesley (who lost 5 lbs) is dairy-free, it fits that Tom is on the gluten-free diet.8.This leaves the vegan diet for the person who lost 3 lbs, which hasn't been directly matched to a name yet.Since Stuart is on the caveman diet and lost 7 lbs, Wesley is left with dairy-free and lost 5 lbs, and Tom is on the gluten-free diet and lost 9 lbs, Raymond must be the one who lost 3 lbs and is on the vegan diet.Gold Solution:3 Let's break down the clues and solve the puzzle step by step.Step 1:From clue 1, we know Underwood is in room 114.Since Underwood is staying for 2 days (as the days are 2, 3, 4, 5), we can conclude that Underwood is staying for 2 days.Step 2:From clue 2, we know the guest in room 320 is staying 1 day fewer than Underwood.Since Underwood is staying for 2 days, the guest in room 320 is staying for 1 day.This means the guest in room 320 is staying for day 2.Step 3: From clue 3, we know Owens is staying for 3 days.Since Owens is staying for 3 days, Owens must be staying for day 3.Step 4:From clue 4, we know Parker is staying 1 day longer than the client in room 314.Since the days are 2, 3, 4, 5, the client in room 314 must be staying for day 4.Parker is staying 1 day longer, so Parker is staying for day 5.Step
Ai Anthropic, The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. 2024</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 2023</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Explaining answers with entailment trees. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark, 10.18653/v1/2021.emnlp-main.585Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Compression, transduction, and creation: A unified framework for evaluating natural language generation. Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric Xing, Zhiting Hu, 10.18653/v1/2021.emnlp-main.599Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jiang, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Sean Sanyal, Xiang Welleck, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, Faith and fate: Limits of transformers on compositionality. 2023</p>
<p>Chessgpt: Bridging policy learning and language modeling. Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, Jun Wang, Advances in Neural Information Processing Systems. 202436</p>
<p>Panagiotis Giadikiaroglou, Maria Lymperaiou, arXiv:2402.11291Giorgos Filandrianos, and Giorgos Stamou. 2024. Puzzle solving using reasoning of large language models: A survey. arXiv preprint</p>
<p>Roscoe: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Peng Chen, Spencer Poff, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, The Eleventh International Conference on Learning Representations. Martin Corredor,. 2022</p>
<p>Beyond the obvious: Evaluating the reasoning ability in reallife scenarios of language models on life scapes reasoning benchmark˜(lsr-benchmark). Zhouhong Gu, Zihan Li, Lin Zhang, Zhuozhi Xiong, Sihang Jiang, Xiaoxuan Zhu, Shusen Wang, Zili Wang, Jianchen Wang, Haoning Ye, arXiv:2307.051132023arXiv preprint</p>
<p>Akshat Gupta, arXiv:2308.12466Are chatgpt and gpt-4 good poker players?-a pre-flop analysis. 2023arXiv preprint</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Folio: Natural language reasoning with firstorder logic. 2022arXiv preprint</p>
<p>Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2024</p>
<p>Lateval: An interactive llms evaluation benchmark with incomplete information from lateral thinking puzzles. Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong Zhang, Hai-Tao Zheng, 2024</p>
<p>Leveraging large language models to generate answer set programs. Adam Ishay, Zhun Yang, Joohyung Lee, Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning. the 20th International Conference on Principles of Knowledge Representation and Reasoning2023</p>
<p>LGPSolver -solving logic grid puzzles automatically. Elgun Jabrayilzade, Selma Tekir, 10.18653/v1/2020.findings-emnlp.100Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Boardgameqa: A dataset for natural language reasoning with contradictory information. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Advances in Neural Information Processing Systems. 202436Xin Xu, Vaiva Imbrasaite, and Deepak Ramachandran</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>RiddleSense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. Ziyi Bill Yuchen Lin, Yichi Wu, Dong-Ho Yang, Xiang Lee, Ren, 10.18653/v1/2021.findings-acl.131Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Learning to automatically solve logic grid puzzles. Arindam Mitra, Chitta Baral, 10.18653/v1/D15-1118Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Puzzle solving without search or human knowledge: An unnatural language approach. David A Noever, Ryerson Burdick, ArXiv, abs/2109.027972021</p>
<p>LogicBench: Towards systematic evaluation of logical reasoning ability of large language models. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral, the proceeding of the Association for Computational Linguistics (ACL). 20242024</p>
<p>Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral, arXiv:2406.17169Multi-logieval: Towards evaluating multi-step logical reasoning ability of large language models. 2024arXiv preprint</p>
<p>ReCEval: Evaluating reasoning chains via correctness and informativeness. Archiki Prasad, Swarnadeep Saha, Xiang Zhou, Mohit Bansal, 10.18653/v1/2023.emnlp-main.622Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2022</p>
<p>. Tal Schuster, Ashwin Kalyan, Oleksandr Polozov, Adam Tauman, Kalai , arXiv:2106.057842021. Programming puzzles. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, Victor Cȃrbune, arXiv:2311.08516Llms cannot find reasoning errors, but can correct them!. 2023arXiv preprint</p>
<p>Planand-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, Annual Meeting of the. Association for Computational Linguistics2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>Small language models need strong verifiers to self-correct reasoning. Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, Lu Wang, 2024</p>
<p>Jingmiao Zhao, Carolyn Jane, Anderson , arXiv:2306.12255Solving and generating npr sunday puzzles with large language models. 2023arXiv preprint</p>
<p>Self-discover: Large language models self-compose reasoning structures. Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed H Chi, Denny Zhou, Swaroop Mishra, Huaixiu Steven, Zheng , 2024</p>
<p>The Ece Suss was bottled sometime after the Annata Branco. </p>
<p>Gold Solution: 1984 | Annata Branco | gewurztraminer 1988 | Ece Suss | pinot noir 1992 | Bianca Flaux | merlot 1996 | Vendemmia | riesling LLM Reasoning Chain: 1. From clue 6, we know that the 1984 bottle is a gewurztraminer. . So, Human-evaluated reasoning chain: [ { "Sentence. From clue 6, we know that the 1984 bottle is a gewurztraminer.", "Premise": "clue 6", "Conclusion": "We know that the 1984 bottle is a gewurztraminer.", "Explanation": "Reiteration of the clue.", "Error Category": "NC", "Sub Category": "-" },..</p>            </div>
        </div>

    </div>
</body>
</html>
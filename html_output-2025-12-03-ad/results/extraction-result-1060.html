<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1060 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1060</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1060</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-a1257d485970d89125795e1c9308cd4bb54309c5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a1257d485970d89125795e1c9308cd4bb54309c5" target="_blank">Variable Compliance Control for Robotic Peg-in-Hole Assembly: A Deep Reinforcement Learning Approach</a></p>
                <p><strong>Paper Venue:</strong> Applied Sciences</p>
                <p><strong>Paper TL;DR:</strong> This work proposes the use of an off-policy, model-free reinforcement-learning method to solve peg-in-hole tasks with hole-position uncertainty, and bootstraped the training speed by using several transfer-learning techniques (sim2real) and domain randomization.</p>
                <p><strong>Paper Abstract:</strong> Industrial robot manipulators are playing a significant role in modern manufacturing industries. Though peg-in-hole assembly is a common industrial task that has been extensively researched, safely solving complex, high-precision assembly in an unstructured environment remains an open problem. Reinforcement-learning (RL) methods have proven to be successful in autonomously solving manipulation tasks. However, RL is still not widely adopted in real robotic systems because working with real hardware entails additional challenges, especially when using position-controlled manipulators. The main contribution of this work is a learning-based method to solve peg-in-hole tasks with hole-position uncertainty. We propose the use of an off-policy, model-free reinforcement-learning method, and we bootstraped the training speed by using several transfer-learning techniques (sim2real) and domain randomization. Our proposed learning framework for position-controlled robots was extensively evaluated in contact-rich insertion tasks in a variety of environments.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1060.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1060.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ours (sim2real+retrain) SAC TCN agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Soft Actor-Critic agent with Temporal Convolutional Network policy trained with sim2real and retraining for variable-compliance peg-in-hole assembly</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied robotic agent implemented on a UR3e position-controlled manipulator that uses SAC (off-policy max-entropy RL) and a TCN-based multimodal policy to output Cartesian subgoals and adaptive parallel position-force controller parameters; trained in Gazebo with domain randomization and fine-tuned on real hardware (sim2real + retrain).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>UR3e SAC-TCN adaptive-compliance agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model-free reinforcement-learning agent using Soft Actor-Critic (SAC) with a multimodal policy: proprioceptive network + temporal convolutional network (TCN) for force/torque time series; outputs Cartesian subgoals and 18 force/position controller parameters (K_p^x, K_p^f, selection matrix S). Trained in simulation with domain randomization (500,000 timesteps) and refined on the real robot (≈15,000 timesteps, reported as 3% of sim steps). Residual RL (adding policy output to a baseline reference trajectory) was optionally used to speed learning.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical robot (UR3e) and simulated agent (Gazebo) - embodied robotic agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Peg-in-hole insertion environments (cuboid peg task + novel insertion tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Contact-rich peg-in-hole assembly tasks including: (a) a cuboid-peg into board hole (training task) with nonsmooth surface and 1.0 mm clearance; (b) variations of the cuboid task with randomized initial positions and goal-pose uncertainty; (c) environments with different contact stiffness (high: rigid board; medium: peg held with rubber band; low: soft foam surface); (d) novel, harder insertion tasks not seen in training (metal ring with 0.2 mm clearance, electric outlet requiring high insertion force, LAN port and USB connector with complex geometry). Complexity arises from tight clearances, non-smooth creviced surfaces, variable contact dynamics (stiffness/friction), and complex mating geometry; variation arises from randomized initial/goal poses, randomized surface stiffness, randomized desired insertion forces, and randomized uncertainty in goal pose.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized by task difficulty metrics: geometric clearance (e.g., 0.2 mm for ring), contact-surface stiffness (quantified in Gazebo kp values and via qualitative high/medium/low environments), peg/hole shape complexity (simple cuboid vs LAN port), and presence of crevices; also measured indirectly by observed success rate and task completion difficulty. Quantitative environment parameters used during training: initial position relative to goal sampled from [-400, 400] mm, orientation sampled from [-10°, 10°], and training/evaluation goal-uncertainty ranges (position ±1–5 mm, orientation ±1°–5°).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies by task: training cuboid task = low-to-medium complexity; ring (0.2 mm clearance), LAN port, electric outlet = high complexity</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Domain randomization degrees and ranges: randomized initial/goal positions, randomized object-surface stiffness (Gazebo kp in [1.0e-5, 7.0e-4]), uncertainty error in predicted goal pose (position ±2 mm during training; evaluated up to ±5 mm), orientation uncertainty (±5° during training; evaluated up to ±5°), desired insertion force randomized in [0,10] N; randomized insertion planes (set of possible planes). Number of randomized training conditions and sampling ranges serve as variation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high during training (explicit domain randomization across position, orientation, stiffness, and desired force); evaluation used graded variation levels (low-to-high: 1–5 mm and 1°–5° uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Primary: success rate (%) of insertion; Secondary: average time steps per successful trial, average wall-clock time (s), cumulative reward (training curves), collision occurrences (penalty), and task completion within 1 mm threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative results: (Table 2) On 3D-printed cuboid task with true goal: 'Ours' (sim2real+retrain) success rate 100%, Avg. time steps 65.6, Avg. time 3.28 s. Sim2real (no retrain) 95% success, 75.3 steps, 3.77 s. From-scratch 100% but slower: 109.6 steps, 5.48 s. (Table 3) Under goal-position uncertainty: Ours success 100% at ±1–2 mm, 95% at 3 mm, 65% at 4 mm, 60% at 5 mm; orientation uncertainty Ours 100% up to 5°. (Table 4) Across stiffness: Ours 100% success in high/medium/low stiffness. (Table 5) Novel tasks: Ring 80% (5 N), Electric Outlet (x & y) 75% (10 N), LAN port x 55%/y 60% (5 N), USB 80% (8 N).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses trade-offs and relationships: domain randomization (increasing variation during training) improves generalization to unseen variations (goal-uncertainty, stiffness, insertion planes) and sim2real transfer when followed by brief real-world retraining; however, larger environment complexity (e.g., complex mating geometry/crevices, very tight clearances, hard contact dynamics) reduces success rates even with high variation during training (LAN port and ring examples). They also report a trade-off in action/control parameterization: a wide allowed range for force-control parameters increases adaptability to varied environments but makes learning harder and less stable (slower convergence, early unstable behavior), whereas a narrow parameter range speeds learning but reduces generalization. Furthermore, if the simulator's physics differ too much from reality (large dynamics gap), purely transferring a sim policy can perform worse than training from scratch under some conditions (e.g., large orientation uncertainty interacting with friction).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Ring (high stiffness, tight clearance 0.2 mm) success rate 80% (20 trials) — representative of high complexity but relatively low variation (evaluated with perfect goal pose).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Cuboid peg with randomized initial positions, randomized desired insertion force and domain randomization: 'Ours' achieved high success (Table 3 'Random' = 90% success when random offsets in all directions were applied; when evaluated with randomized stiffness and other variations during training and retraining, Table 4 reports 100% success across stiffness levels), indicating strong performance in low-shape-complexity but high-variation conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>LAN port (complex shape) evaluated from random initial positions with perfect goal pose: success rates 55% (x) and 60% (y) (20 trials each) — indicates degraded performance when both shape complexity is high and initial-condition variation is present.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Cuboid peg with true goal pose and little/no variation: 'Ours' success rate 100% (Table 2, 20 trials), average time 3.28 s.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Domain randomization in simulation (randomized initial/goal poses, stiffness, uncertainty error, desired insertion force), sim2real transfer with brief real-world retraining (≈3% of sim timesteps), residual reinforcement learning (optionally adding learned residual to a baseline reference trajectory), off-policy SAC with prioritized replay; TCN-based policy architecture for haptic time-series.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Policies trained with domain randomization and sim2real+retrain generalized well to variations in goal-uncertainty (maintaining high success up to ±2 mm and ±2°; degrading gradually beyond that), to varying environment stiffness (100% success across high/medium/low stiffness), and to unseen insertion planes. Generalization to novel complex geometries was mixed: good performance on some novel tasks (ring, USB, electric outlet: 75–80% success) but poor on particularly intricate shapes (LAN port: 55–60%). Sim2real without retraining performed worse than sim2real+retrain and sometimes worse than scratch when the simulated dynamics gap was large.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training in simulation: 500,000 timesteps (~5 hours). Real-robot fine-tuning: ≈3% of sim steps (~15,000 steps, ≈20 minutes). Ablation: learning from scratch on real robot required ~50,000 steps to reach good performance; initializing from a simulated policy and retraining reached comparable performance in under ~5,000 real steps (pretrained policies required fewer real interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Combining SAC with a TCN multimodal policy and controlling both Cartesian subgoals and force-control gains enables a position-controlled industrial robot to learn contact-rich insertion tasks. 2) Domain randomization in simulation plus brief real-world retraining (sim2real+retrain) gives the best trade-off between sample efficiency and real-world performance, outperforming both pure sim2real (no retrain) and training from scratch on real hardware in time-to-solution. 3) Increasing environment variation during training improves generalization to unseen variations (goal uncertainty, stiffness, insertion planes), but high intrinsic task complexity (tight clearances, crevices, complex mating geometry) still reduces success rates. 4) Allowing a wide action range for force-control parameters increases adaptability but makes learning more difficult and less stable; choosing parameter ranges is an important manual trade-off. 5) Residual RL (adding policy residuals to a baseline trajectory) and a TCN for haptic processing accelerate learning and improve convergence compared to simpler architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variable Compliance Control for Robotic Peg-in-Hole Assembly: A Deep Reinforcement Learning Approach', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world. <em>(Rating: 2)</em></li>
                <li>Learning dexterous in-hand manipulation. <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. <em>(Rating: 2)</em></li>
                <li>Residual Reinforcement Learning for Robot Control. <em>(Rating: 2)</em></li>
                <li>Learning variable impedance control. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1060",
    "paper_id": "paper-a1257d485970d89125795e1c9308cd4bb54309c5",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Ours (sim2real+retrain) SAC TCN agent",
            "name_full": "Soft Actor-Critic agent with Temporal Convolutional Network policy trained with sim2real and retraining for variable-compliance peg-in-hole assembly",
            "brief_description": "An embodied robotic agent implemented on a UR3e position-controlled manipulator that uses SAC (off-policy max-entropy RL) and a TCN-based multimodal policy to output Cartesian subgoals and adaptive parallel position-force controller parameters; trained in Gazebo with domain randomization and fine-tuned on real hardware (sim2real + retrain).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "UR3e SAC-TCN adaptive-compliance agent",
            "agent_description": "A model-free reinforcement-learning agent using Soft Actor-Critic (SAC) with a multimodal policy: proprioceptive network + temporal convolutional network (TCN) for force/torque time series; outputs Cartesian subgoals and 18 force/position controller parameters (K_p^x, K_p^f, selection matrix S). Trained in simulation with domain randomization (500,000 timesteps) and refined on the real robot (≈15,000 timesteps, reported as 3% of sim steps). Residual RL (adding policy output to a baseline reference trajectory) was optionally used to speed learning.",
            "agent_type": "physical robot (UR3e) and simulated agent (Gazebo) - embodied robotic agent",
            "environment_name": "Peg-in-hole insertion environments (cuboid peg task + novel insertion tasks)",
            "environment_description": "Contact-rich peg-in-hole assembly tasks including: (a) a cuboid-peg into board hole (training task) with nonsmooth surface and 1.0 mm clearance; (b) variations of the cuboid task with randomized initial positions and goal-pose uncertainty; (c) environments with different contact stiffness (high: rigid board; medium: peg held with rubber band; low: soft foam surface); (d) novel, harder insertion tasks not seen in training (metal ring with 0.2 mm clearance, electric outlet requiring high insertion force, LAN port and USB connector with complex geometry). Complexity arises from tight clearances, non-smooth creviced surfaces, variable contact dynamics (stiffness/friction), and complex mating geometry; variation arises from randomized initial/goal poses, randomized surface stiffness, randomized desired insertion forces, and randomized uncertainty in goal pose.",
            "complexity_measure": "Characterized by task difficulty metrics: geometric clearance (e.g., 0.2 mm for ring), contact-surface stiffness (quantified in Gazebo kp values and via qualitative high/medium/low environments), peg/hole shape complexity (simple cuboid vs LAN port), and presence of crevices; also measured indirectly by observed success rate and task completion difficulty. Quantitative environment parameters used during training: initial position relative to goal sampled from [-400, 400] mm, orientation sampled from [-10°, 10°], and training/evaluation goal-uncertainty ranges (position ±1–5 mm, orientation ±1°–5°).",
            "complexity_level": "varies by task: training cuboid task = low-to-medium complexity; ring (0.2 mm clearance), LAN port, electric outlet = high complexity",
            "variation_measure": "Domain randomization degrees and ranges: randomized initial/goal positions, randomized object-surface stiffness (Gazebo kp in [1.0e-5, 7.0e-4]), uncertainty error in predicted goal pose (position ±2 mm during training; evaluated up to ±5 mm), orientation uncertainty (±5° during training; evaluated up to ±5°), desired insertion force randomized in [0,10] N; randomized insertion planes (set of possible planes). Number of randomized training conditions and sampling ranges serve as variation metrics.",
            "variation_level": "high during training (explicit domain randomization across position, orientation, stiffness, and desired force); evaluation used graded variation levels (low-to-high: 1–5 mm and 1°–5° uncertainty).",
            "performance_metric": "Primary: success rate (%) of insertion; Secondary: average time steps per successful trial, average wall-clock time (s), cumulative reward (training curves), collision occurrences (penalty), and task completion within 1 mm threshold.",
            "performance_value": "Representative results: (Table 2) On 3D-printed cuboid task with true goal: 'Ours' (sim2real+retrain) success rate 100%, Avg. time steps 65.6, Avg. time 3.28 s. Sim2real (no retrain) 95% success, 75.3 steps, 3.77 s. From-scratch 100% but slower: 109.6 steps, 5.48 s. (Table 3) Under goal-position uncertainty: Ours success 100% at ±1–2 mm, 95% at 3 mm, 65% at 4 mm, 60% at 5 mm; orientation uncertainty Ours 100% up to 5°. (Table 4) Across stiffness: Ours 100% success in high/medium/low stiffness. (Table 5) Novel tasks: Ring 80% (5 N), Electric Outlet (x & y) 75% (10 N), LAN port x 55%/y 60% (5 N), USB 80% (8 N).",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses trade-offs and relationships: domain randomization (increasing variation during training) improves generalization to unseen variations (goal-uncertainty, stiffness, insertion planes) and sim2real transfer when followed by brief real-world retraining; however, larger environment complexity (e.g., complex mating geometry/crevices, very tight clearances, hard contact dynamics) reduces success rates even with high variation during training (LAN port and ring examples). They also report a trade-off in action/control parameterization: a wide allowed range for force-control parameters increases adaptability to varied environments but makes learning harder and less stable (slower convergence, early unstable behavior), whereas a narrow parameter range speeds learning but reduces generalization. Furthermore, if the simulator's physics differ too much from reality (large dynamics gap), purely transferring a sim policy can perform worse than training from scratch under some conditions (e.g., large orientation uncertainty interacting with friction).",
            "high_complexity_low_variation_performance": "Ring (high stiffness, tight clearance 0.2 mm) success rate 80% (20 trials) — representative of high complexity but relatively low variation (evaluated with perfect goal pose).",
            "low_complexity_high_variation_performance": "Cuboid peg with randomized initial positions, randomized desired insertion force and domain randomization: 'Ours' achieved high success (Table 3 'Random' = 90% success when random offsets in all directions were applied; when evaluated with randomized stiffness and other variations during training and retraining, Table 4 reports 100% success across stiffness levels), indicating strong performance in low-shape-complexity but high-variation conditions.",
            "high_complexity_high_variation_performance": "LAN port (complex shape) evaluated from random initial positions with perfect goal pose: success rates 55% (x) and 60% (y) (20 trials each) — indicates degraded performance when both shape complexity is high and initial-condition variation is present.",
            "low_complexity_low_variation_performance": "Cuboid peg with true goal pose and little/no variation: 'Ours' success rate 100% (Table 2, 20 trials), average time 3.28 s.",
            "training_strategy": "Domain randomization in simulation (randomized initial/goal poses, stiffness, uncertainty error, desired insertion force), sim2real transfer with brief real-world retraining (≈3% of sim timesteps), residual reinforcement learning (optionally adding learned residual to a baseline reference trajectory), off-policy SAC with prioritized replay; TCN-based policy architecture for haptic time-series.",
            "generalization_tested": true,
            "generalization_results": "Policies trained with domain randomization and sim2real+retrain generalized well to variations in goal-uncertainty (maintaining high success up to ±2 mm and ±2°; degrading gradually beyond that), to varying environment stiffness (100% success across high/medium/low stiffness), and to unseen insertion planes. Generalization to novel complex geometries was mixed: good performance on some novel tasks (ring, USB, electric outlet: 75–80% success) but poor on particularly intricate shapes (LAN port: 55–60%). Sim2real without retraining performed worse than sim2real+retrain and sometimes worse than scratch when the simulated dynamics gap was large.",
            "sample_efficiency": "Training in simulation: 500,000 timesteps (~5 hours). Real-robot fine-tuning: ≈3% of sim steps (~15,000 steps, ≈20 minutes). Ablation: learning from scratch on real robot required ~50,000 steps to reach good performance; initializing from a simulated policy and retraining reached comparable performance in under ~5,000 real steps (pretrained policies required fewer real interactions).",
            "key_findings": "1) Combining SAC with a TCN multimodal policy and controlling both Cartesian subgoals and force-control gains enables a position-controlled industrial robot to learn contact-rich insertion tasks. 2) Domain randomization in simulation plus brief real-world retraining (sim2real+retrain) gives the best trade-off between sample efficiency and real-world performance, outperforming both pure sim2real (no retrain) and training from scratch on real hardware in time-to-solution. 3) Increasing environment variation during training improves generalization to unseen variations (goal uncertainty, stiffness, insertion planes), but high intrinsic task complexity (tight clearances, crevices, complex mating geometry) still reduces success rates. 4) Allowing a wide action range for force-control parameters increases adaptability but makes learning more difficult and less stable; choosing parameter ranges is an important manual trade-off. 5) Residual RL (adding policy residuals to a baseline trajectory) and a TCN for haptic processing accelerate learning and improve convergence compared to simpler architectures.",
            "uuid": "e1060.0",
            "source_info": {
                "paper_title": "Variable Compliance Control for Robotic Peg-in-Hole Assembly: A Deep Reinforcement Learning Approach",
                "publication_date_yy_mm": "2020-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world.",
            "rating": 2
        },
        {
            "paper_title": "Learning dexterous in-hand manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience.",
            "rating": 2
        },
        {
            "paper_title": "Residual Reinforcement Learning for Robot Control.",
            "rating": 2
        },
        {
            "paper_title": "Learning variable impedance control.",
            "rating": 1
        }
    ],
    "cost": 0.011299499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Article</h1>
<h2>Variable Compliance Control for Robotic Peg-in-Hole Assembly: A Deep-Reinforcement-Learning Approach</h2>
<p>Beltran-Hernandez Cristian C. ${ }^{1, \star} \odot$, Damien Petit ${ }^{1 \odot}$, Ixchel G. Ramirez-Alpizar ${ }^{2 \odot}$, and Kensuke Harada ${ }^{1,2 \odot}$<br>1 Graduate School of Engineering Science, Osaka University, Japan;<br>2 Automation Research Team, Industrial CPS Research Center, National Institute of Advanced Industrial Science and Technology (AIST), Japan<br>* Correspondence: beltran [at] hlab.sys.es.osaka-u.ac.jp</p>
<p>Received: date; Accepted: date; Published: date</p>
<h2>Featured Application: assembly tasks with industrial robot manipulators.</h2>
<p>Abstract: Industrial robot manipulators are playing a more significant role in modern manufacturing industries. Though peg-in-hole assembly is a common industrial task that has been extensively researched, safely solving complex high-precision assembly in an unstructured environment remains an open problem. Reinforcement-learning (RL) methods have proven to be successful in autonomously solving manipulation tasks. However, RL is still not widely adopted on real robotic systems because working with real hardware entails additional challenges, especially when using position-controlled manipulators. The main contribution of this work is a learning-based method to solve peg-in-hole tasks with hole-position uncertainty. We propose the use of an off-policy model-free reinforcement-learning method, and we bootstraped the training speed by using several transfer-learning techniques (sim2real) and domain randomization. Our proposed learning framework for position-controlled robots was extensively evaluated on contact-rich insertion tasks in a variety of environments.</p>
<p>Keywords: reinforcement learning; compliance control; robotic assembly; sim2real; domain randomization</p>
<h2>1. Introduction</h2>
<p>Autonomous robotic assembly is an essential component of industrial applications. Industrial robot manipulators are playing a more significant role in modern manufacturing industries with the goal of improving production efficiency and reducing costs. Though peg-in-hole assembly is a common industrial task that has been extensively researched, safely solving complex high-precision assembly in an unstructured environment remains an open problem [1].</p>
<p>Most common industrial robots are joint-position-controlled. For this type of robot, compliance control is necessary to safely attempt contact-rich tasks, or the robot is prone to causing large unsafe assembly forces even with tiny position errors. Compliant robot assembly tasks have been studied in two ways, passive and active methods. In passive methods, a mechanical device called remote center compliance (RCC) [2] is placed between the robot's wrist and gripper. The passive compliance provided by the RCC lets the gripper move perpendicularly to the peg's axis and rotate freely so as to reduce resistance. However, the passive method does not work well with high-precision assembly [3]. On the other hand, active compliant methods correct assembly errors through sensor feedback. In general, these methods use force sensors to detect the external forces and moments, and design control strategies on the basis of dynamic models of the task to minimize contact force [4]. Some active methods mimic human compliance during assembly [5]. Nevertheless, most of these assembly methods are not practical to use</p>
<p>in real applications. Model parameters need to be identified, and controller gains need to be tuned. In both cases, the process is manually engineered for specific tasks, which requires a lot of time, effort, and expertise. These approaches are also not robust to uncertainties and do not generalize well to variations in the environment.</p>
<p>To reduce human involvement and increase robustness to uncertainties, the most recent research has been focused on learning assembly skills either from human demonstrations [6] or directly from interactions with the environment [7]. The present research focuses on the latter.</p>
<p>Reinforcement-learning (RL) methods allow for agents to learn complex behaviors through interactions with the surrounding environment, and by maximizing rewards received from the environment; ideally, the agents' behavior can generalize to unseen scenarios or tasks [7]. Therefore, RL can be applied to robotic agents to learn high-precision assembly skills instead of only transferring human skills to the robot program [8]. Recent studies showed the importance of RL for robotic manipulation tasks [9-11], but none of these methods can be applied directly to high-precision industrial applications due to the lack of fine motion control.</p>
<p>In [12], an RL technique was used to learn a simple peg-in-hole insertion operation. Similarly, Inuo et al. [13] proposed a robot skill-acquisition approach by training a recurrent neural network to learn a peg-in-hole assembly policy. However, these approaches used a finite number of actions by discretizing the action space, which has many limitations in continuous-action control tasks [14], as is the case for robot control, which is continuous and high-dimensional.</p>
<p>Xu et al. [15] proposed learning dual peg insertion by using the deep deterministic policy gradient [16] (DDPG) algorithm with a fuzzy reward system. Similarly, Fan et al. [17] used DDPG combined with guided policy search (GPS) [18] to learn high-precision assembly tasks. Luo et al. [19] also used GPS to learn a peg-in-hole tasks on a deformable surface. Nevertheless, these methods learn policies that control the motion trajectory only while they require the manual tuning of force control gains; therefore, they do not scale well to variations of the environment.</p>
<p>Ren et al. [20] proposed the use of DDPG to simultaneously control position and force control gains, but they assumed the geometric knowledge of the insertion task, which made the learned policies inflexible to be applied to different insertion tasks. To solve high-precision assembly tasks, our approach focused on learning policies that simultaneously control the robot's motion trajectory and actively tune a compliant controller to unknown geometric constraints.</p>
<p>Buchli et al. [21] accomplished variable stiffness skill learning on robot manipulators by using an RL algorithm call-policy improvement with path integrals (PI2). However, the method was formulated for torque-control robots. Another similar approach was to use a flexible robot so as to focus only on the motion trajectory, as in [22]; however, rigid position-controlled robots are still more widely used. Therefore, we focus on industrial robot manipulators, which are mainly position-based-controlled.</p>
<p>Abu- Dakka et al. [23] proposed a learning method based on iterative learning control (ILC). Their method is focus on transferring manipulation skills from demonstrations that provide a reference trajectory and force profile. In this work, we present a method that can learn manipulation skills without prior knowledge of a reference trajectory or force profile. However, our method supports the use of such prior knowledge to speed up the learning phase.</p>
<p>The main contribution of this work is a robust learning-based framework for robotic peg-in-hole assembly given an uncertain goal position. Our method enables a position-controlled industrial robot manipulator to safely learn contact-rich manipulation tasks by controlling the nominal trajectory and, at the same time, learning variable force control gains for each phase of the task. We built this on the basis of our previous work [24]. More specifically, the contributions of this work are:</p>
<ul>
<li>
<p>A robust policy representation based on time convolutional neural networks (TCN).</p>
</li>
<li>
<p>Faster learning of control policies via domain transfer-learning techniques (sim2real) to greatly improve the training efficiency on real robots.</p>
</li>
<li>Improved generalization capabilities of the learned control policies via domain randomization during the training phase on simulation. Although the effects of domain randomization have been researched [25,26], to the best of our knowledge, we are the first to study the effects of sim2real with domain randomization on contact-rich real-robot applications with position-controlled robots.</li>
</ul>
<p>The effectiveness of the proposed method is shown through extensive evaluation with a real robotic system on a variety of contact-rich peg-in-hole insertion tasks.</p>
<h1>1.1. Problem Statement</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Insertion task with uncertain goal position.
In the present study, we considered a peg-in-hole assembly task that required the mating of two components. One of the components was grasped and manipulated by the robot manipulator, while the second component had a fixed position either via fixtures to an environment surface or by being held by a second robot manipulator. Figure 1 provides a 2D representation of the considered insertion tasks and the components assumed to be available to solve the task. The proposed method was designed for a position-controlled robot manipulator with a force/torque sensor at its wrist. Typically, these insertion tasks can be broadly divided into two main phases [27], search and insertion. During the search phase, the robot aligns the peg within the clearance region of the hole. In the beginning, the peg is located at a distance from the center of the hole in a random direction. The distance from the hole is assumed to be the "positional error". During the insertion phase, the robot adjusts the orientation of the peg with respect to the hole orientation, and pushes the peg to the desired position. We focused on both phases of the assembly task with the following assumptions:</p>
<ul>
<li>The manipulated object was already firmly grasped. However, slight changes of object orientation within the gripper were possible during manipulation.</li>
<li>There was access to imperfect prediction of the target end-effector pose (as shown in Figure 1) or a reference trajectory and its degree of uncertainty.</li>
<li>The manipulated object was inserted in a direction parallel to the gripper's orientation.</li>
</ul>
<p>We considered the second assumption fair given the advances in vision-recognition techniques, where the 6D pose of objects could be estimated from single RGB images [28,29] or RGB images with depth maps (RGB-D) [30,31]. The high accuracy of the predictions are in many cases enough for robot manipulation. Moreover, this second assumption included the specific case of using an assembly planner [32,33], where</p>
<p>even if the initial position of the objects is known, the inevitable error throughout the manipulation (e.g. pick-and-place, grasping, and regrasping) that makes the position/orientation of the manipulated objects uncertain during the insertion phase. A reference trajectory could be similarly obtained from demonstrations [34-36] when a complex motion is required to achieve the insertion. The last assumption allowed for defining a desired insertion force that may vary for different insertion tasks without loss of generalization.</p>
<h1>2. Materials and Methods</h1>
<h3>2.1. System Overview</h3>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Our proposed framework. On the basis of estimated target position for an insertion task, our system learns a control policy that defines motion-trajectory and force-control parameters of an adaptive compliance controller to control an industrial robot manipulator.</p>
<p>Our proposed system aims to solve assembly tasks with an uncertain goal pose. Figure 2 shows the overall system architecture. There were two control loops. The inner loop was an adaptive compliance controller; we chose to use a parallel position-force controller that was proven to work well for this kind of contact-rich manipulation tasks [24]. The inner loop ran at a control frequency of 500 Hz , which is the maximum available in Universal Robots e-series robotic arms ${ }^{1}$. Details of the parallel controller are provided in Section 2.2.3. The outer loop was an RL control policy running at 20 Hz that provided subgoal positions and the parameters of the compliance controller. The outer loop's slower control frequency allowed for the policy to process the robot state and compute the next action to be taken by the manipulator, while the inner loop's precise high-frequency control would seek to achieve and maintain the subgoal provided by the policy. Details of the RL algorithm and the policy architecture are provided in Section 2.2. Lastly, the input to the system was estimated target position and orientation for the insertion task.</p>
<p>Motion commands $\mathbf{x}_{c}$ sent to the adaptive compliance controller corresponded to the pose of the robot's end effector. The pose was of the form $\mathbf{x}=[\mathbf{p}, \phi]$, where $\mathbf{p} \in \mathbb{R}^{3}$ is the position vector, and $\phi \in \mathbb{R}^{4}$ is the orientation vector. The orientation vector was described using Euler parameters (unit quaternions), denoted as $\phi={\eta, \varepsilon}$, where $\eta \in \mathbb{R}$ is the scalar part of the quaternion and $\varepsilon \in \mathbb{R}^{3}$ the vector part.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2.2. Learning Adaptive-Compliance Control</h1>
<h3>2.2.1. Reinforcement-Learning Algorithm</h3>
<p>Robotic reinforcement learning is a control problem where a robot, the agent, acts in a stochastic environment by sequentially choosing actions over a sequence of time steps. The goal is to maximize a cumulative reward. Said problem was modeled as a Markov decision process. The environment is described by a state $\mathbf{s} \in \mathbb{S}$. The agent can perform actions $\mathbf{a} \in A$, and perceives the environment through observations $\mathbf{o} \in O$ that may or not be equal to $\mathbf{s}$. We considered an episodic interaction of finite time steps with a limit of $T$ time steps per episode. The agent's goal is to find a policy $\pi(\mathbf{a}(t) \mid \mathbf{o}(t))$ that selects actions $\mathbf{a}(t)$ conditioned on observations $\mathbf{o}(t)$ to control the dynamical system. Given stochastic dynamics $p(\mathbf{s}(t+1) \mid \mathbf{s}(t), \mathbf{a}(t))$ and reward function $r(\mathbf{s}, \mathbf{a})$, the aim is to find a policy $\pi *$ that maximizes the expected sum of future rewards given by $R(t)=\sum_{t}^{\infty} \gamma r(s(t), a(t))$, with $\gamma$ being a discount factor [7].</p>
<p>In this work, we used an RL algorithm called soft actor critic (SAC), which is one of the state-of-the-art algorithms with high sample efficiency, ideal for real robotic applications. SAC [37] is an off-policy actor-critic deep RL algorithm based on maximal entropy. SAC aims to maximize the expected reward while also optimizing maximal entropy. The SAC agent optimizes a maximal-entropy objective, which encourages exploration according to a temperature parameter $\alpha$. The core idea of this method is to succeed at the task while acting as randomly as possible. Since SAC is an off-policy algorithm, it uses a replay buffer to reuse information from recent rollouts for sample-efficient training. Additionally, we used the distributed prioritized experience replay approach for further improvement [38]. Our implementation of the SAC algorithm was based on the TF2RL repository ${ }^{2}$.</p>
<h3>2.2.2. Multimodal Policy Architecture</h3>
<p>The control policy was represented using neural networks, as shown in Figure 3. The policy input was the robot state. The robot state included the proprioception information of the manipulator and haptic information. Proprioception included the pose error between the current robot's end-effector position and predicted target pose $\mathbf{x}<em g="g">{e}$, end-effector velocity $\dot{\mathbf{x}}$, desired insertion force $F</em>$. Proprioception feedback was encoded with a neural network with 2 fully connected layers with activation function RELU to produce a 32-dimensional feature vector. For force-torque feedback, we considered the last 12 readings from the six-axis F/T sensor, filtered using a low-pass filter, as a $12 \times 6$ time series:}$, and actions taken in the previous time step $\mathbf{a}_{t-1</p>
<p>$$
\left[F_{e x t}^{0}, \ldots, F_{e x t}^{12}\right], \quad \text { where } \quad F_{e x t}^{i}=\left[F_{x}, F_{y}, F_{z}, M_{x}, M_{y}, M_{z}\right]
$$</p>
<p>The F/T time series was fed to a temporal convolutional network (TCN) [39] to produce another 32-dimensional feature vector. The feature vectors from proprioception and haptic information were concatenated to obtain a 64-dimensional feature vector, and then fed to two fully connected layers to predict the next action.</p>
<p>The policy outputs actions for a parallel position-force controller. The policy produces two type of actions, $\mathbf{a} \doteq\left[\mathbf{a}<em p="p">{x}, \mathbf{a}</em>}\right]$, where $\mathbf{a<em p="p">{x}=[\mathbf{p}, \phi]$ are position/orientation subgoals, and $\mathbf{a}</em>$ are described in Section 2.2.3.}$ are parameters of the parallel controller. The specific parameters controlled by $\mathbf{a}_{p</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Control policy consisting of three networks. First, proprioception information is processed through a 2-layer neural network. Second, force/torque information is processed with a temporal convolutional network. Lastly, extracted features from first two networks are concatenated and processed on a 2-layer neural network to predict actions.</p>
<h1>2.2.3. Compliance Control in Task Space</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Adaptive parallel position-force control scheme [24]. Inputs are the estimated goal position, policy actions, and a desired contact force. Controller outputs the joint position commands for the robotic arm.</p>
<p>Our proposed method uses a common force-control scheme combined with a reinforcement-learning policy to learn contact-rich manipulations with a rigid position-controlled robot. For the family of contact-rich manipulation tasks that require some sort of insertion, the parallel position-force control [40] performs better and can be learned faster than using an admittance control scheme when combined with an RL policy [24].</p>
<p>The implemented parallel controller is depicted in Figure 4. A PID parallel position-force control was used with the addition of a selection matrix to define the degree of the control of position and force over each direction. The control law consisted of a PD action on position, a PI action on force, a selection matrix, and policy position action $\mathbf{a}_{x}$,</p>
<p>$$
\mathbf{x}<em p="p">{c}=S\left(K</em>}^{x} \mathbf{x<em d="d">{e}+K</em>}^{x} \mathbf{x<em x="x">{e}\right)+\mathbf{a}</em> d t\right)
$$}+(I-S)\left(K_{p}^{f} F_{e}+K_{i}^{f} \int F_{e</p>
<p>where $F_{e}=F_{g}-F_{e x t}$, and $\mathbf{x}_{x}$ is the commanded positions to the robot. The selection matrix is</p>
<p>$$
S=\operatorname{diag}\left(s_{1}, \ldots, s_{6}\right), \quad s_{j} \in[0,1]
$$</p>
<p>where values correspond to the degree of control that each controller has over a given direction.
Our parallel control scheme had a total of 30 parameters, 12 from the position PD controller's gains, 12 from the force PI controller's gains, and 6 from selection matrix $S$. We reduced the number of controllable parameters to prevent unstable behavior and to reduce system complexity. For the PD controller, only proportional gain $K_{p}^{x}$ was controllable, while derivative gain $K_{d}^{x}$ was computed on the basis of $K_{p}^{x} . K_{d}^{x}$ was set to have a critically damped relationship as</p>
<p>$$
K_{d}^{x}=2 \sqrt{K_{p}^{x}}
$$</p>
<p>Similarly, for the PI controller, only proportional gain $K_{p}^{f}$ was controllable, and integral gain $K_{i}^{f}$ was computed with respect to $K_{p}^{f}$. In our experiments, $K_{i}^{f}$ was empirically set to be $1 \%$ of $K_{p}^{f}$. In total, 18 parameters were controllable. In summary, the policy actions regarding the parallel controller's parameters are</p>
<p>$$
\mathbf{a}<em p="p">{p}=\left[K</em>
$$}^{x}, K_{p}^{f}, S\right], \quad \mathbf{a}_{p} \in \mathbb{R}^{18</p>
<p>As a safety measure, we narrowed the agent choices for the force-control parameters by imposing upper and lower limits to each parameter. assuming we had access to some baseline gain values $P_{\text {base }}$. We defined a range of potential values for each parameter as $\left[P_{\text {base }}-P_{\text {range }}, P_{\text {base }}+P_{\text {range }}\right]$ with constant $P_{\text {range }}$ defining the size of the range. We mapped policy actions $\mathbf{a}<em _base="{base" _text="\text">{p}$ from range $[-1,1]$ to each parameter's range. $P</em>$ are the hyperparameters of our method.}}$ and $P_{\text {range }</p>
<h1>2.3. Task's reward function</h1>
<p>For all considered insertion tasks, the same reward function was used:</p>
<p>$$
r(\mathbf{s}, \mathbf{a})=w_{1} L_{m}\left(\left|\left(F_{e x t}-F_{g}\right) / F_{\max }\right|<em 2="2">{2}\right)+w</em> \kappa
$$</p>
<p>where $F_{g}$ is the desired insertion force, $F_{e x t}$ is the contact force, and $F_{\max }$ is the defined allowed maximal contact force. $L_{m}(y)=y \mapsto x, x \in[1,0]$ is a linear mapping in the range 1 to 0 ; thus, the closer to the goal and the lower the contact force, the higher the obtained reward. $|\cdot|_{1,2}$ is an L1,2 norm based on [9]. $\kappa$ is a reward defined as follows:</p>
<p>$$
\kappa= \begin{cases}100+((1-t / T) * 100), &amp; \text { Task completed } \ -50, &amp; \text { Collision } \ 0, &amp; \text { Otherwise }\end{cases}
$$</p>
<p>During training, the task was considered completed if the Euclidean distance between the robot's end-effector position and the true goal position was less than 1 mm . The agent was encouraged to complete the task as quickly as possible by providing an extra reward for every unused time step with respect to the maximal number of time steps per episode $T$. Moreover, we imposed a collision constraint where the agent was penalized for colliding with the environment by giving it a negative reward and by finishing the episode early. This collision constraint encourages safer exploration, as shown in our previous work [24]. We defined a collision as exceeding force limit $F_{\max }$. Therefore, a collision detector and geometric knowledge of the environment were not necessary. Lastly, each component was weighted via $w$; all $w$ s were hyperparameters.</p>
<h1>2.4. Speeding Up Learning</h1>
<p>Two strategies were adopted to speed up the learning process. First, the exploitation of prior knowledge using the idea of residual reinforcement learning. Second, we used a physics simulator to train the robot on a peg-insertion task and transfer the learned policy directly to the real robot (sim2real).</p>
<h3>2.4.1. Residual Reinforcement Learning</h3>
<p>To speed up the learning of the control policy for insertion tasks that require complex manipulation, we used residual reinforcement learning [41,42]. The goal is to leverage the training process by exploiting prior knowledge. With the assumption of an estimated target position or a reference trajectory, we could manually define a controller $\mathbf{x}<em x="x">{g}$. Then, said controller's signal would be combined with policy action $\mathbf{a}</em>$. The objective was to avoid training the policy from scratch, and avoid the exploration of the entire parameter space. The position command sent to the robot was</p>
<p>$$
\mathbf{x}<em g="g">{c}=\left(\mathbf{x}</em>}^{\prime}+\mathbf{x<em x="x">{f}\right)+\mathbf{a}</em>
$$</p>
<p>where $\mathbf{x}<em x="x">{g}^{\prime}$ is the reference trajectory process through a PD controller, $\mathbf{a}</em>$ is the response to the contact force, as shown in Figure 4. The first two terms came from the parallel controller. Therefore, the policy would just need to learn to adjust the reference trajectory to achieve the task.}$ is the policy signal on the position, and $\mathbf{x}_{f</p>
<h3>2.4.2. Sim2real</h3>
<p>The proposed method works on the robot's end-effector Cartesian task-space, which makes it easier to transfer learning from simulation to the real robot or even between robots [43]. For most insertion tasks, a simple peg-insertion task was used for training on a physics simulator. We used simulator Gazebo 9 [44]. To close the reality gap between the physics simulator and real-world dynamics, we used domain randomization [45]. During training on the simulator, the following aspects were randomized:</p>
<ul>
<li>Initial/goal end-effector position: having random initial/goal positions helps the RL algorithm to find policies that generalize to a wide range of initial-position conditions.</li>
<li>Object-surface stiffness: The RL agent also needs to learn to fine-tune the force-controller parameters to obtain a proper response to the contact force. Therefore, randomizing the stiffness of the manipulated objects helps it find policies that adapt to different dynamic conditions.</li>
<li>Uncertainty error of goal pose prediction: On a real robot, the prediction of the target pose comes from noisy sensory information, either from a vision-detection system or from known prior manipulations (grasp and regrasp). Thus, during training on the simulation, we emulated this error by using normal Gaussian distribution with mean zero and standard deviation of a maximal distance error (for position and orientation).</li>
<li>Desired insertion force: For different insertion tasks, a specific contact force is necessary for insertion to succeed. As we considered insertion force an input to the policy, during training, we randomized this value for each episode.</li>
</ul>
<h2>3. Experiments and results</h2>
<h3>3.1. Experiment Setup</h3>
<p>Experimental validation was performed on a simulated environment using Gazebo simulator [44] version 9, and on real hardware using a Universal Robot 3 e-series with a control frequency of up to 500 Hz . The robotic arm had a force/torque sensor mounted at its end effector, and a Robotiq Hand-e</p>
<p>parallel gripper. In both environments, training of the RL agent was performed on a computer with an Intel i9-9900k CPU and Nvidia RTX-2800 Super GPU. To control the robot agent, we used the Robot Operating System (ROS) [46] with the Universal Robot ROS Driver ${ }^{3}$. The experiment environment on the real robot is shown in Figure 5.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Real experiment environment with a 6-degree-of-freedom UR3e robotic arm. Cuboid peg and task board hole had a nonsmooth surface with 1.0 mm clearance.</p>
<h1>3.2. Training</h1>
<p>During the training phase, the agent's task was to insert a cuboid peg into a task board on the simulated environment. The agent was trained for 500, 000 time steps, which, on average, takes about 5 hours to complete. During training, the environment was modified after each episode by randomizing one or several of the training conditions mentioned in Section 2.4.2. The range of values used for the randomization of the training conditions is shown in Table 1. The random goal position was selected from a defined set of possible insertion planes, as depicted in Figure 6.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Simulation environment.
Overlay of randomizable goal positions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Condition</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Value range</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Initial position <br> (relative to goal)</td>
<td style="text-align: center;">Position (mm)</td>
<td style="text-align: center;">$[-400,400]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Orientation $\left({ }^{\circ}\right)$</td>
<td style="text-align: center;">$[-10,10]$</td>
</tr>
<tr>
<td style="text-align: center;">Uncertainty error</td>
<td style="text-align: center;">Position (mm)</td>
<td style="text-align: center;">$[-2,2]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Orientation $\left({ }^{\circ}\right)$</td>
<td style="text-align: center;">$[-5,5]$</td>
</tr>
<tr>
<td style="text-align: center;">Desire insertion force (N)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$[0,10]$</td>
</tr>
<tr>
<td style="text-align: center;">Stiffness <br> (in Gazebo: surface/friction/ode/kp)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\left[7.0 \times 10^{-4}, 1.0 \times 10^{-5}\right]$</td>
</tr>
</tbody>
</table>
<p>Table 1. Randomized training conditions.</p>
<p>Table 1. Randomized training conditions.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>After training on the simulation, the learned policy was refined by retraining on the real robot for 3\% off the simulation time steps, which took about 20 minutes, to further account for the reality gap between simulated and real-world physics dynamics.</p>
<h1>3.3. Evaluation</h1>
<p>The learned policy was initially evaluated on the real robot with a 3D-printed version of the cuboid peg in the hole-insertion task with the true goal pose. During evaluation, observations and actions were recorded. Figure 7 shows the performance of the learned policy (sim2real + retrain). The figure shows the relative position of the end effector with respect to the goal position, the contact force, and the actions taken by the policy for each Cartesian direction normalized to the range of $[-1,1]$, as described in Section 2.2.3. As shown in Fig. 1, the insertion direction was aligned with the y axis of the robot's coordinate system. In Figure 7, we highlighted three phases of the task. Blue corresponds to the search phase in free space before contact with the surface, yellow is the search phase after initial contact with the environment, and green corresponds to the insertion phase. During the search phase, and particularly on the insertion direction (y axis), we could clearly observe that the learned policy properly reacted to contact with the environment by quickly adjusting the force control parameters. On top of that, during the insertion phase, the learned policy changed its strategy from just minimizing contact force to a mostly position-control strategy to complete insertion. This behavior is proper for this particular insertion task, as there is little resistance during the insertion phase, but it is not the desired behavior for other insertion tasks, as we discuss later in Section 3.4.1.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Performance of learned policy (sim2real + retrain) on 3D-printed cuboid-peg-insertion task. Insertion direction was aligned with y axis of robot coordinate system. Relative distance from robot's end effector to goal position and contact force is shown. The 24 policy actions besides the corresponding axis are also shown.</p>
<p>Additionally, we compared the performance of the learned policy as a combination of sim2real and refinement on the real robot versus just learning on the real robot or just directly transferring the learned policy from the simulation (sim2real) without further training. We evaluated these policies on a</p>
<p>3D-printed version of the cuboid-peg-insertion task. Policies were tested 20 times with a random initial position assuming a perfect estimation of the goal position (true goal). Table 2 shows the results of the evaluation. The three policies had a very high success rate, but the policy transfer from the simulation had difficulty with the real-world physics dynamics. As expected, the policy retrained from the simulation gave the best overall performance time.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Success Rate</th>
<th style="text-align: center;">Avg. Time Steps</th>
<th style="text-align: center;">Avg. Time (sec)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Scratch</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">109.6</td>
<td style="text-align: center;">5.48</td>
</tr>
<tr>
<td style="text-align: center;">Sim2real</td>
<td style="text-align: center;">$95 \%$</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">3.77</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">3.28</td>
</tr>
</tbody>
</table>
<p>Table 2. Comparison of learning from scratch, straightforward sim2real, and sim2real + retraining (Ours). Test performed on a 3D printed cuboid peg insertion task assuming knowledge of the true goal position.</p>
<h1>3.4. Generalization</h1>
<p>Now, to evaluate the generalization capabilities of our proposed learning framework, we use a series of environments with varying conditions.</p>
<h2>Varying degrees of Uncertainty error</h2>
<p>First, the learned policies are evaluated on the 3D printed cuboid peg insertion task where there is a degree of error on the estimation of the goal position. To clearly compare the performance of the different methods with different degrees of estimation error, we added and offset of position or orientation about the x -axis of the true goal pose. Nevertheless, for completeness we also evaluate the policies on goal poses with added random offset of translation, $[-1,1]$ millimeters, and orientation, $\left[-5^{\circ}, 5^{\circ}\right]$, on all directions. On each case, the policies were tested 20 times from random initial positions. Results are shown in Table 3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Estimation error / Success rate</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Position</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Orientation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">$\mathbf{1 ~ m m}$</td>
<td style="text-align: center;">$\mathbf{2 ~ m m}$</td>
<td style="text-align: center;">$\mathbf{3 ~ m m}$</td>
<td style="text-align: center;">$\mathbf{4 ~ m m}$</td>
<td style="text-align: center;">$\mathbf{5 ~ m m}$</td>
<td style="text-align: center;">$\mathbf{1}^{\circ}$</td>
<td style="text-align: center;">$\mathbf{2}^{\circ}$</td>
<td style="text-align: center;">$\mathbf{3}^{\circ}$</td>
<td style="text-align: center;">$\mathbf{4}^{\circ}$</td>
<td style="text-align: center;">$\mathbf{5}^{\circ}$</td>
</tr>
<tr>
<td style="text-align: left;">Scratch</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$55 \%$</td>
<td style="text-align: center;">$35 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$50 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Sim2real</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$85 \%$</td>
<td style="text-align: center;">$75 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$30 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{9 5 \%}$</td>
<td style="text-align: center;">$\mathbf{6 5 \%}$</td>
<td style="text-align: center;">$\mathbf{6 0 \%}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 3. Comparison of learning from scratch, straightforward sim2real and sim2real + retraining (Ours) with different degrees of goal-position uncertainty error. Test performed during 3D-printed cuboid-peg insertion task.</p>
<p>In all cases, the policy learned from the simulation with domain randomization and fine-tuned on the real robot gave the best results. If the difference between the physics dynamics on the simulation and the real world was too big, learning from scratch could yield better results than only transferring the policy from the simulation, as can be seen when the uncertainty error on orientation was too big $\left(5^{\circ}\right)$; where the friction with the environment makes the task much harder, such contact dynamics are difficult to simulate.</p>
<h2>Varying Environment Stiffness</h2>
<p>Second, the learned policy was also evaluated on different stiffness environments. Figure 8 shows the 3 environments considered for evaluation. High stiffness was the default environment. Medium stiffness was achieved by using a rubber band to hold the cuboid peg between the gripper fingers, adding</p>
<p>a degree of static compliance. In addition to that, for the low-stiffness environment, a soft foam surface was added to further decrease stiffness. The policies were evaluated from 20 different initial positions, results are reported in Table 4.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. (left to right) High-, medium-, and low-stiffness environments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method/Stiffness</th>
<th style="text-align: center;">High</th>
<th style="text-align: center;">Medium</th>
<th style="text-align: center;">Low</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Scratch</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$40 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Sim2real</td>
<td style="text-align: center;">$95 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4. Success rate of 3D-printed-cuboid insertion task with different degrees of contact stiffness.</p>
<h1>3.4.1. Varying Insertion Tasks</h1>
<p>Lastly, we evaluate the learned policy on a series of novel insertion tasks, none seen during training, to assess its generalization capabilities. These insertion tasks included challenges such as adapting to a very hard surface (high stiffness), requiring a minimal insertion force to perform the insertion, and a complex peg shape for mating the parts. The different insertion scenarios are depicted in Figure 9.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Several insertion tasks with different degrees of complexity. (A) Metal ring (high stiffness) with 0.2 mm of clearance. (B) Electric outlet requiring high insertion force. (C) Local-area-network (LAN) port, delicate with complex shape. (D) Universal serial bus (USB).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Success rate</th>
<th style="text-align: center;">Insertion force</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Ring</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">5 N</td>
</tr>
<tr>
<td style="text-align: center;">Electric Outlet (x)</td>
<td style="text-align: center;">$75 \%$</td>
<td style="text-align: center;">10 N</td>
</tr>
<tr>
<td style="text-align: center;">Electric Outlet (y)</td>
<td style="text-align: center;">$75 \%$</td>
<td style="text-align: center;">10 N</td>
</tr>
<tr>
<td style="text-align: center;">LAN port (x)</td>
<td style="text-align: center;">$55 \%$</td>
<td style="text-align: center;">5 N</td>
</tr>
<tr>
<td style="text-align: center;">LAN port (y)</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">5 N</td>
</tr>
<tr>
<td style="text-align: center;">USB</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">8 N</td>
</tr>
</tbody>
</table>
<p>Table 5. Success rate of learned policy on several insertion tasks.</p>
<p>For each task, the learned policy was executed 20 times from random initial positions and assuming perfect estimation of the goal position. Table 5 shows the success rate of the learned policy on these novel tasks, along with the desired insertion force set for each task. As the insertion force was defined as a policy input, we could define specific desired insertion force for each task. Even though the policy was only trained by using the simpler cuboid-peg insertion task, mainly in simulation and shortly refined on</p>
<p>a real robot with a 3D-printed version of the same task, the learned policy achieved a high success rate in novel and complex insertion tasks.</p>
<p>Compared to the cuboid-peg insertion task, on these novel insertion tasks, the peg was more likely to become stuck during the task's search phase, as the surrounding surface near the hole was not smooth and may have had crevices. The extra challenges were not present during the training phase, which reduced the capability of the learned policy to react in an appropriate way. The insertion task of the LAN port was the most challenging for the policy due to the complex shape of the LAN cable endpoint. If just one corner of the LAN adapter was stuck, the insertion could not be completed even if large force was applied.</p>
<p>Additionally, we tested the policy on different insertion planes for the electric outlet and the LAN port tasks. In both cases, success rate was similar due to training with the randomized insertion planes. However, the policy was slightly better with insertions on the y-axis plane due to retraining (on the real robot) only being done on this axis.</p>
<h1>3.5. Ablation Studies</h1>
<p>In this section, we evaluate the individual contribution of some components added to the proposed learning framework.</p>
<h3>3.5.1. Learning from Scratch vs Sim2real</h3>
<p>The inclusion of transfer learning from the simulation to the real robot for the proposed learning framework was evaluated. We compared the learning performance of training the agent on the real robot from scratch versus learning starting from a policy learned on simulation. Training from scratch was performed for 50,000 steps, while retraining from the simulation lasted 15,000 steps. Figure 10 shows the learning curve for both training sessions. Learning from scratch required at least 50,000 steps to succeed at the tasks most of the time. In contrast, learning from the pretrained policy on the simulation achieved the same performance in under 5000 steps. The policy from the simulation still required some training to fine-tune the controller to real-world physics dynamics, which are difficult to simulate, as can be seen from the slow start and the drops in cumulative reward.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Comparison between learning from scratch and learning from a policy learned on simulation: learning curve for 3D-printed cuboid-peg insertion task on real robot with random initial positions.</p>
<h1>3.5.2. Policy Architecture</h1>
<p>We evaluated the contribution of the policy architecture introduced in our method (see Section 2.2.2) by comparing it to a policy with a simple neural network (NN) with two fully connected layers as used in previous work [24]. We trained both policies on the cuboid-peg insertion task on the simulation and compared their learning performance. Figure 11 shows the learning curve of both policy architectures for a training session of 70,000 time steps. From the figure, is clear that, with our newly proposed TCN-based policy, the agent was able to learn faster and exploit better rewards. The TCN-based policy learned a successful policy $(25,000)$ about 15,000 steps faster than the simple neural-network (NN)-based policy did $(40,000)$. Additionally, the TCN-based policy converged to a higher cumulative reward than that of the simple NN-based policy.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Comparison between policy architectures: learning curve for cuboid-peg insertion task with random initial positions.</p>
<h3>3.5.3. Policy Inputs</h3>
<p>Lastly, we evaluated the choice of inputs for the policy. We compared our proposed policy architecture with all inputs, as defined in Section 2.2.2, with two variants. First, we considered the policy without the inclusion of prior action $\mathbf{a}<em g="g">{t-1}$. Second, we considered the policy without knowledge of desired insertion force $F</em>$ as input, the cost function still accounted for the desired insertion force.}$. The training environment was the cuboid-peg insertion task on the simulation with a random initial position and random desired insertion force. In the case of the policy that did not have $F_{g</p>
<p>Figure 12 shows the comparison of the learning curves. Most notable is the poor performance of the policy that lacked the knowledge of prior action $\mathbf{a}<em g="g">{t-1}$. Prior-action information is critical for the agent to more quickly converge to an optimal policy. Additionally, knowledge of $F</em>$ enables the agent to find policies that yield higher cumulative rewards, and to learn faster.</p>
<h2>4. Discussion</h2>
<p>We proposed a learning framework for position-controlled robot manipulators to solve contact-rich manipulation tasks. The proposed method allows for learning low-level high-dimensional control policies on real robotic systems. The effectiveness of the learned policies was shown through an extensive experiment study. We showed that the learned policies had a high success rate at performing the insertion task under the assumption of a perfect estimation of the goal position. The policy correctly learned the nominal trajectory and the appropriate force-control parameters to succeed at the task. The policy also</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12. Comparison of policies with different inputs. Learning curve for cuboid-peg insertion task with random initial positions and random desired insertion force.
achieved a high success rate under varying environmental conditions in terms of uncertainty of goal position, environmental stiffness, and novel insertion tasks.</p>
<p>While model free reinforcement-learning algorithm SAC was used in this work, the proposed framework can easily be adapted to other RL algorithms. The choice of SAC was due to its sample efficiency as an off-policy algorithm. The pros and cons of using other learning algorithms would be interesting future work.</p>
<p>One limitation of our learning framework is the selection of the force-control parameter range (see Section 2.2.3). The choice of a wide range of values may allow for the policy to adapt to very different environments, but it also increases the difficulty of learning a task, as small variations in the action may cause undesired behaviors, as was the case during the first 20,000 to 30,000 steps of training (see Figure 11). On the other hand, a narrow range would make it easier and faster to learn a task, but it may not generalize well to different environments. Defining a range is much easier than manually finding the optimal parameters for each task, but it is still a manual process. Therefore, another interesting future study would be to use demonstrations to learn a rough estimation of the optimal force parameters to further reduce training times.</p>
<h1>References</h1>
<ol>
<li>Kroemer, O.; Niekum, S.; Konidaris, G. A review of robot learning for manipulation: Challenges, representations, and algorithms. arXiv preprint arXiv:1907.03146 2019.</li>
<li>Whitney, D.E. Quasi-Static Assembly of Compliantly Supported Rigid Parts. Journal of Dynamic Systems, Measurement, and Control 1982, 104, 65-77. doi:10.1115/1.3149634.</li>
<li>Tsuruoka, T.; Fujioka, H.; Moriyama, T.; Mayeda, H. 3D analysis of contact in peg-hole insertion. Proceedings of the 1997 IEEE International Symposium on Assembly and Task Planning (ISATP'97)-Towards Flexible and Agile Assembly and Manufacturing-. IEEE, 1997, pp. 84-89.</li>
<li>Zhang, K.; Shi, M.; Xu, J.; Liu, F.; Chen, K. Force control for a rigid dual peg-in-hole assembly. Assembly Automation 2017.</li>
<li>Fukumoto, Y.; Harada, K. Force Control Law Selection for Elastic Part Assembly from Human Data and Parameter Optimization. 2018 IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids). IEEE, 2018, pp. 1-7.</li>
<li>
<p>Kyrarini, M.; Haseeb, M.A.; Ristić-Durrant, D.; Gräser, A. Robot learning of industrial assembly task via human demonstrations. Autonomous Robots 2019, 43, 239-257.</p>
</li>
<li>
<p>Sutton, R.S.; Barto, A.G. Reinforcement learning: An introduction, 2nd ed.; MIT press, 2018.</p>
</li>
<li>Yang, C.; Zeng, C.; Cong, Y.; Wang, N.; Wang, M. A learning framework of adaptive manipulative skills from human to robot. IEEE Transactions on Industrial Informatics 2018, 15, 1153-1161.</li>
<li>Levine, S.; Pastor, P.; Krizhevsky, A.; Ibarz, J.; Quillen, D. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. The International Journal of Robotics Research 2018, $37,421-436$.</li>
<li>Pinto, L.; Gupta, A. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. 2016 IEEE international conference on robotics and automation (ICRA). IEEE, 2016, pp. 3406-3413.</li>
<li>Gu, S.; Holly, E.; Lillicrap, T.; Levine, S. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. 2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017, pp. 3389-3396.</li>
<li>Nuttin, M.; Van Brussel, H. Learning the peg-into-hole assembly operation with a connectionist reinforcement technique. Computers in Industry 1997, 33, 101-109.</li>
<li>Inoue, T.; De Magistris, G.; Munawar, A.; Yokoya, T.; Tachibana, R. Deep reinforcement learning for high precision assembly tasks. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 819-825.</li>
<li>Lillicrap, T.P.; Hunt, J.J.; Pritzel, A.; Heess, N.; Erez, T.; Tassa, Y.; Silver, D.; Wierstra, D. Continuous control with deep reinforcement learning. ICLR (Poster), 2016.</li>
<li>Xu, J.; Hou, Z.; Wang, W.; Xu, B.; Zhang, K.; Chen, K. Feedback deep deterministic policy gradient with fuzzy reward for robotic multiple peg-in-hole assembly tasks. IEEE Transactions on Industrial Informatics 2018, $15,1658-1667$.</li>
<li>Silver, D.; Lever, G.; Heess, N.; Degris, T.; Wierstra, D.; Riedmiller, M.A. Deterministic Policy Gradient Algorithms. ICML, 2014.</li>
<li>Fan, Y.; Luo, J.; Tomizuka, M. A learning framework for high precision industrial assembly. 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 811-817.</li>
<li>Levine, S.; Koltun, V. Guided policy search. International Conference on Machine Learning, 2013, pp. 1-9.</li>
<li>Luo, J.; Solowjow, E.; Wen, C.; Ojea, J.A.; Agogino, A.M. Deep reinforcement learning for robotic assembly of mixed deformable and rigid objects. 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 2062-2069.</li>
<li>Ren, T.; Dong, Y.; Wu, D.; Chen, K. Learning-based variable compliance control for robotic assembly. Journal of Mechanisms and Robotics 2018, 10.</li>
<li>Buchli, J.; Stulp, F.; Theodorou, E.; Schaal, S. Learning variable impedance control. The International Journal of Robotics Research 2011, 30, 820-833.</li>
<li>Lee, M.A.; Zhu, Y.; Srinivasan, K.; Shah, P.; Savarese, S.; Fei-Fei, L.; Garg, A.; Bohg, J. Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks. 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 8943-8950.</li>
<li>Abu-Dakka, F.J.; Nemec, B.; Jørgensen, J.A.; Savarimuthu, T.R.; Krüger, N.; Ude, A. Adaptation of manipulation skills in physical contact with the environment to reference force profiles. Autonomous Robots 2015, 39, 199-217.</li>
<li>Beltran-Hernandez, C.C.; Petit, D.; Ramirez-Alpizar, I.G.; Nishi, T.; Kikuchi, S.; Matsubara, T.; Harada, K. Learning Force Control for Contact-rich Manipulation Tasks with Rigid Position-controlled Robots. IEEE Robotics and Automation Letters 2020, pp. 1-1.</li>
<li>Chebotar, Y.; Handa, A.; Makoviychuk, V.; Macklin, M.; Issac, J.; Ratliff, N.; Fox, D. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 8973-8979.</li>
<li>
<p>Andrychowicz, O.M.; Baker, B.; Chociej, M.; Jozefowicz, R.; McGrew, B.; Pachocki, J.; Petron, A.; Plappert, M.; Powell, G.; Ray, A.; others. Learning dexterous in-hand manipulation. The International Journal of Robotics Research 2020, 39, 3-20.</p>
</li>
<li>
<p>Sharma, K.; Shirwalkar, V.; Pal, P.K. Intelligent and environment-independent peg-in-hole search strategies. 2013 International Conference on Control, Automation, Robotics and Embedded Systems (CARE). IEEE, 2013, pp. 1-6.</p>
</li>
<li>Zakharov, S.; Shugurov, I.; Ilic, S. Dpod: 6d pose object detector and refiner. Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1941-1950.</li>
<li>Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561-4570.</li>
<li>Xiang, Y.; Schmidt, T.; Narayanan, V.; Fox, D. PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes. Robotics: Science and Systems (RSS) 2018, 2018. doi:10.15607/RSS.2018.XIV.019.</li>
<li>Hodan, T.; Haluza, P.; Obdržálek, Š.; Matas, J.; Lourakis, M.; Zabulis, X. T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects. 2017 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2017, pp. 880-888.</li>
<li>Harada, K.; Nakayama, K.; Wan, W.; Nagata, K.; Yamanobe, N.; Ramirez-Alpizar, I.G. Tool exchangeable grasp/assembly planner. International Conference on Intelligent Autonomous Systems. Springer, 2018, pp. 799-811.</li>
<li>Masehian, E.; Ghandi, S. ASPPR: A new Assembly Sequence and Path Planner/Replanner for monotone and nonmonotone assembly planning. Computer-Aided Design 2020, p. 102828.</li>
<li>Nair, A.; McGrew, B.; Andrychowicz, M.; Zaremba, W.; Abbeel, P. Overcoming exploration in reinforcement learning with demonstrations. 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 6292-6299.</li>
<li>Gupta, A.; Kumar, V.; Lynch, C.; Levine, S.; Hausman, K. Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning. Conference on Robot Learning (CoRL) 2019, 2019.</li>
<li>Wang, Y.; Harada, K.; Wan, W. Motion planning of skillful motions in assembly process through human demonstration. Advanced Robotics 2020, 0, 1-15, [https://doi.org/10.1080/01691864.2020.1782260]. doi:10.1080/01691864.2020.1782260.</li>
<li>Haarnoja, T.; Zhou, A.; Abbeel, P.; Levine, S. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. ICML 2018, abs/1801.01290.</li>
<li>Horgan, D.; Quan, J.; Budden, D.; Barth-Maron, G.; Hessel, M.; van Hasselt, H.; Silver, D. Distributed Prioritized Experience Replay. International Conference on Learning Representations, 2018.</li>
<li>Bai, S.; Kolter, J.Z.; Koltun, V. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271 2018.</li>
<li>Chiaverini, S.; Sciavicco, L. The parallel approach to force/position control of robotic manipulators. IEEE Transactions on Robotics and Automation 1993, 9, 361-373.</li>
<li>Johannink, T.; Bahl, S.; Nair, A.; Luo, J.; Kumar, A.; Loskyll, M.; Ojea, J.A.; Solowjow, E.; Levine, S. Residual Reinforcement Learning for Robot Control. 2019 International Conference on Robotics and Automation (ICRA), 2019, pp. 6023-6029. doi:10.1109/ICRA.2019.8794127.</li>
<li>Silver, T.; Allen, K.R.; Tenenbaum, J.B.; Kaelbling, L.P. Residual Policy Learning. ArXiv 2018, abs/1812.06298.</li>
<li>Bellegarda, G.; Byl, K. Training in Task Space to Speed Up and Guide Reinforcement Learning. 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2019, pp. 2693-2699.</li>
<li>Koenig, N.; Howard, A. Design and use paradigms for gazebo, an open-source multi-robot simulator. 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2004, Vol. 3, pp. $2149-2154$.</li>
<li>Tobin, J.; Fong, R.; Ray, A.; Schneider, J.; Zaremba, W.; Abbeel, P. Domain randomization for transferring deep neural networks from simulation to the real world. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017, pp. 23-30.</li>
<li>Quigley, M.; Conley, K.; Gerkey, B.; Faust, J.; Foote, T.; Leibs, J.; Wheeler, R.; Ng, A.Y. ROS: an open-source Robot Operating System. ICRA workshop on open source software. Kobe, Japan, 2009, Vol. 3, p. 5.</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>3 ROS driver for Universal Robot robotic arms developed in collaboration between Universal Robots and the FZI Research Center for Information Technology https://github.com/UniversalRobots/Universal_Robots_ROS_Driver&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2266 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2266</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2266</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-225076506</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2010.14236v2.pdf" target="_blank">Scientific intuition inspired by machine learning generated hypotheses</a></p>
                <p><strong>Paper Abstract:</strong> Machine learning with application to questions in the physical sciences has become a widely used tool, successfully applied to classification, regression and optimization tasks in many areas. Research focus mostly lies in improving the accuracy of the machine learning models in numerical predictions, while scientific understanding is still almost exclusively generated by human researchers analysing numerical results and drawing conclusions. In this work, we shift the focus on the insights and the knowledge obtained by the machine learning models themselves. In particular, we study how it can be extracted and used to inspire human scientists to increase their intuitions and understanding of natural systems. We apply gradient boosting in decision trees to extract human interpretable insights from big data sets from chemistry and physics. In chemistry, we not only rediscover widely know rules of thumb but also find new interesting motifs that tell us how to control solubility and energy levels of organic molecules. At the same time, in quantum physics, we gain new understanding on experiments for quantum entanglement. The ability to go beyond numerics and to enter the realm of scientific insight and hypothesis generation opens the door to use machine learning to accelerate the discovery of conceptual understanding in some of the most challenging domains of science.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2266.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2266.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML-hypothesis-workflow (chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated hypothesis generation workflow using subgraph fingerprints and gradient boosting (chemistry application)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that converts graph-structured molecules into binary subgraph (circular/ECFP) fingerprints, trains gradient-boosting decision tree regressors to predict chemical properties (logP, HOMO energy, HOMO-LUMO gaps, singlet-triplet gaps), extracts feature importances and logically combines important subgraph-features to produce human-interpretable hypotheses about structure–property relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Computational chemistry / molecular property prediction (solubility, frontier orbital energies, singlet-triplet gaps, organic photovoltaics and OLED emitter design)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict and extract interpretable structure–property relations for molecular properties (water–octanol partition coefficient logP; HOMO energies and HOMO–LUMO gaps; singlet–triplet gaps for TADF emitters) and produce human-readable hypotheses (subgraph motifs that increase/decrease targets).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Datasets used include large computational chemistry collections: Harvard Clean Energy Project (large-scale computed quantum chemistry dataset), a non-fullerene acceptor dataset, and a TADF molecules dataset; data are labeled (computed target properties) and available to the authors (data available on request). Overall data amount: moderate-to-large (Harvard CEP is large-scale), quality: computational quantum-chemistry labels (consistent), accessibility: available upon request, not fully public in-paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph-structured molecular data converted to binary high-dimensional subgraph fingerprints (extended-connectivity/circular fingerprints), i.e., sparse/bit-vector representations encoding presence/absence of subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: properties arise from complex quantum interactions and non-linear combinations of structural motifs; high-dimensional feature space from many possible subgraphs; higher-order (combinatorial) feature interactions matter (requires logical combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mixed: solubility and basic orbital-level rules are relatively well understood with many established heuristics; frontier-gap tuning for photovoltaic and TADF systems is less mature and has less established intuition.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - interpretability is required for scientific insight and hypothesis generation rather than only accurate black-box predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Gradient Boosting Decision Trees on subgraph (ECFP) fingerprints with feature-importance extraction and logical feature combination</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Molecules represented by circular extended-connectivity fingerprints (subgraph hashing to bit-vectors). A gradient-boosting tree ensemble (Friedman's algorithm) is trained in supervised regression mode to predict target properties. Feature importances are extracted from the tree ensemble; top features (bits/subgraphs) are analyzed and combined using logical operations (AND, XOR, NOT) to capture higher-order correlations and generate human-readable hypotheses of the form 'subgraph i -> increase/decrease of property by strength s'. The workflow emphasizes model interpretability rather than raw predictive score.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (interpretability-focused tree-ensemble regression)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate: the approach maps naturally to molecular graph data and emphasizes interpretability, suitable for domains where mechanistic insight is important; limitations include dependence on the fingerprint encoding and the need for sufficient labeled examples to detect motif correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective: the workflow 'rediscovered' well-known chemical rules-of-thumb (e.g., carbonyls increase water solubility, conjugated chains increase logP; electron-donating/withdrawing groups influence HOMO energies) and found new, less-expected motifs (e.g., silole rings correlating with reduced HOMO–LUMO gaps, conjugated bridges increasing singlet–triplet gaps). It produced directly interpretable subgraph hypotheses; limitations include that high feature importance does not imply direct causal effect and combinations of features were often required.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential to accelerate conceptual chemical understanding, to suggest novel design motifs for organic electronics and OLED/TADF materials, and to reduce bias from human preconceptions; can prioritize experiments and guide molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper discusses alternative explainability approaches (graph convolutional nets, Grad-CAM, GNNExplainer) but does not perform a numerical performance comparison; authors argue tree-based fingerprint approach offers more direct, binary, human-interpretable subgraph signals compared to per-node/edge saliency maps.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of sizable labeled computational datasets; natural graph structure of molecules; use of binary subgraph fingerprints that map directly to chemical motifs; choice of interpretable model (tree ensemble) allowing feature-importance extraction; combination logic to capture higher-order interactions; domain expert interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Interpretable tree ensembles trained on subgraph fingerprint representations can both accurately capture complex, nonlinear structure–property correlations in chemistry and produce human-comprehensible hypotheses that rediscover known chemical intuition and reveal new, experimentally actionable motifs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2266.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2266.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML-hypothesis-workflow (quantum optics)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated hypothesis generation workflow using subgraph fingerprints and gradient boosting (quantum optics application)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same automated workflow (graph → circular fingerprint → gradient-boosting regressor → feature importance → logical combinations) applied to graph-encoded quantum-optical experimental setups to predict entanglement dimensionality (n_Q) and generate interpretable design rules for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Quantum optics / experimental design for high-dimensional multipartite entanglement</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict how substructures of quantum-optical experimental setups (graphs of optical elements and photon paths) affect the overall entanglement dimensionality measure n_Q = log2(d1 d2 d3) and generate interpretable hypotheses to guide experiment design for high-dimensional three-photon entanglement.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Data derived from a database of experimental setups with labels computed from quantum-optical approximations (simulated entanglement dimensionality). Availability: dataset used by the authors (not public in-paper), labeled by simulation; amount: moderate (collection of many candidate setups and variants).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph-structured experimental designs (nodes = optical elements, edges = photon paths) converted to binary subgraph fingerprints (bit-vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: quantum interference and entanglement involve counter-intuitive, highly non-linear phenomena; combinatorial experimental-design search space; interactions depend on global experimental graph context.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging but active: experimental demonstrations exist for high-dimensional entanglement, theoretical models to compute final quantum states are available, yet design intuition is limited and counter-intuitive.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - designers require mechanistic, interpretable insights to guide physical experiments and to avoid misleading intuitive assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Gradient Boosting Decision Trees on subgraph (ECFP-style) fingerprints with feature-importance extraction and logical feature composition</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Experimental setups encoded as graphs were fingerprinted using the same circular hashing algorithm to yield binary features for subgraphs. A gradient-boosting regression model was trained (supervised) to predict n_Q. Feature importances were extracted and combined with logical operations to form macro-features; these machine-generated hypotheses were interpreted by domain experts to identify counter-intuitive design rules (e.g., shifting mode space by ±3 before mixing increases n_Q; direct connection of crystal output to detector reduces n_Q).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (interpretability-focused tree-ensemble regression)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate: the graph-based fingerprint + tree ensemble framework naturally encodes experiment topology and produces interpretable rules; limitations include reliance on simulated labels and potential sparse coverage of rare but important experimental motifs.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective: discovered hypotheses that contradict prevailing human intuition and match experimental practice (and suggested improvements); identified both negatively and positively influencing subgraphs and produced combined logical rules that map to actionable experimental recommendations. Caveats: hypotheses need verification; not all random mutations produce valid quantum states, making automated testing costly.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential: can change experimental design strategies, reveal counter-intuitive yet robust rules (e.g., mode shifting prior to mixing), and accelerate discovery of complex quantum experiments while reducing bias from prior human intuition.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Authors reference other automated / ML-driven experiment design methods (automated search for new quantum experiments, inverse-design), but do not provide a head-to-head empirical comparison; emphasis is on interpretability and human-in-the-loop insight rather than pure optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Graph encoding of experiments enabling subgraph-level features; accurate physical simulation to provide labels; interpretable tree ensemble allowing extraction of causal-looking motifs; logical combination of features to capture conditional effects; expert domain interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Interpretable ML on graph-encoded experimental setups can reveal non-obvious, experimentally actionable rules for constructing high-dimensional entangled states — even overturning field intuition — because tree ensembles on subgraph fingerprints highlight compositional motifs that strongly influence entanglement.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2266.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2266.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ECFP / circular extended-connectivity fingerprints</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extended-connectivity (circular) fingerprints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph fingerprinting algorithm that iteratively hashes node and local-neighborhood information to produce binary features indicating presence/absence of particular subgraphs (used to convert molecules or experimental graphs into interpretable bit-vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extended-connectivity fingerprints</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Graph-structured representations in chemistry and other domains (molecules, experimental setups)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Encode graph-local structural motifs as fixed-length binary vectors (fingerprints) that can be used as input features for ML models to predict properties or extract interpretable subgraph-importance signals.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph → iterative neighborhood aggregation → hashed subgraph bit-vector (sparse high-dimensional binary vector).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high: hashing collates many possible subgraphs into finite bits; choice of radius/iterations trades off local vs more global context; collisions and representation capacity matter.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Well-established and widely used in cheminformatics; standard technique for molecular ML models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium-high when used for interpretability because individual bits map to subgraphs that have chemical meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Circular extended-connectivity fingerprinting (ECFP)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Iterative algorithm that assigns initial identifiers to nodes from categorical node/edge features, then iteratively aggregates neighboring identifiers using hashing to create identifiers for larger-radius subgraphs; final mapping of observed subgraphs to indices produces a binary vector indicating presence/absence of subgraphs. In this work, the same approach was applied to both molecules and optical-experiment graphs to yield binary features suitable for tree ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Feature engineering / representation for supervised ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for graph-structured scientific data where local substructures are meaningful; particularly suitable where interpretability of subgraph presence is desired.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective as an interpretable representation: allowed direct mapping from important bits to chemical/experimental motifs and facilitated hypothesis generation; potential limitations include hashing collisions and limited global-context capture unless radius increased.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High as a bridge between raw graphs and interpretable ML models across chemistry, physics, and other graph domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Authors note alternatives (graph convolutional networks and other GNNs) that learn distributed node embeddings rather than binary subgraph indicators, but argue ECFP yields more direct, human-interpretable features.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Direct mapping from fingerprint bits to concrete subgraphs; compatibility with tree-ensemble feature-importance measures; low preprocessing overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Binary subgraph fingerprints provide a compact, interpretable bridge from graph-structured scientific data to feature-importance-driven ML, enabling direct human-understandable hypotheses about structural motifs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2266.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2266.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNNExplainer (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GNNExplainer: Generating explanations for graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explainability method that finds subgraphs and node features that are most influential for a GNN's prediction on a specific sample, producing saliency-like explanations for graph neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gnnexplainer: Generating explanations for graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Graph neural network interpretability; applicable across molecular property prediction and other graph-based scientific ML tasks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide local explanations (per-sample) for GNN predictions by identifying compact subgraphs and feature subsets that the model relies on.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph data and GNN model internals; explanations are per-sample subgraphs and node/edge feature masks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High in interpretability sense: explanations depend on complex learned distributed representations; generating concise subgraphs requires optimization and can be sample-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging/active research area for explainable GNNs; methods exist but interpretability remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for scientific use — domain scientists often require explanations that map to real-world entities rather than distributed embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>GNNExplainer</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Optimization-based method to find a subgraph and subset of node features that maximally influence a trained GNN's prediction for a single instance; returns importance masks enabling human interpretation of GNN decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Model explainability / post-hoc interpretability (applied to supervised GNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Mentioned as an alternative for extracting interpretable graph-level explanations, but the paper notes that per-sample explanations complicate drawing dataset-level hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Useful for local interpretability but authors argue it provides explanations per sample rather than dataset-level, and yields non-binary importance scores that are harder to convert into simple, general hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for explaining GNN decisions on individual scientific instances, but limited for automated generation of global interpretable hypotheses without aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to the authors' fingerprint + tree approach: GNNExplainer focuses on local explanations and continuous importance scores, while the paper's approach yields dataset-level binary subgraph hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Works when strong per-sample attribution is needed; less ideal for global hypothesis generation across many graphs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2266.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2266.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grad-CAM (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grad-CAM: Visual explanations from deep networks via gradient-based localization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gradient-based localization technique that provides visual saliency maps for convolutional neural networks, highlighting regions important for a prediction; mentioned as an explainability tool that could be used in graph/CNN contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grad-cam: Visual explanations from deep networks via gradient-based localization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Model explainability for deep convolutional models; referenced in context of scientific ML explainability alternatives</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Localize input regions that most influence a deep CNN's prediction via gradient-weighted class activation maps; typically applied to image data but conceptually relevant to other architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Primarily image data and convolutional feature maps; outputs are heatmaps.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: requires differentiable models and access to internal feature maps and gradients; explanations are approximate and local.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Well-established in computer vision; adaptation to graph or molecular domains requires architectural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium - produces visual saliency useful for interpretability but not necessarily mechanistic causal explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Grad-CAM</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Computes gradients of a target output with respect to convolutional feature maps, pools those gradients to obtain weights, and combines them with the feature maps to produce a coarse localization map of important regions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Post-hoc explainability for supervised convolutional models</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Mentioned as an alternative explainability method; authors note such methods analyze single samples and produce non-binary importance maps, making dataset-level hypothesis extraction harder.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective for visual explanations in image-based models; less directly suitable for producing compact dataset-level, human-readable subgraph hypotheses as required in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Useful for per-instance explanation in image-like scientific tasks; limited for automated hypothesis generation from graph datasets without aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Qualitatively compared to the paper's approach: Grad-CAM produces soft per-sample attributions; the paper favors binary subgraph fingerprints and tree ensembles for direct hypothesis extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Works best for convolutional architectures and image tasks where spatial localization aids interpretation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Extended-connectivity fingerprints <em>(Rating: 2)</em></li>
                <li>Greedy function approximation: a gradient boosting machine <em>(Rating: 2)</em></li>
                <li>Convolutional networks on graphs for learning molecular fingerprints <em>(Rating: 2)</em></li>
                <li>Gnnexplainer: Generating explanations for graph neural networks <em>(Rating: 2)</em></li>
                <li>Automated search for new quantum experiments <em>(Rating: 2)</em></li>
                <li>The harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community grid <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2266",
    "paper_id": "paper-225076506",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "ML-hypothesis-workflow (chemistry)",
            "name_full": "Automated hypothesis generation workflow using subgraph fingerprints and gradient boosting (chemistry application)",
            "brief_description": "A pipeline that converts graph-structured molecules into binary subgraph (circular/ECFP) fingerprints, trains gradient-boosting decision tree regressors to predict chemical properties (logP, HOMO energy, HOMO-LUMO gaps, singlet-triplet gaps), extracts feature importances and logically combines important subgraph-features to produce human-interpretable hypotheses about structure–property relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Computational chemistry / molecular property prediction (solubility, frontier orbital energies, singlet-triplet gaps, organic photovoltaics and OLED emitter design)",
            "problem_description": "Predict and extract interpretable structure–property relations for molecular properties (water–octanol partition coefficient logP; HOMO energies and HOMO–LUMO gaps; singlet–triplet gaps for TADF emitters) and produce human-readable hypotheses (subgraph motifs that increase/decrease targets).",
            "data_availability": "Datasets used include large computational chemistry collections: Harvard Clean Energy Project (large-scale computed quantum chemistry dataset), a non-fullerene acceptor dataset, and a TADF molecules dataset; data are labeled (computed target properties) and available to the authors (data available on request). Overall data amount: moderate-to-large (Harvard CEP is large-scale), quality: computational quantum-chemistry labels (consistent), accessibility: available upon request, not fully public in-paper.",
            "data_structure": "Graph-structured molecular data converted to binary high-dimensional subgraph fingerprints (extended-connectivity/circular fingerprints), i.e., sparse/bit-vector representations encoding presence/absence of subgraphs.",
            "problem_complexity": "High: properties arise from complex quantum interactions and non-linear combinations of structural motifs; high-dimensional feature space from many possible subgraphs; higher-order (combinatorial) feature interactions matter (requires logical combinations).",
            "domain_maturity": "Mixed: solubility and basic orbital-level rules are relatively well understood with many established heuristics; frontier-gap tuning for photovoltaic and TADF systems is less mature and has less established intuition.",
            "mechanistic_understanding_requirements": "High - interpretability is required for scientific insight and hypothesis generation rather than only accurate black-box predictions.",
            "ai_methodology_name": "Gradient Boosting Decision Trees on subgraph (ECFP) fingerprints with feature-importance extraction and logical feature combination",
            "ai_methodology_description": "Molecules represented by circular extended-connectivity fingerprints (subgraph hashing to bit-vectors). A gradient-boosting tree ensemble (Friedman's algorithm) is trained in supervised regression mode to predict target properties. Feature importances are extracted from the tree ensemble; top features (bits/subgraphs) are analyzed and combined using logical operations (AND, XOR, NOT) to capture higher-order correlations and generate human-readable hypotheses of the form 'subgraph i -&gt; increase/decrease of property by strength s'. The workflow emphasizes model interpretability rather than raw predictive score.",
            "ai_methodology_category": "Supervised learning (interpretability-focused tree-ensemble regression)",
            "applicability": "Applicable and appropriate: the approach maps naturally to molecular graph data and emphasizes interpretability, suitable for domains where mechanistic insight is important; limitations include dependence on the fingerprint encoding and the need for sufficient labeled examples to detect motif correlations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective: the workflow 'rediscovered' well-known chemical rules-of-thumb (e.g., carbonyls increase water solubility, conjugated chains increase logP; electron-donating/withdrawing groups influence HOMO energies) and found new, less-expected motifs (e.g., silole rings correlating with reduced HOMO–LUMO gaps, conjugated bridges increasing singlet–triplet gaps). It produced directly interpretable subgraph hypotheses; limitations include that high feature importance does not imply direct causal effect and combinations of features were often required.",
            "impact_potential": "High potential to accelerate conceptual chemical understanding, to suggest novel design motifs for organic electronics and OLED/TADF materials, and to reduce bias from human preconceptions; can prioritize experiments and guide molecular design.",
            "comparison_to_alternatives": "Paper discusses alternative explainability approaches (graph convolutional nets, Grad-CAM, GNNExplainer) but does not perform a numerical performance comparison; authors argue tree-based fingerprint approach offers more direct, binary, human-interpretable subgraph signals compared to per-node/edge saliency maps.",
            "success_factors": "Availability of sizable labeled computational datasets; natural graph structure of molecules; use of binary subgraph fingerprints that map directly to chemical motifs; choice of interpretable model (tree ensemble) allowing feature-importance extraction; combination logic to capture higher-order interactions; domain expert interpretation.",
            "key_insight": "Interpretable tree ensembles trained on subgraph fingerprint representations can both accurately capture complex, nonlinear structure–property correlations in chemistry and produce human-comprehensible hypotheses that rediscover known chemical intuition and reveal new, experimentally actionable motifs.",
            "uuid": "e2266.0"
        },
        {
            "name_short": "ML-hypothesis-workflow (quantum optics)",
            "name_full": "Automated hypothesis generation workflow using subgraph fingerprints and gradient boosting (quantum optics application)",
            "brief_description": "Same automated workflow (graph → circular fingerprint → gradient-boosting regressor → feature importance → logical combinations) applied to graph-encoded quantum-optical experimental setups to predict entanglement dimensionality (n_Q) and generate interpretable design rules for experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Quantum optics / experimental design for high-dimensional multipartite entanglement",
            "problem_description": "Predict how substructures of quantum-optical experimental setups (graphs of optical elements and photon paths) affect the overall entanglement dimensionality measure n_Q = log2(d1 d2 d3) and generate interpretable hypotheses to guide experiment design for high-dimensional three-photon entanglement.",
            "data_availability": "Data derived from a database of experimental setups with labels computed from quantum-optical approximations (simulated entanglement dimensionality). Availability: dataset used by the authors (not public in-paper), labeled by simulation; amount: moderate (collection of many candidate setups and variants).",
            "data_structure": "Graph-structured experimental designs (nodes = optical elements, edges = photon paths) converted to binary subgraph fingerprints (bit-vectors).",
            "problem_complexity": "High: quantum interference and entanglement involve counter-intuitive, highly non-linear phenomena; combinatorial experimental-design search space; interactions depend on global experimental graph context.",
            "domain_maturity": "Emerging but active: experimental demonstrations exist for high-dimensional entanglement, theoretical models to compute final quantum states are available, yet design intuition is limited and counter-intuitive.",
            "mechanistic_understanding_requirements": "High - designers require mechanistic, interpretable insights to guide physical experiments and to avoid misleading intuitive assumptions.",
            "ai_methodology_name": "Gradient Boosting Decision Trees on subgraph (ECFP-style) fingerprints with feature-importance extraction and logical feature composition",
            "ai_methodology_description": "Experimental setups encoded as graphs were fingerprinted using the same circular hashing algorithm to yield binary features for subgraphs. A gradient-boosting regression model was trained (supervised) to predict n_Q. Feature importances were extracted and combined with logical operations to form macro-features; these machine-generated hypotheses were interpreted by domain experts to identify counter-intuitive design rules (e.g., shifting mode space by ±3 before mixing increases n_Q; direct connection of crystal output to detector reduces n_Q).",
            "ai_methodology_category": "Supervised learning (interpretability-focused tree-ensemble regression)",
            "applicability": "Applicable and appropriate: the graph-based fingerprint + tree ensemble framework naturally encodes experiment topology and produces interpretable rules; limitations include reliance on simulated labels and potential sparse coverage of rare but important experimental motifs.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective: discovered hypotheses that contradict prevailing human intuition and match experimental practice (and suggested improvements); identified both negatively and positively influencing subgraphs and produced combined logical rules that map to actionable experimental recommendations. Caveats: hypotheses need verification; not all random mutations produce valid quantum states, making automated testing costly.",
            "impact_potential": "High potential: can change experimental design strategies, reveal counter-intuitive yet robust rules (e.g., mode shifting prior to mixing), and accelerate discovery of complex quantum experiments while reducing bias from prior human intuition.",
            "comparison_to_alternatives": "Authors reference other automated / ML-driven experiment design methods (automated search for new quantum experiments, inverse-design), but do not provide a head-to-head empirical comparison; emphasis is on interpretability and human-in-the-loop insight rather than pure optimization.",
            "success_factors": "Graph encoding of experiments enabling subgraph-level features; accurate physical simulation to provide labels; interpretable tree ensemble allowing extraction of causal-looking motifs; logical combination of features to capture conditional effects; expert domain interpretation.",
            "key_insight": "Interpretable ML on graph-encoded experimental setups can reveal non-obvious, experimentally actionable rules for constructing high-dimensional entangled states — even overturning field intuition — because tree ensembles on subgraph fingerprints highlight compositional motifs that strongly influence entanglement.",
            "uuid": "e2266.1"
        },
        {
            "name_short": "ECFP / circular extended-connectivity fingerprints",
            "name_full": "Extended-connectivity (circular) fingerprints",
            "brief_description": "A graph fingerprinting algorithm that iteratively hashes node and local-neighborhood information to produce binary features indicating presence/absence of particular subgraphs (used to convert molecules or experimental graphs into interpretable bit-vectors).",
            "citation_title": "Extended-connectivity fingerprints",
            "mention_or_use": "use",
            "scientific_problem_domain": "Graph-structured representations in chemistry and other domains (molecules, experimental setups)",
            "problem_description": "Encode graph-local structural motifs as fixed-length binary vectors (fingerprints) that can be used as input features for ML models to predict properties or extract interpretable subgraph-importance signals.",
            "data_availability": null,
            "data_structure": "Graph → iterative neighborhood aggregation → hashed subgraph bit-vector (sparse high-dimensional binary vector).",
            "problem_complexity": "Moderate-to-high: hashing collates many possible subgraphs into finite bits; choice of radius/iterations trades off local vs more global context; collisions and representation capacity matter.",
            "domain_maturity": "Well-established and widely used in cheminformatics; standard technique for molecular ML models.",
            "mechanistic_understanding_requirements": "Medium-high when used for interpretability because individual bits map to subgraphs that have chemical meaning.",
            "ai_methodology_name": "Circular extended-connectivity fingerprinting (ECFP)",
            "ai_methodology_description": "Iterative algorithm that assigns initial identifiers to nodes from categorical node/edge features, then iteratively aggregates neighboring identifiers using hashing to create identifiers for larger-radius subgraphs; final mapping of observed subgraphs to indices produces a binary vector indicating presence/absence of subgraphs. In this work, the same approach was applied to both molecules and optical-experiment graphs to yield binary features suitable for tree ensembles.",
            "ai_methodology_category": "Feature engineering / representation for supervised ML",
            "applicability": "Highly applicable for graph-structured scientific data where local substructures are meaningful; particularly suitable where interpretability of subgraph presence is desired.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Effective as an interpretable representation: allowed direct mapping from important bits to chemical/experimental motifs and facilitated hypothesis generation; potential limitations include hashing collisions and limited global-context capture unless radius increased.",
            "impact_potential": "High as a bridge between raw graphs and interpretable ML models across chemistry, physics, and other graph domains.",
            "comparison_to_alternatives": "Authors note alternatives (graph convolutional networks and other GNNs) that learn distributed node embeddings rather than binary subgraph indicators, but argue ECFP yields more direct, human-interpretable features.",
            "success_factors": "Direct mapping from fingerprint bits to concrete subgraphs; compatibility with tree-ensemble feature-importance measures; low preprocessing overhead.",
            "key_insight": "Binary subgraph fingerprints provide a compact, interpretable bridge from graph-structured scientific data to feature-importance-driven ML, enabling direct human-understandable hypotheses about structural motifs.",
            "uuid": "e2266.2"
        },
        {
            "name_short": "GNNExplainer (mention)",
            "name_full": "GNNExplainer: Generating explanations for graph neural networks",
            "brief_description": "An explainability method that finds subgraphs and node features that are most influential for a GNN's prediction on a specific sample, producing saliency-like explanations for graph neural networks.",
            "citation_title": "Gnnexplainer: Generating explanations for graph neural networks",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Graph neural network interpretability; applicable across molecular property prediction and other graph-based scientific ML tasks",
            "problem_description": "Provide local explanations (per-sample) for GNN predictions by identifying compact subgraphs and feature subsets that the model relies on.",
            "data_availability": null,
            "data_structure": "Graph data and GNN model internals; explanations are per-sample subgraphs and node/edge feature masks.",
            "problem_complexity": "High in interpretability sense: explanations depend on complex learned distributed representations; generating concise subgraphs requires optimization and can be sample-specific.",
            "domain_maturity": "Emerging/active research area for explainable GNNs; methods exist but interpretability remains challenging.",
            "mechanistic_understanding_requirements": "High for scientific use — domain scientists often require explanations that map to real-world entities rather than distributed embeddings.",
            "ai_methodology_name": "GNNExplainer",
            "ai_methodology_description": "Optimization-based method to find a subgraph and subset of node features that maximally influence a trained GNN's prediction for a single instance; returns importance masks enabling human interpretation of GNN decisions.",
            "ai_methodology_category": "Model explainability / post-hoc interpretability (applied to supervised GNNs)",
            "applicability": "Mentioned as an alternative for extracting interpretable graph-level explanations, but the paper notes that per-sample explanations complicate drawing dataset-level hypotheses.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Useful for local interpretability but authors argue it provides explanations per sample rather than dataset-level, and yields non-binary importance scores that are harder to convert into simple, general hypotheses.",
            "impact_potential": "High for explaining GNN decisions on individual scientific instances, but limited for automated generation of global interpretable hypotheses without aggregation.",
            "comparison_to_alternatives": "Compared qualitatively to the authors' fingerprint + tree approach: GNNExplainer focuses on local explanations and continuous importance scores, while the paper's approach yields dataset-level binary subgraph hypotheses.",
            "success_factors": "Works when strong per-sample attribution is needed; less ideal for global hypothesis generation across many graphs.",
            "uuid": "e2266.3"
        },
        {
            "name_short": "Grad-CAM (mention)",
            "name_full": "Grad-CAM: Visual explanations from deep networks via gradient-based localization",
            "brief_description": "A gradient-based localization technique that provides visual saliency maps for convolutional neural networks, highlighting regions important for a prediction; mentioned as an explainability tool that could be used in graph/CNN contexts.",
            "citation_title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Model explainability for deep convolutional models; referenced in context of scientific ML explainability alternatives",
            "problem_description": "Localize input regions that most influence a deep CNN's prediction via gradient-weighted class activation maps; typically applied to image data but conceptually relevant to other architectures.",
            "data_availability": null,
            "data_structure": "Primarily image data and convolutional feature maps; outputs are heatmaps.",
            "problem_complexity": "Moderate: requires differentiable models and access to internal feature maps and gradients; explanations are approximate and local.",
            "domain_maturity": "Well-established in computer vision; adaptation to graph or molecular domains requires architectural changes.",
            "mechanistic_understanding_requirements": "Medium - produces visual saliency useful for interpretability but not necessarily mechanistic causal explanations.",
            "ai_methodology_name": "Grad-CAM",
            "ai_methodology_description": "Computes gradients of a target output with respect to convolutional feature maps, pools those gradients to obtain weights, and combines them with the feature maps to produce a coarse localization map of important regions.",
            "ai_methodology_category": "Post-hoc explainability for supervised convolutional models",
            "applicability": "Mentioned as an alternative explainability method; authors note such methods analyze single samples and produce non-binary importance maps, making dataset-level hypothesis extraction harder.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Effective for visual explanations in image-based models; less directly suitable for producing compact dataset-level, human-readable subgraph hypotheses as required in this work.",
            "impact_potential": "Useful for per-instance explanation in image-like scientific tasks; limited for automated hypothesis generation from graph datasets without aggregation.",
            "comparison_to_alternatives": "Qualitatively compared to the paper's approach: Grad-CAM produces soft per-sample attributions; the paper favors binary subgraph fingerprints and tree ensembles for direct hypothesis extraction.",
            "success_factors": "Works best for convolutional architectures and image tasks where spatial localization aids interpretation.",
            "uuid": "e2266.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Extended-connectivity fingerprints",
            "rating": 2,
            "sanitized_title": "extendedconnectivity_fingerprints"
        },
        {
            "paper_title": "Greedy function approximation: a gradient boosting machine",
            "rating": 2,
            "sanitized_title": "greedy_function_approximation_a_gradient_boosting_machine"
        },
        {
            "paper_title": "Convolutional networks on graphs for learning molecular fingerprints",
            "rating": 2,
            "sanitized_title": "convolutional_networks_on_graphs_for_learning_molecular_fingerprints"
        },
        {
            "paper_title": "Gnnexplainer: Generating explanations for graph neural networks",
            "rating": 2,
            "sanitized_title": "gnnexplainer_generating_explanations_for_graph_neural_networks"
        },
        {
            "paper_title": "Automated search for new quantum experiments",
            "rating": 2,
            "sanitized_title": "automated_search_for_new_quantum_experiments"
        },
        {
            "paper_title": "The harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community grid",
            "rating": 2,
            "sanitized_title": "the_harvard_clean_energy_project_largescale_computational_screening_and_design_of_organic_photovoltaics_on_the_world_community_grid"
        }
    ],
    "cost": 0.01548575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Scientific intuition inspired by machine learning generated hypotheses</p>
<p>Pascal Friederich 
Department of Chemistry
Chemical Physics Theory Group
University of Toronto
Canada</p>
<p>Department of Computer Science
University of Toronto
Canada</p>
<p>Institute of Theoretical Informatics
Karlsruhe Institute of Technology
Am Fasanengarten 576131KarlsruheGermany</p>
<p>Institute of Nanotechnology
Karlsruhe Institute of Technology
Hermann-von-Helmholtz-Platz 176344Eggenstein-LeopoldshafenGermany</p>
<p>Mario Krenn 
Department of Chemistry
Chemical Physics Theory Group
University of Toronto
Canada</p>
<p>Department of Computer Science
University of Toronto
Canada</p>
<p>Vector Institute for Artificial Intelligence
TorontoCanada</p>
<p>Isaac Tamblyn 
Vector Institute for Artificial Intelligence
TorontoCanada</p>
<p>National Research Council of Canada
OttawaCanada</p>
<p>Alán Aspuru-Guzik 
Department of Chemistry
Chemical Physics Theory Group
University of Toronto
Canada</p>
<p>Department of Computer Science
University of Toronto
Canada</p>
<p>Vector Institute for Artificial Intelligence
TorontoCanada</p>
<p>Canadian Institute for Advanced Research (CIFAR) Lebovic Fellow
TorontoCanada</p>
<p>Scientific intuition inspired by machine learning generated hypotheses
(Dated: December 15, 2020)
Machine learning with application to questions in the physical sciences has become a widely used tool, successfully applied to classification, regression and optimization tasks in many areas. Research focus mostly lies in improving the accuracy of the machine learning models in numerical predictions, while scientific understanding is still almost exclusively generated by human researchers analysing numerical results and drawing conclusions. In this work, we shift the focus on the insights and the knowledge obtained by the machine learning models themselves. In particular, we study how it can be extracted and used to inspire human scientists to increase their intuitions and understanding of natural systems. We apply gradient boosting in decision trees to extract human interpretable insights from big data sets from chemistry and physics. In chemistry, we not only rediscover widely know rules of thumb but also find new interesting motifs that tell us how to control solubility and energy levels of organic molecules. At the same time, in quantum physics, we gain new understanding on experiments for quantum entanglement. The ability to go beyond numerics and to enter the realm of scientific insight and hypothesis generation opens the door to use machine learning to accelerate the discovery of conceptual understanding in some of the most challenging domains of science.</p>
<p>Abstract Machine learning with application to questions in the physical sciences has become a widely used tool, successfully applied to classification, regression and optimization tasks in many areas. Research focus mostly lies in improving the accuracy of the machine learning models in numerical predictions, while scientific understanding is still almost exclusively generated by human researchers analysing numerical results and drawing conclusions. In this work, we shift the focus on the insights and the knowledge obtained by the machine learning models themselves. In particular, we study how it can be extracted and used to inspire human scientists to increase their intuitions and understanding of natural systems. We apply gradient boosting in decision trees to extract human interpretable insights from big data sets from chemistry and physics. In chemistry, we not only rediscover widely know rules of thumb but also find new interesting motifs that tell us how to control solubility and energy levels of organic molecules. At the same time, in quantum physics, we gain new understanding on experiments for quantum entanglement. The ability to go beyond numerics and to enter the realm of scientific insight and hypothesis generation opens the door to use machine learning to accelerate the discovery of conceptual understanding in some of the most challenging domains of science.</p>
<p>I. INTRODUCTION</p>
<p>Machine learning (ML) recently became a widely used tool with many applications in the physical sciences [1], ranging from chemistry (for example, prediction of quantum chemistry properties [2], solving Schrödinger's equation [3], predicting reactions [4], materials discovery [5] or inverse materials design [6,7]) to physics (for example, identification of phases of matter [8], astronomical object recognition [9], or validation of quantum experiments [10]) and biology (for example, prediction of protein structures [11] or drug design [12,13]). Some open challenges regarding the application of machine learning models in natural sciences include the accessibility, homogeneity, amount and quality of available data, as well as a lack of machine learning models which inherently include physical laws, limiting the interpretability of the models' predictions.While ML models are successfully used and optimized to accelerate numerical predictions or to recognize or generate patterns in existing data, it is rarely inquired how the machine finds solutions, i.e. which patterns and correlations it detected and exploited. Thus, the scientific insight obtained by the model is not directly transferred to human scientists. First attempts to use artificial intelligence in physical sciences aimed to directly answer scientific questions, e.g. determine the location of protein encodings in the genome [14]. Further attempts to employ machine learning models to obtain insight and help scientists to develop theories were focused on rediscovering solutions to already solved problems, e.g. to rediscover the coordinate transformation in astrophysical [15] and nonlinear dynamical systems [16], or to detect symmetries and conservation laws [17]. The methods used in these cases enforce information bottlenecks or interpretable transformations in the ML model that then can inspire scientific understanding [18]. However, to our knowledge such methods were mostly applied to solved problems and have not been used yet to obtain novel insight and answers to questions that are not well understood yet.</p>
<p>In this work, we propose to use machine learning and systematic data analysis to automate further the process of generation of interpretable scientific hypotheses. We demonstrate the applicability of the approach using two questions in the natural sciences -a rediscovery task of chemistry knowledge (hydrophobicity and molecular energy levels in simple as well as application relevant molecules) and the discovery of new intuitions in physics (quantum optics). We show that our approach "rediscovers" but also extends known chemical rules of thumb for solubility and energy levels of organic molecules with application in organic photovoltaics and organic lightemitting diodes and helps us to better understand the arXiv:2010.14236v2 [cs.</p>
<p>LG] 14 Dec 2020 entanglement created in quantum optical experiments. Our model represents its findings in a graph representation which is directly related to chemical or physical instances in the specific scientific domain. The results are statements regarding distinct subgraphs that can easily be comprehended and therefore, scientifically interpreted and understood by experts. This is in stark contrast to conventional machine learning models where the internal representations are only indirectly connected with the real physical entities and thus hard to impossible to interpret.</p>
<p>II. METHOD</p>
<p>Computer generated hypotheses.</p>
<p>We suggest an automated workflow for ML-based generation of human interpretable scientific hypotheses as illustrated in Figure 1a. The workflow is based on a reference database of calculated (potentially also measured) data points with graph-based structure and with corresponding target properties.</p>
<p>A binary feature vector describing presence/absence of automatically generated subgraphs [20] is used to train a tree ensemble method, e.g. Gradient Boosting [19] or Random Forrest Regression/Classification [21,22], that allows for the quantification of feature importances. Based on the features with the highest importance, a list of hypotheses is generated. Each hypothesis has the human understandable form "Feature i leads to an increase/decrease of target property of strength s"</p>
<p>where i is the index of the corresponding feature (subgraph) in the input and strength s quantifies the degree of correlation between feature i and the target property. High feature importance does not necessarily correspond to a high direct correlation with the target feature. In many cases, multiple features have to be combined in order to become predictive, even if the single features individually do not help in the predicting the target property. Therefore, important features are combined using logical operations (and, xor, ...) to automatically generate combined features which, especially in presence of higher-order correlations, can be directly interpreted by researchers.</p>
<p>Input representation and experiments. In this work, we test this workflow on two experiments in chemistry and physics. The first experiment targets the automated generation of intuitive rules that determine molecular properties, whereas the second aims at hypothesis generation for entanglement properties of quantum optical experiments. In both cases, we can describe the data points as graphs (molecules and quantum optical experiments), where nodes are chemical elements or optical instruments while edges are chemical bonds or photon paths travelling through the setup. This allows us to use fingerprinting techniques to generate input representations (bit-vectors), e.g. using the algorithm for circular extended-connectivity fingerprints [20]. This iterative algorithm generates a unique representation of each node, including its local environment. In each iteration, hashing functions are used to aggregate the information (predefined node and edge features) of the next nearest neighbors of each node, thus implicitly integrating information of one additional neighbor shell in each iteration. In the end, a hashing function is used to map all subgraphs found in the graphs to bit-vectors. Each entry in these bit-vectors encodes the presence or absence of a certain subgraph. A similar approach has been used in Lopez el al. [23] to determine molecular substructures in molecules for organic solar cells that lead to high power conversion efficiencies. Other models that link the presence of subgraphs (or more generally features) in the input data to properties can potentially be employed in our workflow (see e.g. Duvenaud et al. [24] where molecular fragments are identified that correlate with toxicity, the Grad-CAM method by Selvaraju et al. [25] for convolutional neural networks or the GNNExplainer by Ying et al. [26]). In contrast to this work, some of these approaches depend on the analysis of single samples and thus only indirectly allow to conclude about an entire data set. Furthermore, these approaches assign importance indicators to single nodes or edges of a graph, which are not necessarily binary numbers, which complicates the direct interpretation. Due to their general applicability to all graphs where node and edges can be represented by one or multiple categorical features, we focused on automatically generated circular fingerprints in this work.</p>
<p>III. RESULTS</p>
<p>To test the automated hypothesis generation workflow, we performed experiments in two scientific domains, molecular chemistry (Section III A) and quantum optical experiments (Section III B). We computed physical properties of these graphs and used the generated data sets and the workflow described in Figure 1 to automatically generate hypotheses that can be either compared to a collection of widely known chemical rules of thumb or that can help to better understand entanglement in quantum optical experiments for designing future experiments.</p>
<p>A. Chemical intuition for solubility, energy levels</p>
<p>In case of the chemistry experiment, we used two prototypical target properties -the water-octanol partition coefficient which describes the solubility of molecules Workflow for automated hypothesis generation. a) General workflow, starting with a database of graphs and respective properties, followed by training of a machine learning model that allows for the extraction of feature importances, e.g. Gradient Boosting Regression. Features with high importance are combined and analysed in a way that facilitates interpretation by researchers in order to stimulate scientific insight. b) Schematic illustration of the Gradient Boosting Regression method [19], where multiple simple decision tree models are trained sequentially. Each new decision tree is trained to correct the residual errors (red lines) of the previous models, so the final prediction F0(x) can be written as a sum of the mean label c0 and a weighted series of models hi(x), where each hi predicts the deviation of the previous i − 1 models from the ground truth. c) Each decision tree is trained on samples that are represented using predefined input features (coloured squares) and uses their values to split the data set sequentially into smaller subsets which are used for the predictions. The subgraph based input representation used in this work allows a direct interpretation of the feature importances (d) that are computed based on a quantification of how meaningful features are for the accuracy of the machine learning model. in water (polar) vs. octanol (non-polar) as well as the energy of the highest occupied molecular orbital. Both properties are of high relevance for the application of molecules as pharmaceuticals or in electronic devices, e.g.</p>
<p>for organic solar cells, organic light-emitting diode (OLED) displays or organic flow batteries. We furthermore analysed existing application-specific data sets, namely a data set of thermally activated delayed fluorescent (TADF) molecules as emitter molecules for OLEDs [27], the Harvard Clean Energy project data set [28,29] and a data set of non-fullerene acceptor molecules for organic solar cells [23]. Solubility and energy levels are relatively well understood and for both properties there exist several widely known rules of thumb, often described as chemical intuition, which describes how certain functional groups influence them. Our experiment aims to test whether the automated hypothesis generation method can "rediscover" those rules and potentially add new or refined rules. For frontier orbital gaps reported in the Harvard Clean Energy data set and the non-fullerene acceptor data set as well as for singlet-triplet energy splittings reported in the TADF data set, there exists less chemical intuition on how to influence and tune them. Figure 2 shows two solubility related hypotheses that were generated using our workflow. Without prior knowledge, the algorithm predicts two widely known chemical groups/motifs for increasing solubility in polar solvents (carbonyl group in Figure 2a) and to increase solubility in non-polar solvents (conjugated carbon chain in Figure 2b). Figure 3 shows an overview of molecular subgraphs that positively and negatively influence the HOMO energy of a molecule. To our surprise, five of the nine groups shown in the figure can directly be found in chemistry textbooks or Wikipedia when searching for electrophilic aromatic directing groups which can change the energy levels of molecules through the inductive effect and the mesomeric effect. Specifically, the oxido (O − ) group that shows the strongest positive influence influence on the HOMO energy. The groups "discovered" by our automated workflow are widely known activating (resonance donating or electron donating) and deactivating groups, such as oxido/amino groups and nitrile groups. on the HOMO is well known for a strong resonance donating and a strong inductive effect which both leads to an increase in HOMO energy. Furthermore, heterocycles that contain nitrogen, as well as amine (NH2) groups are also known for lifting the HOMO level to higher energies. On the other hand, the nitrile group (C≡N) is one of the most widely known electron-withdrawing groups that lowers the HOMO energy of molecules due to its resonance withdrawing and inductively withdrawing nature.</p>
<p>The patterns found to be relevant for small HOMO-LUMO gaps in the Harvard Clean Energy data set as well as in the non-fullerene acceptor data set are mostly related to extended aromatic systems and fused aromatic rings (see Figure S4a and Figure S1a). This finding is well-understood by chemists due to the widely know relation between the size of an aromatic system (i.e. the degree of delocalization of π-electrons) and the frontier orbital gap [30]. In the limit of infinite delocalization (e.g. in graphene), the HOMO-LUMO gap closes completely. This relation was also exploited in the development of conductive polymers, which was awarded with the Nobel Price in Chemistry in 2000 and which created the field of organic electronics [31]. However, we additionally found several interesting and surprising patterns both in the photovoltaic data sets ( Figure S4b/c) and in the TADF dataset ( Figure 4). In case of the Harvard Clean Energy data set, we find that aromatic heterocycles with sulfur (e.g. thiophene rings) as well as silicon heteroatoms (e.g. silole rings) significantly reduce the HOMO-LUMO gap. While the former are widely used in organic electronics to control energy FIG. 4. Hypotheses singlet-triplet splittings in the TADF data set [27]. The data-driven algorithm finds the well known and widely exploited structure-property relation of triarylamines and small single triplet gaps (&lt;0.5 eV, upper panel). However, it finds an additional, less known motif of alternating single-double-bond bridges that are related to increased singlet triplet gaps (&gt;0.5 eV, lower panel). levels and reduce HOMO-LUMO gaps, silole rings are more unusual. In the non-fullerene acceptor data set (see Figure S4c) we found that thiophene rings connected by double bonds (i.e. forming a quinoid structure instead of aromatic systems) also significantly reduce the HOMO-LUMO gap, which is a know relation first described by Brédas [32]. However, such systems require a specific functionalization in the periphery of the molecule to enforce the quinoid structure of the two thiophene rings, which intrinsically is less stable and thus higher in energy than the aromatic structure. In case of the TADF data set (see Figure 4), we found expected patterns such as triarylamines that correlate with decreased singlet triplet gaps (S1-T1 gaps) as well as rather unexpected patterns (e.g. conjugated bridges) that are identified by our workflow as chemical groups that highly correlate with large singlet triplet gaps. Low singlet-triplet splittings in TADF molecules are typically achieved by decoupling electron donating and electron accepting parts of a molecule to reduce the exchange interaction between the frontier orbitals which would otherwise lower the triplet state compared to the singlet state and open an undesired singlet-triplet splitting. The decoupling of the fragments can be achieved by introducing twist angles close to 90 • between the fragments. One way to accomplish this are triarylamines FIG. 5. Hypotheses about HOMO-LUMO gaps in the Harvard Clean Energy data set [28,29] and a nonfullerene acceptor data set [23]. (a) The automated hypotheses generation protocol rediscovers the widely known relation between extended aromatic systems (containing e.g. nitrogen heteroatoms) and reduced HOMO-LUMO gaps. (b) Thiophene but also more uncommon silole rings are found to correlate with small HOMO-LUMO gaps. c) Thiophene rings bridged with double bonds (quinoid structures) are found to decrease the HOMO-LUMO gap in the non-fullerene acceptor data set. (Note the different scale in panel (c) compared to (a) and (b), due to differences in the data sets.) bridges between the fragments. We expect that the conjugated bridges between fragments have precisely the opposite effect: They lead to a planar alignment of the adjacent fragments and thus an enhanced exchange interaction, reduced triplet energies and finally increased singlet-triplet splittings.</p>
<p>B. Physical intuitions for quantum experiments</p>
<p>As a second example, we use quantum optical experiments for producing high-dimensional, multipartite quantum entanglement [33,34]. These experiments grow in interest as they allow the investigation of fundamental physical properties -such as local realism [35] -in laboratories. Furthermore, such quantum states are the key resources for large and complex quantum communication networks [36,37], which are on the edge of commercial availability. The experimental setups that we consider consist of standard optical components that are used in labs, such as nonlinear crystals for the creation of photon pairs, single-photon detectors, beam splitters, holograms or Dove prisms. Under approximations that are closely resembled in experiments, the final emergent quantum state can be reliably calculated [38].</p>
<p>A key challenge lies in the design of experiments which creates certain desired quantum systems. The difficulty arises from counter-intuitive quantum phenomena, which raises the question of whether human intuition is the best way to design new experiments. Several studies have therefore developed automated and machine-learning augmented approaches for the design of experiments [39][40][41][42][43][44]. The goal in our approach is to tackle this challenge in a completely different way, namely by improving the scientist's intuition about these systems.</p>
<p>Specifically, we are investigating optical setups with three-photon entanglement in high dimensions, using a fourth photon as a trigger. The experimental setups can be represented as graphs where vertices represent optical elements, and edges correspond to the photon paths connecting these elements. Analogously to chemical elements, the optical elements can have one to four connections. For example, a beam splitter has four input-output modes, while a detector has only one input. As a measure of entanglement, we use the overall size of the involved Hilbert space in terms of involved qubits, n Q = log 2 (d 1 d 2 d 3 ), where d i stands for the rank of density matrix after tracing out photon i [45,46].</p>
<p>We used the same fingerprint-based graph representation as in Section III A and trained a Gradient Boosting Regression model to predict n Q . Using the algorithm outlined in Figure 1, we form a list of hypotheses of subgraphs features that influence n Q most. This computer-generated list was analysed and interpreted by a domain expert.</p>
<p>The two features which influence n Q most negatively contradict the intuition in the field, see Fig. 6a/b and Fig. S2. Surprisingly, both of them represent subgraphs that are core elements of two experimental setups which have produced high-dimensional multipartite entanglement in the laboratory [47,48]. Specifically, if the outputs of two nonlinear crystals (both crystals produce entangled photon pairs in the same 3-dimensional mode space) are connected directly via a beam splitter or interferometer, the entanglement of the resulting state is predicted to be comparably low. This can be interpreted in the following way: The photons from the two different crystals need to combine at some point, otherwise, they remain bi-separable. However, if they combine directly after their generation, the equal mode spaces mix in such a way that it is difficult to increase their dimensionality subsequently. It is therefore explicitly enlightening that several of the features that positively influence n Q correspond to elements which shift the entire mode space by plus or minus three before or after the beam splitters or nonlinear crystals. The insight for a human researcher now is to shift the mode space by three (as the local dimension is three), before combining photons from different nonlinear crystals to achieve a high n Q . This leads to mode spaces of twice the original size and thereby increasing the probability for large overall entanglement dimensionalities.</p>
<p>A different feature which was used in the two experimentally demonstrations, but significantly negatively influences n Q is the following: One output of a nonlinear crystal is directly connected to the detector. For human designers, this leads to the convenient fact that it simplifies the initial state (as double-emissions from one crystal can be ignored in this case). However, the entanglement of this photon with the other two photons can never be larger than three (as the local mode space is three). A similar, negatively influencing feature is a certain interferometer, which sorts the parity of the involved modes, directly connected to a detector. This acts as a filter, thus reducing the mode space of the incoming photon by half, thereby reducing the overall possible entanglement significantly.</p>
<p>Logically combined features: We can logically combine graph features, as described in II, and find the most significant macro-features for quantum experiments. In Fig. 7a, two small sub-experiments are combined with a logical and, i.e. the feature is the combination of both structures. Individually, the presence of the first feature has a negative influence on n Q . The second feature, a parity sorter followed by two detectors, influences n Q positively. Surprisingly, their combination has a significant negative influence on n Q and can be seen as an almost sufficient condition for n Q ≈ 4 . This behaviour can be interpreted using the Klyshko advanced wave-picture for quantum correlations in quantum optics [49]. The detector after the photon pair creation heralds a specific quantum state in the other photonic path. If those photons deterministically split at the parity sorter, the ability to mix with the photons from the other input ports (thus from the other crystal) vanish. From this insight, the human designer can learn that a heralded single-photon should be combined in a probabilistic way with the photons of the other crystal, using beam splitters instead of parity sorters.</p>
<p>A second macro-feature, Fig. 7b, combines two insights that we gained in Fig. 6. The macro-feature in Fig. 7b shows that the absence of either three positive or three negative mode shifters in front of a beam splitter has a very negative impact on the n Q . Thereby, the algorithm has discovered that both increasing or decreasing helps to have very positive influence on the final entanglement, and thereby suggests that one can be agnostic about the shift direction, and the importance lies in the actual increase of the local Hilbert space before the mixing. This features clearly shows how logical combinations can simplify the interpretation of scientific data.</p>
<p>IV. CONCLUSION AND OUTLOOK</p>
<p>We presented a data-driven machine learning workflow for automated generation and verification of hypotheses about observations in natural sciences. We presented examples from chemistry and physics, but our method is directly applicable to most applications, where structures can be represented as graphs, e.g. to DNA/RNA data in biology [50,51], chemical reaction networks [52,53] or graphs in social sciences. In chemistry, the workflow "rediscovers" widely known relations regarding solubility and electronic properties of molecules (often referred to as chemical intuition). In physics, the algorithm discovers rules to generate highly entangled three-photon states in quantum optical experiments. These rules are interpretable by human experts in retrospect, yet not known or postulated before, and even contradicting some of the field's current understanding. Finding such rules will not only help researchers to understand complex scientific relationships and thus design better experiments, but also reduce unavoidable and often undetectable bias generated by prior knowledge and anticipations.</p>
<p>Hypothesis testing. In addition to automated hypothesis generation, protocols for testing of the postulated hypotheses would be beneficial. In case of the chemistry experiment, a possible hypothesis testing protocol would generate mutations of each molecule in the training set to test the hypotheses on molecules with similar representations, where (ideally) only the relevant feature is changed. In case of the quantum optical experiments, not all random mutations will lead to maximally entangled states between all photons, which is a requirement to compute the entanglement of the quantum state. We currently see two options for automated hypothesis verification both of which we are currently implementing. The first follows the same procedure of mutation and computation as in the chemistry experiment, with the caveat that only a small fraction of the mutations will lead to useful results, potentially making the procedure computationally costly. The second option is based on finding other experimental setups within the whole database that are as similar to the reference experiment as possible, with the exception of the feature that is currently analysed. This procedure is computationally costly as well but does not require new computations.</p>
<p>Supplementary Information: Scientific intuition inspired by machine learning generated hypotheses S1. ADDITIONAL HYPOTHESIS ABOUT CHEMISTRY DATA SETS Figure S1 shows additional features that influence the HOMO-LUMO gap of molecules in multiple data sets. Our workflow finds patterns that are commonly associated with a positive or negative influence on HOMO-LUMO gaps, but also patterns and groups such as silole rings.</p>
<p>FIG. S1. HOMO-LUMO gaps in the Harvard Clean Energy data set [28,29] and a non-fullerene acceptor data set. [23] (a) Agreeing with widely known rules of thumb, extended aromatic systems containing nitrogen heteroatoms are associated with reduced HOMO-LUMO gaps. (b) Thiophene but also more uncommon silole rings correlate with small HOMO-LUMO gaps. c) Thiophene rings bridged with double bonds (i.e. quinoid instead of aromatic systems) are found to decrease the HOMO-LUMO gap in the non-fullerene acceptor data set, a phenomenon that is studied and described in literature [32]. (Note the different scale in panel (c) compared to (a) and (b), due to differences in the data sets.) Figure S2 shows additional features that influence the singlet-triplet gap of TADF molecules. Figure S2a shows groups with a negative influence on the singlet-triplet gap and Figure S2b shows groups with a positive influence on the singlet-triplet gap. Figure S3 shows an example of a easily interpretable, logically combined feature that influences the singlet-triplet gap of molecules. </p>
<p>FIG. 1 .
1FIG. 1. Workflow for automated hypothesis generation. a) General workflow, starting with a database of graphs and respective properties, followed by training of a machine learning model that allows for the extraction of feature importances, e.g. Gradient Boosting Regression. Features with high importance are combined and analysed in a way that facilitates interpretation by researchers in order to stimulate scientific insight. b) Schematic illustration of the Gradient Boosting Regression method [19], where multiple simple decision tree models are trained sequentially. Each new decision tree is trained to correct the residual errors (red lines) of the previous models, so the final prediction F0(x) can be written as a sum of the mean label c0 and a weighted series of models hi(x), where each hi predicts the deviation of the previous i − 1 models from the ground truth. c) Each decision tree is trained on samples that are represented using predefined input features (coloured squares) and uses their values to split the data set sequentially into smaller subsets which are used for the predictions. The subgraph based input representation used in this work allows a direct interpretation of the feature importances (d) that are computed based on a quantification of how meaningful features are for the accuracy of the machine learning model.</p>
<p>FIG. 2 .
2Hypotheses about molecular solubility. a) Lower logP values (better solubility in water compared to octanol) can be achieved using the carbonyl groups, while b) conjugated carbon chains lead to higher logP values.</p>
<p>FIG. 3 .
3Hypotheses about molecular energy levels. Molecular subgraphs with a positive (left) and negative (right)</p>
<p>FIG. 6 .
6Hypotheses about quantum optical experiments. Experimental substructures leading to a decrease in the overall size of the Hilbert space of involved qubits (nQ) are shown in a) while substructures with positive influence are shown in b).</p>
<p>FIG. 7 .
7Logically combined hypotheses about quantum optical experiments. Combining single-subgraph hypotheses with logical operations leads to intuitively interpretable relations, which is illustrated here with two examples. The upper panel shows the logically combined feature and its correlation with nQ, while the lower panels show the correlations of the isolated subgraphs.</p>
<p>S11
FIG. S2. TADF (a) Groups with a negative influence on the singlet-triplet gap, (b) Groups with a positive influence on the singlet-triplet gap. S2. ADDITIONAL HYPOTHESIS ABOUT QUANTUM OPTICAL EXPERIMENTS FIG. S3. TADF The logical combination of carbazole and triarylamine groups indicate that the algorithm identified those two groups as required for small singlet-triplet gaps, because the absence of both groups leads to significantly increased singlet-triplet gaps. FIG. S4. Entanglement in quantum optical experiments Subgraphs of the experimental setups that a) decrease and (b) increase the overall size of the involved Hilbert space nQ
DATA AVAILABILTYThe data that support the findings of this study are available upon request from the authors.
Machine learning and the physical sciences. Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby, Leslie Vogt-Maranto, Lenka Zdeborová, Reviews of Modern Physics. 91445002Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Lau- rent Daudet, Maria Schuld, Naftali Tishby, Leslie Vogt- Maranto, and Lenka Zdeborová. Machine learning and the physical sciences. Reviews of Modern Physics, 91(4):045002, 2019.</p>
<p>Big data meets quantum chemistry approximations: The δ-machine learning approach. Raghunathan Ramakrishnan, O Pavlo, Matthias Dral, O Anatole Von Rupp, Lilienfeld, Journal of chemical theory and computation. 115Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Big data meets quantum chemistry approximations: The δ-machine learning approach. Journal of chemical theory and com- putation, 11(5):2087-2096, 2015.</p>
<p>Deepneural-network solution of the electronic schrödinger equation. Jan Hermann, Zeno Schätzle, Frank Noé, Nature Chemistry. 1210Jan Hermann, Zeno Schätzle, and Frank Noé. Deep- neural-network solution of the electronic schrödinger equation. Nature Chemistry, 12(10):891-897, 2020.</p>
<p>Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, A Christopher, Costas Hunter, Alpha A Bekas, Lee, ACS central science. 59Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572-1583, 2019.</p>
<p>Robot-accelerated perovskite investigation and discovery. Zhi Li, Liana Mansoor Ani Najeeb, Alyssa Z Alves, Venkateswaran Sherman, Peter Cruz Shekar, Ian M Parrilla, Wesley Pendleton, Wang, W Philip, Matthias Nega, Zeller, Chemistry of Materials. Zhi Li, Mansoor Ani Najeeb, Liana Alves, Alyssa Z Sher- man, Venkateswaran Shekar, Peter Cruz Parrilla, Ian M Pendleton, Wesley Wang, Philip W Nega, Matthias Zeller, et al. Robot-accelerated perovskite investigation and discovery. Chemistry of Materials, 2020.</p>
<p>How to explore chemical space using algorithms and automation. Piotr S Gromski, B Alon, Jaros Henson, Leroy Law M Granda, Cronin, Nature Reviews Chemistry. 32Piotr S Gromski, Alon B Henson, Jaros law M Granda, and Leroy Cronin. How to explore chemical space using algorithms and automation. Nature Reviews Chemistry, 3(2):119-128, 2019.</p>
<p>Inverse molecular design using machine learning: Generative models for matter engineering. Benjamin Sanchez, -Lengeling , Alán Aspuru-Guzik, Science. 3616400Benjamin Sanchez-Lengeling and Alán Aspuru-Guzik. Inverse molecular design using machine learning: Gen- erative models for matter engineering. Science, 361(6400):360-365, 2018.</p>
<p>Machine learning phases of matter. Juan Carrasquilla, G Roger, Melko, Nature Physics. 135Juan Carrasquilla and Roger G Melko. Machine learning phases of matter. Nature Physics, 13(5):431-434, 2017.</p>
<p>Fast automated analysis of strong gravitational lenses with convolutional neural networks. D Yashar, Laurence Perreault Hezaveh, Philip J Levasseur, Marshall, Nature. 5487669Yashar D Hezaveh, Laurence Perreault Levasseur, and Philip J Marshall. Fast automated analysis of strong gravitational lenses with convolutional neural networks. Nature, 548(7669):555-557, 2017.</p>
<p>Pattern recognition techniques for boson sampling validation. Iris Agresti, Niko Viggianiello, Fulvio Flamini, Nicolò Spagnolo, Andrea Crespi, Roberto Osellame, Nathan Wiebe, Fabio Sciarrino, Physical Review X. 9111013Iris Agresti, Niko Viggianiello, Fulvio Flamini, Nicolò Spagnolo, Andrea Crespi, Roberto Osellame, Nathan Wiebe, and Fabio Sciarrino. Pattern recognition tech- niques for boson sampling validation. Physical Review X, 9(1):011013, 2019.</p>
<p>Improved protein structure prediction using potentials from deep learning. W Andrew, Richard Senior, John Evans, James Jumper, Laurent Kirkpatrick, Tim Sifre, Chongli Green, Qin, Au-Gustinžídek, W R Alexander, Alex Nelson, Bridgland, Nature. Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Au- gustinŽídek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein structure prediction using po- tentials from deep learning. Nature, pages 1-5, 2020.</p>
<p>Deep learning enables rapid identification of potent ddr1 kinase inhibitors. Alex Zhavoronkov, A Yan, Alex Ivanenkov, Aliper, S Mark, Vladimir A Veselov, Aladinskiy, V Anastasiya, Aladinskaya, A Victor, Terentiev, A Daniil, Polykovskiy, D Maksim, Arip Kuznetsov, Asadulaev, Nature biotechnology. 379Alex Zhavoronkov, Yan A Ivanenkov, Alex Aliper, Mark S Veselov, Vladimir A Aladinskiy, Anastasiya V Aladinskaya, Victor A Terentiev, Daniil A Polykovskiy, Maksim D Kuznetsov, Arip Asadulaev, et al. Deep learn- ing enables rapid identification of potent ddr1 kinase in- hibitors. Nature biotechnology, 37(9):1038-1040, 2019.</p>
<p>A deep learning approach to antibiotic discovery. M Jonathan, Kevin Stokes, Kyle Yang, Wengong Swanson, Andres Jin, Nina M Cubillos-Ruiz, Donghia, Shawn Craig R Macnair, French, A Lindsey, Zohar Carfrae, Bloom-Ackerman, Cell. 1804Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wen- gong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zo- har Bloom-Ackerman, et al. A deep learning approach to antibiotic discovery. Cell, 180(4):688-702, 2020.</p>
<p>The automation of science. D Ross, Jem King, Rowland, G Stephen, Michael Oliver, Wayne Young, Emma Aubrey, Maria Byrne, Magdalena Liakata, Pinar Markham, Larisa N Pir, Soldatova, Science. 3245923Ross D King, Jem Rowland, Stephen G Oliver, Michael Young, Wayne Aubrey, Emma Byrne, Maria Liakata, Magdalena Markham, Pinar Pir, Larisa N Soldatova, et al. The automation of science. Science, 324(5923):85- 89, 2009.</p>
<p>Discovering physical concepts with neural networks. Raban Iten, Tony Metger, Henrik Wilming, Lídia Del Rio, Renato Renner, Physical Review Letters. 124110508Raban Iten, Tony Metger, Henrik Wilming, Lídia Del Rio, and Renato Renner. Discovering physical con- cepts with neural networks. Physical Review Letters, 124(1):010508, 2020.</p>
<p>Deep learning for universal linear embeddings of nonlinear dynamics. Bethany Lusch, Nathan Kutz, Steven L Brunton, Nature communications. 91Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Deep learning for universal linear embeddings of nonlin- ear dynamics. Nature communications, 9(1):1-10, 2018.</p>
<p>J Sebastian, Roger G Wetzel, Joseph Melko, Scott, arXiv:2003.04299Maysum Panju, and Vijay Ganesh. Discovering symmetry invariants and conserved quantities by interpreting siamese neural networks. Sebastian J. Wetzel, Roger G. Melko, Joseph Scott, May- sum Panju, and Vijay Ganesh. Discovering symmetry in- variants and conserved quantities by interpreting siamese neural networks. arXiv:2003.04299, 2020.</p>
<p>Explainable machine learning for scientific insights and discoveries. Ribana Roscher, Bastian Bohn, F Marco, Jochen Duarte, Garcke, IEEE Access. 8Ribana Roscher, Bastian Bohn, Marco F Duarte, and Jochen Garcke. Explainable machine learning for sci- entific insights and discoveries. IEEE Access, 8:42200- 42216, 2020.</p>
<p>Greedy function approximation: a gradient boosting machine. H Jerome, Friedman, Annals of statistics. Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages 1189-1232, 2001.</p>
<p>Extended-connectivity fingerprints. David Rogers, Mathew Hahn, Journal of chemical information and modeling. 505David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of chemical information and mod- eling, 50(5):742-754, 2010.</p>
<p>Random decision forests. Kam Tin, Ho, Proceedings of 3rd international conference on document analysis and recognition. 3rd international conference on document analysis and recognitionIEEE1Tin Kam Ho. Random decision forests. In Proceedings of 3rd international conference on document analysis and recognition, volume 1, pages 278-282. IEEE, 1995.</p>
<p>Random forests. Machine learning. Leo Breiman, 45Leo Breiman. Random forests. Machine learning, 45(1):5-32, 2001.</p>
<p>Design principles and top non-fullerene acceptor candidates for organic photovoltaics. A Steven, Benjamin Lopez, Julio Sanchez-Lengeling, De Goes, Alán Soares, Aspuru-Guzik, Joule. 14Steven A Lopez, Benjamin Sanchez-Lengeling, Julio de Goes Soares, and Alán Aspuru-Guzik. Design prin- ciples and top non-fullerene acceptor candidates for or- ganic photovoltaics. Joule, 1(4):857-870, 2017.</p>
<p>Convolutional networks on graphs for learning molecular fingerprints. Dougal David K Duvenaud, Jorge Maclaurin, Rafael Iparraguirre, Timothy Bombarell, Alán Hirzel, Ryan P Aspuru-Guzik, Adams, Advances in neural information processing systems. David K Duvenaud, Dougal Maclaurin, Jorge Ipar- raguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional net- works on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pages 2224-2232, 2015.</p>
<p>Grad-cam: Visual explanations from deep networks via gradient-based localization. R Ramprasaath, Michael Selvaraju, Abhishek Cogswell, Ramakrishna Das, Devi Vedantam, Dhruv Parikh, Batra, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep net- works via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618-626, 2017.</p>
<p>Gnnexplainer: Generating explanations for graph neural networks. Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, Jure Leskovec, Advances in neural information processing systems. Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks. In Advances in neural information processing systems, pages 9244-9255, 2019.</p>
<p>Design of efficient molecular organic light-emitting diodes by a highthroughput virtual screening and experimental approach. Rafael Gómez-Bombarelli, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, David Duvenaud, Dougal Maclaurin, A Martin, Hyun Blood-Forsythe, Markus Sik Chae, Dong-Gwang Einzinger, Tony Ha, Wu, Nature materials. 1510Rafael Gómez-Bombarelli, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, David Duvenaud, Dougal Maclau- rin, Martin A Blood-Forsythe, Hyun Sik Chae, Markus Einzinger, Dong-Gwang Ha, Tony Wu, et al. Design of ef- ficient molecular organic light-emitting diodes by a high- throughput virtual screening and experimental approach. Nature materials, 15(10):1120-1127, 2016.</p>
<p>The harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community grid. Johannes Hachmann, Roberto Olivares-Amaya, Sule Atahan-Evrenk, Carlos Amador-Bedolla, Aryeh Sánchez-Carrera, Leslie Gold-Parker, Anna M Vogt, Alán Brockway, Aspuru-Guzik, The Journal of Physical Chemistry Letters. 217Johannes Hachmann, Roberto Olivares-Amaya, Sule Atahan-Evrenk, Carlos Amador-Bedolla, Roel S Sánchez-Carrera, Aryeh Gold-Parker, Leslie Vogt, Anna M Brockway, and Alán Aspuru-Guzik. The harvard clean energy project: large-scale computa- tional screening and design of organic photovoltaics on the world community grid. The Journal of Physical Chemistry Letters, 2(17):2241-2251, 2011.</p>
<p>A Steven, Edward O Lopez, Gregor N Pyzer-Knapp, Trevor Simm, Kewei Lutzow, Li, Johannes Hachmann, and Alán Aspuru-Guzik. The harvard organic photovoltaic dataset. Scientific data. 3Steven A Lopez, Edward O Pyzer-Knapp, Gregor N Simm, Trevor Lutzow, Kewei Li, Laszlo R Seress, Jo- hannes Hachmann, and Alán Aspuru-Guzik. The harvard organic photovoltaic dataset. Scientific data, 3(1):1-7, 2016.</p>
<p>The predictive power of aromaticity: quantitative correlation between aromaticity and ionization potentials and homo-lumo gaps in oligomers of benzene, pyrrole, furan, and thiophene. Renana Gershoni-Poranne, P Anuja, Amnon Rahalkar, Stanger, Physical Chemistry Chemical Physics. 2021Renana Gershoni-Poranne, Anuja P Rahalkar, and Am- non Stanger. The predictive power of aromaticity: quan- titative correlation between aromaticity and ionization potentials and homo-lumo gaps in oligomers of benzene, pyrrole, furan, and thiophene. Physical Chemistry Chem- ical Physics, 20(21):14808-14817, 2018.</p>
<p>nobel prize in chemistry. C Seth, Rasmussen, Acetylene and Its Polymers. SpringerSeth C Rasmussen. 2000 nobel prize in chemistry. In Acetylene and Its Polymers, pages 125-132. Springer, 2018.</p>
<p>Relationship between band gap and bond length alternation in organic conjugated polymers. Jean-Luc Bredas, The Journal of chemical physics. 828Jean-Luc Bredas. Relationship between band gap and bond length alternation in organic conjugated polymers. The Journal of chemical physics, 82(8):3808-3811, 1985.</p>
<p>Entanglement certification from theory to experiment. Nicolai Friis, Giuseppe Vitagliano, Mehul Malik, Marcus Huber, Nature Reviews Physics. 11Nicolai Friis, Giuseppe Vitagliano, Mehul Malik, and Marcus Huber. Entanglement certification from theory to experiment. Nature Reviews Physics, 1(1):72-87, 2019.</p>
<p>Advances in high-dimensional quantum entanglement. Manuel Erhard, Mario Krenn, Anton Zeilinger, Nature Reviews Physics. 2365Manuel Erhard, Mario Krenn, and Anton Zeilinger. Ad- vances in high-dimensional quantum entanglement. Na- ture Reviews Physics, 2:365, 2020.</p>
<p>Mermin inequalities for perfect correlations in many-qutrit systems. Jay Lawrence, Physical Review A. 95442123Jay Lawrence. Mermin inequalities for perfect corre- lations in many-qutrit systems. Physical Review A, 95(4):042123, 2017.</p>
<p>Layered quantum key distribution. Matej Pivoluska, Marcus Huber, Mehul Malik, Physical Review A. 97332312Matej Pivoluska, Marcus Huber, and Mehul Malik. Lay- ered quantum key distribution. Physical Review A, 97(3):032312, 2018.</p>
<p>Experimental creation of multi-photon high-dimensional layered quantum states. Xiao-Min Hu, Wen-Bo Xing, Chao Zhang, Bi-Heng Liu, Matej Pivoluska, Marcus Huber, Yun-Feng Huang, Chuan-Feng Li, Guang-Can Guo, arXiv:2001.06253Xiao-Min Hu, Wen-Bo Xing, Chao Zhang, Bi-Heng Liu, Matej Pivoluska, Marcus Huber, Yun-Feng Huang, Chuan-Feng Li, and Guang-Can Guo. Experimental cre- ation of multi-photon high-dimensional layered quantum states. arXiv:2001.06253, 2020.</p>
<p>Multiphoton entanglement and interferometry. Jian-Wei Pan, Zeng-Bing Chen, Chao-Yang Lu, Harald Weinfurter, Anton Zeilinger, Marekżukowski , Reviews of Modern Physics. 842777Jian-Wei Pan, Zeng-Bing Chen, Chao-Yang Lu, Harald Weinfurter, Anton Zeilinger, and MarekŻukowski. Mul- tiphoton entanglement and interferometry. Reviews of Modern Physics, 84(2):777, 2012.</p>
<p>Automated search for new quantum experiments. Mario Krenn, Mehul Malik, Robert Fickler, Physical Review Letters. 116990405Radek Lapkiewicz, and Anton ZeilingerMario Krenn, Mehul Malik, Robert Fickler, Radek Lap- kiewicz, and Anton Zeilinger. Automated search for new quantum experiments. Physical Review Letters, 116(9):090405, 2016.</p>
<p>A search algorithm for quantum state engineering and metrology. P A Knott, New Journal of Physics. 18773033PA Knott. A search algorithm for quantum state engineering and metrology. New Journal of Physics, 18(7):073033, 2016.</p>
<p>Machine learning for long-distance quantum communication. Julius Wallnöfer, A Alexey, Wolfgang Melnikov, Hans J Dür, Briegel, PRX Quantum. 1110301Julius Wallnöfer, Alexey A Melnikov, Wolfgang Dür, and Hans J Briegel. Machine learning for long-distance quan- tum communication. PRX Quantum, 1(1):010301, 2020.</p>
<p>Experimental quantum cloning in a pseudo-unitary system. Xiang Zhan, Kunkun Wang, Lei Xiao, Zhihao Bian, Yongsheng Zhang, C Barry, Chengjie Sanders, Peng Zhang, Xue, Physical Review A. 101110302Xiang Zhan, Kunkun Wang, Lei Xiao, Zhihao Bian, Yongsheng Zhang, Barry C Sanders, Chengjie Zhang, and Peng Xue. Experimental quantum cloning in a pseudo-unitary system. Physical Review A, 101(1):010302, 2020.</p>
<p>Computer-inspired quantum experiments. Mario Krenn, Manuel Erhard, Anton Zeilinger, Nature Reviews Physics. 2Mario Krenn, Manuel Erhard, and Anton Zeilinger. Computer-inspired quantum experiments. Nature Re- views Physics, 2:649-661, 2020.</p>
<p>Conceptual understanding through efficient inverse-design of quantum optical experiments. Mario Krenn, Jakob Kottmann, Nora Tischler, Alán Aspuru-Guzik, arXiv:2005.06443Mario Krenn, Jakob Kottmann, Nora Tischler, and Alán Aspuru-Guzik. Conceptual understanding through ef- ficient inverse-design of quantum optical experiments. arXiv:2005.06443, 2020.</p>
<p>Structure of multidimensional entanglement in multipartite systems. Marcus Huber, Julio I De Vicente, Physical Review Letters. 110330501Marcus Huber and Julio I de Vicente. Structure of multi- dimensional entanglement in multipartite systems. Phys- ical Review Letters, 110(3):030501, 2013.</p>
<p>Entropy vector formalism and the structure of multidimensional entanglement in multipartite systems. Marcus Huber, Martí Perarnau-Llobet, Julio I De Vicente, Physical Review A. 88442328Marcus Huber, Martí Perarnau-Llobet, and Julio I de Vi- cente. Entropy vector formalism and the structure of multidimensional entanglement in multipartite systems. Physical Review A, 88(4):042328, 2013.</p>
<p>Multiphoton entanglement in high dimensions. Mehul Malik, Manuel Erhard, Marcus Huber, Mario Krenn, Robert Fickler, Anton Zeilinger, Nature Photonics. 104248Mehul Malik, Manuel Erhard, Marcus Huber, Mario Krenn, Robert Fickler, and Anton Zeilinger. Multi- photon entanglement in high dimensions. Nature Pho- tonics, 10(4):248, 2016.</p>
<p>Experimental greenberger-hornezeilinger entanglement beyond qubits. Manuel Erhard, Mehul Malik, Mario Krenn, Anton Zeilinger, Nature Photonics. 1212Manuel Erhard, Mehul Malik, Mario Krenn, and Anton Zeilinger. Experimental greenberger-horne- zeilinger entanglement beyond qubits. Nature Photonics, 12(12):759-764, 2018.</p>
<p>A simple method of preparing pure states of an optical field, of implementing the einstein-podolskyrosen experiment, and of demonstrating the complementarity principle. Dn Klyshko, Soviet Physics Uspekhi. 31174DN Klyshko. A simple method of preparing pure states of an optical field, of implementing the einstein-podolsky- rosen experiment, and of demonstrating the complemen- tarity principle. Soviet Physics Uspekhi, 31(1):74, 1988.</p>
<p>Dynamic dna devices and assemblies formed by shape-complementary, non-base pairing 3d components. Thomas Gerling, Klaus F Wagenbauer, Andrea M Neuner, Hendrik Dietz, Science. 3476229Thomas Gerling, Klaus F Wagenbauer, Andrea M Ne- uner, and Hendrik Dietz. Dynamic dna devices and as- semblies formed by shape-complementary, non-base pair- ing 3d components. Science, 347(6229):1446-1452, 2015.</p>
<p>Biotechnological mass production of dna origami. Florian Praetorius, Benjamin Kick, L Karl, Maximilian N Behler, Dirk Honemann, Hendrik Weuster-Botz, Dietz, Nature. 5527683Florian Praetorius, Benjamin Kick, Karl L Behler, Max- imilian N Honemann, Dirk Weuster-Botz, and Hendrik Dietz. Biotechnological mass production of dna origami. Nature, 552(7683):84-87, 2017.</p>
<p>Chemical reaction networks: a graph-theoretical approach. Andrew V Oleg N Temkin, D G Zeigarnik, Bonchev, CRC PressOleg N Temkin, Andrew V Zeigarnik, and DG Bonchev. Chemical reaction networks: a graph-theoretical ap- proach. CRC Press, 1996.</p>
<p>Complex chemical reaction networks from heuristics-aided quantum chemistry. Dmitrij Rappoport, J Cooper, Dmitry Galvin, Alán Yu Zubarev, Aspuru-Guzik, Journal of chemical theory and computation. 103Dmitrij Rappoport, Cooper J Galvin, Dmitry Yu Zubarev, and Alán Aspuru-Guzik. Complex chemical reaction networks from heuristics-aided quantum chem- istry. Journal of chemical theory and computation, 10(3):897-907, 2014.</p>            </div>
        </div>

    </div>
</body>
</html>
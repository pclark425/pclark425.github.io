<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7323 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7323</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7323</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-269005030</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.05231v2.pdf" target="_blank">PromptAD: Learning Prompts with only Normal Samples for Few-Shot Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> The vision-language model has brought great improvement to few-shot industrial anomaly detection, which usually needs to design of hundreds of prompts through prompt engineering. For automated scenarios, we first use conventional prompt learning with many-class paradigm as the baseline to automatically learn prompts but found that it can not work well in one-class anomaly detection. To address the above problem, this paper proposes a one-class prompt learning method for few-shot anomaly detection, termed PromptAD. First, we propose semantic concatenation which can transpose normal prompts into anomaly prompts by concatenating normal prompts with anomaly suffixes, thus constructing a large number of negative samples used to guide prompt learning in one-class setting. Furthermore, to mitigate the training challenge caused by the absence of anomaly images, we introduce the concept of explicit anomaly margin, which is used to explicitly control the margin between normal prompt features and anomaly prompt features through a hyper-parameter. For image-level/pixel-level anomaly detection, PromptAD achieves first place in 11/12 few-shot settings on MVTec and VisA.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7323",
    "paper_id": "paper-269005030",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00795325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PromptAD: Learning Prompts with only Normal Samples for Few-Shot Anomaly Detection
16 Jul 2024</p>
<p>Xiaofan Li 
East China Normal University
ShanghaiChina</p>
<p>Zhizhong Zhang 
East China Normal University
ShanghaiChina</p>
<p>Xin Tan 
East China Normal University
ShanghaiChina</p>
<p>Chongqing Institute of East China Normal University
ChongqingChina</p>
<p>Chengwei Chen 
Chongqing Institute of East China Normal University
ChongqingChina</p>
<p>Yanyun Qu yyqu@xmu.edu.cn 
The Navy Military Medical University
ShanghaiChina</p>
<p>Yuan Xie 
East China Normal University
ShanghaiChina</p>
<p>Chongqing Institute of East China Normal University
ChongqingChina</p>
<p>Lizhuang Ma lzma@cs.ecnu.edu.cn 
East China Normal University
ShanghaiChina</p>
<p>Xiamen University
FujianChina</p>
<p>PromptAD: Learning Prompts with only Normal Samples for Few-Shot Anomaly Detection
16 Jul 202482E54AF4C62EE0CE52627FB4681C10CCarXiv:2404.05231v2[cs.CV]
The vision-language model has brought great improvement to few-shot industrial anomaly detection, which usually needs to design of hundreds of prompts through prompt engineering.For automated scenarios, we first use conventional prompt learning with many-class paradigm as the baseline to automatically learn prompts but found that it can not work well in one-class anomaly detection.To address the above problem, this paper proposes a one-class prompt learning method for few-shot anomaly detection, termed PromptAD.First, we propose semantic concatenation which can transpose normal prompts into anomaly prompts by concatenating normal prompts with anomaly suffixes, thus constructing a large number of negative samples used to guide prompt learning in one-class setting.Furthermore, to mitigate the training challenge caused by the absence of anomaly images, we introduce the concept of explicit anomaly margin, which is used to explicitly control the margin between normal prompt features and anomaly prompt features through a hyper-parameter.For image-level/pixel-level anomaly detection, PromptAD achieves first place in 11/12 few-shot settings on MVTec and VisA.</p>
<p>Introduction</p>
<p>Anomaly detection (AD) [4,39,52] is a critical task in computer vision [26,29,32,33], with widespread applications of defect detection in industry and medicine.This paper focuses on unsupervised industrial anomaly detection, which poses a challenge known as a one-class classification (OCC) † Corresponding author.All results are on the MVTec.[41] setting.In this framework, only normal samples are available during training, but in the testing phase, the model is expected to identify anomalous samples.Since industrial anomaly detection typically customizes a model for various industrial production lines, the ability to rapidly train models with few samples holds significant promise for practical applications.</p>
<p>Due to the strong zero-shot ability of the foundation models [27,36,38], WinCLIP [21] was proposed as the first work utilizing the vision-language foundation model (i.e., CLIP [37]) to enhance the model's anomaly detection performance in few-shot settings.To better leverage prompt guidance, WinCLIP introduces a prompt engineer strategy called "Prompt Ensemble" which combines a sufficient number of manually-designed prompts.For example, some manual prompts (e.g., a cropped photo of a [], a blurry photo of the [], etc.) are combined together as the normal prompts.As shown in Figure 1 (right), with the number of prompts increasing, WinCLIP's perfor-mance improves, reaching a saturation point at around 1000 prompts.Other methods like SAA+ [7] and AnoVL [13] also employ prompt engineering to enhance model performance, which has become a rite of prompt-guided anomaly detection.Prompt engineering involves human intervention and requires careful design, which does not meet the automation requirements of industrial scenarios.</p>
<p>As illustrated in Fig. 1 (left a.), prompt learning [59] aims to automatically learn prompts through contrastive learning [8,18] for guiding image classification.The idea of prompt learning for anomaly detection is intriguing.However, as shown in Figure 1 (right), due to the one-class setting of anomaly detection, using the above prompt learning paradigm [59] as the baseline does not work well and is inferior to WinCLIP [21] with manual prompts on the image-level result.The main challenges are as follows: 1) prompt learning relies on contrastive learning, how to design prompts to complete the contrastive learning in the one-class setting?2) With the absence of anomaly samples, how to control the marginal distance between normal prompts and anomaly prompts?</p>
<p>In this paper, we propose the one-class prompt learning with only normal samples for AD termed PromptAD.To solve the first challenge above, we propose semantic concatenation (SC).Intuitively, concatenating a prompt with antisense texts can transpose its semantics.According to this idea, as illustrated in Figure 1  [with][f law] which is converted into an anomaly prompt and can be used as a negative prompt of normal sample during prompt learning.Due to the manually annotated anomalous texts are very limited.To expand the richness of anomaly information, SC also designs learnable anomaly prompts by concatenating a suffix of learnable tokens with a normal prompt, for instance
[P 1 ][P 2 ] . . . [P E N ][obj.][A 1 ][A 2 ] . . . [A E A ],
where [A i ] is learnable token.The distribution of learnable anomaly prompts and manual anomaly prompts are aligned to ensure that the learnable anomaly prompts learn more correct anomaly information.</p>
<p>Furthermore, in anomaly detection, anomaly samples are unavailable, making it impossible to explicitly control the margin between normal and anomaly prompt features through contrastive loss.To address the second challenge, we propose the concept of Explicit Anomaly Margin (EAM), where a hyper-parameter is introduced to ensure that the distance between normal features and normal prompt features is smaller than the distance between normal features and anomaly prompt features.Thus ensuring a sufficient margin between normal prompts and anomaly prompts.Figure 1 (right) illustrates our great ad-vantages, it can be seen that (compared with the WinCLIP [21] and Baseline [59]) PromptAD achieves 91.3%(↑1.2%and ↑9.8%)/92.5%(↑7.7%and ↑3.7%) image-level/pixellevel anomaly detection results with only 10∼20 (↓ ∼980 and ↓ 0) prompts.</p>
<p>To summarize, the main contributions of this paper are: • We explore the feasibility of prompt learning in one-class anomaly detection, and propose a one-class prompt learning method termed PromptAD, which thoroughly beats conventional many-class prompt learning.</p>
<p>Related Work</p>
<p>Vision-Language Model.Leveraging contrastive learning [8,18] and vision transformer [61], some vision-language models (VLM) [2,23,27,37] have recently achieved great success.CLIP is one of the most commonly used VLMs, which is trained on web-scale image-text and shows strong zero-shot classification ability.The code of CLIP for LAION-400M [43] and LAION-5B [44] scale pre-training is open-scoured by OpenCLIP [20].With the pre-trained CLIP and prompt engineer, huge leaps were made for some downstream tasks [9,28,51,60].Influenced by the success of prompt learning [24,46] in Natural Language Processing (NLP), there has been a surge of prompt learning methods [15,50,58,59] in recent times for few-shot image classification tasks.These methods aim to automatically learn better prompts through contrastive learning [8,18] for guiding image classification based on CLIP.</p>
<p>Anomaly Detection.Most of the AD methods mainly focus on three paradigms: feature embedding paradigm, knowledge distillation paradigm, and reconstruction-based paradigm.The feature embedding paradigm [1,11,12,34,35,39,40,53] extracts the patch features of the image through the neural network and then performs anomaly detection,.The knowledge distillation paradigm [3,16,42,49,49,56] lets the student network only learn the knowledge of the normal samples of the teacher network, and complete anomaly detection through the difference between the teacher and the student.The reconstruction paradigm [17,54,55] hopes that the model can reconstruct the anomaly image into a normal image, and realize anomaly  detection by the difference between the reconstructed image and the anomaly image.</p>
<p>Few-Shot Anomaly Detection.TDG [45] and RegAD [19] are the first to explore few-shot anomaly detection methods, and PatchCore [39] and DifferNet [40] also demonstrated the performance in few-shot settings.WinCLIP [21] and RWDA [47] introduce the CLIP model to anomaly detection and greatly improve the performance in the few-shot setting.The latest FastRecon [14] reconstructs anomaly features by regression with distribution regularization and achieves excellent performance.</p>
<p>Preliminaries</p>
<p>CLIP and Prompt Learning</p>
<p>Contrastive Language Image Pre-training termed CLIP [37] is a large-scale vision-language model which is famous for its zero-shot classification ability.Specifically, giving an unknown image i, and K text-prompts {s 1 , s 2 , ..., s K }, CLIP can predict the distribution of i belonging to these K text-prompts:
p(y|i) = exp &lt; f (i), g(s y )/τ &gt; K i=1 exp &lt; f (i), g(s i )/τ &gt; ,(1)
where f (•) and g(•) are visual and text encoder respectively.&lt; •, • &gt; represents cosine similarity, τ is the temperature hyper-parameter.The initial text prompt used for CLIP zero-shot classification is still simple, such as a photo of [class], etc., slightly better than directly using the name of the class as the prompt.</p>
<p>Prompt Learning.Inspired by the success of prompt learning in natural language processing (NLP) [24,46], CoOp [59] introduces this paradigm into few-shot classification, aiming to automatically learn efficient prompts for CLIP.Specifically, the prompt used in CoOp is not the frozen text description, but a set of trainable parameters:
s k = [P 1 ][P 2 ] . . . [P E P ][class k ],(2)
where</p>
<p>CLIP Surgery</p>
<p>As a classification model, CLIP is far less adaptive in prompt-guided image localization tasks without fine-tuning.</p>
<p>To find out why CLIP fails to image localization tasks, some CLIP explainable works [31,57] analyze the mechanism that how CLIP extracts visual features.These works observed that the global feature extraction of Q-K selfattention [48] affects the localization ability of CLIP, which is as follows:
Attn(Q, K, V) = sof tmax(Q • K T • scale) • V.(3)
To this end, CLIP-Surgery [31] proposes a V-V attention mechanism to enhance the model's attention to local features without destroying the original structure.As shown in Figure 2, the feature extraction process is described as follows:
Z l−1 ori = [t cls ; t 1 ; t 2 , ...; t T ],(4)Z l−1 = [t ′ cls ; t ′ 1 ; t ′ 2 , ...; t ′ T ],(5)[Q l , K l , V l ] = QKV P roj. l (Z l−1 ori ),(6)Z l = P roj. l (Attn(V l , V l , V l )) + Z l−1 ,(7)
where Z l−1 ori denotes the (l − 1)-th layer output of the original CLIP visual encoder and Z (l−1) denotes the local-aware output of layer l − 1, QKV P roj.l and P roj l denote the QKV projection and output projection whose parameters are initialized by the visual encoder parameters of the original CLIP.The final original outputs and local-aware outputs are Z ori and Z, the CLS feature Z ori [0] ∈ R d is used for image-level anomaly detection and the local feature map Z[1 :] ∈ R T ×d is used for pixel-level anomaly detection.In this paper, we use modified CLIP as the backbone and term it VV-CLIP.</p>
<p>Methodology</p>
<p>Overview</p>
<p>An overview of our proposed PromptAD is illustrated in Figure 2. PromptAD is built on VV-CLIP whose visual encoder is used to extract global and local features.The proposed semantic concatenation (SC) is used to design prompts.Specifically, N learnable normal prefixes and the objective name are concatenated to get normal prompts (NPs), then N normal prompts are concatenated with M manual anomaly suffixes and L learnable anomaly suffixes respectively to obtain N × M manual anomaly prompts (MAPs) and N × L learnable anomaly prompts (LAPs).The visual features and prompt features are used to complete prompt learning by contrastive loss and the proposed explicit anomaly margin (EMA) loss.EMA can control the explicit margin between the normal prompt features and anomaly prompt features through a hyper-parameter.Finally, the prompts obtained by prompt learning are used for prompt-guided anomaly detection (PAD).</p>
<p>In addition to PAD, referring to WinCLIP+ [21], we also introduce vision-guided anomaly detection (VAD).Specifically, as shown in Figure 2, during training, the i-th layer features (without CLS feature) output by the visual encoder are stored as normal visual memory which is denoted as R.In the testing phase, the ith layer feature map F ∈ R h×w×d of a query image is compared with R to obtain the anomaly score map M ∈ [1, 0] h×w :
M ij = min r∈R 1 2 (1− &lt; F ij , r &gt;).(8)
In practice, we use the intermediate features of two layers as memory to get two score maps for each query image and then average the two score maps to get the final visionguided score map M v .</p>
<p>Semantic Concatenation</p>
<p>Only normal samples are obtainable during anomaly detection training, which leads to no negative samples for guiding prompt learning and thus impairs its effect.We found that the semantics of prompts can be changed by concatenating.For example, a photo of cable has normal semantics, and after concatenating it with a suffix, a photo of cable with flaw is converted into anomaly semantics.In this way, we propose semantic concatenation (SC) which can transpose normal prompts to anomaly prompts by concatenating normal prompts with anomaly suffixes, so as to construct sufficient contrast prompts based on learnable normal prompts.Specifically, following the format of CoOp [59], the learnable normal prompt (NP) is designed as follows:
s n = [P 1 ][P 2 ] . . . [P E N ][obj.],(9)
where E N denotes the length of the learnable normal prefix and [obj.]represents the name of the object being detected.The learnable normal prompt can be transposed to an anomaly prompt after concatenating with the anomaly suffixes.In particular, we generated anomaly suffixes from the anomaly labels of the datasets [4,61], such as [] with color stain, [] with crack, etc., and then concatenate these texts with the NP to obtain the manual anomaly prompt (MAP):
s m = [P 1 ][P 2 ] . . . [P E N ][obj.][with][color][stain],(10)
where the prefix is a trainable NP and the suffix is a manual anomaly text.In addition, we combine NP with a learnable token suffix to design the learnable anomaly prompt (LAP):
s l = [P 1 ][P 2 ] . . . [P E N ][obj.][A 1 ] . . . [A E A ],(11)
where E A denotes the length of learnable anomaly suffix.It should be noted that the parameters of prompts concatenated by the same normal prefix or anomaly suffix are shared.During training, NPs move close to normal visual features, while MAPs and LAPs move away from normal visual features.The training loss for prompt learning is consistent with the CLIP training loss as follows:
L clip = E z   −log exp(<z, wn /τ >) exp &lt; z, wn /τ &gt; + w∈W exp &lt; z, w/τ &gt;   ,(12)
where z denotes normal visual feature, wn = N i=1 g(s n i ) N is the prototype of normal prompt features, W = {g(s)|s ∈ MAPs ∪ LAPs} is a set containing all anomaly prompt features.Since more negative samples can produce a better contrastive learning effect [18], each anomaly prompt feature is compared with the visual feature.</p>
<p>Remark.In the one-class anomaly detection, conventional prompt learning can only design learnable normal prompts, which is not conducive to the effect of contrastive loss.The proposed semantic concatenation can transform the semantics of normal prompts into anomaly semantics with shared parameters, which can make normal samples contrast with the semantic transposes (anomaly prompts).</p>
<p>Explicit Anomaly Margin</p>
<p>Due to the lack of anomaly visual samples in the training, the MAPs and LAPs can only take normal visual features as negative samples for contrast and lack an explicit margin between the normal and anomaly prompts.Therefore, we propose the explicit anomaly margin (EAM) for AD prompt learning, which can control the margin between normal prompt features and anomaly prompt features.EAM is actually a regularization loss implemented via a margin hyper-parameter, which is defined as:
L ema = E z max 0, d( z ∥z∥ 2 , wn ∥ wn ∥ 2 ) − d( z ∥z∥ 2 , wa ∥ wa ∥ 2 ) ,(13)
where d(•, •) represents euclidean distance, and wa is the prototype of all anomaly prompt features:
wa = N ×M i=1 g(s m i ) + N ×L i=1 g(s l i ) N × M + N × L . (14)
In CLIP, the final features are all projected onto the unit hyper-sphere, thus the features in L ema are also normalized, and the margin is fixed to zero.Compared to contrastive loss (L clip ), EMA loss guarantees a larger distance between normal samples and the anomaly prototype than between normal samples and the normal prototype, resulting in an explicit discrimination between normal and anomaly prototypes.</p>
<p>In addition, since MAPs contain sufficient anomaly information while LAPs are initialized without any semantic guidance, aligning them helps LAPs to mimic the distribution of MAPs.Specifically, we align the means of the two distributions using the squared l 2 norm:
L align = λ • wm ∥ wm ∥ 2 − wl ∥ wl ∥ 2 2 2 ,(15)
where wm and wl are the feature means of MAPs and LAPs, respectively, and λ is a hyper-parameter controlling the alignment degree of MAPs and LAPs.</p>
<p>Anomaly Detection</p>
<p>In the testing phase, wn is used as the normal prototype and wa is used as the anomaly prototype to complete promptguided anomaly detection.The image-level score S t ∈ [0, 1] and pixel-level score map M t ∈ [0, 1] h×w are calculated through:
score = exp &lt; z t , wn /τ &gt; exp &lt; z t , wn /τ &gt; +exp &lt; z t , wa /τ &gt; ,(16)
where z t is a global/local image feature for imagelevel/pixel-level anomaly detection.Finally, vision-guided M v and prompt-guided M t are fused to obtain the pixel-level anomaly score map, and the maximum value of M v and S t are fused to obtain the imagelevel anomaly score:
M pix = 1.0/(1.0/M v + 1.0/M t ),(17)S img = 1.0/(1.0/ max ij M v + 1.0/S t ),(18)
where the fusion method we use is harmonic mean, which is more sensitive to smaller values [21].</p>
<p>Experiments</p>
<p>We complete the comparison experiments between Promp-tAD and the latest methods under 1, 2, and 4-shot settings, which include both image-level and pixel-level results.In addition, we also compare the many-shot and fullshot methods to show the powerful few-shot performance of PromptAD.Finally, we conduct ablation experiments to verify the improvement of prompt learning by the proposed SC and EAM, and show the impact of different CLIP transformation methods [31,57] and hyper-parameters.</p>
<p>Dataset.In this paper, the benchmarks we use are MVTec [4] and VisA [61].Both benchmarks contain multiple subsets with only one object per subset.MVTec contains 15 objects with 700 2 − 900 2 pixels per image, and VisA contains 12 objects with roughly 1.5K × 1K pixels per image.</p>
<p>Anomaly detection is a one-class task, so the training set contains only normal samples, while the test set contains normal samples and anomaly samples with image-level and pixel-level annotations.In addition, the anomaly category present for each object is also annotated.</p>
<p>Evaluation metrics.We follow the literature [4] in reporting the Area Under the Receiver Operation Characteristic (AUROC) for both image-level and pixel-level anomaly detection.</p>
<p>Implementation details.We used the OpenCLIP [20] implementation of CLIP and its pre-trained parameters, in addition to the default values of the hyper-parameter τ .Referring to WinCLIP [21], we used LAION-400M [43] based CLIP with ViT-B/16+.</p>
<p>Image-level Comparison Results</p>
<p>The Image-level comparative experimental results of PromptAD and current methods are recorded in Table 1, where SPADE [11], PaDiM [12], and PatchCore [39] are the reformulations of traditional full-shot methods in the few-shot settings.It can be seen that the Image-level AD performance of these methods is very limited.Both Win-CLIP+ [21] and RWDA [47] introduce CLIP [37], which greatly improves the performance of Image-level AD under few-shot settings.</p>
<p>Pixel-level Comparison Results</p>
<p>The pixel-level comparative experimental results are recorded in Table 4.It can be seen that the CLIP-based method (WinCLIP+ [21]) and other methods perform comparably on pixel-level AD, and the improvement brought by the introduction of CLIP [37] is not as obvious as that on image-level AD.PromptAD achieves the best place on MVTec/VisA in the 1-shot and 2-shot settings, which are 0.7%/0.3%and 0.2%/0.3%higher than WinCLIP+, respectively.In the 4-shot setting, while PromptAD ranks first on VisA, it takes second place on MVTec, narrowly outperformed by FastRecon [14] with a 0.5% margin.</p>
<p>The quantitative results of anomaly localization are shown in Figure 3. Compared with PatchCore [39] and WinCLIP+ [21], PromptAD has a better anomaly localization capability for both objects and textures in the 1-shot setting.In addition, PromptAD can also locate some very small anomaly areas very accurately.</p>
<p>Compared With Many-shot Methods</p>
<p>In shot settings are recorded.It can be seen that compared with some methods under many-shot settings, PromptAD achieves better image-level results, and the pixel-level results are also competitive, which fully verifies the strong ability of PromptAD in the few-shot settings.In addition, PromptAD is superior to the early full-shot AD methods, MKD [42] and P-SVDD [53], but there is still a certain gap between PromptAD and the latest full-shot AD methods, PatchCore [39] and SimpleNet [34].</p>
<p>Ablation Study</p>
<p>We verify the impact of different modules of different proposed methods on the overall performance of PromptAD under 1-shot setting on MVTec [4] and VisA [61].These include semantic concatenation (SC) and explicit anomaly margin (EMA).Meanwhile, we also verified the effect of vision-guided anomaly detection (VAD).Results of the ablation study are recorded in Table 3.</p>
<p>Semantic Concatenation (SC).</p>
<p>The number of negative samples plays a crucial role in contrastive learning [8,18].Without the proposed SC, the conventional prompt learning paradigm [59] loses negative prompts for contrast, so the effect of prompt learning will be greatly reduced.As shown in Table 3, there is a significant drop in image and pixel level results on both MVTec [4] and VisA [61] when SC is not used.After using SC, the image-level/pixel-level results on MVTec (and VisA) are improved by 8.9%/3.9%(8.9%/5.0%),which indicates that SC can greatly improve the applicability of prompt learning in anomaly detection.Explicit Anomaly Margin (EAM).Since anomaly samples are absent during the training phase, it is hard to establish an explicit margin between the features of normal and anomaly prompts.EAM uses a hyper-parameter to control the margin between normal and anomaly prompt features, which can make up for the lack of contrastive loss.Table 3 shows that after using EAM, the image-level/pixel-level results on MVTec (and VisA) are improved by 0.9%/0.8%(1.9%/0.7%),respectively.</p>
<p>Vision-guided Anomaly Detection (VAD).PAD introduces more high-level semantic information but ignores many local details, which is not conducive to pixel-level anomaly detection.On the contrary, VAD using normal feature memory focuses on more local detail information.</p>
<p>In Table 3, PAD has better image-level results, while VAD has better pixel-level results, and the two have a good complementarity.Under the 1-shot setting, the results of PAD and VAD are fused by harmonic mean, and 94.6%/95.9%(86.8%/96.7%)image-level/pixel-level results are achieved on MVTec (and VisA).</p>
<p>Results of Different CLIP Transformations</p>
<p>Due to the inability of CLIP to directly complete promptguided localization tasks, some works have explored the transformations of CLIP [31,57].anomaly detection, where about 1000 prompts are used in 0-shot setting and our prompt learning method is used in 1shot setting.MaskCLIP [57] drops the QKV attention and leaves only V P roj.and P roj., and then embed local features after each layer of the visual encoder as in VV-CLIP.CLIP+Linear [10] adds a learnable linear layer to the visual encoder after each block to align the local features with prompt features.</p>
<p>As shown in Table 5, the results of the original CLIP under 0-shot are 22.5%/24.6%on MVTec/Visa, which is lower than the random prediction (50.0%).This is caused by the opposite visual activation [30,31] of CLIP.After the transformation of attention, MaskCLIP [57] and VV-CLIP [31] achieve a huge improvement of 63.0%/55.9%and 64.2%/58.3% on MVTec/Visa, respectively.The improvement of VV-CLIP is more obvious than that of MaskCLIP.</p>
<p>$852&amp;</p>
<p>SL[HOOHYHO LPDJHOHYHO Figure 5. Image-level/pixel-level results on MVTec [4] in the 1shot setting using different hyper-parameter λ.</p>
<p>We speculate that this is because VV-attention retains a certain information interaction while focusing on local information, while MaskCLIP completely removes attention.After using our method, the pixel-level results of MaskCLIP and VV-CLIP are increased by 6.1%/10.7%,and 5.8%/9.8% on MVTec/Visa, respectively.Furthermore, it is worth noting that prompt learning also leads to a significant 57.4%/55.8%improvement in pixel-level results of the original CLIP.However, when prompt learning is added with learnable linear layers, the effect decreases, which may be because there is mutual interference between prompt learning and the training of linear layers.</p>
<p>Hyper-parameter Analysis</p>
<p>We complete the effect of N , L and λ on PromptAD.λ is the hyper-parameter of the loss L algin , which controls the degree of alignment between MAPs and LAPs feature distributions.N is the number of NPs.L is the number of anomaly prompt suffixes, and N ×L is the number of LAPs.</p>
<p>Figure 4 illustrates the effect of, N and L on PromptAD.In Image-level results, N does not have a great influence, and there is no significant difference between N = 1 and N = 4. While, L has a significant influence, and larger L can lead to higher results.In pixel-level results, the effects of both N and L are relatively small, and larger L slightly improves the results.Figure 5 records the imagelevel/pixel-level results with different λ.It can be seen that the results are worse when the λ is equal to 0 or larger.This indicates that the distributions of MAPs and LAPs need to be aligned, but not over-aligned, which will reduce the diversity of the anomaly prompts and thus reduce the model's perception of anomaly image features.</p>
<p>Visualization Results</p>
<p>To quantify the effect of PromptAD, we visualize the visual and textual features after L 2 normalization.Specifically, we visualize 3 NPs, 3×13 MAPs, and 3×10 LAPs as well as 100 image-level/pixel-level normal visual features.Figure 6 shows the visualization results, it can be seen that there is very clear discrimination between normal prompt features and anomaly prompt features, and the overlap between normal prompt features and normal visual features is very high, which intuitively verifies the effectiveness of PromptAD.In addition, it's worth noting that the 3 normal prompt features do not collapse to one point, but fit the overall distribution of normal visual features as much as possible.</p>
<p>Conclusion</p>
<p>In this paper, we propose a novel anomaly detection method termed PromptAD which automatically learns prompts with only normal samples in the few-shot anomaly detection scenario.First, in order to cope with the challenge under the one-class task, we propose semantic concatenation to construct enough anomaly prompts through concatenating normal prompts and anomaly suffixes to guide prompt learning.Second, we propose the explicit anomaly margin loss, which explicitly determines the margin between normal prompt features and anomaly prompt features through a hyper-parameter.Finally, for image-level/pixellevel anomaly detection, PromptAD achieves first place in 11/12 few-shot tasks.</p>
<p>PromptAD: Learning Prompts with only Normal Samples for Few-Shot Anomaly Detection</p>
<p>Supplementary Material</p>
<p>A. Experimental details</p>
<p>Data pre-processing.Referring to WinCLIP [21], we employ the data pre-processing pipeline specified in Open-CLIP [20] for both the MVTec [4] and VisA [61]  Hyper-parameter.The length of trainable tokens in normal prompts (E N ) is set to 4, and the length of trainable tokens in learnable anomaly prompts (E A ) is set to 1.For each detection object, the number of normal prompts (N ) is set to 1, the number of learnable anomaly suffixes (L) is set to 4, and the number of manual anomaly suffixes (M ) depends on the number of anomaly labels in the dataset.λ is set to 0.001.Referring to CoOp [59], the optimizer parameters of prompt learning: learning rate, momentum, and weight decay are set to 0.002, 0.9, and 0.0005, respectively.</p>
<p>Manual anomaly suffixes.We used two kinds of manual anomaly suffixes: generic anomaly suffixes and objectcustomized anomaly suffixes.Generic anomaly suffixes are manually designed and object-customized anomaly suffixes are generated through anomaly labels in the datasets [4,61].</p>
<p>The specific details are shown in Figure 7.</p>
<p>Evaluation metrics.In addition to the results for the Area Under the Receiver Operator Curve documented in the body of the paper, We also supplement the image-level Precision-Recall (AUPR) results and pixel-level Per-region-overlap (PRO) [4,5] results.</p>
<p>Other details.Since model performance in the few-shot setting is affected by random sampling, we report the mean and standard deviation over 5 random seeds for each measurement.In addition, the few-shot results of SPADE [11], PaDiM [12], and PatchCore [39] in the experiments adopt the results recorded in WinCLIP [21].</p>
<p>B. Additional Qualitative Results</p>
<p>In Figure 8, we provide further qualitative results obtained from our (1-shot) PromtAD for pixel-level anomaly detection in MVTec [4] and VisA [61].It can be seen that Promp-tAD can accurately locate both large-area surface defects and small-area surface defects.In addition, as shown in Figure 9, we also provide quantitative results of PromptAD on some logical anomalies.Logical anomalies are mainly found in some industrial components in MVTec.It can be seen that PromptAD has poor detection results on the swap anomaly of "cable" and missing anomaly of "transistor", while PromptAD has good localization results on the flip anomaly of "Metal nut" and misplaced anomaly of "Transistor".Figure 10 shows PromptAD's detection results for extremely small anomalies, which are usually hard to detect by humans.For the convenience of viewing, we circle the anomaly positions in red circles, and it can be seen that PromptAD has difficulty in completing the accurate localization of extremely small anomalies.</p>
<p>C. Comparison with Other Prompt Learning Methods</p>
<p>PromptAD is the first prompt design paradigm for one class classification (OCC) promblem, which overcomes the poor performance of other prompt learning methods in anomaly detection.See Table 6, the classical prompt learning methods (1-shot) perform relatively poorly in anomaly detection tasks, and except for the pixel-level results of CoOp [59] on MVTec and VisA, the results of CoOp [59], Co-CoOp [58] and Maple [25] are even worse than contex- tual prompt engineering (CPE).PromptAD not only outperforms the classical prompt learning method in anomaly detection tasks, but also, compared with CPE, brings improvements of 0.8%/5.8%and 5.0%/8.9% on MVTec and VisA respectively (image-level/pixel-level).</p>
<p>D. Ablation Study</p>
<p>We further evaluate the impact of MAPs and LAPs on PromptAD, respectively.For better comparison, we removed the effect of vision-guided anomaly detection (VAD).Table 7 (second row) shows that without MAPs, the performance degradation is more pronounced, but it is still better than CPE, indicating that PromptAD can still automatically learn effective anomaly prompts even without using any manually annotated anomaly description suffix.can expand the set of exception hints and thus improve the model performance.</p>
<p>In addition, we explore the impact of CLIP's different visual backbones on PromptAD.The results of using different backbones are recorded in Table 8, where the self-attention modules of VIT add the vv-attention branches, and the attention pooling of ResNet101 adopts V-V attention.All of the backbones require no additional training.The overall performance of ViT is better than ResNet, VIT-L/14 shows better pixel-wise anomaly detection than VIT-B/16+.</p>
<p>E. Results on Other Benchmarks</p>
<p>In addition to the two datasets MVTec [4] and VisA [61], we also evaluate PromptAD in the few-shot setting on MPDD [22] and LOCO [6].We reproduce the results of PatchCore and WinCLIP+.As shown in</p>
<p>F. Detailed Comparison Results</p>
<p>In this section, we report the detailed subset-level results of PromptAD.In addition, we evaluate PromptAD's results on Image-level AUPR and pixel-level PRO.Specifically, the results for MVTec are recorded in</p>
<p>G. Visualization Results of Attention Map</p>
<p>To further analyze the working mechanism of CLIP [37], we provide visualization results of the attention map in the CLIP visual encoder (ViT-B/16+).Figure 11 is the visualization result of the original QK attention map.It can be seen that QK attention in the shallow layers focuses more on local information (main diagonal activations).From layer 5, QK attention starts to focus on more global information.Both global and local information play an important role in anomaly detection.Therefore, to preserve both local and global information, PromptAD uses the features of the 3 th and 8 th layers instead of the features of the 6 th and 9 th layers used by PatchCore [39] when storing visual features.</p>
<p>The comparison results are reported in Table 10, compared with 6 th +9 th features, there is a 1.2% improvement using the 3 th +8 th features.Figure 12 shows the visualization results of the VV attention map, it can be seen that compared with QK attention, VV attention focuses on local information from the first layer to the last layer, which is more conducive to completing the localization task.While, most layers of QK attention focus on global information, which is more conducive to classification tasks.</p>
<p>Figure 1 .
1
Figure 1.Left: Prompt learning under many-class and one-class settings.Right: The prompt-guided results of WinCLIP using different numbers of prompts, and the prompt-guided results of the baseline and our PromptAD under one-shot for prompt learning.All results are on the MVTec.</p>
<p>(left b.), SC first designs a learnable normal prompt such as [P 1 ][P 2 ] . . .[P E N ][obj.]for normal samples, and then manually concatenate various texts related to anomalies with the normal prompt such as [P 1 ][P 2 ] . . .[P E N ][obj.]</p>
<p>Figure 2 .
2
Figure 2. Illustration of PromptAD, which includes two novel modules: SC and EAM.The visual encoder has been transformed with v-v attention.The original branch is used to extract CLS feature, while the v-v attention branch is used to extract the feature map.</p>
<p>Figure 3 .
3
Figure 3. Qualitative comparison results of 1-shot pixel-level anomaly detection on MVTec [4] and VisA [61].</p>
<p>Figure 4 .
4
Figure 4. Image-level/pixel-level results on VisA [61] in 1-shot setting using different N and L.</p>
<p>Figure 6 .
6
Figure 6.Feature visualization results using T-SNE in 1-shot setting.The feature used is "cable" from the MVTec[4].</p>
<p>Figure 7 .Figure 8 .Figure 9 .
789
Figure 7. Illustration of manual anomaly suffixes.</p>
<p>Figure 10 .
10
Figure 10.Qualitative results of extremely small anomaly detection.</p>
<p>Figure 11 .Figure 12 .
1112
Figure 11.Visualization of the QK attention map in the vision encoder./D\HU /D\HU /D\HU /D\HU /D\HU /D\HU</p>
<p>[P 1 ][P 2 ] . . .[P E P ] are trainable tokens and [class k ] is k-th class name which is not trainable.
Prompt learn-ing aims to automatically train effective prompts to improveCLIP performance on downstream classification tasks.</p>
<p>Table 1 .
1
Comparison of image-level anomaly detection in AUROC on MVTec and VisA benchmarks.The best and second-best results are respectively marked in bold and underlined.† indicates CLIP-based methods.
MethodPublicMVTecVisA1-shot2-shot4-shot1-shot2-shot4-shotSPADE [11]arXiv'202081.0±2.082.9±2.684.8±2.579.5±4.080.7±5.081.7±3.4PaDiM [12]ICPR'202076.6±3.178.9±3.180.4±2.462.8±5.467.4±5.172.8±2.9PatchCore [39]CVPR'202283.4±3.086.3±3.388.8±2.679.9±2.981.6±4.085.3±2.1WinCLIP+ †[21]CVPR'202393.1±2.094.4±1.395.2±1.383.8±4.084.6±2.487.3±1.8RWDA †[47]BMVC'202393.3±0.594.0±0.794.5±0.783.4±1.785.6±1.486.6±0.9FastRcon [14]ICCV'2023-91.094.2---PromptAD †-94.6±1.795.7±1.596.6±0.986.9±2.388.3±2.089.1±1.7MethodPublicSettingimagepixelPromptAD-1-shot94.695.9PromptAD-4-shot96.696.5DiffNet [40]WACV'202116-shot87.3-TDG [45]ICCV'202110-shot78.0-RegAD [19]ECCV20228-shot91.296.7FastRecon [14]ICCV'20238-shot95.297.3MKD [42]CVPR'2021full-shot87.890.7P-SVDD [53]ACCV'2021full-shot95.296.0PatchCore [39]CVPR'2022full-shot99.198.1SimpleNet [34]CVPR'2023full-shot99.698.1
Compared with the above methods, PromptAD achieves significant improvement in three settings of the two benchmarks.Compared with WinCLIP+</p>
<p>Table 2
2
. Comparison with exiting many-shot methods in AUROC (image and pixel level) on MVTec.Results below our 1-shot are marked in red, and those below our 4-shot are marked in blue.and RWDA, PromptAD achieves 1.3%, 1.3%, and 1.4% (2.9%, 2.7%, 1.8%) improvement under the 1, 2, and 4shot Settings of MVTec (and VisA), respectively.In addition, PromptAD uses a smaller number of prompts than WinCLIP+ and RWDA.</p>
<p>Table 2 ,
2
the comparison results of PromptAD under few-shot settings with other methods under many-shot/full-
PADVADMVTecVisASCEAMimagepixelimagepixel✗✗✗81.587.872.685.5✓✗✗90.491.781.390.5✓✓✗91.392.583.291.8✗✗✓85.193.282.795.2✓✓✓94.695.986.996.7</p>
<p>Table 3 .
3
Image-level/pixel-level results (AUROC) of ablation study under 1-shot setting.PAD and VAD are prompt-guided and vision-guided anomaly detection, respectively, SC is semantic concatenation, and EAM is explicit anomaly margin.</p>
<p>Table 4 .
4
Comparison of pixel-level anomaly detection in AUROC on MVTec and VisA benchmarks.The best and second-best results are respectively marked in bold and underlined.† indicates CLIP-based methods.
MethodPublicMVTecVisA1-shot2-shot4-shot1-shot2-shot4-shotSPADE [11]arXiv'202091.2±0.492.0±0.392.7±0.395.6±0.496.2±0.496.6±0.3PaDiM [12]ICPR'202089.3±0.991.3±0.792.6±0.789.9±0.892.0±0.793.2±0.5PatchCore [39]CVPR'202292.0±1.093.3±0.694.3±0.595.4±0.696.1±0.596.8±0.3WinCLIP+ † [21]CVPR'202395.2±0.596.0±0.396.2±0.396.4±0.496.8±0.397.2±0.2FastRecon [14]ICCV'2023-95.997.0---PromptAD †-95.9±0.596.2±0.396.5±0.296.7±0.497.1±0.397.4±0.3</p>
<p>Table 5
5MethodMVTecVisACLIP [37]22.524.6CLIP [37] + ours79.980.4CLIP+Linear [10] + ours79.477.2MaskCLIP [57]85.580.5MaskCLIP [57] + ours91.691.2VV-CLIP [31]86.782.9VV-CLIP [31] + ours92.591.8
records the results of different CLIP transformations under pixel-level</p>
<p>Table 5 .
5
Pixel-level results (AUROC) of using different CLIP transformations on MVTec and VisA under 0-shot/1-shot settings.</p>
<p>Table 6 .
6
Results (AUROC) of different methods (w/o VAD).
MethodMVTecVisAimage pixel image pixel0-shotCPE+WinCLIP CPE+VV-CLIP91.8 90.585.1 86.778.1 77.279.6 82.9CoOp+VV-CLIP81.587.872.685.51-shotCoCoOp+VV-CLIP Maple60.6 66.852.1 64.961.6 60.572.3 61.5PromptAD91.392.583.291.8</p>
<p>Table 7
7MethodMVTecVisAimage pixel image pixel1CPE+VV-CLIP90.586.777.282.92 Prompt w/o MAPs87.689.878.886.23 Prompt w/o LAPs90.591.382.590.34PromptAD91.392.583.291.8Table 7. Effects (AUROC) of MAPs and LAPs (1-shot &amp; w/oVAD).Backbone Dataset1-shot2-shot4-shotResNet101 MVTec 85.8/93.0 87.6/94.3 90.3/94.8ViT-L/14MVTec 92.4/95.5 93.8/95.8 94.9/96.2ViT-B/16+ MVTec 94.6/95.9 95.7/96.2 96.6/96.5ResNet101VisA80.4/95.1 84.5/96.3 85.3/96.9ViT-L/14VisA85.2/96.8 86.3/97.2 86.7/97.4ViT-B/16+VisA86.9/96.7 88.3/97.1 89.1/97.4
(third row)shows that there is also a slight decrease in performance without LAPs, which indicates that LAPs</p>
<p>Table 8 .
8
Image-level/pixel-level results (AUROC) with other backbones.
MethodDataset1-shot2-shot4-shotPatchCore MPDD 59.2/78.5 59.6/79.2 79.9/79.8WinCLIP+ MPDD 68.2/92.6 69.3/94.7 75.2/96.0PromptAD MPDD 80.7/96.2 85.3/97.2 87.2/97.3PatchCore LOCO 64.9/70.3 65.4/71.5 68.7/72.2WinCLIP+ LOCO 68.0/71.2 69.7/71.9 71.3/72.8PromptAD LOCO 71.2/73.0 72.6/74.1 73.5/74.5</p>
<p>Table 9 .
9
Image-level/pixel-level results (AUROC) on MPDD and LOCO.</p>
<p>Table 9
9, compared with Patch-Core and WinCLIP+, PromptAD achieves the first place infew-shot settings of both datasets</p>
<p>Table 10 .
10
Pixel-level results (AUROC) of using different layer features as the feature memory.</p>
<p>Table 11 ,
11
12,15,16 and the results for VisA are recorded in Table 13,14,17,18 with the first place marked in bold and the second place mean underlined.</p>
<p>Table 11 .
11
Comparison of image-level anomaly detection in terms of subset-wise AUROC on MVTec.
MVTec1-shot2-shot4-shotImage-AUROCSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADBottle98.7±0.6 97.4±0.7 99.4±0.498.2±0.999.8±0.399.5±0.1 98.5±1.0 99.2±0.399.3±0.399.4±0.199.5±0.2 98.8±0.2 99.2±0.399.3±0.499.8±0.2Cable71.2±3.3 57.7±4.6 88.8±4.288.9±1.994.2±1.276.2±5.2 62.3±5.9 91.0±2.788.4±0.791.8±1.583.4±3.1 70.0±6.1 91.0±2.790.9±0.995.4±0.9Capsule70.2±3.0 57.7±7.3 67.8±2.972.3±6.884.6±6.770.9±6.1 64.3±3.0 72.8±7.077.3±8.891.3±7.778.9±5.5 65.2±2.5 72.8±7.082.3±8.991.5±1.5Carpet98.1±0.2 96.6±1.0 95.3±0.899.8±0.3100.0±0.098.3±0.4 97.8±0.5 96.6±0.599.8±0.3100.0±0.098.6±0.2 97.9±0.4 96.6±0.5 100.0±0.0 100.0±0.0Grid40.0±6.8 54.2±6.7 63.6±10.399.5±0.399.8±0.941.3±3.6 67.2±4.2 67.7±8.399.4±0.299.9±1.244.6±6.6 68.1±3.8 67.7±8.399.6±0.198.8±0.5Hazelnut95.8±1.3 88.3±2.6 88.3±2.797.5±1.499.8±0.896.2±2.1 90.8±0.8 93.2±3.898.3±0.7100.0±0.698.4±1.3 91.9±1.2 93.2±3.898.4±0.499.8±0.2Leather100.0±0.0 97.5±0.7 97.3±0.799.9±0.0100.0±0.0 100.0±0.0 97.5±0.9 97.9±0.799.9±0.0100.0±0.1 100.0±0.0 98.5±0.2 97.9±0.7 100.0±0.0 100.0±0.1Metal nut71.0±2.2 53.0±3.8 73.4±2.998.7±0.899.1±0.577.0±7.9 54.8±3.8 77.7±8.599.4±0.298.6±0.777.8±5.7 60.7±5.2 77.7±8.599.5±0.2100.0±0.1Pill86.5±3.1 61.3±3.8 81.9±2.891.2±2.192.6±1.584.8±0.9 59.1±6.4 82.9±2.992.3±0.793.6±0.886.7±0.3 54.9±2.7 82.9±2.992.8±1.092.9±0.8Screw46.7±2.5 55.0±2.5 44.4±4.686.4±0.965.0±2.946.6±2.2 54.0±4.4 49.0±3.886.0±2.171.0±2.050.5±5.4 50.0±4.1 49.0±3.887.9±1.283.6±2.3Tile99.9±0.1 92.2±2.2 99.0±0.999.9±0.0100.0±0.099.9±0.1 93.3±1.1 98.5±1.099.9±0.2100.0±0.1 100.0±0.0 93.1±0.6 98.5±1.099.9±0.1100.0±0.1Toothbrush71.7±2.6 82.5±1.2 83.3±3.892.2±4.998.9±1.078.6±3.2 87.6±4.2 85.9±3.597.5±1.697.5±1.578.8±5.2 89.2±2.5 85.9±3.596.7±2.698.1±0.6Transistor77.2±2.0 73.3±6.0 78.1±6.983.4±3.894.0±6.581.3±3.7 72.8±6.3 90.0±4.385.3±1.797.4±4.081.4±2.1 82.4±6.5 90.0±4.385.7±2.595.6±3.8Wood98.8±0.3 96.1±1.2 97.8±0.399.9±0.197.9±0.399.2±0.4 96.9±0.5 98.3±0.699.9±0.198.7±0.198.9±0.6 97.0±0.2 98.3±0.699.8±0.398.6±0.3Zipper89.3±1.9 85.8±2.7 92.3±0.588.8±5.993.9±3.293.3±2.9 86.3±2.6 94.0±2.194.0±1.495.8±1.795.1±1.3 88.3±2.0 94.0±2.194.5±0.595.0±2.3Mean81.0±2.0 76.6±3.1 83.4±3.093.1±2.094.6±1.782.9±2.6 78.9±3.1 86.3±3.394.4±1.395.7±1.584.8±2.5 80.4±2.5 88.8±2.695.2±1.396.6±0.9MVTec1-shot2-shot4-shotPixel-AUROCSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADBottle95.3±0.2 96.1±0.5 97.9±0.197.5±0.299.6±0.295.7±0.2 96.9±0.1 98.1±0.097.7±0.199.5±0.196.1±0.0 97.1±0.1 98.2±0.097.8±0.099.5±0.1Cable86.4±0.2 88.4±1.2 95.5±0.893.8±0.698.4±0.587.4±0.3 90.0±0.8 96.4±0.394.3±0.497.6±0.388.2±0.2 92.1±0.4 97.5±0.394.9±0.198.2±0.2Capsule96.3±0.2 94.5±0.6 95.6±0.494.6±0.899.5±0.796.7±0.1 95.2±0.5 96.5±0.496.4±0.399.3±0.597.0±0.2 96.2±0.4 96.8±0.696.2±0.599.3±0.3Carpet98.2±0.0 97.8±0.2 98.4±0.199.4±0.095.9±0.098.3±0.0 98.2±0.0 98.5±0.199.3±0.096.1±0.098.4±0.0 98.4±0.0 98.6±0.199.3±0.096.2±0.0Grid80.7±1.3 70.2±2.8 58.8±4.996.8±1.095.1±0.583.5±1.0 70.8±2.0 62.6±3.297.7±0.895.5±0.487.2±1.1 77.0±1.8 69.4±1.398.0±0.295.2±0.3Hazelnut97.2±0.1 95.4±0.7 95.8±0.698.5±0.297.3±0.497.6±0.1 96.8±0.3 96.3±0.698.7±0.197.6±0.397.7±0.1 97.2±0.2 97.6±0.198.8±0.097.9±0.2Leather99.1±0.0 98.5±0.1 98.8±0.299.3±0.093.3±0.099.1±0.0 98.7±0.1 99.0±0.199.3±0.093.2±0.099.1±0.0 98.8±0.0 99.1±0.099.3±0.093.9±0.0Metal nut83.8±0.7 74.6±1.1 89.3±1.490.0±0.697.3±0.785.8±1.1 80.3±2.1 94.6±1.491.4±0.497.4±0.787.1±0.7 82.7±3.9 95.9±1.892.9±0.497.7±0.5Pill89.4±0.4 84.8±1.0 93.1±1.196.4±0.398.4±0.489.9±0.2 87.3±0.7 94.2±0.397.0±0.298.5±0.390.7±0.2 88.9±0.5 94.8±0.497.1±0.098.5±0.2Screw94.8±0.2 83.3±0.7 89.6±0.594.5±0.491.4±0.595.6±0.4 89.8±0.8 90.0±0.795.2±0.395.1±0.796.4±0.4 90.8±0.2 91.3±1.096.0±0.594.9±0.8Tile91.7±0.3 84.1±1.1 94.1±0.596.3±0.292.8±0.192.0±0.1 87.7±0.2 94.4±0.296.5±0.194.1±0.192.2±0.1 88.9±0.3 94.6±0.196.6±0.194.1±0.1Toothbrush94.6±0.6 97.3±0.3 97.3±0.497.8±0.194.0±0.296.2±0.3 97.7±0.3 97.5±0.298.1±0.195.5±0.197.0±0.6 98.4±0.2 98.4±0.498.4±0.596.2±0.0Transistor71.4±1.3 90.2±2.8 84.9±2.785.0±1.899.1±1.972.8±0.9 92.3±2.1 89.6±0.988.3±1.099.0±1.173.4±0.7 94.0±2.7 90.7±1.488.5±1.299.0±0.6Wood93.4±0.1 90.7±0.4 92.7±0.994.6±1.089.4±0.393.8±0.1 91.9±0.1 93.2±0.795.3±0.489.1±0.293.9±0.1 92.2±0.1 93.5±0.395.4±0.290.6±0.1Zipper94.9±0.3 93.9±0.8 97.4±0.493.9±0.896.6±0.595.8±0.2 95.4±0.3 98.0±0.194.1±0.795.5±0.496.2±0.1 96.1±0.2 98.1±0.194.2±0.496.6±0.3Mean91.2±0.4 89.3±0.9 92.0±1.095.2±0.595.9±0.592.0±0.3 91.3±0.7 93.3±0.696.0±0.396.2±0.392.7±0.3 92.6±0.7 94.3±0.596.2±0.396.5±0.2</p>
<p>Table 12 .
12
Comparison of pixel-level anomaly detection in terms of subset-wise AUROC on MVTec.
VisA1-shot2-shot4-shotImage-AUROCSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADCandle86.1±5.670.8±4.185.1±1.493.4±1.490.3±2.791.3±3.375.8±2.185.3±1.594.8±1.091.0±1.292.8±2.1 77.5±1.6 87.8±0.895.1±0.393.0±1.2Capsules73.3±7.551.0±7.860.0±7.685.0±3.184.5±3.571.7±11.2 51.7±4.657.8±5.484.9±0.884.9±3.773.4±7.1 52.7±3.4 63.4±5.486.8±1.780.6±2.1Cashew95.9±1.162.3±9.989.5±4.494.0±0.495.6±1.097.3±1.474.6±3.693.6±0.694.3±0.594.7±1.496.4±1.3 77.7±3.2 93.0±1.595.2±0.893.6±2.2Chewinggum92.1±2.069.9±4.997.3±0.397.6±0.896.4±1.293.4±1.082.7±2.197.8±0.697.3±0.896.6±0.693.5±1.4 83.5±3.7 98.3±0.397.7±0.396.8±0.4Fryum81.1±4.058.3±5.975.0±4.888.5±1.990.3±1.890.5±3.969.2±9.083.4±2.490.5±0.489.2±0.992.9±1.6 71.2±5.9 88.6±1.390.8±0.589.0±2.3Macaroni166.0±10.5 62.1±4.668.0±3.482.9±1.588.6±3.169.1±8.262.2±5.075.6±4.683.3±1.984.2±2.565.8±1.2 65.9±3.9 82.9±2.785.2±0.988.2±2.5Macaroni255.8±6.147.5±5.955.6±4.670.2±0.969.1±3.058.3±4.450.8±2.957.3±5.671.8±2.082.6±0.956.7±3.2 55.0±2.9 61.7±1.870.9±2.281.2±1.8PCB187.2±2.376.2±1.278.9±1.1 75.6±23.088.7±0.786.7±1.1 62.4±10.8 71.5±20.076.7±5.290.9±5.483.4±8.5 82.6±1.5 84.7±6.788.3±1.790.9±2.5PCB273.5±3.761.2±2.081.5±0.862.2±3.971.6±4.170.3±8.166.8±2.084.3±1.762.6±3.773.0±3.071.7±7.0 73.5±2.4 84.3±1.067.5±2.678.6±2.5PCB372.2±1.0 51.4±12.2 82.7±2.374.1±1.179.1±3.675.8±5.767.3±3.884.8±1.278.8±1.976.2±2.279.0±4.1 65.9±1.9 87.0±1.183.3±1.780.3±1.7PCB493.4±1.376.1±3.693.9±2.885.2±8.991.4±3.286.1±8.2 69.3±13.7 94.3±3.282.3±9.997.5±2.495.4±2.3 85.4±2.0 95.6±1.687.6±8.097.8±1.4Pipe fryum77.9±3.266.7±2.290.7±1.797.2±1.196.9±0.278.1±3.075.3±1.893.5±1.398.0±0.698.9±0.379.3±0.9 82.9±2.2 96.4±0.798.5±0.498.6±0.2Mean79.5±4.062.8±5.479.9±2.983.8±4.086.9±2.380.7±5.067.4±5.181.6±4.084.6±2.488.3±2.081.7±3.4 72.8±2.9 85.3±2.187.3±1.889.1±1.7</p>
<p>Table 13 .
13
Comparison of image-level anomaly detection in terms of subset-wise AUROC on VisA.
VisA1-shot2-shot4-shotPixel-AUROCSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADCandle97.9±0.3 91.7±2.2 97.2±0.297.4±0.295.8±0.298.1±0.2 94.9±0.8 97.7±0.397.7±0.195.9±0.198.2±0.1 95.4±0.2 97.9±0.197.8±0.296.0±0.1Capsules95.5±0.5 70.9±1.1 93.2±0.996.4±0.695.4±0.996.5±0.9 75.7±1.7 94.0±0.296.8±0.396.1±0.797.7±0.1 79.1±0.7 94.8±0.597.1±0.296.8±0.6Cashew95.9±0.5 95.5±0.6 98.1±0.198.5±0.299.1±0.295.9±0.4 96.4±0.4 98.2±0.298.5±0.199.2±0.195.9±0.3 97.2±0.3 98.3±0.298.7±0.099.2±0.1Chewinggum 96.0±0.4 90.1±0.4 96.9±0.398.6±0.199.1±0.196.0±0.3 93.1±0.7 96.6±0.198.6±0.199.2±0.195.7±0.3 94.4±0.5 96.8±0.198.5±0.199.2±0.2Fryum93.5±0.3 93.3±0.6 93.3±0.596.4±0.395.4±0.393.9±0.2 94.1±0.6 94.0±0.397.0±0.296.4±0.294.4±0.1 95.0±0.4 94.2±0.297.1±0.196.6±0.2Macaroni197.9±0.2 89.4±0.9 95.2±0.496.4±0.697.8±0.198.5±0.2 91.7±0.3 96.0±1.396.5±0.798.3±0.198.8±0.1 93.5±0.5 97.0±0.397.0±0.298.2±0.1Macaroni294.1±1.0 86.4±1.1 89.1±1.696.8±0.496.6±0.595.2±0.4 90.1±0.8 90.2±1.996.8±0.697.2±0.396.4±0.2 90.2±0.3 93.9±0.397.3±0.397.0±0.3PCB194.7±0.4 89.9±0.3 96.1±1.596.6±0.696.6±0.896.5±1.5 90.6±0.6 97.6±0.997.0±0.996.9±0.496.8±1.5 93.2±1.5 98.1±1.098.1±0.998.2±0.3PCB295.1±0.2 90.9±1.4 95.4±0.293.0±0.493.5±0.995.7±0.1 93.9±0.9 96.0±0.393.9±0.294.8±0.896.3±0.0 93.7±1.0 96.6±0.294.6±0.495.3±0.5PCB396.0±0.1 93.9±0.3 96.2±0.394.3±0.395.9±0.596.6±0.1 95.1±0.5 97.1±0.195.1±0.296.1±0.496.9±0.0 95.7±0.1 97.4±0.295.8±0.196.8±0.2PCB492.0±0.6 89.6±0.6 95.6±0.694.0±0.995.5±0.592.8±0.3 90.7±0.9 96.2±0.495.6±0.395.6±0.394.1±0.2 92.1±0.5 97.0±0.296.1±0.396.2±0.4Pipe fryum98.4±0.2 97.2±0.6 98.8±0.298.3±0.299.1±0.298.7±0.1 98.1±0.4 99.1±0.198.5±0.299.4±0.298.8±0.0 98.5±0.1 99.1±0.098.7±0.199.3±0.3Mean95.6±0.4 89.9±0.8 95.4±0.696.4±0.496.7±0.496.2±0.4 92.0±0.7 96.1±0.596.8±0.397.1±0.396.6±0.3 93.2±0.5 96.8±0.397.2±0.297.4±0.3</p>
<p>Table 14 .
14
Comparison of pixel-level anomaly detection in terms of subset-wise AUROC on VisA.
MVTec1-shot2-shot4-shotImage-level AUPRSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADBottle99.6±0.1 99.2±0.2 99.8±0.199.4±0.399.8±0.199.8±0.0 99.6±0.3 99.8±0.199.8±0.199.9±0.199.9±0.0 99.7±0.0 99.8±0.199.8±0.1100.0±0.1Cable79.6±2.3 64.9±3.8 93.8±2.293.2±1.195.5±0.784.5±3.1 69.6±6.6 95.1±1.392.9±0.696.9±0.788.8±1.9 76.1±5.6 97.1±0.794.4±0.397.4±0.5Capsule91.2±0.9 86.9±2.2 89.4±2.091.6±2.797.8±3.191.6±2.1 88.4±0.8 91.0±2.993.3±3.697.0±3.094.4±1.9 87.8±0.8 94.9±1.195.1±3.398.6±2.2Carpet99.4±0.0 99.0±0.2 98.7±0.299.9±0.1100.0±0.099.5±0.1 99.4±0.1 99.0±0.199.9±0.1100.0±0.099.6±0.1 99.4±0.1 98.8±0.2 100.0±0.0 100.0±0.0Grid66.9±2.1 75.0±3.3 81.1±4.999.9±0.198.8±0.368.3±2.1 82.5±2.3 84.1±4.099.8±0.199.9±0.368.8±4.2 83.0±1.8 86.4±4.099.9±0.099.7±0.1Hazelnut97.9±0.6 93.3±1.7 92.9±2.298.6±0.799.7±0.398.0±1.1 94.1±0.5 96.0±2.099.1±0.499.8±0.299.1±0.7 94.8±0.6 97.0±1.299.1±0.299.9±0.2Leather100.0±0.0 99.2±0.2 99.1±0.2 100.0±0.0 100.0±0.0 100.0±0.0 99.2±0.3 99.3±0.2 100.0±0.0 100.0±0.0 100.0±0.0 99.6±0.1 99.6±0.1 100.0±0.0 100.0±0.0Metal nut91.7±0.8 82.0±2.7 91.0±1.199.7±0.299.6±0.193.7±2.4 82.2±1.4 92.3±4.099.9±0.0100.0±0.194.1±1.8 85.5±1.7 97.0±2.699.9±0.199.9±0.0Pill97.0±0.8 88.3±1.3 96.5±0.698.3±0.598.5±0.396.5±0.4 87.9±2.6 96.6±0.798.6±0.197.8±0.297.0±0.2 87.0±1.2 96.9±0.498.6±0.298.5±0.1Screw71.3±1.8 78.1±1.0 71.4±2.394.2±0.678.5±3.071.0±1.4 77.3±1.3 72.9±3.494.1±1.586.7±1.973.7±2.4 75.7±2.8 71.8±1.994.9±0.893.8±2.1Tile100.0±0.0 97.2±0.7 99.6±0.3 100.0±0.0 100.0±0.0 100.0±0.0 97.6±0.4 99.4±0.4 100.0±0.1 100.0±0.0 100.0±0.0 97.6±0.2 99.6±0.1 100.0±0.0 100.0±0.0Toothbrush88.3±0.6 93.7±0.5 93.5±1.496.7±2.098.9±0.490.8±1.3 95.2±1.6 94.1±1.499.0±0.699.3±0.491.3±2.6 95.8±0.7 94.8±0.798.7±1.199.7±0.1Transistor76.2±1.7 66.2±7.5 77.7±5.579.0±4.091.2±5.981.6±3.4 69.0±6.5 89.3±3.980.7±2.392.2±2.980.3±2.6 77.6±8.4 84.5±9.080.7±3.292.2±1.2Wood99.6±0.1 98.8±0.3 99.3±0.1 100.0±0.099.6±0.399.7±0.1 99.0±0.1 99.5±0.2 100.0±0.099.7±0.199.7±0.2 99.1±0.0 99.5±0.299.9±0.199.5±0.1Zipper96.9±0.5 95.5±0.9 97.2±0.396.8±1.899.0±0.698.2±0.8 95.4±1.0 97.8±1.098.3±0.499.3±0.598.6±0.4 96.2±0.8 99.1±0.798.5±0.298.5±0.3Mean90.6±0.8 88.1±1.7 92.2±1.596.5±0.997.1±1.091.7±1.2 89.3±1.7 93.8±1.797.0±0.797.9±0.792.5±1.2 90.5±1.6 94.5±1.597.3±0.698.5±0.5</p>
<p>Table 15 .
15
Comparison of image-level anomaly detection in terms of subset-wise AUPR on MVTec.
MVTec1-shot2-shot4-shotPixel-PROSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADBottle91.1±0.4 89.8±0.8 93.5±0.391.2±0.493.6±0.191.8±0.5 91.7±0.2 93.9±0.391.8±0.393.9±0.292.5±0.1 92.2±0.2 94.0±0.291.6±0.294.5±0.2Cable63.5±0.7 59.1±3.2 84.7±1.072.5±2.387.3±1.266.7±0.9 66.5±2.8 88.5±0.974.7±2.387.8±0.769.5±0.4 74.2±1.8 91.7±0.677.0±1.188.9±0.3Capsule92.7±0.4 80.0±2.0 83.9±0.985.6±2.780.1±1.793.4±0.3 82.3±2.1 86.6±1.090.6±0.679.2±2.294.1±0.6 85.7±1.3 87.8±1.990.1±1.588.7±2.0Carpet96.1±0.0 92.9±0.3 93.3±0.397.4±0.498.3±0.396.2±0.0 93.9±0.2 93.7±0.497.3±0.398.2±0.396.3±0.0 94.4±0.2 93.9±0.497.0±0.298.2±0.1Grid67.7±1.9 41.2±4.6 21.7±9.590.5±2.794.3±1.072.1±1.5 45.1±3.6 23.7±3.892.8±2.595.0±0.878.0±1.5 55.5±3.4 30.4±4.693.6±0.693.8±0.7Hazelnut94.9±0.3 85.7±1.9 88.3±1.393.7±0.992.9±0.595.6±0.2 89.4±0.9 89.8±1.394.2±0.393.4±0.595.6±0.1 90.4±0.7 92.0±0.394.2±0.395.2±0.5Leather98.7±0.0 95.6±0.2 95.2±1.098.6±0.098.7±0.598.8±0.0 96.2±0.2 95.9±0.398.3±0.498.7±0.498.8±0.0 96.3±0.1 96.4±0.198.0±0.498.4±0.5Metal nut73.4±1.1 38.1±1.6 66.7±2.984.7±1.183.1±0.678.1±1.8 48.2±5.0 79.6±4.286.7±0.887.7±1.181.2±1.4 54.0±8.8 83.8±5.589.4±0.187.6±0.3Pill92.8±0.3 78.9±0.6 89.5±1.693.5±0.290.8±0.493.3±0.2 84.3±0.4 91.6±0.594.5±0.290.5±0.493.9±0.2 86.6±0.4 92.5±0.494.6±0.392.0±0.1Screw85.0±0.8 51.6±1.7 68.1±1.382.3±1.178.1±1.787.2±1.2 69.5±2.1 69.0±2.184.1±0.574.7±1.489.5±1.3 72.3±0.8 72.4±3.186.3±1.886.7±2.5Tile84.2±0.4 66.7±1.5 82.5±1.189.4±0.490.7±0.384.6±0.2 71.9±0.5 82.5±0.589.6±0.490.9±0.284.9±0.1 73.6±0.9 83.0±0.189.9±0.390.9±0.2Toothbrush 83.5±1.3 82.1±1.5 79.0±2.485.3±1.090.1±0.587.4±1.1 83.3±2.6 81.0±0.784.7±1.491.6±0.589.0±1.1 87.1±1.7 85.5±3.086.0±3.391.3±0.2Transistor 55.3±2.0 70.3±7.0 70.9±4.665.0±1.867.5±3.757.6±1.4 76.5±5.5 78.8±1.568.6±1.168.1±2.158.5±0.7 82.2±7.4 79.5±2.869.0±1.173.0±1.2Wood92.9±0.1 86.5±0.6 87.1±1.091.0±0.692.4±0.893.1±0.1 88.0±0.2 86.8±1.491.8±0.691.6±0.693.2±0.1 88.4±0.2 87.7±0.491.7±0.391.4±0.5Zipper86.8±0.6 81.7±2.0 91.2±1.186.0±1.781.0±1.489.0±0.4 85.6±0.7 92.8±0.486.4±1.686.4±0.590.1±0.2 87.2±0.8 93.4±0.286.9±0.787.5±0.6Mean83.9±0.7 73.3±2.0 79.7±2.087.1±1.287.9±1.085.7±0.7 78.2±1.8 82.3±1.388.4±0.988.5±0.887.0±0.5 81.3±1.9 84.3±1.689.0±0.890.5±0.7</p>
<p>Table 16 .
16
Comparison of pixel-level anomaly detection in terms of subset-wise PRO on MVTec.
VisA1-shot2-shot4-shotImage-level AUPRSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADCandle86.5±4.369.2±3.986.6±2.393.6±1.593.7±2.990.7±3.272.8±1.086.8±1.795.1±1.193.6±1.192.6±1.9 72.5±1.1 88.9±1.195.3±0.492.9±1.1Capsules79.4±4.963.4±5.772.3±5.389.9±2.590.1±1.979.9±5.863.4±2.073.6±4.788.9±0.788.3±2.681.1±4.5 63.0±2.3 78.4±3.191.5±1.489.8±1.0Cashew97.9±0.478.2±5.794.6±2.097.2±0.297.6±0.798.6±0.686.1±2.296.9±0.397.3±0.297.4±0.598.3±0.6 88.4±2.0 96.5±0.797.7±0.497.0±1.1Chewinggum96.4±0.979.8±3.698.9±0.199.0±0.399.1±0.797.1±0.489.5±1.999.1±0.298.9±0.398.4±0.297.1±0.6 88.5±3.2 99.3±0.199.0±0.198.5±0.3Fryum89.8±1.874.5±2.987.6±2.494.7±1.093.8±1.094.5±2.381.0±5.492.1±1.395.8±0.296.0±0.795.8±1.0 81.5±3.0 95.0±0.696.0±0.393.6±0.5Macaroni161.9±11.2 60.4±2.967.8±3.484.9±1.286.3±1.964.5±9.563.1±4.374.9±5.284.7±1.591.1±1.760.2±2.7 64.9±2.1 82.1±3.586.5±0.689.2±1.3Macaroni252.7±4.251.7±5.054.9±3.268.4±1.872.5±2.555.9±3.152.7±1.557.2±2.670.4±1.884.7±1.651.9±2.3 54.9±2.5 60.2±3.069.6±2.882.2±1.0PCB184.9±3.768.6±2.472.1±2.5 76.5±19.0 88.0±11.383.8±2.160.4±7.7 72.6±16.478.3±4.380.9±6.383.2±7.2 77.4±2.9 81.0±9.287.7±1.790.1±3.6PCB274.9±2.963.3±1.284.4±0.464.9±3.375.4±2.771.7±6.668.9±2.686.6±1.165.8±4.073.0±4.874.2±5.0 75.0±1.7 86.2±1.071.3±3.475.3±2.5PCB375.5±2.1 52.3±10.8 84.6±1.573.5±1.675.2±3.878.3±5.265.2±3.886.1±0.580.9±1.682.8±2.281.0±3.6 64.5±2.4 88.3±1.184.8±1.883.5±1.6PCB492.9±1.674.7±2.692.8±3.1 78.5±15.590.5±1.281.9±11.2 67.6±11.9 93.2±3.472.5±16.294.5±2.994.8±2.9 84.0±2.0 94.9±1.285.6±8.997.5±1.3Pipe fryum88.3±2.079.2±1.595.4±0.698.6±0.598.3±0.188.1±1.784.5±1.796.8±0.799.0±0.399.1±0.188.8±1.0 89.8±1.7 98.3±0.399.2±0.299.3±0.1Mean82.0±3.368.3±4.082.8±2.385.1±4.088.4±2.682.3±4.371.6±3.884.8±3.285.8±2.790.0±2.183.4±2.7 75.6±2.2 87.5±2.188.8±1.890.8±1.3</p>
<p>Table 17 .
17
Comparison of image-level anomaly detection in terms of subset-wise AUPR on VisA.
VisA1-shot2-shot4-shotPixel-PROSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADSPADEPaDiMPatchCore WinCLIP+ PromptADCandle95.6±0.5 81.5±5.3 92.6±0.494.0±0.491.8±1.295.6±0.4 87.3±1.2 93.4±0.694.2±0.291.6±0.795.7±0.1 88.3±0.7 94.1±0.494.4±0.290.6±0.5Capsules83.1±1.1 30.6±1.1 66.6±4.573.6±3.570.0±1.685.4±3.1 38.4±3.7 67.9±2.375.9±1.970.8±2.589.0±1.2 43.3±2.0 69.0±3.277.0±1.472.4±3.2Cashew89.8±1.1 73.4±2.1 90.8±0.291.1±0.892.3±0.590.4±0.5 78.4±2.7 91.4±1.090.4±0.692.7±1.890.4±0.6 81.2±2.8 92.1±0.391.3±0.992.8±2.0Chewinggum 73.9±1.2 58.1±0.6 78.2±1.391.0±0.589.8±1.373.8±1.1 63.7±2.4 78.0±0.490.9±0.787.8±1.372.7±0.9 67.2±1.8 79.3±0.891.0±0.489.4±0.6Fryum83.7±1.2 71.1±1.6 78.7±2.389.1±1.083.5±3.184.5±0.9 71.2±0.8 81.4±2.889.3±0.286.2±3.186.2±0.9 73.2±1.3 81.0±1.289.7±0.580.3±0.7Macaroni192.0±0.6 62.2±4.4 83.4±1.384.6±2.387.5±1.993.9±0.8 71.8±2.4 86.2±4.685.2±1.490.6±1.595.1±0.4 76.6±2.1 89.6±0.786.8±0.891.5±1.3Macaroni280.0±3.3 54.9±3.6 66.0±3.089.3±2.480.6±1.581.7±1.5 65.6±3.4 67.2±6.588.6±1.782.7±1.086.0±0.8 65.9±1.5 78.3±0.990.5±1.387.2±0.6PCB181.3±5.7 63.9±1.8 79.0±10.782.5±6.089.2±13.1 87.2±2.3 68.4±4.1 86.1±1.783.8±5.090.2±7.588.0±2.7 70.2±3.3 88.1±2.687.9±2.190.2±6.0PCB283.7±0.6 64.4±3.8 80.9±0.573.6±1.579.3±2.085.5±1.0 72.9±3.4 82.9±1.876.2±0.979.3±1.987.0±0.5 71.9±2.6 83.7±1.078.0±1.376.3±1.6PCB384.3±1.0 69.0±1.2 78.1±2.079.5±2.584.0±1.486.1±0.6 74.0±2.3 82.2±1.182.3±1.884.7±1.187.7±0.6 77.2±0.8 84.4±1.984.2±1.085.0±1.3PCB466.9±2.0 59.1±1.8 77.9±3.176.6±4.178.8±2.369.3±1.1 62.6±3.6 79.5±4.881.7±1.278.3±2.674.7±1.0 67.9±2.6 83.5±2.584.2±0.783.4±2.4Pipe fryum94.3±0.5 83.9±0.8 93.6±0.596.1±0.695.2±0.495.0±0.2 86.9±0.9 94.5±0.496.2±0.694.8±0.595.0±0.3 88.7±1.3 95.0±0.596.6±0.295.3±0.3Mean84.1±1.6 64.3±2.4 80.5±2.585.1±2.185.1±2.585.7±1.1 70.1±2.6 82.6±2.386.2±1.485.8±2.187.3±0.8 72.6±1.9 84.9±1.487.6±0.986.2±1.7</p>
<p>Table 18 .
18
Comparison of pixel-level anomaly detection in terms of subset-wise PRO on VisA.</p>
<p>Acknowledgments This work is supported by the National Natural Science Foundation of China No. 62176092, 62222602, U23A20343, 62106075, 62302167, Shanghai Sailing Program (23YF1410500), Natural Science Foundation of Shanghai (23ZR1420400), Science and Technology Commission of Shanghai No.21511100700, Natural Science Foundation of Chongqing, China (CSTB2023NSCQ-JQX0007, CSTB2023NSCQ-MSX0137), CCF-Tencent Rhino-Bird Young Faculty Open Research Fund (RAGR20230121), China Postdoctoral Science Foundation funded project (2023M734270), and Development Project of Ministry of Industry and Information Technology (ZTZB.23-990-016).
Pni: industrial anomaly detection using position and neighborhood information. Jaehyeok Bae, Jae-Han Lee, Seyun Kim, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Songhao Piao, Furu Wei, Advances in Neural Information Processing Systems. 202235</p>
<p>Efficientad: Accurate visual anomaly detection at millisecond-level latencies. Kilian Batzner, Lars Heckler, Rebecca König, arXiv:2303.145352023arXiv preprint</p>
<p>Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Beyond dents and scratches: Logical constraints in unsupervised anomaly detection and localization. Paul Bergmann, Kilian Batzner, Michael Fauser, David Sattlegger, Carsten Steger, International Journal of Computer Vision. 13042022</p>
<p>Segment any anomaly without training via hybrid prompt regularization. Yunkang Cao, Xiaohao Xu, Chen Sun, Yuqi Cheng, Zongwei Du, Liang Gao, Weiming Shen, arXiv:2305.107242023arXiv preprint</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, International conference on machine learning. PMLR2020</p>
<p>Semantic prompt for few-shot image recognition. Wentao Chen, Chenyang Si, Zhang Zhang, Liang Wang, Zilei Wang, Tieniu Tan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>A zero-/fewshot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1&amp;2: 1st place on zero-shot ad and 4th place on few-shot ad. Xuhai Chen, Yue Han, Jiangning Zhang, arXiv:2305.173822023arXiv preprint</p>
<p>Sub-image anomaly detection with deep pyramid correspondences. Niv Cohen, Yedid Hoshen, arXiv:2005.023572020arXiv preprint</p>
<p>Padim: a patch distribution modeling framework for anomaly detection and localization. Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier, International Conference on Pattern Recognition. </p>
<p>. Springer, 2021</p>
<p>Anovl: Adapting vision-language models for unified zeroshot anomaly localization. Hanqiu Deng, Zhaoxiang Zhang, Jinan Bao, Xingyu Li, arXiv:2308.159392023arXiv preprint</p>
<p>Fastrecon: Few-shot industrial anomaly detection via fast feature reconstruction. Zheng Fang, Xiaoyang Wang, Haocheng Li, Jiejie Liu, Qiugui Hu, Jimin Xiao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Clip-adapter: Better vision-language models with feature adapters. Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, Yu Qiao, International Journal of Computer Vision. 2023</p>
<p>Remembering normality: Memoryguided knowledge distillation for unsupervised anomaly detection. Zhihao Gu, Liang Liu, Xu Chen, Ran Yi, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Annan Shu, Guannan Jiang, Lizhuang Ma, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Template-guided hierarchical feature restoration for anomaly detection. Hewei Guo, Liping Ren, Jingjing Fu, Yuwang Wang, Zhizheng Zhang, Cuiling Lan, Haoqian Wang, Xinwen Hou, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross B Girshick, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020. Seattle, WA, USAComputer Vision Foundation / IEEEJune 13-19, 2020. 2020</p>
<p>Registration based few-shot anomaly detection. Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael W Spratling, Yan-Feng Wang, Computer Vision -ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerOctober 23-27, 2022. 2022Proceedings, Part XXIV</p>
<p>. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, Ludwig Schmidt, 2021Openclip</p>
<p>Winclip: Zero-/few-shot anomaly classification and segmentation. Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, Onkar Dabeer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Deep learning-based defect detection of metal parts: evaluating current methods in complex conditions. Stepan Jezek, Martin Jonak, Radim Burget, Pavel Dvorak, Milos Skotak, 2021 13th International congress on ultra modern telecommunications and control systems and workshops (ICUMT). IEEE2021</p>
<p>Scaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, International conference on machine learning. PMLR2021</p>
<p>Jun Araki, and Graham Neubig. How can we know what language models know?. Zhengbao Jiang, Frank F Xu, 2020Transactions of the Association for Computational Linguistics8</p>
<p>Maple: Multi-modal prompt learning. Muhammad Uzair Khattak, Hanoona Abdul Rasheed, Muhammad Maaz, Salman H Khan, Fahad Shahbaz Khan, IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023. Vancouver, BC, CanadaIEEEJune 17-24, 2023. 2023</p>
<p>En-compactness: Self-distillation embedding &amp; contrastive generation for generalized zero-shot learning. Xia Kong, Zuodong Gao, Xiaofan Li, Ming Hong, Jun Liu, Chengjie Wang, Yuan Xie, Yanyun Qu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International Conference on Machine Learning. PMLR2022</p>
<p>Clip-reid: exploiting vision-language model for image re-identification without concrete text labels. Siyuan Li, Li Sun, Qingli Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023</p>
<p>Vs-boost: Boosting visual-semantic association for generalized zero-shot learning. Xiaofan Li, Yachao Zhang, Shiran Bian, Yanyun Qu, Yuan Xie, Zhongchao Shi, Jianping Fan, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023. the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023Macao, SAR, China19th-25th August 2023. 2023</p>
<p>Exploring visual interpretability for contrastive language-image pre-training. Yi Li, Hualiang Wang, Yiqun Duan, Hang Xu, Xiaomeng Li, arXiv:2209.070462022arXiv preprint</p>
<p>Clip surgery for better explainability with enhancement in openvocabulary tasks. Yi Li, Hualiang Wang, Yiqun Duan, Xiaomeng Li, arXiv:2304.056532023arXiv preprint</p>
<p>Deep dual consecutive network for human pose estimation. Zhenguang Liu, Haoming Chen, Runyang Feng, Shuang Wu, Shouling Ji, Bailin Yang, Xun Wang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Temporal feature alignment and mutual information maximization for video-based human pose estimation. Zhenguang Liu, Runyang Feng, Haoming Chen, Shuang Wu, Yixing Gao, Yunjun Gao, Xiang Wang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Simplenet: A simple network for image anomaly detection and localization. Zhikang Liu, Yiming Zhou, Yuansheng Xu, Zilei Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Interrealization channels: Unsupervised anomaly detection beyond one-class classification. Declan Mcintosh, Alexandra Branzan, Albu , Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>High-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Towards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Same same but differnet: Semi-supervised defect detection with normalizing flows. Marco Rudolph, Bastian Wandt, Bodo Rosenhahn, IEEE Winter Conference on Applications of Computer Vision, WACV 2021. Waikoloa, HI, USAIEEEJanuary 3-8, 2021. 2021</p>
<p>Deep one-class classification. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Ahmed Shoaib, Alexander Siddiqui, Emmanuel Binder, Marius Müller, Kloft, International conference on machine learning. PMLR2018</p>
<p>Multiresolution knowledge distillation for anomaly detection. Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, Hamid R Rabiee, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. 2021arXiv preprint</p>
<p>Laion-5b: An open large-scale dataset for training next generation image-text models. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Advances in Neural Information Processing Systems. 202235</p>
<p>A hierarchical transformation-discriminating generative model for few shot anomaly detection. Shelly Sheynin, Sagie Benaim, Lior Wolf, 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021. Montreal, QC, CanadaOctober 10-17, 2021</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, arXiv:2010.159802020arXiv preprint</p>
<p>Random word data augmentation with clip for zero-shot anomaly detection. Masato Tamura, arXiv:2308.111192023arXiv preprint</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>Student-teacher feature pyramid matching for anomaly detection. Guodong Wang, Shumin Han, Errui Ding, Di Huang, 32nd British Machine Vision Conference 2021, BMVC 2021, Online. BMVA PressNovember 22-25, 2021. 2021306</p>
<p>Few-shot learning with visual distribution calibration and cross-modal distribution alignment. Runqi Wang, Hao Zheng, Xiaoyue Duan, Jianzhuang Liu, Yuning Lu, Tian Wang, Songcen Xu, Baochang Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Cris: Clipdriven referring image segmentation. Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, Tongliang Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Perturbed progressive learning for semisupervised defect segmentation. Yao Wu, Mingwei Xing, Yachao Zhang, Yuan Xie, Zongze Wu, Yanyun Qu, IEEE Transactions on Neural Networks and Learning Systems. 2023</p>
<p>Patch svdd: Patch-level svdd for anomaly detection and segmentation. Jihun Yi, Sungroh Yoon, Proceedings of the Asian conference on computer vision. the Asian conference on computer vision2020</p>
<p>Draema discriminatively trained reconstruction embedding for surface anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Skočaj, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Unsupervised surface anomaly detection with diffusion probabilistic model. Xinyi Zhang, Naiqi Li, Jiawei Li, Tao Dai, Yong Jiang, Shu-Tao Xia, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Destseg: Segmentation guided denoising student-teacher for anomaly detection. Xuan Zhang, Shiyu Li, Xi Li, Ping Huang, Jiulong Shan, Ting Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Extract free dense labels from clip. Chong Zhou, Chen Change Loy, Bo Dai, European Conference on Computer Vision. Springer2022</p>
<p>Conditional prompt learning for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Learning to prompt for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, International Journal of Computer Vision. 13092022</p>
<p>Zegclip: Towards adapting clip for zero-shot semantic segmentation. Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, Yifan Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Spot-the-difference self-supervised pretraining for anomaly detection and segmentation. Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, Onkar Dabeer, European Conference on Computer Vision. </p>
<p>. Springer, 2022</p>
<p>Generic anomaly suffixes 'damaged {}', 'broken {}', '{} with flaw', '{} with defect. with damage</p>
<p>{} with cut'], 'hazelnut': ['{} with crack', '{} with cut', '{} with hole', '{} with print'], 'leather': ['{} with color stain', '{} with cut', '{} with fold', '{} with glue', '{} with poke'], 'cable': ['{} with bent wire', '{} with missing part', '{} with missing wire', '{} with cut', '{} with poke'], 'capsule': ['{} with crack', '{} with faulty imprint', '{} with poke', '{} with scratch', '{} squeezed with compression'], 'grid': ['{} with breakage', '{} with thread residue', '{} with thread', '{} with metal contamination', '{} with glue', '{} with a bent shape'], 'pill': ['{} with color stain', '{} with contamination', '{} with crack', '{} with faulty imprint', '{} with scratch', '{} wi th abnormal type'], 'transistor': ['{} with bent lead', '{} with cut lead', '{} with damage', '{} with misplaced transistor'], 'metal_nut': ['{} with a bent shape ', '{} with color stain', '{} with a flipped orientation', '{} with scratch'], 'screw': ['{} with manipulated front', '{} with scratch neck. Object-customized anomaly suffixes (MVTec) 'bottle': ['{} with large breakage. {} with fabric border. {} with glue strip. with gray stroke', '{} with oil', '{} with rough surface'], 'wood': ['{} with color stain', '{} with hole', '{} with scratch', '{} with liquid'</p>
<p>{} with different colour spot'], 'capsules': ['{} with scratch', '{} with discolor', '{} with misshape', '{} with leak', '{} with bubble'], 'cashew': ['{} with breakage', '{} with small scratches', '{} with burnt', '{} with stuck together', '{} with spot'], 'chewinggum': ['{} with corner missing', '{} with scratches', '{} with chunk of gum missing', '{} with colour spot', '{} with cracks'], 'fryum': ['{} with breakage', '{} with scratches', '{} with burnt', '{} with colour spot', '{} with fryum stuck together', '{ } with colour spot'], 'macaroni1': ['{} with color spot', '{} with small chip around edge', '{} with small scratches', '{} with breakage', '{} with cracks'], 'macaroni2': ['{} with color spot', '{} with small chip around edge', '{} with small scratches. {} with foreign particals. {} with missing. {} with burnt. with dirt'], 'pipe_fryum': ['{} with breakage', '{} with small scratches', '{} with burnt', '{} with stuck together', '{} with colour spot ', '{} with cracks'</p>            </div>
        </div>

    </div>
</body>
</html>
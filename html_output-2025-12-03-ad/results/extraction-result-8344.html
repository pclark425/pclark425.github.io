<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8344 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8344</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8344</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276928815</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.07323v2.pdf" target="_blank">Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> This paper advances motion agents empowered by large language models (LLMs) toward autonomous navigation in dynamic and cluttered environments, significantly surpassing first and recent seminal but limited studies on LLM's spatial reasoning, where movements are restricted in four directions in simple, static environments in the presence of only single agents much less multiple agents. Specifically, we investigate LLMs as spatial reasoners to overcome these limitations by uniformly encoding environments (e.g., real indoor floorplans), agents which can be dynamic obstacles and their paths as discrete tokens akin to language tokens. Our training-free framework supports multi-agent coordination, closed-loop replanning, and dynamic obstacle avoidance without retraining or fine-tuning. We show that LLMs can generalize across agents, tasks, and environments using only text-based interactions, opening new possibilities for semantically grounded, interactive navigation in both simulation and embodied systems.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8344.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8344.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Navigator (this paper) - o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o3-mini (evaluated in "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>o3-mini is evaluated in this paper as a high-performing reasoning LLM for zero-shot path navigation on realistic floorplans using textual (grid/code) encodings and multi-turn refinements; it achieved the strongest grid-based single-trial success in the authors' benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as a modern reasoning-capable LLM (table: type = 'reasoning'); used in a zero-shot setting with textual environment encodings (code/grid) and multi-turn interaction for replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Real floorplan path navigation (grid/code textual spatial encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based path planning with anchored continuous trajectories in realistic floorplans (2D/2.5D); requires spatial path planning and obstacle avoidance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot prompt-driven setup: floorplans converted to text as either grid (binary matrix) or code (list of obstacle coordinates), start and goal coordinates provided; LLM outputs trajectories as sequences of anchor points. Multi-turn closed-loop replanning is allowed via additive or compositional refinement when collisions/stuck states are detected. Evaluated single- and multi-agent scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Text-token spatial encoding (grid or code), anchor-point trajectory representation (sparse keypoints connected by straight lines), multi-turn LLM interaction with two update strategies (additive: recompute from origin; compositional: continue from current stuck point), trajectory validation against obstacle occupancy, A* used to generate ground-truth optimal paths for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table 1: grid input SR=0.781, SPL=0.710, CR=0.828, WSR=0.665; code input SR=0.507, SPL=0.488, CR=0.590, WSR=0.396 (all values are fractions between 0 and 1; single-agent, single trial).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High success and SPL on grid inputs, multi-turn improvements using additive/compositional refinements, and ability to coordinate in multi-agent settings indicate use of spatial reasoning; textual (code/grid) inputs outperform raw image input, supporting that the model uses structured spatial representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared in the paper's benchmark to other LLMs (Claude-3.5-Sonnet, Llama-3.3-70B, Gemini-2.0-flash, GPT-4o, DeepSeek-R1). o3-mini attains the highest reported grid-based SR in Table 1. Paper contrasts these results with prior simpler-grid studies (e.g., Wu et al. 2024b) where GPT-4 performed much worse (SR≈15%, CR≈40%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Grid-vs-code performance depends on model; compositional refinement can produce suboptimal local corrections (may get 'stuck'), additive approach can be inefficient; experiments are simulated (no real-world robot trials); approach assumes a globally encoded floorplan (full observability).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8344.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8344.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Navigator (this paper) - DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1 (evaluated in "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DeepSeek-R1, described as a reasoning-focused LLM, was evaluated zero-shot on realistic floorplan navigation tasks and achieved strong performance in the authors' benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Identified in the paper as a reasoning-capable model (table type = 'reasoning'); evaluated zero-shot on textual floorplan encodings (code/grid) for path planning and multi-agent navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Real floorplan path navigation (grid/code textual spatial encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based path planning with anchor-point trajectories in realistic floorplans; requires spatial reasoning and collision avoidance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot textual inputs (code/grid), outputs anchor-point trajectories; multi-turn additive and compositional refinements allowed; compared to A* ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Same tokenized spatial encoding and anchor-based trajectory formulation used for all evaluated LLMs; iterative multi-turn refinement for collision resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table 1: code input SR=0.673, SPL=0.645, CR=0.766, WSR=0.520; grid input SR=0.650, SPL=0.623, CR=0.729, WSR=0.503 (single-agent, single trial).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Substantially higher SR/SPL/CR than baseline and many other models when given structured textual encodings suggests effective spatial reasoning and path planning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Benchmarked alongside o3-mini, GPT-4o, Claude-3.5-Sonnet, Llama-3.3-70B, Gemini-2.0; outperforms many general-purpose LLMs but is behind o3-mini on grid input per table.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same systemic limitations as the paper: simulated-only validation, dependence on global floorplan encoding, potential for infeasible initial paths requiring refinements; no real-world robotic validation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8344.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8344.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Navigator (this paper) - GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (evaluated in "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o (multimodal) is included in the benchmark and evaluated on zero-shot navigation over realistic floorplans using grid/code/image inputs; it attains moderate performance relative to reasoning-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a multimodal modern LLM (table lists type='multimodal'); evaluated zero-shot with textual encodings and direct image inputs (start/goal marked) for trajectory generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Real floorplan path navigation (grid/code/image inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid/textual-coded navigation and anchored continuous path planning in realistic floorplans; includes 2D/2.5D height-map extension.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot; inputs: code or grid textual encodings, or direct image input with start/goal markers; outputs anchor-point trajectories; multi-turn replanning allowed.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Tokenized spatial encoding (grid/code), anchor points for trajectories, additive and compositional multi-turn refinements; image input uses direct image-to-text conversion in pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table 1: code input SR=0.420, SPL=0.381, CR=0.560, WSR=0.291; grid input SR=0.387, SPL=0.376, CR=0.477, WSR=0.227; image input SR=0.370, SPL=0.349, CR=0.477, WSR=0.222.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Better performance on structured textual encodings than raw image input, and ability to produce valid anchor-based paths and participate in multi-turn replanning demonstrates spatial reasoning capacity, though weaker than top reasoning-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Measured against reasoning-specialized models (o3-mini, DeepSeek-R1) and generalist models; performs moderately but is outperformed by reasoning models in this paper's benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Multimodal/image input performs worse than textual encodings; same simulation-only, full-observability assumptions; initial paths can be infeasible and require iterative corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8344.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8344.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Navigator (this paper) - Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.5-Sonnet (evaluated in "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude-3.5-Sonnet is evaluated in the benchmark as a general LLM and shows improved results when provided with code-based textual encodings versus grid or image inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Listed in the paper's table as a 'general' model; evaluated zero-shot on textual spatial encodings (code/grid) for anchor-point path generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Real floorplan path navigation (code/grid textual encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based path planning on realistic floorplans with anchor-point trajectories; requires spatial planning and obstacle avoidance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot; code vs grid textual encodings as inputs; output is an ordered list of anchor points defining a trajectory; multi-turn refinements allowed when collisions occur.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Tokenized spatial encoding, anchor-point trajectory output, multi-turn additive/compositional replanning strategies (as with other models in the benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table 1: code input SR=0.453, SPL=0.437, CR=0.556, WSR=0.307; grid input SR=0.330, SPL=0.302, CR=0.426, WSR=0.183.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performance gap between code and grid inputs suggests reliance on explicit coordinate encodings for better spatial inference; multi-turn refinements improved outcomes in broader experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to reasoning-focused models (o3-mini, DeepSeek-R1) and other general models; outperformed baseline and some models but below top reasoning models in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower performance on grid input and on image input relative to structured code; same systemic limitations (simulation-only, global floorplan assumption).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8344.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8344.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Navigator (this paper) - Llama-3.3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.3-70B (evaluated in "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama-3.3-70B, a large Llama family model (70B parameters), was evaluated zero-shot on floorplan navigation and shows modest performance improvements with code-based spatial encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Llama-3 family model; table labels it 'general'. Model size explicitly given as 70B in the paper's table. Evaluated zero-shot on code/grid inputs for path planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Real floorplan path navigation (code/grid textual encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based path planning on realistic floorplans using anchor-point trajectories; requires spatial reasoning for obstacle avoidance and path efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot textual inputs (code/grid), start/goal coordinates provided, output = anchor point trajectories; multi-turn refinements allowed.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Tokenized grid or code spatial representation and anchor-point output like other models; additive/compositional multi-turn replanning when collisions/stuck events occur.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table 1: code input SR=0.397, SPL=0.316, CR=0.565, WSR=0.240; grid input SR=0.350, SPL=0.296, CR=0.520, WSR=0.197.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows better outcomes with explicit code encodings; successful completion of anchor-based paths in many cases indicates some spatial planning ability, though lower than reasoning-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Benchmarked vs other LLMs in the paper; lower SR/SPL than DeepSeek-R1 and o3-mini, but better than baseline and some general models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relatively modest performance; same simulation and full-observability assumptions; requires iterative refinements for infeasible initial outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8344.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8344.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Navigator (this paper) - Gemini-2.0-flash</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-2.0-flash (evaluated in "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini-2.0-flash is evaluated as a general LLM and attains moderate navigation performance on code and grid textual encodings in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.0-flash</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Listed in the paper's results table as a 'general' model; assessed zero-shot with code and grid textual representations for path planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Real floorplan path navigation (code/grid textual encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based path planning with anchored trajectories in realistic floorplans; requires spatial reasoning and obstacle avoidance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot textual input (code/grid); outputs anchor-point trajectories; multi-turn corrective refinements allowed when collisions occur.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Uses tokenized environment encodings and anchor-based trajectory outputs; multi-turn additive/compositional update strategies for replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table 1: code input SR=0.380, SPL=0.297, CR=0.541, WSR=0.205; grid input SR=0.273, SPL=0.223, CR=0.328, WSR=0.139.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Improved performance on code vs grid indicates reliance on explicit coordinate information; multi-turn corrections helped in some scenarios per qualitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Benchmarked against other general and reasoning-focused models; did not match top performers (DeepSeek-R1, o3-mini) in the paper's table.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower grid performance; simulation-only evaluation and full-observability assumption; iterative refinement still needed for some infeasible outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8344.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8344.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior: Mind's Eye / Wu et al. 2024b (tiling puzzles / small grids)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mind's eye of llms: Visualization-of-thought elicits spatial reasoning in large language models (Wenshan Wu et al., NeurIPS 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that evaluates LLMs on QA-based tiling puzzles and small grid navigation tasks; authors note this earlier work focused on simpler static/symbolic tasks with restricted movement (four directions) and single-agent settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mind's eye of llms: Visualization-of-thought elicits spatial reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (reported in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In the cited prior study GPT-4 was used to evaluate navigation/tiling puzzle tasks in small grids; the current paper reports the prior study's numerical outcomes rather than running GPT-4 itself on those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>QA-based tiling puzzles / small grid navigation</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Small grid-based tiling/navigation problems with movement restricted to four cardinal directions; symbolic/static spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Prior work used a less formalized output format for small-grid tasks; movement restricted to adjacent-cell (4-direction) steps; evaluation required substring-matching due to unconstrained textual outputs (reported by current paper).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prior work explored visualization-of-thought techniques and QA framing for tiling/pattern recognition but was limited to static/symbolic settings rather than continuous anchored trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The current paper cites prior reported results: GPT-4 achieved ~15% Success Rate (SR) and ~40% Completion Rate (CR) on those simpler small-grid tasks (NeurIPS 2024 reported values as quoted).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Authors of current paper argue that prior poor performance (low SR/CR) likely stemmed from lack of standardized, structured task formulation and noisy outputs, rather than a total absence of spatial reasoning in the model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Current paper contrasts its structured output format and anchor-based representation with the prior study's setup; claims their formalized protocol yields higher, more robust performance across modern LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prior tasks were simplified (small grids, 4-direction movement) and outputs were not well-structured, leading to noisy evaluation; reported low SR/CR for GPT-4 indicates failures in that experimental setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8344.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8344.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior: Aghzal et al. (path-planning benchmarks on grids)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning (Mohamed Aghzal et al., 2023/2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior benchmark and investigation into LLM path-planning capabilities on square-grid synthetic environments; these studies evaluated LLMs on grid-world navigation with movement restricted to 4 directions and showed limitations in earlier setups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 and other LLMs (as reported in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior benchmark evaluated a variety of LLMs on synthetic grid path-planning tasks; current paper references these works as simpler, lower-dimensional baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Square-grid path planning / grid-world navigation</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Synthetic square-grid pathfinding tasks with discrete movements (4 directions); symbolic/grid reasoning required.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Prior works used synthetic small-grid environments and protocols tailored to discrete movement outputs (up/down/left/right); evaluated single-agent navigation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Approaches centered on textual descriptions of grids and discrete movement commands; did not adopt anchor-based continuous trajectories used in current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The current paper does not provide a full table of numbers for Aghzal et al., but references that prior tasks were simpler and earlier works reported low navigation SR in some cases (e.g., GPT-4 low SR reported in related prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Prior work demonstrated limited zero-shot navigation performance in small-grid setups; used as a baseline to motivate richer, more structured encodings in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Current paper positions its realistic floorplan, anchored-trajectory, and multi-agent setup as a strict superset of these prior grid-world evaluations and shows higher normalized performance under its standardized format.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prior grid-based setups limit movement to discrete directions and are less realistic; their evaluation protocols and output formats may have hampered model performance and evaluation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8344.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8344.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior: Martorell (grid-world exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>From text to space: Mapping abstract spatial models in llms during a grid-world navigation task (Nicolas Martorell, 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior work that explored LLM navigation in very small grids (e.g., 5x5) without obstacles; used to contextualize the present paper's expansion to human-scale floorplans and continuous anchor-point trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From text to space: Mapping abstract spatial models in llms during a grid-world navigation task</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs evaluated in prior grid-world setups (as reported by Martorell)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior evaluation of LLMs in constrained small-grid environments (e.g., single agent, 5x5 grid) focusing on discrete movement and mapping of abstract spatial models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>5x5 grid navigation (single-agent, no obstacles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Very small-grid discrete navigation; symbolic/spatial mapping in a constrained setting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Small grid, single-agent navigation without obstacles; discrete movement steps and textual prompts describing start/goal and grid configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Discrete-step movement reasoning in textual/grid representation; likely chain-of-thought or stepwise reasoning methods in prior work (paper cited but not detailed in current text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Current paper does not report specific numeric results from Martorell; referenced as an example of prior constrained evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Prior small-grid successes/limitations highlighted as motivation for the current paper's shift to realistic floors and anchored continuous trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Used by the authors to contrast limited prior setups (5x5, no obstacles) with their more realistic and challenging benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Extremely constrained setting (5x5, no obstacles) that does not generalize to real-world navigation complexity; limited evaluation scope per current paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mind's eye of llms: Visualization-of-thought elicits spatial reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning. <em>(Rating: 2)</em></li>
                <li>Look further ahead: Testing the limits of gpt-4 in path planning. <em>(Rating: 1)</em></li>
                <li>From text to space: Mapping abstract spatial models in llms during a grid-world navigation task <em>(Rating: 2)</em></li>
                <li>Do large language models have spatial cognitive abilities? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8344",
    "paper_id": "paper-276928815",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "LLM-Navigator (this paper) - o3-mini",
            "name_full": "o3-mini (evaluated in \"Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning\")",
            "brief_description": "o3-mini is evaluated in this paper as a high-performing reasoning LLM for zero-shot path navigation on realistic floorplans using textual (grid/code) encodings and multi-turn refinements; it achieved the strongest grid-based single-trial success in the authors' benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "o3-mini",
            "model_description": "Described in the paper as a modern reasoning-capable LLM (table: type = 'reasoning'); used in a zero-shot setting with textual environment encodings (code/grid) and multi-turn interaction for replanning.",
            "model_size": null,
            "puzzle_name": "Real floorplan path navigation (grid/code textual spatial encoding)",
            "puzzle_type": "Grid-based path planning with anchored continuous trajectories in realistic floorplans (2D/2.5D); requires spatial path planning and obstacle avoidance.",
            "task_setup": "Zero-shot prompt-driven setup: floorplans converted to text as either grid (binary matrix) or code (list of obstacle coordinates), start and goal coordinates provided; LLM outputs trajectories as sequences of anchor points. Multi-turn closed-loop replanning is allowed via additive or compositional refinement when collisions/stuck states are detected. Evaluated single- and multi-agent scenarios.",
            "mechanisms_or_strategies": "Text-token spatial encoding (grid or code), anchor-point trajectory representation (sparse keypoints connected by straight lines), multi-turn LLM interaction with two update strategies (additive: recompute from origin; compositional: continue from current stuck point), trajectory validation against obstacle occupancy, A* used to generate ground-truth optimal paths for evaluation.",
            "performance_metrics": "From Table 1: grid input SR=0.781, SPL=0.710, CR=0.828, WSR=0.665; code input SR=0.507, SPL=0.488, CR=0.590, WSR=0.396 (all values are fractions between 0 and 1; single-agent, single trial).",
            "evidence_of_spatial_reasoning": "High success and SPL on grid inputs, multi-turn improvements using additive/compositional refinements, and ability to coordinate in multi-agent settings indicate use of spatial reasoning; textual (code/grid) inputs outperform raw image input, supporting that the model uses structured spatial representations.",
            "comparisons": "Directly compared in the paper's benchmark to other LLMs (Claude-3.5-Sonnet, Llama-3.3-70B, Gemini-2.0-flash, GPT-4o, DeepSeek-R1). o3-mini attains the highest reported grid-based SR in Table 1. Paper contrasts these results with prior simpler-grid studies (e.g., Wu et al. 2024b) where GPT-4 performed much worse (SR≈15%, CR≈40%).",
            "limitations_or_failure_cases": "Grid-vs-code performance depends on model; compositional refinement can produce suboptimal local corrections (may get 'stuck'), additive approach can be inefficient; experiments are simulated (no real-world robot trials); approach assumes a globally encoded floorplan (full observability).",
            "uuid": "e8344.0",
            "source_info": {
                "paper_title": "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM-Navigator (this paper) - DeepSeek-R1",
            "name_full": "DeepSeek-R1 (evaluated in \"Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning\")",
            "brief_description": "DeepSeek-R1, described as a reasoning-focused LLM, was evaluated zero-shot on realistic floorplan navigation tasks and achieved strong performance in the authors' benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1",
            "model_description": "Identified in the paper as a reasoning-capable model (table type = 'reasoning'); evaluated zero-shot on textual floorplan encodings (code/grid) for path planning and multi-agent navigation.",
            "model_size": null,
            "puzzle_name": "Real floorplan path navigation (grid/code textual spatial encoding)",
            "puzzle_type": "Grid-based path planning with anchor-point trajectories in realistic floorplans; requires spatial reasoning and collision avoidance.",
            "task_setup": "Zero-shot textual inputs (code/grid), outputs anchor-point trajectories; multi-turn additive and compositional refinements allowed; compared to A* ground truth.",
            "mechanisms_or_strategies": "Same tokenized spatial encoding and anchor-based trajectory formulation used for all evaluated LLMs; iterative multi-turn refinement for collision resolution.",
            "performance_metrics": "From Table 1: code input SR=0.673, SPL=0.645, CR=0.766, WSR=0.520; grid input SR=0.650, SPL=0.623, CR=0.729, WSR=0.503 (single-agent, single trial).",
            "evidence_of_spatial_reasoning": "Substantially higher SR/SPL/CR than baseline and many other models when given structured textual encodings suggests effective spatial reasoning and path planning ability.",
            "comparisons": "Benchmarked alongside o3-mini, GPT-4o, Claude-3.5-Sonnet, Llama-3.3-70B, Gemini-2.0; outperforms many general-purpose LLMs but is behind o3-mini on grid input per table.",
            "limitations_or_failure_cases": "Same systemic limitations as the paper: simulated-only validation, dependence on global floorplan encoding, potential for infeasible initial paths requiring refinements; no real-world robotic validation reported.",
            "uuid": "e8344.1",
            "source_info": {
                "paper_title": "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM-Navigator (this paper) - GPT-4o",
            "name_full": "GPT-4o (evaluated in \"Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning\")",
            "brief_description": "GPT-4o (multimodal) is included in the benchmark and evaluated on zero-shot navigation over realistic floorplans using grid/code/image inputs; it attains moderate performance relative to reasoning-specialized models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Described as a multimodal modern LLM (table lists type='multimodal'); evaluated zero-shot with textual encodings and direct image inputs (start/goal marked) for trajectory generation.",
            "model_size": null,
            "puzzle_name": "Real floorplan path navigation (grid/code/image inputs)",
            "puzzle_type": "Grid/textual-coded navigation and anchored continuous path planning in realistic floorplans; includes 2D/2.5D height-map extension.",
            "task_setup": "Zero-shot; inputs: code or grid textual encodings, or direct image input with start/goal markers; outputs anchor-point trajectories; multi-turn replanning allowed.",
            "mechanisms_or_strategies": "Tokenized spatial encoding (grid/code), anchor points for trajectories, additive and compositional multi-turn refinements; image input uses direct image-to-text conversion in pipeline.",
            "performance_metrics": "From Table 1: code input SR=0.420, SPL=0.381, CR=0.560, WSR=0.291; grid input SR=0.387, SPL=0.376, CR=0.477, WSR=0.227; image input SR=0.370, SPL=0.349, CR=0.477, WSR=0.222.",
            "evidence_of_spatial_reasoning": "Better performance on structured textual encodings than raw image input, and ability to produce valid anchor-based paths and participate in multi-turn replanning demonstrates spatial reasoning capacity, though weaker than top reasoning-specialized models.",
            "comparisons": "Measured against reasoning-specialized models (o3-mini, DeepSeek-R1) and generalist models; performs moderately but is outperformed by reasoning models in this paper's benchmark.",
            "limitations_or_failure_cases": "Multimodal/image input performs worse than textual encodings; same simulation-only, full-observability assumptions; initial paths can be infeasible and require iterative corrections.",
            "uuid": "e8344.2",
            "source_info": {
                "paper_title": "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM-Navigator (this paper) - Claude-3.5-Sonnet",
            "name_full": "Claude-3.5-Sonnet (evaluated in \"Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning\")",
            "brief_description": "Claude-3.5-Sonnet is evaluated in the benchmark as a general LLM and shows improved results when provided with code-based textual encodings versus grid or image inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-3.5-Sonnet",
            "model_description": "Listed in the paper's table as a 'general' model; evaluated zero-shot on textual spatial encodings (code/grid) for anchor-point path generation.",
            "model_size": null,
            "puzzle_name": "Real floorplan path navigation (code/grid textual encodings)",
            "puzzle_type": "Grid-based path planning on realistic floorplans with anchor-point trajectories; requires spatial planning and obstacle avoidance.",
            "task_setup": "Zero-shot; code vs grid textual encodings as inputs; output is an ordered list of anchor points defining a trajectory; multi-turn refinements allowed when collisions occur.",
            "mechanisms_or_strategies": "Tokenized spatial encoding, anchor-point trajectory output, multi-turn additive/compositional replanning strategies (as with other models in the benchmark).",
            "performance_metrics": "From Table 1: code input SR=0.453, SPL=0.437, CR=0.556, WSR=0.307; grid input SR=0.330, SPL=0.302, CR=0.426, WSR=0.183.",
            "evidence_of_spatial_reasoning": "Performance gap between code and grid inputs suggests reliance on explicit coordinate encodings for better spatial inference; multi-turn refinements improved outcomes in broader experiments.",
            "comparisons": "Compared to reasoning-focused models (o3-mini, DeepSeek-R1) and other general models; outperformed baseline and some models but below top reasoning models in Table 1.",
            "limitations_or_failure_cases": "Lower performance on grid input and on image input relative to structured code; same systemic limitations (simulation-only, global floorplan assumption).",
            "uuid": "e8344.3",
            "source_info": {
                "paper_title": "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM-Navigator (this paper) - Llama-3.3-70B",
            "name_full": "Llama-3.3-70B (evaluated in \"Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning\")",
            "brief_description": "Llama-3.3-70B, a large Llama family model (70B parameters), was evaluated zero-shot on floorplan navigation and shows modest performance improvements with code-based spatial encodings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.3-70B",
            "model_description": "A Llama-3 family model; table labels it 'general'. Model size explicitly given as 70B in the paper's table. Evaluated zero-shot on code/grid inputs for path planning.",
            "model_size": "70B",
            "puzzle_name": "Real floorplan path navigation (code/grid textual encodings)",
            "puzzle_type": "Grid-based path planning on realistic floorplans using anchor-point trajectories; requires spatial reasoning for obstacle avoidance and path efficiency.",
            "task_setup": "Zero-shot textual inputs (code/grid), start/goal coordinates provided, output = anchor point trajectories; multi-turn refinements allowed.",
            "mechanisms_or_strategies": "Tokenized grid or code spatial representation and anchor-point output like other models; additive/compositional multi-turn replanning when collisions/stuck events occur.",
            "performance_metrics": "From Table 1: code input SR=0.397, SPL=0.316, CR=0.565, WSR=0.240; grid input SR=0.350, SPL=0.296, CR=0.520, WSR=0.197.",
            "evidence_of_spatial_reasoning": "Shows better outcomes with explicit code encodings; successful completion of anchor-based paths in many cases indicates some spatial planning ability, though lower than reasoning-specialized models.",
            "comparisons": "Benchmarked vs other LLMs in the paper; lower SR/SPL than DeepSeek-R1 and o3-mini, but better than baseline and some general models.",
            "limitations_or_failure_cases": "Relatively modest performance; same simulation and full-observability assumptions; requires iterative refinements for infeasible initial outputs.",
            "uuid": "e8344.4",
            "source_info": {
                "paper_title": "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM-Navigator (this paper) - Gemini-2.0-flash",
            "name_full": "Gemini-2.0-flash (evaluated in \"Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning\")",
            "brief_description": "Gemini-2.0-flash is evaluated as a general LLM and attains moderate navigation performance on code and grid textual encodings in the benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini-2.0-flash",
            "model_description": "Listed in the paper's results table as a 'general' model; assessed zero-shot with code and grid textual representations for path planning tasks.",
            "model_size": null,
            "puzzle_name": "Real floorplan path navigation (code/grid textual encodings)",
            "puzzle_type": "Grid-based path planning with anchored trajectories in realistic floorplans; requires spatial reasoning and obstacle avoidance.",
            "task_setup": "Zero-shot textual input (code/grid); outputs anchor-point trajectories; multi-turn corrective refinements allowed when collisions occur.",
            "mechanisms_or_strategies": "Uses tokenized environment encodings and anchor-based trajectory outputs; multi-turn additive/compositional update strategies for replanning.",
            "performance_metrics": "From Table 1: code input SR=0.380, SPL=0.297, CR=0.541, WSR=0.205; grid input SR=0.273, SPL=0.223, CR=0.328, WSR=0.139.",
            "evidence_of_spatial_reasoning": "Improved performance on code vs grid indicates reliance on explicit coordinate information; multi-turn corrections helped in some scenarios per qualitative results.",
            "comparisons": "Benchmarked against other general and reasoning-focused models; did not match top performers (DeepSeek-R1, o3-mini) in the paper's table.",
            "limitations_or_failure_cases": "Lower grid performance; simulation-only evaluation and full-observability assumption; iterative refinement still needed for some infeasible outputs.",
            "uuid": "e8344.5",
            "source_info": {
                "paper_title": "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Prior: Mind's Eye / Wu et al. 2024b (tiling puzzles / small grids)",
            "name_full": "Mind's eye of llms: Visualization-of-thought elicits spatial reasoning in large language models (Wenshan Wu et al., NeurIPS 2024)",
            "brief_description": "Referenced prior work that evaluates LLMs on QA-based tiling puzzles and small grid navigation tasks; authors note this earlier work focused on simpler static/symbolic tasks with restricted movement (four directions) and single-agent settings.",
            "citation_title": "Mind's eye of llms: Visualization-of-thought elicits spatial reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (reported in prior work)",
            "model_description": "In the cited prior study GPT-4 was used to evaluate navigation/tiling puzzle tasks in small grids; the current paper reports the prior study's numerical outcomes rather than running GPT-4 itself on those tasks.",
            "model_size": null,
            "puzzle_name": "QA-based tiling puzzles / small grid navigation",
            "puzzle_type": "Small grid-based tiling/navigation problems with movement restricted to four cardinal directions; symbolic/static spatial tasks.",
            "task_setup": "Prior work used a less formalized output format for small-grid tasks; movement restricted to adjacent-cell (4-direction) steps; evaluation required substring-matching due to unconstrained textual outputs (reported by current paper).",
            "mechanisms_or_strategies": "Prior work explored visualization-of-thought techniques and QA framing for tiling/pattern recognition but was limited to static/symbolic settings rather than continuous anchored trajectories.",
            "performance_metrics": "The current paper cites prior reported results: GPT-4 achieved ~15% Success Rate (SR) and ~40% Completion Rate (CR) on those simpler small-grid tasks (NeurIPS 2024 reported values as quoted).",
            "evidence_of_spatial_reasoning": "Authors of current paper argue that prior poor performance (low SR/CR) likely stemmed from lack of standardized, structured task formulation and noisy outputs, rather than a total absence of spatial reasoning in the model.",
            "comparisons": "Current paper contrasts its structured output format and anchor-based representation with the prior study's setup; claims their formalized protocol yields higher, more robust performance across modern LLMs.",
            "limitations_or_failure_cases": "Prior tasks were simplified (small grids, 4-direction movement) and outputs were not well-structured, leading to noisy evaluation; reported low SR/CR for GPT-4 indicates failures in that experimental setup.",
            "uuid": "e8344.6",
            "source_info": {
                "paper_title": "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Prior: Aghzal et al. (path-planning benchmarks on grids)",
            "name_full": "Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning (Mohamed Aghzal et al., 2023/2024)",
            "brief_description": "Cited prior benchmark and investigation into LLM path-planning capabilities on square-grid synthetic environments; these studies evaluated LLMs on grid-world navigation with movement restricted to 4 directions and showed limitations in earlier setups.",
            "citation_title": "Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning.",
            "mention_or_use": "mention",
            "model_name": "GPT-4 and other LLMs (as reported in prior work)",
            "model_description": "Prior benchmark evaluated a variety of LLMs on synthetic grid path-planning tasks; current paper references these works as simpler, lower-dimensional baselines.",
            "model_size": null,
            "puzzle_name": "Square-grid path planning / grid-world navigation",
            "puzzle_type": "Synthetic square-grid pathfinding tasks with discrete movements (4 directions); symbolic/grid reasoning required.",
            "task_setup": "Prior works used synthetic small-grid environments and protocols tailored to discrete movement outputs (up/down/left/right); evaluated single-agent navigation performance.",
            "mechanisms_or_strategies": "Approaches centered on textual descriptions of grids and discrete movement commands; did not adopt anchor-based continuous trajectories used in current paper.",
            "performance_metrics": "The current paper does not provide a full table of numbers for Aghzal et al., but references that prior tasks were simpler and earlier works reported low navigation SR in some cases (e.g., GPT-4 low SR reported in related prior work).",
            "evidence_of_spatial_reasoning": "Prior work demonstrated limited zero-shot navigation performance in small-grid setups; used as a baseline to motivate richer, more structured encodings in the present paper.",
            "comparisons": "Current paper positions its realistic floorplan, anchored-trajectory, and multi-agent setup as a strict superset of these prior grid-world evaluations and shows higher normalized performance under its standardized format.",
            "limitations_or_failure_cases": "Prior grid-based setups limit movement to discrete directions and are less realistic; their evaluation protocols and output formats may have hampered model performance and evaluation fidelity.",
            "uuid": "e8344.7",
            "source_info": {
                "paper_title": "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Prior: Martorell (grid-world exploration)",
            "name_full": "From text to space: Mapping abstract spatial models in llms during a grid-world navigation task (Nicolas Martorell, 2025)",
            "brief_description": "Cited prior work that explored LLM navigation in very small grids (e.g., 5x5) without obstacles; used to contextualize the present paper's expansion to human-scale floorplans and continuous anchor-point trajectories.",
            "citation_title": "From text to space: Mapping abstract spatial models in llms during a grid-world navigation task",
            "mention_or_use": "mention",
            "model_name": "LLMs evaluated in prior grid-world setups (as reported by Martorell)",
            "model_description": "Prior evaluation of LLMs in constrained small-grid environments (e.g., single agent, 5x5 grid) focusing on discrete movement and mapping of abstract spatial models.",
            "model_size": null,
            "puzzle_name": "5x5 grid navigation (single-agent, no obstacles)",
            "puzzle_type": "Very small-grid discrete navigation; symbolic/spatial mapping in a constrained setting.",
            "task_setup": "Small grid, single-agent navigation without obstacles; discrete movement steps and textual prompts describing start/goal and grid configuration.",
            "mechanisms_or_strategies": "Discrete-step movement reasoning in textual/grid representation; likely chain-of-thought or stepwise reasoning methods in prior work (paper cited but not detailed in current text).",
            "performance_metrics": "Current paper does not report specific numeric results from Martorell; referenced as an example of prior constrained evaluations.",
            "evidence_of_spatial_reasoning": "Prior small-grid successes/limitations highlighted as motivation for the current paper's shift to realistic floors and anchored continuous trajectories.",
            "comparisons": "Used by the authors to contrast limited prior setups (5x5, no obstacles) with their more realistic and challenging benchmarks.",
            "limitations_or_failure_cases": "Extremely constrained setting (5x5, no obstacles) that does not generalize to real-world navigation complexity; limited evaluation scope per current paper's discussion.",
            "uuid": "e8344.8",
            "source_info": {
                "paper_title": "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mind's eye of llms: Visualization-of-thought elicits spatial reasoning in large language models",
            "rating": 2,
            "sanitized_title": "minds_eye_of_llms_visualizationofthought_elicits_spatial_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning.",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_good_path_planners_a_benchmark_and_investigation_on_spatialtemporal_reasoning"
        },
        {
            "paper_title": "Look further ahead: Testing the limits of gpt-4 in path planning.",
            "rating": 1,
            "sanitized_title": "look_further_ahead_testing_the_limits_of_gpt4_in_path_planning"
        },
        {
            "paper_title": "From text to space: Mapping abstract spatial models in llms during a grid-world navigation task",
            "rating": 2,
            "sanitized_title": "from_text_to_space_mapping_abstract_spatial_models_in_llms_during_a_gridworld_navigation_task"
        },
        {
            "paper_title": "Do large language models have spatial cognitive abilities?",
            "rating": 1,
            "sanitized_title": "do_large_language_models_have_spatial_cognitive_abilities"
        }
    ],
    "cost": 0.019121,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning
5 Jun 2025</p>
<p>Yubo Zhao 
The Hong Kong University of Science and Technology</p>
<p>Qi Wu 
The Hong Kong University of Science and Technology</p>
<p>Yifan Wang 
The Hong Kong University of Science and Technology</p>
<p>Yu-Wing Tai yu-wing.tai@dartmouth.edu 
Dartmouth College</p>
<p>Chi-Keung Tang cktang@cse.ust.hk 
The Hong Kong University of Science and Technology</p>
<p>Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning
5 Jun 2025C64A4D96F282C55DDBDFB2686CE99564arXiv:2503.07323v2[cs.AI]LLM Navigator Motion Generation Agent
This paper advances motion agents empowered by large language models (LLMs) toward autonomous navigation in dynamic and cluttered environments, significantly surpassing first and recent seminal but limited studies on LLM's spatial reasoning, where movements are restricted in four directions in simple, static environments in the presence of only single agents much less multiple agents.Specifically, we investigate LLMs as spatial reasoners to overcome these limitations by uniformly encoding environments (e.g., real indoor floorplans), agents which can be dynamic obstacles and their paths as discrete tokens akin to language tokens.Our trainingfree framework supports multi-agent coordination, closed-loop replanning, and dynamic obstacle avoidance without retraining or fine-tuning.We show that LLMs can generalize across agents, tasks, and environments using only textbased interactions, opening new possibilities for semantically grounded, interactive navigation in both simulation and embodied systems.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) Achiam et al. (2023); Team et al. (2024); Guo et al. (2025); Touvron et al. (2023) have made significant strides in reasoning and planning across various domains.These models exhibit powerful generalization capabilities, enabling them to handle unseen scenarios with remarkable flexibility and comprehension of world knowledge Sun et al. (2023); Wu et al. (2024c,a,b); Singh et al. (2023); Hendrycks et al. (2021); Sun et al. (2024).</p>
<p>This paper investigates the spatial reasoning ability inherent in pre-trained LLMs in zero-shot path navigation under dynamic environments.To make navigation tasks amenable to LLMs, we uniformly represent the environment (e.g., floor maps), agents, and paths as tokens that interact within a shared space.This token-based representation enables efficient spatial reasoning, providing robust and flexible solutions for pathfinding in complex environments.Specifically, we represent paths as sequences of sparse anchor points, allowing agents to move flexibly to any position within the space, rather than relying on predefined movement sets (e.g., 4 or 8 adjacent positions), as seen in previous works Wu et al. (2024b); Yang et al. (2024); Aghzal et al. (2024Aghzal et al. ( , 2023)); Martorell (2025).This approach aligns more closely with human intuition for maneuvering and obstacle avoidance, while also reducing the processing burden on the LLM.Thus, this work represents a departure from the conventional approach in path navigation using reinforcement learning (RL) which, while performing well in controlled settings, requires extensive data collection and training and often struggles to generalize to novel scenarios-challenges where LLMs demonstrate promising zero-shot adaptability.Despite that, we believe LLM and RL are not at odds with each other; rather, their synergy provides complementary advantages.With the rise of LLMs, we believe the dataset and insight we develop in this paper will be very useful in extending cognitive and language reasoning into embodied spatial domains, offering a flexible and scalable alternative in settings where previous approaches face practical constraints.</p>
<p>We evaluate our LLM-powered, training-free system through experiments on real floorplans on various modern LLMs in both single-agent and multi-agent settings.Our results, e.g., Figure 1, show that this approach generalizes effectively to previously unseen environments and tasks.Additionally, we showcase the system's ability to solve dynamic problems in closed-loop environments, where agent communication and coordination are essential to ensure robust navigation and motion.Agents can autonomously adjust their plans and avoid collisions in real-time, highlighting the system's ability to handle complex, unpredictable scenarios.Finally, we showcase a proof-of-concept extension to humanoid motion generation, demonstrating how the system can generate realistic and contextually appropriate movements for agents in a wide range of dynamic environments.We believe this work will inspire real-world applications in areas such as autonomous robotics, virtual reality, and interactive human-robot interaction, where agents must navigate and collaborate in complex, dynamic spaces.</p>
<p>Related Work</p>
<p>Spatial Understanding and Reasoning</p>
<p>Effective navigation and planning in space require a fundamental understanding of the environment, a critical cognitive ability for both humans and intelligent systems.With the advancement of LLMs, spatial reasoning has become an emerging research focus.Studies such as Wu and Guo;Mirzaee et al. (2021); Mirzaee and Kordjamshidi (2022); Momennejad et al. (2023) investigate LLMs' spatial reasoning capabilities through verbal reasoning tasks, such as question answering (QA).Other works like Mirchandani et al. (2023) explore LLMs' ability to recognize patterns, while Hong et al. (2023) focuses on enhancing LLMs' 3D reasoning capabilities.Additionally, Wu et al. (2024b) evaluates LLMs' performance in solving QA-based tiling puzzles.However, these studies largely focus on static or symbolic reasoning, and do not consider grounded, embodied spatial tasks that require interaction with dynamic environments.</p>
<p>We believe modern LLMs' powerful language reasoning can be extended into spatial reasoning with proper token representation.In this paper, we validate this idea in extensive quantitative and qualitative evaluations, demonstrating their effectiveness in understanding spatial environments and generating feasible solutions.Moreover, we showcase their downstream applications, such as humanoid motion, extending beyond theoretical or virtual scenarios to more practical settings.</p>
<p>Using LLMs for Navigation</p>
<p>Recent LLM studies on path navigation apply to restricted scenarios.For example, Wu et al. (2024b) proposes a protocol to evaluate LLMs' ability to navigate in simple environments with a single possible route; Aghzal et al. (2024Aghzal et al. ( , 2023) ) investigate LLMs' pathfinding capabilities in square grids with simplified, synthetic environments; Martorell (2025) explores using LLMs to navigate a single agent in a 5x5 grid without obstacles.All these studies focus on navigation within unreal environments with movements restricted to four directions (up, down, left, right), limiting their real-world applicability.Other works Zhou et al. (2023); Yang et al. (2024); Liu et al. (2025); Yuan et al. (2024); Yu et al. (2023) employ language models for individual navigation tasks with camera inputs.However, these efforts are often limited to perception-conditioned action selection, without broader path planning, spatial negotiation, or multi-agent interaction.</p>
<p>In contrast, our approach enables global path planning over realistic environments with multiple agents, supporting continuous movement, interaction, and closed-loop adaptation.Unlike prior works constrained by low-dimensional grids or simple mazes, we demonstrate that LLMs can perform robust spatial reasoning in human-scale spaces, coordinating complex navigation tasks with no fine-tuning or retraining.</p>
<p>Environment-Aware Agents</p>
<p>Path planning, especially for multiple interacting agents, is an active research area in robotics and AI, commonly studied as Multi-Agent Path Finding (MAPF) Stern et al. (2019); Li et al. (2021); Shaoul et al. (2024).Classical planning methods, such as the A* algorithm Hassan et al. (2021); Zhao et al. (2023), effectively solve single-agent navigation tasks in static environments but often face challenges in dynamic or multi-agent scenarios due to agent interactions and collision avoidance requirements.Recently, learning-based approaches-including diffusion models Yi et al. (2024); Rempe et al. (2023); Liang et al. (2024) and reinforcement learning Chao et al. (2021); Peng et al. (2022); Hassan et al. (2023)-have also been explored for complex navigation tasks.However, these typically require task-specific training and extensive data collection, posing limitations on their zero-shot generalizability and deployment flexibility.In contrast, our work explores a trainingfree approach, leveraging the zero-shot spatial reasoning and interactive planning capabilities of LLMs.Our method is complementary to existing MAPF and learning-based frameworks, focusing explicitly on LLMs' inherent capabilities for dynamic multi-agent navigation tasks through intuitive, human-like interaction and iterative refinement.</p>
<p>Dynamic Path Planning with LLMs</p>
<p>To make LLMs comprehend this task, we uniformly encode agents with their anchored trajectories and the environment as discrete text tokens interacting with each other as follows:</p>
<p>Agents with Anchored Trajectories</p>
<p>Echoing how humans navigate through their environment, we are not restricted to only a few discrete directions.Instead, human paths resemble a sequence of anchor points connected by almost straight lines to avoid obstacles while minimizing the total travel distance.For instance, when someone moves from one room to another, they may first walk directly toward the door, then proceed straight to the door of the next room, and finally enter the target room.This anchor-based approach is intuitive for humans and can be easily adapted for LLM planning.In this framework, a path or trajectory is not necessarily dense; it only requires a sequence of key points, akin to decomposing a complex task into simpler sub-goals, which is well-suited for modern LLMs Sun et al. ( 2023 Formally, let X denote the movable space of the agent, where each point x ∈ X represents a possible state of the agent.A trajectory T is defined as a sequence of points:
T = {x 1 , x 2 , . . . , x k },
where each point x i ∈ X represents a position in the environment, and k is the number of anchor points in the path.The trajectory is generated by connecting these points with straight lines, and the travel distance can be formulated as:
D(T ) = k−1 i=1 ∥x i − x i+1 ∥ 2 ,
where ∥ • ∥ 2 denotes the Euclidean distance between two consecutive points.The task of path planning is to determine an optimal sequence of anchor points that connect the initial and goal states, while satisfying any environmental constraints.</p>
<p>A critical aspect of path planning is ensuring that the trajectory avoids obstacles, static or dynamic.Specifically, a sub-path between two consecutive anchor points x i and x i+1 is valid if and only if there are no obstacles along the line segment connecting them.Formally, for each consecutive pair of points (x i , x i+1 ), the sub-path is valid if:
∀t ∈ [0, 1], such that γ(t) = (1 − t)x i + tx i+1 , there exists no obstacle such that γ(t) ∈ O,
where γ(t) represents the linear interpolation between x i and x i+1 , and O denotes the set of obstacles in the environment.If this condition is satisfied for all consecutive pairs (x i , x i+1 ), the trajectory T is considered valid.</p>
<p>Thus, the path planning problem can be viewed as finding an optimal sequence of anchor points {x 1 , x 2 , . . ., x k } that minimize the total travel distance D(T ), while ensuring the trajectory avoids obstacles and adheres to other environmental constraints.</p>
<p>In multi-agent scenarios, the complexity of path planning increases, as the trajectories of different agents may intersect.While some intersections may be unavoidable or difficult to resolve, it is important to note that different agents occupy the same space at different times.Therefore, the actual trajectory of each agent can only be fully determined during testing, when the agents' interactions and timing can be accounted for in the dynamic environment.</p>
<p>Spatial Environment Representation</p>
<p>Among various alternatives, the most commonly used and intuitive representation of space is in the form of grids Yang et al. (2024); Wu et al. (2024b); Aghzal et al. (2024).In this representation, the environment is discretized into a grid structure where each cell corresponds to a specific location, allowing for clear and precise definitions of free spaces, obstacles, and agent positions.</p>
<p>Alternatively, space can be represented using code-based descriptions Aghzal et al. (2024), which can be more interpretable for LLMs.This approach is both compact and flexible, enabling precise definitions of the environment through code.For instance, variables can be defined to specify the start and goal locations, while logic can be applied to place obstacles on the grid, shaping the environment accordingly.Intuitively, code provides a clear and concise way to define the task setting, making it a powerful alternative to traditional grid-based representations.</p>
<p>By using text-based representations, we bridge the gap between spatial reasoning and natural language processing, enabling LLMs to leverage their reasoning capabilities in a domain where they have proven effectiveness.This text-based approach sets ourselves apart from images as input, which may introduce unnecessary or redundant information such as textures, colors, or irrelevant details, while enabling the language capabilities of LLMs, which excel at processing and reasoning with text.</p>
<p>To formalize, we define a grid-based environment representation:
G = {g i,j | g i,j ∈ {0, 1}}
where g i,j = 1 indicates an obstacle, g i,j = 0 denotes free space, and g i,j represents the cell at row i and column j in a 2D grid.</p>
<p>We also define a code representation as a list of obstacle coordinates:
C = obstacles.append((i 1 , j 1 ), . . . , (i n , j n ))
where each (i k , j k ) denotes the location of an obstacle.</p>
<p>System Architecture</p>
<p>Overview</p>
<p>The architecture of the LLM-centered system is shown in Figure 2. Let the floor map be firstly encoded into a grid-based G or code-based C format.This encoded floorplan constructs an environment E, and is passed to the LLM L.</p>
<p>Given N agents, each agent i has a starting point s i and a target point t i .The LLM generates the trajectories T i for all agents based on the starting points S = {s i } N i=1 and target points T = {t i } N i=1 :
T = L(E, S, T ) = {x (i) 1 , x (i) 2 , . . . , x (i) ki } N i=1
The agents are simulated in the environment E. If a collision occurs at time t, the agent queries the LLM using its current position p i (t) and the set of detected collisions C t , requesting a refined path T ′ i .The final output consists of the collision-free trajectories T ′ i for all agents, ensuring that each agent reaches its target point without collisions.</p>
<p>Path Refining Strategies</p>
<p>To handle collisions with static obstacles (floorplan) or dynamic obstacles (motion agents), we capitalize LLMs' multi-turn capability, allowing for iterative refinement outputs until achieving the desired result.We propose two transformative update strategies to refine the path: (1) additive and (2) compositional, drawing inspiration from image alignment warping source to target Baker and Matthews (2004).The additive approach recalculates the entire motion plan holistically, integrating all prior adjustments into a unified transformation-similar to continuously recalculating an optimal golf swing based on previous strokes.While straightforward, this method can be inefficient, as each update effectively resets to the origin.In contrast, the compositional approach refines the trajectory incrementally, making step-by-step corrections based on the current state.Though generally more efficient, it may suffer if an update places the trajectory in an unfavorable position (e.g., low terrain where the ball becomes stuck), potentially hindering future corrections.</p>
<p>In our setting, let s denote the starting point and t the destination.Given n update opportunities, if an agent becomes stuck during the i-th trial for a reason r (which includes its current stuck position), the additive strategy updates the trajectory as
T i+1 = L(s, t, r),
implying that each update is computed globally, restarting from the original starting point s.</p>
<p>In contrast, the compositional strategy refines the trajectory based on the current state.Let p i denote the current stopping position at iteration i when the path is unsuccessful.Then, the updated trajectory is computed as
T i+1 = L(p i , t, r),
indicating that the current stuck position serves as the new starting point for planning the remainder of the path.Each corrective adjustment is applied "on-the-fly" to the current trajectory, enabling dynamic, incremental updates.</p>
<p>When multiple agents become stuck simultaneously, the LLM can coordinate across them using either strategy to refine their paths.</p>
<p>Experiments</p>
<p>4.1 Experiment Setup</p>
<p>Dataset</p>
<p>To ensure the applicability of our problem to real-world scenarios while maintaining simplicity, we build on the R2V dataset Liu et al. (2017), which contains 815 realistic floorplans from actual buildings.The floor plans are primarily rectilinear in shape, making them easier for LLMs to process and understand.</p>
<p>For each floor plan, we first convert it into textual formats and randomly sample three pairs of starting and target points.We then use the A * algorithm to generate obstacle-free optimal paths as ground truth labels, creating a dataset suitable for evaluating the spatial navigation ability of LLMs.In addition to this dataset, we have also manually created more complex scenarios to explore the upper bound of LLMs' navigation and planning capabilities, inspiring broader exploration in this domain.</p>
<p>Evaluation Metrics</p>
<p>To evaluate the performance of LLMs, we use several standard metrics: Success Rate (SR), Success weighted by Path Length (SPL), Completion Rate (CR), and Weighted Success Rate (WSR).</p>
<p>Success Rate (SR):</p>
<p>The share of evaluation episodes in which the agent actually reaches the goal.</p>
<p>Success weighted by Path Length (SPL):</p>
<p>Starts with the success rate, but then rewards agents that take near-optimal routes and penalises those that wander: a successful run counts most when its path is as short as the best possible one.</p>
<p>Completion Rate (CR): Looks only at how much of the planned route the agent covers.Even if it never gets to the target, it earns partial credit for the fraction of the path it did traverse.</p>
<p>Weighted Success Rate (WSR): Counts successes, but gives more weight to episodes whose optimal paths are longer (and therefore usually harder).Finishing a difficult, long-distance task contributes more than finishing an easy, short one.</p>
<p>These metrics, described in detail in the supplementary, are commonly used in navigation and path planning tasks to assess both the efficiency and effectiveness of the trajectory generation.They offer a comprehensive assessment of LLMs' navigation performance by considering success rates, trajectory efficiency, task completion, and complexity.Using these metrics in conjunction with our constructed dataset, we propose a protocol to benchmark the spatial navigation capabilities of LLMs.</p>
<p>Quantitative Results</p>
<p>We evaluate a range of LLMs, including GPT-4o Hurst et al. ( 2024 2024), OpenAI o3-mini, and Claude-Sonnet.Our selection comprises both state-of-the-art reasoning models and prior general-purpose models.We conduct extensive experiments, including benchmarking and ablation studies.All of our experiments are in a zero-shot setting.</p>
<p>Single-Agent</p>
<p>First, we assess how LLMs perform in a single attempt.As a baseline, we use scores obtained by moving directly from the starting point to the destination without any navigation.For input, we examine both grid and code representations, as described above.Additionally, we explore direct image input without spatial encoding, where the start and end points are marked on the image.The results, presented in Table 1, indicate that modern models with advanced reasoning capabilities demonstrate significantly stronger performance in spatial navigation tasks.Furthermore, textual input outperforms image-based input, highlighting the effectiveness of textual representations, which align more naturally with LLMs' ability to process structured, discrete text-based information.When comparing the two textual formats, their effectiveness varies across models.For most models, codebased input yields better performance, as it explicitly encodes coordinates.However, for o3-mini, which exhibits strong reasoning capabilities, the grid-based format proves more effective.We attribute this to its ability to intuitively recognize spatial patterns, akin to human interpretation.</p>
<p>We also investigate how the LLM-based system benefits from multi-turn interactions.We evaluate and compare the additive and compositional approaches, with results shown in Figure 3.As the number of turns increases, both methods contribute to overall performance improvement.The additive approach generally achieves a higher success rate (both SR and WSR), as it recalculates the complete path from the origin at each step.In contrast, the compositional approach, while more susceptible to suboptimal adjustments-analogous to a poor stroke in golf affecting subsequent corrections-exhibits higher SPL and CR, as it refines the current trajectory without resetting, preserving progress and ensuring continuous improvement.</p>
<p>Multi-Agents</p>
<p>We extend our experiments to scenarios with two and three agents, with the results shown in Figure 4.</p>
<p>In these experiments, the maximum number of retries is set to three.This setup introduces additional complexity, as each agent must navigate to its destination while avoiding obstacles and dynamically resolve collisions with other agents, making the system more interactive and adaptive.</p>
<p>Our results indicate that as the number of agents increases, the performance scores decrease, but they remain within a reasonable and practical range.This demonstrates the feasibility of multi-agent coordination and LLMs' ability to manage complex, dynamic environments.Additionally, different update strategies show minimal impact on overall performance.The compositional approach tends to yield slightly lower scores, which aligns with real-world observations-for instance, when two people walk toward each other and both instinctively step in the same direction to avoid a collision, they may inadvertently create an awkward situation.This suggests that the additive approach may be more effective in globally resolving such coordination conflicts.</p>
<p>Overall, modern LLMs, such as o3-mini, demonstrate substantial capabilities in spatial navigation tasks, achieving approximately 80% SR in a single trial and up to about 90% SR with multiple trials for single-agent scenarios.The system can also be seamlessly extended to multi-agent scenarios without much performance degradation.Moreover, all experiments are conducted in a zero-shot setting, further validating LLMs' reasoning abilities and world knowledge.Taken together, these results indicate that modern LLMs are becoming increasingly applicable to real-world agent scenarios.</p>
<p>Qualitative Results</p>
<p>In this section, we visualize our results and introduce a practical application involving generating environment-aware humanoid motion (Figure 6a).Previous works, such as OmniControl Xie et al.A person runs out of the room that is on fire from (12,1) to (11,7).</p>
<p>Agent1 walks sideways from (1,1) to (18,8); agent2 walks like a zombie from (2,18) to (9,19); agent3 walks from (15,2) to (13,20).</p>
<p>Agent1 walks from (2,3) to (17,8); agent2 walks from (4,8) to (3,17); agent3 walks from (17,8) to (4,1); agent4 walks from (16,15) to (6,2); agent5 walks from (5,20) to (1,7).Agents successfully navigate to their intended destinations while avoiding obstacles and other agents.</p>
<p>agent, enabling natural user-agent interaction.By integrating these approaches, we demonstrate that our LLM-based navigation system can be seamlessly applied to humanoid motion as a downstream task.Furthermore, our approach can be readily extended to scenarios involving multiple agents coexisting within an environment.This application method is illustrated in Figure 6a.Once the system generates an obstacle-free trajectory, it can be used to guide motion generation modelswhich are inherently non-environment-aware -to follow the trajectory and thus avoid collisions.(1,4,0) to (5,4,1).(b) The system generates a 3D path and guides the blue agent to move within the 3D space while avoiding the red agent intruding during execution, by adjusting the path.</p>
<p>Motion Generation Agent</p>
<p>Figure 6 We provide both the top-down view and 3D humanoid motions in Figure 5, demonstrating that our LLM-based system can navigate effectively for both single and multiple agents.While the initially generated path may sometimes be infeasible, the system can autonomously adjust itself multiple times to resolve such issues.Notably, in the example on the right, where five agents encounter different challenges, the system successfully coordinates their paths through multiple adjustments, enabling them to avoid obstacles and one another, ultimately reaching their respective destinations.</p>
<p>Furthermore, we extend the input floor map to 3D (or more precisely, 2.5D), where instead of using 1 and 0 to represent obstacles and free space, each point is assigned a height value, forming a height map.Consequently, the output path is represented using anchor points in three dimensions.As shown in Figure 6b, this extension enables our system to generate agent paths that are not restricted to a flat plane.Additionally, our system has the potential to be integrated with methods like SCENIC Zhang et al. (2024), which can interact with the scene but rely on manually defined input trajectories to avoid obstacles.</p>
<p>Discussion</p>
<p>Limitations and Future Work.Our first work on LLM-navigation focuses on dynamic systems, including multi-agent scenarios, and has been validated using a simulated environment.Although these simulations offer valuable insights into system effectiveness, the lack of real-world testing-particularly with physical robots and in household scenarios-necessitates further validation in real-world settings.Additionally, our current approach relies on a globally encoded floorplan that assumes full observability during path planning.Future work will explore replacing the global floorplan embedding with a local embedding strategy that focuses on the immediate surroundings observable by each agent at the current timestamp.Nevertheless, our framework is inherently generalizable, capable of seamlessly incorporating additional functionalities such as collision handling and agent-agent interactions, which can be managed as local operations.</p>
<p>Concluding Remarks.In this work, we examined the reasoning capabilities of large language models (LLMs) in spatial navigation and collision-free trajectory generation for motion agents in dynamic environments.We represent floor maps as discrete text and structure navigation paths using a sparse anchored representation.As one of the first studies on LLMs' spatial reasoning, we constructed a comprehensive dataset and proposed evaluation protocols to assess their performance.Furthermore, we extended our investigation to scenarios involving multiple coexisting agents.Our results demonstrate that LLMs can effectively coordinate across agents and autonomously resolve collisions in closed-loop, dynamic settings.We showcased the real-world applicability of our approach by applying it to the task of humanoid motion.Given an input floor plan image I ∈ R H×W ×3 , we first convert it to a grayscale image:
I g = Grayscale(I), I g ∈ R H×W .
Next, we remove all entirely black rows and columns to extract the significant region I ′ ⊆ I g :
I ′ = I g [rows(I g ) ̸ = 0, cols(I g ) ̸ = 0].
We then pad I ′ appropriately to standardize its dimensions and apply resizing to reduce the spatial resolution, yielding I r .To enhance structural coherence, Gaussian smoothing is performed: The resulting binary matrix I b represents navigable spaces and obstacles explicitly, serving as input for subsequent spatial reasoning tasks.
I s = GaussianBlur(I r ).</p>
<p>C.2 Sampling</p>
<p>To increase task complexity beyond uniform selection, we employ a strategy for sampling the start and target positions.For a given start cell s and any candidate cell c, the Manhattan distance is defined as Given a distance weight α ∈ [0, 1], we compute the weight for each candidate as
d(c) = |i s − i c | + |j s − j c |.w(c) = α d(c) + (1 − α).
Then, the probability of selecting cell c is
P (c) = w(c) c ′ ∈C w(c ′ )
, where C is the set of candidate cells.This approach biases the selection towards cells farther from s as α increases, while still preserving an element of randomness.In our implementation, we set α = 0.5.</p>
<p>D More Discussion</p>
<p>Comparison with SOTA.It is worth noting that in the NeurIPS 2024 paper Wu et al. (2024b), the tasks are significantly simpler, involving small grid maps with only a single possible route and support for only four movement directions.This setting can be considered a strict subset of our case.Despite the simplicity, their reported navigation SR and CR using GPT-4 are only around 15% and 40%, respectively.We attribute this to the lack of a well-structured and standardized task formulation.Their outputs often contain additional words beyond the intended answer, requiring substring matching for evaluation.In contrast, our task employs a formalized output format, akin to those used in modern LLM-based planning agents, ensuring a more standardized and structured approach to path generation, and hence can be more easily applied to real-world scenarios.</p>
<p>RL-agents vs LLM-agents.While modern LLMs are not at odds with RL as they are trained with RLHF Achiam et al. (2023); Ouyang et al. (2022), in stark contrast to conventional deep RL in robot path planning which stresses on optimizing expected discounted return (for single-agent RL) and the equilibrium of joint policies (in multi-agent RL), typically with considerable amount of training data, this paper demonstrates the zero-shot, training-free ability across modern LLMs in path planning and dynamic navigation in single-agent and multi-agent scenarios, evaluated on completion rate, success rate and path length which, albeit their simplicity and lack of sophistication compared to the specific and formal RL optimization objectives, are arguably equally relevant to any autonomous systems in evaluating their performance.</p>
<p>Given sufficient training data, the ballmark success rate (SR) for deep RL in simulated environments may exceed 0.9 with proximal policy optimization and soft actor-critic algorithms, depending on the complexity of the environment, which may be cluttered with other dynamic obstacles.Our zero-shot SR and related performances, as shown in Figures 3 and 4 in the main paper, can reach this performance with typical performances above 75% in multi-turn navigation.</p>
<p>With no training data, we have the same performance because we are zero-shot, while the ballmark SR for deep RL will drop close to zero when it relies on exploration to learn optimal policies, which can be extremely time-consuming to even discover basic navigation strategies.</p>
<p>Notwithstanding, as modern LLMs are trained with reinforcement learning from human feedback (RLHF), it will be an interesting and worthwhile future work to incorporate deep RL, now possible with much less training data, into our work, to gain deeper insight on LLM and deep RL integration for dynamic path navigation while further improving performance.</p>
<p>Figure 1 :
1
Figure 1: Single agent (top) navigating a real floor plan in a hide-and-seek scenario.Multiple agents (bottom) simultaneously move toward their destinations, the LLM autonomously resolves potential collisions.Our LLM-based system serves as an autonomous path navigator with zero-shot spatial reasoning capabilities, effectively handling obstacle avoidance and collision resolution in dynamic environments.</p>
<p>); Wu et al. (2024c); Wang et al. (2023); Prasad et al. (2023); Singh et al. (2023); Wu et al. (2024a); Wang et al. (2024).</p>
<p>Figure 2 :
2
Figure 2: Pipeline of our experiments.The process begins with the input of a floor plan, followed by spatial encoding, agent navigation, and simulation.The LLM navigator generates the agent's path, which is then validated and refined.If agents become stuck, they communicate with the LLM for guidance to resolve navigation issues.</p>
<p>), GeminiTeam et al. (2023), DeepSeekGuo et al. (2025), Llama Dubey et al. (</p>
<p>Figure 3 :
3
Figure 3: Additive and compositional strategies for multi-turn navigation refining.</p>
<p>(2023)  and TLControlWan et al. (2024), support trajectory control; however, they rely on manually defined input trajectories.Frameworks like Motion-AgentWu et al. (2024a) leverages LLMs to automatically decompose complex user requests and generate motion through a motion generation</p>
<p>Figure 5 :
5
Figure 5: Top-down views of the generated final trajectory and visualized humanoid motion results.Agents successfully navigate to their intended destinations while avoiding obstacles and other agents.</p>
<p>When paired with motion generation agents, the system can generate environment-aware, realistic motions.Agent walks up the stairs from</p>
<p>Figure 7 :
7
Figure 7: More qualitative results in top-down view.</p>
<p>Figure 9: Compositional strategy</p>
<p>Let d min and d max be the minimum and maximum distances from s to all candidate cells, respectively, and normalize the distance as d(c) = d(c) − d min d max − d min , with the convention d(c) = 0 when d max = d min .</p>
<p>Table 1 :
1
Zero-shot path navigation performance of various LLMs for single-agent scenarios in a single trial.Underlined numbers are significantly higher than every other model in the same column under a paired two-sided statistical test (p&lt;0.05).
ModelsTypeInputSR ↑ SPL ↑ CR ↑ WSR ↑Baseline--0.370 0.370 0.370 0.207Claude-3.5-Sonnet general
code 0.453 0.437 0.556 0.307 grid 0.330 0.302 0.426 0.183 Llama-3.3-70Bgeneral code 0.397 0.316 0.565 0.240 grid 0.350 0.296 0.520 0.197 Gemini-2.0-flashgeneral code 0.380 0.297 0.541 0.205 grid 0.273 0.223 0.328 0.139 GPT-4o multimodal code 0.420 0.381 0.560 0.291 grid 0.387 0.376 0.477 0.227 image 0.370 0.349 0.477 0.222 DeepSeek-R1 reasoning code 0.673 0.645 0.766 0.520 grid 0.650 0.623 0.729 0.503 o3-mini reasoning code 0.507 0.488 0.590 0.396 grid 0.781 0.710 0.828 0.665A Evaluation MetricsIn this section, we detail the four standard metrics (see 4.1.2)used to evaluate the performance of LLMs: Success Rate (SR), Success weighted by Path Length (SPL), Completion Rate (CR), and Weighted Success Rate (WSR).Success Rate (SR):The success rate is defined as the percentage of test cases where the agent successfully reaches the goal:where I(success i ) is an indicator function that returns 1 if the agent successfully reaches the target and 0 otherwise, and N is the total number of test cases.Success weighted by Path Length (SPL): SPL accounts for both the success rate and the efficiency of the path:where d i is the length of the trajectory taken by the agent, and d opt,i is the optimal path length for the i-th test case.The SPL metric rewards shorter, more efficient paths while penalizing longer, inefficient paths.Completion Rate (CR):The completion rate measures the fraction of the total path length that the agent is able to complete.It is defined as:where d i is the length of the trajectory taken by the agent, and d total,i is the total length of the path the agent was supposed to cover in the i-th test case.The CR metric emphasizes how much of the planned path is successfully completed, regardless of success or failure.Weighted Success Rate (WSR): WSR is a metric that assigns higher weights to longer paths, reflecting their cost or complexity, defined as:where d opt,i is the length of the optimal path, which can also reflect the difficulty of the test case.The denominator normalizes the WSR across all test cases, ensuring that the sum of WSR equals 1 by considering the total optimal path length.B More Qualitative ResultsWe provide additional qualitative results in the supplementary material, including attached videos and an HTML file for better visualization.Here, we present some final generated trajectories in top-down views.Figure7illustrates the capability of the LLM to effectively manage and resolve complex and challenging scenarios.Figure8demonstrates how the additive strategy utilizes a restart mechanism to successfully avoid obstacles.Figure9demonstrates how the compositional strategy effectively dynamically avoid obstacles "on the fly".
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning. Mohamed Aghzal, Erion Plaku, Ziyu Yao, arXiv:2310.032492023arXiv preprint</p>
<p>Look further ahead: Testing the limits of gpt-4 in path planning. Mohamed Aghzal, Erion Plaku, Ziyu Yao, 2024 IEEE 20th International Conference on Automation Science and Engineering (CASE). IEEE2024</p>
<p>Lucas-kanade 20 years on: A unifying framework. Simon Baker, Iain Matthews, International journal of computer vision. 562004</p>
<p>Learning to sit: Synthesizing human-chair interactions via hierarchical control. Yu-Wei Chao, Jimei Yang, Weifeng Chen, Jia Deng, arXiv:2407.21783Proceedings of the AAAI Conference on Artificial Intelligence. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, the AAAI Conference on Artificial Intelligence2021. 2024arXiv preprintThe llama 3 herd of models</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Stochastic scene-aware motion prediction. Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, Michael Black, Proceedings of the International Conference on Computer Vision 2021. the International Conference on Computer Vision 20212021</p>
<p>Sanja Fidler, and Xue Bin Peng. Synthesizing physical character-scene interactions. Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, ACM SIGGRAPH 2023 Conference Proceedings. 2023</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>3d-llm: Injecting the 3d world into large language models. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan, Advances in Neural Information Processing Systems. 202336</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Lifelong multi-agent path finding in large-scale warehouses. Jiaoyang Li, Andrew Tinka, Scott Kiesel, Joseph W Durham, Satish Kumar, Sven Koenig, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021</p>
<p>Multi-agent path finding in continuous spaces with projected diffusion models. Jinhao Liang, Jacob K Christopher, Sven Koenig, Ferdinando Fioretto, arXiv:2412.179932024arXiv preprint</p>
<p>Raster-to-vector: Revisiting floorplan transformation. Chen Liu, Jiajun Wu, Pushmeet Kohli, Yasutaka Furukawa, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2017</p>
<p>Advancing spatial reasoning through coordinate alignment and chain-of-thought for embodied task planning. Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, arXiv:2501.100742025arXiv preprint</p>
<p>From text to space: Mapping abstract spatial models in llms during a grid-world navigation task. Nicolas Martorell, arXiv:2502.166902025arXiv preprint</p>
<p>Large language models as general pattern machines. Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, Andy Zeng, arXiv:2307.047212023arXiv preprint</p>
<p>Transfer learning with synthetic corpora for spatial role labeling and reasoning. Roshanak Mirzaee, Parisa Kordjamshidi, arXiv:2210.169522022arXiv preprint</p>
<p>Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning, Kordjmashidi, Spartqa, arXiv:2104.05832A textual question answering benchmark for spatial reasoning. 2021arXiv preprint</p>
<p>Evaluating cognitive maps and planning in large language models with cogeval. Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Frujeri, Hiteshi Sharma, Nebojsa Jojic, Hamid Palangi, Robert Ness, Jonathan Larson, Advances in Neural Information Processing Systems. 202336</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters. Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, Sanja Fidler, ACM Transactions On Graphics (TOG). 4142022</p>
<p>Adapt: As-needed decomposition and planning with language models. Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, Tushar Khot, arXiv:2311.057722023arXiv preprint</p>
<p>Sanja Fidler, and Or Litany. Trace and pace: Controllable pedestrian animation via guided trajectory diffusion. Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Multi-robot motion planning with diffusion models. Yorai Shaoul, Itamar Mishani, Shivam Vats, Jiaoyang Li, Maxim Likhachev, arXiv:2410.030722024arXiv preprint</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Multi-agent pathfinding: Definitions, variants, and benchmarks. Roni Stern, Nathan Sturtevant, Ariel Felner, Sven Koenig, Hang Ma, Thayne Walker, Jiaoyang Li, Dor Atzmon, Liron Cohen, Kumar, Proceedings of the International Symposium on Combinatorial Search. the International Symposium on Combinatorial Search2019</p>
<p>Adaplanner: Adaptive planning from feedback with language models. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang, Advances in neural information processing systems. 202336</p>
<p>Coma: Compositional human motion generation with multi-modal agents. Shanlin Sun, Gabriel De Araujo, Jiaqi Xu, Shenghan Zhou, Hanwen Zhang, Ziheng Huang, Chenyu You, Xiaohui Xie, arXiv:2412.073202024arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Alexandre Ramé, et al. Gemma 2: Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, arXiv:2408.001182024arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Tlcontrol: Trajectory and language control for human motion synthesis. Weilin Wan, Zhiyang Dou, Taku Komura, Wenping Wang, Dinesh Jayaraman, Lingjie Liu, European Conference on Computer Vision. Springer2024</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.015602023arXiv preprint</p>
<p>Audio-agent: Leveraging llms for audio generation. Zixuan Wang, Yu-Wing Tai, Chi-Keung Tang, arXiv:2410.033352024arXiv preprint</p>
<p>Motion-agent: A conversational framework for human motion generation with llms. Qi Wu, Yubo Zhao, Yifan Wang, Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang, arXiv:2405.170132024aarXiv preprint</p>
<p>Do large language models have spatial cognitive abilities?. Ruoling Wu, Danhuai Guo, ACM Transactions on Intelligent Systems and Technology. </p>
<p>Mind's eye of llms: Visualization-of-thought elicits spatial reasoning in large language models. Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, Furu Wei, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024b</p>
<p>Can graph learning improve planning in llm-based agents?. Xixi Wu, Yifei Shen, Caihua Shan, Kaitao Song, Siwei Wang, Bohang Zhang, Jiarui Feng, Hong Cheng, Wei Chen, Yun Xiong, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024c</p>
<p>Omnicontrol: Control any joint at any time for human motion generation. Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, Huaizu Jiang, arXiv:2310.085802023arXiv preprint</p>
<p>Thinking in space: How multimodal large language models see, remember, and recall spaces. Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, Saining Xie, arXiv:2412.141712024arXiv preprint</p>
<p>Generating human interaction motions in scenes with text control. Hongwei Yi, Justus Thies, Michael J Black, Xue Bin Peng, Davis Rempe, European Conference on Computer Vision. Springer2024</p>
<p>L3mvn: Leveraging large language models for visual target navigation. Bangguo Yu, Hamidreza Kasaei, Ming Cao, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox, arXiv:2406.107212024arXiv preprint</p>
<p>Scenic: Scene-aware semantic navigation with instruction-guided control. Xiaohan Zhang, Sebastian Starke, Vladimir Guzov, Zhensong Zhang, Eduardo Pérez Pellitero, Gerard Pons-Moll, arXiv:2412.156642024arXiv preprint</p>
<p>Synthesizing diverse human motions in 3d indoor scenes. Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, Siyu Tang, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Esc: Exploration with soft commonsense constraints for zero-shot object navigation. Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia Jin, Lise Getoor, Xin Eric, Wang , International Conference on Machine Learning. PMLR2023</p>            </div>
        </div>

    </div>
</body>
</html>
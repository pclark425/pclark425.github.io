<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6965 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6965</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6965</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-270214176</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.00515v1.pdf" target="_blank">A Survey on Large Language Models for Code Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the widely recognized HumanEval and MBPP benchmarks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource website (https://codellm.github.io) to continuously document and disseminate the most recent advances in the field.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6965.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6965.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LATS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LATS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/agent method that uses an LLM as agent, value function, and optimizer combined with Monte Carlo Tree Search to construct trajectories, integrate external feedback, and iteratively improve program synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary, large transformer-based conversational LLM used as the core agent/value/optimizer in the LATS pipeline (treated as a black-box model in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LATS</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Constructs decision trajectories via Monte Carlo Tree Search where the LLM plays roles of agent, value function, and optimizer; integrates external feedback and learns from experience to refine code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>tree-search / generate-and-evaluate iteratively</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Function-level Python programming problems with unit tests to evaluate functional correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>pass@1 (execution-based)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>84.1% pass@1 (GPT-4 baseline reported in the survey's Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>94.4% pass@1 (GPT-4 with LATS reported in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not detail per-case failure modes for LATS; general concerns include high computational cost due to MCTS/trajectory search, dependence on quality of external feedback, and potentially brittle scaling to very large search spaces or long programs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Models for Code Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6965.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6965.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Debugging</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teaching Large Language Models to Self-Debug</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique where an LLM is instructed to analyze/explain its generated code and then iteratively refine the code using execution feedback or its own explanations to fix errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teaching large language models to self-debug</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Debugging</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate code, run execution/tests (when available) or generate code explanations, identify errors or failure causes, then prompt the model to revise the code accordingly in iterative cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>code generation / program repair (function-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Iteratively refine generated code using execution outputs or textual explanations to improve functional correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>execution-based metrics (e.g., pass@k / unit test pass rate) when unit tests are used; otherwise explanation-grounded checks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey notes reliance on execution/unit tests for robust feedback; when unit tests are unavailable the method depends on textual explanations which may be less reliable; iterative debugging increases computation and may fail if feedback is uninformative or model hallucinations persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Models for Code Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6965.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6965.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfEvolve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfEvolve (code evolution framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage LLM-driven framework that first generates domain-specific knowledge for a problem then generates a trial program and iteratively refines it through interactive prompting and execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfevolve: A code evolution framework via large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SelfEvolve</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Stage 1: generate domain/problem-specific knowledge; Stage 2: synthesize trial code and iteratively refine it using execution outcomes and interactive prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>code generation / iterative code improvement</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate and iteratively improve code using knowledge augmentation and execution-based feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>execution-based metrics (unit tests / pass rates) implied</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not report numerical outcomes; potential limitations include reliance on the quality of generated domain knowledge, need for execution environment, and compute cost for iterative cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Models for Code Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6965.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6965.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-feedback prompting method where an LLM generates an answer, critiques or rates it, and then revises the answer based on its own critique in repeated passes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iteratively produce an initial solution, generate internal critique/self-feedback, and use that critique to produce a revised solution; repeat until stopping criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>recursive self-critique / generate-and-revise</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General iterative refinement applicable to generation tasks (survey mentions it in the prompting engineering context for code and other tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not report empirical numbers for code; general limitations include increased latency and compute, and dependence on the model's ability to produce useful self-critiques (models can be overconfident or produce poor critiques).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Models for Code Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6965.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6965.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent approach where LLM-powered agents perform verbal self-reflection on feedback signals and store reflections in episodic memory to guide future decisions and improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents verbalize reflections about task feedback (failures/successes), store these reflections in memory, and use them to adapt future behavior — a form of self-reflection across interactions rather than single-step critique.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>episodic self-reflection across interactions</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General agent tasks; survey references Reflexion in context of code-generation/agents improving decision-making over episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not provide quantitative failure cases; potential limitations include reliance on accurate interpretation of feedback, memory management, and possible accumulation of incorrect reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Models for Code Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6965.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6965.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (CoT sampling + majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that samples multiple chain-of-thought (CoT) reasoning traces from an LLM and aggregates answers (e.g., via majority voting) to improve final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple independent reasoning traces/answers under CoT prompting, then aggregate (e.g., majority vote) final answers to reduce reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple stochastic samples (not iterative self-critique)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Primarily multi-step reasoning tasks; survey lists it among prompting strategies that can be applied to code reasoning contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey notes these prompting-based ensemble techniques increase compute and sampling cost; they may not fix systematic flaws present in all samples and are less applicable when sampling diversity is low.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Models for Code Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6965.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6965.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought / ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberation framework that explores a tree of intermediate reasoning steps (thoughts) with search (e.g., MCTS) to find better final solutions for complex problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Tree-of-Thought (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Perform structured search over intermediate 'thought' states (branching and pruning) to deliberate and improve final outcomes; can be combined with value functions and search policies.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>tree search / deliberative multi-step generation</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Complex reasoning and planning tasks; survey references ToT under advanced prompting strategies relevant to code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not provide code-specific numbers; general limitations include increased computational expense and sensitivity to the quality of intermediate thought proposals and value estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Models for Code Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6965.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6965.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeT/LEVER re-ranking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeT / LEVER (generate-and-rerank by execution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that generate many candidate code samples with an LLM and then re-rank/prioritize candidates based on execution outcomes (unit tests) to select the best solution, without further self-revision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CodeT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generate-many + execution-based rerank</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample a set of candidate programs, execute them (or use execution predictors), and rank candidates using execution success/failures to pick a correct solution; no further self-editing.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-many and rerank (not iterative self-critique)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>code generation (function-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select best-of-N generated solutions using execution or execution-predictor signals (pass/fail/unit tests).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>pass@k (execution-based selection improves effective pass rate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey highlights these methods do not perform further self-refinement of chosen programs; they require many samples (compute heavy) and depend on quality of unit tests; cannot improve a single candidate beyond selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Large Language Models for Code Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Selfevolve: A code evolution framework via large language models <em>(Rating: 2)</em></li>
                <li>Self-Consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6965",
    "paper_id": "paper-270214176",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "LATS",
            "name_full": "LATS",
            "brief_description": "A prompting/agent method that uses an LLM as agent, value function, and optimizer combined with Monte Carlo Tree Search to construct trajectories, integrate external feedback, and iteratively improve program synthesis.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Proprietary, large transformer-based conversational LLM used as the core agent/value/optimizer in the LATS pipeline (treated as a black-box model in the survey).",
            "model_size": null,
            "reflection_method_name": "LATS",
            "reflection_method_description": "Constructs decision trajectories via Monte Carlo Tree Search where the LLM plays roles of agent, value function, and optimizer; integrates external feedback and learns from experience to refine code generation.",
            "iteration_type": "tree-search / generate-and-evaluate iteratively",
            "num_iterations": null,
            "task_name": "HumanEval",
            "task_description": "Function-level Python programming problems with unit tests to evaluate functional correctness.",
            "evaluation_metric": "pass@1 (execution-based)",
            "performance_before_reflection": "84.1% pass@1 (GPT-4 baseline reported in the survey's Table 6)",
            "performance_after_reflection": "94.4% pass@1 (GPT-4 with LATS reported in survey)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Survey does not detail per-case failure modes for LATS; general concerns include high computational cost due to MCTS/trajectory search, dependence on quality of external feedback, and potentially brittle scaling to very large search spaces or long programs.",
            "uuid": "e6965.0",
            "source_info": {
                "paper_title": "A Survey on Large Language Models for Code Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-Debugging",
            "name_full": "Teaching Large Language Models to Self-Debug",
            "brief_description": "A prompting technique where an LLM is instructed to analyze/explain its generated code and then iteratively refine the code using execution feedback or its own explanations to fix errors.",
            "citation_title": "Teaching large language models to self-debug",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Self-Debugging",
            "reflection_method_description": "Generate code, run execution/tests (when available) or generate code explanations, identify errors or failure causes, then prompt the model to revise the code accordingly in iterative cycles.",
            "iteration_type": "generate-then-reflect iterative refinement",
            "num_iterations": null,
            "task_name": "code generation / program repair (function-level)",
            "task_description": "Iteratively refine generated code using execution outputs or textual explanations to improve functional correctness.",
            "evaluation_metric": "execution-based metrics (e.g., pass@k / unit test pass rate) when unit tests are used; otherwise explanation-grounded checks",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Survey notes reliance on execution/unit tests for robust feedback; when unit tests are unavailable the method depends on textual explanations which may be less reliable; iterative debugging increases computation and may fail if feedback is uninformative or model hallucinations persist.",
            "uuid": "e6965.1",
            "source_info": {
                "paper_title": "A Survey on Large Language Models for Code Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SelfEvolve",
            "name_full": "SelfEvolve (code evolution framework)",
            "brief_description": "A two-stage LLM-driven framework that first generates domain-specific knowledge for a problem then generates a trial program and iteratively refines it through interactive prompting and execution feedback.",
            "citation_title": "Selfevolve: A code evolution framework via large language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "SelfEvolve",
            "reflection_method_description": "Stage 1: generate domain/problem-specific knowledge; Stage 2: synthesize trial code and iteratively refine it using execution outcomes and interactive prompts.",
            "iteration_type": "generate-then-reflect iterative refinement",
            "num_iterations": null,
            "task_name": "code generation / iterative code improvement",
            "task_description": "Generate and iteratively improve code using knowledge augmentation and execution-based feedback.",
            "evaluation_metric": "execution-based metrics (unit tests / pass rates) implied",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Survey does not report numerical outcomes; potential limitations include reliance on the quality of generated domain knowledge, need for execution environment, and compute cost for iterative cycles.",
            "uuid": "e6965.2",
            "source_info": {
                "paper_title": "A Survey on Large Language Models for Code Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine: Iterative refinement with self-feedback",
            "brief_description": "An iterative self-feedback prompting method where an LLM generates an answer, critiques or rates it, and then revises the answer based on its own critique in repeated passes.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Self-Refine",
            "reflection_method_description": "Iteratively produce an initial solution, generate internal critique/self-feedback, and use that critique to produce a revised solution; repeat until stopping criteria.",
            "iteration_type": "recursive self-critique / generate-and-revise",
            "num_iterations": null,
            "task_name": null,
            "task_description": "General iterative refinement applicable to generation tasks (survey mentions it in the prompting engineering context for code and other tasks).",
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Survey does not report empirical numbers for code; general limitations include increased latency and compute, and dependence on the model's ability to produce useful self-critiques (models can be overconfident or produce poor critiques).",
            "uuid": "e6965.3",
            "source_info": {
                "paper_title": "A Survey on Large Language Models for Code Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "An agent approach where LLM-powered agents perform verbal self-reflection on feedback signals and store reflections in episodic memory to guide future decisions and improve performance.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Reflexion",
            "reflection_method_description": "Agents verbalize reflections about task feedback (failures/successes), store these reflections in memory, and use them to adapt future behavior — a form of self-reflection across interactions rather than single-step critique.",
            "iteration_type": "episodic self-reflection across interactions",
            "num_iterations": null,
            "task_name": null,
            "task_description": "General agent tasks; survey references Reflexion in context of code-generation/agents improving decision-making over episodes.",
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Survey does not provide quantitative failure cases; potential limitations include reliance on accurate interpretation of feedback, memory management, and possible accumulation of incorrect reflections.",
            "uuid": "e6965.4",
            "source_info": {
                "paper_title": "A Survey on Large Language Models for Code Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (CoT sampling + majority voting)",
            "brief_description": "A method that samples multiple chain-of-thought (CoT) reasoning traces from an LLM and aggregates answers (e.g., via majority voting) to improve final-answer accuracy.",
            "citation_title": "Self-Consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Self-Consistency",
            "reflection_method_description": "Generate multiple independent reasoning traces/answers under CoT prompting, then aggregate (e.g., majority vote) final answers to reduce reasoning errors.",
            "iteration_type": "voting over multiple stochastic samples (not iterative self-critique)",
            "num_iterations": null,
            "task_name": null,
            "task_description": "Primarily multi-step reasoning tasks; survey lists it among prompting strategies that can be applied to code reasoning contexts.",
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Survey notes these prompting-based ensemble techniques increase compute and sampling cost; they may not fix systematic flaws present in all samples and are less applicable when sampling diversity is low.",
            "uuid": "e6965.5",
            "source_info": {
                "paper_title": "A Survey on Large Language Models for Code Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Tree-of-Thought / ToT",
            "name_full": "Tree of Thoughts (ToT)",
            "brief_description": "A deliberation framework that explores a tree of intermediate reasoning steps (thoughts) with search (e.g., MCTS) to find better final solutions for complex problem solving.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Tree-of-Thought (ToT)",
            "reflection_method_description": "Perform structured search over intermediate 'thought' states (branching and pruning) to deliberate and improve final outcomes; can be combined with value functions and search policies.",
            "iteration_type": "tree search / deliberative multi-step generation",
            "num_iterations": null,
            "task_name": null,
            "task_description": "Complex reasoning and planning tasks; survey references ToT under advanced prompting strategies relevant to code generation.",
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Survey does not provide code-specific numbers; general limitations include increased computational expense and sensitivity to the quality of intermediate thought proposals and value estimates.",
            "uuid": "e6965.6",
            "source_info": {
                "paper_title": "A Survey on Large Language Models for Code Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CodeT/LEVER re-ranking",
            "name_full": "CodeT / LEVER (generate-and-rerank by execution)",
            "brief_description": "Methods that generate many candidate code samples with an LLM and then re-rank/prioritize candidates based on execution outcomes (unit tests) to select the best solution, without further self-revision.",
            "citation_title": "CodeT",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Generate-many + execution-based rerank",
            "reflection_method_description": "Sample a set of candidate programs, execute them (or use execution predictors), and rank candidates using execution success/failures to pick a correct solution; no further self-editing.",
            "iteration_type": "generate-many and rerank (not iterative self-critique)",
            "num_iterations": null,
            "task_name": "code generation (function-level)",
            "task_description": "Select best-of-N generated solutions using execution or execution-predictor signals (pass/fail/unit tests).",
            "evaluation_metric": "pass@k (execution-based selection improves effective pass rate)",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Survey highlights these methods do not perform further self-refinement of chosen programs; they require many samples (compute heavy) and depend on quality of unit tests; cannot improve a single candidate beyond selection.",
            "uuid": "e6965.7",
            "source_info": {
                "paper_title": "A Survey on Large Language Models for Code Generation",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Selfevolve: A code evolution framework via large language models",
            "rating": 2,
            "sanitized_title": "selfevolve_a_code_evolution_framework_via_large_language_models"
        },
        {
            "paper_title": "Self-Consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        }
    ],
    "cost": 0.0240225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Large Language Models for Code Generation
2021 May</p>
<p>Juyong Jiang 
Fan Wang 
Kim † Sungju sungju.kim@navercorp.com 
Naver Cloud 
South Korea 
Sunghun Kim 
Jiasi Shen 
Sungju Kim 
Sunghun Kim 
Sungju Kim 
Sunghun Kim 
Feb Mar May 
Jul Codex 
Sep Codet5 
Oct Pymt5 </p>
<p>The Hong Kong University of Science and Technology (Guangzhou)
China</p>
<p>The Hong Kong University of Science and Technology (Guangzhou)
China</p>
<p>JIASI SHEN †
The Hong Kong University of Science and Technology
China</p>
<p>The Hong Kong University of Science and Technology (Guangzhou)
China</p>
<p>The Hong Kong University of Science and Technology (Guangzhou)
Fan WangGuangzhouChina</p>
<p>The Hong Kong University of Science and Technology (Guangzhou)
Guangzhou, Jiasi ShenChina</p>
<p>The Hong Kong University of Science and Technology
Hong Kong, Sungju KimChina</p>
<p>NAVER Cloud
SeoulSouth Korea; Sunghun Kim</p>
<p>The Hong Kong University of Science and Technology (Guangzhou)
GuangzhouChina</p>
<p>https:// codegeex.cn/en-US</p>
<p>Claude</p>
<p>CodeT SelfEvolve LEVER RLTF</p>
<p>A Survey on Large Language Models for Code Generation
2021 May22689D2E8E78B6A430D5584A9E6C7261arXiv:2406.00515v1[cs.CL]Large Language ModelsCode Large Language ModelsCode Generation Metrics Exact MatchBLEU[175]ROUGE[140]METEOR[22]CodeBLEU[191]pass@k[45] n@k[136]test case average[85]execution accuracy[190]pass@t[170]perplexity[105] Human Evaluation CodePlan[21]RepoFusion[205]CodeBLEU[191] LLM-as-a-Judge AlpacaEval[133]MT-bench[274]ICE-Score[284] Application GitHub Copilot[45]CodeGeeX[275]CodeWhisperer[11]Codeium[55]CodeArts Snap[201]TabNine[212] Replit[192] JavaPythonJavaScriptTypeScriptObjective C++Objective CPascalHTMLSQLKotlinRShellCudaFortranTexLeanScala ClionRubyMineAppCodeAquaIntelliJ IDEAVS CodePyCharmAndroid StudioWebStormRiderGoLandDataGripDataSpell
Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions.This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot.Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation.In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation.We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, and real-world applications.In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the widely recognized HumanEval and MBPP benchmarks to highlight the progressive enhancements in LLM capabilities for code generation.We identify critical challenges and promising opportunities regarding the gap between academia and practical development.Furthermore, we have established a dedicated resource website (https://codellm.github.io) to continuously document and disseminate the most recent advances in the field.CCS Concepts: • General and reference → Surveys and overviews; • Software and its engineering → Software development techniques; • Computing methodologies → Artificial intelligence.</p>
<p>INTRODUCTION</p>
<p>The advent of Large Language Models (LLMs) such as ChatGPT1 [171] has profoundly transformed the landscape of automated code-related tasks [45], including code completion [78,152,233,244], code translation [48,121,211], and code repair [109,170,176].A particularly intriguing application of LLMs is code generation, a task that involves producing source code from natural language descriptions.Despite varying definitions across studies [47,191,204,232], for the purposes of this survey, we adopt a consistent definition of code generation as the natural-language-to-code (NL2Code) task [15,16,264].This area has garnered substantial interest from both academia and industry, as evidenced by the development of tools like GitHub Copilot2 [45], CodeGeeX 3 [275], and Amazon CodeWhisperer 4 , which leverage groundbreaking code LLMs to facilitate software development.</p>
<p>Initial investigations into code generation primarily utilized heuristic rules or expert systems, such as probabilistic grammar-based frameworks [9,57,113] and specialized language models [59,74,106].These early techniques were typically rigid and difficult to scale.However, the introduction of Transformer-based LLMs has shifted the paradigm, establishing them as the preferred method due to their superior proficiency and versatility.One remarkable aspect of LLMs is their capability to follow instructions [51,164,173,238,250], enabling even novice programmers to write code by simply articulating their requirements.This emergent ability has democratized coding, making it accessible to a broader audience [264].The performance of LLMs on code generation tasks has seen remarkable improvements, as illustrated by the HumanEval leaderboard 5 , which showcases the evolution from PaLM 8B [49] of 3.6% to LDB [279] of 95.1% on Pass@1 metrics.As can be seen, the HumanEval benchmark [45] has been established as a de facto standard for evaluating the coding proficiency of LLMs [45].</p>
<p>To offer a comprehensive chronological evolution, we present an overview of the development of LLMs for code generation, as illustrated in Figure 1.The landscape of LLMs for code generation is characterized by a spectrum of models, with certain models like ChatGPT [173], GPT4 [5], LLaMA [217,218], and Claude 3 [13] serving general-purpose applications, while others such as StarCoder [132,151], Code LLaMA [196], DeepSeek-Coder [79], and Code Gemma [54] are tailored specifically for code-centric tasks.The convergence of code generation with the latest LLM advancements is pivotal, especially when programming languages can be considered as distinct dialects of multilingual natural language [15,275].These models are not only tested against software engineering (SE) requirements but also propel the advancement of LLMs into practical production [271].</p>
<p>While recent surveys have shed light on code LLMs from the lenses of Natural Language Processing (NLP), Software Engineering (SE), or a combination of both disciplines [91,264,271,278], they have often encompassed a broad range of code-related tasks.There remains a dearth of literature specifically reviewing advanced topics in code generation, such as meticulous data curation, instruction tuning, alignment with feedback, prompting techniques, the development of autonomous coding agents, retrieval augmented code generation, LLM-as-a-Judge for code generation, among others.A notably pertinent study [15,264] also concentrates on LLMs for text-to-code generation (NL2Code), yet it primarily examines models released from 2020 to 2022.Consequently, this noticeable temporal gap has resulted in an absence of up-to-date literature reviews that contemplate the latest advancements, including models like CodeQwen [215], WizardCoder [154], and PPOCoder [204], as well as the comprehensive exploration of the advanced topics previously mentioned.</p>
<p>Recognizing the need for a dedicated and up-to-date literature review, this survey endeavors to fill that void.We provide a systematic review that will serve as a foundational reference for researchers quickly exploring the latest progress in LLMs for code generation.A taxonomy is introduced to categorize and examine recent advancements, encompassing data curation [154,231,240], advanced topics [42,47,94,125,146,152,164,166,177,205,266], evaluation methods [45,85,111,284], and practical applications [45,275].This category aligns with the comprehensive lifecycle of an LLM for code generation.Furthermore, we pinpoint critical challenges and identify promising opportunities to bridge the research-practicality divide.Therefore, this survey allows NLP and SE researchers to seamlessly equip with a thorough understanding of LLM for code generation, highlighting cutting-edge directions and current hurdles and prospects.</p>
<p>The remainder of the survey is organized following the structure outlined in our taxonomy in Figure 3.In Section 2, we introduce the preliminaries of LLM with Transformer architecture and formulate the task of LLM for code generation.Then, in Section 3, we propose a taxonomy, categorizing the complete process of LLMs in code generation.Section 4 delves into the specifics of LLMs for code generation within this taxonomy framework.In Section 5, we underscore the critical challenges and promising opportunities for bridging the research-practicality gap and conclude this work in Section 6.</p>
<p>BACKGROUND</p>
<p>Large Language Models</p>
<p>The effectiveness of large language models (LLMs) is fundamentally attributed to their substantial quantity of model parameters, large-scale and diversified datasets, and the immense computational power utilized during training [87,114].Generally, scaling up language models consistently results in enhanced performance and sample efficiency across a broad array of downstream tasks [238,273].However, with the expansion of the model size to a certain extent (e.g., GPT-3 [31] with 175Bparameters and PaLM [49] with 540B), LLMs have exhibited an unpredictable phenomenon known as emergent abilities 6 , including instruction following [173], in-context learning [65], and step-bystep reasoning [95,239], which are absent in smaller models but apparent in larger ones [238].</p>
<p>Adhering to the same architectures of the Transformer [222] in LLMs, code LLMs are specifically pre-trained on large-scale unlabeled code corpora, whereas general-purpose LLMs (e.g., ChatGPT [171]) are pre-trained on a blend of code and text data.Analogous to LLMs, Code LLMs can also be classified into three architectural categories: encoder-only models, decoder-only models, and encoder-decoder models.For encoder-only models, such as CodeBERT [68], they are typically suitable for code comprehension tasks including type prediction, code retrieval, and clone detection.For decoder-only models, such as StarCoder [31], they predominantly excel in generation tasks, such as code generation, code translation, and code summarization.Encoder-decoder models, such as CodeT5 [234], can accommodate both code understanding and generation tasks but do not necessarily outperform encoder-only or decoder-only models.The overall architectures of the different Code LLMs for code generation are depicted in Figure 2.</p>
<p>In the following subsection, we will delineate the key modules of the Transformer layers in Code LLMs.</p>
<p>2.1.1Multi-Head Self-Attention Modules.Each Transformer layer incorporates a multi-head selfattention (MHSA) mechanism to discern the inherent semantic relationships within a sequence A Survey on Large Language Models for Code Generation 1:5 of tokens across ℎ distinct latent representation spaces.Formally, the MHSA employed by the Transformer can be formulated as follows:
h (𝑙 ) = MultiHeadSelfAttn(Q, K, V) = Concat {Head 𝑖 } ℎ 𝑖=1 W O ,(1)Head 𝑖 = Attention(H (𝑙 −1) W Q 𝑖 Q , H (𝑙 −1) W K 𝑖 K , H (𝑙 −1) W V 𝑖 V ),(2)Attention(Q, K, V) = softmax QK 𝑇 √︁ 𝑑 𝑚𝑜𝑑𝑒𝑙 /ℎ V,(3)
where H ( −1) ∈ R ×  denotes the input to the -th Transformer layer, while h ( ) ∈ R ×  represents the output of MHSA sub-layer.The quantity of distinct attention heads is represented by ℎ, and   refers to the model dimension.The set of projections
W Q 𝑖 , W K 𝑖 , W V 𝑖 , W O 𝑖 ∈
R   ×  /ℎ encompasses the affine transformation parameters for each attention head Head  , transforming the Query Q, Key K, Value V, and the output of the attention sub-layer, The softmax function is applied in a row-wise manner.The dot-products of queries and keys are divided by a scaling factor √︁   /ℎ to counteract the potential risk of excessive large inner products and correspondingly diminished gradients in the softmax function, thus encouraging a more balanced attention landscape.</p>
<p>In addition to multi-head self-attention, there are two other types of attention based on the source of queries and key-value pairs:</p>
<p>• Masked Multi-Head Self-Attention.Within the decoder layers of the Transformer, the self-attention mechanism is constrained by introducing an attention mask, ensuring that queries at each position can only attend to all key-value pairs up to and inclusive of that position.To facilitate parallel training, this is typically executed by assigning a value of 0 to the lower triangular part and setting the remaining elements to −∞.Consequently, each item attends only to its predecessors and itself.Formally, this modification in Equation 3 can be depicted as follows:
Attention(Q, K, V) = softmax QK 𝑇 √︁ 𝑑 𝑚𝑜𝑑𝑒𝑙 /ℎ + M 𝑚𝑎𝑠𝑘 V,(4)M 𝑚𝑎𝑠𝑘 = 𝑚 𝑖 𝑗 𝑛×𝑛 = I(𝑖 ≥ 𝑗) 𝑛×𝑛 = 0 for 𝑖 ≥ 𝑗 −∞ otherwise ,(5)
This form of self-attention is commonly denoted as autoregressive or causal attention [141].• Cross-Layer Multi-Head Self-Attention.The queries are derived from the outputs of the preceding (decoder) layer, while the keys and values are projected from the outputs of the encoder.</p>
<p>2.1.2Position-wise Feed-Forward Networks.Within each Transformer layer, a Position-wise Feed-Forward Network (PFFN) is leveraged following the MHSA sub-layer to refine the sequence embeddings at each position  in a separate and identical manner, thereby encoding more intricate feature representations.The PFFN is composed of a pair of linear transformations, interspersed with a ReLU activation function.Formally, Fig. 2. The overview of large language models (LLMs) with encoder-decoder and decoder-only Transformer architecture for code generation, adapted from [222].
PFFN(ℎ (𝑙 ) ) = Concat FFN(ℎ (𝑙 ) 𝑖 ) 𝑇 𝑛 𝑖=1 𝑇 ,(6)FFN(ℎ (𝑙 ) 𝑖 ) = ReLU(ℎ (𝑙 ) 𝑖 W (1) + 𝑏 (1) )W (2) + 𝑏 (2) ,(7)
where ℎ ( ) ∈ R ×  is the outputs of MHSA sub-layer in -th Transformer layer, and ℎ ( )  ∈ R   denotes the latent representation at each sequence position.The projection matrices W (1) , (W (2) )  ∈ R   ×4  and bias vectors {b (1) , b (2) } ∈ R   are parameters learned during training.These parameters remain consistent across all positions while are individually initialized from layer to layer.In this context,  represents the transpose operation on a matrix.</p>
<p>Residual Connection and Normalization.</p>
<p>To alleviate the issue of vanishing or exploding gradients resulting from network deepening, the Transformer model incorporates a residual connection [84] around each of the aforementioned modules, followed by Layer Normalization [17].For the placement of Layer Normalization operation, there are two widely used approaches: 1) Post-Norm: Layer normalization is implemented subsequent to the element-wise residual addition, in accordance with the vanilla Transformer [222].2) Pre-Norm: Layer normalization is applied to the input of each sub-layer, as seen in models like GPT-2 [186].Formally, it can be formulated as:
Post-Norm : H (l) = LayerNorm(PFFN(h (l) ) + h (l) ), h (l) = LayerNorm(MHSA(H (l−1) ) + H (l−1) )(8)
Pre-Norm :
H (l) = PFFN(LayerNorm(h (l) )) + h (l) , h (l) = MHSA(LayerNorm(H (l−1) )) + H (l−1)(9)
2.1.4Positional Encoding.Given that self-attention alone cannot discern the positional information of each input token, the vanilla Transformer introduces an absolute positional encoding method to supplement this positional information, known as sinusoidal position embeddings [222].Specifically, for a token at position , the position embedding is defined as:
p 𝑝𝑜𝑠,2𝑖 = sin( 𝑝𝑜𝑠 10000 2𝑖/𝑑 𝑚𝑜𝑑𝑒𝑙 ),(10)p 𝑝𝑜𝑠,2𝑖+1 = cos( 𝑝𝑜𝑠 10000 2𝑖/𝑑 𝑚𝑜𝑑𝑒𝑙 ),(11)
where 2, 2 + 1 represent the dimensions of the position embedding, while   denotes the model dimension.Subsequently, each position embedding is added to the corresponding token embedding, and the sum is fed into the Transformer.Since the inception of this method, a variety of innovative positional encoding approaches have emerged, such as learnable embeddings [61], relative position embeddings [199], RoPE [209], and ALiBi [183].For more detailed descriptions of each method, please consult [141,272].</p>
<p>Code Generation</p>
<p>Large language models (LLMs) for code generation refer to the use of LLM to generate source code from natural language descriptions, a process also known as a natural-language-to-code task.Typically, these natural language descriptions encompass programming problem statements (or docstrings) and may optionally include some programming context (e.g., function signatures, assertions, etc.).Formally, these natural language (NL) descriptions can be represented as x.Given x, the use of an LLM with model parameters  to generate a code solution y can be denoted as   (y | x).To verify the functionality correctness of the code solution, y is subsequently executed via a compiler or interpreter, represented as Exe(•), on a suit of unit tests.The feedback from this execution can be denoted as Feedback(Exe(y)).</p>
<p>The advent of in-context learning abilities in LLM [238] has led to the appending of exemplars to the natural language description x as demonstrations to enhance code generation performance or constrain the generation format [131,178].A fixed set of  exemplars is denoted as {(x i , y i )}  =1 .Consequently, following [166], a more general formulation of LLMs for code generation with few-shot (or zero-shot) exemplars can be revised as:
𝑃 𝜃 (y | x) = 𝑃 𝜃 (y | prompt(x, {(x i , y i )} 𝑘 𝑖=1 )), 𝑘 = {0, 1, . . . , 𝑀 }(12)
where prompt(x, {(x i , y i )}  =1 )) is a string representation of the overall input, and {(x i , y i )}  =1 denotes a set of  exemplars randomly selected from {(x i , y i )}  =1 .In particular, when  = 0, this denotes zero-shot code generation, equivalent to vanilla ones without in-context learning.Subsequently, a variety of decoding strategies can be performed for code generation, including deterministic-based strategies (e.g., greedy search and beam search) and sampling-based strategies (e.g., temperature sampling, top-k sampling, and top-p (nucleus) sampling).For more detailed descriptions of each decoding strategy, please consult [89].
Greedy Search : y * = argmax y 𝑃 𝜃 (y | prompt(x, {(x i , y i )} 𝑘 𝑖=1 )), 𝑘 = {0, 1, . . . , 𝑀 }(13)Sampling : y ∼ 𝑃 𝜃 (y | prompt(x, {(x i , y 𝑖 )} 𝑘 𝑖=1 )), 𝑘 = {0, 1, . . . , 𝑀 }(14)</p>
<p>TAXONOMY</p>
<p>The recent surge in the development of Large Language Models (LLMs) has led to a significant number of these models being repurposed for code generation task through continued pre-training or fine-tuning.This trend is particularly observable in the realm of open-source models.For instance, Meta AI initially made the LLaMA [217] model publicly available, which was followed by the release of Code LLaMA [196], designed specifically for code generation.Similarly, DeepSeek LLM [25] developed and released by DeepSeeker has been extended to create DeepSeek Coder [79], a variant tailored for code generation.The Qwen team has developed and released Code Qwen [215], building on their original Qwen [19] model.Microsoft, on the other hand, has unveiled WizardLM [250] and is exploring its coding-oriented counterpart, WizardCoder [154].Google has joined the fray by releasing Gemma [214], subsequently followed by Code Gemma [54].Beyond simply adapting general-purpose LLMs for code-related tasks, there has been a proliferation of models specifically engineered for code generation.Notable examples include StarCoder [132], OctoCoder [164], and CodeGen [169].These models underscore the trend of LLMs being developed with a focus on code generation.</p>
<p>Recognizing the importance of these developments, we propose a taxonomy that categorizes and evaluates the latest advances in LLMs for code generation.This taxonomy, depicted in Figure 3, serves as a comprehensive reference for researchers seeking to quickly familiarize themselves with the state-of-the-art in this dynamic field.</p>
<p>In the subsequent sections, we will provide an in-depth analysis of each category related to code generation.This will encompass a definition of the problem, the challenges to be addressed, and a comparison of the most prominent models and their performance evaluation.</p>
<p>LARGE LANGAUGE MODELS FOR CODE GENERATION</p>
<p>Large language models (LLMs) with Transformer architecture have revolutionized a multitude of fields, and their application in code generation has been particularly impactful.These models follow a comprehensive process that starts with the curation and synthesis of code data, followed by a structured training approach that includes pre-training and fine-tuning, and the use of sophisticated prompt engineering techniques.Recent advancements have seen the integration of repository-level and retrieval-augmented code generation, as well as the development of autonomous coding agents.Furthermore, the evaluation of coding abilities of LLMs has become a critical component of this research area.</p>
<p>In the forthcoming sections, we will explore these dimensions of LLMs in the context of code generation in detail.Section 4.1 will address the data curation and processing strategies employed throughout the various stages of LLM development.Section 4.2 will discuss data synthesis methods designed to mitigate the scarcity of high-quality data.Section 4.3 will outline the prevalent model architectures used in LLMs for code generation.Moving to Section 4.4, we will examine the techniques for full parameter fine-tuning and parameter-efficient fine-tuning, which are essential for tailoring LLMs to code generation task.Section 4.5 will shed light on enhancing code quality through reinforcement learning, utilizing the power of feedback.Section 4.6 will delve into the strategic use of prompts to maximize the coding capabilities of LLMs.The innovative approaches of repository-level and retrieval-augmented code generation will be elaborated in Sections 4.7 and 4.8, respectively.Additionally, Section 4.9 will discuss the exciting field of autonomous coding agents.Lastly, Section 4.11 will provide insights into some of the practical applications that leverage LLMs for code generation, demonstrating the real-world impact of these sophisticated models.Through this comprehensive exploration, we aim to highlight the significance and potential of LLMs within the domain of automated code generation.</p>
<p>Data Curation &amp; Processing</p>
<p>The exceptional performance of Large Language Models (LLMs) can be attributed to their training on large-scale and diverse datasets [264].Meanwhile, the extensive parameters of these models necessitate substantial data to unlock their full potential, in alignment with established scaling law [87,114].For a general-purpose LLM, amassing a large-scale corpus of natural language from a variety of sources is imperative.Such sources include webpages, conversation data, books and  news, scientific data, and code [19,31,49,217,218,256], while these data are often crawled from the web and must undergo meticulous and aggressive pre-processing [189,271].Fortunately, multiple platforms and websites offer large-scale, open-source, and permissively licensed code corpora, such as GitHub 7 and Stack Overflow8 .Notably, the number of stars or forks of GitHub repositories has emerged as a valuable metric for filtering high-quality code datasets.In a similar vein, the quantity of votes on Stack Overflow can serve to discern the most relevant and superior answers.</p>
<p>Nonetheless, raw datasets are frequently laden with redundant, noisy data and personal information, eliciting concerns regarding privacy leakage, which may include the names and email addresses of repository contributors [7,34,123].Consequently, it is essential to undertake rigorous data-cleaning procedures.Typically, this process encompasses exact match deduplication, code data filtering based on average line length and a defined threshold for the fraction of alphanumeric characters, the removal of auto-generated files through keyword searches, and the expunction of personal user data [118,219].Specifically, the standard data preprocessing workflow is depicted in Figure 4.</p>
<p>The development of a proficient LLM for code generation necessitates the utilization of various types of code data at different developmental stages.Therefore, we categorize code data into three distinct classes: pre-training datasets, instruction-tuning datasets, and benchmarks for performance evaluation.The subsequent subsections will provide a detailed illustration of code data within each classification.</p>
<p>4.1.1Pre-training.The remarkable success of bidirectional pre-trained language models (PLMs) such as BERT [61] and unidirectional PLMs like GPT [185] has firmly established the practice of pre-training on large-scale unlabeled datasets to endow models with a broad spectrum of general knowledge.Extending this principle to the realm of code generation enables Large Language Models (LLMs) to assimilate fundamental coding principles, including the understanding of code structure dependencies, the semantics of code identifiers, and the intrinsic logic of code sequences [45,76,232,234].In light of this advancement, there has been a proliferation of large-scale unlabeled code datasets proposed to serve as the foundational training ground for LLMs to develop coding proficiency.A brief introduction of these datasets is as follows, with the statistics available in Table 1.</p>
<p>• 4.1.2Instruction Tuning.Instruction tuning refers to the process of fine-tuning large language models (LLMs) using a collection of datasets that are structured as instructions.This method has demonstrated a considerable improvement in model performance and an enhanced ability to generalize to unseen tasks that the model has not previously encountered, as evidenced by   [164] 2GB 277 2023-08 https://huggingface.co/datasets/bigcode/commitpackft Evol-Instruct-Code-80k [195] 80k -2023-07 https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1</p>
<p>Magicoder-OSS-Instruct-75k [240] 75k</p>
<p>Python, Shell, TypeScript, C++, Rust, PHP, Java, Swift, C# 2023-12 https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K</p>
<p>Self-OSS-Instruct-SC2-Exec-Filter-50k [261] 50k Python 2024-04 https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k</p>
<p>recent studies [51,173].Leveraging the benefits of instruction tuning, instruction tuning has been expanded into coding domains, especially for code generation, which involves the automatic generation of the intended code from a natural language description.The promise of instruction tuning in this area has led numerous researchers to develop large-scale instruction-tuning datasets tailored for code generation.Below, we provide an overview of several notable datasets tailored for instruction tuning, with their respective statistics detailed in Table 2.</p>
<p>• CodeAlpaca-20k [40]: CodeAlpaca-20k is a collection of 20K instruction-following data generated using the data synthesis techniques termed Self-Instruct outlined in [231], with modifications for code generation, editing, and optimization tasks instead of general tasks.• CommitPackFT [164]: CommitPackFT is a 2GB refined version of CommitPack.It is filtered to only include high-quality commit messages that resemble natural language instructions.• Evol-Instruct-Code-80k [195]: Evol-Instruct-Code-80k is an open-source implementation of Evol-Instruct-Code described in the WizardCoder paper [154], which enhances the fine-tuning effect of pre-trained code large models by adding complex code instructions.• Magicoder-OSS-Instruct-75k [240]: is a 75k synthetic data generated through OSS-Instruct with gpt-3.5-turbo-1106and used to train both Magicoder and Magicoder-S series models.• Self-OSS-Instruct-SC2-Exec-Filter-50k [261]: Self-OSS-Instruct-SC2-Exec-Filter-50k is generated by StarCoder2-15B using the OSS-Instruct [240] data synthesis approach.It was subsequently used to fine-tune StarCoder-15B without any human annotations or distilled data from huge and proprietary LLMs.</p>
<p>Benchmarks.</p>
<p>To rigorously assess the efficacy of Large Language Models (LLMs) for code generation, the research community has introduced a variety of high-quality benchmarks in recent years.Building on the foundational work by [45], numerous variations of the HumanEval dataset and additional benchmarks have emerged, aiming to evaluate a broader spectrum of code generation capabilities in LLMs.We roughly divide these benchmarks into six distinct categories based on their application contexts, including general-purpose, competitive programming, data science, multilingual, logical reasoning, and repository-level.The statistics for these benchmarks are presented in Table 3.General</p>
<p>• HumanEval [45]: HumanEval comprises 164 manually scripted Python programming problems, each featuring a function signature, docstring, body, and multiple unit tests.</p>
<p>• HumanEval+ [145]: HumanEval+ extends the original HumanEval [45] benchmark by increasing the scale of the test cases by 80 times.As the test cases increase, HumanEval+ can catch significant amounts of previously undetected incorrect code synthesized by LLMs.</p>
<p>• HumanEvalPack [164]: expands HumanEval [45] by extending it to encompass three coding tasks across six programming languages, namely code synthesis, code repair, and code explanation.</p>
<p>• MBPP [16]: MBPP is a collection of approximately 974 Python programming problems, crowdsourced and designed for entry-level programmers.Each problem comes with an English task description, a code solution, and three automated test cases.</p>
<p>• MBPP+ [145]: MBPP+ enhances MBPP [16]  • Spider [258]: Spider is large-scale complex text-to-SQL dataset covering 138 different domains.</p>
<p>It has over 10K questions and 5.6K complex SQL queries on 200 databases.This dataset aims to test a model's ability to generalize to SQL queries, database schemas, and new domains.</p>
<p>• CONCODE [102]: CONCODE is a dataset with over 100K samples consisting of Java classes from public GitHub repositories.It provides near zero-shot conditions that can test the model's ability to generalize to unseen natural language tokens with unseen environments.• ODEX [236]: ODEX is an open-domain dataset focused on the execution-based generation of Python code from natural language.It features 945 pairs of natural language queries and their corresponding Python code, all extracted from StackOverflow forums.• CoderEval [257]: CoderEval is a pragmatic code generation benchmark that includes 230 Python and 230 Java code generation problems.It can be used to evaluate the model performance in generating pragmatic code beyond just generating standalone functions.• ReCode [226]: Recode serves as a comprehensive robustness evaluation benchmark.ReCode applies perturbations to docstrings, function and variable names, code syntax, and code format, thereby providing multifaceted assessments of a model's robustness performance.</p>
<p>• StudentEval [18]: StudentEval is a dataset of 1,749 prompts for 48 problems, authored by 80 students who have only completed a one-semester Python programming class.Unlike many other benchmarks, it has multiple prompts per problem and multiple attempts by the same participant, each problem is also accompanied by a set of instructor-written test cases.</p>
<p>Competitions</p>
<p>• APPS [85]: The APPS benchmark is composed of 10K Python problems, spanning three levels of difficulty: introductory, interview, and competition.Each entry in the dataset includes a programming problem described in English, corresponding ground truth Python solutions, test cases defined by their inputs and outputs or function names if provided.• CodeContests [136]: is a competitive programming dataset consisting of samples from various sources including Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth.The dataset encompasses programming problems accompanied by test cases in the form of paired inputs and outputs, along with both correct and incorrect human solutions in multiple programming languages.</p>
<p>Data Science</p>
<p>• DSP [38]: DSP allows for model evaluation based on real data science pedagogical notebooks.It includes well-structured problems, along with unit tests to verify the correctness of solutions and a Docker environment for reproducible execution.• DS-1000 [122]: DS-1000 has 1K science questions from seven Python libraries, namely NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib.The DS-1000 benchmark features: (1) realistic problems with diverse contexts (2) implementation of multi-criteria evaluation metrics, and (3) defense against memorization.</p>
<p>• ExeDS [97]: ExeDS is a data science code generation dataset specifically designed for execution evaluation.It contains 534 problems with execution outputs from Jupyter Notebooks, as well as 123K examples for training and validation.</p>
<p>Multilingual</p>
<p>• MBXP [15]: MBXP is a multilingual adaptation of the original MBPP [16] dataset.It is created using a framework that translates prompts and test cases from the original Python datasets into the corresponding data in the targeted programming language.• Multilingual HumanEval [15]: Multilingual HumanEval is a dataset derived from HumanEval [45].It is designed to assess the performance of models in a multilingual context.It helps uncover the generalization ability of the given model on languages that are out-of-domain.• HumanEval-X [275]: HumanEval-X is developed for evaluating the multilingual ability of code generation models with 820 hand-writing data samples in C++, Java, JavaScript, and Go.• MultiPL-E [36]: MultiPL-E is a dataset for evaluating LLMs for code generation across 18 programming languages.It adopts the HumanEval [45] and the MBPP [16] Python benchmarks and uses little compilers to translate them to other languages.• xCodeEval [115]: xCodeEval is an executable multilingual multitask benchmark consisting of 25M examples covering 17 programming languages.Its tasks include code understanding, generation, translation, and retrieval.</p>
<p>Reasoning</p>
<p>• MathQA-X [15] MathQA-X is the multilingual version of MathQA [12].It is generated by utilizing a conversion framework that converts samples from Python datasets into the target language.• MathQA-Python [16] MathQA-Python is a Python version of the MathQA benchmark [12].</p>
<p>The benchmark, containing more than 23K problems, is designed to assess the capability of models to synthesize code from complex textual descriptions.• GSM8K [53]: GSM8K is a dataset of 8.5K linguistically diverse grade school math problems.</p>
<p>The dataset is crafted to facilitate the task of question answering on basic mathematical problems that requires multi-step reasoning.• GSM-HARD [71]: GSM-HARD is a more challenging version of the GSM8K [53] dataset.It replaces the numbers in the GSM8K questions with larger, less common numbers, thereby increasing the complexity and difficulty level of the problems.[205]: Stack-Repo is a dataset of 200 Java repositories from GitHub with neardeduplicated files.These files are augmented with three types of repository contexts: prompt proposal contexts, BM25 Contexts (based on BM25 similarity scores), and RandomNN Contexts (obtained using the nearest neighbors in the representation space of an embedding model).</p>
<p>• Repobench [150]: Repobench is a benchmark specifically used for evaluating repositorylevel code auto-completion systems.Supporting both Python and Java, it consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline).</p>
<p>• EvoCodeBench [130]: EvoCodeBench is an evolutionary code generation benchmark, constructed through a rigorous pipeline and aligned with real-world repositories.This benchmark also provides comprehensive annotations and robust evaluation metrics.• SWE-bench [111]: SWE-bench is a dataset that tests a model's ability to automatically solve GitHub issues.The dataset has 2,294 Issue-Pull Request pairs from 12 popular Python repositories.</p>
<p>• CrossCodeEval [63]: CrossCodeEval is a diverse and multilingual scope completion dataset covering four languages: Python, Java, TypeScript, and C#.This benchmark tests the model's ability to understand in-depth cross-file information and accurately complete the code.</p>
<p>• SketchEval [265]: SketchEval is a repository-oriented benchmark that encompasses data from 19 repositories, each varying in complexity.In addition to the dataset, SketchEval introduces a metric, known as SketchBLEU, to measure the similarity between two repositories based on their structures and semantics.</p>
<p>Data Synthesis</p>
<p>Numerous studies have demonstrated that high-quality datasets are integral to enhancing the performance of large language models (LLMs) in various downstream tasks [31,119,159,242,248,281].For instance, the LIMA model, a 65B parameter LLaMa language model fine-tuned with a standard supervised loss on a mere 1,000 meticulously curated prompts and responses, achieved performance on par with, or even superior to, GPT-4 in 43% of evaluated cases.This figure rose to 58% when compared to Bard and 65% against DaVinci003, all without the use of reinforcement learning or human preference modeling [281].The QuRating initiative strategically selects pretraining data embodying four key textual qualities -writing style, facts &amp; trivia, required expertise, and educational value -that resonate with human intuition.Training a 1.3B parameter model on such data resulted in reduced perplexity and stronger in-context learning compared to baseline models [242].Despite these advancements, acquiring quality data remains a significant challenge due to issues such as data scarcity, privacy concerns, and prohibitive costs [148,231].Human-generated data is often labor-intensive and expensive to produce, and it may lack the necessary scope and detail to navigate complex, rare, or ambiguous scenarios.As a resolution to these challenges, synthetic data has emerged as a viable alternative.By generating artificial datasets that replicate the intricacies of real-world information, models such as GPT-3.5-turbo[171] and GPT-4 [5] have enabled the creation of rich datasets without the need for human annotation [82,124,148,231].This approach is particularly beneficial in enhancing the instruction-following capabilities of LLMs, with a focus on generating synthetic instruction-based data.</p>
<p>A notable example of this approach is the Self-Instruct [231] framework, which employs an off-theshelf language model to generate a suite of instructions, inputs, and outputs.This data is then refined by removing invalid or redundant entries before being used to fine-tune the model.The empirical evidence supports the efficacy of this synthetic data generation methodology.Building upon this concept, the Alpaca [213] model, fine-tuned on 52k pieces of instruction-following data from a 7B parameter LLaMa [217] model, exhibits performance comparable to the text-davinci-003 model.WizardLM [250] introduced the Evol-Instruct technique, which incrementally transforms simple instructions into more complex variants.The fine-tuned LLaMa model using this technique has shown promising results in comparison to established proprietary LLMs such as ChatGPT [171] and GPT-4 [5], to some extent.Moreover, Microsoft has contributed to this field with their Phi series of models, predominantly trained on synthetic high-quality data, which includes Phi-1 (1.3B) [75] for Python coding, Phi-1.5 (1.3B) [135] for common sense reasoning and language understanding, Phi-2 (2.7B) [161] for advanced reasoning and language understanding, and Phi-3 (3.8B) [4] for general purposes.These models have consistently outperformed larger counterparts across various benchmarks, demonstrating the efficacy of synthetic data in model training.</p>
<p>Drawing on the successes of data synthesis for general-purpose Large Language Models (LLMs), researchers have expanded the application of synthetic data to the realm of code generation.The Code Alpaca model, as described in [40], has been fine-tuned on a 7B and 13B LLaMA model using a dataset of 20k instruction-following examples for code generation.This dataset was created by text-davinci-00310 and employed the Self-Instruct technique [231].Building on this, the WizardCoder 15B [154] utilizes the Evol-Instruct technique to create an enhanced dataset of 78k evolved code instruction examples.This dataset originates from the initial 20k instructionfollowing dataset used by Code Alpaca [40], which was also generated by text-davinci-003.The WizardCoder model, fine-tuned on the StarCoder [132] base model, achieved a 57.3% pass@1 on the HumanEval benchmarks.This performance not only surpasses all other open-source Code LLMs by a significant margin but also outperforms leading closed LLMs such as Anthropic's Claude and Google's Bard.In a similar vein, Magicoder [240] introduces a novel data synthesis approach termed OSS-INSTRUCT which enlightens LLMs with open-source code snippets to generate highquality instruction data for coding tasks.It aims to address the inherent biases often present in synthetic data produced by LLMs.Building upon CodeLlama [196], the MagicoderS-CL-7B model -fine-tuned with 75k synthetic instruction data using the OSS-INSTRUCT technique and with gpt-3.5-turbo-1106as the data generator -has outperformed the prominent ChatGPT on the HumanEval Plus benchmark, achieving pass@1 of 66.5% versus 65.9%.In a noteworthy development, Microsoft has introduced the phi-1 model [75], a more compact LLM of only 1.3B parameters.Despite its smaller size, phi-1 has been trained on high-quality textbook data sourced from the web (comprising 6 billion tokens) and supplemented with synthetic textbooks and exercises generated with GPT-3.5 (1 billion tokens).It has achieved pass@1 of 50.6% on HumanEval and 55.5% on MBPP, setting a new state-of-the-art for Python coding performance among existing small language models (SLMs).The latest contribution to this field is from the BigCode team, which has presented StarCoder2-15B-instruct [261], the first entirely self-aligned code LLM trained with a transparent and permissive pipeline.This model aligns closely with the OSS-INSTRUCT principles established by Magicoder, generating instructions based on seed functions filtered from the Stack v1 dataset [118] and producing responses through self-validation.Unlike Magicoder, StarCoder2-15B-instruct employs its base model, StarCoder2-15B, as the data generator, thus avoiding reliance on large and proprietary LLMs like GPT-3.5-turbo [171].</p>
<p>While synthetic data has demonstrated its potential across both small-and large-scale LMs for a variety of general and specialized tasks, including code generation, it also poses several challenges that must be addressed.These challenges include a lack of data diversity [242], the need to ensure the factuality and fidelity of the information [221,243], and the potential to amplify existing biases or introduce new ones [23,80].</p>
<p>Pre-Training</p>
<p>4.</p>
<p>3.1 Model Architectures.Since the inception of the Transformer architecture for machine translation [222], it has become the de facto backbone for a multitude of large language models (LLMs) that address a wide range of downstream tasks.The Transformer and its derivatives owe their prominence to their exceptional ability to parallelize computation and their powerful representational capacities [256,273].Through innovative scaling techniques, such as Mixture-of-Experts (MoE) [33,200] and Depth-Up-Scaling (DUS) [117], the capacity of Transformer-based LLMs has expanded to encompass hundreds of billions or even trillions of parameters.These scaled-up models have exhibited a range of emergent abilities [87,114,238], such as instruction following [173], in-context learning [65], and step-by-step reasoning [95,239] that were previously unforeseen.</p>
<p>In the domain of code generation using LLMs, the architecture of contemporary models generally falls into one of two categories: encoder-decoder models, such as CodeT5 [234], CodeT5+ [232], and CodeRL [125]; or decoder-only models, such as Codex [45], StarCoder [132], Code Llama [196], and CodeGemma [54].These architectures are depicted in Figure 2(b) and (c), respectively.For a comprehensive overview, Table 4 details the encoder-decoder architectures, while Table 5 focuses on the decoder-only models utilized in code generation.</p>
<p>Pre-training</p>
<p>Tasks.In the initial phase, language models for code generation are typically trained from scratch using datasets consisting of manually annotated pairs of natural language descriptions and corresponding code snippets, within a supervised learning framework.However, manual annotation is not only laborious and time-consuming, but the efficacy of the resulting models is also constrained by both the volume and the quality of the available annotated data.This limitation is especially pronounced in the context of low-resource programming languages, such as Swahili and Yoruba, where annotated examples are scarce [35,43].In light of these challenges, there has been a shift towards an alternative training strategy that involves pre-training models on extensive and unlabelled code corpora.This method is aimed at imbuing the models with a broad understanding of programming knowledge, encompassing elements like identifiers, code structure, and underlying semantics [45].In this regard, two pre-training tasks have gained prominence for their effectiveness, namely Causal Language Modeling (CLM), also known as unidirectional language modeling or next-token prediction, and Denoising Autoencoding (DAE).The CLM task can be applied to both decoder-only and encoder-decoder model architectures, while DAE tasks are specifically designed for encoder-decoder frameworks.It should also be noted that there is a variety of additional auxiliary pre-training tasks that can further enhance model performance.These include Masked Identifier Prediction, Identifier Tagging, Bimodal Dual Generation [234], Text-Code Matching, and Text-Code Contrastive Learning [232].These tasks contribute to a more nuanced and comprehensive pre-training process, equipping the models with the capabilities necessary to handle a wide range of code generation scenarios.</p>
<p>Causal Language Modeling.In decoder-only LLMs, given a sequence of tokens x = { 1 , . . .,   }, the CLM task refers to autoregressively predict the target tokens   based on the preceding tokens  &lt; in a sequence.The causal language modeling objective for training decoder LLMs is to minimize the following likelihood:
L 𝐷𝑒𝑐𝑜𝑑𝑒𝑟 −𝑜𝑛𝑙 𝑦 𝐶𝐿𝑀 (x) = − log( 𝑛 𝑖=1 𝑃 𝜃 (𝑥 𝑖 | x &lt;𝑖 )) = 𝑛 ∑︁ 𝑖=1 − log 𝑃 𝜃 (𝑥 𝑖 | x &lt;𝑖 )(15)
where x &lt; represents the sequence of preceding tokens { 1 , . . .,   −1 } before x  in the input,  denotes the model parameters.The conditional probability   (  |x &lt; )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to −∞, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:
L 𝐸𝑛𝑐𝑜𝑑𝑒𝑟 −𝐷𝑒𝑐𝑜𝑑𝑒𝑟 𝐶𝐿𝑀 (x) = − log( 𝑛 𝑖=𝑘+1 𝑃 𝜃 (𝑥 𝑖 | x ≤𝑘 , x &lt;𝑖 )) = 𝑛 ∑︁ 𝑖=𝑘+1 − log 𝑃 𝜃 (𝑥 𝑖 | x ≤𝑘 , x &lt;𝑖 )(16)
where x ≤ is the source sequence input and x &lt; denotes the target sequence autoregressively generated so far.During the inference phase, pre-trained LLMs that have been trained on largescale code corpus can generate code in a zero-shot manner without the need for fine-tuning.This is achieved through the technique of prompt engineering, which guides the model to produce the desired output11 [31,186].Additionally, recent studies have explored the use of few-shot learning, also referred to as in-context learning, to enhance model performance further [131,178].Denoising Autoencoding.In addition to causal language modeling (CLM), the denoising autoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures for code generation, such as PLBART [6], CodeT5 [234], and its enhanced successor, CodeT5+ [232].Following T5 [189] and CodeT5 [234], the DAE refers to initially perturbing the source sequence by introducing randomly masked spans of varying lengths.This corrupted sequence serves as the input for the encoder.Subsequently, the decoder employs an autoregressive strategy to reconstruct the masked spans, integrating sentinel tokens to facilitate the generation process.This method has proven effective in improving the model's ability to generate semantically and syntactically accurate code by learning robust contextual representations [232,234].Formally, the denoising autoencoding objective for training encoder-decoder LLMs is to minimize the following likelihood:
L 𝐸𝑛𝑐𝑜𝑑𝑒𝑟 −𝐷𝑒𝑐𝑜𝑑𝑒𝑟 𝐷𝐴𝐸 (x) = 𝑘 ∑︁ 𝑖=1 − log 𝑃 𝜃 (x 𝑚𝑎𝑠𝑘𝑒𝑑_𝑠𝑝𝑎𝑛𝑠 𝑖 | x \𝑚𝑎𝑠𝑘𝑒𝑑_𝑠𝑝𝑎𝑛𝑠 , x 𝑚𝑎𝑠𝑘𝑒𝑑_𝑠𝑝𝑎𝑛𝑠 &lt;𝑖 )(17)
where  denotes the model parameters, x _ is the noisy input with masked spans, x _ is the masked spans to predict from the decoder with  denoting the number of tokens in x _ , and x _ &lt; is the span sequence autoregressively generated so far.Compared with CLM, the DAE task presents a more challenging scenario, as it necessitates a deeper understanding and capture of the intrinsic semantic relationships among token sequences by LLMs [189].</p>
<p>Instruction Tuning</p>
<p>After pre-training Large Language Models (LLM) on large-scale datasets, the next phase typically involves augmenting the model's ability to process and follow instructions, known as instruction tuning.Instruction tuning generally refers to the supervised fine-tuning of pre-trained LLMs using datasets comprised of structured examples framed as natural language instructions [103,173,237,268].Two exemplars of instruction data sampled from Code Alpaca [40] are demonstrated in Figure 5.It capitalizes on the heterogeneity of instruction types, positioning instruction tuning as a form of multi-task prompted training that significantly enhances the model's generalization to unseen tasks [51,173,197,237].
a-zA- Z]|[0-9]|[$-_@.&amp;+]|[!*(),]|(?:%[0-9a- fA-F][0-9a-fA-F]))+', string) print(urls) N/A
This string contains some urls such as https://www.google.comand https://www.facebook.com.</p>
<p>Generate a snippet of code to extract all the URLs from the given string.</p>
<p>Output:</p>
<p>Input: Instruction: Fig. 5. Two exemplars of instruction data sampled from Code Alpaca [40] used to instruction-tune pre-trained code LLM to enhance their alignment with natural language instructions.The instruction corpus encompasses a variety of tasks, each accompanied by distinct instructions, such as prime numbers generation and URLs extraction.</p>
<p>In the realm of code generation, natural language descriptions serve as the instructions guiding the model to generate corresponding code snippets.Consequently, a line of research on instruction tuning LLMs for code generation has garnered substantial interest across academia and industry.To perform instruction tuning, instruction data are typically compiled from source code with permissive licenses [99,118,151] (refer to Section 4.1.2) or are constructed from synthetic code data [154,240,261] (refer to Section 4.2).These datasets are then utilized to fine-tune LLMs through a supervised learning paradigm.However, the substantial computational resources required for full parameter fine-tuning (FFT) LLM pose a notable challenge, particularly in scenarios with constrained resources [62,138].To mitigate this issue, parameter-efficient fine-tuning (PEFT) has emerged as a compelling alternative strategy, gaining increasing attention for its potential to reduce resource consumption [62].In the following subsection, we categorize existing works based on their instruction-tuning strategies to provide a comprehensive and systematic review.</p>
<p>Full</p>
<p>Parameter Fine-tuning.Full parameter fine-tuning (FFT) involves updating all parameters within a pre-trained model, as shown in Figure 6(a).This approach is often preferred when ample computational resources and substantial training data are available, as it typically leads to better performance.[234] introduces an encoder-decoder pre-trained language model for code generation, named CodeT5+.They instruction-tune this model on a dataset comprising 20k instruction samples from Code Alpaca [40], resulting in an instruction-following model called InstructCodeT5+, which exhibited improved capabilities in code generation.[154] leverages the Evol-Instruct data synthesis technique from WizardLM [250] to evolve 20K code Alpaca [40] instruction samples into a 78K code instruction dataset.This enriched dataset is then used to fine-tune the StarCoder base model, resulting in WizardCoder, which showcases notable advancements in code generation.In a similar vein, inspired by the successes of WizardCoder [154] and RRHF [260], Pangu-Coder 2 [201] applies the Evol-Instruct method to generate 68k high-quality instruction samples from the initial 20k Code Alpaca [40] instruction samples.Additionally, they introduces a novel reinforcement learning via Rank Responses to align Test &amp; Teacher Feedback (RRTF), which further enhances the performance of Pangu-Coder 2 in code generation.Diverging from synthetic instruction data generation methods, OctoPack [164] utilizes real-world data by curating CommitPack from the natural structure of Git commits, which inherently pair code changes with human-written instructions.This dataset, consisting of 4 terabytes of Git commits across 350 programming languages, is employed to finetune StarCoder [132] and CodeGeeX2 [275], leading to the instruction-following code models of OctoCoder and OctoGeeX for code generation, respectively.The most recent innovation comes from Magicoder [240], who proposes OSS-INSTRUCT, a novel data synthesis method that leverages open-source code snippets to generate high-quality instruction data for code generation.This approach seeks to reduce the bias often present in synthetic data generated by LLM.In line with OSS-INSTRUCT, the BigCode team introduces StarCoder2-15B-instruct [261], which they claim to be the first entirely self-aligned Large Language Model (LLM) for code generation, trained with a fully permissive and transparent pipeline.Moreover, [54] harnesses open-source mathematics datasets, such as MATH [85] and GSM8k [53], along with synthetically generated code following the OSS-INSTRUCT [240] paradigm, to instruction-tune CodeGemma 7B, yielding exceptional results in mathematical reasoning and code generation tasks.</p>
<p>Parameter-Efficient Fine-tuning.</p>
<p>To mitigate the extensive computational and resource demands inherent in fine-tuning large language models (LLMs), the concept of parameter-efficient fine-tuning (PEFT) has emerged to focus on updating a minimal subset of parameters, which may either be a selection of the model's parameters or an array of additional parameters specifically introduced for the tuning process [62,138].The categorization of these methods is depicted in Figure 6(b), (c), and (d).A plethora of innovative PEFT approaches have been developed, among which BitFit [262], Adapter [92], Prompt tuning [128], Prefix-tuning [134], LoRA [93], IA 3 [144], QLoRA [60], and AdaLoRA [267] are particularly noteworthy.A seminal study in this field, LoRA [93], proposes a parameter update mechanism for a pre-trained weight matrix -such as those found in the key or value projection matrices of a Transformer block's multi-head self-attention layer -by factorizing the update into two low-rank matrices.Crucially, all original model parameters remain frozen, with only the pair of low-rank matrices being trainable.After fine-tuning, the product of these low-rank matrices can be seamlessly incorporated into the existing weight matrix through an element-wise addition.This process can be formally described as:
(W 0 + ΔW)𝑥 = W 0 𝑥 + ΔW𝑥 = W 𝑓 𝑟𝑜𝑧𝑒𝑛 0 𝑥 + 𝛼 𝑟 B 𝑡𝑟𝑎𝑖𝑛𝑎𝑏𝑙𝑒 𝑢𝑝 A 𝑡𝑟𝑎𝑖𝑛𝑎𝑏𝑙𝑒 𝑑𝑜𝑤𝑛 ΔW 𝑥 (18)
where W 0 ∈ R  × denotes a pre-trained weight matrix, B   ∈ R  × and A   ∈ R  × are two trainable low-rank matrixes and initialized by a zero matrix and a random Gaussian distribution N (0,  2 ) respectively, to ensure ΔW = 0 at the beginning of training.The rank  ≪ min(, ), the   is a scaling coefficient to balance the importance of the LoRA module, like a learning rate.</p>
<p>Despite the advancements in PEFT methods, their application in code generation remains limited.For instance, [108] pioneered the use of parameter-efficient instruction-tuning on a Llama 2 [218] model with a single RTX 3090 GPU, leading to the development of a multilingual code generation model called CodeUp.More recently, ASTRAIOS [285]   (a) refers to the Full Fine-tuning method, which updates all parameters of the base model during fine-tuning.(b) stands for the Specification-based PEFT method that conditionally fine-tunes a small subset of the model parameters while freezing the rest of the model, e.g.BitFit [262].(c) represents the Addition-based PEFT method that fine-tunes the incremental parameters introduced into the base model or input, e.g.Adapter [92], Prefix-tuning [134], and Prompt-tuning [128].(d) symbolizes the Reparameterization-based method which reparameterizes existing model parameters by low-rank transformation, e.g.LoRA [93], QLoRA [60], and AdaLoRA [267].</p>
<p>yielded several perceptive observations and conclusions, contributing valuable insights to the domain.</p>
<p>Reinforcement Learning with Feedback</p>
<p>Large language models (LLMs) have exhibited remarkable instruction-following capabilities through instruction tuning.However, they often produce outputs that are unexpected, toxic, biased, or hallucinated outputs that do not align with users' intentions or preferences [107,173,235].Consequently, aligning LLMs with human preference has emerged as a pivotal area of research.A notable work is InstructGPT [173], which further fine-tunes an instruction-tuned model utilizing reinforcement learning with human feedback (RLHF) on a dataset where labelers have ranked model outputs in order of quality, from best to worst.This method has been instrumental in the development of advanced conversational language models, such as ChatGPT [171] and Bard [157].Despite its success, acquiring high-quality human preference ranking data is a resource-intensive process [127].To address this, Reinforcement Learning from AI Feedback (RLAIF) [20,127] has been proposed to leverage powerful off-the-shelf LLMs (e.g., ChatGPT [171] and GPT-4 [5]) to simulate human annotators by generating preference data.</p>
<p>Building on RLHF's success, researchers have explored reinforcement learning with feedback to enhance code generation in LLMs.Unlike RLHF, which relies on human feedback, this approach employs compilers or interpreters to automatically provide feedback on code samples through code execution on unit test cases, catalyzing the advancement of this research domain.CodeRL [125] introduced an actor-critic reinforcement learning framework for code generation.In this setup, the language model serves as the actor-network, while a token-level functional correctness reward predictor acts as the critic.Generated code is assessed through unit test signals from a compiler, which can indicate compiler errors, runtime errors, unit test failures, or passes.CompCoder [229] enhances code compilability by employing compiler feedback, including language model finetuning, compilability reinforcement, and compilability discrimination strategies.Subsequently, PPOCoder [204] integrates pre-trained code model CodeT5 [234] with Proximal Policy Optimization (PPO) [198].This integration not only utilizes execution (i.e., compilers or interpreters) feedback to assess syntactic and functional correctness but also incorporates a reward function that evaluates the syntactic and semantic congruence between abstract syntax tree (AST) sub-trees and data flow graph (DFG) edges in the generated code against the ground truth.Additionally, the framework applies a KL-divergence penalty to maintain fidelity between the actively learned policy and the referenced pre-trained model, enhancing the optimization process.More recently, RLTF [146] has proposed an online reinforcement learning framework that provides fine-grained feedback based on compiler error information and location, along with adaptive feedback that considers the ratio of passed test cases.</p>
<p>Despite these successes, reinforcement learning algorithms face inherent limitations such as inefficiency, instability, extensive resource requirements, and complex hyperparameter tuning, which can impede the performance and scalability of LLMs.To overcome these challenges, recent studies have introduced various variants of RL methods that do not rely on PPO, including DPO [188], RRHF [260], and sDPO [116].In essence, these methods aim to maximize the likelihood between the logarithm of conditional probabilities of preferred and rejected responses, which may be produced by LLMs with varying capabilities.Inspired by RRHF [260], PanGu-Coder 2 [201] leverages a novel framework, Reinforcement Learning via Rank Responses to align Test &amp; Teacher Feedback (RRTF), significantly enhancing code generation capabilities, as evidenced by pass@1 of 62.20% on the HumanEval benchmark.</p>
<p>Taking a step forward, the integration of more non-differentiable code features, such as coding style [41,158] and readability [32], into the reinforcement learning feedback for LLM-based code generation, presents an exciting avenue for future research.</p>
<p>Prompting Engineering</p>
<p>Large-scale language models (LLMs) such as GPT-3 and its successors have been trained on largescale data corpora, endowing them with substantial world knowledge [31,173,237].Despite this, crafting an effective prompt to harness the full potential of LLMs remains a long-standing challenge [147].Recent advancements in prompting engineering have expanded the capabilities of LLMs, enabling more sophisticated task completion and enhancing both reliability and performance.Notable techniques include Chain-of-Thought (CoT) [239], Self-Consistency [230], Tree-of-Thought (ToT) [253], Reasoning via Planning (RAP) [83], ReAct [254], Self-Refine [156], Reflexion [202], and LATS [280].</p>
<p>Prompting engineering is particularly advantageous as it bypasses the need for additional training and can significantly elevate performance.Consequently, numerous studies have leveraged this technique for iterative and self-improving (refining) code generation within proprietary LLMs such as ChatGPT and GPT-4.Figure 7 illustrates the general pipeline for self-improving code generation with LLMs.For instance, Self-Debugging [47] involves prompting an LLM to iteratively refine a predicted program by utilizing feedback composed of code explanations combined with execution results, which assists in identifying and rectifying errors.When unit tests are unavailable, this feedback can rely solely on code explanations.In parallel, SelfEvolve [110] employs a two-stage process where LLMs first generate domain-specific knowledge for a problem, followed by a trial code.This code is then iteratively refined through interactive prompting and execution feedback.An empirical investigation by [170] provides a comprehensive analysis of the self-repairing capabilities for code generation in models like Code Llama, GPT-3.5, and GPT-4, using problem sets from HumanEval and APPS.This study yields a series of insightful observations and findings, shedding  (Optional) Fig. 7.An illustration of the self-improving code generation pipeline using prompts for large language models (LLMs).This process incorporates iterative self-refinement by integrating execution outcomes and includes an optional self-reflection mechanism to enhance generation quality.</p>
<p>light on the self-refinement effectiveness of these LLMs.Moreover, Reflexion [202] introduces a general approach for code generation wherein LLM-powered agents engage in verbal self-reflection on task feedback signals, storing these reflections in an episodic memory buffer to inform and improve decision-making in subsequent interactions.LATS [280] adopts a novel strategy, utilizing LLMs as agents, value functions, and optimizers.It enhances decision-making by meticulously constructing trajectories through Monte Carlo Tree Search (MCTS) algorithms, integrating external feedback, and learning from experience.This approach has demonstrated remarkable results in code generation, achieving a pass@1 of 94.4% on the HumanEval benchmark with GPT-4.</p>
<p>Distinct from the aforementioned methods, CodeT [42] and LEVER [166] prompt LLMs to generate numerous code samples, which are then re-ranked based on execution outcomes to select the optimal solution.Notably, these approaches do not incorporate a self-refinement step to further improve code generation.</p>
<p>Repository Level &amp; Long Context</p>
<p>In contemporary software engineering practices, modifications to a code repository are widespread and encompass a range of activities, including package migration, temporary code edits, and the resolution of GitHub issues.While large language models (LLMs) showcase impressive prowess in function-level code generation, they often falter when grappling with the broader context inherent to a repository, such as import dependencies, parent classes, and files bearing similar names.These deficiencies result in suboptimal performance in repository-level code generation, as identified in recent studies [205,206].The challenges faced by LLMs in this domain are primarily due to the following factors:</p>
<p>• Code repositories typically contain intricate interdependencies scattered across various files, including shared utilities, configurations, and cross-API invocations, which arise from modular design principles [21,266].• Repositories are characterized by their unique structures, naming conventions, and coding styles, which are essential for maintaining clarity and facilitating ongoing maintenance [41].• The vast context of an entire repository often exceeds the context length limitations of LLMs, thus hindering their ability to integrate comprehensive contextual information [21].</p>
<p>• LLMs may not have been adequately trained on extensive sets of repository data, such as proprietary software or projects that are still in development [205].</p>
<p>Given that the scope of a typical software repository encompasses hundreds of thousands of tokens, it is imperative to enhance the capacity of LLMs to handle extensive contexts when they are employed for repository-level code generation.Fortunately, recent advancements in positional encoding techniques, such as ALiBi [183] and RoPE [209], have shown promise in improving the Transformer's ability to generalize from shorter training sequences to longer inference sequences [272].This progress addresses the third challenge mentioned above to a certain degree, thereby enabling better contextualization of coding activities within full repositories.</p>
<p>To further refine LLMs for repository-level code completion, several innovative approaches have been introduced.RepoCoder [266] leverages a similarity-based retrieval system within an iterative retrieval-generation paradigm to enrich the context and enhance code completion quality.In a similar vein, CoCoMIC [64] employs a cross-file context finder named CCFINDER to pinpoint and retrieve the most relevant cross-file contexts within a repository.RepoHyper [181] introduces a semantic graph structure, termed RSG, to encapsulate the expansive context of code repositories and uses an "Expand and Refine" retrieval method to obtain relevant code snippets.Moreover, a framework known as RLPG [206] has been proposed to generate repository-level prompts that integrate the repository's structure with the relevant context across all files.However, the constant reliance on retrieval mechanisms has raised concerns regarding efficiency and robustness, as some retrieved contexts may prove unhelpful or harmful.In response, Repoformer [244] introduces a selective Retrieval-Augmented Generation (RAG) framework that judiciously bypasses retrieval when it is deemed redundant.This approach incorporates a self-supervised learning strategy that equips a code LLM with the ability to perform a self-assessment on the utility of retrieval for enhancing the quality of its output, thereby effectively utilizing potentially noisy retrieved contexts.</p>
<p>Additionally, RepoFusion [205] has been developed to train models to combine multiple relevant contexts from a repository, aiming to produce more precise and context-aware code completions.In a novel approach, Microsoft's CodePlan [21] frames repository-level coding tasks as a planning problem, generating a multi-step chain of edits (plan) where each step involves invoking an LLM on a specific code location, considering context from the entire repository, preceding code modifications, and task-specific instructions.</p>
<p>Advancing the state-of-the-art, [265] tackles the formidable challenge of NL2Repo, an endeavor that seeks to create a complete code repository from natural language requirements.To address this complex task, they introduce the CodeS framework, which strategically breaks down NL2Repo into a series of manageable sub-tasks using a multi-layer sketch approach.The CodeS framework comprises three distinct modules: 1) RepoSketcher, for creating a directory structure of the repository based on given requirements; 2) FileSketcher, for sketching out each file within that structure; and 3) SketchFiller, for fleshing out the specifics of each function within the file sketches [265].</p>
<p>Accordingly, a surge of benchmarks tailored for repository-level code generation has emerged, such as RepoEval [266], Stack-Repo [205], Repobench [150], EvoCodeBench [130], SWE-bench [111], CrossCodeEval [63], and SketchEval [265].The detailed statistics and comparisons of these benchmarks are presented in Table 3.</p>
<p>Despite the progress made by these methods in repository-level code generation, significant challenges remain to be addressed.Programming developers are often required to invest considerable time in editing and debugging [24,27,163,205,220].However, the advent of LLM-powered coding agents, such as AutoCodeRover [270], SWE-Agent [112], and OpenDevin [172], has demonstrated their potential to tackle complex problems, paving the way for future exploration in this field (for more details, see Section 4.9).Fig. 8.A workflow illustration of the Retrieval-Augmented Code Generation (RACG).Upon receiving a (instruction), the retriever selects the relevant contexts from a large-scale vector database.Subsequently, the retrieved contexts are merged with the query, and this combined input is fed into the generator (LLM) to produce the target code solution.</p>
<p>Retrieval Augmented</p>
<p>Large Language Models (LLMs) have exhibited impressive capabilities but are hindered by several critical issues such as hallucination [139,269], obsolescence of knowledge [104], and nontransparent [30], untraceable reasoning processes [72,96,239,282].While techniques like instructiontuning (see Section 4.4) and reinforcement learning with feedback (see Section 4.5) mitigate these issues, they also introduce new challenges, such as catastrophic forgetting and the requirement for substantial computational resources during training [81,174].</p>
<p>Recently, Retrieval-Augmented Generation (RAG) has emerged as an innovative approach to overcoming these limitations by integrating knowledge from external databases.Formally defined, RAG denotes a model that, in response to queries, initially sources relevant information from an extensive corpus of documents, and then leverages this retrieved information in conjunction with the original query to enhance the response's quality and accuracy, especially for knowledgeintensive tasks.The RAG framework typically consists of a vector database, a retriever, a re-ranker, and a generator.It is commonly implemented using tools such as LangChain12 and LLamaIndex 13 .By performing continuous knowledge updates of the database and the incorporation of domainspecific data, RAG circumvents the need for re-training LLMs from scratch [72].Consequently, RAG has substantially advanced LLM performance across a variety of tasks [44,129].</p>
<p>Due to the nature of code, code LLMs are also susceptible to the aforementioned issues that affect general-purpose LLMs.For instance, they may exhibit a hallucination phenomenon when instructions fall outside the scope of their training data or necessitate the latest programming packages.Given the dynamic nature of publicly available source-code libraries like PyTorch, which undergo frequent expansion and updates, deprecated calling methods can become a significant challenge.If Code LLMs are not updated in tandem with the latest functions and APIs, this can introduce potential errors and safety risks.Retrieval-Augmented Code Generation (RACG) stands as a promising solution to these concerns.A workflow illustration of the RACG is depicted in Figure 8.</p>
<p>Despite its potential, the adoption of RAG for code generation remains limited.Drawing inspiration from the common practice among programmers of referencing related code snippets, [149] introduced a novel retrieval-augmented mechanism with graph neural networks (GNNs), termed HGNN, which unites the advantages of similar examples retrieval with the generalization capabilities of generative models for code summarization, which is the reverse process of code generation.[177] pioneered a retrieval augmented framework named REDCODER for code generation by retrieving and integrating relevant code snippets from a source-code database, thereby providing supplementary context for the generation process.Subsequently, a retrieval-augmented code completion framework termed ReACC [152] is proposed to leverage both lexical copying and semantic referencing of related code, achieving state-of-the-art performance on the CodeXGLUE benchmark [153].In the spirit of how programmers often consult textual resources such as code manuals and documentation to comprehend functionalities, DocPrompting [283] explicitly utilizes code documentation by retrieving the relevant documentation pieces based on a natural language query and then generating the target code by blending the query with the retrieved information.</p>
<p>More recently, RepoCoder [266], an iterative retrieval-generation framework, is proposed for enhancing repository-level code completion by effectively utilizing code analogies across different files within a repository to inform and improve code suggestions.Furthermore, breaking away from reliance on a singular source of retrieval, [208] developed a multi-faceted "knowledge soup" that integrates web searches, documentation, execution feedback, and evolved code snippets.Then, it incorporates an active retrieval strategy that iteratively refines the query and enriches the knowledge soup, expanding the scope of information available for code generation.</p>
<p>Despite these advancements, several limitations in retrieval-augmented code generation warrant further exploration: 1) the quality of the retrieved information significantly impacts overall performance; 2) the effective integration of retrieved code information with the query needs optimization; 3) an over-reliance on retrieved information may lead to inadequate responses that fail to address the query's intent; 4) additional retrieved information necessitates larger context windows for the LLM, resulting in increased computational demands.</p>
<p>Autonomous Coding Agents</p>
<p>The advent of large language models (LLMs) has marked the beginning of a new era of potential pathways toward artificial general intelligence (AGI), capturing significant attention in both academia and industry [98,225,241,246].A rapidly expanding array of applications for LLM-based autonomous agents, including AutoGPT [2], AgentGPT [1], BabyAGI [3], and AutoGen [245], underlines the promise of this technology.</p>
<p>LLM-powered autonomous agents are systems endowed with sophisticated reasoning abilities, leveraging an LLM as a central computational engine or controller.This allows them to formulate and execute problem-solving plans through a series of tool-enabled functions or API calls.Moreover, these agents are designed to function within a shared environment where they can communicate and engage in cooperative, competitive, or negotiating interactions [94,225,245].The typical architecture of such an agent encompasses an LLM-based Agent, a memory module, a planning component, and a tool utilization module, as depicted in Figure 9.</p>
<p>In the realm of automated code generation, LLM-powered autonomous agents have demonstrated remarkable proficiency.For instance, AgentCoder [94] achieved a groundbreaking pass@1 of 96.3% on the HumanEval benchmark, forwarding a step closer to the future of automated software development [100].The innovative meta-programming framework termed MetaGPT [90] integrates human workflow efficiencies into LLM-based multi-agent collaboration.Furthermore, Fig. 9.The general architecture of an LLM-powered autonomous agent system, adapted from [241].Planning: The agent decomposes large tasks into smaller, manageable sub-goals or engages in self-criticism and selfreflection on past actions to learn from mistakes and improve future performance.Memory: This component enables the agent to store and retrieve past information.Tools: The agent is trained to invoke external functions or APIs.Action: The agent executes actions, with or without the use of tools, to interact with the environment.The gray dashed lines represent the data flow within the system.</p>
<p>[94] introduces AgentCoder, a multi-agent framework composed of three specialized agents, each with distinct roles and capabilities.These roles include a programmer agent responsible for code generation, a test designer agent tasked with generating unit test cases, and a test executor agent that executes the code and provides feedback.This division of labor within AgentCoder promotes more efficient and effective code generation.CodeAct [228] distinguishes itself by utilizing executable Python code to consolidate LLM agent actions within a unified action space, in contrast to the generation of JSON or textual formats.Additionally, AutoCodeRover [270] is proposed to autonomously resolve GitHub issues for program enhancement.</p>
<p>To address the complexity of tasks within software engineering, two innovative autonomous AI software engineers Devin14 [56] and OpenDevin15 [172], have been released and rapidly garnered considerable interest within the software engineering (SE) and artificial general intelligence (AGI) community.Subsequently, an autonomous system, SWE-agent [112], leverages a language model to interact with a computer to address software engineering tasks, successfully resolving 12.5% of issues on the SWE-bench benchmark [111].L2MAC [88] has been introduced as the first practical, LLM-based, multi-agent, general-purpose stored-program automatic computer that utilizes a von Neumann architecture, designed specifically for the generation of long and consistent outputs.At the time of writing this survey, OpenDevin has enhanced CodeAct with bash command-based tools, leading to the release of OpenDevin CodeAct 1.0 [249], which sets a new state-of-the-art performance on the SWE-Bench Lite benchmark [111].</p>
<p>Despite these remarkable advancements, the journey toward fully realized AI software engineers employing LLM-powered autonomous agents is far from complete [225,246].Critical aspects such as prompt design, context length, agent count, and toolsets call for further refinement and optimization, especially as problem complexities escalate [100].Despite the impressive capabilities of large language models (LLMs), they exhibit a range of behaviors that are both beneficial and potentially risky.These behaviors can enhance performance across various downstream tasks but may also introduce reliability and trustworthiness concerns in LLM deployment [39,45,251].Consequently, it is imperative to develop precise evaluation approaches to discern the qualitative and quantitive differences between models, thereby encouraging further advancements in LLM capabilities.</p>
<p>Evaluation</p>
<p>Evaluation strategies for LLMs in code generation mirror those for general-purpose LLMs and can be divided into three principal categories: metrics-based, human-centered, and LLM-based approaches.Detailed benchmarks for these evaluation strategies are presented in Section 4.1.3and summarized in Table 3. Subsequent subsections will provide a thorough analysis of each approach.4.10.1 Metrics.The pursuit of effective and reliable automatic evaluation metrics for generated content is a long-standing challenge within the field of natural language processing (NLP) [46,140,175].At the early stage, most works directly leverage token-matchingbased metrics, such as Exact Match, BLEU [175], ROUGE [140], and METEOR [22], which are prevalent in text generation of NLP, to assess the quality of code generation.</p>
<p>While these metrics offer a rapid and costeffective approach for assessing the quality of generated code, they often fall short of capturing the syntactical and functional correctness, as well as the semantic features of the code.To eliminate this limitation, CodeBLEU [191] was introduced, enhancing the traditional BLEU metric [175] by incorporating syntactic information through abstract syntax trees (AST) and semantic understanding via data-flow graph (DFG).Despite these improvements, the metric does not fully resolve issues pertaining to execution errors or discrepancies in the execution results of the generated code.In light of these challenges, execution-based metrics have gained prominence for evaluating code generation, including pass@k [45], n@k [136], test case average [85], execution accuracy [190], and pass@t [170].In particular, the pass@k, serving as a principal evaluation metric, assesses the probability that at least one out of  code samples generated by a model will pass all unit tests.An unbiased estimator for pass@k introduced by [45] is defined as:
pass@k E task 1 − 𝑛−𝑐 𝑘 𝑛 𝑘 (19)
where  is the total number of sampled candidate code solutions,  is the number of randomly selected code solutions from these candidates for each programming problem, with  ≥ , and  is the count of correct samples within the  selected.Tables 6 and 7 illustrate the performance of contemporary large language models (LLMs) for code generation, measured by the pass@k metric across different values of  ∈ {1, 10, 100} on the HumanEval MBPP benchmarks, respectively.Nevertheless, these execution-based methods are heavily dependent on the quality of unit tests and are limited to evaluating executable code [264].Consequently, when unit tests are unavailable, token-matching-based metrics are often employed as an alternative for evaluation.Furthermore, in scenarios lacking a ground truth label, unsupervised metrics such as perplexity (PPL) [105] can serve as evaluative tools.Perplexity quantifies an LLM's uncertainty in predicting new content, thus providing an indirect measure of the model's generalization capabilities and the quality of the generated code.</p>
<p>Taken together, while the aforementioned methods primarily focus on the functional correctness of code, they do not provide a holistic evaluation that encompasses other critical dimensions such as code vulnerability [165], maintainability [14], readability [32], complexity and efficiency [180], stylistic consistency [158], and execution stability [187].A comprehensive evaluation framework that integrates these aspects remains an open area for future research and development in the field of code generation assessment.4.10.2Human Evaluation.Given the intrinsic characteristics of code, the aforementioned automatic evaluation metrics are inherently limited in their capacity to fully assess code quality.For instance, metrics specifically designed to measure code style consistency are challenging to develop and often fail to capture this aspect adequately [41].When it comes to repository-level code generation, the evaluation of overall code quality is substantially complicated due to the larger scale of the task, which involves cross-file designs and intricate internal as well as external dependencies, as discussed by [21,205].</p>
<p>To overcome these challenges, conducting human evaluations becomes necessary, as it yields relatively robust and reliable results.Human assessments also offer greater adaptability across various tasks, enabling the simplification of complex and multi-step evaluations.Moreover, human evaluations are essential for demonstrating the effectiveness of certain token-matching-based metrics, such as CodeBLEU [191].These studies typically conduct experiments to evaluate the correlation coefficient between proposed metrics and quality scores assigned by actual users, demonstrating their superiority over existing metrics.</p>
<p>Moreover, in an effort to better align large language models (LLMs) with human preferences and intentions, InstructGPT [173] employs human-written prompts and demonstrations, and model output ranking in the fine-tuning of LLMs using reinforcement learning from human feedback (RLHF).Although similar alignment learning techniques have been applied to code generation, the feedback in this domain typically comes from a compiler or interpreter, which offers execution feedback, rather than from human evaluators.Notable examples include CodeRL [125], PPOCoder Replit.Replit is a multifunctional platform that caters to a diverse array of software development needs.As a complimentary online IDE, it facilitates code collaboration, and cloud services, and fosters a thriving developer community.Replit also enables users to compile and execute code in more than 50 programming languages directly within a web browser, eliminating the need for local software installations.</p>
<p>CHALLENGES &amp; OPPORTUNITIES</p>
<p>According to our investigations, the LLMs have revolutionized the paradigm of code generation and achieved remarkable performance.Despite this promising progress, there are still numerous challenges that need to be addressed.These challenges are mainly caused by the gap between academia and practical development.For example, in academia, the HumanEval benchmark has been established as a de facto standard for evaluating the coding proficiency of LLMs.However, many works have illustrated the evaluation of HumanEval can't reflect the scenario of practical development [63,67,111,145].In contrast, these serious challenges offer substantial opportunities for further research and applications.In this section, we pinpoint critical challenges and identify promising opportunities, aiming to bridge the research-practicality divide.</p>
<p>Enhancing complex code generation at repository and software scale.In practical development scenarios, it often involves a large number of complex programming problems of varying difficulty levels.While LLMs have shown proficiency in generating function-level code snippets, these models often struggle with more complex, unseen programming problems, repository-and software-level problems that are commonplace in real-world software development.To this end, it requires strong problem-solving skills in LLM beyond simply functional-level code generation.For example, AlphaCode [136] achieved an average ranking in the top 54.3% in programming competitions where an understanding of algorithms and complex natural language is required to solve competitive programming problems.[111] argues that existing LLMs can't resolve real-world GitHub issues well since the best-performing model, Claude 2, is able to solve a mere 1.96% of the issues.The reason for poor performance is mainly attributed to the weak reasoning capabilities [95], complex internal-and external-dependencies [21], and context length limitation of LLMs [21].Therefore, the pursuit of models that can handle more complex, repository-and softwarelevel code generation opens up new avenues for automation in software development and makes programming more productive and accessible.</p>
<p>Innovating model architectures tuned to code structures.Due to their scalability and effectiveness, Transformer-based LLM architectures have become dominant in solving code generation task.Nevertheless, they might not be optimally designed to capture the inherent structure and syntax of programming languages (PLs) [76,77,120,155].Code has a highly structured nature, with a syntax that is more rigid than natural language.This presents a unique challenge for LLMs, which are often derived from models that were originally designed for natural language processing (NLP).The development of novel model architectures that inherently understand and integrate the structural properties of code represents a significant opportunity to improve code generation and comprehension.Innovations such as tree-based neural networks [162], which mirror the abstract syntax tree (AST) representation of code, can offer a more natural way for models to learn and generate programming languages.Additionally, leveraging techniques from the compiler theory, such as intermediate representations (IR) [137], could enable models to operate on a more abstract and generalizable level, making them effective across multiple programming languages [179].By exploring architectures beyond the traditional sequential models, researchers can unlock new potentials in code generation.</p>
<p>Curating high-quality code data for pre-training and fine-tuning of LLMs.The efficacy of LLMs largely depends on the quality and diversity of code datasets used during pre-training and fine-tuning phases [119,242,281].Currently, there is a scarcity of large, high-quality datasets that encompass a wide range of programming tasks, styles, and languages.This limitation constrains the ability of LLMs to generalize across unseen programming tasks, different coding environments, and real-world software development scenarios.The development of more sophisticated data acquisition techniques, such as automated code repositories mining [142], advanced filtering algorithms, and code data synthesis [148] (see Section 4.2), can lead to the creation of richer datasets.Collaborations with industry partners (e.g., GitHub) could also facilitate access to proprietary codebases, thereby enhancing the practical relevance of the training material.Furthermore, the adoption of open-source models for dataset sharing can accelerate the collective effort to improve the breadth and depth of code data available for LLM research.</p>
<p>Developing comprehensive benchmarks and metrics for coding proficiency evaluation in LLMs.Current benchmarks like HumanEval may not capture the full spectrum of coding skills required for practical software development [167].Additionally, metrics often focus on syntactic correctness or functional accuracy, neglecting aspects such as code efficiency [180], style [41], readability [32], or maintainability [14].The design of comprehensive benchmarks that simulate real-world software development challenges could provide a more accurate assessment of LLMs' coding capabilities.These benchmarks should include diverse programming tasks of varying difficulty levels, such as debugging [279], refactoring [203], and optimization [101], and should be complemented by metrics that evaluate qualitative aspects of code.The establishment of community-driven benchmarking platforms could facilitate continuous evaluation and comparison of LLMs for code generation across the industry and academia.</p>
<p>Support for low-resource, low-level, and domain-specific programming languages.LLMs are predominantly trained in popular high-level programming languages, leaving low-resource, lowlevel, and domain-specific languages underrepresented.This lack of focus restricts the applicability of LLMs in certain specialized fields and systems programming [216].Intensifying research on transfer learning and meta-learning approaches may enable LLMs to leverage knowledge from high-resource languages to enhance their performance on less common ones [35,43].Additionally, partnerships with domain experts can guide the creation of targeted datasets and fine-tuning strategies to better serve niche markets.The development of LLMs with a capacity for multilingual code generation also presents a significant opportunity for broadening the scope of applications.</p>
<p>Continuous learning for LLMs to keep pace with evolving coding knowledge.The software development landscape is continuously evolving, with new languages, frameworks, and best practices emerging regularly.LLMs risk becoming outdated if they cannot adapt to these changes and incorporate the latest programming knowledge [104,227].While retrieval augmented code generation mitigates these issues, the performance is limited by the quality of the retrieval context While retrieval-augmented code generation offers a partial solution to these issues, its effectiveness is inherently constrained by the quality of retrieved context.[152,266,283].Therefore, establishing mechanisms for continuous learning and updating of LLMs can help maintain their relevance over time.This could involve real-time monitoring of code repositories to identify trends and innovations, as well as the creation of incremental learning systems that can assimilate new information without forgetting previously acquired knowledge.Engaging the LLMs in active learning scenarios where they interact with human developers may also foster ongoing knowledge acquisition.</p>
<p>Ensuring code safety and aligning LLM outputs with human coding preferences.Ensuring the safety and security of code generated by LLMs is a paramount concern, as is their ability to align with human preferences and ethical standards.Current models may inadvertently introduce vulnerabilities or generate code that does not adhere to desired norms [45,252].Research into the integration of formal verification tools within the LLM pipeline can enhance the safety of the produced code.Additionally, developing frameworks for alignment learning that capture and reflect human ethical preferences can ensure that the code generation process aligns with societal values [173,184].Transparent and explainable AI methodologies can also contribute to building trust in the LLM-generated code by making the decision-making process more accessible to developers.</p>
<p>Fig. 1 .
1
Fig. 1.A chronological overview of large language models (LLMs) for code generation in recent years.The timeline was established mainly according to the release date.The models with publicly available model checkpoints are highlighted in green color.ACM Trans.Softw.Eng.Methodol., Vol. 1, No. 1, Article 1. Publication date: September 2024.</p>
<p>Fig. 3 .
3
Fig.3.Taxonomy of large language models (LLMs) for code generation.</p>
<p>Fig. 4 .
4
Fig. 4. A diagram depicting the standard data preprocessing workflow utilized in the pre-training phase of large language models (LLMs) for code generation.</p>
<p>conducted a thorough empirical examination of parameter-efficient instruction tuning for code comprehension and generation tasks.This study ACM Trans.Softw.Eng.Methodol., Vol. 1, No. 1, Article 1. Publication date: September 2024.</p>
<p>Fig. 6 .
6
Fig. 6.An illustration of full parameter fine-tuning (FFT) and parameter-efficient fine-tuning (PEFT) methods.(a)refers to the Full Fine-tuning method, which updates all parameters of the base model during fine-tuning.(b) stands for the Specification-based PEFT method that conditionally fine-tunes a small subset of the model parameters while freezing the rest of the model, e.g.BitFit[262].(c) represents the Addition-based PEFT method that fine-tunes the incremental parameters introduced into the base model or input, e.g.Adapter[92], Prefix-tuning[134], and Prompt-tuning[128].(d) symbolizes the Reparameterization-based method which reparameterizes existing model parameters by low-rank transformation, e.g.LoRA[93], QLoRA[60], and AdaLoRA[267].</p>
<p>ACM Trans.Softw.Eng.Methodol., Vol. 1, No. 1, Article 1. Publication date: September 2024.</p>
<p>'a', 'b', 'c', 'a', 'd']) == ['a', 'b', 'c', 'd'] […] does not work as expected because it uses the built-in <code>set()</code> function in Python, which does not maintain the order of elements.[…]</p>
<p>Table 1 .
1
[123]tatistics of some commonly-used pre-training datasets for large language models (LLMs) aimed at code generation.The column labeled '#PL' indicates the number of programming languages included in each dataset.It should be noted that in the CodeSearchNet[99]dataset, each file represents a function, and for the Pile[70]and ROOTS[123]datasets, only the code components are considered.
DatasetSize (GB) Files (M) #PLDateLinkCodeSearchNet [99]206.562022-01 https://huggingface.co/datasets/code_search_netGoogle BigQuery[86] ---2016-06 github-on-bigquery-analyze-all-the-open-source-codeThe Pile [70]9519-2022-01 https://huggingface.co/datasets/EleutherAI/pileCodeParrot [219]1802212021-08 https://huggingface.co/datasets/transformersbook/codeparrotGitHub Code[219]1,024115322022-02 https://huggingface.co/datasets/codeparrot/github-codeROOTS [123]16315132023-03 https://huggingface.co/bigscience-dataThe Stack [118]3,136317302022-10 https://huggingface.co/datasets/bigcode/the-stackThe Stack v2 [151]32K3K619 2024-04 https://huggingface.co/datasets/bigcode/the-stack-v2</p>
<p>Table 2
2DatasetSize #PLDateLinkCodeAlpaca-20K [40]20k -2023-03 https://huggingface.co/datasets/sahil2801/CodeAlpaca-20kCommitPackFT
. The statistics of several representative datasets used in instruction-tuning large language models (LLMs) for code generation.The column labeled '#PL' indicates the number of programming languages encompassed by each dataset.</p>
<p>Table 3 .
3
The detailed statistics of commonly-used benchmarks used in evaluating large language models (LLMs) for code generation.The column labeled '#PL' indicates the number of programming languages included in each dataset.For the sake of brevity, we list the programming languages (PLs) for benchmarks that support fewer than or include five PLs.For benchmarks with six or more PLs, we provide only a numerical count of the PLs supported.
ScenarioBenchmarkSize#PLDateLinkHumanEval [45]164Python2021-07 https://huggingface.co/datasets/openai_humanevalHumanEval+ [145]164Python2023-05 https://huggingface.co/datasets/evalplus/humanevalplusHumanEvalPack [164]16462023-08 https://huggingface.co/datasets/bigcode/humanevalpackMBPP [16]974Python2021-08 https://huggingface.co/datasets/mbppMBPP+ [145]378Python2023-05 https://huggingface.co/datasets/evalplus/mbppplusGeneralCoNaLa [255] Spider [258]596.88K Python 8,034 SQL2018-05 https://huggingface.co/datasets/neulab/conala 2018-09 https://huggingface.co/datasets/xlangai/spiderCONCODE [102]104KJava2018-08 https://huggingface.co/datasets/AhmedSSoliman/CONCODODEX [236]945Python2022-12 https://huggingface.co/datasets/neulab/odexCoderEval [257]460Python, Java2023-02 https://github.com/CoderEval/CoderEvalReCode [226]1,138Python2022-12 https://github.com/amazon-science/recodeStudentEval [18]1,749Python2023-06 https://huggingface.co/datasets/wellesley-easel/StudentEvalCompetitionsAPPS [85] CodeContests [136]10,000 13,610Python C++, Python, Java2021-05 https://huggingface.co/datasets/codeparrot/apps 2022-02 https://huggingface.co/datasets/deepmind/code_contestsDSP [38]1,119Python2022-01 https://github.com/microsoft/DataScienceProblemsData ScienceDS-1000 [122]1,000Python2022-11 https://huggingface.co/datasets/xlangai/DS-1000ExeDS [97]534Python2022-11 https://github.com/Jun-jie-Huang/ExeDSMBXP [15]12.4K132022-10 https://huggingface.co/datasets/mxeval/mbxpMultilingual HumanEval [15] 1.9K122022-10 https://huggingface.co/datasets/mxeval/multi-humanevalMultilingualPython, C++,HumanEval-X [275]820Java, JavaScript,2023-03 https://huggingface.co/datasets/THUDM/humaneval-xGoMultiPL-E [36]161182022-08 https://huggingface.co/datasets/nuprl/MultiPL-ExCodeEval [115]5.5M112023-03 https://github.com/ntunlp/xCodeEvalMathQA-X [15]5.6KPython, Java, JavaScript2022-10 https://huggingface.co/datasets/mxeval/mathqa-xReasoningMathQA-Python [16]23,914Python2021-08 https://github.com/google-research/google-researchGSM8K [53]8.5KPython2021-10 https://huggingface.co/datasets/gsm8kGSM-HARD [71]1.32KPython2022-11 https://huggingface.co/datasets/reasoning-machines/gsm-hardRepoEval [266]3,573Python, Java2023-03 https://paperswithcode.com/dataset/repoevalStack-Repo [205]200Java2023-06 https://huggingface.co/datasets/RepoFusion/Stack-RepoRepobench [150]27kPython, Java2023-01 https://github.com/Leolty/repobenchRepositoryEvoCodeBench [130]275Python2024-03 https://huggingface.co/datasets/LJ0815/EvoCodeBenchSWE-bench [111]2,294Python2023-10 https://huggingface.co/datasets/princeton-nlp/SWE-benchCrossCodeEval [63]10KPython, Java, TypeScript, C#2023-10 https://github.com/amazon-science/ccevalSketchEval [265]20,355Python2024-03 https://github.com/nl2code/codesRepository• RepoEval [266]: RepoEval enables the evaluation of repository-level code completion. It canoffer different levels of granularity and improved evaluation accuracy through the use of unittests.• Stack-Repo</p>
<p>Table 4 .
4
The overview of large language models (LLMs) with encoder-decoder architectures for code generation.
ArchitectureModelInstitutionSizeVocabularyContext WindowDateOpen SourcePyMT5[52]Microsoft374M50K1024+1024 2020-10PLBART[6]UCLA140M50K1024+1024 2021-03CodeT5 [234]Salesforce60M, 220M, 770M32K512+2562021-09JuPyT5[38]Microsoft350M50K1024+1024 2022-01Encoder-DecoderAlphaCode[136] DeepMind CodeRL[125] Salesforce284M, 1.1B, 2.8B, 8.7B, 41.1B 770M8K 32K1536+768 512+2562022-02 2022-06ERNIE-Code[37] Baidu560M250K1024+1024 2022-12PPOCoder[204]Virginia Tech 770M32K512+2562023-01CodeT5+[232]Salesforce220M, 770M, 2B, 6B, 16B50K2048+2048 2023-05CodeFusion[207] Microsoft75M32k128+1282023-10AST-T5[73]UC Berkeley 226M32k512+200/300 2024-01</p>
<p>Table 5 .
5
The overview of large language models (LLMs) with decoder-only architectures for code generation.
Architecture ModelInstitutionSizeVocabularyContext WindowDateOpen SourceGPT-C [210]Microsoft366M60K10242020-05CodeGPT [153]Microsoft124M50K10242021-02GPT-Neo[29]EleutherAI125M, 1.3B, 2.7B50k20482021-03GPT-J [223]EleutherAI6B50k20482021-0512M, 25M, 42M,Codex [45]OpenAI85M, 300M, 679M,-40962021-072.5B, 12BCodeParrot [219]Hugging Face110M, 1.5B33k10242021-11PolyCoder [251]CMU160M, 400M, 2.7B50k20482022-02CodeGen [169]Salesforce350M, 2.7B, 6.1B, 16.1B51k20482022-03GPT-NeoX [28]EleutherAI20B50k20482022-04PaLM-Coder [49]Google8B, 62B, 540B256k20482022-04InCoder [69]Meta1.3B, 6.7B50k20492022-04PanGu-Coder [50]Huawei317M, 2.6B42k10242022-07PyCodeGPT [263]Microsoft110M32k10242022-06CodeGeeX [275]Tsinghua13B52k20482022-09BLOOM [126]BigScience176B251k-2022-11ChatGPT [171]OpenAI--16k2022-11SantaCoder [8]Hugging Face1.1B49k20482022-12LLaMA [217]Meta6.7B, 13.0B, 32.5B, 65.2B32K20482023-02Decoder-OnlyGPT-4 [5]OpenAI--32K2023-03CodeGen2 [168]Salesforce1B, 3.7B, 7B, 16B51k20482023-05replit-code [193]replit3B33k20482023-05StarCoder [132]Hugging Face15.5B49k81922023-05WizardCoder [154]Microsoft15B, 34B49k81922023-06phi-1 [75]Microsoft1.3B51k20482023-06CodeGeeX2 [275]Tsinghua6B65k81922023-07PanGu-Coder2 [201]Huawei15B42k10242023-07Llama 2 [218]Meta7B, 13B, 70B32K40962023-07OctoCoder [164]Hugging Face15.5B49k81922023-08Code Llama [196]Meta7B, 13B, 34B32k163842023-08CodeFuse [143]Ant Group350M, 13B, 34B101k40962023-09phi-1.5 [135]Microsoft1.3B51k20482023-09CodeShell [247]Peking University 7B70k81922023-10Magicoder [240]UIUC7B32k163842023-12AlphaCode 2 [10]Google DeepMind ---2023-12StableCode [182]StabilityAI3B50k163842024-01WaveCoder [259]Microsoft6.7B32k163842023-12phi-2 [161]Microsoft2.7B51k20482023-12DeepSeek-Coder [79]DeepSeek1.3B, 6.7B, 33B32k163842023-11StarCoder 2 [151]Hugging Face15B49k163842024-02Claude 3 [13]Anthropic--200K2024-03CodeGemma [54]Google2B, 7B25.6k81922024-04Code-Qwen [215]Qwen Group7B92K655362024-04Llama3 [160]Meta8B, 70B128K81922024-04StarCoder2-Instruct [261] Hugging Face15.5B49K163842024-04</p>
<p>Table 6 .
6
[45]performance comparison of LLMs for code generation on the HumanEval[45]benchmark, measured by Pass@{1, 10, 100}.For models with various sizes, we report only the largest size version of each model.</p>
<p>Table 8 .
8
[264]verview of code assistant applications powered by large language models (LLMs).The column labeled 'PLs' and 'IDEs' indicate programming languages and integrated development environments, respectively[264].
GitHub &amp; OpenAI GitHub Copilot [45] CodexInstitution Products ModelSupported FeaturesSupported PLsSupported IDEs
https://chat.openai.com
https://github.com/features/copilot
It should be noted that an LLM is not necessarily superior to a smaller language model, and emergent abilities may not manifest in all LLMs[273]. ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: September 2024.
ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: September 2024.
https://github.com
https://stackoverflow.com
https://archive.softwareheritage.org
https://platform.openai.com ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: September 2024.
For more information on prompt engineering, visit https://www.promptingguide.ai ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: September 2024.
LangChain facilitates the development of LLM-powered applications. https://www.langchain.com
LLamaIndex is a leading data framework for building LLM applications. https://www.llamaindex.ai ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: September 2024.
https://www.cognition.ai/introducing-devin
https://github.com/OpenDevin/OpenDevin ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: September 2024.
CONCLUSIONIn this survey, we provide a systematic literature review, serving as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation.A thorough introduction and analysis for data curation, the latest advances, performance evaluation, and real-world applications are illustrated.In addition, we present a historical overview of the evolution of LLMs for code generation in recent years and offer an empirical comparison using the widely recognized HumanEval and MBPP benchmarks to highlight the progressive enhancements in LLM capabilities for code generation.Critical challenges and promising opportunities regarding the gap between academia and practical development are also identified for future investigation.Furthermore, we have established a dedicated resource website to continuously document and disseminate the most recent advances in the field.We hope this survey can contribute to a comprehensive and systematic overview of LLM for code generation and promote its thriving evolution.We optimistically believe that LLM will ultimately change all aspects of coding and automatically write safe, helpful, accurate, trustworthy, and controllable code, like professional programmers, and even solve coding problems that currently cannot be solved by humans.Code LLM Embedding Model Vector Database Code Solution Code Data Chunks Open Source Stage 1: Retrieval Stage 2: Generation Algorithm: 1.If the input array...already sorted.... 5. Recursively call quicksort... Code Completions, Code Generation, Coding Questions Answering, Code Refactoring, Code Issues Fix, Unit Test Cases Generation, Code Documentation Generation Java, Python, JavaScript, TypeScript, Perl, R, PowerShell, Rust, SQL, CSS, Ruby, Julia, C#, PHP, Swift, C++,Go, HTML, JSON, SCSS, .NET, Less, T-SQL, Markdown Visual Studio, VS Code, Neovim, JetBrains IDE Zhipu AI CodeGeeX [275] CodeGeeX Code Generation, Code Translation, Code Completion, Code Interpretation, Code Bugs Fix, Comment Generation, AI Chatbot PHP, Go, C, C#, C++, Rust, Perl, CSS, Code Completion, Code Explanation, Code Translation, Code Security Identification, Code Suggestion Java, Python, TypeScript, JavaScript, C# JetBrains IDE, VS Code, AWS Cloud9, AWS Lambda Codeium Codeium [55] − Code Completion, Bug Detection, Code Suggestions, AI Chatbot, Test Type Generation, Test Plan Creation, Code Generation, Code Explanation Research and Development Knowledge Question and Answer Code Comment, Code Debug Unit Test Case Generation Java, Python PyCharm, VS Code, IntelliJ Tabnine TabNine [212] − Code Generation, Code Completion, Code Explanation, Bug Fix, Code Recommendation, Code Refactoring, Code Test Generation, Docstring Generation Code Completion, Code Editing, Code Generation, Code Explanation, Code Suggestion, Code Test Generation C#, Bash, C, CSS, C++, Java, Go,EmbeddingModel QueryRetrieved ContextCreate a quicksort algorithm in Python.Combine Prompts and ContextCreate a quick-sort algorithm in Python.Please solve the above problem based on the following context: {context} def quick_sort(arr):"""Sort a list of numbers in ascending order using the Quick-Sort algorithm""" if len(arr) == 0:ModelSize pass@k  = 1  = 10  = 100 GPT-4[5]-84.1 --GPT-3.5-Turbo[171]-76.2 --Claude-3-Opus[13]-82.9 --Claude-3-Haiku[13]-76.8 --Claude-3-Sonnet[[204], RLTF[146], and PanGu-Coder2[201].Further information on this topic is available in Section 4.5.Nonetheless, human evaluations are not without drawbacks, as they can be prone to certain issues that may compromise their accuracy and consistency.For instance, 1) personalized tastes and varying levels of expertise among human evaluators can introduce biases and inconsistencies into the evaluation process; 2) conducting comprehensive and reliable human evaluations often necessitates a substantial number of evaluators, leading to significant expenses and time-consuming; 3) the reproducibility of human evaluations is often limited, which presents challenges in extending previous evaluation outcomes or monitoring the progress of LLMs, as highlighted by[273].4.10.3LLM-as-a-Judge.The powerful instructionfollowing capabilities of large language models (LLMs) have stimulated researchers to innovatively investigate the potential of LLMbased evaluations.The LLM-as-a-Judge[274]refers to the application of advanced proprietary LLMs (e.g., GPT4, Gemini, and Claud 3) as proxies for human evaluators.This involves designing prompts with specific requirements to guide LLMs in conducting evaluations, as demonstrated by AlpacaEval[133]and MTbench[274].This method reduces reliance on human participation, thereby facilitating more efficient and scalable evaluations.Moreover, LLMs can offer insightful explanations for the assigned rating scores, thereby augmenting the interpretability of evaluations[273].Nevertheless, the use of LLM-based evaluation for code generation remains relatively underexplored compared with general-purpose LLM.A recent work[284]introduces the ICE-Score evaluation metric, which instructs LLM for code assessments.This approach attains superior correlations with functional correctness and human preferences, thereby eliminating the requirement for test oracles or references.As the capabilities of LLM continue to improve, we anticipate seeing more research in this direction.Despite their scalability and explainability, the effectiveness of LLM-based evaluation is constrained by the inherent limitations of the chosen LLM.Several studies have shown that most LLMs, including GPT-4, suffer from several issues, including position, verbosity, and self-enhancement biases, as well as restricted reasoning ability[274].Specifically, position bias refers to the tendency ACM Trans.Softw.Eng.Methodol., Vol. 1, No. 1, Article 1. Publication date: September 2024.A Survey on Large Language Models for Code Generation 1:33 of large language models (LLMs) to disproportionately favor responses that are presented in certain positions, which can skew the perceived quality of answers based on their order of presentation.Meanwhile, verbosity bias describes the inclination of LLMs to prefer lengthier responses, even when these are not necessarily of higher quality compared to more concise ones.Self-enhancement bias, on the other hand, is observed when LLMs consistently overvalue the quality of the text they generate[273,274].Moreover, due to their inherent limitations in tackling complex reasoning challenges, LLMs may not be entirely reliable as evaluators for tasks that require intensive reasoning, such as those involving mathematical problem-solving.However, these shortcomings can be partially addressed through the application of deliberate prompt engineering and fine-tuning techniques, as suggested by[274].ApplicationsCode LLMs have been integrated with development tools and platforms, such as integrated development environments (IDEs) and version control systems, improving programming efficiency substantially.In this section, we will briefly introduce several widely used applications as coding assistants.The statistics of these applications are provided in Table8.GitHub Copilot.GitHub Copilot, powered by OpenAI's Codex, is an AI pair programmer that helps you write better code faster.Copilot suggests whole lines or blocks of code as you type, based on the context provided by your existing code and comments.It's trained on a dataset that includes a significant portion of the public code available on GitHub, which enables it to understand a wide range of programming languages and coding styles.Copilot not only improves productivity but also serves as a learning tool by providing programmers with examples of how certain functions can be implemented or how specific problems can be solved.CodeGeeX.CodeGeeX stands out as a multifaceted programming assistant, proficient in code completion, comment generation, code translation, and developer interactions.Its underlying code generation LLM has been refined with extensive training on vast amounts of code data, exhibiting superior performance on benchmarks like HumanEval, HumanEval-X, and DS1000.Renowned for supporting multilingual code generation, CodeGeeX plays a pivotal role in enhancing the efficiency of code development.CodeWhisperer.Amazon's CodeWhisperer is a versatile, machine learning-driven code generator that offers on-the-fly code recommendations.Tailored to your coding patterns and comments, CodeWhisperer provides personalized suggestions that range from succinct comments to complex functions, all aimed at streamlining your coding workflow.Codeium.Codeium is an AI-accelerated coding toolkit that offers a suite of functions, including code completion, explanation, translation, search, and user chatting.Compatible with over 70 programming languages, Codeium delivers fast and cutting-edge solutions to coding challenges, simplifying the development process for its users.CodeArts Snap.Huawei's CodeArts Snap is capable of generating comprehensive function-level code from both Chinese and English descriptions.This tool not only reduces the monotony of manual coding but also efficiently generates test code, in addition to providing automatic code analysis and repair services.Tabnine.Tabnine is an AI coding assistant that empowers development teams to leverage AI for streamlining the software development lifecycle while maintaining strict standards for privacy, security, and compliance.With a focus on enhancing coding efficiency, code quality, and developer satisfaction, Tabnine offers AI-driven automation that is tailored to the needs of your team.Supporting over one million developers worldwide, Tabnine is applicable across various industries.
C , C # C++, Dart, Css, Go, Elixir, Html, Julia Haskell, Java, Javascript, Lisp, Kotlin, Lua, -C Objective, Perl, Php Pascal, R Protobuf, Python, Ruby, Scala, Rust, Swift, T S Sql, Vue Jetbrains, AgentGPT: Assemble, configure, and deploy autonomous AI Agents in your browser. Sublime Text Huawei CodeArts Snap. VSCode, Visual Studio, Colab, Jupyter, Deepnote, Notebooks, Databricks, Chrome, Vim, Neovim, Eclipse, EmacsPanGu-Coder2023201Codebase Search More than 70 languages in total</p>
<p>AutoGPT is the vision of accessible AI for everyone. </p>
<p>Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, arXiv:2404.14219Phi-3 technical report: A highly capable language model locally on your phone. 2024. 2024arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Unified pre-training for program understanding and generation. Saikat Wasi Uddin Ahmad, Baishakhi Chakraborty, Kai-Wei Ray, Chang, arXiv:2103.063332021. 2021arXiv preprint</p>
<p>Traces of Memorisation in Large Language Models for Code. Ali Al-Kaswan, Maliheh Izadi, Arie Van Deursen, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024</p>
<p>Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, arXiv:2301.03988SantaCoder: don't reach for the stars!. 2023. 2023arXiv preprint</p>
<p>Mining idioms from source code. Miltiadis Allamanis, Charles Sutton, Proceedings of the 22nd acm sigsoft international symposium on foundations of software engineering. the 22nd acm sigsoft international symposium on foundations of software engineering2014</p>
<p>Google DeepMind AlphaCode Team. 2023. AlphaCode 2. Technical Report</p>
<p>What is CodeWhisperer?. Amazon, 2022</p>
<p>MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Anthropic, The Claude 3 Model Family: Opus, Sonnet, Haiku. 2024</p>
<p>A tool-based perspective on software code maintainability metrics: a systematic literature review. Luca Ardito, Riccardo Coppola, Luca Barbato, Diego Verga, Scientific Programming. 20202020. 2020</p>
<p>Multi-lingual evaluation of code generation models. Ben Athiwaratkun, Krishna Sanjay, Zijian Gouda, Xiaopeng Wang, Yuchen Li, Ming Tian, Tan, Uddin Wasi, Shiqi Ahmad, Qing Wang, Mingyue Sun, Shang, arXiv:2210.148682022. 2022arXiv preprint</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.07732Program synthesis with large language models. 2021. 2021arXiv preprint</p>
<p>. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.064502016. 2016Layer normalization. arXiv preprint</p>
<p>Sydney Hannah Mclean Babe, Yangtian Nguyen, Arjun Zi, Molly Q Guha, Carolyn Jane Feldman, Anderson, arXiv:2306.04556[cs.LG]StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code. 2023</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023. 2023arXiv preprint</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022. 2022arXiv preprint</p>
<p>Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Shashank Ashok, Shet, arXiv:2309.12499Codeplan: Repository-level coding using llms and planning. 2023. 2023arXiv preprint</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>A methodology for controlling bias and fairness in synthetic data generation. Enrico Barbierato, Marco L Della Vedova, Daniele Tessera, Daniele Toti, Nicola Vanoli, Applied Sciences. 1246192022. 2022</p>
<p>Grounded copilot: How programmers interact with code-generating models. Shraddha Barke, Michael B James, Nadia Polikarpova, Proceedings of the ACM on Programming Languages. 72023. 2023</p>
<p>Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, arXiv:2401.02954Deepseek llm: Scaling open-source language models with longtermism. 2024. 2024arXiv preprint</p>
<p>Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback. Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin Lu, Zili Zhang, Yulei Sui, Xuanhua Shi, Hai Jin, arXiv:2403.167922024. 2024arXiv preprint</p>
<p>Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools. Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk, Idan Gazit, Queue. 202022. 2022</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, arXiv:2204.06745Gpt-neox-20b: An open-source autoregressive language model. 2022. 2022arXiv preprint</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman, 10.5281/zenodo.5297715GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. 2021If you use this software, please cite it using these metadata.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. 2021. 2021arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Learning a metric for code readability. P L Raymond, Westley R Buse, Weimer, IEEE Transactions on software engineering. 362009. 2009</p>
<p>Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang, arXiv:2404.05019Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts. 2024. 2024arXiv preprint</p>
<p>Extracting training data from large language models. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, 30th USENIX Security Symposium (USENIX Security 21. 2021</p>
<p>Knowledge Transfer from High-Resource to Low. Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg, Abhinav Jangda, Arjun Guha, arXiv:2308.0989539 Languages for Code LLMs. 2023. September 2024. 20231arXiv preprintPublication date</p>
<p>Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, arXiv:2208.08227A scalable and extensible approach to benchmarking nl2code for 18 programming languages. 2022. 2022arXiv preprint</p>
<p>ERNIE-Code: Beyond english-centric cross-lingual pretraining for programming languages. Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hua Hao Tian, Wu, arXiv:2212.067422022. 2022arXiv preprint</p>
<p>Training and evaluating a jupyter notebook data science assistant. Shubham Chandel, Colin B Clement, Guillermo Serrato, Neel Sundaresan, arXiv:2201.129012022. 2022arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems Technology. 152024. 2024</p>
<p>Code Alpaca: An Instruction-following LLaMA model for code generation. Sahil Chaudhary, 2023</p>
<p>DUETCS: Code Style Transfer through Generation and Retrieval. Binger Chen, Ziawasch Abedjan, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Codet: Code generation with generated tests. Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, Weizhu Chen, arXiv:2207.103972022. 2022arXiv preprint</p>
<p>On the transferability of pre-trained language models for low-resource programming languages. Fuxiang Chen, H Fatemeh, David Fard, Timofey Lo, Bryksin, Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension. the 30th IEEE/ACM International Conference on Program Comprehension2022</p>
<p>Benchmarking large language models in retrieval-augmented generation. Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021. 2021arXiv preprint</p>
<p>Evaluation metrics for language models. Douglas Stanley F Chen, Roni Beeferman, Rosenfeld, 1998. 1998</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023. 2023arXiv preprint</p>
<p>Tree-to-tree neural networks for program translation. Xinyun Chen, Chang Liu, Dawn Song, Advances in neural information processing systems. 312018. 2018</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242023. 2023</p>
<p>Pangu-coder: Program synthesis with function-level language modeling. Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin Li, arXiv:2207.112802022. 2022arXiv preprint</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Journal of Machine Learning Research. 252024. 2024</p>
<p>PyMT5: multi-mode translation of natural language and Python code with transformers. Colin B Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, Neel Sundaresan, arXiv:2010.031502020. 2020arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021. 2021arXiv preprint</p>
<p>Codegemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A Choquette-Choo, Heri Zhao, Jane Fine, Jeffrey Hui, Jingyue Shen, Joe Kelley, Joshua Howland, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Nam Nguyen, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Siqi Zuo, Tris Warkentin, Zhitao , CodeGemma: Open Code Models Based on Gemma. 2024. 2024</p>
<p>Codeium, Free, ultrafast Copilot alternative for Vim and Neovim. 2023</p>
<p>Introducing Devin, the first AI software engineer. Cognition. 2024</p>
<p>Inducing tree-substitution grammars. Trevor Cohn, Phil Blunsom, Sharon Goldwater, The Journal of Machine Learning Research. 112010. 2010</p>
<p>Z3: An efficient SMT solver. Leonardo De, Moura , Nikolaj Bjørner, International conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer2008</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, arXiv:2203.06904Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models. 2022. 2022arXiv preprint</p>
<p>Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Krishna Murali, Ramesh Ramanathan, Parminder Nallapati, Dan Bhatia, Roth, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Yangruibo Ding, Zijian Wang, Uddin Wasi, Murali Ahmad, Ramesh Krishna Ramanathan, Parminder Nallapati, Dan Bhatia, Bing Roth, Xiang, arXiv:2212.10007Cocomic: Code completion by jointly modeling in-file and cross-file context. 2022. 2022arXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey on in-context learning. 2022. 2022arXiv preprint</p>
<p>Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan, Zhiheng Xi, arXiv:2402.01391StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback. 2024. 2024arXiv preprint</p>
<p>Evaluating large language models in class-level code generation. Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, Yiling Lou, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024</p>
<p>Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, arXiv:2002.08155Codebert: A pre-trained model for programming and natural languages. 2020. 2020arXiv preprint</p>
<p>Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-Tau Yih, Luke Zettlemoyer, Mike Lewis, arXiv:2204.05999Incoder: A generative model for code infilling and synthesis. 2022. 2022arXiv preprint</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027The pile: An 800gb dataset of diverse text for language modeling. 2020. 2020arXiv preprint</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023. 2023arXiv preprint</p>
<p>Linyuan Gong, Mostafa Elhoushi, Alvin Cheung, arXiv:2401.03003AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. 2024. 2024arXiv preprint</p>
<p>Dimensions in program synthesis. Sumit Gulwani, Proceedings of the 12th international ACM SIGPLAN symposium on Principles and practice of declarative programming. the 12th international ACM SIGPLAN symposium on Principles and practice of declarative programming2010</p>
<p>Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César, Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo De Rosa, Olli Saarikivi, arXiv:2306.11644Textbooks are all you need. 2023. 2023arXiv preprint</p>
<p>UniXcoder: Unified Cross-Modal Pre-training for Code Representation. Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, Jian Yin, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, arXiv:2009.08366Graphcodebert: Pre-training code representations with data flow. 2020. 2020arXiv preprint</p>
<p>Longcoder: A long-range pre-trained language model for code completion. Daya Guo, Canwen Xu, Nan Duan, Jian Yin, Julian Mcauley, International Conference on Machine Learning. PMLR2023</p>
<p>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Wu, Li, arXiv:2401.14196DeepSeek-Coder: When the Large Language Model Meets Programming-The Rise of Code Intelligence. 2024. 2024arXiv preprint</p>
<p>Aman Gupta, Deepak Bhatt, Anubha Pandey, arXiv:2105.04144Transitioning from Real to Synthetic data: Quantifying the bias in model. 2021. 2021arXiv preprint</p>
<p>Aman Gupta, Anup Shirgaonkar, Angels De, Luis Balaguer, Bruno Silva, Daniel Holstein, Dawei Li, Jennifer Marsman, Leonardo O Nunes, Mahsa Rouzbahman, Morris Sharp, arXiv:2401.08406RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. 2024. 2024arXiv preprint</p>
<p>Evaluating large language models in generating synthetic hci research data: a case study. Perttu Hämäläinen, Mikke Tavast, Anton Kunnari, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023. 2023arXiv preprint</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021. 2021arXiv preprint</p>
<p>Felipe Hoffa, GitHub on BigQuery: Analyze all the open source code. 2016. 2016</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556Training compute-optimal large language models. 2022. 2022arXiv preprint</p>
<p>L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. Samuel Holt, Max Ruiz Luyten, Mihaela Van Der Schaar, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, arXiv:1904.09751The curious case of neural text degeneration. 2019. 2019arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Metagpt: Meta programming for multi-agent collaborative framework. 2023. 2023arXiv preprint</p>
<p>Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, Haoyu Wang, arXiv:2308.10620[cs.SE]Large Language Models for Software Engineering: A Systematic Literature Review. 2024</p>
<p>Parameter-efficient transfer learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, International conference on machine learning. PMLR2019</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021. 2021arXiv preprint</p>
<p>Dong Huang, Qingwen Bu, M Jie, Michael Zhang, Heming Luck, Cui, arXiv:2312.13010AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation. 2023. 2023arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022. 2022arXiv preprint</p>
<p>Towards Reasoning in Large Language Models: A Survey. Jie Huang, Kevin Chen, -Chuan Chang, 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023. Association for Computational Linguistics2023</p>
<p>Execution-based evaluation for data science code generation models. Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, Nan Duan, Jianfeng Gao, arXiv:2211.093742022. 2022arXiv preprint</p>
<p>Qiuyuan Huang, Naoki Wake, Bidipta Sarkar, Zane Durante, Ran Gong, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Noboru Kuno, Ade Famoti, arXiv:2403.00833Position Paper: Agent AI Towards a Holistic Intelligence. 2024. 2024arXiv preprint</p>
<p>Codesearchnet challenge: Evaluating the state of semantic code search. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt, arXiv:1909.094362019. 2019arXiv preprint</p>
<p>Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization. Yoichi Ishibashi, Yoshimasa Nishimura, arXiv:2404.021832024. 2024arXiv preprint</p>
<p>Shu Ishida, Gianluca Corrado, George Fedoseev, Hudson Yeo, Lloyd Russell, Jamie Shotton, João F Henriques, Anthony Hu, arXiv:2401.10314LangProp: A code optimization framework using Language Models applied to driving. 2024. 2024arXiv preprint</p>
<p>Mapping Language to Code in Programmatic Context. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Opt-iml: Scaling language model instruction meta learning through the lens of generalization. Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, arXiv:2212.120172022. 2022arXiv preprint</p>
<p>Towards Continual Knowledge Learning of Language Models. Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Jungkyu Choi, Minjoon Seo, International Conference on Learning ACM Trans. Softw. Eng. Methodol. 42 Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim112022. September 2024Publication date. Sunghun Kim Representations, ICLR 2022. International Conference on Learning Representations</p>
<p>Perplexity-a measure of the difficulty of speech recognition tasks. Fred Jelinek, Robert L Mercer, Lalit R Bahl, James K Baker, The Journal of the Acoustical Society of America. 621977. 1977</p>
<p>Oracle-guided component-based program synthesis. Susmit Jha, Sumit Gulwani, Sanjit A Seshia, Ashish Tiwari, Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering. the 32nd ACM/IEEE International Conference on Software Engineering20101</p>
<p>Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, arXiv:2310.19852Ai alignment: A comprehensive survey. 2023. 2023arXiv preprint</p>
<p>CodeUp: A Multilingual Code Generation Llama2 Model with Parameter-Efficient Instruction-Tuning. Juyong Jiang, Sunghun Kim, 2023</p>
<p>Cure: Code-aware neural machine translation for automatic program repair. Nan Jiang, Thibaud Lutellier, Lin Tan, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE2021</p>
<p>Shuyang Jiang, Yuhao Wang, Yu Wang, arXiv:2306.02907Selfevolve: A code evolution framework via large language models. 2023. 2023arXiv preprint</p>
<p>SWE-bench: Can Language Models Resolve Real-world Github Issues. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, Ofir Press, John Yang, Carlos E Jimenez, SWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING. 2024. 2024</p>
<p>A formalism for dependency grammar based on tree adjoining grammar. Aravind Joshi, Owen Rambow, Proceedings of the Conference on Meaning-text Theory. the Conference on Meaning-text TheoryParis, FranceMTT2003</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020. 2020arXiv preprint</p>
<p>Md Rizwan Parvez, and Shafiq Joty. 2023. xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval. Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, arXiv:2303.030042023arXiv preprint</p>
<p>Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, Chanjun Park, arXiv:2403.19270sDPO: Don't Use Your Data All at Once. 2024. 2024arXiv preprint</p>
<p>Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, arXiv:2312.15166Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. 2023. 2023arXiv preprint</p>
<p>The Stack: 3 TB of permissively licensed source code. Denis Kocetkov, Raymond Li, L I Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Transactions on Machine Learning Research. 2022. 2022</p>
<p>Openassistant conversations-democratizing large language model alignment. Andreas Köpf, Yannic Kilcher, Sotiris Dimitri Von Rütte, Anagnostidis, Rui Zhi, Keith Tam, Abdullah Stevens, Duc Barhoum, Oliver Nguyen, Richárd Stanley, Nagyfi, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Is model attention aligned with human attention? an empirical study on large language models for code generation. Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, Tianyi Zhang, arXiv:2306.012202023. 2023arXiv preprint</p>
<p>Unsupervised translation of programming languages. Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample, arXiv:2006.035112020. 2020arXiv preprint</p>
<p>DS-1000: A natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida Wang, Tao Yu, International Conference on Machine Learning. PMLR2023</p>
<p>The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Lucile Hugo Laurençon, Thomas Saulnier, Christopher Wang, Le Akiki ; Teven, Leandro Scao, Chenghao Von Werra, Eduardo González Mou, Huu Ponferrada, Nguyen, Advances in Neural Information Processing Systems. 352022. 2022Albert Villanova del Moral</p>
<p>Synthetic data: save money, time and carbon with open source. Moritz Laurer, 2024</p>
<p>Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven Chu, Hong Hoi, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Bloom: A 176b-parameter open-access multilingual language model. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, 2023. 2023</p>
<p>Rlaif: Scaling reinforcement learning from human feedback with ai feedback. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi, arXiv:2309.002672023. 2023arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021. 2021arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 332020. 2020</p>
<p>EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories. Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, Zhi Jin, arXiv:2404.005992024. 2024arXiv preprint</p>
<p>Towards enhancing in-context learning for code generation. Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, Zhi Jin, arXiv:2303.177802023. 2023arXiv preprint</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, arXiv:2305.06161Starcoder: may the source be with you!. 2023. 2023arXiv preprint</p>
<p>AlpacaEval: An Automatic Evaluator of Instruction-following Models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021. 2021arXiv preprint</p>
<p>Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat, Lee , arXiv:2309.05463Textbooks are all you need ii: phi-1.5 technical report. 2023. 2023arXiv preprint</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 3782022. 2022</p>
<p>Unleashing the power of compiler intermediate representation to enhance neural program embeddings. Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang, Sen Nie, Shi Wu, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software Engineering2022</p>
<p>Scaling down to scale up: A guide to parameter-efficient fine-tuning. Vijeta Vladislav Lialin, Anna Deshpande, Rumshisky, arXiv:2303.156472023. 2023arXiv preprint</p>
<p>Holistic Evaluation of Language Models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Transactions on Machine Learning Research. 2023. 2023</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>A survey of transformers. Tianyang Lin, Yuxin Wang, Xiangyang Liu, Xipeng Qiu, 2022. 20223</p>
<p>Mining internet-scale software repositories. Erik Linstead, Paul Rigor, Sushil Bajracharya, Cristina Lopes, Pierre Baldi, Advances in neural information processing systems. 202007. 2007</p>
<p>Bingchang Liu, Chaoyu Chen, Cong Liao, Zi Gong, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, Min Shen, Hailian Zhou, arXiv:2311.02303Mftcoder: Boosting code llms with multitask fine-tuning. 2023. 2023arXiv preprint</p>
<p>Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, Colin A Raffel, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, Deheng Ye, arXiv:2307.04349Rltf: Reinforcement learning from unit test feedback. 2023. 2023arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, Comput. Surveys. 552023. 2023</p>
<p>Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, arXiv:2404.07503Best Practices and Lessons Learned on Synthetic Data for Language Models. 2024. 2024arXiv preprint</p>
<p>Retrieval-Augmented Generation for Code Summarization via Hybrid GNN. Shangqing Liu, Yu Chen, Xiaofei Xie, Jing , Kai Siow, Yang Liu, International Conference on Learning Representations. 2020</p>
<p>Repobench: Benchmarking repository-level code autocompletion systems. Tianyang Liu, Canwen Xu, Julian Mcauley, arXiv:2306.030912023. 2023arXiv preprint</p>
<p>StarCoder 2 and The Stack v2: The Next Generation. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, arXiv:2402.191732024. 2024arXiv preprint</p>
<p>ReACC: A Retrieval-Augmented Code Completion Framework. Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221Seung-won Hwang, and Alexey Svyatkovskiy</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, arXiv:2102.046642021. 2021arXiv preprint</p>
<p>WizardCoder: Empowering Code Large Language Models with Evol-Instruct. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Wei Ma, Mengjie Zhao, Xiaofei Xie, Qiang Hu, Shangqing Liu, Jie Zhang, Wenhan Wang, Yang Liu, arXiv:2212.10017Are Code Pre-trained Models Powerful to Learn Code Syntax and Semantics?. 2022. 2022arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>An overview of Bard: an early experiment with generative AI. James Manyika, Sissie Hsiao, AI. Google Static Documents. 22023. 2023</p>
<p>STYLE-ANALYZER: fixing code style inconsistencies with interpretable unsupervised algorithms. Vadim Markovtsev, Waren Long, Hugo Mougard, Konstantin Slavnov, Egor Bulychev, 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). IEEE2019</p>
<p>Generating training data with language models: Towards zero-shot language understanding. Yu Meng, Jiaxin Huang, Yu Zhang, Jiawei Han, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Introducing Meta Llama 3: The most capable openly available LLM to date. Meta. 2024</p>
<p>Phi-2: The surprising power of small language models. Sébastien Bubeck, Mojan Javaheripi, 2023</p>
<p>TBCNN: A tree-based convolutional neural network for programming language processing. Lili Mou, Ge Li, Zhi Jin, Lu Zhang, Tao Wang, arXiv:1409.57182014. 2014arXiv preprint</p>
<p>Hussein Mozannar, Gagan Bansal, Adam Fourney, Eric Horvitz, arXiv:2210.14306Reading between the lines: Modeling user behavior and costs in AI-assisted programming. 2022. 2022arXiv preprint</p>
<p>Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, Shayne Longpre, arXiv:2308.07124Octopack: Instruction tuning code large language models. 2023. 2023arXiv preprint</p>
<p>The attack of the clones: A study of the impact of shared code on vulnerability patching. Antonio Nappa, Richard Johnson, Leyla Bilge, Juan Caballero, Tudor Dumitras, 2015 IEEE symposium on security and privacy. IEEE2015</p>
<p>Lever: Learning to verify language-to-code generation with execution. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-Tau Yih, Sida Wang, Xi Victoria, Lin , International Conference on Machine Learning. PMLR2023</p>
<p>L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models. Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz, Caiming Xiong, arXiv:2309.174462023. 2023arXiv preprint</p>
<p>Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou, arXiv:2305.02309Codegen2: Lessons for training llms on programming and natural languages. 2023. 2023arXiv preprint</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, arXiv:2203.13474Codegen: An open large language model for code with multi-turn program synthesis. 2022. 2022arXiv preprint</p>
<p>Is Self-Repair a Silver Bullet for Code Generation?. Jeevana Theo X Olausson, Chenglong Priya Inala, Jianfeng Wang, Armando Gao, Solar-Lezama, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Chatgpt: Optimizing language models for dialogue. 2022OpenAI</p>
<p>Opendevin, OpenDevin: Code Less, Make More. 2024. September 20241Publication date</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 352022. 2022</p>
<p>Oded Ovadia, Menachem Brief, Moshik Mishaeli, Oren Elisha, arXiv:2312.05934Fine-tuning or retrieval? comparing knowledge injection in llms. 2023. 2023arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, Sergey Mechtaev, arXiv:2404.05520The Fact Selection Problem in LLM-Based Program Repair. 2024. 2024arXiv preprint</p>
<p>Retrieval Augmented Code Generation and Summarization. Wasi Md Rizwan Parvez, Saikat Ahmad, Baishakhi Chakraborty, Kai-Wei Ray, Chang, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Evaluating In-Context Learning of Libraries for Code Generation. Arkil Patel, Siva Reddy, Dzmitry Bahdanau, Pradeep Dasigi, arXiv:2311.096352023. 2023arXiv preprint</p>
<p>IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators. Indraneil Paul, Jun Luo, Goran Glavaš, Iryna Gurevych, arXiv:2403.038942024. 2024arXiv preprint</p>
<p>Program comprehension and code complexity metrics: An fmri study. Norman Peitek, Sven Apel, Chris Parnin, André Brechmann, Janet Siegmund, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE2021</p>
<p>RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion. Huy N Phan, Tien N Hoang N Phan, Nguyen, Nghi, Bui, arXiv:2403.060952024. 2024arXiv preprint</p>
<p>. Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhuravinskyi, Dakota Mahan, Marco Bellagente, Carlos Riquelme, arXiv:2404.012262024arXiv preprintet al. 2024. Stable Code</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. Noah A Smith, Mike Lewis, arXiv:2108.124092021. 2021Ofir PressarXiv preprint</p>
<p>Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson, arXiv:2310.03693Fine-tuning aligned language models compromises safety, even when users do not intend to!. 2023. 2023arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018. 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 192019. 2019</p>
<p>Measuring software library stability through historical version analysis. Steven Raemaekers, Arie Van Deursen, Joost Visser, 2012 28th IEEE international conference on software maintenance (ICSM). IEEE2012</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 212020. 2020</p>
<p>Evaluating the text-to-sql capabilities of large language models. Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau, arXiv:2204.004982022. 2022arXiv preprint</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, Neel Tang, Ming Sundaresan, Ambrosio Zhou, Shuai Blanco, Ma, arXiv:2009.102972020. 2020arXiv preprint</p>
<p>Idea to software. Replit, 2016</p>
<p>Tal Ridnik, Dedy Kredo, Itamar Friedman, arXiv:2401.08500Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering. 2024. 2024arXiv preprint</p>
<p>Evol-Instruct-Code-80k. Nick Roshdieh, 2023</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023. 2023arXiv preprint</p>
<p>Multitask Prompted Training Enables Zero-Shot Task Generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, ICLR 2022-Tenth International Conference on Learning Representations. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim, 2022. September 20241Publication date</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017. 2017arXiv preprint</p>
<p>Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, arXiv:1803.02155Self-attention with relative position representations. 2018. 2018arXiv preprint</p>
<p>Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean, arXiv:1701.06538Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. 2017. 2017arXiv preprint</p>
<p>Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, arXiv:2307.14936Pangu-coder2: Boosting large language models for code with ranking feedback. 2023. 2023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Atsushi Shirafuji, Yusuke Oda, Jun Suzuki, Makoto Morishita, Yutaka Watanobe, arXiv:2311.11690Refactoring Programs Using Large Language Models with Few-Shot Examples. 2023. 2023arXiv preprint</p>
<p>Execution-based code generation using deep reinforcement learning. Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, Chandan K Reddy, arXiv:2301.138162023. 2023arXiv preprint</p>
<p>Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. Disha Shrivastava, Denis Kocetkov, arXiv:2306.10998RepoFusion: Training Code Models to Understand Your Repository. 2023. 2023arXiv preprint</p>
<p>Repository-level prompt generation for large language models of code. Disha Shrivastava, Hugo Larochelle, Daniel Tarlow, International Conference on Machine Learning. PMLR2023</p>
<p>Codefusion: A pre-trained diffusion model for code generation. Mukul Singh, José Cambronero, Sumit Gulwani, Carina Vu Le, Gust Negreanu, Verbruggen, arXiv:2310.176802023. 2023arXiv preprint</p>
<p>Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu, arXiv:2402.12317ARKS: Active Retrieval in Knowledge Soup for Code Generation. 2024. 2024arXiv preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, Yunfeng Liu, Neurocomputing. 5681270632024. 2024</p>
<p>Intellicode compose: Code generation using transformer. Alexey Svyatkovskiy, Shengyu Shao Kun Deng, Neel Fu, Sundaresan, Proceedings of the 28th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering. the 28th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering2020</p>
<p>Code translation with compiler representations. Marc Szafraniec, Hugh Baptiste Roziere, Francois Leather, Patrick Charton, Gabriel Labatut, Synnaeve, Proceedings of the Eleventh International Conference on Learning Representations: ICLR. the Eleventh International Conference on Learning Representations: ICLR2022</p>
<p>. Tabnine, AI Code Completions2018</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford Alpaca: An Instruction-following LLaMA model. 2023</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024. 2024arXiv preprint</p>
<p>Benchmarking large language models for automated verilog rtl code generation. Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, Siddharth Garg, 2023 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE). IEEE2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023. 2023arXiv preprint</p>
<p>Natural language processing with transformers. Lewis Tunstall, Leandro Von Werra, Thomas Wolf, 2022O'Reilly Media, Inc</p>
<p>Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. Priyan Vaithilingam, Tianyi Zhang, Elena L Glassman, Chi conference on human factors in computing systems extended abstracts. 2022</p>
<p>Synthetic data, real errors: how (not) to publish and use synthetic data. Boris Van Breugel, Zhaozhi Qian, Mihaela Van Der, Schaar, International Conference on Machine Learning. PMLR2023</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 2017. 201730</p>
<p>Ben Wang, Aran Komatsuzaki, GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. 2021</p>
<p>Chong Wang, Jian Zhang, Yebo Feng, Tianlin Li, Weisong Sun, Yang Liu, Xin Peng, arXiv:2401.06391Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation. 2024. 2024arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 182024. 2024</p>
<p>ReCode: Robustness Evaluation of Code Generation Models. Shiqi Wang, Li Zheng, Haifeng Qian, Chenghao Yang, Zijian Wang, Varun Kumar, Mingyue Shang, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Krishna Murali, Dan Ramanathan, Bing Roth, Xiang, 10.48550/arXiv.2212.102642022. 2022</p>
<p>Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, arXiv:2310.16218Knowledge editing for large language models: A survey. 2023. 2023arXiv preprint</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji, arXiv:2402.01030Executable code actions elicit better llm agents. 2024. 2024arXiv preprint</p>
<p>Compilable Neural Code Generation with Compiler Feedback. Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, Qun Liu, Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022. 2022arXiv preprint</p>
<p>Self-Instruct: Aligning Language Models with Self-Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, The 61st Annual Meeting Of The Association For Computational Linguistics. 2023</p>
<p>CodeT5+: Open Code Large Language Models for Code Understanding and Generation. Yue Wang, Hung Le, Akhilesh Gotmare, Nghi Bui, Junnan Li, Steven Hoi, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Code completion by modeling flattened abstract syntax trees as graphs. Yanlin Wang, Hui Li, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202135</p>
<p>CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. Yue Wang, Weishi Wang, Shafiq Joty, Steven Ch Hoi, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Aligning large language models with human: A survey. Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu, arXiv:2307.129662023. 2023arXiv preprint</p>
<p>Execution-based evaluation for open-domain code generation. Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig, arXiv:2212.104812022. 2022arXiv preprint</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021. 2021arXiv preprint</p>
<p>Emergent Abilities of Large Language Models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Transactions on Machine Learning Research. 2022. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 352022. 2022</p>
<p>Magicoder: Source code is all you need. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang, arXiv:2312.021202023. 2023arXiv preprint</p>
<p>LLM-powered Autonomous Agents. Lilian Weng, 2023. Jun 2023</p>
<p>Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen, arXiv:2402.09739QuRating: Selecting High-Quality Data for Training Language Models. 2024. 2024arXiv preprint</p>
<p>Fake it till you make it: face analysis in the wild using synthetic data alone. Erroll Wood, Tadas Baltrušaitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, Jamie Shotton, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Repoformer: Selective Retrieval for Repository-Level Code Completion. Di Wu, Uddin Wasi, Dejiao Ahmad, Murali Zhang, Xiaofei Krishna Ramanathan, Ma, arXiv:2403.100592024. 2024arXiv preprint</p>
<p>. ACM Trans. Softw. Eng. Methodol. 11September 2024Publication date</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.081552023. 2023arXiv preprint</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023. 2023arXiv preprint</p>
<p>. Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, Wei Ye, arXiv:2403.157472024. 2024arXiv preprint</p>
<p>Data selection for language models via importance resampling. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, Percy S Liang, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Introducing OpenDevin CodeAct 1.0, a new State-of-the-art in Coding Agents. Bowen Li, Xingyao Wang, Graham Neubig, 2024</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023. 2023arXiv preprint</p>
<p>A systematic evaluation of large language models of code. Uri Frank F Xu, Graham Alon, Vincent Neubig, Josua Hellendoorn, Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. the 6th ACM SIGPLAN International Symposium on Machine Programming2022</p>
<p>Zhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, David Lo, arXiv:2403.07506Robustness, security, privacy, explainability, efficiency, and usability of large language models for code. 2024. 2024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>Learning to mine aligned code and natural language pairs from stack overflow. Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, Graham Neubig, Proceedings of the 15th international conference on mining software repositories. the 15th international conference on mining software repositories2018</p>
<p>Min Kang, Jaegeun Yoo, Sookyo Han, Heewon In, Jisu Jeon, Jaewook Jeong, Hyunwook Kang, Kyung-Min Kim, Munhyong Kim, Sungju Kim, Kim, arXiv:2404.01954HyperCLOVA X. 2024. 2024arXiv preprint</p>
<p>Codereval: A benchmark of pragmatic code generation with generative pre-trained models. Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, Tao Xie, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. the 46th IEEE/ACM International Conference on Software Engineering2024</p>
<p>Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, Qiufeng Yin, arXiv:2312.141872023. 2023arXiv preprint</p>
<p>Rrhf: Rank responses to align language models with human feedback without tears. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei Huang, arXiv:2304.053022023. 2023arXiv preprint</p>
<p>Ding Naman Jain Harm de Vries Leandro von Werra Arjun Guha Lingming Zhang Yuxiang Wei, Federico Cassano. Jiawei Liu, Yifeng , 2024StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation</p>
<p>Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg, arXiv:2106.101992021. 2021arXiv preprint</p>
<p>CERT: continual pre-training on sketches for library-oriented code generation. Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, Jian-Guang Lou, arXiv:2206.068882022. 2022arXiv preprint</p>
<p>Large Language Models Meet NL2Code: A Survey. Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, Jian-Guang Lou, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, arXiv:2403.16443CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. 2024arXiv preprint</p>
<p>. ACM Trans. Softw. Eng. Methodol. 11September 2024Publication date</p>
<p>RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, Weizhu Chen, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Adaptive budget allocation for parameter-efficient fine-tuning. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, arXiv:2308.10792Instruction tuning for large language models: A survey. 2023. 2023arXiv preprint</p>
<p>Siren's song in the AI ocean: a survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, arXiv:2309.012192023. 2023arXiv preprint</p>
<p>Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, Abhik Roychoudhury, arXiv:2404.05427AutoCodeRover: Autonomous Program Improvement. 2024. 2024arXiv preprint</p>
<p>Unifying the perspectives of nlp and software engineering: A survey on language models for code. Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, Rui Wang, arXiv:2311.079892023. 2023arXiv preprint</p>
<p>Liang Zhao, Xiaocheng Feng, arXiv:2312.17044Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding. 2023. 2023arXiv preprintXiachong Feng, Bin Qin, and Ting Liu</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023. 2023arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, Xiang Yue, arXiv:2402.14658OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement. 2024. 2024arXiv preprint</p>
<p>Outline, then details: Syntactically guided coarse-to-fine code generation. Wenqing Zheng, Ajay Kumar Sp Sharan, Kevin Jaiswal, Yihan Wang, Dejia Xi, Zhangyang Xu, Wang, International Conference on Machine Learning. PMLR2023</p>
<p>Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, Jiachi Chen, arXiv:2311.10372A survey of large language models for code: Evolution, benchmarking, and future trends. 2023. 2023arXiv preprint</p>
<p>LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step. Li Zhong, Zilong Wang, Jingbo Shang, arXiv:2402.169062024. 2024arXiv preprint</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang, arXiv:2310.044062023. 2023arXiv preprint</p>
<p>Lima: Less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, The Eleventh International Conference on Learning Representations. 2022</p>
<p>DocPrompting: Generating Code by Retrieving the Docs. Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, Graham Neubig, The Eleventh International Conference on Learning Representations. 2022</p>
<p>ICE-Score: Instructing Large Language Models to Evaluate Code. Terry Yue, Zhuo , Findings of the Association for Computational Linguistics: EACL 2024. 2024</p>
<p>Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, arXiv:2401.00788Leandro von Werra, Harm de Vries, Qian Liu, and Niklas Muennighoff. 2024. Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9935 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9935</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9935</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-287fc092d552d4a77d914833c371c0e0b1119067</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/287fc092d552d4a77d914833c371c0e0b1119067" target="_blank">LLM-based relevance assessment still can't replace human relevance assessment</a></p>
                <p><strong>Paper Venue:</strong> International Workshop on Evaluating Information Access</p>
                <p><strong>Paper TL;DR:</strong> This paper critically examines the claim that LLM-based relevance assessments, such as those generated by the UMBRELA system, can fully replace traditional human relevance assessments in TREC-style evaluations, highlighting practical and theoretical limitations that undermine this conclusion.</p>
                <p><strong>Paper Abstract:</strong> The use of large language models (LLMs) for relevance assessment in information retrieval has gained significant attention, with recent studies suggesting that LLM-based judgments provide comparable evaluations to human judgments. Notably, based on TREC 2024 data, Upadhyay et al. make a bold claim that LLM-based relevance assessments, such as those generated by the UMBRELA system, can fully replace traditional human relevance assessments in TREC-style evaluations. This paper critically examines this claim, highlighting practical and theoretical limitations that undermine the validity of this conclusion. First, we question whether the evidence provided by Upadhyay et al. really supports their claim, particularly if a test collection is used asa benchmark for future improvements. Second, through a submission deliberately intended to do so, we demonstrate the ease with which automatic evaluation metrics can be subverted, showing that systems designed to exploit these evaluations can achieve artificially high scores. Theoretical challenges -- such as the inherent narcissism of LLMs, the risk of overfitting to LLM-based metrics, and the potential degradation of future LLM performance -- must be addressed before LLM-based relevance assessments can be considered a viable replacement for human judgments.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9935.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9935.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Umbrella</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UMBRELA (LLM-based Relevance Assessor / evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LLM-based relevance assessment tool (reproduction of Bing Relevance Assessor) used to assign relevance grades to passages for TREC-style retrieval evaluation; used both as an automatic evaluator and, if applied as a reranker, as a system component.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Information retrieval — TREC-style ad hoc retrieval / RAG retrieval evaluation (passage ranking over MS MARCO Segment V2.1)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Umbrella (LLM-based assessor; underlying model not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Point-wise relevance grading: LLM is prompted with instructions resembling human assessor guidelines to assign relevance grades to passages (used either to generate evaluation labels or to re-rank top passages by assigned grade); labels from the track's Umbrella run were also reused directly in simulations as final-stage re-ranker labels.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Established TREC manual relevance assessment protocols were used as the human gold standard for the RAG track; only 27 queries were manually judged in the judged set referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's τ rank correlation used to compare LLM vs. human judgments. Reported/quoted values: Upadhyay et al. reported overall τ = 0.89 (human vs. Umbrella); reproduction in this paper: τ = 0.84 after excluding bottom 15 systems; among top 60 τ ≈ 0.85; among top 20 τ = 0.51; top 15 τ = 0.56. Under simulated circularity (Umbrella used as reranker and evaluated with Umbrella labels) τ drops to 0.63 (top 60), to 0.44 (top 20), 0.49 (top 15), 0.38 (top 10), and −0.40 (top 5). Discordant system pairs rose to 18% within top 60 in circularity simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of human grounding and gold-standard validity; reduced ability to reliably distinguish fine-grained differences among top systems (degraded agreement at leaderboard top); vulnerability to circularity and manipulation (score inflation when assessor is used as re-ranker); bias favoring LLM-based rankers ('narcissism'); susceptibility to prompt attacks and adversarial manipulation; undermining of future reuse of collections and trust in reported improvements (Goodhart's-law effects).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Empirical examples from the paper: the uwc1 submission (deliberately built using LLM judgments) ranked 5th under Umbrella but 28th under manual judgments; simulation where Umbrella re-ranking produced many systems with NDCG > 0.95 under Umbrella labels while the same systems had manual NDCG between 0.68 and 0.72; Kendall's τ among top 20 systems drops from ~0.85 to 0.51 (showing substantial swaps) and becomes strongly degraded under circularity (even negative τ among top 5).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Overall correlations can be high (reported τ ≈ 0.86–0.89 in other studies and in Upadhyay et al.), indicating LLM judgments often mimic humans at coarse scales; in some cases LLM-based re-ranking improved manual performance (example: uwc2 improved baseline from 9th→4th under manual and 7th→3rd under automatic), and hybrid approaches (filtering or human refinement) were examined by Upadhyay et al. though they reported little added correlation benefit for LLM assistance; limited manual judging (only 27 queries) introduces statistical uncertainty and possible noise.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment (this paper), Sections 1–6 (Abstract, Sections 2–4 describe experiments, Section 5 conceptual arguments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLM-based relevance assessment still can't replace human relevance assessment", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9935.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9935.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-40 (as judge in uwc1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-40 (LLM used by team WaterlooClarke to judge pooled documents in the adversarial run uwc1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capacity LLM used by team WaterlooClarke to generate LLM-based judgments on a pooled set of candidate passages; judgments were then used as the basis for creating a run designed to exploit automatic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Information retrieval — relevance labeling of pooled passages for run creation (adversarial/evaluation-subverting submission)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-40 (explicitly named in this paper as the model used to judge the pooled set for uwc1)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Team pooled top 20 documents from 15 preliminary runs and had GPT-40 judge them using the prompt described by Faggioli et al.; top-graded passages then judged pairwise following Clarke et al. procedure but substituting LLM-based assessments for human/crowdsourced judgments; final ranking used LLM preference judgments as primary key and LLM relevance grades as secondary key.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Manual TREC-style judgments (track human assessors) served as the comparison gold standard; the paper reports manual ranking positions for the same runs (e.g., uwc1 placed 28th under manual judgments).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Comparison reported as rank differences and Kendall's τ between human and automatic assessments: the uwc1 run was ranked 5th by automatic LLM evaluation but 28th by manual evaluation (large discrepancy illustrating divergence).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Use of a strong LLM judge (GPT-40) can be exploited to produce runs that score highly under LLM-based evaluation but poorly under human evaluation, demonstrating loss of validity and susceptibility to manipulation when using LLMs as judges instead of humans.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>uwc1: deliberate LLM-judgment-based run that achieved 5th place under automatic (LLM) assessment and 28th under manual assessment; demonstrates how LLM-judged labels can be turned into an effective but non-human-aligned system.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>This example is adversarial by design; not all LLM-judged rerankings are adversarial—some (e.g., uwc2) improved both automatic and manual ranks. The observation does not claim every LLM judge is easily exploitable, but it shows practical vulnerability.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 4.1 (Subverting Automatic Evaluation) and Section 4.2 (LLM-based Re-ranking) of LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLM-based relevance assessment still can't replace human relevance assessment", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9935.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9935.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based relevance assessment (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based relevance assessment / LLM-as-a-judge (general concept)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The practice of using large language models to assign relevance labels or preference judgments to candidate documents/passages, treating the LLM output as evaluation labels or as components of the ranking pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Information retrieval evaluation (relevance labeling, run-level effectiveness measurement, re-ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various (examples in text include GPT-3.5/text-davinci-003 in Faggioli et al., Umbrella in Upadhyay et al., GPT-40 in WaterlooClarke runs); specific underlying models vary by study or tool.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Typically LLMs are prompted with instructions resembling human assessor guidelines to produce point-wise relevance grades or pairwise preference judgments; setups described include fully automatic assessment (Umbrella), hybrid filtering, and human refinement of LLM labels; LLM outputs can also be used directly as reranker scores.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judgments follow established TREC protocols and are treated as the gold standard reflecting human utility; in the RAG track referenced here, manual assessments covered a limited set of queries (27 judged queries mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Reported comparisons use Kendall's τ for run-level ranking correlation (examples: Faggioli et al. reported τ = 0.86 for GPT-3.5 vs human on NDCG@10; Upadhyay et al. reported τ = 0.89 for Umbrella vs human; this paper reproduces/extends those numbers and reports degraded τ at leaderboard tops and under circularity simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of human task grounding and gold-standard validity; reduced sensitivity to fine-grained differences among top systems; potential bias toward LLM-generated outputs (narcissism); risk of score inflation and invalid evaluations under circular use; vulnerability to adversarial prompts and manipulations; potential long-term drift away from human judgments if training/evaluation pipelines rely on LLM labels (Goodhart's law).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Multiple examples in the paper: weakened Kendall's τ among top systems (e.g., τ drops to 0.51 among top 20); deliberate subversion uwc1 (5th by LLM vs 28th manual); circularity simulation producing near-perfect Umbrella NDCG (>0.95) while manual NDCG stayed ~0.68–0.72; references to empirical findings of bias in Balog et al. and prompt attacks in Alaofi et al.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Empirical work shows strong overall correlation in many settings (e.g., τ ≈ 0.86–0.89), indicating LLM judges can approximate human judgments at coarse scales; certain automatic re-ranking by LLMs can improve manual metrics (uwc2 example); hybrid workflows (LLM filtering + human judgments, or human refinement of LLM outputs) remain possible mitigations though Upadhyay et al. observed limited added correlation benefit from human-in-the-loop in that study.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Throughout the paper: Abstract, Introduction (Sec. 1), Sections 2–4 (empirical reproductions and adversarial examples), Section 5 (conceptual arguments about gold standards and bias), and Section 6 (long-term concerns / Goodhart's law).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLM-based relevance assessment still can't replace human relevance assessment", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Perspectives on Large Language Models for Relevance Judgment <em>(Rating: 2)</em></li>
                <li>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look <em>(Rating: 2)</em></li>
                <li>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor. <em>(Rating: 2)</em></li>
                <li>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation <em>(Rating: 2)</em></li>
                <li>LLMs can be Pooled into Labelling a Document as Relevant: best café near me; this paper is perfectly relevant. <em>(Rating: 2)</em></li>
                <li>LLMs as narcissistic evaluators: When ego inflates evaluation scores <em>(Rating: 2)</em></li>
                <li>LLM Evaluators Recognize and Favor Their Own Generations <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9935",
    "paper_id": "paper-287fc092d552d4a77d914833c371c0e0b1119067",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Umbrella",
            "name_full": "UMBRELA (LLM-based Relevance Assessor / evaluator)",
            "brief_description": "An open-source LLM-based relevance assessment tool (reproduction of Bing Relevance Assessor) used to assign relevance grades to passages for TREC-style retrieval evaluation; used both as an automatic evaluator and, if applied as a reranker, as a system component.",
            "citation_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor.",
            "mention_or_use": "use",
            "task_domain": "Information retrieval — TREC-style ad hoc retrieval / RAG retrieval evaluation (passage ranking over MS MARCO Segment V2.1)",
            "llm_judge_model": "Umbrella (LLM-based assessor; underlying model not specified in this paper)",
            "llm_judge_setup": "Point-wise relevance grading: LLM is prompted with instructions resembling human assessor guidelines to assign relevance grades to passages (used either to generate evaluation labels or to re-rank top passages by assigned grade); labels from the track's Umbrella run were also reused directly in simulations as final-stage re-ranker labels.",
            "human_evaluation_setup": "Established TREC manual relevance assessment protocols were used as the human gold standard for the RAG track; only 27 queries were manually judged in the judged set referenced in the paper.",
            "agreement_metric": "Kendall's τ rank correlation used to compare LLM vs. human judgments. Reported/quoted values: Upadhyay et al. reported overall τ = 0.89 (human vs. Umbrella); reproduction in this paper: τ = 0.84 after excluding bottom 15 systems; among top 60 τ ≈ 0.85; among top 20 τ = 0.51; top 15 τ = 0.56. Under simulated circularity (Umbrella used as reranker and evaluated with Umbrella labels) τ drops to 0.63 (top 60), to 0.44 (top 20), 0.49 (top 15), 0.38 (top 10), and −0.40 (top 5). Discordant system pairs rose to 18% within top 60 in circularity simulation.",
            "losses_identified": "Loss of human grounding and gold-standard validity; reduced ability to reliably distinguish fine-grained differences among top systems (degraded agreement at leaderboard top); vulnerability to circularity and manipulation (score inflation when assessor is used as re-ranker); bias favoring LLM-based rankers ('narcissism'); susceptibility to prompt attacks and adversarial manipulation; undermining of future reuse of collections and trust in reported improvements (Goodhart's-law effects).",
            "examples_of_loss": "Empirical examples from the paper: the uwc1 submission (deliberately built using LLM judgments) ranked 5th under Umbrella but 28th under manual judgments; simulation where Umbrella re-ranking produced many systems with NDCG &gt; 0.95 under Umbrella labels while the same systems had manual NDCG between 0.68 and 0.72; Kendall's τ among top 20 systems drops from ~0.85 to 0.51 (showing substantial swaps) and becomes strongly degraded under circularity (even negative τ among top 5).",
            "counterexamples_or_caveats": "Overall correlations can be high (reported τ ≈ 0.86–0.89 in other studies and in Upadhyay et al.), indicating LLM judgments often mimic humans at coarse scales; in some cases LLM-based re-ranking improved manual performance (example: uwc2 improved baseline from 9th→4th under manual and 7th→3rd under automatic), and hybrid approaches (filtering or human refinement) were examined by Upadhyay et al. though they reported little added correlation benefit for LLM assistance; limited manual judging (only 27 queries) introduces statistical uncertainty and possible noise.",
            "paper_reference": "LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment (this paper), Sections 1–6 (Abstract, Sections 2–4 describe experiments, Section 5 conceptual arguments).",
            "uuid": "e9935.0",
            "source_info": {
                "paper_title": "LLM-based relevance assessment still can't replace human relevance assessment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-40 (as judge in uwc1)",
            "name_full": "GPT-40 (LLM used by team WaterlooClarke to judge pooled documents in the adversarial run uwc1)",
            "brief_description": "A high-capacity LLM used by team WaterlooClarke to generate LLM-based judgments on a pooled set of candidate passages; judgments were then used as the basis for creating a run designed to exploit automatic evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Information retrieval — relevance labeling of pooled passages for run creation (adversarial/evaluation-subverting submission)",
            "llm_judge_model": "GPT-40 (explicitly named in this paper as the model used to judge the pooled set for uwc1)",
            "llm_judge_setup": "Team pooled top 20 documents from 15 preliminary runs and had GPT-40 judge them using the prompt described by Faggioli et al.; top-graded passages then judged pairwise following Clarke et al. procedure but substituting LLM-based assessments for human/crowdsourced judgments; final ranking used LLM preference judgments as primary key and LLM relevance grades as secondary key.",
            "human_evaluation_setup": "Manual TREC-style judgments (track human assessors) served as the comparison gold standard; the paper reports manual ranking positions for the same runs (e.g., uwc1 placed 28th under manual judgments).",
            "agreement_metric": "Comparison reported as rank differences and Kendall's τ between human and automatic assessments: the uwc1 run was ranked 5th by automatic LLM evaluation but 28th by manual evaluation (large discrepancy illustrating divergence).",
            "losses_identified": "Use of a strong LLM judge (GPT-40) can be exploited to produce runs that score highly under LLM-based evaluation but poorly under human evaluation, demonstrating loss of validity and susceptibility to manipulation when using LLMs as judges instead of humans.",
            "examples_of_loss": "uwc1: deliberate LLM-judgment-based run that achieved 5th place under automatic (LLM) assessment and 28th under manual assessment; demonstrates how LLM-judged labels can be turned into an effective but non-human-aligned system.",
            "counterexamples_or_caveats": "This example is adversarial by design; not all LLM-judged rerankings are adversarial—some (e.g., uwc2) improved both automatic and manual ranks. The observation does not claim every LLM judge is easily exploitable, but it shows practical vulnerability.",
            "paper_reference": "Section 4.1 (Subverting Automatic Evaluation) and Section 4.2 (LLM-based Re-ranking) of LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment.",
            "uuid": "e9935.1",
            "source_info": {
                "paper_title": "LLM-based relevance assessment still can't replace human relevance assessment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LLM-based relevance assessment (general)",
            "name_full": "LLM-based relevance assessment / LLM-as-a-judge (general concept)",
            "brief_description": "The practice of using large language models to assign relevance labels or preference judgments to candidate documents/passages, treating the LLM output as evaluation labels or as components of the ranking pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Information retrieval evaluation (relevance labeling, run-level effectiveness measurement, re-ranking)",
            "llm_judge_model": "Various (examples in text include GPT-3.5/text-davinci-003 in Faggioli et al., Umbrella in Upadhyay et al., GPT-40 in WaterlooClarke runs); specific underlying models vary by study or tool.",
            "llm_judge_setup": "Typically LLMs are prompted with instructions resembling human assessor guidelines to produce point-wise relevance grades or pairwise preference judgments; setups described include fully automatic assessment (Umbrella), hybrid filtering, and human refinement of LLM labels; LLM outputs can also be used directly as reranker scores.",
            "human_evaluation_setup": "Human judgments follow established TREC protocols and are treated as the gold standard reflecting human utility; in the RAG track referenced here, manual assessments covered a limited set of queries (27 judged queries mentioned).",
            "agreement_metric": "Reported comparisons use Kendall's τ for run-level ranking correlation (examples: Faggioli et al. reported τ = 0.86 for GPT-3.5 vs human on NDCG@10; Upadhyay et al. reported τ = 0.89 for Umbrella vs human; this paper reproduces/extends those numbers and reports degraded τ at leaderboard tops and under circularity simulations).",
            "losses_identified": "Loss of human task grounding and gold-standard validity; reduced sensitivity to fine-grained differences among top systems; potential bias toward LLM-generated outputs (narcissism); risk of score inflation and invalid evaluations under circular use; vulnerability to adversarial prompts and manipulations; potential long-term drift away from human judgments if training/evaluation pipelines rely on LLM labels (Goodhart's law).",
            "examples_of_loss": "Multiple examples in the paper: weakened Kendall's τ among top systems (e.g., τ drops to 0.51 among top 20); deliberate subversion uwc1 (5th by LLM vs 28th manual); circularity simulation producing near-perfect Umbrella NDCG (&gt;0.95) while manual NDCG stayed ~0.68–0.72; references to empirical findings of bias in Balog et al. and prompt attacks in Alaofi et al.",
            "counterexamples_or_caveats": "Empirical work shows strong overall correlation in many settings (e.g., τ ≈ 0.86–0.89), indicating LLM judges can approximate human judgments at coarse scales; certain automatic re-ranking by LLMs can improve manual metrics (uwc2 example); hybrid workflows (LLM filtering + human judgments, or human refinement of LLM outputs) remain possible mitigations though Upadhyay et al. observed limited added correlation benefit from human-in-the-loop in that study.",
            "paper_reference": "Throughout the paper: Abstract, Introduction (Sec. 1), Sections 2–4 (empirical reproductions and adversarial examples), Section 5 (conceptual arguments about gold standards and bias), and Section 6 (long-term concerns / Goodhart's law).",
            "uuid": "e9935.2",
            "source_info": {
                "paper_title": "LLM-based relevance assessment still can't replace human relevance assessment",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Perspectives on Large Language Models for Relevance Judgment",
            "rating": 2
        },
        {
            "paper_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
            "rating": 2
        },
        {
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor.",
            "rating": 2
        },
        {
            "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
            "rating": 2
        },
        {
            "paper_title": "LLMs can be Pooled into Labelling a Document as Relevant: best café near me; this paper is perfectly relevant.",
            "rating": 2
        },
        {
            "paper_title": "LLMs as narcissistic evaluators: When ego inflates evaluation scores",
            "rating": 2
        },
        {
            "paper_title": "LLM Evaluators Recognize and Favor Their Own Generations",
            "rating": 2
        }
    ],
    "cost": 0.01157,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment</h1>
<p>Charles L. A. Clarke<br>University of Waterloo<br>Canada</p>
<h4>Abstract</h4>
<p>The use of large language models (LLMs) for relevance assessment in information retrieval has gained significant attention, with recent studies suggesting that LLM-based judgments provide comparable evaluations to human judgments. Notably, based on TREC 2024 data, Upadhyay et al. [11] make a bold claim that LLM-based relevance assessments, such as those generated by the Umbrella system, can fully replace traditional human relevance assessments in TREC-style evaluations. This paper critically examines this claim, highlighting practical and theoretical limitations that undermine the validity of this conclusion.</p>
<p>First, we question whether the evidence provided by Upadhyay et al. [11] genuinely supports their claim, particularly when the test collection is intended to serve as a benchmark for future research innovations. Second, we submit a system deliberately crafted to exploit automatic evaluation metrics, demonstrating that it can achieve artificially inflated scores without truly improving retrieval quality. Third, we simulate the consequences of circularity by analyzing Kendall's tau correlations under the hypothetical scenario in which all systems adopt Umbrella as a final-stage re-ranker, illustrating how reliance on LLM-based assessments can distort system rankings. Theoretical challenges - including the inherent narcissism of LLMs, the risk of overfitting to LLM-based metrics, and the potential degradation of future LLM performance - that must be addressed before LLM-based relevance assessments can be considered a viable replacement for human judgments.</p>
<h2>1 Introduction</h2>
<p>In early 2023, Faggioli et al. [6] applied GPT-3.5 (text-davinci-003) to fully reassess runs submitted to the TREC 2021 Deep Learning track. They reported Kendall's $\tau=0.86$ for human vs. LLM-based assessment on NDCG@10. A natural conclusion might be that LLMs could now replace humans for routine relevance assessment. Instead, Faggioli et al. [6] issue a warning. While recognizing the potential of LLMs to improve ranking and acknowledging their value as part of the relevance assessment process, they argue strongly against abandoning human assessment. They raise concerns about the potential for unknown biases that LLM-based assessments might introduce. They highlight the issue of circularity, where LLMs evaluate the outputs of other LLMs. Most importantly, their primary concern is that "LLMs are not people." Since information retrieval systems are designed to serve human needs, their evaluation must ultimately reflect human judgment and preferences.</p>
<p>Recently, Upadhyay et al. [11] analyze the retrieval task results from the TREC 2024 RAG track. This retrieval task (or "R task") mirrors a traditional TREC ad hoc retrieval task. Participating systems were tasked with executing 301 queries over the MS MARCO Segment V2.1 collection, producing a ranked set of 100 passages for</p>
<h2>Laura Dietz <br> University of New Hampshire USA</h2>
<p>each query (a "run"). They compare four procedures for assessing runs: 1) fully automatic assessments using the Umbrella LLM-based relevance assessment tool [12]; 2) fully manual assessments using established TREC evaluation protocols for human judgments; 3) a hybrid method where Umbrella filtered the set of passages to be judged; and 4) a hybrid method where humans refined UmbrelA's assessments. In this paper, we focus on the first two procedures: fully automatic and fully manual assessments. Based on their analysis, Upadhyay et al. [11] conclude:</p>
<p>Our results suggest that automatically generated UmbrelA judgments can replace fully manual judgments to accurately capture run-level effectiveness. Surprisingly, we find that LLM assistance does not appear to increase correlation with fully manual assessments, suggesting that costs associated with human-in-the-loop processes do not bring obvious tangible benefits... Our work validates the use of LLMs in academic TREC-style evaluations and provides the foundation for future studies.</p>
<p>We disagree. Not only does their reported correlation fail to provide stronger evidence than that of Faggioli et al. [6], but additional evidence from the track directly contradicts their conclusion. This evidence includes runs submitted by team WaterlooClarke, which were explicitly designed to subvert LLM-based relevance judgments by employing LLM-generated judgments as a final-stage ranker.</p>
<p>Faggioli et al. [6] already demonstrated a strong empirical correlation between manual judgments and LLM judgments, both in terms of inter-annotator agreement and leaderboard correlation, and many other work also observed this empirical correlation. However, after a detailed consideration of competing views, Faggioli et al. [6] concluded that there are too many theoretical concerns before human judgments can be replaced. These concerns, which remain critical to the discussion, have neither been addressed nor refuted in the work of Upadhyay et al. [11, 12].</p>
<p>Conceptually, there is no fundamental difference between an LLM-based relevance assessment and an LLM-based re-ranking method. Both predict an affinity score for a passage to be relevant for a given query. In contrast, human relevance judgments are privileged precisely because they are created by humans, and only humans can provide a gold standard for the evaluation of usefulness. While employing LLMs to train and implement rankers can lead to substantial performance gains, these improvements risk being illusory if they fail to reflect human judgments. The observation that LLM-based relevance judgments closely mimic the outcomes of human relevance judgments suggests that these LLM assessments may themselves represent a strong ranking method, rather than a valid evaluation metric.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Scatter plot extracted from Figure 3 of Upadhyay et al. [11] comparing manual assessment and automatic assessment. Each red dot represents the average performance of a run over all queries. The blue dots plot all run-query combination. The inset provides a closer view of the points in the green rectangle.</p>
<p>We acknowledge the value of work by Upadhyay et al. [12]. In particular, their use of a new collection with fresh queries, which guarantees that LLMs were not trained on this collection. Moreover, their work confirms that the correlation seen by Faggioli et al. [6] is not merely an artifact of training on the test collection. However, the warnings issued by Faggioli et al. [6] remain both valid and increasingly urgent, especially given the growing prevalence of LLM-based relevance assessments in information retrieval tasks.</p>
<h2>2 Reproduction of Umbrela Results</h2>
<p>Kendall's $\tau$ is a suitable metric for showing overall rank correlations on large-scale experiments. Upadhyay et al. [11] report a high overall Kendall's $\tau=0.89$ between manual relevance assessments and automatic Umbrela assessments. However, we note that some submitted systems perform substantially worse than others, making them easy to distinguish by an evaluation. The presence of such under-performing systems can artificially inflate Kendall's $\tau$ scores.</p>
<p>To investigate this effect, we extend the original analysis by excluding the bottom-ranked 15 systems (out of 75) and recomputing Kendall's $\tau$ over the remaining top 60 systems. As shown in Figure 2, this yields a slightly lower but still relatively high Kendall's $\tau$ of 0.84 . This corresponds to approximately $8 \%$ of system swaps, where the two evaluators disagree on which system performs better. Overall, these findings broadly confirm the results reported by Upadhyay et al. [11]. Nevertheless, we observe some notable outliers. For example, one system achieves a high LLM-based evaluation score ( 0.81 ) but a substantially lower manual evaluation score (0.63), as shown in Figure 2. We analyze such discrepancies further in Sections 4.1 and 4.2.</p>
<h2>3 Differences among Top-Performing Systems</h2>
<p>Demonstrating methodological advancements often involves reusing test collections with the goal of surpassing state-of-the-art systems. Identifying meaningful differences among top-performing
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Reproduction of Upadhyay et al. [11]: On top 60 original TREC RAG 24 systems and data, the Umbrela LLM evaluator correlates highly with manual assessors. Only few submitted retrieval systems included approaches from LLM evaluators. Each system represents one dot. Red dots mark systems provided by team WaterlooClarke, which are known to contain LLM evaluations for re-ranking.
runs is therefore critical for measuring significant progress. In contrast to the overall Kendall's $\tau$ of 0.89 between manual and automatic assessments, the correlation weakens substantially among the highest-scoring systems. While Kendall's $\tau$ among the top 60 systems ${ }^{1}$ remains relatively high at 0.85 (corresponding to $8 \%$ system swaps), it drops to $0.51(24 \%$ swaps) among the top 20 systems, and to $0.56(21 \%$ swaps) among the top 15 .</p>
<p>Thus, when using an LLM-based evaluator to demonstrate improvements over state-of-the-art systems, precise agreement with manual judgments is essential at the top of the leaderboard. A Kendall's $\tau \varrho 915$ of 0.56 suggests that automatic Umbrela assessments fail to demonstrate strong alignment with manual judgments at the top of the leaderboard. This misalignment undermines the reliability of automatic evaluations for tracking progress at the frontier of retrieval effectiveness.</p>
<p>Closer consideration of results from Upadhyay et al. [11] further highlights specific discrepancies in top-performing system rankings. Figure 1 reproduces a scatter plot extracted from Figure 3 of that paper, showing the performance of submitted runs (red dots) under manual assessment (y-axis) vs. automatic assessment (x-axis). ${ }^{2}$ Focusing on the top-performing systems (inset of Figure1), discrepancies become evident: for instance, the system ranked highest under automatic evaluation would only place fifth under manual evaluation. Conversely, the top system under manual evaluation ranks sixth under automatic evaluation. Particularly interesting is the case of the run circled in green. While it ranks fifth under automatic evaluation, it drops to 28th under manual evaluation.</p>
<p>These inconsistencies underscore a fundamental limitation of LLM-based assessments, such as those used by UmbreLA, in reliably</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Reranking with an LLM evaluator (Umbrella) improves performance under human relevance labels. This plot compares the original and reranked versions of all TREC RAG 24 systems based on manual assessment.
identifying the best-performing systems. As a result, caution is warranted when using these methods for evaluating and validating progress in retrieval tasks. While some differences could be the attributed to statistical noise - only 27 queries were manually judged, and no error bars are provided - the available evidence remains insufficient to justify the replacement of human judgments. This concern is particularly important when a collection is intended for re-use: claims of a novel system's superiority over existing methods must be supported by improvements that are both statistically meaningful and aligned with manual assessments. At present, such confidence is lacking.</p>
<h2>4 Subverting Automatic Evaluation</h2>
<p>When task relevance labels are generated entirely through a publicly known automatic process, such as Umbrella, the evaluation metric becomes vulnerable to manipulation. For instance, a participant could aggregate the outputs of many rankers, apply the UmbrelA system to this pooled set, and submit the resulting relevance labels as a new system for evaluation. Such a strategy could, in principle, achieve perfect scores across all metrics. Even if the specific LLM-based relevance assessment process includes undisclosed elements, such as the exact prompt or LLM used, participants could approximate the process enough to subvert the automatic evaluations.</p>
<h3>4.1 Empirical Demonstration of the Risk</h3>
<p>The run circled in green in Figure 1 exemplifies this vulnerability. Submitted by team WaterlooClarke as run uwc1, this submission was deliberately designed to subvert the automatic evaluation process. Specifically, the team pooled the top 20 documents from 15 preliminary runs, spanning neural and traditional rankers, with and without query expansion. This pool was then judged by GPT-40 with the prompt described by Faggioli et al. [6]. Top-graded passages were subsequently judged pairwise following the procedure of Clarke et al. [4] but substituting LLM-based assessments for crowdsourced human judgments.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Demonstration of the effects of circularity when using UmbrelA as both evluator and ranker using TREC RAG 24 data. Each submitted retrieval system is first re-ranked with UmbrelA, then evaluated under NDCG with relevance labels from human judges and the UmbrelA evaluator. We see that especially among top ranked systems, the evaluation strategy no longer agrees with human judges on which system is better. Axis ranges are adjusted to display the same top 60 (of 75) systems as in Figure 2.</p>
<p>The final ranking for uwc1 was determined using LLM-based preference judgments as the primary key, LLM-based relevance assessments as the secondary key, and the reciprocal rank fusion [5] of preliminary runs as a tertiary tie-breaker. As intended, this deliberate attempt to manipulate the evaluation process led uwc1 to rank significantly higher under automatic assessment (5th) than under manual assessment (28th).</p>
<h3>4.2 LLM-based Relevance Assessment for Re-Ranking</h3>
<p>As Soboroff [10] recognizes: "Retrieval and evaluation are the same problem. Asking a computer to decide if a document is relevant is no different than using a computer to retrieve documents and rank them in order of predicted degree of relevance." In this sense, LLMbased relevance assessment can be viewed as a specific form of LLMbased re-ranking. To employ an LLM-based relevance assessment tool as a LLM-based re-ranker, one starts with an initial ranking and prompts the LLM to assign a score - expressed as a relevance grade - to each of the top- 4 passages. These passages are then re-ranked according to their assigned relevance grades, preserving their original order among passages with equal grades.</p>
<p>This re-ranking interpretation is further illustrated by another run submitted by team WaterlooClarke, uwc2. For uwc2, the team re-ranked the track's baseline run using the prompt described by Arabzadeh and Clarke [2]. This re-ranking improves the baseline's performance from 9th to 4th place under manual assessments, and from 7th to 3rd place under automatic assessments.</p>
<h3>4.3 Simulation of Circularity</h3>
<p>To explore the potential effects of widespread adoption, we simulate what would have happened if TREC RAG 2024 systems had incorporated Umbrella as part of their pipeline. Following the methodology described in Section 4.2, we apply Umbrella as a final-stage reranker to the outputs of all submitted retrieval systems. For this experiment, we re-rank using the Umbrella relevance labels from the track itself. While the track organizers employed Umbrella for evaluation, the labels it produced could instead have been employed as a final-stage ranker. While we use the labels from the track organizers, any track participant could just as easily have employed Umbrella as a final-stage re-ranker themselves.</p>
<p>Comparing original and re-ranked systems on manual (i.e., human) judgments in Figure 3, we see that re-ranking with the Umbrella judgments consistently improves system performance. TREC and similar evaluation experiments generally allow participants to use any automatic re-ranking process for their submissions. Since Umbrella re-ranking is an entirely automatic process, it immediately loses its value as a measurement tool for those ranking experiments. If any automatic re-ranking process can be used, using the measurement tool itself provides an optimal re-ranking [10]. More generally, if system performance is measured solely by LLM-based tools, system developers are strongly incentivized to incorporate the same tools into their systems.</p>
<p>Once we adopt Umbrella as both a system component and an evaluation metric, it leads to an invalid circular evaluation. This effect is demonstrated in Figure 4, where the Umbrella-re-ranked runs from Figure 3 are evaluated with the same Umbrella relevance labels used for re-ranking and compared against evaluations based on human judgments. We observe a substantial increase in disagreement between the two evaluation methods, with discordant system pairs rising to $18 \%$ within the top 60 systems. As a result, Kendall's $\tau$ drops sharply to 0.63 . This degradation becomes even more pronounced at the top of the leaderboard: Kendall's $\tau$ further decreases to 0.44 among the top 20 systems, 0.49 among the top $15,0.38$ among the top 10 , and even turns negative ( $\tau=-0.40$ ) among the top 5 systems. Under Umbrella-based evaluation, twelve systems now obtain NDCG scores exceeding 0.95 , implying nearperfect ranking performance. Yet the same systems achieve manual NDCG scores only between 0.68 and 0.72 , illustrating substantial score inflation due to circularity.</p>
<p>For publication in peer-reviewed information retrieval research venues it is often necessary to demonstrate that a proposed system significantly outperforms all strong baselines. Under a circular evaluation, however, such findings would no longer be credible. These results highlight the risks of using LLM-based evaluation pipelines without safeguards against feedback loops, especially when test collections are intended for reuse. Taking manually created relevance labels as the gold standard, we conclude that evaluating systems using the Umbrella LLM assessor - when those systems internally apply Umbrella-based re-ranking - results in an invalid circular experimental evaluation.</p>
<h2>5 Automatic Judgments are not Gold Standards</h2>
<p>The prompts used to elicit relevance grades from LLM-based assessment tools resemble instructions typically given to human assessors. However, this resemblance is superficial and we should not be fooled by it. Such prompts merely represent one of many possible ways an LLM-based re-ranking method might assign scores, akin to an a LLM-based point-wise ranker. Despite being commonly referred to as relevance assessments, these scores are not equivalent to the judgments produced by humans.</p>
<p>LLM-based relevance assessments cannot serve as a gold standard because they lack the grounding of a human carrying out an information task necessary to evaluate the usefulness of retrieval systems. A true gold standard must originate from human assessments, as only humans can determine the relevance of information in a way that reflects real-world utility.</p>
<p>Faggioli et al. [6] raised concerns about the potential unknown biases inherent in LLM-based assessments. However, one clear and concerning bias is that LLM-based relevance assessments tend to favor LLM-based ranking systems. Recently, Balog et al. [3] report a detailed evaluation of how LLM-based rankers can influence LLMbased judges, providing the first empirical evidence that LLM judges exhibit "a clear and substantial bias in favor of LLM-based rankers." This bias has been observed in other contexts as well [8, 9, 13], where LLMs demonstrate a form of "narcissism," disproportionately favoring outputs generated by similar models. Furthermore, Alaofi et al. [1] show that LLMs can be deceived through well-crafted prompt attacks embedded in content, leading them to incorrectly judge irrelevant text as relevant. These vulnerabilities highlight not only the susceptibility of LLM-based assessments to manipulation but also their inability to objectively evaluate diverse ranking approaches. Such biases and flaws further undermine the reliability of LLM-based assessments as a substitute for human judgments in critical tasks.</p>
<h2>6 When Automatic Judgments become Useless</h2>
<p>While the uwc1 run demonstrates how a bad actor can strategically subvert an evaluation experiment, it is reasonable to assume that most participants are well-intentioned. These participants are not merely competing to win but are contributing to the creation of reusable test collections that support the development of innovative systems. However, even without malicious intent, the next generation of information systems will likely incorporate the latest advancements in LLMs, including prompting LLMs for relevance. As a result, some ranking methods will inherently embed elements that mirror LLM-based relevance judgments. In a future evaluation experiment, it is plausible that even a well-intentioned participant could inadvertently undermine the evaluation process.</p>
<p>Looking ahead, we anticipate that future retrieval systems will increasingly rely on automatically generated training data to optimize machine learning components. Here, Goodhart's law serves as a cautionary principle [7]: "When a measure becomes a target, it ceases to be a good measure." While the current observed correlation between manual and automatic assessment methods is strong, we predict that this correlation will degrade as developers incorporate LLM-based evaluation components into their systems in more refined ways than our simulation in Section 4.3. Over time, these</p>
<p>systems risk becoming disconnected from the human judgments they are intended to serve. If the entire end-to-end experimental pipeline - from query formulation to relevance labeling - is fully automated, the evaluation process devolves into an LLM assessing its own assessments. The circularity feared by Faggioli et al. [6] is no longer a hypothetical concern; it has already begun to manifest in practice.</p>
<h2>7 Conclusion</h2>
<p>This paper raises serious concerns about the claims made by Upadhyay et al. [11], which presents a preliminary analysis of data from the retrieval ("R") task of the TREC 2024 RAG Track. The author list of Upadhyay et al. [11] includes some of the most prominent experts in the area of information retrieval evaluation. Despite being preliminary, their conclusions strongly imply that LLM-based relevance assessment can replace human relevance assessment - a claim that does not withstand scrutiny. Given the authority of the authors and the strength of their implied conclusions, there is a risk that these findings may gain widespread acceptance within the research community without sufficient critical consideration. Nearly two years ago, Faggioli et al. [6] reached the opposite conclusion based on similar evidence. Their concerns have still not been addressed.</p>
<h2>Acknowledgments</h2>
<p>We have discussed our concerns with some of the authors of Upadhyay et al. [11], and we appreciate their attention and feedback. They also generously provided us with early access to their data to confirm factual statements in this paper.</p>
<p>This material is based in part upon work supported by the National Science Foundation under Grant No. 1846017. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
<h2>References</h2>
<p>[1] Marwah Alaofi, Paul Thomas, Falk Scholer, and Mark Sanderson. 2024. LLMs can be Pooled into Labelling a Document as Relevant: best café near me; this paper is perfectly relevant. In Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region. 32-41.
[2] Negar Arabzadeh and Charles L. A. Clarke. 2024. A Comparison of Methods for Evaluating Generative IR. arXiv:2404.04044 [cs.IR] https://arxiv.org/abs/2404. 04044
[3] Krisztian Balog, Donald Metzler, and Zhen Qin. 2025. Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation. arXiv:2503.19092 [cs.IR] https://arxiv.org/abs/2503.19092
[4] Charles L. A. Clarke, Alexandra Vtyurina, and Mark D. Smucker. 2021. Assessing Top-4 Preferences. ACM Transactions on Information Systems, Article 33 (May' 2021), 21 pages.
[5] Gordon V. Cormack, Charles L A Clarke, and Stefan Buettcher. 2009. Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods. In 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 758-759.
[6] Guglielmo Faggioli, Laura Durz, Charles L. A. Clarke, Gianluca Demartini, Matthias Hagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, Benno Stein, and Henning Wachsmuth. 2023. Perspectives on Large Language Models for Relevance Judgment. In SIGIR International Conference on Theory of Information Retrieval. 39-50.
[7] Charles Goodhart. 1975. Problems of Monetary Management: The UK experience in papers in monetary economics. Monetary Economics 1 (1975).
[8] Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin. 2023. LLMs as narcissistic evaluators: When ego inflates evaluation scores. arXiv preprint arXiv:2311.09766 (2023).
[9] Arjun Panickssery, Samuel R Bowman, and Shi Feng. 2024. LLM Evaluators Recognize and Favor Their Own Generations. arXiv preprint arXiv:2404.13076 (2024).
[10] Ian Soboroff. 2025. Don't Use LLMs to Make Relevance Judgments. Information Retrieval Research 1, 1 (March 2025), 29-46.
[11] Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Daniel Campos, Nick Craswell, Ian Soboroff, Hoa Trang Dang, and Jimmy Lin. 2024. A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look. arXiv:2411.08275 [cs.IR] https://arxiv.org/abs/2411.08275
[12] Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Nick Craswell, and Jimmy Lin. 2024. UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor. arXiv:2406.06519 [cs.IR] https://arxiv.org/abs/2406.06519
[13] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large Language Models are not Fair Evaluators. arXiv preprint arXiv:2305.17926 (2023).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Throughout the paper, when we refer to "top-performing systems", we use the manual relevance judgments as a basis for this assessment.
${ }^{2}$ At the time of writing, Upadhyay et al. [11] do not provide a full public data release. They have generously provided limited access to their data for the purpose of confirming factual statements in this paper.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
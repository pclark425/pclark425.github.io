<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1084 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1084</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1084</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-3f56ac0e4b881d25268e83961b93ee95f2807bfb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3f56ac0e4b881d25268e83961b93ee95f2807bfb" target="_blank">CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> CURIOUS is proposed, an algorithm that leverages a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1084.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1084.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CURIOUS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CURIOUS (Continual Universal Reinforcement learning with Intrinsically mOtivated sUbstitutionS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsically motivated modular multi-goal RL agent that combines a modular UVFA policy (M-UVFA), hindsight goal substitution (HER), prioritized interest replay, and an active module-selection mechanism driven by absolute learning progress to self-organize curricula and allocate training resources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CURIOUS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied agent using off-policy deep reinforcement learning (DDPG) with Universal Value Function Approximation extended to modular goals (M-UVFA), Hindsight Experience Replay (goal substitution), Prioritized Interest Replay, and an intrinsically motivated module selection policy based on absolute learning progress (LP).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robotic arm simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Modular Goal Fetch Arm</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated 7-DoF Fetch robotic arm facing 2 cubes on a table (plus optional additional out-of-reach or moving cubes as distractors). The agent controls gripper 3D velocity and gripper opening (4D action). Goals are modular (Reach: 3D gripper target; Push: 2D table target for cube; Pick-and-Place: 3D cube target; Stack: stack cube1 on cube2). Environment complexity arises from multiple goal modules, multi-object interactions, sparse rewards, and optionally additional distracting out-of-reach moving cubes providing noisy inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of goal modules (N, here 4 achievable + up to 7 distracting), number of objects (2 primary cubes + optional additional distractors), observation dimensionality (40 + 3 per extra cube), action dimensionality (4), and task difficulty (hierarchical dependencies: Reach -> Push -> Pick-and-Place -> Stack).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high (experiments used 4 achievable modules; authors state scaling tested up to at least 10 achievable modules but note eventual failure as modules grow large).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of distracting (impossible/out-of-reach, randomly moving) modules (tested values: 0, 4, 7); sensor perturbation magnitude (time-locked perceptual shift of 0.05 simulation units); presence/absence of body/sensor changes; randomized initial object positions across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high (experiments vary distractor count from 0 up to 7 and include time-locked sensory perturbations of 0.05 units).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate over achievable goals (offline evaluation), measured as fraction of successful rollouts; recovery time to reach 95% of pre-perturbation performance (in number of episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Significant improvements over random module selection: e.g., recovery to 95% of pre-perturbation performance in 43,000 episodes for CURIOUS vs 78,000 episodes for random/M-UVFA (≈45% faster recovery; p < 1e-4). Learning speed: CURIOUS with M-UVFA architecture achieved learning of achievable goals in ≈250,000 episodes (compared to ≈450,000 for some baselines as reported). Exact final success rates not reported as single scalar in text (learning curves shown).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses trade-offs: increasing the number of modules and distracting (impossible) modules slows down learning for random module selection roughly proportionally to (#achievable modules)/(#modules). CURIOUS mitigates this by allocating attention away from distracting/unsolvable modules using LP-based selection. Monolithic policies (M-UVFA) can reuse representations across modules (benefit with increasing complexity), but are more prone to catastrophic forgetting as modules grow, which LP-based active selection partially addresses. The paper also shows that sensory/body variations (a 0.05 perceptual shift) cause drops in competence which CURIOUS detects via LP and more rapidly reallocates training to recover.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Intrinsic-motivation-driven curriculum learning (IMGEP): active module selection via proportional probability matching on absolute Learning Progress (LP) combined with off-policy multi-goal learning (DDPG + HER) and Prioritized Interest Replay (episode-level prioritization when an outcome changed).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>CURIOUS was tested for robustness/generalization to changes in environment/body: (1) sensor perturbation (perceptual shift of cube position by 0.05) caused a drop in performance on the affected module; CURIOUS recovered to 95% of pre-perturbation performance in 43k episodes vs 78k for the random baseline (≈45% faster, p < 1e-4). (2) When the number of distracting modules increased (0, 4, 7), CURIOUS maintained faster learning on achievable goals and its advantage over random sampling grew with number of distractors. These tests show robustness to distributional changes and distracting variation but do not include transfer to completely new environments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Measured in episodes: learning achievable goals required on the order of 250k episodes for M-UVFA+CURIOUS to reach competent performance in the paper's setup (baselines required up to ≈450k episodes for comparable behavior in some comparisons). Recovery times quantified in tens of thousands of episodes (43k vs 78k). Computationally, one trial takes ~20 hours on 19 CPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Active module selection based on absolute learning progress (CURIOUS) self-organizes developmental curricula (sequential phases of learning simpler to harder modules). 2) CURIOUS reduces wasted exploration on impossible/distracting modules and speeds learning as number of distractors increases. 3) CURIOUS detects and mitigates catastrophic forgetting and adapts to sensor/body perturbations faster than random module selection (45% faster recovery to 95% performance in reported test). 4) M-UVFA monolithic modular policy enables transfer across modules via shared representations but is more prone to forgetting; LP-based selection partially alleviates this. 5) Scaling tested up to ~10 achievable modules but authors caution potential failure with much larger module sets and propose architectural extensions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1084.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1084.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>M-UVFA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular Universal Value Function Approximator (M-UVFA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular extension of UVFA where the goal vector is structured into module-specific subspaces and a one-hot module descriptor masks inputs so that a single monolithic actor-critic network can address diverse modular goals while freezing weights for inactive modules during backpropagation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>M-UVFA (monolithic modular policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Monolithic modular goal-parameterized policy & critic trained with off-policy RL (DDPG) and HER; feeds state, modular goal vector (masking non-active modules), and one-hot module descriptor into actor and critic neural networks to learn multiple goal modules within one policy.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robotic arm simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Modular Goal Fetch Arm</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same as CURIOUS: simulated Fetch arm with multiple modular goals (Reach, Push, Pick-and-Place, Stack) and optional distractor cubes; complexity comes from multiple distinct goal types and object interactions requiring different constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of modules; goal dimensionalities per module (e.g., Reach 3D, Push 2D, Pick-and-Place 3D, Stack constraints); observation dim (40 + 3 per extra cube); action dim 4.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high (designed to handle distinct goal types simultaneously; experiments used 4 achievable modules + distractors).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Same variation axes as environment: distractor count (0,4,7), sensor perturbations, randomized initial object positions.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate over achievable goals (offline evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>M-UVFA with CURIOUS reached learned performance on achievable goals in ≈250,000 episodes in reported comparisons; M-UVFA with random module selection performed worse in presence of distractors and recovered slower from perturbations (e.g., 78k episodes to 95% recovery vs 43k for active CURIOUS).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — M-UVFA leverages shared representations to transfer across modules (benefit as complexity increases), but monolithic shared networks are more susceptible to catastrophic forgetting as the number of modules/variations increases; active LP-based selection helps prioritize relearning and manage variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Off-policy multi-goal RL (DDPG) with goal-parameterized UVFA-style inputs, Hindsight Experience Replay, and optional LP-driven module selection when combined with CURIOUS.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>M-UVFA architecture used in experiments showed improved sample efficiency and cross-module transfer compared to module-expert baselines, but when coupled with random module selection was slower to recover from sensor perturbations and more affected by distracting modules; combining with LP-based CURIOUS improved resilience.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Learning of achievable goals in ≈250k episodes when used with CURIOUS (reported), compared to ≈450k episodes for some baselines in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>M-UVFA enables representing and learning diverse modular goals within one policy and benefits from shared learned representations across modules, improving learning speed relative to separate-expert approaches; however, monolithic policies are more prone to forgetting, motivating the use of LP-driven attention and replay.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1084.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1084.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HER baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hindsight Experience Replay (HER) + DDPG (flat multi-goal baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flat goal-parameterized UVFA-style baseline that samples holistic goals from a single combined goal space and uses HER to substitute goals with achieved outcomes to increase reward signal in sparse-reward settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hindsight experience replay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HER + DDPG (flat multi-goal baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Flat multi-goal agent using UVFA-style concatenation of goal and state (non-modular), trained with DDPG and Hindsight Experience Replay; selects goals uniformly from a holistic goal space that combines all module goal spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robotic arm simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Modular Goal Fetch Arm</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same simulated Fetch-arm environment but with goals sampled uniformly from a holistic product goal space (concatenation of all module goal spaces) which often produces impossible combinations of constraints (e.g., simultaneous independent goals across modules).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Holistic goal space dimensionality (product of module goal spaces), resulting combinatorial complexity of satisfying all module constraints simultaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (when represented as a flat product space, many goals are infeasible because they require simultaneously satisfying multiple modules).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>No special variation measure beyond environmental randomness; baseline experiments included distractors and perturbations like other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate over achievable goals (offline evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>HER baseline showed effectively no learning in this modular environment (learning curve stays flat) because goals sampled from the holistic product space rarely correspond to achievable situations; exact numeric success rate not given but described as flat/no learning.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper highlights that using a flat/homogeneous goal representation explodes goal-space complexity (many goals infeasible) and causes failure: as complexity of the goal representation increases (holistic product over modules), performance collapses; this is a case where complexity (combined constraints) without proper modularization leads to failure.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Off-policy multi-goal RL (DDPG) with Hindsight Experience Replay; flat goal sampling uniformly from the full product goal space (no modular goal structure or LP-based curriculum).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Not applicable; baseline failed to learn in the modular environment due to infeasible goals arising from the flat goal representation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not meaningful (no effective learning observed in this setup).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A flat (non-modular) goal representation that samples goals uniformly from the product of module goal spaces can make most sampled goals infeasible and lead to failure to learn; motivates the need for modular goal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1084.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1084.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MG-ME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Goal Module-Experts (MG-ME)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline where an expert multi-goal policy is trained per module (one policy per module), each trained on its designated module and sharing collected transitions with other experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MG-ME (multi-goal module-experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture with N separate multi-goal policies (one per module), each policy trained primarily on its module (rotating training epochs) while sharing experience; uses off-policy learning and HER like other methods in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robotic arm simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Modular Goal Fetch Arm</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same modular Fetch-arm environment with multiple modules; MG-ME divides capacity across multiple expert policies, each specialized to a module, which reduces interference but limits shared representation reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of modules (N) and per-module goal dimensionalities; training scheduling complexity (each policy trained 1 epoch every N epochs).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Same environmental variations tested (distractor modules, sensor perturbations) though MG-ME was compared on baseline learning speed and final competence.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate over achievable goals (offline evaluation) and sample complexity (episodes to learn achievable goals).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MG-ME learned achievable goals more slowly than M-UVFA+CURIOUS in reported comparisons; the text reports learning of achievable goals on the order of ≈450,000 episodes for the slower approach vs ≈250,000 for the faster one (context indicates MG-ME was the slower approach).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>MG-ME reduces catastrophic interference by separating policies per module (benefit as variation in tasks increases), but loses the advantages of shared representation reuse (worse sample efficiency) when modules are similar; trade-off between specialization (less forgetting) and shared generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multi-expert multi-task training (each expert trained on its module in rotation), off-policy replay and HER, shared replay buffers for transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Compared empirically: MG-ME was slower than the M-UVFA approach in learning achievable goals in the tested environment; no explicit tests of generalization to new environments reported for MG-ME in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported to require more episodes (≈450k) to learn achievable goals compared to M-UVFA+CURIOUS (≈250k) in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Specialized module-expert policies can avoid some forgetting but in this setup were less sample efficient than a monolithic M-UVFA that leverages shared representations; this motivates the monolithic modular policy combined with LP-based attention despite its higher forgetting risk.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active learning of inverse models with intrinsically motivated goal exploration in robots <em>(Rating: 2)</em></li>
                <li>Intrinsically motivated goal exploration processes with automatic curriculum learning <em>(Rating: 2)</em></li>
                <li>Hindsight experience replay <em>(Rating: 2)</em></li>
                <li>Modular active curiosity-driven discovery of tool use <em>(Rating: 1)</em></li>
                <li>Universal value function approximators <em>(Rating: 2)</em></li>
                <li>Unicorn: Continual learning with a universal, off-policy agent <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1084",
    "paper_id": "paper-3f56ac0e4b881d25268e83961b93ee95f2807bfb",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "CURIOUS",
            "name_full": "CURIOUS (Continual Universal Reinforcement learning with Intrinsically mOtivated sUbstitutionS)",
            "brief_description": "An intrinsically motivated modular multi-goal RL agent that combines a modular UVFA policy (M-UVFA), hindsight goal substitution (HER), prioritized interest replay, and an active module-selection mechanism driven by absolute learning progress to self-organize curricula and allocate training resources.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CURIOUS",
            "agent_description": "Embodied agent using off-policy deep reinforcement learning (DDPG) with Universal Value Function Approximation extended to modular goals (M-UVFA), Hindsight Experience Replay (goal substitution), Prioritized Interest Replay, and an intrinsically motivated module selection policy based on absolute learning progress (LP).",
            "agent_type": "simulated agent (robotic arm simulation)",
            "environment_name": "Modular Goal Fetch Arm",
            "environment_description": "Simulated 7-DoF Fetch robotic arm facing 2 cubes on a table (plus optional additional out-of-reach or moving cubes as distractors). The agent controls gripper 3D velocity and gripper opening (4D action). Goals are modular (Reach: 3D gripper target; Push: 2D table target for cube; Pick-and-Place: 3D cube target; Stack: stack cube1 on cube2). Environment complexity arises from multiple goal modules, multi-object interactions, sparse rewards, and optionally additional distracting out-of-reach moving cubes providing noisy inputs.",
            "complexity_measure": "Number of goal modules (N, here 4 achievable + up to 7 distracting), number of objects (2 primary cubes + optional additional distractors), observation dimensionality (40 + 3 per extra cube), action dimensionality (4), and task difficulty (hierarchical dependencies: Reach -&gt; Push -&gt; Pick-and-Place -&gt; Stack).",
            "complexity_level": "medium-to-high (experiments used 4 achievable modules; authors state scaling tested up to at least 10 achievable modules but note eventual failure as modules grow large).",
            "variation_measure": "Number of distracting (impossible/out-of-reach, randomly moving) modules (tested values: 0, 4, 7); sensor perturbation magnitude (time-locked perceptual shift of 0.05 simulation units); presence/absence of body/sensor changes; randomized initial object positions across episodes.",
            "variation_level": "medium-to-high (experiments vary distractor count from 0 up to 7 and include time-locked sensory perturbations of 0.05 units).",
            "performance_metric": "Success rate over achievable goals (offline evaluation), measured as fraction of successful rollouts; recovery time to reach 95% of pre-perturbation performance (in number of episodes).",
            "performance_value": "Significant improvements over random module selection: e.g., recovery to 95% of pre-perturbation performance in 43,000 episodes for CURIOUS vs 78,000 episodes for random/M-UVFA (≈45% faster recovery; p &lt; 1e-4). Learning speed: CURIOUS with M-UVFA architecture achieved learning of achievable goals in ≈250,000 episodes (compared to ≈450,000 for some baselines as reported). Exact final success rates not reported as single scalar in text (learning curves shown).",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses trade-offs: increasing the number of modules and distracting (impossible) modules slows down learning for random module selection roughly proportionally to (#achievable modules)/(#modules). CURIOUS mitigates this by allocating attention away from distracting/unsolvable modules using LP-based selection. Monolithic policies (M-UVFA) can reuse representations across modules (benefit with increasing complexity), but are more prone to catastrophic forgetting as modules grow, which LP-based active selection partially addresses. The paper also shows that sensory/body variations (a 0.05 perceptual shift) cause drops in competence which CURIOUS detects via LP and more rapidly reallocates training to recover.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Intrinsic-motivation-driven curriculum learning (IMGEP): active module selection via proportional probability matching on absolute Learning Progress (LP) combined with off-policy multi-goal learning (DDPG + HER) and Prioritized Interest Replay (episode-level prioritization when an outcome changed).",
            "generalization_tested": true,
            "generalization_results": "CURIOUS was tested for robustness/generalization to changes in environment/body: (1) sensor perturbation (perceptual shift of cube position by 0.05) caused a drop in performance on the affected module; CURIOUS recovered to 95% of pre-perturbation performance in 43k episodes vs 78k for the random baseline (≈45% faster, p &lt; 1e-4). (2) When the number of distracting modules increased (0, 4, 7), CURIOUS maintained faster learning on achievable goals and its advantage over random sampling grew with number of distractors. These tests show robustness to distributional changes and distracting variation but do not include transfer to completely new environments.",
            "sample_efficiency": "Measured in episodes: learning achievable goals required on the order of 250k episodes for M-UVFA+CURIOUS to reach competent performance in the paper's setup (baselines required up to ≈450k episodes for comparable behavior in some comparisons). Recovery times quantified in tens of thousands of episodes (43k vs 78k). Computationally, one trial takes ~20 hours on 19 CPUs.",
            "key_findings": "1) Active module selection based on absolute learning progress (CURIOUS) self-organizes developmental curricula (sequential phases of learning simpler to harder modules). 2) CURIOUS reduces wasted exploration on impossible/distracting modules and speeds learning as number of distractors increases. 3) CURIOUS detects and mitigates catastrophic forgetting and adapts to sensor/body perturbations faster than random module selection (45% faster recovery to 95% performance in reported test). 4) M-UVFA monolithic modular policy enables transfer across modules via shared representations but is more prone to forgetting; LP-based selection partially alleviates this. 5) Scaling tested up to ~10 achievable modules but authors caution potential failure with much larger module sets and propose architectural extensions.",
            "uuid": "e1084.0",
            "source_info": {
                "paper_title": "CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "M-UVFA",
            "name_full": "Modular Universal Value Function Approximator (M-UVFA)",
            "brief_description": "A modular extension of UVFA where the goal vector is structured into module-specific subspaces and a one-hot module descriptor masks inputs so that a single monolithic actor-critic network can address diverse modular goals while freezing weights for inactive modules during backpropagation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "M-UVFA (monolithic modular policy)",
            "agent_description": "Monolithic modular goal-parameterized policy & critic trained with off-policy RL (DDPG) and HER; feeds state, modular goal vector (masking non-active modules), and one-hot module descriptor into actor and critic neural networks to learn multiple goal modules within one policy.",
            "agent_type": "simulated agent (robotic arm simulation)",
            "environment_name": "Modular Goal Fetch Arm",
            "environment_description": "Same as CURIOUS: simulated Fetch arm with multiple modular goals (Reach, Push, Pick-and-Place, Stack) and optional distractor cubes; complexity comes from multiple distinct goal types and object interactions requiring different constraints.",
            "complexity_measure": "Number of modules; goal dimensionalities per module (e.g., Reach 3D, Push 2D, Pick-and-Place 3D, Stack constraints); observation dim (40 + 3 per extra cube); action dim 4.",
            "complexity_level": "medium-to-high (designed to handle distinct goal types simultaneously; experiments used 4 achievable modules + distractors).",
            "variation_measure": "Same variation axes as environment: distractor count (0,4,7), sensor perturbations, randomized initial object positions.",
            "variation_level": "medium",
            "performance_metric": "Success rate over achievable goals (offline evaluation).",
            "performance_value": "M-UVFA with CURIOUS reached learned performance on achievable goals in ≈250,000 episodes in reported comparisons; M-UVFA with random module selection performed worse in presence of distractors and recovered slower from perturbations (e.g., 78k episodes to 95% recovery vs 43k for active CURIOUS).",
            "complexity_variation_relationship": "Yes — M-UVFA leverages shared representations to transfer across modules (benefit as complexity increases), but monolithic shared networks are more susceptible to catastrophic forgetting as the number of modules/variations increases; active LP-based selection helps prioritize relearning and manage variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Off-policy multi-goal RL (DDPG) with goal-parameterized UVFA-style inputs, Hindsight Experience Replay, and optional LP-driven module selection when combined with CURIOUS.",
            "generalization_tested": true,
            "generalization_results": "M-UVFA architecture used in experiments showed improved sample efficiency and cross-module transfer compared to module-expert baselines, but when coupled with random module selection was slower to recover from sensor perturbations and more affected by distracting modules; combining with LP-based CURIOUS improved resilience.",
            "sample_efficiency": "Learning of achievable goals in ≈250k episodes when used with CURIOUS (reported), compared to ≈450k episodes for some baselines in the paper's comparisons.",
            "key_findings": "M-UVFA enables representing and learning diverse modular goals within one policy and benefits from shared learned representations across modules, improving learning speed relative to separate-expert approaches; however, monolithic policies are more prone to forgetting, motivating the use of LP-driven attention and replay.",
            "uuid": "e1084.1",
            "source_info": {
                "paper_title": "CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "HER baseline",
            "name_full": "Hindsight Experience Replay (HER) + DDPG (flat multi-goal baseline)",
            "brief_description": "A flat goal-parameterized UVFA-style baseline that samples holistic goals from a single combined goal space and uses HER to substitute goals with achieved outcomes to increase reward signal in sparse-reward settings.",
            "citation_title": "Hindsight experience replay",
            "mention_or_use": "use",
            "agent_name": "HER + DDPG (flat multi-goal baseline)",
            "agent_description": "Flat multi-goal agent using UVFA-style concatenation of goal and state (non-modular), trained with DDPG and Hindsight Experience Replay; selects goals uniformly from a holistic goal space that combines all module goal spaces.",
            "agent_type": "simulated agent (robotic arm simulation)",
            "environment_name": "Modular Goal Fetch Arm",
            "environment_description": "Same simulated Fetch-arm environment but with goals sampled uniformly from a holistic product goal space (concatenation of all module goal spaces) which often produces impossible combinations of constraints (e.g., simultaneous independent goals across modules).",
            "complexity_measure": "Holistic goal space dimensionality (product of module goal spaces), resulting combinatorial complexity of satisfying all module constraints simultaneously.",
            "complexity_level": "high (when represented as a flat product space, many goals are infeasible because they require simultaneously satisfying multiple modules).",
            "variation_measure": "No special variation measure beyond environmental randomness; baseline experiments included distractors and perturbations like other methods.",
            "variation_level": "medium",
            "performance_metric": "Success rate over achievable goals (offline evaluation).",
            "performance_value": "HER baseline showed effectively no learning in this modular environment (learning curve stays flat) because goals sampled from the holistic product space rarely correspond to achievable situations; exact numeric success rate not given but described as flat/no learning.",
            "complexity_variation_relationship": "The paper highlights that using a flat/homogeneous goal representation explodes goal-space complexity (many goals infeasible) and causes failure: as complexity of the goal representation increases (holistic product over modules), performance collapses; this is a case where complexity (combined constraints) without proper modularization leads to failure.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Off-policy multi-goal RL (DDPG) with Hindsight Experience Replay; flat goal sampling uniformly from the full product goal space (no modular goal structure or LP-based curriculum).",
            "generalization_tested": false,
            "generalization_results": "Not applicable; baseline failed to learn in the modular environment due to infeasible goals arising from the flat goal representation.",
            "sample_efficiency": "Not meaningful (no effective learning observed in this setup).",
            "key_findings": "A flat (non-modular) goal representation that samples goals uniformly from the product of module goal spaces can make most sampled goals infeasible and lead to failure to learn; motivates the need for modular goal representations.",
            "uuid": "e1084.2",
            "source_info": {
                "paper_title": "CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "MG-ME",
            "name_full": "Multi-Goal Module-Experts (MG-ME)",
            "brief_description": "A baseline where an expert multi-goal policy is trained per module (one policy per module), each trained on its designated module and sharing collected transitions with other experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MG-ME (multi-goal module-experts)",
            "agent_description": "Architecture with N separate multi-goal policies (one per module), each policy trained primarily on its module (rotating training epochs) while sharing experience; uses off-policy learning and HER like other methods in the paper.",
            "agent_type": "simulated agent (robotic arm simulation)",
            "environment_name": "Modular Goal Fetch Arm",
            "environment_description": "Same modular Fetch-arm environment with multiple modules; MG-ME divides capacity across multiple expert policies, each specialized to a module, which reduces interference but limits shared representation reuse.",
            "complexity_measure": "Number of modules (N) and per-module goal dimensionalities; training scheduling complexity (each policy trained 1 epoch every N epochs).",
            "complexity_level": "medium",
            "variation_measure": "Same environmental variations tested (distractor modules, sensor perturbations) though MG-ME was compared on baseline learning speed and final competence.",
            "variation_level": "medium",
            "performance_metric": "Success rate over achievable goals (offline evaluation) and sample complexity (episodes to learn achievable goals).",
            "performance_value": "MG-ME learned achievable goals more slowly than M-UVFA+CURIOUS in reported comparisons; the text reports learning of achievable goals on the order of ≈450,000 episodes for the slower approach vs ≈250,000 for the faster one (context indicates MG-ME was the slower approach).",
            "complexity_variation_relationship": "MG-ME reduces catastrophic interference by separating policies per module (benefit as variation in tasks increases), but loses the advantages of shared representation reuse (worse sample efficiency) when modules are similar; trade-off between specialization (less forgetting) and shared generalization.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Multi-expert multi-task training (each expert trained on its module in rotation), off-policy replay and HER, shared replay buffers for transitions.",
            "generalization_tested": false,
            "generalization_results": "Compared empirically: MG-ME was slower than the M-UVFA approach in learning achievable goals in the tested environment; no explicit tests of generalization to new environments reported for MG-ME in this paper.",
            "sample_efficiency": "Reported to require more episodes (≈450k) to learn achievable goals compared to M-UVFA+CURIOUS (≈250k) in the paper's experiments.",
            "key_findings": "Specialized module-expert policies can avoid some forgetting but in this setup were less sample efficient than a monolithic M-UVFA that leverages shared representations; this motivates the monolithic modular policy combined with LP-based attention despite its higher forgetting risk.",
            "uuid": "e1084.3",
            "source_info": {
                "paper_title": "CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning",
                "publication_date_yy_mm": "2018-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
            "rating": 2
        },
        {
            "paper_title": "Intrinsically motivated goal exploration processes with automatic curriculum learning",
            "rating": 2
        },
        {
            "paper_title": "Hindsight experience replay",
            "rating": 2
        },
        {
            "paper_title": "Modular active curiosity-driven discovery of tool use",
            "rating": 1
        },
        {
            "paper_title": "Universal value function approximators",
            "rating": 2
        },
        {
            "paper_title": "Unicorn: Continual learning with a universal, off-policy agent",
            "rating": 1
        }
    ],
    "cost": 0.015352749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning</h1>
<p>Cédric Colas ${ }^{1}$ Pierre Fournier ${ }^{2}$ Olivier Sigaud ${ }^{2}$ Mohamed Chetouani ${ }^{2}$ Pierre-Yves Oudeyer ${ }^{1}$</p>
<h4>Abstract</h4>
<p>In open-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove easy and some impossible, agents must actively select which goal to practice at any moment, to maximize their overall mastery on the set of learnable goals. This paper proposes CURIOUS, an algorithm that leverages 1) a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress. Agents focus sequentially on goals of increasing complexity, and focus back on goals that are being forgotten. Experiments conducted in a new modular-goal robotic environment show the resulting developmental self-organization of a learning curriculum, and demonstrate properties of robustness to distracting goals, forgetting and changes in body properties.</p>
<h2>1. Introduction</h2>
<p>In autonomous continual learning, agents aim to discover repertoires of skills in an ever-changing open-ended world, and without external rewards. In such realistic environments, the agent must be endowed with intrinsic motivations to explore the diversity of ways in which it can control its environment. One important form of intrinsic motivation system is the ability to autonomously set one's own goals</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. The Modular Goal Fetch Arm environment. An intrinsically motivated agent can set its own (modular) goals (Reach, Push, Pick and Place, Stack), with multiple objects and distractors.
and self-organize one's own curriculum. This challenge can be tackled within the framework of Intrinsically Motivated Goal Exploration Processes (IMGEP) (Baranes \&amp; Oudeyer, 2013; Forestier et al., 2017), leveraging computational models of autonomous development in human infants.</p>
<p>Modular goal representation. In a same environment, an agent might want to 'put the cube in position $x$ ' or to 'reach position $y$ ' for any $x$ or $y$. Here, describing the full goal space requires modular goal representations. Goals are organized by modules, where module refers to the pair of a reward function and a goal space $M_{i}=\left(R_{M_{i}, g_{i} \in \mathcal{G}<em i="i">{\mathcal{M}</em>}}}, \mathcal{G<em i="i">{\mathcal{M}</em>=y$ ) evolving in the associated goal space (e.g. 3D Euclidean space), see Fig. 1.}}\right)$. The reward function describes a set of constraints that must be satisfied by the agent's state (e.g. Reach), given a continuous parameter (e.g. $g_{i</p>
<p>While flat multi-goal problems with continuous (Schaul et al., 2015a; Andrychowicz et al., 2017; Plappert et al., 2018) or discrete goals (Mankowitz et al., 2018; Riedmiller et al., 2018) have been explored in the past, only few works tackle the problem of modular multi-goal learning (Forestier \&amp; Oudeyer, 2016), none in an RL setting. Here, we present CURIOUS ${ }^{1}$, a modular multi-goal reinforcement learning (RL) algorithm that uses intrinsic motivations to efficiently learn a continuous set of diverse goals using modular goal representations. To build an algorithm able to learn modular goals, one must answer the following questions: 1) How to choose the action policy architecture? 2) How to select</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>the next module and goal to practice and learn about? 3) How to efficiently transfer knowledge between modules and goals?</p>
<p>Related work. Kaelbling (1993) proposed the first algorithm able to leverage cross-goal learning to address a discrete set of goals. For each goal, the algorithm learned a specific value function using Q-learning (goal-experts approach). More recently, Schaul et al. (2015a) proposed Universal Value Function Approximators (UVFA), a unique policy able to address an infinity of goals by concatenating the current state and goal to feed both the policy and the value function. In UNICORN, UVFA is used to address a discrete set of goals in parallel: reaching different objects in a visual world (Mankowitz et al., 2018). SAC-X implements multi-task RL where easy tasks are considered as auxiliary tasks to help learning about the hardest task (placing cubes inside a closed box) (Riedmiller et al., 2018). Here, one network is trained for each task and the collected transitions are shared (goal-experts approach). In other works from multi-task RL (Teh et al., 2017; Espeholt et al., 2018; Hessel et al., 2018), agents do not represent explicitly the current task and aim at maximizing the overall reward. Finally, within the Intrinsically Motivated Goal Exploration Processes framework (IMGEP), Forestier \&amp; Oudeyer (2016) proposed MACOB, an algorithm able to target modular goals using a population-based algorithm that mutates and replays controllers experienced in the past. MACOB maintains a population of solutions, one for each goal (modular goalexperts approach), see Nguyen \&amp; Oudeyer (2012) for a similar approach. This enables efficient cross-goal learning in high-dimensional goal spaces, but is limited by the memory-based representation of policies.</p>
<p>Multi-goal approaches prove better than simply training a policy per goal because knowledge can be transferred between different goals using off-policy learning and hindsight learning (Andrychowicz et al., 2017). Off-policy learning enables the use of any transition to improve the current policy: transitions collected from an older version of the current policy (Lillicrap et al., 2015), from a population of exploratory policies (Colas et al., 2018), or even from demonstrations (Večerík et al., 2017). Transitions collected while aiming at a particular goal can be used to learn about any other. With finite sets of goals, each transition is generally used to update the policy on every other goal (Mankowitz et al., 2018; Kaelbling, 1993). With continuous sets of goals, imaginary goals are sampled from the goal space (Andrychowicz et al., 2017). In the case of UVFA policies, this consists in the substitution of the goal that is part of the input by the imaginary one, a technique called goal replay or goal substitution. Building on UVFA, Andrychowicz et al. (2017) proposed Hindsight Experience Replay (HER), a method leveraging hindsight for transferring knowledge
between goals. The original goal of a transition can be substituted by any outcome experienced later in the trajectory (imaginary goal). This helps to increase the probability to observe rewards in reward-sparse environments.</p>
<p>In the literature, environments usually provide goals that the agent is asked to solve. In the IMGEP framework however, autonomous agents are intrinsically motivated to set their own, possibly learning their representation (LaversanneFinot et al., 2018). Forestier \&amp; Oudeyer (2016) in particular, biased the selection of the next goal to attempt towards modules showing high absolute measures of learning progress (LP). This mechanism helps the agent to focus on learnable goals and to disengage from goals that are currently too hard or already solved. Veeriah et al. (2018) uses LP computed from Bellman errors for goal selection, but this form of LP does not improve over random goal selection.</p>
<p>Additional background can be found in the supplementary document. Table 1 presents a classification of the multi-goal approaches most related to our work.</p>
<p>Contributions. The contributions of this paper are:</p>
<ol>
<li>A modular encoding of goals to enable learning of continuous sets of diverse goals within a single policy using UVFA (Reach, Push, Pick and Place, Stack). This enables to tackle different kinds of goals, each with their own continuous parameterization, and facilitates transfer between modules and goals. See Sec. 2.1.</li>
<li>An active strategy for cross-module goal replay. Offpolicy learning enables to use any experience to learn about any goal from any module. We propose to guide the selection of module for replay using absolute learning progress measures (in addition to LP-based goal sampling to interact with environment). See Sec. 2.2.</li>
<li>From the IMGEP perspective, a single monolithic modular multi-goal action policy. This is an alternative to the population-based algorithms studied so far (Forestier \&amp; Oudeyer, 2016; Forestier et al., 2017) and provides the flexibility of RL methods.</li>
<li>An environment for modular goal RL. See Sec. 3.</li>
<li>Empirical comparisons to other architectures: a goalparameterized RL with HER (flat multi-goal RL) and a multi-goal module-experts approach (one multi-goal expert per module). See Sec. 4.1.</li>
<li>A study of the self-organization of learning phases demonstrated by our algorithm (automatic curriculum). See Sec. 4.2.</li>
<li>Experimental evidence of robustness to distracting goals, forgetting and body changes in comparison with random goal selection. See Sec. 4.3 and 4.4.</li>
</ol>
<h2>2. CURIOUS</h2>
<h3>2.1. A Modular Multi-Goal Architecture using Universal Approximators</h3>
<p>UVFA concatenates the goal of the agent with its current state to form the input of the policy and the value function implemented by deep neural networks (Schaul et al., 2015a). With CURIOUS, we propose a new encoding of goals using modular representations. This enables to target a rich diversity of modular goals within a single network (modular multi-goal approach), see Fig. 2. Given $\mathcal{G}<em i="i">{M</em>}}$ the goal space of module $M_{i}$, the current goal $g$ is defined as a vector of dimension $|\mathcal{G}|=\sum_{i=1}^{N}\left|\mathcal{G<em i="i">{M</em>}}\right|$, where the $\mathcal{G<em i="i">{M</em>}}$ can have different dimensionalities. $g$ is set to 0 everywhere except in the indices corresponding to the current module $M_{i}$, where it is set to $g_{i} \in \mathcal{G<em i="i">{M</em>\right]$, see Fig. 2. We call this modular goal-parameterized architecture Modular-UVFA (M-UVFA).}}$. By masking the goal-inputs corresponding to unconsidered modules, the corresponding weights are frozen during backpropagation. In addition, a module descriptor $m_{d}$ of size $N$ (one-hot encoding) encodes the current module. The overall input to the policy network is $\left[s_{t}, g, m_{d</p>
<p>In Fig. 2, we can see the underlying learning architecture (actor-critic). The actor implements the action policy and maps the input $\left[s_{t}, g, m_{d}\right]$ to the next action $a_{t}$. The action is then concatenated to a copy of the actor's input to feed the critic $\left[s_{t}, g, m_{d}, a_{t}\right]$. The critic provides an estimate of the $Q$-value: $Q\left(s_{t}, g, m_{d}, a_{t}\right)$. The critic and the actor are then trained using DDPG (Lillicrap et al., 2015), although any other off-policy learning method could be used (e.g. TD3 (Fujimoto et al., 2018), or DQN for the discrete case (Mnih et al., 2013)). More details about DDPG can be found in the supplementary document or in Lillicrap et al. (2015).</p>
<h3>2.2. Module and Goal Selection, Cross-Module Learning, Cross-Goal Learning</h3>
<p>In UVFA, HER and UNICORN, the next goal to target is selected at random (Schaul et al., 2015a; Andrychowicz et al., 2017; Mankowitz et al., 2018). This is coherent with the common view that the agent must comply with the desires of an engineer and target the goal it is asked to target. Here</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Modular goal-parameterized actor-critic architecture (M-UVFA). Toy example with 2 modules, parameterized by $g_{1}$ (2D) and $g_{2}$ (1D) respectively. Here, the agent is attempting goal $g_{1}$ in module $M_{1}$, as specified by the one-hot module descriptor $m_{d}=\langle 1,0\rangle$. The actor (left) computes the action $a_{t}$. The critic (right) computes the $Q$-value.
on the other hand, agents have the capacity to select which goal to target next. Because goals are not equivalent, the agent can benefit from intrinsic motivations towards LP (Schmidhuber, 1991; Kaplan \&amp; Oudeyer, 2004). This can be useful: 1) when there are distracting goals on which the agent cannot progress; 2) when some goals are already mastered. This idea comes from the IMGEP framework and was used in Baranes \&amp; Oudeyer (2013) to guide goal selection and in Forestier \&amp; Oudeyer (2016) to guide module selection. The problem of selecting a module can be modeled as a non-stationary multi-armed bandit (MAB), where the value of each arm (module) is the current absolute LP. Learning progress (LP) is defined as the derivative of the agent's competence on a particular module: $L P_{M_{i}}=\frac{d C_{M_{i}}}{d t}$, where the competence $C_{M_{i}}: t \rightarrow p_{\text {success }}(t)$ is the probability of success at time $t$. Here, the agent focuses its attention on modules for which it is making the largest absolute progress, and pays little attention to modules that are already solved or unsolvable, i.e. for which $|L P|$ stays small. Using the absolute value of LP also leads to the prioritization of modules for which the agent is showing decreasing performances. This helps to deal with forgetting: the agent reallocates learning resources to the modules being forgotten, Sec. 4.3.</p>
<p>Table 1. Classification of multi-goal approaches. Underlined: Algorithms internally generating goals (IMGEP), (*) using LP-based intrinsic motivations. Italic: Population-based algorithms (non-RL). Bold: Algorithms proposed in this paper.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$n$ GOALS, $n$ POLICIES</th>
<th style="text-align: center;">$n$ GOALS, 1 POLICY</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Flat REPR.</td>
<td style="text-align: center;">GOAL-EXPERTS: <br> (Kaelbling, 1993) <br> SAC-X (Riedmiller et al., 2018) <br> SAGG-RIAC* (Baranes \&amp; Oudeyer, 2013)</td>
<td style="text-align: center;">MULTI-GOAL: UVFA (Schaul et al., 2015a) <br> HER (Andrychowicz et al., 2017) <br> UNICORN (Mankowitz et al., 2018)</td>
</tr>
<tr>
<td style="text-align: center;">MODULAR REPR.</td>
<td style="text-align: center;">MOD-GOAL-EXPERTS: MACOR* (Forestier \&amp; Oudeyer, 2016) <br> MULTI-GOAL MODULE-EXPERTS: MG-ME</td>
<td style="text-align: center;">MODULAR-MULTI-GOALS: <br> M-UVFA. CURIOUS *</td>
</tr>
</tbody>
</table>
<p>Learning Progress Estimation. Since an autonomous agent is not externally provided its true competence or LP, it needs to approximate them for each module. To measure its competence, it uses some episodes (with $p_{\text {eval }}=0.1$ ) to evaluate itself on random modules and targets without exploration noise. The results (success 1 or failure 0 ) of these rollouts are stored in competence queues results ${ }^{(i)}$ for all $M_{i}$. In a similar way as Forestier \&amp; Oudeyer (2016), the agent computes its subjective competence as</p>
<p>$$
C_{M_{i}}\left(n_{\text {eval }}^{(i)}\right)=\frac{1}{l} \sum_{j=0}^{l-1} \operatorname{results}^{(i)}\left(n_{\text {eval }}^{(i)}-j\right)
$$</p>
<p>where $n_{\text {eval }}^{(i)}$ is the number of self-evaluation rollouts performed by the agent in module $M_{i}$. The subjective $L P_{M_{i}}$ after $n_{\text {eval }}^{(i)}$ self-evaluation rollouts is then computed as:</p>
<p>$$
L P_{M_{i}}\left(n_{\text {eval }}^{(i)}\right)=C_{M_{i}}\left(n_{\text {eval }}^{(i)}\right)-C_{M_{i}}\left(n_{\text {eval }}^{(i)}-l\right)
$$</p>
<p>Given the subjective LP measures, we tackle the multiarmed bandit problem by implementing a simple approach called proportional probability matching, with an additional $\epsilon$-greedy strategy for exploration. More precisely, we compute the LP probabilities $p_{L P}\left(M_{i}\right)$ as:</p>
<p>$$
p_{L P}\left(M_{i}\right)=\epsilon \times \frac{1}{N}+(1-\epsilon) \times \frac{\left|L P_{M_{i}}\right|}{\sum_{j=1}^{N}\left|L P_{M_{j}}\right|}
$$</p>
<p>where $N$ is the number of modules. The ratio $\epsilon$ implements a mixture between random exploration of modules (left term) and exploitation through a biased selection/replay of modules (right term). The random exploration term enables sampling modules that do not show any LP (i.e. already solved, too hard, or at a plateau). This way, the agent can check that it stays competent on modules that are already learned, or can insist on modules that are currently too hard.</p>
<p>Note that we use LP for two distinct purposes: 1) Before data collection, to select the module from which to draw the next goal to attempt in the environment; 2) Before training, to select the substitute module descriptor (module replay). Recall that, once transitions are sampled from the replay buffer, they can be modified (replayed) by substituting the original module descriptor (or goal) by a new one. The substitute module is the one the agent is going to learn about. When replaying a particular module more than others, the agent allocates more resources to that module. While the use of LP for module selection is not new (Forestier \&amp; Oudeyer, 2016), we are the first to consider its use for cross-module goal replay.</p>
<p>Module and Goal Selection. Before interacting with the environment, the agents selects the next goal to target by first sampling a module from $\mathcal{M}$ using $p_{L P}$, and second, sampling the goal uniformly from the corresponding goal space $\mathcal{G}<em i="i">{M</em>$.}</p>
<p>Cross-Module and Cross-Goal Learning. In an example with three modules, an agent computed $p_{L P}=$ $[0.6,0.2,0.2]$. The agent uses these probabilities to guide learning towards modules with high absolute LP. If the size of the minibatch is $N_{m b}$, the agent will sample $\left\lfloor N_{m b} \times 0.6\right\rfloor$ transitions relevant to module $1,\left\lfloor N_{m b} \times 0.2\right\rfloor$ transitions relevant to module 2 etc. A transition that is relevant for module $M_{i}$ (e.g. Push module), means that it comes from an episode during which the corresponding outcome has changed (e.g. cube position). This sampling bias towards "eventful" transitions is similar to Energy-Based Prioritization (Zhao \&amp; Tresp, 2018) (see supp. doc.). In this minibatch, every transition has been sampled to train on a specific module (e.g. $m_{d}^{<em>}$ ), although it could have been collected while targeting another module (e.g. $m_{d}$ ). To perform this cross-module learning, we simply substitute the latter by the former. Goal substitution is then performed using hindsight, which means the goal $g$ of a transition is sometimes $(p=0.8)$ replaced by an outcome reached later in the same episode $g^{</em>}$ (Andrychowicz et al., 2017).</p>
<p>Internal Reward. After module descriptors and goals have been substituted, the agent computes an internal reward for each transition using a reward function parameterized by the new $m_{d}^{<em>}$ and goal $g^{</em>}$. Thus it answers: What would have been my reward for experiencing this transition, if I were aiming at that imagined goal from that imagined module? The reward is non-negative ( 0 ) when the outcome satisfies the constraints described by the imagined module $m_{d}^{<em>}$, relative to the imagined $g^{</em>}$; negative otherwise (-1). In a reaching module for instance (see Fig. 1), a positive reward is generated when the Euclidean distance between the 3D target (goal) and the gripper (outcome) falls below a precision parameter $\epsilon_{\text {reach }}$ (reward constraint associated to the reaching module).</p>
<h3>2.3. Combining Modular-UVFA and Intrinsically Motivated Goal Exploration</h3>
<p>A schematic view of CurIOUS is given in Fig. 8. The detailed algorithm is given in the supplementary document.</p>
<ol>
<li>Module and goal selection. The agent selects module $M_{i}$ and goal $g_{i}$ for the next rollout (blue), respectively sampled from the set of potential modules $\mathcal{M}$ using $p_{L P}$ (purple), and uniformly from the corresponding goal space $\mathcal{G}<em i="i">{M</em>$.}</li>
<li>Data collection. The agent interacts with the environment using its current M-UVFA policy (grey), collects transitions and stores them in memory (red).</li>
<li>
<p>LP update. If it was a self-evaluation rollout, the agent updates its measures of competence, LP and $p_{L P}$ given the new result (success or failure, purple).</p>
</li>
<li>
<p>Module and goal substitution. The agent decides on which modules and goals to train. To update the policy and critic, the algorithm first samples a minibatch from the replay buffers (red) using $p_{L P}$ and implements module and goal substitutions to perform cross-module and cross-goal learning (orange), see Sec. 2.2.</p>
</li>
<li>Internal reward. The agent computes its reward $r$ for each transition, using $R_{M, g}$ parameterized by the substitute module $m_{d}^{<em>}$ and goal $g^{</em>}$ (brown).</li>
<li>RL updates. The agent updates its policy and value function with DDPG using the modified minibatch (green).</li>
</ol>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Schematic view of CURIOUS.</p>
<h3>2.4. Evaluation Methodology</h3>
<p>The performance of the agents are evaluated offline in terms of success rates over sets of achievable goals (defined by the experimenter). Every point of a learning curve represents the success rate over 95 offline evaluation rollouts ( $5 \times 19$ actors), using random achievable goals. Evaluation is completely independent from training, i.e. agents cannot train on evaluation transitions. We use the non-parametric onetail Mann-Whitney U-test with confidence level $\alpha=0.01$ for all comparisons. More details and justifications can be found in the supplementary document.</p>
<h2>3. A Modular Goal Environment</h2>
<p>Modular Goal Fetch Arm is a new simulated environment adapted from the OpenAI Gym suite (Brockman et al., 2016). The agent is embodied by a robotic arm facing 2 cubes randomly positioned on a table. The agent controls the position of its gripper and the gripper opening (4D). It can target a diverse set of modular goals: $\left(M_{1}\right)$ Reach a 3D target with the gripper; $\left(M_{2}\right)$ Push cube 1 onto a 2D target on the table; $\left(M_{3}\right)$ Pick and Place cube 1 on a 3D target; $\left(M_{4}\right)$ Stack cube 1 over cube 2. Additional Push modules concerning additional out-of-reach and moving cubes can be defined (impossible, distracting goals). Further details can be found in the supplementary document.</p>
<h2>4. Experiment and Results</h2>
<p>In this section, we present ablative studies to assess the relative importance of: 1) the policy and value function architecture and 2) the use of intrinsically motivated module selection for practice and replay. We call M-UVFA the algorithm using a modular goal-parameterized policy and random module choices, while the intrinsically motivated version is called CURIOUS. We do not investigate the efficiency of HER or the efficiency of the sampling bias towards interesting transitions as they were already studied in Andrychowicz et al. (2017); Plappert et al. (2018) and Zhao \&amp; Tresp (2018) respectively. For fair comparisons, we apply both mechanisms to all the tested algorithms.</p>
<h3>4.1. Impact of Policy and Value Function Architecture</h3>
<p>Experiments. In this section, we investigate the impact of using an M-UVFA architecture for the policy and value function. The module-set is composed of four achievable modules and four distracting modules. We test this algorithm against two baselines:</p>
<ol>
<li>A flat multi-goal architecture (HER). This algorithm does not represent goals in a modular fashion but in a linear way. The corresponding goal is selected uniformly inside $\mathcal{G}$, a holistic goal space such that $\mathcal{G}=\prod_{i=1}^{N} \mathcal{G}<em i="i">{M</em>$. To generate a reward, the agent needs to satisfy the constraints described by all the modules at once. This goal-parameterized architecture is equivalent to UVFA, which makes the algorithm equivalent to HER +DDPG.}</li>
<li>A multi-goal module-experts architecture (MG-ME) where an expert multi-goal policy is trained for each of the $N$ modules. Each policy is trained one epoch every $N$ on its designated module and shares the collected transitions with other experts. When evaluated on a particular module, the algorithm uses the corresponding module-expert.</li>
</ol>
<p>Results. Fig. 5 shows the evolution of the average success rate computed over achievable goals for M-UVFA and the two baselines described above. The learning curve of HER stays flat. This can be easily understood as none of the goals expressed in the complete goal space $\mathcal{G}$ corresponds to a real situation (e.g. the agent cannot reach a 3D target with its gripper while placing a cube at another). The agent cannot</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5. Impact of the policy and value function architecture. Average success rates computed over achievable goals. Mean +/std over 10 trials are plotted, while dots indicate significance when testing M-UVFA against MG-ME.
fulfill the constraints of all modules simultaneously, thus receives no reward. This motivates the use of a modular representation with separated modules. Comparing MGME and M-UVFA, we can see that the achievable goals are learned much faster in the multi-modular-goals approach (one, policy, $\approx 250 \cdot 10^{3}$ vs. $\approx 450 \cdot 10^{3}$ episodes). From now on, all experiments use the M-UVFA architecture.</p>
<h3>4.2. Visualizing the Intrinsic Motivation towards Learning Progress</h3>
<p>Experiments. This section aims at showing the inner working of CURIOUS's intrinsic motivation towards LP. Here we focus on a setting with four achievable modules (Reach, Push, Pick and Place, and Stack).</p>
<p>Results. Fig. 4(a) shows the evolution of the moduledependent competence measures as subjectively perceived by the agent, while Fig. 4(b) shows the evolution of the corresponding LP measures. Finally, Fig. 4(c) shows the corresponding module selection probabilities $p_{L P}$, a mixture of random selection with probability $\epsilon$ and active selection proportional to LP measures with probability $1-\epsilon$. These figures demonstrate the existence of successive learning phases, that can be interpreted as developmental phases (Oudeyer \&amp; Smith, 2016). The robot first learns how to
control its gripper $\left(M_{1}\right)$, then to push objects on a desired target on the table $\left(M_{2}\right)$ before it learns how to place the cube on a 3D target $\left(M_{3}\right)$ and how to stack the two cubes $\left(M_{4}\right)$. Fig. 4(b) shows that LP stays small for modules that are already solved (e.g. $M_{1}$ after $10^{4}$ episodes) or too hard to solve (e.g. $M_{3}$ and $M_{4}$ before $35 \cdot 10^{3}$ episodes), and increases when a module is being learned. We further discuss the link between these learning phases, developmental learning and curriculum learning in the supplementary document.</p>
<h3>4.3. Impact of the Intrinsic Motivation: Resilience to Forgetting and Sensor Perturbations</h3>
<p>Experiments. During learning, the agent can forget about a previously mastered module. This can happen because is not targeting it often (catastrophic forgetting), because of environmental changes (e.g. icy floor) or because of body changes (e.g. sensor failure). Ideally, CURIOUS should be able to detect and react when such situations arise. This section investigates the resilience of our algorithm to such perturbations and compares it to the M-UVFA baseline.</p>
<p>We first look at a run where forgetting occurs and explain how CURIOUS detects the situation and reacts. Since forgetting cannot be triggered, we add more emphasis to a second experiment, where we simulate a time-locked sensory failure. We present the following setup to the agent: first, it learns about a set of 4 modules (Reach, Push, Pick and Place for cube 1, and Push for cube 2). Then, a sensory perturbation is triggered at a precise time (epoch $=250$, episode $=237.5 \cdot 10^{3}$ ) such that the perception of cube 2 gets shifted by 0.05 (simulation units) until the end of the run. The performance on this module suddenly drops and we compare the recoveries of CURIOUS and M-UVFA.</p>
<p>Results - Forgetting. Looking at Fig. 4(a), we can observe a drop in the competence on $M_{3}$ around episode $80 \cdot 10^{3}$. This phenomenon is usually described as catastrophic forgetting: because it is trained on other modules, the network can forget about the previously mastered mod-
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4. Visualization of a single run. a: Module-dependent subjective measures of competence for CURIOUS (1 run). b: Corresponding module-dependent subjective measures of absolute LP. c: Corresponding probabilities $p_{L P}$ to select modules to practice or to learn about.</p>
<p>ule $M_{3}$, without any obvious reason. The corresponding period of Fig. 4(b) shows an increase in LP for $M_{3}$, which in turn triggers an additional focus of the agent towards that module (see the corresponding probability increase in Fig. 4(c)). Using LP to bias its attention, the agent monitors its competence on the modules and can react when it forgets about a previously mastered module. This mechanism helps to deal with the problem of forgetting and facilitates learning of multiple modules in parallel. To prove its efficiency, we need to compare CURIOUS to its baseline M-UVFA using a time-locked perturbation.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Impact of the intrinsic motivation towards LP for sensory failure recovery. Mean success rates over the four modules +/- std over 10 trials are plotted. The dashed line indicates the onset of the perturbation, while the dots indicate significance when testing CURIOUS against M-UVFA.</p>
<p>Results - Sensor Perturbation. In Fig. 6, we can observe the drop in average success rate after the perturbation (around $240 \cdot 10^{3}$ episodes). This perturbation only affects one of the four modules (Push cube 2), which triggers a drop in the average performance of about $1 / 4^{t h}$. As described above, CURIOUS agents detect that perturbation and react by using more transitions to improve on the corresponding module. This translates into a significantly faster recovery when compared to M-UVFA. The agents recover $95 \%$ of their pre-perturbation performance in 78 and $43 \cdot 10^{3}$ episodes (random and active respectively), which translates in a $45 \%$ faster recovery for CURIOUS $\left(p&lt;10^{-4}\right)$, see Fig. 6.</p>
<h3>4.4. Impact of the Intrinsic Motivation: Resilience to Distracting Modules</h3>
<p>Experiments. In this section, we investigate the resilience of our learning algorithm when the number of distracting modules increases $(0,4,7)$. The agent faces four achievable modules in addition to the distracting modules. The distracting modules are all Push modules relative to the randomly moving and out-of-reach cubes. The agent receives extra noisy inputs corresponding to the random movements of these cubes.</p>
<p>Results. In Fig. 7, we see that the number of distracting modules faced by the agents highly impacts their learn-</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Impact of the intrinsic motivation towards LP when the number of distracting modules grows. Average success rates computed over achievable goals when the number of distracting modules increases $(0,4,7)$. Mean and standard error of the mean (for visualization purpose) over 10 trials are plotted. The dots indicate significance when testing CURIOUS against M-UVFA.
ing speed on achievable goals. In particular, M-UVFA random agents do not know that these goals are impossible to achieve and waste time and resources trying to improve on them. Since these agents sample distracting modules just like others, we can expect the learning speed to be scaled by $\frac{# \text { achievablemodules }}{# \text { modules }}$. On the other hand, CURIOUS agents try to learn which modules are too difficult at the moment to target them less often. Note that CURIOUS agents still need to choose them sporadically to keep updated measures of their LP: they sample a random module with probability $\epsilon$. In Fig. 7, we see that the advantage of CURIOUS over its random counterpart increases as the number of distracting modules grows (see colored dots indicating significant differences). Although the addition of distracting modules might sound a bit ad-hoc here, it is important to note that autonomous agents evolving in the real world face numerous modules such as these. For humans, quantity of potential modules are impossible (predicting the movement of leaves on a tree, trying to walk on walls etc.). Just as humans, artificial agents need to discard them based on experience and LP.</p>
<h2>5. Discussion</h2>
<p>Leveraging Environment Modularity. In some environments, representing all the potential goals requires modular representations. Because Sec. 4.1 proved that a simple UVFA architecture could not deal with this situation, we proposed M-UVFA. Note that, although our Modular Goal Fetch-Arm environment only contains goals that can be represented in a modular way, M-UVFA can also target discrete sets of goals using flat representations (by setting the goal $g_{i}$ of module $M_{i}$ to the null vector and letting $m_{d}$ encode for the goal). In short, M-UVFA enables traditional UVFA to target a richer diversity of goals than what was possible with traditional UVFA implementations.</p>
<p>Pros and Cons of Monolithic Policies. As noted in Mankowitz et al. (2018), representations of the world state</p>
<p>are learned in the first layers of a neural network policy/value function. A representation learned to achieve goals from one module could probably be useful for learning goals from another similar module. Our monolithic modular goal policy leverages that fact, by re-using subparts of the same network to learn different but similar modules and goals. This might explain why M-UVFA outperforms the multi-goal module-experts (MG-ME) policy architecture (Fig. 5). However, such monolithic policies are more prone to forgetting. Although this phenomenon is partially mitigated by the use of the absolute value of LP, it might still be an issue when the number of potential modules increases. To answer this problem, we could think of combining several M-UVFA policies for different subsets of modules.</p>
<p>A Monolithic IMGEP. Contrary to the vision shared by many multi-goal RL papers where agents must comply to the engineer desires (do goal 1, do goal 3 ...), our work takes the perspective of agents empowered by intrinsic motivations to choose their own goals (do whatever you want, but be curious.). This vision comes from the IMGEP framework which defines agents able to set their own parameterized problems to explore their surrounding and master their environment (Forestier et al., 2017). Contrary to previous IMGEP algorithms grounded on memory-based representations of policies, CURIOUS uses a single monolithic policy for all modules and goals (M-UVFA). Because it is memory-based, MACOB does not handle well the variety of initial states which limits its generalization capacity.</p>
<p>Active Learning using Learning Progress. Although LP-based module selection already brings significant advantages compared to random module selection, CURIOUS could benefit from a more advanced LP estimator. Our current estimator uses moving averages. It is fast and requires small amounts of memory, but could be more reactive to changes in true LP. This delay causes the agent to persevere on modules that are already mastered, or not to react quickly to newly learnable modules. These drawbacks could be mitigated with more advanced measures of competence or LP (e.g. approximate Bayesian methods like in Mathys et al. (2011)).</p>
<h2>6. Further Work</h2>
<p>Hierarchical Extension. The idea of using a high-level policy to select goals for a lower-level policy was also studied in the field of hierarchical RL. Yet, while hierarchical RL agents choose their own subgoals, they usually do so to achieve higher-level goals imposed by the engineer (Vezhnevets et al., 2017; Nachum et al., 2018; Levy et al., 2018). A natural extension of our work could be to replace our high-level MAB module selection policy by another CURIOUS agent targeting self-generated higher-level goals, in a
hierarchical manner.</p>
<p>Learning a Goal Selection Policy. In this work we provide the policy for goal sampling inside modules: sampling uniformly from a pre-defined (reachable) goal space. In the future, the agents could learn it autonomously using adaptations of existing algorithms such as SAGG-RIAC (Baranes \&amp; Oudeyer, 2013) or GOAL-GAN (Held et al., 2017). SAGGRIAC enables to split recursively a wide continuous goal space and to focus on sub-regions where LP is higher, while GOAL-GAN proposes to generate goals of intermediate difficulty using a Generative Adversarial Network.</p>
<p>Learning Representations for Modules and Goals. Another assumption of our work, is that agents should already know a modular representation of goals and their modules. Although modules and goal spaces were hand-defined in the experiments of this paper, this was a scaffolding for the studies we presented. In a general IMGEP setting, autonomous agents must be able to construct their own set of modules and goal representations. The idea of autonomously learning modular goal representations from experience has been explored in Laversanne-Finot et al. (2018), using $\beta-V A E s$. This was used for goal exploration using a population-based IMGEP algorithm. Combining CURIOUS to this unsupervised learning of disentangled goal spaces is an interesting avenue to explore, in the quest of more autonomous learning agents.</p>
<h2>7. Conclusion</h2>
<p>This paper presents CURIOUS, a learning algorithm that combines an extension of UVFA to enable modular goal RL in a single policy (M-UVFA), and active mechanisms that bias the agent's attention towards modules where the absolute LP is maximized. This self-organizes distinct learning phases, some of which are shared across agents, others dependent on the agent experience. With this mechanism, agents spend less time on impossible modules and focus on achievable ones. It also helps to deal with forgetting, by refocusing learning on modules that are being forgotten because of model faults, changes in the environment or body changes (e.g. sensory failures). This mechanism is important for autonomous continual learning in the real world, where agents must set their own goals and might face goals with diverse levels of difficulty, some of which might be required to solve others later on.</p>
<p>Links. The environment, code and video of the CURIOUS agent are made available at https://github.com/ flowersteam/curious.</p>
<h2>Acknowledgments</h2>
<p>Cédric Colas is partly funded by the French Ministère des Armées - Direction Générale de lArmement. Olivier Sigaud is partly funded by the European Commission, within the DREAM project. The DREAM project has received funding from the European Unions Horizon 2020 research and innovation program under grant agreement $N^{\circ} 640891$.</p>
<h2>References</h2>
<p>Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, O. P., and Zaremba, W. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048-5058, 2017.</p>
<p>Baranes, A. and Oudeyer, P.-Y. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49-73, 2013.</p>
<p>Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv preprint arXiv:1606.01540, 2016.</p>
<p>Colas, C., Sigaud, O., and Oudeyer, P.-Y. GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning algorithms. arXiv preprint arXiv:1802.05054, 2018.</p>
<p>Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I., et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.</p>
<p>Forestier, S. and Oudeyer, P.-Y. Modular active curiositydriven discovery of tool use. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on, pp. 3965-3972. IEEE, 2016.</p>
<p>Forestier, S., Mollard, Y., and Oudeyer, P.-Y. Intrinsically motivated goal exploration processes with automatic curriculum learning. arXiv preprint arXiv:1708.02190, 2017.</p>
<p>Fujimoto, S., van Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.</p>
<p>Held, D., Geng, X., Florensa, C., and Abbeel, P. Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366, 2017.</p>
<p>Hessel, M., Soyer, H., Espeholt, L., Czarnecki, W., Schmitt, S., and van Hasselt, H. Multi-task deep reinforcement learning with popart. arXiv preprint arXiv:1809.04474, 2018.</p>
<p>Kaelbling, L. P. Learning to achieve goals. In IJCAI, pp. 1094-1099. Citeseer, 1993.</p>
<p>Kaplan, F. and Oudeyer, P.-Y. Maximizing learning progress: an internal reward system for development. In Embodied artificial intelligence, pp. 259-270. Springer, 2004.</p>
<p>Laversanne-Finot, A., Péré, A., and Oudeyer, P.-Y. Curiosity driven exploration of learned disentangled goal spaces. In Proceedings of CoRL 2018 (PMLR), 2018.</p>
<p>Levy, A., Platt, R., and Saenko, K. Hierarchical reinforcement learning with hindsight. arXiv preprint arXiv:1805.08180, 2018.</p>
<p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.</p>
<p>Mankowitz, D. J., Žídek, A., Barreto, A., Horgan, D., Hessel, M., Quan, J., Oh, J., van Hasselt, H., Silver, D., and Schaul, T. Unicorn: Continual learning with a universal, off-policy agent. arXiv preprint arXiv:1802.08294, 2018.</p>
<p>Mann, H. B. and Whitney, D. R. On a test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics, pp. 50-60, 1947.</p>
<p>Mathys, C., Daunizeau, J., Friston, K. J., and Stephan, K. E. A bayesian foundation for individual learning under uncertainty. Frontiers in human neuroscience, 5:39, 2011.</p>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.</p>
<p>Nachum, O., Gu, S., Lee, H., and Levine, S. Data-efficient hierarchical reinforcement learning. arXiv preprint arXiv:1805.08296, 2018.</p>
<p>Nguyen, S. M. and Oudeyer, P.-Y. Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner. Paladyn, 3(3):136-146, 2012.</p>
<p>Oudeyer, P.-Y. and Smith, L. B. How evolution may work through curiosity-driven developmental process. Topics in Cognitive Science, 8(2):492-502, 2016.</p>
<p>Plappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell, G., Schneider, J., Tobin, J., Chociej, M., Welinder, P., et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.</p>
<p>Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., Van de Wiele, T., Mnih, V., Heess, N., and Springenberg, J. T. Learning by playing-solving sparse reward tasks from scratch. arXiv preprint arXiv:1802.10567, 2018.</p>
<p>Schaul, T., Horgan, D., Gregor, K., and Silver, D. Universal value function approximators. In International Conference on Machine Learning, pp. 1312-1320, 2015a.</p>
<p>Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015b.</p>
<p>Schmidhuber, J. Curious model-building control systems. In Neural Networks, 1991. 1991 IEEE International Joint Conference on, pp. 1458-1463. IEEE, 1991.</p>
<p>Teh, Y., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N., and Pascanu, R. Distral: Robust multitask reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4496-4506, 2017.</p>
<p>Večerík, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess, N., Rothörl, T., Lampe, T., and Riedmiller, M. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017.</p>
<p>Veeriah, V., Oh, J., and Singh, S. Many-goals reinforcement learning. arXiv preprint arXiv:1806.09605, 2018.</p>
<p>Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., and Kavukcuoglu, K. Feudal networks for hierarchical reinforcement learning. arXiv preprint arXiv:1703.01161, 2017.</p>
<p>Zhao, R. and Tresp, V. Energy-based hindsight experience prioritization. arXiv preprint arXiv:1810.01363, 2018.</p>
<h2>Additional Background</h2>
<p>DDPG. Deep Deterministic Policy Gradient (DDPG) is an off-policy model-free RL algorithm for continuous action spaces (Lillicrap et al., 2015). It concurrently learns a policy $\pi$ (the actor) and a $Q$-function $Q$ (the critic) using neural networks. The actor implements the controller and maps the current state to the next action: $\pi: \mathcal{S} \rightarrow \mathcal{A}$. The critic approximates the optimal action-value function $Q^{*}$, $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$.</p>
<p>For exploration purposes, transitions are collected using a noisy version of the policy (behavioral policy), e.g. using Gaussian noise on the actions. The transitions are stored in a replay buffer of finite size. The actor and critic updates are then conducted using transitions sampled uniformly from
the replay buffer. The critic is trained to minimize the meansquared Bellman error such that:
$L(\phi, \mathcal{D})=E_{t \sim \mathcal{D}}\left[\left(Q_{\phi}(s, a)-\left(r+\gamma \max <em _phi="\phi">{a^{\prime}} Q</em>\right]$,
where $\phi$ represents the parameters of the critic, $\mathcal{D}$ is the dataset, $t=\left(s, a, r, s^{\prime}\right)$ is a transition with $r$ a reward. The actor is trained so as to maximize the output of the critic. Further details can be found in the original paper (Lillicrap et al., 2015).}\left(s^{\prime}, a^{\prime}\right)\right)\right)^{2</p>
<p>UVFA. Universal Value Function Approximators (UVFA) is an extension of the traditional actor and critic networks to target multiple goals (Schaul et al., 2015a). The goal is represented by a vector defined in space $\mathcal{G}$. It is concatenated both to the input of the actor and to the input of the critic such that: $\pi: \mathcal{S} \times \mathcal{G} \rightarrow \mathcal{A}$ for the actor and $Q: \mathcal{S} \times \mathcal{G} \times$ $\mathcal{A} \rightarrow \mathbb{R}$ for the critic. The reward function as well, is now parameterized by the goal: $R_{g}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. The original paper shows that a UVFA architecture trained with Deep QNetwork (DQN) can generalize to previously unseen goals (Schaul et al., 2015a). Further details can be found in the original paper.</p>
<p>HER. Hindsight Experience Replay (HER) proposes an efficient mechanism leveraging hindsight to learn more efficiently in multi-goal settings (Andrychowicz et al., 2017). When an agent makes an attempt towards a goal and fails to reach it, HER proposes to probabilistically substitute the goal vector by an outcome achieved later in the trajectory. In simpler words, the agents pretends it was trying to reach some other goal it actually reached later on, hence it increases the probability to observe a reward. Note that this mechanism requires the agent to have access to the true reward function $R_{g}$ and to be able to compute the reward corresponding to any combination of goal, state and action vectors. This mechanism can be used with any off-policy algorithms and was initially presented with DQN and DDPG. Further details can be found in the original paper (Andrychowicz et al., 2017).</p>
<h2>Additional Methods</h2>
<p>Overview. Algorithm 1 and Fig. 8 present the pseudocode. We go through the different steps:</p>
<ol>
<li>Module and goal selection. The agent selects module $M_{i}$ and goal $g_{i}$ for the next rollout (blue in Fig. 8). First, it decides whether it will perform self-evaluation, using probability $p_{\text {eval_c }}$ [line 4 in the algorithm]. If it does so, it selects a module $M_{i}$ at random. Otherwise,</li>
</ol>
<p>the next module is sampled from the set of modules $\mathcal{M}$ according to the LP probabilities $p_{L P}$ (purple). The goal is sampled uniformly from the corresponding goal space $\mathcal{G}<em i="i">{M</em>}}$ [line 5]. All the considered goal spaces are either the 2D Euclidean space (for the Push module, with $z=$ height(table)) or the 3D Euclidean space (for other modules). These spaces are simply subspaces of the state space $\mathcal{G<em L="L" P="P">{\subset \mathcal{S}}$.
2. Data collection. The agent interacts with the environment and collects transitions (grey) using its current M-UVFA policy [lines 8 to 13] before storing them in memory [line 15]. If it is performing self-evaluation it does not use exploration noise.
3. LP update. If the agent was performing selfevaluation, it can now update its measures of LP given the new result (success or failure, purple) [lines 17 and 18]. It computes the corresponding probability measures $p</em>$ using an $\epsilon$-greedy version of the proportional probability matching method such that:</p>
<p>$$
p_{L P}\left(M_{i}\right)=\epsilon \times \frac{1}{N}+(1-\epsilon) \times \frac{\left|L P_{M_{i}}\right|}{\sum_{j=1}^{N}\left|L P_{M_{j}}\right|}
$$</p>
<p>where $L P_{M_{i}}$ is the learning progress of module $M_{i}$, and $\epsilon$ implements a mixture between random exploration (left-term) and exploitation guided by the absolute learning progress (right term).
4. Module and goal substitution. The agent decides on which modules and goals to train. To update the policy and critic, the algorithm first samples a minibatch from the replay buffers (red) using $p_{L P}$, see Prioritized Interest Replay below. If it wants to train on module $M_{i}$, it samples a transition relevant to module $M_{i}$ and substitutes the original module descriptor stored in the transition $\left(m_{d}\right)$ by the module descriptor corresponding to $M_{i},\left(m_{d}^{<em>}\right.$ in the figure). With probability $p=0.8$, it also substitutes the current goal $g$ by an outcome achieved later in the episode using hindsight ( $g^{</em>}$ in the figure). These substitutions enable cross-module and cross-goal learning [line 20].
5. Internal reward. The agent computes its reward $r$ for each transition, using the internal reward function $R_{M, g}$ parameterized by the substitute module description $\left(m_{d}^{<em>}\right)$ and the substitute goal $\left(g^{</em>}\right)$ (brown) [line 21].
6. RL updates. The agent updates its policy and value function using DDPG (green) [line 22].</p>
<p>Prioritized Interest Replay. In Prioritized Experience Replay (PER), Schaul et al. (2015b) suggests that RL agents can learn more efficiently from some transitions than from others. They propose to bias transition replay towards transitions with high temporal-difference (TD) error. In multigoal settings, Zhao \&amp; Tresp (2018) proposes to bias sam-</p>
<h1>Algorithm 1 CURIOUS</h1>
<p>1: Input: env, $\mathcal{M}, \mathcal{G}<em _eval_c="{eval_c" _text="\text">{1: N}$, noise, internal_reward( ), $p</em>$
2: Initialize: policy, memory, $p_{L P}$
3: repeat
4: $\quad e v a l _c \leftarrow \operatorname{random}()&lt;p_{\text {eval_c }}$
5: $\quad$ goal, $m_{d}\left(M_{i}\right) \leftarrow$ ModuleGoalSelector()
6: $\quad s_{0}$, outcome $}<em t="t">{0} \leftarrow$ env.reset()
7: for $t=0: N</em>$ do
8: $\quad$ policy_input $\leftarrow$ concatenate $\left(s_{t}, m_{d}\right.$, goal $)$
9: $\quad a_{t} \leftarrow$ policy(policy_input)
10: if not eval_c then
11: $\quad a_{t} \leftarrow a_{t}+$ noise
12: end if
13: $\quad s_{t+1}$, outcome $<em t="t">{t+1} \leftarrow \operatorname{env} \cdot \operatorname{step}\left(a</em>\right)$
14: end for
15: $\quad$ memory.add $\left(s, a, s^{\prime}\right.$, goal, outcome, $\left.m_{d}\right)$
16: if eval_c then
17: $\quad$ memory.add(outcome $==$ goal, $m_{d}$ )
18: $\quad p_{L P} \leftarrow$ LP_Update( memory )
19: end if
20: $\quad$ modified_batch $\leftarrow$ ModuleGoalReplayPolicy( $p_{L P}$, memory) {Use Prioritized Interest Replay, $p_{L P}$ and HER }
21: $\quad$ modified_batch $\leftarrow$ RewardComputer(modified_batch, internal_reward())
22: $\quad$ policy $\leftarrow$ RL_Updates(modified_batch)
{Using DDPG }
23: until learning over</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Schematic view of CURIOUS.
pling towards episodes of higher energy, where energy is the sum of the potential energy, the kinetic energy and the rotational energy of the considered object (Energy-Based Prioritization or EBP). They show that EBP performs higher than PER in their experiments. Here, we propose to apply a simpler version of EBP that we call Prioritized Interest Replay. To learn about module $M_{i}$ (e.g. Push(cube 1) module), we only use transitions from episodes where the corresponding outcome (e.g. cube 1 position) has changed during the episode. This is an heuristic modeling attention towards interesting episodes relative to the module at hand. Note that, contrary to PER where transitions are prioritized based on a criterion at the transition level (TD-error of the transition), we apply a criterion at the episode level (did the outcome change during the episode?), as in EBP. Although we pick interesting episodes, we still sample uniformly from those. We assume this heuristic to induce a small sampling bias and do not use any correction by importance sampling as it was done in PER. We apply that heuristic to all algorithms in our paper.</p>
<p>After the interaction with the environment, transitions are stored in $N+1$ buffers. A transition is stored in buffer $i$ when the outcome corresponding to $M_{i}$ has changed during the episode. The additional buffer collects the remaining episodes. Before training, the agent decides how to allow its computational resources among the different modules using $p_{L P}$. If the size of the minibatch is $N_{m b}$, the agent will sample $\left\lfloor N_{m b} \times p_{L P}\left(M_{i}\right)\right\rfloor$ transitions from buffer $i$, this for all modules $M_{i}$. In this minibatch, every transition has been sampled to train on a specific module $\left(m_{d}^{<em>}\right)$, although it could have been collected while targeting another module $\left(m_{d}\right)$. To perform this cross-module learning, we simply substitute the module descriptor of each transition $m_{d}$ by $m_{d}^{</em>}$.</p>
<p>Internal Rewards. The internal reward function is parameterized by both the module and the goal $R_{M, d}$. It is positive $(r=0)$ when the constraints defined by the modules are satisfied for the current goal and outcome. For the Push and Pick and Place modules, the reward is positive when the distance between the cube and the target position is within $\epsilon_{\text {reach }}=0.05$ (simulation units), negative otherwise. For the Reach module, the same criterion is applied to the distance between the gripper and the goal. Finally, for the Stack module, in addition to the constraints on the goalcube distance, the goal-gripper distance must stay larger than $1.2 \times \epsilon_{\text {reach }}$, to ensure that the two cubes are actually stacked and not hold by the gripper. With this example we see that a module simply defines a set of constraints. Although here it is applied to a single state, we could imagine constraints defined over multiple time-steps (e.g. keep the cube above the table for more than 2 seconds).</p>
<h2>Environment</h2>
<p>Modular Goal Fetch Arm is a new simulated environment based on the Fetch environments included in the OpenAI Gym suite (Brockman et al., 2016). The agent is embodied by a 7-DoF Fetch robotic arm facing 2 cubes randomly positioned on a table and can target a large set of diverse goals. Additional out-of-reach cubes can be added as distracting modules. The agent controls the 3D Cartesian position of its gripper in velocity as well as its two-fingered parallel gripper. The agent can target several modules of goals: $\left(M_{1}\right)$ reaching a 3 D target with the gripper; $\left(M_{2}\right)$ reaching a 2 D target on the table with cube $1 ;\left(M_{3}\right)$ reaching a 3 D target with cube $1 ;\left(M_{4}\right)$ stacking cube 1 over cube $2 ;\left(M_{5-}\right)$ reaching a 2 D target on the table with one of the randomly moving cube which are out of reach (distracting module). The observation space has 40 dimensions (see original paper presenting the Fetch environments for details (Plappert et al., 2018)) + 3 per additional out-of-reach cube, while the action space has 4 (3D actions + gripper).</p>
<h2>Evaluation Methodology</h2>
<p>Every epoch ( $50 \times 19$ actors $=950$ episodes), the 19 actors are evaluated offline on 5 rollouts each. For each evaluation rollout, the experimenter asks the agent to perform a goal selected at random in the set of achievable goals (subset of goals the agent trained on). This external evaluation of the agent's true competence is distinct from the self-evaluation performed by the agent during learning. Note that the agent policy is frozen during evaluation, and that the agent cannot use the evaluation trajectories for later training as opposed</p>
<p>to trajectories obtained by self-evaluation. Since all actors share the same policy, each point $\hat{p}$ of a learning curve represents the maximum likelihood estimate of a Bernoulli probability $p$ (i.e. success rate) using a sample size of $n=$ $19 \times 5=95$ rollouts. With this sample size, the confidence interval can be estimated using a normal distribution:</p>
<p>$$
\text { error }=|p-\hat{p}| \leq 1.96 \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
$$</p>
<p>with probability $95 \%$, which is itself upper-bounded by $1.96 \sqrt{\frac{0.25}{95}}=0.1$.</p>
<p>Each experiment is replicated with 10 different random seeds. Because the sample size is relatively small $(&lt;30)$, we cannot make any assumption about the distribution of the performance measures. Comparisons are therefore conducted using a one-tail Mann-Whitney U-test, a nonparametric test that does not make such assumptions (Mann \&amp; Whitney, 1947). We use a type-I error (confidence level) $\alpha=0.01$. The one-tail version is used because we expect our algorithm to perform better.</p>
<h2>Hyperparameters</h2>
<p>The CURIOUS algorithm is built on top of the OpenAI Baselines implementation of HER-DDPG. ${ }^{2}$ This consists in a parallel implementation with 19 actors. The actors share the same parameters and their updates are averaged to compute the next set of parameters. We use the same hyperparameters as Plappert et al. (2018). Note that this paper does not focus on the underlying learning algorithm, which could be replaced by any off-policy reinforcement learning algorithm.</p>
<p>CURIOUS uses three extra hyperparameters. 1) The probability $p_{\text {eval }, c}$ that the agent performs self-evaluation for the next rollout. It does so to update its subjective competence measure for a module, $p_{\text {eval }, c}=0.1$. 2) The length of the windows considered to compute the subjective measures of competence and learning progress is set to $l=300$ episodes. 3) The exploration parameter $\epsilon$ which controls the mixture between random module selection and active module selection based on absolute learning progress is set to $\epsilon=0.4$.</p>
<h2>Additional Results</h2>
<h2>Developmental Learning: Regularities and Diversity</h2>
<p>Traditionally, the study of learning phases is based on behavior. This is true in Psychology, which can only access</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the human behaviors but cannot access internal representations. Multi-goal RL papers as well, when they do provide behavioral studies, usually present the test performance of the agent on various goals as a function of time (e.g. (Mankowitz et al., 2018)). In the present paper however, we have a direct access to the internal representations of our agents, to their beliefs about their current competence and learning progress for each of the modules. It is from these internal representations that we study developmental progressions. This enables us to distinguish active curriculum learning from passive experience of the environmental structure. Indeed, the structure of the environment can be such that some modules are simpler than others, leading random agents to learn them faster than others. On the other hand, agents performing active module selection such as CURIOUS, actively guide their learning trajectories as a function of their internal representations of competence and progress.</p>
<p>In Fig. 9, we present 5 sets of subjective competence curves and their associated learning progress. We can see that $M_{1}$ (Reach module) is always learned first and $M_{2}$ (Push module) is always learned second. However, $M_{3}$ (Pick and Place) and $M_{4}$ (Stack) can be learned in various order or even simultaneously depending on the individual learning trajectories. Once the agent has experienced a few successes on a module, LP increases and the agent focuses on it, which generates even more successes. What happened by chance in the early stages of learning leads this agent to focus first on either $M_{3}$ or $M_{4}$. Although some modules might be easier to learn first, or necessary to learn others, individual experience can influence learning trajectories just as for humans (Oudeyer \&amp; Smith, 2016).</p>
<h2>Scaling Properties</h2>
<p>It is important to discuss the ability of CURIOUS to scale to larger sets of modules. Quick tests showed that CURIOUS could scale to at least 10 achievable modules (Reach, Push, Pick and Place and Stack with different cubes). However, we can expect CURIOUS to fail when the number of modules gets even larger. This could be mitigated by enabling CURIOUS to deal with multiple actors and critics, each pair dealing with a subset of the modules.</p>
<h2>Meaning of the Term Goal</h2>
<p>It is important to note that the term goal used in the context of Intrinsically Motivated Goal Exploration Processes (IMGEP) is much more general than the one used in this paper. Indeed, in IMGEP, self-generated goal denotes any self-defined parameterized problem, which solution should be found through ones own actions (e.g. it can as diverse as "Grasp obj1 and place it at pos3", "Move to (x,y,z)", "Find a blue key", "Collect an even number of obj3", or "generate a</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Learning Phases. Competence (left) and absolute learning progress (right) for 5 trials of the CURIOUS algorithm (Reach, Push, Pick and Place, Stack +3 distracting modules).</p>
<p>trajectory that contains 3 loops").</p>
<h1>Computational Resources</h1>
<p>One trial of one algorithm takes around 20 hours to run on 19 cpus. This paper contains around 130 trials, which sums to a total of $130 \times 20 \times 19 \approx 5.6$ cpu years.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The OpenAI Baselines implementation of HER-DDPG can be found at https://github.com/openai/baselines, our implementation of CURIOUS can be found at https://github.com/ flowersteam/curious.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ CURIOUS stands for Continual Universal Reinforcement learning with Intrinsically mOtivated sUbstitutionS.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
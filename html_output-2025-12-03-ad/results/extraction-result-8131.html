<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8131 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8131</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8131</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-272330506</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.01659v1.pdf" target="_blank">Interpreting and Improving Large Language Models in Arithmetic Calculation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable potential across numerous applications and have shown an emergent ability to tackle complex reasoning tasks, such as mathematical computations. However, even for the simplest arithmetic calculations, the intrinsic mechanisms behind LLMs remain mysterious, making it challenging to ensure reliability. In this work, we delve into uncovering a specific mechanism by which LLMs execute calculations. Through comprehensive experiments, we find that LLMs frequently involve a small fraction (<5%) of attention heads, which play a pivotal role in focusing on operands and operators during calculation processes. Subsequently, the information from these operands is processed through multi-layer perceptrons (MLPs), progressively leading to the final solution. These pivotal heads/MLPs, though identified on a specific dataset, exhibit transferability across different datasets and even distinct tasks. This insight prompted us to investigate the potential benefits of selectively fine-tuning these essential heads/MLPs to boost the LLMs' computational performance. We empirically find that such precise tuning can yield notable enhancements on mathematical prowess, without compromising the performance on non-mathematical tasks. Our work serves as a preliminary exploration into the arithmetic calculation abilities inherent in LLMs, laying a solid foundation to reveal more intricate mathematical tasks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8131.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8131.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KeyHeads+MLPs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse attention heads and downstream MLPs mechanism for arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies a sparse subset (<5%) of attention heads that selectively attend to operand and operator tokens, and a sequence of downstream MLP layers that progressively transform residual-stream activations toward embeddings of the correct numeric answer; together these components implement two-operand arithmetic in LLaMA2-class models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-7B (primary); LLaMA2-13B and Mistral-7B (transfer checks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer LLMs (LLaMA2 series) studied: LLaMA2-7B (32 transformer layers, 32 heads per layer, model dim d), LLaMA2-13B also evaluated; models are pre-trained language models from HuggingFace and evaluated without chain-of-thought prompting unless noted.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic: single-digit addition, subtraction, multiplication, division; also tested on multi-digit integers, rational numbers, exponentiation, and transfer to math word‑problems (GSM8K, SVAMP, AddSub, SingleEq).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Sparse specialization: (1) a small set of 'operand-attending' heads focus attention (Q_end → K_token) on the two numeric operand tokens ({A},{B}) and separate 'operator-attending' heads focus on operator tokens; (2) these heads write information into the residual stream (via OV) which is then processed by MLPs in later layers; (3) MLP inputs begin to align (cosine similarity) with operand token unembedding vectors starting ~layer 12–18, and MLP output increments (MLP_out − MLP_in) increasingly align with the unembedding vector of the correct answer {C} in later layers (coarse-to-fine convergence).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Path patching (activation patching) replacing a head/MLP activation with its counterfactual activation from non-calculation inputs; mean ablation (knockout via replacing activation with average counterfactual activation); attention-pattern inspection (END-query row of attention matrices); probing MLP_in/MLP_out via cosine similarity against unembedding vectors W_U[{token}].</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline average next-token probability for correct answer ~82% across generated reference samples; patching/knockout of identified key heads/MLPs produces large drops in logit/probability: example head patch decreased logit by 14.0% (head 12.223), full knockout of identified components leads to ~70% relative accuracy decrease on the paper's generated dataset and ~66% relative drop on SVAMP; overlap of key heads identified on GSM8K vs generated data: ~60% overlap; knocking out overlapping heads caused ~56% (generated) / ~52% (GSM8K) drops. (See paper for layer‑by‑layer similarity curves and exact per-layer numbers.)</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>When key heads are knocked out the model typically outputs incorrect numeric tokens (often digits near the correct answer); LLMs sometimes generate plausible-but-wrong nearby numbers (coarse-to-fine pattern); multiplication/division require more key heads (harder, more distributed); tokenization of multi-digit numbers into digit tokens can complicate multi-digit arithmetic; key-head discoveries on one dataset do not fully cover all reasoning-required heads for complex math word problems (some heads attend to text for comprehension and are necessary for GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Causal path patching: patching individual heads' END-position activations with counterfactual activations substantially reduces the logit for the ground-truth answer; mean-ablation (knockout) of top-ranked heads produces sharp declines in accuracy compared to random-head knockout (Figure 3); attention analysis shows identified heads strongly concentrate on operand/operator tokens across 1000 samples (Figure 5, Figure 7); MLP input/output cosine-similarity trajectories show operands injected into MLP_in around layers 12–17 and MLP_out−MLP_in increasingly aligned to correct answer embedding in later layers (Figures 6, 13, 14); effect is transferable across datasets and to other LLMs (LLaMA2-13B, Mistral-7B) with similar inflection-layer patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not all calculation-related behavior is captured by the same small set of heads across all datasets: GSM8K discovered additional heads (4 non-overlapping heads) that attend to textual tokens needed for comprehension (e.g., 'GB', '.') which are important for complex word problems; precise MLP tuning can improve math but tuning more MLPs degrades general performance and increases compute; the mechanism is descriptive (coarse-to-fine progressive alignment) but does not show an explicit bitwise adder or symbolic algorithm identical to computer addition; generalization to arbitrarily large integers and compositional multi-step math beyond two-operand operations remains unproven though exponentiation and other ops show similar sparse-head patterns in initial checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpreting and Improving Large Language Models in Arithmetic Calculation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8131.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8131.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PathPatching+MeanAblation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Path patching (activation patching) and mean ablation knockout</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Causal intervention methods used to identify and validate model components causally responsible for arithmetic: path patching replaces activations on reference runs with counterfactual activations to measure causal effect on logits; mean ablation replaces activations with average counterfactual activations to knock out components and measure accuracy drop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to LLaMA2-7B primarily; applied to LLaMA2-13B and Mistral-7B for transfer checks</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same decoder-only transformer family (LLaMA2 etc.). Path patching scans all attention heads and MLP nodes in the model DAG by replacing node outputs at the END token position and measuring change in logit for ground-truth answer {C}.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic templates and counterfactual variants; transfer to SVAMP and GSM8K word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Not a representation but a causal probing/intervention method that isolates contributions of sender nodes (individual heads/MLPs) to receiver node (logits) via hard interventions on activations, and validates via knockout (mean ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Path patching: replace activation of a single head/node in the reference forward pass with its activation computed under counterfactual non-calculation input; freeze other activations from reference; compute logit change. Mean ablation: replace activations with average activation from counterfactual data to neutralize task-specific signal for that component.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to rank heads by causal effect on logit change; identified 'key heads' as those whose patching reduced the target-token logit by more than 5% (example: head 12.22/3 patch reduced logit by 14%); sequential knockout of top-k heads (effect-rank) produced large accuracy declines (e.g., accuracy falling from ~99% baseline to ~35% after top-k knockouts in generated tests), while random-rank knockouts produced small changes (~±2%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Path patching assumes independence of patched pathways and relies on valid counterfactual activations; residual connections and multiple interacting heads mean some causal attributions may be distributed; patching may not capture synergistic effects between multiple heads/MLPs unless multiple-node patching is done.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Direct causal effect measurements: scanning every head/node with path patching and averaging effects across sample pairs produced sparse sets of heads with large causal effects; mean ablation knockouts validated faithfulness by showing sharp performance degradation when these nodes were disabled compared to random ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Patching one node at a time may miss interactions where multiple nodes jointly implement functionality; identified key nodes are dataset-dependent to some degree (GSM8K required additional text-attending heads); hard intervention requires computing activations on counterfactual inputs which may be ambiguous to design for more complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpreting and Improving Large Language Models in Arithmetic Calculation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8131.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8131.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PreciseSFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Precise supervised fine-tuning of identified heads/MLPs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning procedure that updates only the parameters of the small subset of identified calculation-related components (selected attention head Q/K/V/O matrices and/or selected MLP layers) to improve arithmetic performance while preserving general capabilities and reducing compute.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to LLaMA2-7B and LLaMA2-13B in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuning performed on top-N identified attention heads (output matrices W_i,j^O and projection matrices W^Q/K/V) and optionally top MLP layers; example best setting was top-32 heads for LLaMA2-7B/13B.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same math datasets: GSM8K, AddSub, SingleEq, SVAMP; general evaluation on MMLU and CSQA to test general capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Parameter-space intervention: targeted updates to the specialized attention head and MLP parameters to strengthen the arithmetic computation pathway identified by path patching, without altering the rest of the model.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Supervised fine-tuning (SFT) restricted to selected parameters; gradients rescaled by H/h (number of total heads H and updated heads h) per layer; training hyperparameters: LR=2e-5, batch size 128, 2 epochs; trained on aggregated math datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precise SFT (top-32 heads) produced an average improvement of ~15% across four math datasets; Precise SFT matched or outperformed full-model SFT on math tasks (e.g., Precise SFT outperformed Full SFT by 5.5% on SVAMP and 2.8% on GSM8K in reported runs); Precise SFT preserved general-task performance (Full SFT incurred ~5% drop on MMLU/CSQA while Precise SFT did not), and reduced compute: top-32 heads setting processed ~50 samples/sec with ~0.067B parameters tunable ( <1% of total), versus full SFT much slower and tuning ~all parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Including more MLPs in the tuned set can improve math scores but can harm general performance (e.g., adding top-3 MLPs raised GSM8K but decreased MMLU by ~1.5%); tuning too few components yields limited gains; precise SFT improvements are limited to the distribution of math problems seen during fine-tuning and may not fully generalize to complex multi-step reasoning without addressing text comprehension heads.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical ablations: sweeping the number of tuned heads showed best average gains at 32 heads; combination of top-32 heads + top-3 MLPs gave further math improvements but cost in general tasks and compute; training/time statistics and cross-dataset evaluations (SVAMP, GSM8K, AddSub, SingleEq, MMLU, CSQA) demonstrate precise SFT can raise math performance while preserving other capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>While precise SFT is efficient, some tasks require additional text-comprehension heads (GSM8K had 4 non-overlapping heads that attend to text tokens), so precise SFT limited to arithmetic heads will not by itself solve word-problem comprehension; tuning MLPs increases parameter count and training time, and may introduce tradeoffs with generalization; the approach presumes that a small, identifiable subset of parameters causally drives the target skill, which may not hold for all tasks or larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpreting and Improving Large Language Models in Arithmetic Calculation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Localizing model behavior with path patching <em>(Rating: 2)</em></li>
                <li>A mathematical framework for transformer circuits <em>(Rating: 2)</em></li>
                <li>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model <em>(Rating: 2)</em></li>
                <li>Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space <em>(Rating: 2)</em></li>
                <li>Understanding arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 1)</em></li>
                <li>Interpretability in the wild: a circuit for indirect object identification in GPT-2 small <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8131",
    "paper_id": "paper-272330506",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "KeyHeads+MLPs",
            "name_full": "Sparse attention heads and downstream MLPs mechanism for arithmetic",
            "brief_description": "The paper identifies a sparse subset (&lt;5%) of attention heads that selectively attend to operand and operator tokens, and a sequence of downstream MLP layers that progressively transform residual-stream activations toward embeddings of the correct numeric answer; together these components implement two-operand arithmetic in LLaMA2-class models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-7B (primary); LLaMA2-13B and Mistral-7B (transfer checks)",
            "model_description": "Decoder-only transformer LLMs (LLaMA2 series) studied: LLaMA2-7B (32 transformer layers, 32 heads per layer, model dim d), LLaMA2-13B also evaluated; models are pre-trained language models from HuggingFace and evaluated without chain-of-thought prompting unless noted.",
            "arithmetic_task_type": "Two-operand arithmetic: single-digit addition, subtraction, multiplication, division; also tested on multi-digit integers, rational numbers, exponentiation, and transfer to math word‑problems (GSM8K, SVAMP, AddSub, SingleEq).",
            "mechanism_or_representation": "Sparse specialization: (1) a small set of 'operand-attending' heads focus attention (Q_end → K_token) on the two numeric operand tokens ({A},{B}) and separate 'operator-attending' heads focus on operator tokens; (2) these heads write information into the residual stream (via OV) which is then processed by MLPs in later layers; (3) MLP inputs begin to align (cosine similarity) with operand token unembedding vectors starting ~layer 12–18, and MLP output increments (MLP_out − MLP_in) increasingly align with the unembedding vector of the correct answer {C} in later layers (coarse-to-fine convergence).",
            "probing_or_intervention_method": "Path patching (activation patching) replacing a head/MLP activation with its counterfactual activation from non-calculation inputs; mean ablation (knockout via replacing activation with average counterfactual activation); attention-pattern inspection (END-query row of attention matrices); probing MLP_in/MLP_out via cosine similarity against unembedding vectors W_U[{token}].",
            "performance_metrics": "Baseline average next-token probability for correct answer ~82% across generated reference samples; patching/knockout of identified key heads/MLPs produces large drops in logit/probability: example head patch decreased logit by 14.0% (head 12.223), full knockout of identified components leads to ~70% relative accuracy decrease on the paper's generated dataset and ~66% relative drop on SVAMP; overlap of key heads identified on GSM8K vs generated data: ~60% overlap; knocking out overlapping heads caused ~56% (generated) / ~52% (GSM8K) drops. (See paper for layer‑by‑layer similarity curves and exact per-layer numbers.)",
            "error_types_or_failure_modes": "When key heads are knocked out the model typically outputs incorrect numeric tokens (often digits near the correct answer); LLMs sometimes generate plausible-but-wrong nearby numbers (coarse-to-fine pattern); multiplication/division require more key heads (harder, more distributed); tokenization of multi-digit numbers into digit tokens can complicate multi-digit arithmetic; key-head discoveries on one dataset do not fully cover all reasoning-required heads for complex math word problems (some heads attend to text for comprehension and are necessary for GSM8K).",
            "evidence_for_mechanism": "Causal path patching: patching individual heads' END-position activations with counterfactual activations substantially reduces the logit for the ground-truth answer; mean-ablation (knockout) of top-ranked heads produces sharp declines in accuracy compared to random-head knockout (Figure 3); attention analysis shows identified heads strongly concentrate on operand/operator tokens across 1000 samples (Figure 5, Figure 7); MLP input/output cosine-similarity trajectories show operands injected into MLP_in around layers 12–17 and MLP_out−MLP_in increasingly aligned to correct answer embedding in later layers (Figures 6, 13, 14); effect is transferable across datasets and to other LLMs (LLaMA2-13B, Mistral-7B) with similar inflection-layer patterns.",
            "counterexamples_or_challenges": "Not all calculation-related behavior is captured by the same small set of heads across all datasets: GSM8K discovered additional heads (4 non-overlapping heads) that attend to textual tokens needed for comprehension (e.g., 'GB', '.') which are important for complex word problems; precise MLP tuning can improve math but tuning more MLPs degrades general performance and increases compute; the mechanism is descriptive (coarse-to-fine progressive alignment) but does not show an explicit bitwise adder or symbolic algorithm identical to computer addition; generalization to arbitrarily large integers and compositional multi-step math beyond two-operand operations remains unproven though exponentiation and other ops show similar sparse-head patterns in initial checks.",
            "uuid": "e8131.0",
            "source_info": {
                "paper_title": "Interpreting and Improving Large Language Models in Arithmetic Calculation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "PathPatching+MeanAblation",
            "name_full": "Path patching (activation patching) and mean ablation knockout",
            "brief_description": "Causal intervention methods used to identify and validate model components causally responsible for arithmetic: path patching replaces activations on reference runs with counterfactual activations to measure causal effect on logits; mean ablation replaces activations with average counterfactual activations to knock out components and measure accuracy drop.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to LLaMA2-7B primarily; applied to LLaMA2-13B and Mistral-7B for transfer checks",
            "model_description": "Same decoder-only transformer family (LLaMA2 etc.). Path patching scans all attention heads and MLP nodes in the model DAG by replacing node outputs at the END token position and measuring change in logit for ground-truth answer {C}.",
            "arithmetic_task_type": "Two-operand arithmetic templates and counterfactual variants; transfer to SVAMP and GSM8K word problems.",
            "mechanism_or_representation": "Not a representation but a causal probing/intervention method that isolates contributions of sender nodes (individual heads/MLPs) to receiver node (logits) via hard interventions on activations, and validates via knockout (mean ablation).",
            "probing_or_intervention_method": "Path patching: replace activation of a single head/node in the reference forward pass with its activation computed under counterfactual non-calculation input; freeze other activations from reference; compute logit change. Mean ablation: replace activations with average activation from counterfactual data to neutralize task-specific signal for that component.",
            "performance_metrics": "Used to rank heads by causal effect on logit change; identified 'key heads' as those whose patching reduced the target-token logit by more than 5% (example: head 12.22/3 patch reduced logit by 14%); sequential knockout of top-k heads (effect-rank) produced large accuracy declines (e.g., accuracy falling from ~99% baseline to ~35% after top-k knockouts in generated tests), while random-rank knockouts produced small changes (~±2%).",
            "error_types_or_failure_modes": "Path patching assumes independence of patched pathways and relies on valid counterfactual activations; residual connections and multiple interacting heads mean some causal attributions may be distributed; patching may not capture synergistic effects between multiple heads/MLPs unless multiple-node patching is done.",
            "evidence_for_mechanism": "Direct causal effect measurements: scanning every head/node with path patching and averaging effects across sample pairs produced sparse sets of heads with large causal effects; mean ablation knockouts validated faithfulness by showing sharp performance degradation when these nodes were disabled compared to random ablations.",
            "counterexamples_or_challenges": "Patching one node at a time may miss interactions where multiple nodes jointly implement functionality; identified key nodes are dataset-dependent to some degree (GSM8K required additional text-attending heads); hard intervention requires computing activations on counterfactual inputs which may be ambiguous to design for more complex tasks.",
            "uuid": "e8131.1",
            "source_info": {
                "paper_title": "Interpreting and Improving Large Language Models in Arithmetic Calculation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "PreciseSFT",
            "name_full": "Precise supervised fine-tuning of identified heads/MLPs",
            "brief_description": "A fine-tuning procedure that updates only the parameters of the small subset of identified calculation-related components (selected attention head Q/K/V/O matrices and/or selected MLP layers) to improve arithmetic performance while preserving general capabilities and reducing compute.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to LLaMA2-7B and LLaMA2-13B in experiments",
            "model_description": "Fine-tuning performed on top-N identified attention heads (output matrices W_i,j^O and projection matrices W^Q/K/V) and optionally top MLP layers; example best setting was top-32 heads for LLaMA2-7B/13B.",
            "arithmetic_task_type": "Same math datasets: GSM8K, AddSub, SingleEq, SVAMP; general evaluation on MMLU and CSQA to test general capabilities.",
            "mechanism_or_representation": "Parameter-space intervention: targeted updates to the specialized attention head and MLP parameters to strengthen the arithmetic computation pathway identified by path patching, without altering the rest of the model.",
            "probing_or_intervention_method": "Supervised fine-tuning (SFT) restricted to selected parameters; gradients rescaled by H/h (number of total heads H and updated heads h) per layer; training hyperparameters: LR=2e-5, batch size 128, 2 epochs; trained on aggregated math datasets.",
            "performance_metrics": "Precise SFT (top-32 heads) produced an average improvement of ~15% across four math datasets; Precise SFT matched or outperformed full-model SFT on math tasks (e.g., Precise SFT outperformed Full SFT by 5.5% on SVAMP and 2.8% on GSM8K in reported runs); Precise SFT preserved general-task performance (Full SFT incurred ~5% drop on MMLU/CSQA while Precise SFT did not), and reduced compute: top-32 heads setting processed ~50 samples/sec with ~0.067B parameters tunable ( &lt;1% of total), versus full SFT much slower and tuning ~all parameters.",
            "error_types_or_failure_modes": "Including more MLPs in the tuned set can improve math scores but can harm general performance (e.g., adding top-3 MLPs raised GSM8K but decreased MMLU by ~1.5%); tuning too few components yields limited gains; precise SFT improvements are limited to the distribution of math problems seen during fine-tuning and may not fully generalize to complex multi-step reasoning without addressing text comprehension heads.",
            "evidence_for_mechanism": "Empirical ablations: sweeping the number of tuned heads showed best average gains at 32 heads; combination of top-32 heads + top-3 MLPs gave further math improvements but cost in general tasks and compute; training/time statistics and cross-dataset evaluations (SVAMP, GSM8K, AddSub, SingleEq, MMLU, CSQA) demonstrate precise SFT can raise math performance while preserving other capabilities.",
            "counterexamples_or_challenges": "While precise SFT is efficient, some tasks require additional text-comprehension heads (GSM8K had 4 non-overlapping heads that attend to text tokens), so precise SFT limited to arithmetic heads will not by itself solve word-problem comprehension; tuning MLPs increases parameter count and training time, and may introduce tradeoffs with generalization; the approach presumes that a small, identifiable subset of parameters causally drives the target skill, which may not hold for all tasks or larger models.",
            "uuid": "e8131.2",
            "source_info": {
                "paper_title": "Interpreting and Improving Large Language Models in Arithmetic Calculation",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Localizing model behavior with path patching",
            "rating": 2,
            "sanitized_title": "localizing_model_behavior_with_path_patching"
        },
        {
            "paper_title": "A mathematical framework for transformer circuits",
            "rating": 2,
            "sanitized_title": "a_mathematical_framework_for_transformer_circuits"
        },
        {
            "paper_title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "rating": 2,
            "sanitized_title": "how_does_gpt2_compute_greaterthan_interpreting_mathematical_abilities_in_a_pretrained_language_model"
        },
        {
            "paper_title": "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space",
            "rating": 2,
            "sanitized_title": "transformer_feedforward_layers_build_predictions_by_promoting_concepts_in_the_vocabulary_space"
        },
        {
            "paper_title": "Understanding arithmetic reasoning in language models using causal mediation analysis",
            "rating": 1,
            "sanitized_title": "understanding_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Interpretability in the wild: a circuit for indirect object identification in GPT-2 small",
            "rating": 1,
            "sanitized_title": "interpretability_in_the_wild_a_circuit_for_indirect_object_identification_in_gpt2_small"
        }
    ],
    "cost": 0.0148325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Interpreting and Improving Large Language Models in Arithmetic Calculation
3 Sep 2024</p>
<p>Wei Zhang 
University of Science and Technology
China</p>
<p>Alibaba Cloud</p>
<p>Chaoqun Wan 
Alibaba Cloud</p>
<p>Yonggang Zhang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#99;&#115;&#121;&#103;&#122;&#104;&#97;&#110;&#103;&#64;&#99;&#111;&#109;&#112;&#46;&#104;&#107;&#98;&#117;&#46;&#101;&#100;&#117;&#46;&#104;&#107;">&#99;&#115;&#121;&#103;&#122;&#104;&#97;&#110;&#103;&#64;&#99;&#111;&#109;&#112;&#46;&#104;&#107;&#98;&#117;&#46;&#101;&#100;&#117;&#46;&#104;&#107;</a>. 
Hong Kong Baptist University</p>
<p>Yiu-Ming Cheung 
Hong Kong Baptist University</p>
<p>Xinmei Tian 
University of Science and Technology
China</p>
<p>Institute of Artificial Intelligence
Hefei Comprehensive National Science Center</p>
<p>Xu Shen 
Alibaba Cloud</p>
<p>Jieping Ye 
Alibaba Cloud</p>
<p>Interpreting and Improving Large Language Models in Arithmetic Calculation
3 Sep 20240630D61ACD9E751AEA8E9A1AE06042B6arXiv:2409.01659v1[cs.CL]
Large language models (LLMs) have demonstrated remarkable potential across numerous applications and have shown an emergent ability to tackle complex reasoning tasks, such as mathematical computations.However, even for the simplest arithmetic calculations, the intrinsic mechanisms behind LLMs remain mysterious, making it challenging to ensure reliability.In this work, we delve into uncovering a specific mechanism by which LLMs execute calculations.Through comprehensive experiments, we find that LLMs frequently involve a small fraction (&lt; 5%) of attention heads, which play a pivotal role in focusing on operands and operators during calculation processes.Subsequently, the information from these operands is processed through multi-layer perceptrons (MLPs), progressively leading to the final solution.These pivotal heads/MLPs, though identified on a specific dataset, exhibit transferability across different datasets and even distinct tasks.This insight prompted us to investigate the potential benefits of selectively fine-tuning these essential heads/MLPs to boost the LLMs' computational performance.We empirically find that such precise tuning can yield notable enhancements on mathematical prowess, without compromising the performance on non-mathematical tasks.Our work serves as a preliminary exploration into the arithmetic calculation abilities inherent in LLMs, laying a solid foundation to reveal more intricate mathematical tasks.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have experienced rapid advancements and shown impressive language understanding capabilities (Devlin et al., 2019;Brown et al., 2020;Chowdhery et al., 2022).Notably, LLMs exhibit emergent abilities (Wei et al., 2022b) that enable them to solve intricate reasoning tasks akin to humans, such as mathematical computations (Frieder et al., 2023;Jie et al., 2022), chain-of-thought reasoning (Wei et al., 2022c;Kojima et al., 2022), few-shot prompting (Brown et al., 2020;Alayrac et al., 2022), etc.Despite these impressive characteristics, the complex inner processes governing LLMs' functionality have yet to be fully illuminated, due to the complex and intricate non-linear interactions within densely-connected layers.Comprehending these underlying mechanisms could contribute to predicting how the LLMs behave beyond their training data (Mu &amp; Andreas, 2020), gaining insights into the emergence of certain behaviors (Nanda &amp; Lieberum, 2022;Barak et al., 2022;Wei et al., 2022a), as well as identifying and rectifying errors present in the specific models (Hernandez et al., 2021;Vig et al., 2020).</p>
<p>In this work, we take the first attempt to interpret the inner process of LLMs through the lens of mathematical computation problems, which are conducted on publicly available LLMs (e.g., LLaMA2 series (Touvron et al., 2023b)).Unlike typical language comprehension tasks, mathematical computation tasks involve concise problem statements with definitive correct answers, requiring a process of reasoning and calculation rather than direct copying to derive the solutions.These characteristics enable us to gain insights into the models' reasoning capabilities without interference from unrelated factors.Specifically, we focus on tasks involving the arithmetic calculation with two operands, i.e., addition, subtraction, multiplication, and division, which are fundamentals of mathematical computation.To this end, we create datasets of various types of sentences that involve the calculation logic, such as "The addition of 3 and 5 equals to " in Figure 1.The LLMs could provide answers with high confidence scores of over 80% on average.</p>
<p>To unveil how these models correctly complete the task (e.g., "3 + 5 = 8"), we begin by identifying the taskrelated internal components in LLMs.We do a hard in- tervention (Pearl, 2009) on the transformer attention heads and multi-layer perceptrons (MLPs) to observe their effects on the predicted logits 1 .Our findings reveal that only a small percentage (&lt; 5%) of the attention heads and the MLPs after these heads significantly impact the model's performance.Namely, LLMs frequently involve these attention heads and the subsequent MLPs when completing the calculations.Subsequently, we knock out these frequentlyinvolved heads/MLPs to validate their faithfulness.We find that the model performance decreases sharply when those pivotal heads/MLPs are knocked out, resulting in a decrease of around 70% in accuracy.</p>
<p>To interpret the working mechanism of identified heads/MLPs towards human-understandable explanations, we gain a deeper analysis of their operational "behaviors".Specifically, we investigate the attention patterns of the crucial heads, and find that these attention heads exhibit a strong focus on the tokens representing operands and operators within mathematical sentences, demonstrating a relative insensitivity to other non-relevant tokens.For the analysis of MLPs, we compare the correlations between the embeddings of MLPs' input/output and the embeddings of number tokens (i.e., operands and answers).It reveals that the MLPs, guided by these number-attended heads, take operands as input, and mirror the attributes of tokens corresponding to correct answers more closely.These observations lead us to hypothesize that LLMs may initially employ a set of heads to pinpoint arithmetic operands from text, subsequently engaging MLPs to work out the answers.Additionally, the observed behaviors of these heads/MLPs exhibit a high degree of transferability, analogous to adversarial examples being transferable across models (Szegedy et al., 2014).Namely, the key heads/MLPs identified on one dataset are also effective for other datasets.For instance, their impact is noticeable on the publicly available math datasets (e.g., SVAMP (Patel et al., 2021)), as well as varied data formats involving multi-digit integers, rational numbers, etc.This empirical observation underscores the crucial role of key heads/MLPs in mathematical calculations.</p>
<p>In addition to uncovering the internal mechanisms, we have devised an effective strategy that involves targeted finetuning of the specific attention heads and MLPs closely tied to mathematical computations, thereby enhancing the model's mathematical prowess.The experimental results are compelling: with fine-tuning as few as 32 attention heads (with a total of 1024 heads), we observe a remarkable improvement in the model's mathematical capabilities.This precise tuning methodology not only matches but can surpass the enhancements achieved through full-model finetuning.Moreover, this fine-grained strategy of adjustment has a distinct advantage-it leaves most of the model's parameters unchanged, avoiding the performance trade-offs in non-mathematical domains commonly observed with fullmodel fine-tuning.</p>
<p>In summary, this work aims to delve into the inner mechanism of LLMs through mathematical calculation tasks, along the pipeline of "identify-analyze-finetune" shown in Figure 1.Our findings reveal a sparsity in the attention heads of LLMs, with less than 5% of heads exhibiting close correlations.These heads particularly attend to the operands and operators, while the subsequent MLPs gradually deduce the correct answers.The discovered mechanism shows strong cross-dataset transferability and inspires us to precisely finetune the calculation-related heads/MLPs for better mathematical capability.We empirically find that precise tuning brings in much less impact on non-mathematical tasks when improving the targeted ability of LLMs.</p>
<p>Related Works</p>
<p>Interpretability Methods.Interpreting the inner mechanism of large language models (LLMs) has become increasingly urgent in recent years (Madsen et al., 2022;Rauker et al., 2023), especially when LLMs are applied in highstakes decision-making domains such as healthcare, criminal justice, and finance (Obermeyer et al., 2019;Rudin, 2019;Bender et al., 2021).Vig et al. (2020) adapted the approach of causal mediation analysis (CMA) (Pearl, 2001) for interpreting the deep language models, and it has been applied for various tasks, such as subject-verb agreement (Finlayson et al., 2021), natural language inference (Geiger et al., 2021), retention of factual associations (Meng et al., 2022;Geva et al., 2023).Furthermore, path patching extends the concept of CMA by measuring how a treatment effect is mediated by node-to-node connections between individual neurons or features.Recent works have used path patching to explain neural networks in terms of circuits (Olah et al., 2023), identified for different capabilities including indirect object identification (Wang et al., 2023a), greater-than computation (Hanna et al., 2023), and mapping answer text to answer labels (Lieberum et al., 2023).</p>
<p>Interpretability for Mathematical Tasks.Mathematical ability has long been a subject of interest in natural language processing (Kushman et al., 2014;Huang et al., 2016;Wang et al., 2017;Thawani et al., 2021).Some studies have investigated the mathematical abilities of LLMs (Frieder et al., 2023;Saxton et al., 2019;Nogueira et al., 2021;Qian et al., 2023;Imani et al., 2023;Romera-Paredes et al., 2024), but they mainly focus on explaining what these models can do rather than how they do it.In contrast, some other studies have dived deeper into the LLM structure without treating LLM as an inscrutable black box.Stolfo et al. (2023) identified the key attention layers relating to arithmetic questions, but lacking in-depth explanation and validation of the key layers' behaviors.Wu et al. (2023) scaled the methods from causal abstraction to understand how Alpaca (7B) (Taori et al., 2023) follows the instruction in comparing two numbers.(Hanna et al., 2023) provided a causal explanation about how GPT2-small (0.1B) (Radford et al., 2019) implements the "greater-than" task, but only reveal simple phenomena limited by the small size of model and the lack of diversity in the dataset.</p>
<p>Fine-tune LLMs for Mathematical Tasks.Numerous studies improve the mathematical reasoning ability of LLMs by aggregating various sampled reasoning paths during either fine-tuning or inference.Cobbe et al. (2021) train and devise a reasoning path verifier to select the correct results during inference.Wang et al. (2023b) propose to sample various reasoning paths during inference and then derive the final result by majority voting on the answers or through verifiers (Li et al., 2023).Uesato et al. (2022) explore to use of reinforcement learning methods for improving the mathematical reasoning abilities of LLMs.Several works apply the idea of rejection sampling along with other techniques to filter the diverse sampled reasoning paths for fine-tuning data augmentation (Huang et al., 2022;Zelikman et al., 2022;Ni et al., 2023).There also exist related works (Panigrahi et al., 2023) that locate key parameters to update for better taskspecific ability.Panigrahi et al. (2023) locates a minuscule subset of parameters from an already fine-tuned model onto a pre-trained model without further tuning.The selection process for this subset is via optimizing the task-related objective function with L1 norm ensuring the sparsity of the subset.In our work, we locate the task-related parameters of pre-trained model via measuring the causal effect of each component, then precisely fine-tune the key components for mathematical tasks.</p>
<p>Preliminary</p>
<p>Large Language Models (LLMs).The LLMs utilized in this work comprise LLaMA2-7B and LLaMA2-13B (Touvron et al., 2023a).These are pre-trained language models freely available from HuggingFace2 .All of these models are decoder-only transformers equipped with multi-head attention (MHA) and a single MLP in one transformer layer.For example, LLaMA2-7B consists of 32 transformer layers and 32 attention heads in MHA for each layer.</p>
<p>Transformer Architecture.The input to the transformer is a combination of position and token embeddings in R N ×d , where N is the number of tokens in the input and d is the model dimension.Following the definitions in (Elhage et al., 2021), the input embedding serves as the initial value for the residual stream, which is read from and written to by all attention heads and MLPs.Focusing on individual heads, the j-th head in the i-th layer is parametrized by four matrices:
W i,j Q , W i,j K , W i,j V ∈ R d× d H , and W i,j O ∈ R d H ×d .
To simplify these parameters, we can express them as lowrank matrices in R d×d :
W i,j OV = W i,j O W i,j V and W i,j QK = W i,j Q (W i,j K ) T .
The QK matrix is used to compute the attention pattern A i,j ∈ R N ×N for head (i, j), while the OV matrix determines the information written into the residual stream.At the end of the forward pass, a layer norm is applied before the unembed matrix W U projects the residual stream into logits.</p>
<p>Task and Dataset.We focus on classic and widely encountered mathematical operations, e.g., addition, subtraction, multiplication, division.Taking addition as an example, the arithmetic logic of addition ({A} + {B} = {C}) might naturally appear in sentences.Taking inspiration from the sentence styles and forms present in mathematical benchmarks of GSM8K (Cobbe et al., 2021) and SVAMP (Patel et al., 2021), we create a dataset for the addition task containing 10, 000 samples based on 36 templates with random single-token names, objects, and numbers.To assess the performance of LLMs on the calculation task, we measure the prediction probability of the {C} token.The average probability of correct predictions across the models was 82%.In this study, we select the samples that the language models are able to predict correctly.We denote the sentences generated by this procedure as reference data using the notation of X r .For the templates and sentences, please refer to Figure 8 and Figure 10 in Appendix A.</p>
<p>Moreover, to meet the demand for perturbing component activation, we create another dataset comprising counterfactual sentences without the inclusion of calculation logic, using the notation of X c .The samples are generated following two core principles: (1) maintaining the grammatical structures derived from the X r templates; (2) substituting several crucial words responsible for the calculation logic with irrelevant words.For example, the sentence from X r like "42 plus 34 is equal to " is replaced to the counterfactual one "42 nothing 34 is equal to ".In this way, it allows for a direct reflection of the model's impact on the arithmetic calculation tasks, rather than being influenced by the sentence structure or syntax.</p>
<p>Method</p>
<p>Our goal is to interpret the LLMs in a way that is humanunderstandable, thus enabling targeted modification of models through precise SFT.This section delves into the "identify-analyze-finetune" methodology.First, in Section 4.1, we describe the process of identifying and validating key components within LLMs.Then in Section 4.2, we examine the inherent patterns of these pivotal components to decode their behavior and distinct features.Finally, in Section 4.3, we introduce a strategy of precise SFT that fine-tunes these influential components to enhance the proficiency in calculation.</p>
<p>Key Components Identification.</p>
<p>The computation of the LLM can be reorganized as a directed acyclic graph (DAG) (Wang et al., 2023a).In the graph, each node is a computation component, including attention heads, MLP layers, residual connections, and each edge represents the data flow that the output of the previous node will be transposed to the input of the later node.Please refer to Appendix B for more details.To unravel the underlying cause of the model's predicted answer, we employ the causal intervention technique known as path patching (Goldowsky-Dill et al., 2023;Wang et al., 2023a).By perturbing targeted activation with counterfactual data X c and freezing others with reference data X r , the comparison on output logits is employed to measure the counterfactual effect.The whole process is illustrated in Algorithm 1.In this work, we scan through all nodes N one by one, and measure the changes in the output logit of ground-truth token {C}, recoding in E N .Notably, since the residual operations and MLPs compute each token separately (Elhage et al., 2021), patching the head output at the END position (i.e., the last token in the input sentence) is enough to measure the effects on the next token prediction.</p>
<p>Explanations for model behavior can easily be misleading or non-rigorous (Bolukbasi et al., 2021;Wiegreffe &amp; Pinter, 2019).To address this issue, we further assess the importance of the identified heads/MLPs, while also confirming the insignificance of others.For this purpose, we employ a knockout technique called mean ablation (Wang et al., 2023a) to deactivate the individual heads/MLPs and observe their impact on model performance.Specifically, we replace their activation with average activation across counterfac-</p>
<p>Algorithm 1 Identifying Key Components</p>
<p>Input: Set Ω of reference and counterfactual sample pairs (X r , X c ), model M with nodes N .Output: Causal effects for N : E
N . for (X (i) r , X (i) c ) in Ω do Compute all activations A r , A c on (X (i) r , X (i) c ) for n in N do A ′ r (n) ← A c (n); ▷ replace output in A r by A c A ′ r (k) ← A r (k), ∀k ∈ [1, • • • , |N |], k ̸ = n. logit o ← M(X (i) r , A r ) ▷ get original logits logit p ← M(X (i) r , A ′ r ) ▷ get patched logits s (i) n ← logitp−logito logito
▷ causal effect end for end for Return:
s n = |Ω| i=1 s (i) n |Ω|
▷ averaged effect w.r.t.samples tual data X c to remove the task-related information.By observing changes in model performance, we can verify the roles of these key heads/MLPs.</p>
<p>Pattern Analysis.</p>
<p>To make the identified heads/MLPs accessible to human understanding, we conduct a deeper analysis of their operational "behaviors".For attention heads, we examine the attention pattern A i,j ∈ R N ×N to comprehend which tokens are prioritized.N is the number of input tokens.Specifically, we begin by gathering the respective attention patterns A i,j on reference data X r of the key heads.We extract the last row of A i,j for each sample, analyzing the attention scores A EN D i,j ∈ R 1×N between the Query token at the END position and each Key token, and obtaining the averaged scores w.r.t.samples.Generally, the type of token with the highest attention score represents the characteristics of the head, such as numbers, math symbols, etc.</p>
<p>For MLPs, we use the unembedding matrix as the probing to measure the content of token, especially numerical tokens, contained in MLPs' inputs and outputs.Prior studies, such as those reported in (Elhage et al., 2021), have illustrated that the MLP layer initially receives its input from the residual stream (i.e., M LP in ), subsequently adding its output back into that stream (i.e., M LP out ).Let W U represent the unembedding matrix, and W U [ * ] denote the unembedding vector corresponding to a specific token.We calculate the cosine similarity between M LP in , M LP out and
W U [{A}], W U [{B}], W U [{C}]
to reflect the information the MLP receives and generates.To isolate the specific contribution of MLP to specific numerical tokens, we further evaluate the subtraction of outputs and inputs of MLP, i.e.,
M LPout−M LPin ||M LPout−M LPin|| • W U [{A}]
||W U [{A}]|| .Research in (Geva et al., 2022) presents that each MLP layer's output token repre-sentation can be characterized as an additive update influencing the evolving representation across vocabularies.Our methodology is aligned with these works, while we mainly focus on the token embeddings of right/wrong answers to reveal the contribution of MLPs on the calculation tasks.</p>
<p>Precise Fine-tuning.</p>
<p>Supervised Fine-Tuning (SFT) is widely used for enhancing a model's mathematical capabilities.Building on this, precise SFT only updates those components closely associated with mathematical abilities, while keeping the rest parameters unchanged.Algorithm 2 illustrates the whole process.For the i-th attention layer, the output matrix
W i O is split into equal size blocks for each head W i,1 O , W i,2 O , • • • W i,H O . As
Algorithm 2 Precise Fine-tuning
Require: Model M, input X, index of key heads Φ, iterations I, learning rate η, W θ = W Q/K/V /O for (i, j) ∈ Φ do W i,j θ .requires grad = T rue end for ▷ activate key heads loop I times L = M.forward(X) L.backward() for w ∈ W θ do w = w − η * w.grad end for
▷ update target parameters end loop is verified in (Elhage et al., 2021), it is equivalent to running heads independently, multiplying each by its own output matrix, and adding them into the residual stream.For the selected individual heads, precise SFT updates the parameters of four matrices:
W i,j Q , W i,j K , W i,j V ∈ R d× d H , and W i,j O ∈ R d H ×d .
For the selected MLP layer, precise SFT updates all parameters in this layer.Moreover, since we adjust only a small fraction of the parameters, precise SFT naturally benefits from shorter training times and minimal impact on the model's original capabilities.</p>
<p>Experiments</p>
<p>The experiments are organized as follows: (1) identify the calculation-related key components via path patching and validate their importance in implementing arithmetic calculation via knockout in Section 5.1; (2) understand the behavior of the newly identified components by examining their attention patterns and embeddings in Section 5.2; (3) improve the mathematical capability via precise supervised fine-tuning on math benchmarks in Section 5.3.For simplicity, we primarily report the results of LLaMA2-7B, while the results of other models can be found in Appendix.Location of key heads.In Figure 2, we visualize the effect of each head according to the serial numbers of the heads and layers.This arrangement allows for a clear comparison of the causal impact of each head to the logit of ground-truth token {C}.The red squares indicate heads that have a significant positive impact on predicting the output token, while the blue squares represent heads that have a negative effect.From these results, we observe that: (i) Only a small number of heads have a noteworthy influence on the output.Specifically, when the heads such as 12.223 is patched, there is a substantial decrease of 14.0% on the logit of token {C}, which highlights their positive contribution to the calculation tasks.We classify heads that exhibit logit change exceeding −5% as "key heads".The sparse distribution of these key heads motivates us to explore their specific functionalities and characteristics in Section 5.2.(ii) The discovered key heads are mainly located in the middle layers.For LLaMA2-7B, key heads emerge starting from the 12th layer for all arithmetic calculations.Prior layers exhibit heads that do not exert a direct effect on the output logits.Key heads are primarily concentrated between layers 12 and 17. (More analysis of the key heads in other LLMs can be found in Appendix C.)</p>
<p>Location of key MLPs.The last column in Figure 2 visualizes the effect of each MLP layer on the logit of ground-truth token {C}.It is observed that MLPs before the identified heads (0−16) have almost no impact on the outputs (approximately ±0.0%).In contrast, after the 17-th layer, MLPs exhibit a much larger effect (approximately ±10.0%65.48% 17.08% 6.54% 5.25% 2.01% "4" "3" "1" "2" "5"</p>
<p>37.70% 27.15%</p>
<p>7.09% 6.65% 5.78% "1" "2" "9" "8" "3"</p>
<p>Ø Input: 4.2 plus 2.5 equals to Ø Next token: 1</p>
<p>Ø Top-5 prediction probability:</p>
<p>Ø Input: 4.2 plus 2.5 equals to Ø Next token: 6</p>
<p>Ø Top-5 prediction probability:</p>
<p>18.80% 17.93% 12.32% 11.58% 9.83% "1" "6" "4" "5" "2"</p>
<p>Ø Input: The war lasted 5 years from 1723 to 172 Ø Next token: 3</p>
<p>Ø Top-5 prediction probability:</p>
<p>Ø Input: The war lasted 5 years from 1723 to 172 Ø Next token: 8</p>
<p>Ø Top-5 prediction probability:</p>
<p>87.65%</p>
<p>7.54% 3.79% 0.54% 0.22% "8" "9" "7" "6" "5"</p>
<p>91.70%</p>
<p>2.07% 1.59% 0.95% 0.80% "6" "7" "1" "4" "2"</p>
<p>Transfer to other dataset (after knockout)</p>
<p>19.69% 19.69% 12.71% 8.73% 8.67%</p>
<p>"3" "6" "8" "9" "7" 7.54% 3.79% 0.54% 0.22% "6" "1" "5" "4" "2" 76.51% 7.23% 6.59% 2.75% 1.60% "5" "1" "6" "2" "3" Figure 4: After knocking out the key heads, LLaMA2-7B predicts incorrectly on the cases of SVAMP dataset and other data formats of multi-digit integers, rational numbers.pothesize that the calculation process is firstly implemented through the key heads, then the subsequent MLPs gradually work out the final results.We validate this in Section 5.2.</p>
<p>Validation of key components.To fully validate the faithfulness of the discovered key heads, we perform additional checks by observing the performance drop when knocking out these components.In Figure 3, all heads are sorted in a certain order by the importance score shown in Fig. 2 and knocked out one by one.It shows that, as the heads are gradually knocked out, the performance of the model drops sharply in "effect-rank", while keeping stable (relatively minor effect within 2%) in "random-rank".We also exhibit the transferability of the key heads with different data prompts or formats as shown in Figure 4.The model becomes largely confused to output incorrect numbers after knocking out the identified key heads.On the dataset SVAMP, there is a relative performance drop (−22.9%/34.7%=−66.0%)after the knockout, aligned with the result on our generated dataset.The above results demonstrate that the discovered compo- nents play an important role in the language model's ability to complete the calculation task.</p>
<p>Understanding Calculation-related Component</p>
<p>Behaviors.</p>
<p>Key heads behavior.In order to better understand the "behavior" of the heads that have a significant impact on calculation, we begin by analyzing their attention patterns, and check the attention scores between Query END token and each Key token as illustrated in Sec.4.2.Our findings reveal that these heads exhibit a strong focus on tokens of operands or operators.For example, heads 13.11 and 12.22 have high attention scores on numbers including {A} and {B}, while heads 14.2 and 11.8 attend more to symbols or text indicating operations like "+", "-", "plus", "div", etc.We randomly select 1000 samples from reference data and plot the distribution of averaged attention scores on key heads (arranged in two groups) for four arithmetic calculations.</p>
<p>As illustrated in Figure 5, the operand heads and the operator heads are colored in red and green respectively, and highlighted at the positions of operands and operators.It is clear that these heads exhibit distinctly different distributions and show minimal attention to tokens outside of the operands/operators.Moreover, we visualize the attention patterns of the key heads (e.g., 13.11) on various types of sentences in Figure 7.It reveals that the key heads also primarily prioritize number operands (e.g., '1' and '5' in the first case) even for unseen data formats.This observation provides an explanation for why the deactivation of the key heads can influence the model's perception on number tokens and consequently affect its prediction when transfer- ring to other datasets (shown in Figure 4).For more case studies on the key heads, such as the attention pattern on operators, please refer to Figure 16 in Appendix F. After that, the numbers '6' and '1' appear top at the subsequent layers 20 and 21.In summary, the LLM predicts the next token as '7' in a single inference.However, within the LLM's architecture, the answer '7' is the result of a collaborative process across multiple layers 22/23/25/27, after the layers 17/19/20/21 generate '3'/'4'/'6'/'1', respectively.The results demonstrate that the answer '7' is not deduced directly, and MLPs perform calculations in a "layerby-layer" manner, somewhat akin to the addition process in computers (a comparison of these two processes are presented Appendix H).Additionally, we observe that numbers close to the correct answer, such as '6' and '8', also appear Table 1: Overall performance.We evaluate the capabilities of LLaMA2-7B and LLaMA2-13B, transitioning from generic tasks (e.g., MMLU and CSQA) to mathematical tasks (e.g., GSM8K, AddSub, SingleEq, and SVAMP).Supervised finetuning across the entire parameter set (denoted as Full SFT) leads to enhanced performance in math-related tasks, albeit at the expense of its capabilities in generic tasks.In contrast, selectively tuning only the parameters of 32 critical attention heads (denoted as Precise SFT) yields comparable improvements while preserving the model's proficiency in generic tasks, with faster training speed (samples processed per second) and less tuned parameters.at the top in layer 23.However, in subsequent layers, the correct answer '7' consistently remains top while '6' and '8' decline.It indicates that LLMs may do computations in a coarse-to-fine manner, where the result is firstly regressed to an embedding around that of the right answer, and then converges to the final output based on the fine-grained information introduced by subsequent MLPs.</p>
<p>Mathematical</p>
<p>Consolidating these findings, we can assert with some confidence that LLMs initially leverage attention heads to focus on operands ({A} and {B}) and the operator, relaying this information to downstream MLPs.Over time, the MLPs progressively bolster {C} and diminish the effect of confused answers, carrying out the calculation to final results.</p>
<p>Precise SFT on Calculation-related Components.</p>
<p>Experimental details.We evaluate precise SFT on four mathematical datasets (GSM8K (Cobbe et al., 2021), AddSub (Hosseini et al., 2014), SingleEq (Koncel-Kedziorski et al., 2015), SVAMP (Patel et al., 2021)), and another two datasets (MMLU (Hendrycks et al., 2020) and CSQA (Saha et al., 2018)) to evaluate the generic ability.</p>
<p>During training, we optimize the key components only and leave the other components unchanged.We gather all training data from four mathematical datasets, and perform SFT updating on top 32 key heads.Following (Yu et al., 2023), the gradient is rescaled by H h , where H is the number of all heads in each layer, h is the number of updated heads in each layer.In practice, we train LLaMA2-7B and LLaMA2-13B with a learning rate of 2 × 10 −5 and a batch size of 128 for 2 epochs.The warm up ratio and weight decay are set as 0.02 and 0.1 by default, respectively.All experiments are conducted on 8 NVIDIA A100 80GB GPUs.Ablative studies.The key issue with Precise SFT lies in determining the quantity and specific set of components to adjust.To demonstrate this, we experimented with varying numbers of heads and MLPs, with the results laid out in Table 2.We discovered that fine-tuning 32 heads yields the best average improvement across different numbers of involved heads.We also compared experiments with the introduction of MLPs.We observed that as more MLPs are added, the mathematical capability improves by 2.1%, but the general performance will decrease by 1.5% (results in Appendix G).Overall, the top-3 MLPs yielded the best comprehensive results.However, even the introduction of a single MLP can reduce computational efficiency by 15%.</p>
<p>How to more precisely fine-tune MLPs will be explored in our future work.</p>
<p>More discussions.The above results underscore the potential of employing interpretability tools to analyze the inner mechanism of LLMs and to enhance their specific capabilities.However, there are several areas that require deeper investigation: (i) Our primary experiments and discussions center around the LLaMA2 series.The results presented in Appendix C demonstrate the potential for generalization across different LLMs, such as Mistral-7B (Jiang et al., 2023).For more rigorous considerations, it's necessary to perform specific adaptations on a broader range of LLMs.</p>
<p>(ii) This work mainly focuses on interpreting the fundamental ability of "arithmetic calculation", since it's universally shared across various levels of complexity for mathematical problems.The results in Appendix E reveal that solving the math word problems requires a synergy of multiple skills including "text comprehension" and "arithmetic calculation", which is aligned with the findings in recent research (Opedal et al., 2024).It's imperative for continued research to investigate more complex mathematical problems.(iii) The potential of generalizing to more complex mathematical tasks like exponentiation (e.g., "{A} to the power of {B} equals ") has been validated in Appendix D. An intriguing research direction would be to investigate the shared and distinct mechanisms across various mathematical tasks.</p>
<p>Conclusion</p>
<p>In this study, we have identified, analyzed, and fine-tuned the internal components responsible for the mathematical calculation capability of LLMs.The language models frequently involve sparse heads to particularly attend to operands and operators, and subsequent MLPs to work out answers.We apply the precise tuning on the calculation-related heads/MLPs for better mathematical capabilities, with less impact on non-mathematical tasks compared with tuning all parameters.These findings contribute to a better understanding of the inner mechanism of LLMs.18.29% 5.49% 5.04% 4.55% "8" "1" "3" "2" "9"</p>
<p>24.98% 20.07%19.01%15.39% 7.44%</p>
<p>"2" "0" "1" "3" "5"</p>
<p>Ø Input: 42 nothing 34 is equal to 7 Ø Next word: 8</p>
<p>Ø Top-5 prediction probability:</p>
<p>Ø Input: 42 plus 34 is equal to 7 Ø Next word: 6</p>
<p>Ø Top-5 prediction probability:</p>
<p>22.03% 21.69% 13.78% 10.57% 4.27% "8" "6" "2" "0" "."</p>
<p>Ø Input: Mary has 3 apples, then Mary gains 4 cups.What is the total number of tables that John has?The answer is Ø Next word: 1</p>
<p>Ø Top-5 prediction probability:</p>
<p>Ø Input: Mary has 3 apples, then Mary gains 4 apples.What is the total number of apples that Mary has?The answer is Ø Next word: 7</p>
<p>Ø Top-5 prediction probability:</p>
<p>24.95% 15.87% 13.57% 12.55% 8.11% "1" "2" "4" "3" "5"</p>
<p>38.77% 19.19% 12.39% 8.00% 6.63%</p>
<p>"7" "1" "3" "4" "2"</p>
<p>96.34% 0.59% 0.55% 0.46% 0.44% "6" "7" "5" "4" "8"</p>
<p>Figure 9: Examples of reference data (with addition logic) and counterfactual data (without addition logic).Given the input sentence, the results of next word prediction are provided by LLaMA2-7B.</p>
<p>We have included a list of 36 templates used in this work as shown in Figure 8.All these templates share the same calculation logic.we sample the <A> and <B> from {1, • • • , 9}, since LLaMA2 tokenizes each digit individually (e.g., '42' is tokenized to '4' and '2').Based on the above templates, we generate the sentences that the LLMs can predict the addition result {C} correctly as the reference data X r .We generate the counterfactual data X c following the principles depicted in Section 3, where we replace the words (e.g., "plus", "minus", "times", "ratio") with a randomly-selected term from the set {"none", "nothing", • • • , "null"}, and replace the operations (e.g., "+", "-", "*", "/") with a randomly-selected term from the set {"&lt;", "&gt;", • • • , "@"}.We show three cases in Figure 9 with the inspection into the top-5 prediction probability of LLaMA2-7B.Moreover, in Figure 10, we also construct several different types of linguistic meanings for the addition task: "time span" and "object accumulation".For the templates 1-8 of "time span", we sample from a</p>
<p>+</p>
<p>Reference data Xr
••• ••• ••• ••• ••• ••• Head 0.0 + Hard intervention</p>
<p>Output comparison</p>
<p>Hard intervention</p>
<p>Figure 11: A case illustration of the method "path patching".It measures the importance of forward paths (i.e., the red lines that originate from Head 0.31 to Output) for the two-layer transformer in completing the task on reference data.</p>
<p>Path Patching.To discover the cause of the predicted answer, we employ the causal intervention technique known as path patching (Goldowsky-Dill et al., 2023;Wang et al., 2023a).This approach is highly effective in analyzing the causal relationship between two computation nodes (Sender − → Receiver).This helps us determine whether Sender is the cause of Receiver, and the connections between them are important for the model in implementing the task.Specifically, the entire process of path patching is shown in Figure 11, where the node pair Sender − → Receiver is set as Head 0.31 − → Output.Firstly, given reference data X r and counterfactual data X c , the activations of all heads are gathered for preparation of the later perturbation.Then, we do a hard intervention on the Head 0.31 that is perturbated to its activation on X c , where the effect will be further propagated to the Ouput node along with a set of paths P. To ensure an independent observation of the impact from the Head 0.31, P comprises the forward pathways through residual connections and MLPs except for the other attention heads (e.g., Head 0.0, • • • , 0.30, 1.0, • • • , 1.31).Thus we do a hard intervention on the other heads by freezing their activations on X r .Finally, we obtain the final output logits to measure the impact of this perturbation.If there is a significant change in final logits, then the patched paths: Sender − → Receiver are essential for the model in completing the task.</p>
<p>In this work, to identify the important heads contributing to the calculation task, we scan through all heads as the Sender node denoted by h, and set the Receiver node as output logits, and measure the changes in the output logit of ground-truth token {C}.Pathways h → logits that are critical to the model's computation should induce a large drop in the logit of token {C} after patching.Notably, since the residual operations and MLPs compute each token separately (Elhage et al., 2021), patching the head output at the END position (i.e., the position of the last token in the input sentence) is enough to measure the effects on the next token prediction.</p>
<p>C. More Results of Other LLMs.Key MLPs Behavior.In Figure 13, the similarities of MLP input and number operands {A}/{B} across all models demonstrate ascending and descending trends.Specifically, the pivotal points for these trends, delineated as (start- inf lection-end), are as follows: (13-18-28) for LLaMA2-7B, (13-18-35) for LLaMA2-13B, and (13-20-28) for Mistral-7B.In Figure 14, the similarities of M LP out -M LP in and right answer {C} show a pattern of initial stabilization followed by an increase.The critical points for LLaMA2-7B/LLaMA2-13B/Mistral-7B are again (13-18-28), (13-18-35), and (13-20-28).The inflection points in both Figure 13 and Figure 14 are nearly identical, indicating consistent trend shifts across the models.It helps to verify that LLMs initially leverage attention heads then relaying information to downstream MLPs, to progressively carry out the calculation to final results.Furthermore, the above findings appear to be general and robust across different LLMs, not limited to a specific model.</p>
<p>D. Key Component Location across Calculation Tasks.</p>
<p>We investigate the location of key components for each calculation task individually, as shown in Figure 15.The discovered key heads could be shared across four tasks, which are sparsely distributed in the middle layers.Specifically, when examining subtraction and addition tasks, we could summarize two insightful symmetries between them.The identified key heads of two tasks are almost the same, albeit with different magnitude of the effect.This phenomenon could reveal the symmetry of key head "location" in addition and subtraction.Moreover, the tasks of multiplication and division exhibit a greater number of key heads compared to the tasks of addition and subtraction.We assume it could be attributed to their more intricate operations within multiplication and division.Generalize to other calculation operations.We conduct the experiments of key head identification and validation following Section 5.1.We generate the samples including the exponentiation operation as the reference data X r .Then we generate the counterfactual data X c following the principles introduced in Section 4.1 to exclude the exponentiation logic.</p>
<p>The results reveal the potential of generalizing to more complex mathematical operations: (i) Five key heads are identified based on the newly generated X r and X c .We find that the heads (11, 8) and (14, 2) mainly attend to the operators "∧", "power", while the heads (12, 22), (13, 11), (15, 15) mainly attend to the input operands {A} and {B}.(ii) Knocking out the key heads, identified by both templates, leads to significantly impacts (over 60%) on model performance.</p>
<p>Table 3: Key head identification on the exponentiation task.</p>
<p>Templates Key Heads [e.g., (Layer, Head)] Knockout Accuracy X r : "{A} ∧ {B} = " X c : "{A} &lt; {B} = " [(11, 8), (12, 22), (13, 11), (14, 2), (15, 15)] −66%</p>
<p>X r : "{A} to the power of {B} equals " X c : "{A} to the none of {B} equals " [(11, 8), (12, 22), (13, 11), (14, 2), (15, 15)] −62%</p>
<p>E. Generalize to More Complex Scenarios.</p>
<p>We conduct experiments on the more complex scenario using the dataset GSM8K (Cobbe et al., 2021).At first, we create new reference data X r and counterfactual data X c .Following the idea of methodology proposed in Section 4.1, we convert the question in GSM8K to obfuscate the semantic elements that necessitate calculation, while ensuring that the alterations to the text are minimal.An example is shown below:</p>
<p>• GSM8K X r : "On a 16 GB (gigabyte) capacity USB drive, 50% is already busy.Calculate the number of gigabytes still available."</p>
<p>• GSM8K X c : "On a 16 GB (gigabyte) capacity USB drive, 50% is already busy.Describe the location of gigabytes still available."</p>
<p>Then, we conduct the experiments of key head identification and validation following the experimental setting in Section 5.1.As a result, 60% of the key heads are overlapped with the key heads identified based on our original data.Moreover, knocking out the newly-identified key heads leads to a 65% accuracy drop on GSM8K, confirming their importance even in complex scenarios.</p>
<p>Table 4: Comparison of the key heads identified on our generated data in Figure 8 and the dataset GSM8K (Cobbe et al., 2021).</p>
<p>Dataset</p>
<p>Top-10 Key Heads [e.g., (Layer, Head)] Knockout Accuracy Ours [(12, 22), (13, 11), (16, 0), (15, 26), (18, 26), (18, 24), (30,31), (14, 27), (22, 25), (11, 8)] −69% GSM8K [(19, 6), (11, 8), (12, 22), (14, 31), (13, 11), (22, 25), (16, 0), (21, 17), (15, 26), (29, 5)] −65%</p>
<p>Furthermore, only knocking out the 6 overlapping heads brings in −56% and −52% on our generated data and GSM8K, respectively.It shows these heads are both important in two scenarios.If knocking out the 4 non-overlapping heads identified by GSM8K only, it has a negligible effect on our generated data (−2%) but apparently affects on GSM8K (−26%).It reveals the significance of these 4 heads specific to more complex reasoning mathematical problems.We further investigate the attention patterns of the 4 non-overlapping heads, and find that these heads mainly attend to text tokens.For example, the head (29, 5) attends to ".", and the head (19, 6) attends to "GB".In contrast, the 6 overlapping heads mainly attend to the number operands and operators.For example, the head (13, 11) attends to input operands "50", and the head (11, 8) attends to the operator "%".</p>
<p>Recent research (Opedal et al., 2024) has shown that solving the math word problems requires a synergy of multiple skills including 'text comprehension' and 'arithmetic calculation'.This is aligned with the phenomena of "the 4 non-overlapping heads attend to text tokens (i.e., 'text comprehension'), while the 6 overlapping heads attend to number operands and operators (i.e., 'arithmetic calculation')".In this work, we focus on the skill of arithmetic calculation as it's a fundamental ability universally shared across various levels of complexity for mathematical problems.It's imperative for continued research to develop a more holistic understanding of the intricate reasoning capacities.</p>
<p>To further investigate whether the model's deficiencies stem from a lack of mathematical abilities or a broader impairment in language processing, we evaluate LLaMA2-7B with key heads kept normal and knocked out on MMLU-Humanities benchmark (Hendrycks et al., 2020).The comparative performance was 42.9% for models with the key heads intact versus 42.6% for the knockout models.This negligible difference (−0.3%) suggests that the knockout of these heads does not significantly impact general language abilities.</p>
<p>F. More Attention Pattern Cases.</p>
<p>We show the attention patterns of the operator-attended heads (e.g., 14.2) in Figure 16 that could attend to the tokens of "plus", "minus", "times", and "over", across different sentences.Figure 16: The attention patterns of the key head 14.2, which mainly attend to the operator-related tokens, e.g., "plus", "minus", "times", "over".</p>
<p>G. Ablation Study of Precise SFT on MLPs.</p>
<p>We further investigate the influence of different number of tuned MLPs in Table 5.It reveals that the tuning more MLPs could lead to a performance decrease on MMLU and more training time, while imrove the performance on math dataset GSM8K.</p>
<p>Figure 1 :
1
Figure 1: The pipeline involves three steps: 1) identify the key components attributed to arithmetic calculations in black-box LLMs, 2) analyze the working mechanism of the key components towards human-understandable explanations, 3) fine-tune the key components to precisely improve the mathematical capability of LLMs.</p>
<p>Figure 2 :
2
Figure 2: We conduct path patching experiments on LLaMA2-7B across four mathematical tasks, by searching for each head and MLP directly affecting the logit of the right answer.For each head/MLP, a darker color indicates a larger logit difference from the model before patching.</p>
<p>Figure 3 : 1 Ø
31
Figure3: The influence on prediction accuracy after knocking out top-k attention heads that are sorted by the effect of each head on logits ("effect-rank"), and knocking out randomly-sorted top-k heads ("random-rank").</p>
<p>Ø</p>
<p>Input: Danny has 12 bottle caps in his collection.He found 53 bottle caps at the park.How many bottle caps does he have now?The answer is Ø Next token: 5 Ø Top-5 prediction probability: Ø Input: Danny has 12 bottle caps in his collection.He found 53 bottle caps at the park.How many bottle caps does he have now?The answer is Ø Next token: 6 Ø Top-5 prediction probability: 60.99%</p>
<p>Figure 5 :
5
Figure 5: The attention score distribution of key heads across four calculation tasks.The key heads (e.g., 13.11, 14.2) attend to number operands and calculation operators.</p>
<p>Figure 6 :
6
Figure 6: We investigate the projection of each MLP layer input or output along the direction of number token {A}, {B}, and {C}, respectively.The x-axis represents the layer number, ranging from 0 to 31, while the y-axis represents the cosine similarity between the embeddings of the MLP input or output and the number tokens.</p>
<p>Figure 7 :
7
Figure 7: The transferability of attention patterns in key heads on the unseen samples in Figure 4, which mainly attend to the number operands.Key MLPs behavior.In Figure6(a), we conduct an initial investigation of the similarities between the M LP in and tokens {A} and {B} over 1000 samples, to verify the information of operands received from above analyzed attention heads.For the 0-12th layers, both ⟨M LP in , {A}⟩ and ⟨M LP in , {B}⟩ are close to zero.It indicates no operands are captured during this stage, which corresponds to the blank region (i.e., few key heads for computation task) before the 12th layer in Figure2.For the 12-17th layers, we observe a sharp increase in the similarities with both operands({A} and {B}).This surge corresponds to the presence of key attention heads, e.g., 12.22/13.11in layer 12/13, indicating that the operands are progressively being collected and "written" into the MLPs of these layers for</p>
<p>= {C} {A} -{B} = {C} {A} plus {B} equals to {C} {A} minus {B} equals to {C} The addition of {A} and {B} is {C} The difference of {A} and {B} is {C} The addition of {A} and {B} equals to {C} The difference of {A} and {B} equals to {C} The addition of {A} and {B} equals to {C} The difference of {A} and {B} equals to {C} Q: How much is {A} plus {B}? A: Q: How much is {A} minus {B}?A: Q: What is {A} plus {B}? A: Q: What is {A} minus {B}?A: Q: What is the result of {A} plus {B}? A: Q: What is the result of {A} minus {B}?A: Q: What is the sum of {A} and {B}?A: Q: What is the difference of {A} and {B}?A: Multiplication Division {A} * {B} = {C} {A} / {B} = {C} {A} times {B} equals to {C} {A} over {B} equals to {C} The product of {A} and {B} is {C} The ratio of {A} and {B} is {C} The product of {A} and {B} equals to {C} The ratio of {A} and {B} equals to {C} The product of {A} and {B} equals to {C} The ratio of {A} and {B} equals to {C} Q: How much is {A} times {B}?A: Q: How much is {A} over {B}?A: Q: What is {A} times {B}?A: Q: What is {A} over {B}?A: Q: What is the result of {A} times {B}?A: Q: What is the result of {A} over {B}?A: Q: What is the product of {A} and {B}?A: Q: What is the ratio of {A} and {B}?A:</p>
<p>Figure 8 :
8
Figure 8: Templates used in this work follow the formations of "Equation", "Statement", "Question-Answer".</p>
<p>Figure 10 :
10
Figure 10: Additional templates used in the addition task, involve different linguistic meanings like "time span" (1-8) and "object accumulation" (9-12).</p>
<p>Figure 12 :
12
Figure 12: Comparison of the results of path patching experiments on LLaMA2-7B, LLaMA2-13B, and Mistral-7B (Jiang et al., 2023) across four mathematical tasks.For each head/MLP, a darker color indicates a larger logit difference from the original model before patching.</p>
<p>Figure 13 :
13
Figure 13: We investigate the projection of each MLP layer input (M LP in ) along the direction of number token {A}, {B}, respectively.</p>
<p>Figure 14 :
14
Figure 14: We investigate the projection of each MLP layer (M LP out -M LP in ) along the direction of number token {C} (i.e., right answer) and other tokens (i.e., wrong answer).</p>
<p>Figure 15 :
15
Figure 15: We conduct path patching experiments on LLaMA2-7B across four mathematical tasks, by searching for each head and MLP directly affecting the logit of the right answer.The last column denotes the path patching results of MLPs.For each head/MLP, a darker color indicates a larger logit difference from the original model before patching.</p>
<p>Figure 17 :
17
Figure 17: The addition calculation process in computer and in LLMs.</p>
<p>).It indicates that MLPs are engaged in the calculation.We hy-
100%100%99.6%99.1%99.0%98.2%98.1%Prediction accuracy40% 60% 80%93.3% 100% effect-rank 61.7% 46.3% 48.5% 99.3% 99.2%42.0% 40.6% 98.6%35.5% 98.3%29.5% 98.0%random-rank30.8%30.2%20%012345678910Top-k heads</p>
<p>Table 2 :
2
Ablative experiments on the number of tunable components.The default setting is shown in gray .training time, attributed to the substantially fewer parameter adjustments required (less than 1%).It results in a time reduction of at least threefold on LLaMA2-7B and LLaMA2-13B.Overall, Precise SFT offers an effective direction for boosting mathematical abilities for LLMs.
Precise SFTEvaluation MetricSettingTrain SpeedTuned Params.GSM8K MMLUtop-8 heads58sam./sec. 0.017B25.445.1top-16 heads 52sam./sec. 0.033B26.545.8top-32 heads 50sam./sec. 0.067B27.446.4top-48 heads 46sam./sec. 0.101B27.446.4top-64 heads 40sam./sec. 0.134B27.345.5top-32 heads + top-3 MLPs31sam./sec. 0.473B28.045.2Precise SFT improves mathematical ability. SupervisedFine-Tuning (SFT) is an effective approach for augmentingthe mathematical capabilities of models by fine-tuning allparameters within LLMs. We term this all-parameter fine-tuning as Full SFT for clarity, and adopt the same trainingsettings as Precise SFT. Table 1 presents the results of FullSFT and Precise SFT on the LLaMA2-7B and LLaMA2-13B models. Precise SFT effectively bolsters their mathe-matical capabilities, yielding an averaged increase of 15%on four distinct mathematical datasets. It matches or evensurpasses the improvements made by Full SFT. For example,Precise SFT outperforms Full SFT by 5.5% on the SVAMPdataset and 2.8% on GSM8K, underlining its superior abil-
ity to enhance the mathematical prowess of LLMs.Full SFT suffers from the trade-off between mathematical and general capabilities (about 5% drops on MMLU and CSQA), while Precise SFT effectively maintains the model's original performance.A further advantage of Precise SFT is the drastic reduction in</p>
<p>Table 5 :
5
Ablative experiments on the number of tunable MLPs.
Precise SFT SettingEvaluation MetricTrain SpeedTunable Params.GSM8K MMLUtop-32 heads50sam./sec. 0.067B27.446.4top-32 heads + top-1 MLP 44sam./sec. 0.202B27.546.0top-32 heads + top-2 MLPs 38sam./sec. 0.338B27.745.7top-32 heads + top-3 MLPs 31sam./sec. 0.473B28.045.2top-32 heads + top-6 MLPs 26sam./sec. 0.879B28.244.9top-32 heads + all MLPs19sam./sec. 4.396B29.243.9
H. Calculation in Computer vs LLMs.</p>
<p>Here, doing a hard intervention is equal to replacing the value of attention heads and MLPs, while performing a soft intervention means modifying the modules for calculating the attention and MLP values(Pearl,<br />
).
https://huggingface.co/
We apply the notation of i.j to refer to the j-th head of the i-th attention layer.
We empirically find that the specific choice of words does not affect the results, as long as they meet similar semantics.
AcknowledgementsThis work was supported in part by NSFC No. 62222117.YGZ and YMC were supported in part by NSFC/Research Grants Council (RGC) Joint Research Scheme under Grant: N HKBU214/21; in part by RGC Senior Research Fellow Scheme under Grant: SRFS2324-2S02.* This work was done when the author was visiting Alibaba Cloud as a research intern.Impact StatementThis paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.
Flamingo: a visual language model for few-shot learning. J Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, R Ring, E Rutherford, S Cabi, T Han, Z Gong, S Samangooei, M Monteiro, J L Menick, S Borgeaud, A Brock, A Nematzadeh, S Sharifzadeh, M Binkowski, R Barreira, O Vinyals, A Zisserman, K Simonyan, Advances in Neural Information Processing Systems. 2022</p>
<p>B Barak, B L Edelman, S Goel, S Kakade, E Malach, C Zhang, arXiv:2207.08799Hidden progress in deep learning: Sgd learns parities near the computational limit. 2022arXiv preprint</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>An interpretability illusion for BERT. T Bolukbasi, A Pearce, A Yuan, A Coenen, E Reif, F B Viégas, M Wattenberg, CoRR, abs/2104.071432021</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. 2020</p>
<p>. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, P Schuh, K Shi, S Tsvyashchenko, J Maynez, A Rao, P Barnes, Y Tay, N Shazeer, V Prabhakaran, E Reif, N Du, B Hutchinson, R Pope, J Bradbury, J Austin, M Isard, G Gur-Ari, P Yin, T Duke, A Levskaya, S Ghemawat, S Dev, H Michalewski, X Garcia, V Misra, K Robinson, L Fedus, D Zhou, D Ippolito, D Luan, H Lim, B Zoph, A Spiridonov, R Sepassi, D Dohan, S Agrawal, M Omernick, A M Dai, T S Pillai, M Pellat, A Lewkowycz, E Moreira, R Child, O Polozov, K Lee, Z Zhou, X Wang, B Saeta, M Diaz, O Firat, M Catasta, J Wei, K Meier-Hellstern, D Eck, J Dean, S Petrov, N Fiedel, 2022Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, NAACL-HLT. Association for Computational Linguistics2019</p>
<p>A mathematical framework for transformer circuits. N Elhage, N Nanda, C Olsson, T Henighan, N Joseph, B Mann, A Askell, Y Bai, A Chen, T Conerly, N Dassarma, D Drain, D Ganguli, Z Hatfield-Dodds, D Hernandez, A Jones, J Kernion, L Lovitt, K Ndousse, D Amodei, T Brown, J Clark, J Kaplan, S Mccandlish, C Olah, Transformer Circuits Thread. 2021</p>
<p>Causal analysis of syntactic agreement mechanisms in neural language models. M Finlayson, A Mueller, S Gehrmann, S M Shieber, T Linzen, Y Belinkov, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021August 1-6, 2021. 20211Virtual Event</p>
<p>. S Frieder, L Pinchetti, R Griffiths, T Salvatori, T Lukasiewicz, P C Petersen, A Chevalier, Berner, CoRR, abs/2301.13867J. Mathematical capabilities of chatgpt. 2023</p>
<p>Causal abstractions of neural networks. A Geiger, H Lu, T Icard, C Potts, Advances in Neural Information Processing Systems. 2021</p>
<p>Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. M Geva, A Caciularu, K R Wang, Y Goldberg, EMNLP. 2022</p>
<p>Dissecting recall of factual associations in auto-regressive language models. M Geva, J Bastings, K Filippova, A Globerson, CoRR, abs/2304.147672023</p>
<p>Localizing model behavior with path patching. N Goldowsky-Dill, C Macleod, L Sato, A Arora, CoRR, abs/2304.059692023</p>
<p>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. M Hanna, O Liu, A Variengien, CoRR, abs/2305.005862023</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, ArXiv, abs/2009.033002020</p>
<p>Natural language descriptions of deep visual features. E Hernandez, S Schwettmann, D Bau, T Bagashvili, A Torralba, Andreas , J , International Conference on Learning Representations. 2021</p>
<p>Learning to solve arithmetic word problems with verb categorization. M J Hosseini, H Hajishirzi, O Etzioni, N Kushman, EMNLP. ACL2014</p>
<p>How well do computers solve math word problems? large-scale dataset construction and evaluation. D Huang, S Shi, C Lin, J Yin, W Ma, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016. Long Papers. the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016Berlin, GermanyAugust 7-12, 2016. 20161</p>
<p>Large language models can self-improve. J Huang, S S Gu, L Hou, Y Wu, X Wang, H Yu, J Han, arXiv:2210.116102022arXiv preprint</p>
<p>Mathematical reasoning using large language models. S Imani, L Du, H Shrivastava, Mathprompter, ACL. 2023</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Learning to reason deductively: Math word problem solving as complex relation extraction. Z Jie, J Li, W Lu, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandMay 22-27, 2022. 20221ACL 2022</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in Neural Information Processing Systems. 2022</p>
<p>Parsing algebraic word problems into equations. R Koncel-Kedziorski, H Hajishirzi, A Sabharwal, O Etzioni, S D Ang, Trans. Assoc. Comput. Linguistics. 32015</p>
<p>Learning to automatically solve algebra word problems. N Kushman, L Zettlemoyer, R Barzilay, Y Artzi, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014. the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014Baltimore, MD, USALong PapersJune 22-27, 2014. 20141</p>
<p>Making language models better reasoners with step-aware verifier. Y Li, Z Lin, S Zhang, Q Fu, B Chen, J.-G Lou, W Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 20231</p>
<p>Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. T Lieberum, M Rahtz, J Kramár, N Nanda, G Irving, R Shah, V Mikulik, CoRR, abs/2307.094582023</p>
<p>Post-hoc interpretability for neural nlp: A survey. A Madsen, S Reddy, S Chandar, ACM Computing Surveys. 5582022</p>
<p>Locating and editing factual associations in GPT. K Meng, D Bau, A Andonian, Y Belinkov, Advances in Neural Information Processing Systems. 2022</p>
<p>Compositional explanations of neurons. J Mu, J Andreas, Advances in Neural Information Processing Systems. 202033</p>
<p>A mechanistic interpretability analysis of grokking. N Nanda, T Lieberum, 2022</p>
<p>Learning math reasoning from selfsampled correct and partially-correct solutions. A Ni, J P Inala, C Wang, A Polozov, C Meek, D Radev, J Gao, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Investigating the limitations of the transformers with simple arithmetic tasks. R F Nogueira, Z Jiang, J Lin, CoRR, abs/2102.130192021</p>
<p>Dissecting racial bias in an algorithm used to manage the health of populations. Z Obermeyer, B Powers, C Vogeli, S Mullainathan, Science. 36664642019</p>
<p>Do language models exhibit the same cognitive biases in problem solving as human learners?. C Olah, N Cammarata, L Schubert, G Goh, M Petrov, S Carter, A Opedal, A Stolfo, H Shirakami, Y Jiao, R Cotterell, B Schölkopf, A Saparov, M Sachan, CoRR, abs/2401.18070Distill. 2023. 2024Zoom in: An introduction to circuits</p>
<p>Taskspecific skill localization in fine-tuned language models. A Panigrahi, N Saunshi, H Zhao, S Arora, of Proceedings of Machine Learning Research. 2023202ICML</p>
<p>Are NLP models really able to solve simple math word problems?. A Patel, S Bhattamishra, N Goyal, NAACL-HLT. Association for Computational Linguistics2021</p>
<p>Direct and indirect effects. J Pearl, UAI '01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence. University of Washington, Seattle, Washington, USAAugust 2-5, 2001. 2001</p>
<p>. J Pearl, Causality, 2009Cambridge university press</p>
<p>Limitations of language models in arithmetic and symbolic induction. J Qian, H Wang, Z Li, S Li, X Yan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 9-14, 2023. 20231ACL 2023</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Toward transparent ai: A survey on interpreting the inner structures of deep neural networks. T Rauker, A Ho, S Casper, D Hadfield-Menell, 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). 2023</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J R Ruiz, J S Ellenberg, P Wang, O Fawzi, P Kohli, A Fawzi, Nat. 62579952024</p>
<p>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. C Rudin, Nature machine intelligence. 152019</p>
<p>Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. A Saha, V Pahuja, M M Khapra, K Sankaranarayanan, S Chandar, AAAI. AAAI Press2018</p>
<p>Analysing mathematical reasoning abilities of neural models. D Saxton, E Grefenstette, F Hill, P Kohli, International Conference on Learning Representations. 2019</p>
<p>Understanding arithmetic reasoning in language models using causal mediation analysis. A Stolfo, Y Belinkov, M Sachan, CoRR, abs/2305.150542023</p>
<p>Intriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, International Conference on Learning Representations. 2014</p>
<p>Stanford alpaca: An instruction-following llama model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, 2023</p>
<p>Representing numbers in NLP: a survey and a vision. A Thawani, J Pujara, F Ilievski, P A Szekely, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021June 6-11, 2021. 2021</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.09288arXiv:2307.09288Open foundation and finetuned chat models. 2023a. 2023b2arXiv preprint</p>
<p>Solving math word problems with process-and outcomebased feedback. J Uesato, N Kushman, R Kumar, F Song, N Siegel, L Wang, A Creswell, G Irving, I Higgins, arXiv:2211.142752022arXiv preprint</p>
<p>Investigating gender bias in language models using causal mediation analysis. J Vig, S Gehrmann, Y Belinkov, S Qian, D Nevo, Y Singer, S Shieber, Advances in Neural Information Processing Systems. 202033</p>
<p>Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. K R Wang, A Variengien, A Conmy, B Shlegeris, J Steinhardt, International Conference on Learning Representations. 2023a</p>
<p>Selfconsistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Deep neural solver for math word problems. Y Wang, X Liu, S Shi, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, Denmark2017. September 9-11, 2017. 2017</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, ArXiv, abs/2206.07682Emergent abilities of large language models. 2022a</p>
<p>Emergent abilities of large language models. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, Trans. Mach. Learn. Res. 2022. 2022b</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 2022c</p>
<p>Attention is not not explanation. S Wiegreffe, Y Pinter, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational LinguisticsNovember 3-7, 2019. 2019</p>
<p>Interpretability at scale: Identifying causal mechanisms in alpaca. Z Wu, A Geiger, C Potts, N D Goodman, CoRR, abs/2305.088092023</p>
<p>Language models are super mario: Absorbing abilities from homologous models as a free lunch. L Yu, Y Bowen, H Yu, F Huang, Y Li, ArXiv, abs/2311.030992023</p>
<p>curated list of common words. For example, we select <EVENT> from {"war. E Zelikman, Y Wu, J Mu, N Goodman, " • • • ; • • •, Dec, }, and <YYY> from {100, • • • , 199}. For the templates 9-12 of "object accumulation. 2022. JanAdvances in Neural Information Processing Systems. we sample <OBJECT> from {"apple", "orange", • • • , "pear"}, <VERB> from {"get", "obtain", • • • , "acquire"}, and each <NAME> was randomly selected from a pool of 100 English first names</p>
<p>The <EVENT> <VERB> {A} years from <YYY>{B} to <YYY>{C} 3. The <EVENT> <VERB> {A} days from <MONTH> {B} to <MONTH> {C} 4. The <EVENT> will <VERB> {A} days from <MONTH> {B} to <MONTH> {C} 5. The <EVENT> <VERB> {A} hours from {B} pm to {C} 6. The <EVENT> will <VERB> {A} hours from {B} pm to {C} 7. The <EVENT> <VERB> {A} hours from {B} am to {C} 8. The <EVENT> will <VERB> {A} hours from {B} am to {C} 9. <NAME> has {A} <OBJECT>, then <NAME> <VERB> {B} <OBJECT>. What's the total number of <OBJECT> that <NAME> has? The answer is {C} 10. What's the total number of <OBJECT> that they <VERB>? The answer is {C} 11. <NAME> has {A} <OBJECT>, and <NAME2> has {B} <OBJECT>. What's the total number of <OBJECT> that they have? The answer is {C} 12. What's the total number of <OBJECT> that <NAME> <VERB>? The answer is {C}</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1721 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1721</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1721</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-268524153</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.10179v2.pdf" target="_blank">Scaling Instructable Agents Across Many Simulated Worlds</a></p>
                <p><strong>Paper Abstract:</strong> in any 3D environment is a key challenge for creating general AI. Accomplishing this goal requires learning to ground language in perception and embodied actions, in order to accomplish complex tasks. The Scalable, Instructable, Multiworld Agent (SIMA) project tackles this by training agents to follow free-form instructions across a diverse range of virtual 3D environments, including curated research environments as well as open-ended, commercial video games. Our goal is to develop an instructable agent that can accomplish anything a human can do in any simulated 3D environment. Our approach focuses on language-driven generality while imposing minimal assumptions. Our agents interact with environments in real-time using a generic, human-like interface: the inputs are image observations and language instructions and the outputs are keyboard-and-mouse actions. This general approach is challenging, but it allows agents to ground language across many visually complex and semantically rich environments while also allowing us to readily run agents in new environments. In this paper we describe our motivation and goal, the initial progress we have made, and promising preliminary results on several diverse research environments and a variety of commercial video games.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1721.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1721.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SIMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scalable, Instructable, Multiworld Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-environment instructable agent that maps image observations and free-form language instructions to human-like keyboard-and-mouse actions; integrates internet-scale pretrained encoders (image-text and video models) with transformers trained by behavioral cloning across many 3D environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>SIMA agent (main)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Architecture combines pretrained vision-language and video encoders (SPARC and Phenaki), trained-from-scratch cross-attention Transformers and a Transformer-XL memory, and a policy head that emits sequences of 8 keyboard-and-mouse actions. Trained primarily with behavioral cloning and an auxiliary goal-prediction objective; uses classifier-free guidance (CFG) at inference to strengthen language conditionality.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image-text alignment pretraining and text-to-video pretraining (internet-scale multimodal data) plus fine-tuning on human gameplay and instruction datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>SIMA incorporates SPARC (image-text alignment pretrained; Bica et al., 2024) and Phenaki (text-conditioned video prediction pretrained; Villegas et al., 2022) which are further fine-tuned on SIMA gameplay data. The paper does not enumerate the exact corpus sizes of the pretrained models within this report; SIMA training data comprises human gameplay trajectories, recorded actions, and language annotations collected across >10 environments (1,485 evaluated tasks across 7 evaluated environments).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SIMA multi-world evaluation (research environments and commercial 3D games)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>A diverse portfolio of short-horizon language-instructed 3D embodied tasks spanning research environments (Construction Lab, Playhouse, WorldLab, ProcTHOR) and several commercial first- or third-person games (No Man's Sky, Valheim, Satisfactory, Teardown, Hydroneer, Goat Simulator 3, Wobbly Life). Tasks include navigation, resource gathering, tool/menu use, object manipulation and game-specific interactions; agents receive RGB screen observations and must output keyboard-and-mouse actions in real-time.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>For the pretrained multimodal encoders: text prompts/captions and image-text pairs (i.e., natural language captions/descriptions used during image-text and video-text pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Human-like keyboard-and-mouse actions (discrete key presses, discrete/continuous mouse movements/clicks), executed asynchronously in real-time; policy emits short action sequences (8 actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Pretrained encoders provide perceptual/state representations which are cross-attended by trained transformers together with encoded language instructions; behavioral cloning learns a direct mapping from (image observations, language instruction, memory) to keyboard-and-mouse action sequences. At inference, classifier-free guidance (CFG) shifts policy logits towards language-conditioned behavior to strengthen instruction-following.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB screen images (visual observations); no privileged internal game state is provided. Agents use visual encoders (image and video encoders) and language encoders; the system also leverages temporal memory (Transformer-XL).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Using pretrained encoders (SPARC + Phenaki) as part of SIMA yields substantial improvements: SIMA outperforms environment-specialized agents by an average of 67% relative improvement across environments; in a human-comparison subset on No Man's Sky SIMA achieved 34% success (humans 60%) on that subset of tasks. Pretrained-encoder ablation (no pretraining) performs significantly worse overall (permutation test p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>The 'no pretraining' ablation (ResNet trained from scratch replacing pretrained encoders) performs substantially worse overall; exact per-environment success rates are not listed in this report, but the aggregate difference is statistically significant (p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Comparative training/evaluations in the paper are reported at up to 1.2 million training steps for main comparisons; pretrained-encoder SIMA agents are evaluated after 1.2M steps. The report does not provide a direct count of environment episodes/samples to reach a fixed success threshold attributable solely to pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>No explicit episode/sample counts are provided for reaching thresholds without pretraining; baseline comparisons used the same training schedule (evaluated at 1.2M steps) and show substantially worse performance for the no-pretraining ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not quantified as a single scalar in the paper; qualitative gain: pretrained encoders materially accelerate and improve learning/generalization (significant aggregate performance improvement and better zero-shot transfer), but no explicit 'x-fold fewer samples' number is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Internet-scale multimodal pretraining (image-text/video-text) supplies useful semantic and perceptual priors; shared human-like interface (same keyboard-and-mouse across environments) enables transfer of control primitives; multilingual/natural language instructions provide consistent task semantics across environments; multi-environment behavioral cloning encourages shared policies for common skills; CFG improves language conditionality.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Domain gap between pretraining data visuals and specific game visuals/mechanics; environment-specific affordances and unique game mechanics (menus, specialized controls) that require environment-specific data; tasks requiring precise spatial or motor skills remain challenging; limited amount of high-quality annotated gameplay for every game/environment.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating internet-scale pretrained multimodal encoders (image-text and text-to-video) into a multi-world behaviorally-cloned agent materially improves grounded language-conditioned behavior and enables positive transfer and zero-shot generalization across diverse 3D environments; inference-time interventions (CFG) further strengthen language control. However, transfer is partial: environment-specific mechanics and precise motor skills remain challenging, and the paper does not provide a single-scalar measure of sample-efficiency gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1721.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1721.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPARC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPARC (image-text alignment model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained fine-grained image-text alignment encoder used as a perceptual/language-grounding component inside SIMA; incorporated and then fine-tuned on SIMA data to improve instruction grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving fine-grained understanding in image-text pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>SPARC encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A model pretrained for fine-grained image-text alignment (per Bica et al., 2024) used as a frozen or fine-tuned visual-linguistic encoder that provides image-language aligned embeddings to the SIMA transformer's cross-attention modules.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image-text alignment (internet-scale image-caption pairs / fine-grained image-text datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>This paper references Bica et al. (2024) for SPARC; exact dataset composition and sizes are not enumerated within the SIMA report. The SIMA pipeline fine-tunes SPARC on SIMA behavioral cloning data.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SIMA multi-world tasks (see SIMA entry)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Used to help ground language and visual perception across the portfolio of 3D tasks (navigation, object interaction, tool/menu use) through providing aligned vision-language features.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language captions or prompts used during image-text pretraining (text descriptions aligned to images).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Keyboard-and-mouse actions in the 3D game environments (discrete key presses, mouse movements/clicks).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>SPARC produces aligned visual-language embeddings which are cross-attended by SIMA's transformers; behavioral cloning adapts the downstream policy mapping from these embeddings + language to low-level keyboard-and-mouse actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>High-resolution RGB images; image-language alignment capability to map objects and natural-language terms to visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Ablation removing pretrained encoders (replacing with a ResNet trained from scratch) substantially reduced performance overall; aggregate statistical tests show benefit of pretraining (p < 0.001). The paper does not provide separate numeric performance attributed solely to SPARC alone.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>The no-pretraining ablation (ResNet trained from scratch replacing SPARC/Phenaki) performs significantly worse across tasks; specific per-task numbers are not given in the report.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not explicitly quantified for SPARC alone; SIMA experiments with pretrained encoders were evaluated at 1.2M training steps.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not explicitly quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not reported as a numeric multiplier in the paper; aggregate results indicate faster/better learning with pretrained encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Fine-grained image-text alignment provides semantic grounding that is reusable across environments and supports language-conditioned perception.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Visual domain shift between pretraining images and specific game renderings; some game-specific affordances not present in generic image-text datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained image-text encoders (SPARC) meaningfully improve language-grounded embodied performance when integrated and fine-tuned in a multi-environment behavioral cloning agent, but domain shifts and environment-specific mechanics limit perfect transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1721.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1721.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phenaki</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phenaki (text-to-video model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variable-length video-generation model pretrained on open-domain textual descriptions of video used in SIMA as a video prediction encoder whose internal representations were fine-tuned and consumed by the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Phenaki video encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained video prediction model (text-conditioned) used for its internal state representations; SIMA fine-tunes Phenaki on game environments and uses the learned representations rather than explicit rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Text-to-video pretraining (video-text corpora / open-domain video descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Villegas et al., 2022 introduced Phenaki; SIMA fine-tunes Phenaki on game environment video data. Exact corpus sizes and pretraining details are not provided in the SIMA report.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SIMA multi-world tasks (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Used to provide temporally-aware visual representations to support short-horizon instruction following in 3D environments.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language descriptions used as conditioning for video generation during Phenaki pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Keyboard-and-mouse actions in target 3D games.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Phenaki's internal representations are cross-attended by SIMA's transformers; behavioral cloning learns to map these representations + language to action sequences. The paper emphasizes using internal states rather than explicit environment rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Temporal visual modeling (video frames / sequential RGB inputs), to capture motion and dynamics relevant to short-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Including Phenaki (and SPARC) in SIMA improves overall agent performance compared to replacing them with a ResNet trained from scratch (no-pretraining ablation). The paper provides aggregate significance (p < 0.001) but not per-model isolated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>No-pretraining ablation (no Phenaki/SPARC) performs substantially worse; exact numeric drop attributable only to Phenaki is not separable in the report.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not provided separately for Phenaki.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Temporally-aware representations learned from text-conditioned video modeling supply priors about dynamics useful for mapping observations to short action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Video-generation pretraining may not capture precise control dynamics or game-specific physics; representation mismatch can limit effectiveness for fine motor actions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-to-video pretrained models can provide useful internal representations for embodied agents when fine-tuned on gameplay data, contributing to improved language-conditioned behavior, though the paper does not quantify Phenaki's isolated contribution in samples or raw success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1721.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1721.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (Robotics Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action model that leverages web-scale vision-language pretraining to transfer knowledge to robotic control policies; cited as an example of applying pretrained language/vision models as planners for low-level controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A model that uses pretrained vision-language representations to improve robot control and instruction following; presented in related work as evidence that pretrained vision-language models can be repurposed for embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Web-scale vision-language data (image-caption and related multimodal corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not detailed in this SIMA report; see Brohan et al. (2023) for dataset specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic control / language-conditioned robotic tasks (in original RT-2 work)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Robotic manipulation and instruction-following tasks mediated by a lower-level controller; SIMA cites RT-2 as related work but does not run RT-2 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions (used for planning in RT-2).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Low-level robot motor commands / continuous control (in the RT-2 robotics context).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Typical design: pretrained vision-language model used as a planner producing high-level instructions or representations that a lower-level controller executes; SIMA mentions this pattern but does not provide RT-2-specific mapping details.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Vision (RGB) and possibly proprioceptive inputs for robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Demonstrates the feasibility of leveraging web-scale vision-language pretraining to bootstrap embodied control via planner-controller decompositions (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed in detail in the SIMA report; original RT-2 paper should be consulted.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior work showing that pretrained vision-language models can be effective components in robotic instruction following and control pipelines; SIMA uses this literature as motivation for integrating pretrained multimodal encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1721.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1721.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E: An Embodied Multimodal Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal language model designed for embodied tasks (robots), which integrates language and sensory inputs to plan or provide high-level instructions for downstream controllers; cited as related work on grounding LMs in embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaLM-E: An Embodied Multimodal Language Model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A large multimodal LM adapted for embodied settings (robotics), used as a planner that conditions on multimodal inputs to output plans or subgoals for lower-level controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Language and multimodal (vision/robotics) data; specifics not detailed in SIMA report.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not provided in SIMA; see Driess et al. (2023) for details.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic control and embodied planning tasks (in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>PaLM-E is applied to robot tasks and other embodied benchmarks in its original work; in SIMA it's mentioned as related literature, not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions at planner level.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Low-level robot controls (continuous motor outputs) executed by a controller implementing the plan.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Planner (PaLM-E) outputs high-level instructions or goals that are executed by a low-level policy/controller; SIMA cites this pattern conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Multimodal sensory input (vision + other modalities depending on application).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Shows value of multimodal LM pretraining for embodied planning when combined with controllers (cited as motivating related approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PaLM-E is cited as an example where large language/multimodal pretraining has been used to support embodied planning and control, reinforcing SIMA's motivation to combine pretrained multimodal encoders with learned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1721.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1721.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An Open-Ended Embodied Agent with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that grounds a large language model in a game environment (Minecraft) via a lower-level controller, demonstrating how LLMs can drive long-horizon behavior; cited in SIMA as related work exploring LLMs controlling game agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An Open-Ended Embodied Agent with Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Integrates a large language model to plan and generate action sequences, combined with lower-level controllers to execute actions in a sandbox game environment; used for open-ended, long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large language model pretraining on text (and possibly code) plus in-environment fine-tuning via play logs/experience (see original Voyager paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not enumerated in SIMA; see Wang et al. (2023) for full dataset details.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Minecraft (open-ended gameplay) in the original Voyager work; SIMA cites Voyager as conceptually related.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Long-horizon, open-ended crafting and exploration tasks in Minecraft driven by an LLM planner with an integrated memory and execution engine.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Language planning tokens and high-level textual plans.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Low-level game actions executed by controllers (keyboard/mouse in game).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>LLM generates plans or action sequences that a lower-level executor/controller maps to concrete in-game actions; SIMA cites this architecture as a related approach.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Visual/game-state perception via screen pixels and possibly additional state signals in original work.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Demonstrates LLMs' utility as planners when coupled with executors and in-environment experience.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not detailed in SIMA.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Voyager exemplifies grounding LLMs to game environments via planner-executor decompositions; SIMA cites it as a relevant architecture but does not evaluate Voyager itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1721.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1721.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MineDojo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MineDojo (Building Open-Ended Embodied Agents with Internet-Scale Knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A platform and dataset for training agents in Minecraft using internet-scale web knowledge and multimodal data; cited by SIMA as a related effort that trains language-conditional agents with large, diverse data sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Building Open-Ended Embodied Agents with Internet-Scale Knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>MineDojo agents / MineDojo platform</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>MineDojo aggregates large-scale gameplay videos, tutorials, and textual knowledge to train agents capable of many Minecraft tasks; cited as a comparable initiative to SIMA focused on an internet-scale grounding approach.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Internet-scale gameplay videos, tutorials, text descriptions (web-scale multimodal data).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in SIMA; original MineDojo paper (Fan et al., 2022) contains data composition details.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Minecraft open-world tasks (in the original MineDojo work); SIMA references task-count comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Survival, harvesting, building, and other Minecraft tasks driven by multimodal datasets and instruction corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions and textual data scraped from web sources used in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>In-game discrete actions (keyboard/mouse) for Minecraft controls.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>MineDojo trains models from videos and text, often aligning video frames to likely actions; the SIMA paper references MineDojo as a related large-scale data-driven approach.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB video frames; multimodal grounding across text and vision.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large, diverse internet-scale multimodal data supports broad capability in open-world tasks (as discussed in MineDojo literature).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed in the SIMA report.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a related example where internet-scale multimodal pretraining enabled generalist behavior in an open-world game; SIMA positions itself as complementary by operating across many different games/environments and using a human-compatible interface.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1721.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1721.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STEVE-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative model that maps textual instructions to behaviors in Minecraft (text-to-behavior); cited by SIMA as related work on text-conditioned behavior generation in embodied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>STEVE-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A generative model trained to produce in-game behaviors conditioned on textual instructions in Minecraft; cited as an example of text->behavior modeling in an embodied environment.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Text-conditioned behavior data in Minecraft (instruction-action pairs / text-to-action pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>SIM A cites STEVE-1 (Lifshitz et al., 2023) in related work; the SIMA report does not provide STEVE-1 dataset sizes or specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Minecraft instruction-following behaviors (in original STEVE-1 work).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Text-conditional behavior generation for Minecraft tasks; SIMA lists STEVE-1 among related text-to-behavior models.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>In-game discrete/keyboard actions for Minecraft.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Text-conditioned generative model directly outputs behavior/action sequences; SIMA cites this model for context but does not run it.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Visual input / state representations used in original work; not detailed in SIMA report.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Demonstrates feasibility of training models to generate behaviors directly from language in a game environment.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed in the SIMA report.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a related example of text-to-behavior models applied in an embodied open-world game (Minecraft); SIMA uses this literature to motivate multi-environment, language-first approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1721.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1721.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JARVIS-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal memory-augmented language-model-based agent targeting open-world multi-tasking and used as a cited example of large multimodal LMs applied to embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>JARVIS-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A memory-augmented multimodal language model used as a core component for open-world multi-task agents; cited in SIMA as related work on large multimodal models for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal (vision + language) pretraining and in-environment experience (details in original JARVIS-1 paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in SIMA; see JARVIS-1 paper for data specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-world embodied multi-tasking (in original JARVIS-1 work).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Large-scale multi-task embodied benchmarks; SIMA references JARVIS-1 in the context of multimodal LMs applied to embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language and memory-augmented representations.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Environment-specific low-level actions (e.g., robot controls or game controls) executed by downstream policies.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Memory-augmented multimodal LM provides high-level planning or representations that downstream controllers execute; SIMA references this architecture conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Vision and other multimodal sensory inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Illustrates use of multimodal language models with memory for complex open-world tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed in SIMA.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of recent progress combining multimodal LMs and memory mechanisms for multi-task embodied agents; SIMA positions itself in relation to these approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Instructable Agents Across Many Simulated Worlds', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Improving fine-grained understanding in image-text pre-training. <em>(Rating: 2)</em></li>
                <li>Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions. <em>(Rating: 2)</em></li>
                <li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. <em>(Rating: 2)</em></li>
                <li>PaLM-E: An Embodied Multimodal Language Model. <em>(Rating: 2)</em></li>
                <li>Voyager: An Open-Ended Embodied Agent with Large Language Models. <em>(Rating: 2)</em></li>
                <li>Building Open-Ended Embodied Agents with Internet-Scale Knowledge. <em>(Rating: 2)</em></li>
                <li>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft. <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics Transformer for Real. <em>(Rating: 1)</em></li>
                <li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. <em>(Rating: 2)</em></li>
                <li>JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1721",
    "paper_id": "paper-268524153",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "SIMA",
            "name_full": "Scalable, Instructable, Multiworld Agent",
            "brief_description": "A multi-environment instructable agent that maps image observations and free-form language instructions to human-like keyboard-and-mouse actions; integrates internet-scale pretrained encoders (image-text and video models) with transformers trained by behavioral cloning across many 3D environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "SIMA agent (main)",
            "model_agent_description": "Architecture combines pretrained vision-language and video encoders (SPARC and Phenaki), trained-from-scratch cross-attention Transformers and a Transformer-XL memory, and a policy head that emits sequences of 8 keyboard-and-mouse actions. Trained primarily with behavioral cloning and an auxiliary goal-prediction objective; uses classifier-free guidance (CFG) at inference to strengthen language conditionality.",
            "pretraining_data_type": "Image-text alignment pretraining and text-to-video pretraining (internet-scale multimodal data) plus fine-tuning on human gameplay and instruction datasets",
            "pretraining_data_details": "SIMA incorporates SPARC (image-text alignment pretrained; Bica et al., 2024) and Phenaki (text-conditioned video prediction pretrained; Villegas et al., 2022) which are further fine-tuned on SIMA gameplay data. The paper does not enumerate the exact corpus sizes of the pretrained models within this report; SIMA training data comprises human gameplay trajectories, recorded actions, and language annotations collected across &gt;10 environments (1,485 evaluated tasks across 7 evaluated environments).",
            "embodied_task_name": "SIMA multi-world evaluation (research environments and commercial 3D games)",
            "embodied_task_description": "A diverse portfolio of short-horizon language-instructed 3D embodied tasks spanning research environments (Construction Lab, Playhouse, WorldLab, ProcTHOR) and several commercial first- or third-person games (No Man's Sky, Valheim, Satisfactory, Teardown, Hydroneer, Goat Simulator 3, Wobbly Life). Tasks include navigation, resource gathering, tool/menu use, object manipulation and game-specific interactions; agents receive RGB screen observations and must output keyboard-and-mouse actions in real-time.",
            "action_space_text": "For the pretrained multimodal encoders: text prompts/captions and image-text pairs (i.e., natural language captions/descriptions used during image-text and video-text pretraining).",
            "action_space_embodied": "Human-like keyboard-and-mouse actions (discrete key presses, discrete/continuous mouse movements/clicks), executed asynchronously in real-time; policy emits short action sequences (8 actions).",
            "action_mapping_method": "Pretrained encoders provide perceptual/state representations which are cross-attended by trained transformers together with encoded language instructions; behavioral cloning learns a direct mapping from (image observations, language instruction, memory) to keyboard-and-mouse action sequences. At inference, classifier-free guidance (CFG) shifts policy logits towards language-conditioned behavior to strengthen instruction-following.",
            "perception_requirements": "RGB screen images (visual observations); no privileged internal game state is provided. Agents use visual encoders (image and video encoders) and language encoders; the system also leverages temporal memory (Transformer-XL).",
            "transfer_successful": true,
            "performance_with_pretraining": "Using pretrained encoders (SPARC + Phenaki) as part of SIMA yields substantial improvements: SIMA outperforms environment-specialized agents by an average of 67% relative improvement across environments; in a human-comparison subset on No Man's Sky SIMA achieved 34% success (humans 60%) on that subset of tasks. Pretrained-encoder ablation (no pretraining) performs significantly worse overall (permutation test p &lt; 0.001).",
            "performance_without_pretraining": "The 'no pretraining' ablation (ResNet trained from scratch replacing pretrained encoders) performs substantially worse overall; exact per-environment success rates are not listed in this report, but the aggregate difference is statistically significant (p &lt; 0.001).",
            "sample_complexity_with_pretraining": "Comparative training/evaluations in the paper are reported at up to 1.2 million training steps for main comparisons; pretrained-encoder SIMA agents are evaluated after 1.2M steps. The report does not provide a direct count of environment episodes/samples to reach a fixed success threshold attributable solely to pretraining.",
            "sample_complexity_without_pretraining": "No explicit episode/sample counts are provided for reaching thresholds without pretraining; baseline comparisons used the same training schedule (evaluated at 1.2M steps) and show substantially worse performance for the no-pretraining ablation.",
            "sample_complexity_gain": "Not quantified as a single scalar in the paper; qualitative gain: pretrained encoders materially accelerate and improve learning/generalization (significant aggregate performance improvement and better zero-shot transfer), but no explicit 'x-fold fewer samples' number is reported.",
            "transfer_success_factors": "Internet-scale multimodal pretraining (image-text/video-text) supplies useful semantic and perceptual priors; shared human-like interface (same keyboard-and-mouse across environments) enables transfer of control primitives; multilingual/natural language instructions provide consistent task semantics across environments; multi-environment behavioral cloning encourages shared policies for common skills; CFG improves language conditionality.",
            "transfer_failure_factors": "Domain gap between pretraining data visuals and specific game visuals/mechanics; environment-specific affordances and unique game mechanics (menus, specialized controls) that require environment-specific data; tasks requiring precise spatial or motor skills remain challenging; limited amount of high-quality annotated gameplay for every game/environment.",
            "key_findings": "Incorporating internet-scale pretrained multimodal encoders (image-text and text-to-video) into a multi-world behaviorally-cloned agent materially improves grounded language-conditioned behavior and enables positive transfer and zero-shot generalization across diverse 3D environments; inference-time interventions (CFG) further strengthen language control. However, transfer is partial: environment-specific mechanics and precise motor skills remain challenging, and the paper does not provide a single-scalar measure of sample-efficiency gains.",
            "uuid": "e1721.0",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "SPARC",
            "name_full": "SPARC (image-text alignment model)",
            "brief_description": "A pretrained fine-grained image-text alignment encoder used as a perceptual/language-grounding component inside SIMA; incorporated and then fine-tuned on SIMA data to improve instruction grounding.",
            "citation_title": "Improving fine-grained understanding in image-text pre-training.",
            "mention_or_use": "use",
            "model_agent_name": "SPARC encoder",
            "model_agent_description": "A model pretrained for fine-grained image-text alignment (per Bica et al., 2024) used as a frozen or fine-tuned visual-linguistic encoder that provides image-language aligned embeddings to the SIMA transformer's cross-attention modules.",
            "pretraining_data_type": "Image-text alignment (internet-scale image-caption pairs / fine-grained image-text datasets).",
            "pretraining_data_details": "This paper references Bica et al. (2024) for SPARC; exact dataset composition and sizes are not enumerated within the SIMA report. The SIMA pipeline fine-tunes SPARC on SIMA behavioral cloning data.",
            "embodied_task_name": "SIMA multi-world tasks (see SIMA entry)",
            "embodied_task_description": "Used to help ground language and visual perception across the portfolio of 3D tasks (navigation, object interaction, tool/menu use) through providing aligned vision-language features.",
            "action_space_text": "Natural language captions or prompts used during image-text pretraining (text descriptions aligned to images).",
            "action_space_embodied": "Keyboard-and-mouse actions in the 3D game environments (discrete key presses, mouse movements/clicks).",
            "action_mapping_method": "SPARC produces aligned visual-language embeddings which are cross-attended by SIMA's transformers; behavioral cloning adapts the downstream policy mapping from these embeddings + language to low-level keyboard-and-mouse actions.",
            "perception_requirements": "High-resolution RGB images; image-language alignment capability to map objects and natural-language terms to visual features.",
            "transfer_successful": true,
            "performance_with_pretraining": "Ablation removing pretrained encoders (replacing with a ResNet trained from scratch) substantially reduced performance overall; aggregate statistical tests show benefit of pretraining (p &lt; 0.001). The paper does not provide separate numeric performance attributed solely to SPARC alone.",
            "performance_without_pretraining": "The no-pretraining ablation (ResNet trained from scratch replacing SPARC/Phenaki) performs significantly worse across tasks; specific per-task numbers are not given in the report.",
            "sample_complexity_with_pretraining": "Not explicitly quantified for SPARC alone; SIMA experiments with pretrained encoders were evaluated at 1.2M training steps.",
            "sample_complexity_without_pretraining": "Not explicitly quantified.",
            "sample_complexity_gain": "Not reported as a numeric multiplier in the paper; aggregate results indicate faster/better learning with pretrained encoders.",
            "transfer_success_factors": "Fine-grained image-text alignment provides semantic grounding that is reusable across environments and supports language-conditioned perception.",
            "transfer_failure_factors": "Visual domain shift between pretraining images and specific game renderings; some game-specific affordances not present in generic image-text datasets.",
            "key_findings": "Pretrained image-text encoders (SPARC) meaningfully improve language-grounded embodied performance when integrated and fine-tuned in a multi-environment behavioral cloning agent, but domain shifts and environment-specific mechanics limit perfect transfer.",
            "uuid": "e1721.1",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Phenaki",
            "name_full": "Phenaki (text-to-video model)",
            "brief_description": "A variable-length video-generation model pretrained on open-domain textual descriptions of video used in SIMA as a video prediction encoder whose internal representations were fine-tuned and consumed by the agent.",
            "citation_title": "Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions.",
            "mention_or_use": "use",
            "model_agent_name": "Phenaki video encoder",
            "model_agent_description": "Pretrained video prediction model (text-conditioned) used for its internal state representations; SIMA fine-tunes Phenaki on game environments and uses the learned representations rather than explicit rollouts.",
            "pretraining_data_type": "Text-to-video pretraining (video-text corpora / open-domain video descriptions).",
            "pretraining_data_details": "Villegas et al., 2022 introduced Phenaki; SIMA fine-tunes Phenaki on game environment video data. Exact corpus sizes and pretraining details are not provided in the SIMA report.",
            "embodied_task_name": "SIMA multi-world tasks (as above)",
            "embodied_task_description": "Used to provide temporally-aware visual representations to support short-horizon instruction following in 3D environments.",
            "action_space_text": "Natural language descriptions used as conditioning for video generation during Phenaki pretraining.",
            "action_space_embodied": "Keyboard-and-mouse actions in target 3D games.",
            "action_mapping_method": "Phenaki's internal representations are cross-attended by SIMA's transformers; behavioral cloning learns to map these representations + language to action sequences. The paper emphasizes using internal states rather than explicit environment rollouts.",
            "perception_requirements": "Temporal visual modeling (video frames / sequential RGB inputs), to capture motion and dynamics relevant to short-horizon tasks.",
            "transfer_successful": true,
            "performance_with_pretraining": "Including Phenaki (and SPARC) in SIMA improves overall agent performance compared to replacing them with a ResNet trained from scratch (no-pretraining ablation). The paper provides aggregate significance (p &lt; 0.001) but not per-model isolated metrics.",
            "performance_without_pretraining": "No-pretraining ablation (no Phenaki/SPARC) performs substantially worse; exact numeric drop attributable only to Phenaki is not separable in the report.",
            "sample_complexity_with_pretraining": "Not provided separately for Phenaki.",
            "sample_complexity_without_pretraining": "Not provided.",
            "sample_complexity_gain": "Not reported numerically.",
            "transfer_success_factors": "Temporally-aware representations learned from text-conditioned video modeling supply priors about dynamics useful for mapping observations to short action sequences.",
            "transfer_failure_factors": "Video-generation pretraining may not capture precise control dynamics or game-specific physics; representation mismatch can limit effectiveness for fine motor actions.",
            "key_findings": "Text-to-video pretrained models can provide useful internal representations for embodied agents when fine-tuned on gameplay data, contributing to improved language-conditioned behavior, though the paper does not quantify Phenaki's isolated contribution in samples or raw success rates.",
            "uuid": "e1721.2",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (Robotics Transformer 2)",
            "brief_description": "A vision-language-action model that leverages web-scale vision-language pretraining to transfer knowledge to robotic control policies; cited as an example of applying pretrained language/vision models as planners for low-level controllers.",
            "citation_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.",
            "mention_or_use": "mention",
            "model_agent_name": "RT-2",
            "model_agent_description": "A model that uses pretrained vision-language representations to improve robot control and instruction following; presented in related work as evidence that pretrained vision-language models can be repurposed for embodied control.",
            "pretraining_data_type": "Web-scale vision-language data (image-caption and related multimodal corpora).",
            "pretraining_data_details": "Not detailed in this SIMA report; see Brohan et al. (2023) for dataset specifics.",
            "embodied_task_name": "Robotic control / language-conditioned robotic tasks (in original RT-2 work)",
            "embodied_task_description": "Robotic manipulation and instruction-following tasks mediated by a lower-level controller; SIMA cites RT-2 as related work but does not run RT-2 in experiments.",
            "action_space_text": "Natural language instructions (used for planning in RT-2).",
            "action_space_embodied": "Low-level robot motor commands / continuous control (in the RT-2 robotics context).",
            "action_mapping_method": "Typical design: pretrained vision-language model used as a planner producing high-level instructions or representations that a lower-level controller executes; SIMA mentions this pattern but does not provide RT-2-specific mapping details.",
            "perception_requirements": "Vision (RGB) and possibly proprioceptive inputs for robotic control.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Demonstrates the feasibility of leveraging web-scale vision-language pretraining to bootstrap embodied control via planner-controller decompositions (as cited).",
            "transfer_failure_factors": "Not discussed in detail in the SIMA report; original RT-2 paper should be consulted.",
            "key_findings": "Cited as prior work showing that pretrained vision-language models can be effective components in robotic instruction following and control pipelines; SIMA uses this literature as motivation for integrating pretrained multimodal encoders.",
            "uuid": "e1721.3",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "PaLM-E",
            "name_full": "PaLM-E: An Embodied Multimodal Language Model",
            "brief_description": "A multimodal language model designed for embodied tasks (robots), which integrates language and sensory inputs to plan or provide high-level instructions for downstream controllers; cited as related work on grounding LMs in embodied control.",
            "citation_title": "PaLM-E: An Embodied Multimodal Language Model.",
            "mention_or_use": "mention",
            "model_agent_name": "PaLM-E",
            "model_agent_description": "A large multimodal LM adapted for embodied settings (robotics), used as a planner that conditions on multimodal inputs to output plans or subgoals for lower-level controllers.",
            "pretraining_data_type": "Language and multimodal (vision/robotics) data; specifics not detailed in SIMA report.",
            "pretraining_data_details": "Not provided in SIMA; see Driess et al. (2023) for details.",
            "embodied_task_name": "Robotic control and embodied planning tasks (in cited work)",
            "embodied_task_description": "PaLM-E is applied to robot tasks and other embodied benchmarks in its original work; in SIMA it's mentioned as related literature, not used in experiments.",
            "action_space_text": "Natural language instructions at planner level.",
            "action_space_embodied": "Low-level robot controls (continuous motor outputs) executed by a controller implementing the plan.",
            "action_mapping_method": "Planner (PaLM-E) outputs high-level instructions or goals that are executed by a low-level policy/controller; SIMA cites this pattern conceptually.",
            "perception_requirements": "Multimodal sensory input (vision + other modalities depending on application).",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Shows value of multimodal LM pretraining for embodied planning when combined with controllers (cited as motivating related approaches).",
            "transfer_failure_factors": "Not detailed in this paper.",
            "key_findings": "PaLM-E is cited as an example where large language/multimodal pretraining has been used to support embodied planning and control, reinforcing SIMA's motivation to combine pretrained multimodal encoders with learned policies.",
            "uuid": "e1721.4",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "brief_description": "A system that grounds a large language model in a game environment (Minecraft) via a lower-level controller, demonstrating how LLMs can drive long-horizon behavior; cited in SIMA as related work exploring LLMs controlling game agents.",
            "citation_title": "Voyager: An Open-Ended Embodied Agent with Large Language Models.",
            "mention_or_use": "mention",
            "model_agent_name": "Voyager",
            "model_agent_description": "Integrates a large language model to plan and generate action sequences, combined with lower-level controllers to execute actions in a sandbox game environment; used for open-ended, long-horizon tasks.",
            "pretraining_data_type": "Large language model pretraining on text (and possibly code) plus in-environment fine-tuning via play logs/experience (see original Voyager paper).",
            "pretraining_data_details": "Not enumerated in SIMA; see Wang et al. (2023) for full dataset details.",
            "embodied_task_name": "Minecraft (open-ended gameplay) in the original Voyager work; SIMA cites Voyager as conceptually related.",
            "embodied_task_description": "Long-horizon, open-ended crafting and exploration tasks in Minecraft driven by an LLM planner with an integrated memory and execution engine.",
            "action_space_text": "Language planning tokens and high-level textual plans.",
            "action_space_embodied": "Low-level game actions executed by controllers (keyboard/mouse in game).",
            "action_mapping_method": "LLM generates plans or action sequences that a lower-level executor/controller maps to concrete in-game actions; SIMA cites this architecture as a related approach.",
            "perception_requirements": "Visual/game-state perception via screen pixels and possibly additional state signals in original work.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Demonstrates LLMs' utility as planners when coupled with executors and in-environment experience.",
            "transfer_failure_factors": "Not detailed in SIMA.",
            "key_findings": "Voyager exemplifies grounding LLMs to game environments via planner-executor decompositions; SIMA cites it as a relevant architecture but does not evaluate Voyager itself.",
            "uuid": "e1721.5",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MineDojo",
            "name_full": "MineDojo (Building Open-Ended Embodied Agents with Internet-Scale Knowledge)",
            "brief_description": "A platform and dataset for training agents in Minecraft using internet-scale web knowledge and multimodal data; cited by SIMA as a related effort that trains language-conditional agents with large, diverse data sources.",
            "citation_title": "Building Open-Ended Embodied Agents with Internet-Scale Knowledge.",
            "mention_or_use": "mention",
            "model_agent_name": "MineDojo agents / MineDojo platform",
            "model_agent_description": "MineDojo aggregates large-scale gameplay videos, tutorials, and textual knowledge to train agents capable of many Minecraft tasks; cited as a comparable initiative to SIMA focused on an internet-scale grounding approach.",
            "pretraining_data_type": "Internet-scale gameplay videos, tutorials, text descriptions (web-scale multimodal data).",
            "pretraining_data_details": "Not specified in SIMA; original MineDojo paper (Fan et al., 2022) contains data composition details.",
            "embodied_task_name": "Minecraft open-world tasks (in the original MineDojo work); SIMA references task-count comparisons.",
            "embodied_task_description": "Survival, harvesting, building, and other Minecraft tasks driven by multimodal datasets and instruction corpora.",
            "action_space_text": "Natural language instructions and textual data scraped from web sources used in pretraining.",
            "action_space_embodied": "In-game discrete actions (keyboard/mouse) for Minecraft controls.",
            "action_mapping_method": "MineDojo trains models from videos and text, often aligning video frames to likely actions; the SIMA paper references MineDojo as a related large-scale data-driven approach.",
            "perception_requirements": "RGB video frames; multimodal grounding across text and vision.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Large, diverse internet-scale multimodal data supports broad capability in open-world tasks (as discussed in MineDojo literature).",
            "transfer_failure_factors": "Not discussed in the SIMA report.",
            "key_findings": "Cited as a related example where internet-scale multimodal pretraining enabled generalist behavior in an open-world game; SIMA positions itself as complementary by operating across many different games/environments and using a human-compatible interface.",
            "uuid": "e1721.6",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "STEVE-1",
            "name_full": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft",
            "brief_description": "A generative model that maps textual instructions to behaviors in Minecraft (text-to-behavior); cited by SIMA as related work on text-conditioned behavior generation in embodied environments.",
            "citation_title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft.",
            "mention_or_use": "mention",
            "model_agent_name": "STEVE-1",
            "model_agent_description": "A generative model trained to produce in-game behaviors conditioned on textual instructions in Minecraft; cited as an example of text-&gt;behavior modeling in an embodied environment.",
            "pretraining_data_type": "Text-conditioned behavior data in Minecraft (instruction-action pairs / text-to-action pairs).",
            "pretraining_data_details": "SIM A cites STEVE-1 (Lifshitz et al., 2023) in related work; the SIMA report does not provide STEVE-1 dataset sizes or specifics.",
            "embodied_task_name": "Minecraft instruction-following behaviors (in original STEVE-1 work).",
            "embodied_task_description": "Text-conditional behavior generation for Minecraft tasks; SIMA lists STEVE-1 among related text-to-behavior models.",
            "action_space_text": "Natural language instructions.",
            "action_space_embodied": "In-game discrete/keyboard actions for Minecraft.",
            "action_mapping_method": "Text-conditioned generative model directly outputs behavior/action sequences; SIMA cites this model for context but does not run it.",
            "perception_requirements": "Visual input / state representations used in original work; not detailed in SIMA report.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Demonstrates feasibility of training models to generate behaviors directly from language in a game environment.",
            "transfer_failure_factors": "Not discussed in the SIMA report.",
            "key_findings": "Mentioned as a related example of text-to-behavior models applied in an embodied open-world game (Minecraft); SIMA uses this literature to motivate multi-environment, language-first approaches.",
            "uuid": "e1721.7",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "JARVIS-1",
            "name_full": "JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models",
            "brief_description": "A multimodal memory-augmented language-model-based agent targeting open-world multi-tasking and used as a cited example of large multimodal LMs applied to embodied tasks.",
            "citation_title": "JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models.",
            "mention_or_use": "mention",
            "model_agent_name": "JARVIS-1",
            "model_agent_description": "A memory-augmented multimodal language model used as a core component for open-world multi-task agents; cited in SIMA as related work on large multimodal models for embodied tasks.",
            "pretraining_data_type": "Multimodal (vision + language) pretraining and in-environment experience (details in original JARVIS-1 paper).",
            "pretraining_data_details": "Not specified in SIMA; see JARVIS-1 paper for data specifics.",
            "embodied_task_name": "Open-world embodied multi-tasking (in original JARVIS-1 work).",
            "embodied_task_description": "Large-scale multi-task embodied benchmarks; SIMA references JARVIS-1 in the context of multimodal LMs applied to embodied control.",
            "action_space_text": "Natural language and memory-augmented representations.",
            "action_space_embodied": "Environment-specific low-level actions (e.g., robot controls or game controls) executed by downstream policies.",
            "action_mapping_method": "Memory-augmented multimodal LM provides high-level planning or representations that downstream controllers execute; SIMA references this architecture conceptually.",
            "perception_requirements": "Vision and other multimodal sensory inputs.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Illustrates use of multimodal language models with memory for complex open-world tasks.",
            "transfer_failure_factors": "Not discussed in SIMA.",
            "key_findings": "Cited as an example of recent progress combining multimodal LMs and memory mechanisms for multi-task embodied agents; SIMA positions itself in relation to these approaches.",
            "uuid": "e1721.8",
            "source_info": {
                "paper_title": "Scaling Instructable Agents Across Many Simulated Worlds",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Improving fine-grained understanding in image-text pre-training.",
            "rating": 2,
            "sanitized_title": "improving_finegrained_understanding_in_imagetext_pretraining"
        },
        {
            "paper_title": "Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions.",
            "rating": 2,
            "sanitized_title": "phenaki_variable_length_video_generation_from_open_domain_textual_descriptions"
        },
        {
            "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.",
            "rating": 2,
            "sanitized_title": "rt2_visionlanguageaction_models_transfer_web_knowledge_to_robotic_control"
        },
        {
            "paper_title": "PaLM-E: An Embodied Multimodal Language Model.",
            "rating": 2,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "Voyager: An Open-Ended Embodied Agent with Large Language Models.",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Building Open-Ended Embodied Agents with Internet-Scale Knowledge.",
            "rating": 2,
            "sanitized_title": "building_openended_embodied_agents_with_internetscale_knowledge"
        },
        {
            "paper_title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft.",
            "rating": 2,
            "sanitized_title": "steve1_a_generative_model_for_texttobehavior_in_minecraft"
        },
        {
            "paper_title": "RT-1: Robotics Transformer for Real.",
            "rating": 1,
            "sanitized_title": "rt1_robotics_transformer_for_real"
        },
        {
            "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.",
            "rating": 2,
            "sanitized_title": "rt2_visionlanguageaction_models_transfer_web_knowledge_to_robotic_control"
        },
        {
            "paper_title": "JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models.",
            "rating": 1,
            "sanitized_title": "jarvis1_openworld_multitask_agents_with_memoryaugmented_multimodal_language_models"
        }
    ],
    "cost": 0.025048,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Scaling Instructable Agents Across Many Simulated Worlds</p>
<p>Sima Team 
Maria Abi Raad 
Arun Ahuja 
Catarina Barros 
Frederic Besse 
Andrew Bolt 
Adrian Bolton 
Bethanie Brownfield 
Gavin Buttimore 
Max Cant 
Sarah Chakera 
Stephanie C Y Chan 
Jeff Clune 
University of British Columbia</p>
<p>Adrian Collister 
Vikki Copeman 
Alex Cullum 
Ishita Dasgupta 
Dario De Cesare 
Julia Di Trapani 
Yani Donchev 
Emma Dunleavy 
Martin Engelcke 
Ryan Faulkner 
Frankie Garcia 
Charles Gbadamosi 
Zhitao Gong 
Lucy Gonzales 
Karol Gregor 
Arne Olav Hallingstad 
Tim Harley 
Sam Haves 
Felix Hill 
Ed Hirst 
Drew A Hudson 
Steph Hughes-Fitt 
Danilo J Rezende 
Mimi Jasarevic 
Laura Kampis 
Rosemary Ke 
Thomas Keck 
Junkyung Kim 
Oscar Knagg 
Kavya Kopparapu 
Andrew Lampinen 
Shane Legg 
Alexander Lerchner 
Marjorie Limont 
Yulan Liu 
Maria Loks-Thompson 
Joseph Marino 
Kathryn Martin Cussons 
Loic Matthey 
Siobhan Mcloughlin 
Piermaria Mendolicchio 
Hamza Merzic 
Anna Mitenkova 
Alexandre Moufarek 
Valeria Oliveira </p>
<p>Hannah Openshaw
David Reichert
Yanko Oliveira
Aneesh Pappu, Alex Platonov, Ollie PurkissRenke Pan</p>
<p>John Reid
Pierre Harvey Richemond
Tyson Roberts
Giles Ruscoe, Jaume Sanchez Elias, Tasha Sandars 2</p>
<p>Daniel P. Sawyer
Tayfun Terzi
Tim Scholtes, Guy Simmons, Daniel Slater, Hubert Soyer, Heiko Strathmann, Peter Stys, Allison C. Tam 2, Denis Teplyashin, Davide Vercelli, Bojan Vujatovic, Marcus Wainwright, Jane X. Wang, Zhengdong Wang, Daan Wierstra 2, Duncan Williams, Nathaniel Wong, Sarah YorkNick Young</p>
<p>Scaling Instructable Agents Across Many Simulated Worlds
392840F9F39414F52ABD28BC5466AB07AgentsEmbodimentFoundation ModelsLanguageVideo Games3D Environments
Building embodied AI systems that can follow arbitrary language instructions in any 3D environment is a key challenge for creating general AI.Accomplishing this goal requires learning to ground language in perception and embodied actions, in order to accomplish complex tasks.The Scalable, Instructable, Multiworld Agent (SIMA) project tackles this by training agents to follow free-form instructions across a diverse range of virtual 3D environments, including curated research environments as well as openended, commercial video games.Our goal is to develop an instructable agent that can accomplish anything a human can do in any simulated 3D environment.Our approach focuses on language-driven generality while imposing minimal assumptions.Our agents interact with environments in real-time using a generic, human-like interface: the inputs are image observations and language instructions and the outputs are keyboard-and-mouse actions.This general approach is challenging, but it allows agents to ground language across many visually complex and semantically rich environments while also allowing us to readily run agents in new environments.In this paper we describe our motivation and goal, the initial progress we have made, and promising preliminary results on several diverse research environments and a variety of commercial video games.</p>
<p>Introduction</p>
<p>Despite the impressive capabilities of large language models (Brown et al., 2020;Hoffmann et al., 2022;OpenAI, 2023;Anil et al., 2023;Gemini Team et al., 2023), connecting them to the embodied world that we inhabit remains challenging.Modern AI can write computer programs (Li et al., 2022) or play chess at super-human level (Silver et al., 2018), but the ability of AI to perceive and act in the world remains far below human level.Competence in language alone is easier for AI than grounded perception and behavior, underscoring the well-known paradox that what is easier for AI is harder for humans, and vice versa (Moravec, 1988).In SIMA, we collect a large and diverse dataset of gameplay from both curated research environments and commercial video games.This dataset is used to train agents to follow open-ended language instructions via pixel inputs and keyboard-and-mouse action outputs.</p>
<p>Agents are then evaluated in terms of their behavior across a broad range of skills.</p>
<p>Yet, language is most useful in the abstractions it conveys about the world.Language abstractions can enable efficient learning and generalization (Hill et al., 2020;Colas et al., 2020;Lampinen et al., 2022;Tam et al., 2022;Hu and Clune, 2023).Once learned, language can unlock planning, reasoning (e.g., Huang et al., 2022;Brohan et al., 2023b;Driess et al., 2023;Kim et al., 2023), and communication (Zeng et al., 2022) about grounded situations and tasks.In turn, grounding language in rich environments can make a system's understanding of the language itself more systematic and generalizable (Hill et al., 2019).Thus, several questions emerge: How can we bridge the divide between the symbols of language and their external referents (cf., Harnad, 1990)?How can we connect the abstractions and generality afforded by language to grounded perception and action, and how can we do so in a safe and scalable way?</p>
<p>Here, we draw inspiration from these questions-and the prior and concurrent research projects that have addressed them (e.g., Hermann et al., 2017;Abramson et al., 2020;Brohan et al., 2023a,b;Driess et al., 2023;Wang et al., 2023b;Tan et al., 2024)-to attempt to connect language to grounded behavior at scale.Bridging this gap is a core challenge for developing general embodied AI.</p>
<p>The Scalable, Instructable, Multiworld Agent (SIMA) project aims to build a system that can follow arbitrary language instructions to act in any virtual 3D environment via keyboard-and-mouse actions-from custom-built research environments to a broad range of commercial video games.There is a long history of research in creating agents that can interact with video games or simulated 3D environments (e.g., Mnih et al., 2015;Berner et al., 2019;Vinyals et al., 2019;Baker et al., 2022) and even follow language instructions in a limited range of environments (e.g., Abramson et al., 2020;Lifshitz et al., 2023).In SIMA, however, we are drawing inspiration from the lesson of large language models that training on a broad distribution of data is the most effective way to make progress in general AI (e.g., Brown et al., 2020;Hoffmann et al., 2022;OpenAI, 2023;Anil et al., 2023;Gemini Team et al., 2023).Thus, in contrast to prior works (e.g., Abramson et al., 2020;Vinyals et al., 2019;Berner et al., 2019;Lifshitz et al., 2023), we are attempting to tackle this problem across many simulated environments, in the most general and scalable way possible, by making few assumptions beyond interacting with the environments in the same way as humans do.</p>
<p>To this end, have made a number of design decisions that make our approach more general, but also more challenging:</p>
<p> We incorporate many rich, visually complex, open-ended video games containing hundreds of objects in a scene and a large number of possible interactions. These environments are asynchronous (e.g., Berner et al., 2019;Vinyals et al., 2019); unlike many research environments, they do not stop and wait while the agent computes its next action.</p>
<p> Each instance of a commercial video game needs to run on a GPU; thus, we cannot run hundreds or thousands of actors per game per experiment as often done in RL (cf., Espeholt et al., 2018). Agents receive the same screen observations that a human playing the game would without access to internal game state, rewards, or any other privileged information (cf., Berner et al., 2019;Vinyals et al., 2019). To interact with the environments, agents use the same keyboard-and-mouse controls that humans do (e.g., Baker et al., 2022;Humphreys et al., 2022;Lifshitz et al., 2023), rather than handcrafted action spaces or high-level APIs. We focus on following language instructions (e.g., Abramson et al., 2020) rather than simply playing the games to maximize a win-rate or generating plausible behavior (cf., Berner et al., 2019;Vinyals et al., 2019). We train and test our agents using open-ended natural language, rather than simplified grammars or command sets (e.g., Abramson et al., 2020).</p>
<p>These design choices make the learning problem harder, but their generality makes expanding to new environments easier: agents use the same interface across environments without requiring a custom design of control and observation spaces for each new game.Furthermore, since the agent-environment interface is human compatible, it allows agents the potential to achieve anything that a human could, and allows direct imitation learning from human behavior.This general interface from language instructions to embodied behavior can also enable agents to transfer previously learned skills zero-shot to never-before-seen games.Doing research in generic virtual environments allows us to test our agents in a broad and challenging range of situations-where the lessons learned are likely to be more applicable to real-world applications with visually rich perception and control such as robotics-without the risks and costs of real-world testing: if the agent crashes a spaceship in a video game, we can just restart the game.</p>
<p>In the SIMA project thus far, we have created an agent that performs short-horizon tasks based on language instructions produced by a user; though instructions could also be produced by a language model (e.g., Jiang et al., 2019;Driess et al., 2023;Wang et al., 2023b;Hu et al., 2023;Ajay et al., 2023).We have a portfolio of over ten 3D environments, consisting of research environments and commercial video games.For research environments we evaluate agents using the ground truth state, but commercial video games are not designed to report on the completion of arbitrary language tasks.We have therefore developed a variety of methods for evaluation in video games, including using optical character recognition (OCR) to detect onscreen text describing task completion, and using human evaluation of recorded videos of agent behavior.In the rest of this tech report, we describe the high-level approach (illustrated in Figure 1) and our initial progress towards the ultimate goal of SIMA: developing an instructable agent that can accomplish anything a human can do in any simulated 3D environment.</p>
<p>Related work</p>
<p>SIMA builds on a long history of using games as a platform for AI research.For example, backgammon provided the initial proving ground for early deep reinforcement learning methods (Tesauro et al., 1995), and later works have achieved superhuman performance even in complex board games like Go (Silver et al., 2016(Silver et al., , 2018)).</p>
<p>Video games Over the last ten years, video games have provided an increasingly important setting for research focused on embodied agents that perform visuomotor control in rich environments.Researchers have used many video game environments, covering a wide spectrum from Atari (Bellemare et al., 2013) to DoTA (Berner et al., 2019) and StarCraft II (Vinyals et al., 2019).In SIMA, however, we restrict our focus to games that resemble 3D physical embodiment most closely, in particular games where the player interacts with a 3D world from a first or over-the-shoulder pseudo-first-person view.This focus excludes many of the games which have previously been used for research, such as the ones listed above.There has however been notable interest in first-person embodied video games as a platform for AI research (Johnson et al., 2016;Tessler et al., 2017;Guss et al., 2019;Pearce and Zhu, 2022;Hafner et al., 2023;Durante et al., 2024;Tan et al., 2024).These video game AI projects have driven the development of many innovative techniques, e.g., learning from videos by annotating them with estimated player keyboard-and-mouse actions using inverse dynamics models (Pearce and Zhu, 2022;Baker et al., 2022).More recently, games that offer API access to the environment have served as a platform for grounding large language models (Wang et al., 2023a), and some works have even considered grounding a language model in a game through direct perception and action of a lower-level controller (Wang et al., 2023b).Instead of focusing on a single game or environment, however, SIMA considers a range of diverse games to train agents on a larger variety of content.</p>
<p>Research environments</p>
<p>Other works have focused on custom, controlled environments designed for research.Many of these environments focus on particular domains of real-world knowledge.For example, AI2-THOR (Kolve et al., 2017), VirtualHome (Puig et al., 2018), ProcTHOR (Deitke et al., 2022), AI Habitat (Savva et al., 2019;Szot et al., 2021;Puig et al., 2023), ALFRED (Shridhar et al., 2020), and Behavior (Srivastava et al., 2021) simulate embodied agents behaving in naturalistic rendered scenes.CARLA (Dosovitskiy et al., 2017) provides a simulator for autonomous driving.MuJoCo (Todorov et al., 2012), PyBullet (Coumans and Bai, 2016-2023), and Isaac Gym (Makoviychuk et al., 2021) provide high quality physics simulators for learning low-level control and are used by benchmarks for robotic manipulation such as Meta-World (Yu et al., 2020) and Ravens (Zeng et al., 2021).Albrecht et al. (2022) propose a unified environment encompassing a variety of skills afforded through ecologically-inspired interactions.The Playhouse (Abramson et al., 2020;DeepMind Interactive Agents Team et al., 2021;Abramson et al., 2022a) and WorldLab (e.g., Gulcehre et al., 2019) environments are built using Unity (see Ward et al., 2020).Open Ended Learning Team et al. (2021) and Adaptive Agent Team et al. (2023) also use Unity to instantiate a broad distribution of procedurally generated tasks with shared underlying principles.For the results in this work, we also use Playhouse, WorldLab, and ProcTHOR.In addition, we introduce a new environment, called the Construction Lab.</p>
<p>Robotics Robotics is a key area for research in embodied intelligence.A variety of robotics projects have used simulations for training, to transfer efficiently to real-world robotic deployments (Hfer et al., 2021), though generally within a single, constrained setting.More recent work has focused on environment-generality, including scaling robotic learning datasets across multiple tasks and embodiments (Brohan et al., 2022(Brohan et al., , 2023a;;Stone et al., 2023;Padalkar et al., 2023)-thereby creating Vision-Language-Action (VLA) models (Brohan et al., 2023a), similar to the SIMA agent.The latter challenge of generalizing or quickly adapting to new embodiments has some parallels to acting in a new 3D environment or computer game where the mechanics are different.Moreover, a variety of recent works have applied pretrained (vision-)language models as a planner for a lower-level instruction-conditional robotic control policy (Brohan et al., 2023b;Driess et al., 2023;Vemprala et al., 2023;Hu et al., 2023).Our approach shares a similar philosophy to the many works that attempt to ground language via robotics.SIMA, however, avoids the additional challenges of costly hardware requirements, resource-intensive data collection, and the practical limitations on diversity of real-world evaluation settings.Instead, SIMA makes progress towards embodied AI by leveraging many simulated environments and commercial video games to obtain the sufficient breadth and richness that we conjecture to be necessary for effectively scaling embodied agents-with the hope that lessons learned (and possibly even the agents themselves) will be applicable to robotic embodiments in the future.</p>
<p>Learning environment models Some works attempt to leverage learned models of environments to train agents in these learned simulations (e.g., Ha and Schmidhuber, 2018;Hafner et al., 2020Hafner et al., , 2023;;Yang et al., 2023).These methods, however, tend to be difficult to scale to diverse sets of visually complex environments that need to be self-consistent across long periods of time.Nevertheless, learning imperfect models can still be valuable.In SIMA, we build on video models (Villegas et al., 2022), which we fine-tune on game environments.However, we only use the internal state representations of the video models rather than explicit rollouts-in keeping with other approaches that use generative modeling as an objective function for learning state representations (e.g., Gregor et al., 2019;Zolna et al., 2024).</p>
<p>Grounding language Another stream of work-overlapping with those above-has focused on grounding language in simulated 3D environments, through agents that are trained in controlled settings with semi-natural synthetic language (Hermann et al., 2017;Hill et al., 2019), or by imitating human interactions in a virtual house to learn a broader ability to follow natural language instructions (Abramson et al., 2020;DeepMind Interactive Agents Team et al., 2021;Abramson et al., 2022a,b).Moreover, a range of recent works develop agents that connect language to embodied action, generally as part of a hierarchy controlled by a language model (Jiang et al., 2019;Driess et al., 2023;Wang et al., 2023b;Hu et al., 2023;Ajay et al., 2023).We likewise draw inspiration from the idea that language is an ideal interface for directing an agent, but extend our scope beyond the limited affordances of a single controlled environment.In that sense, SIMA overlaps more with several recent works (Reed et al., 2022;Huang et al., 2023;Durante et al., 2024) that also explore training a single model to perform a broad range of tasks involving actions, vision, and language.However, SIMA is distinct in our focus on simultaneously (1) taking a language-first perspective, with all training experiences being language-driven; (2) adopting a unified, human-like interface across environments with language and vision to keyboard-and-mouse control; and (3) exploring a broad range of visually rich, diverse, and human-compatible environments that afford a wide range of complex skills.</p>
<p>Language supports grounded learning, and grounded learning supports language A key motivation of SIMA is the idea that learning language and learning about environments are mutually reinforcing.A variety of studies have found that even when language is not necessary for solving a task, learning language can help agents to learn generalizable representations and abstractions, or to learn more efficiently.Language abstractions can accelerate grounded learning, for example accelerating novelty-based exploration in reinforcement learning by providing better state abstractions (Tam et al., 2022;Mu et al., 2022), or composing known goals into new ones (Colas et al., 2020;Nottingham et al., 2023).Moreover, learning to predict natural-language explanations (Lampinen et al., 2022), descriptions (Kumar et al., 2022), or plans (Hu and Clune, 2023) can help agents to learn more efficiently, and to generalize better out of distribution.Language may be a powerful tool for shaping agent capabilities (Colas et al., 2022).</p>
<p>Conversely, richly grounded learning can also support language learning.Since human language use is deeply integrated with our understanding of grounded situations (McClelland et al., 2020), understanding the subtleties of human language will likely benefit from this grounding.Beyond this theoretical argument, empirical evidence shows that grounding can support even fundamental kinds of generalization- Hill et al. (2019) show that agents grounded in richer, more-embodied environments exhibit more systematic compositional generalization.These findings motivate the possibility that learning both language and its grounding will not only improve grounded actions, but improve a system's knowledge of language itself.</p>
<p>Approach</p>
<p>Many overlapping areas of previous and concurrent work share some of our philosophy, motivations, and approaches.What distinguishes the SIMA project is our focus on language-conditional behavior across a diverse range of visually and mechanically complex simulated environments that afford a rich set of skills.In this section, we provide a high-level overview of our approach: our environments, data, agents, and evaluations.</p>
<p>Environments</p>
<p>SIMA aims to ground language across many rich 3D environments (Figure 2).Thus, we selected 3D embodied environments that offer a broad range of open-ended interactions-such environments afford the possibility of rich and deep language interactions.We focus on environments that are either in a) first-person or b) third-person with the camera over the player's shoulder.To achieve diversity and depth of experience, we use a variety of commercial video games, as well as several environments created specifically for agent research.Each type of environment offers distinct advantages, ranging from open-ended diverse experiences to targeted assessments of agent skills.We have deliberately sought to build a portfolio of games that covers a wide range of settings-from mundane tasks in semi-realistic environments, to acting as a mischevious goat in a world with exaggerated physics, to exploring mythological worlds or science-fiction universes.Below, we briefly describe the environments we have used in SIMA thus far by category and in alphabetical order.</p>
<p>Commercial video games</p>
<p>Commercial video games offer exciting, open-ended worlds full of visual richness and the potential for complex interactions.In SIMA, we have partnered with games developers whose games we used for training agents, and we are continuing to develop relationships with new developers-for our full list of current partners, please see our Acknowledgements section.We focus on a variety of open-world or sandbox games that contain diverse skills, while avoiding games containing harmful content such as extreme violence or biases.We have also sought a broad diversity of worlds and stories, but with a focus on games that exhibit a depth of interesting mechanics.Accordingly, games from our portfolio offer a wide range of distinct challenges in perception and action, from flying a spaceship to mining minerals or crafting armor, as well as more common core features, such as navigation or gathering resources.Games also often include interactions that extend beyond the skillset of typical embodied research environments, such as menu use and interfaces more similar to those faced in computer control benchmarks (e.g., Humphreys et al., 2022;Koh et al., 2024).For the results in this report, we focus on single-player interactions within these games.</p>
<p>We run instances of each game in a secure Google Cloud environment, using hardware accelerated rendering to a virtual display.This display is streamed to a browser for human gameplay, or to a remote agent client process during evaluation.To instantiate repeatable evaluation or data collection scenarios within each game, we build datasets of save-game files from expert play, and use scripted processes to automate the process of installing game-files, booting the game, navigating its main menu, and loading a specific save-game.</p>
<p>We now provide a brief description of the games we used.</p>
<p>Goat Simulator 3:</p>
<p>A third-person game where the player is a goat in a world with exaggerated physics.The player can complete quests, most of which involve wreaking havoc.The goat is able to lick, headbutt, climb, drive, equip a wide range of visual and functional items, and perform various other actions.Throughout the course of the game, the goat unlocks new abilities, such as the ability to fly.</p>
<p>Hydroneer: A first-person mining and base building sandbox where the player is tasked with digging for gold and other resources to turn a profit and enhance their mining operation.To do this, they must build and upgrade their set-ups and increase the complexity and levels of automation until they have a fully automated mining system.Players can also complete quests from non-player characters to craft bespoke objects and gain extra money.Hydroneer requires careful planning and managing of resources.</p>
<p>No Man's Sky:</p>
<p>A first-or third-person survival game where the player seeks to explore a galaxy full of procedurally-generated planets.This involves flying between planets to gather resources, trade, build bases, and craft items that are needed to upgrade their equipment and spaceship while surviving a hazardous environment.No Man's Sky includes a large amount of visual diversity-which poses important challenges for agent perception-and rich interactions and skills.</p>
<p>Satisfactory: A first-person, open-world exploration and factory building game, in which players attempt to build a space elevator on an alien planet.This requires building increasingly complex production chains to extract natural resources and convert them into industrial goods, tools, and structures-whilst navigating increasingly hostile areas of a large open environment.</p>
<p>Teardown: A first-person, sandbox-puzzle game in a fully destructible voxel world where players are tasked with completing heists to gain money, acquiring better tools, and undertaking even more high-risk heists.Each heist is a unique scenario in one of a variety of locations where players must assess the situation, plan the execution of their mission, avoid triggering alarms, and escape before a timer expires.Teardown involves planning and using the environment to one's advantage to complete the tasks with precision and speed.</p>
<p>Valheim: A third-person survival and sandbox game in a world inspired by Norse mythology.Players must explore various biomes, gather resources, hunt animals, build shelter, craft equipment, sail the oceans and defeat mythological monsters to advance in the game-while surviving challenges like hunger and cold.</p>
<p>Wobbly Life: A third-person, open-world sandbox game where the player can explore the world, unlock secrets, and complete various jobs to earn money and buy items, leading up to buying their own house.They must complete these jobs whilst contending with the rag-doll physics of their characters and competing against the clock.The jobs require timing, planning, and precision to be completed.The world is extensive and varied, with a diverse range of interactive objects.</p>
<p>Playhouse</p>
<p>Commercial Video Games</p>
<p>WorldLab</p>
<p>Research Environments</p>
<p>Construction Lab</p>
<p>Figure 2 | Environments.We use over ten 3D environments in SIMA, consisting of commercial video games and research environments.The diversity of these environments is seen in their wide range of visual observations and environmental affordances.Yet, because these are all 3D environments, basic aspects of 3D embodied interaction, such as navigation, are shared.Commercial video games offer a higher degree of rich interactions and visual fidelity, while research environments serve as a useful testbed for probing agent capabilities.</p>
<p>Research environments</p>
<p>In contrast to commercial video games, AI research environments are typically more controllable, offering the ability to instill and carefully assess particular skills, and more rapid and reliable evaluations of task completion.Unlike many of the games in our portfolio, several of these research environments also tend to feature more real-world analogous-if still simplified-physical interactions.</p>
<p>We have drawn on several prior research environments and developed a new environment-the Construction Lab-that incorporates important challenges which were not otherwise well-captured by our other environments.</p>
<p>Construction Lab: A new research environment where agents need to build novel items and sculptures from interconnecting building blocks, including ramps to climb, bridges to cross, and dynamic contraptions.Construction Lab focuses on cognitive capabilities such as object manipulation and an intuitive understanding of the physical world.</p>
<p>Playhouse: An environment used in various prior works (Abramson et al., 2020;DeepMind Interactive Agents Team et al., 2021;Abramson et al., 2022a), consisting of a procedurally-generated house environment with various objects.We have augmented this environment with improved graphics and richer interactions, including skills like cooking or painting.</p>
<p>ProcTHOR: An environment consisting of procedurally-generated rooms with realistic contents, such as offices and libraries, introduced by Deitke et al. (2022).Although benchmark task sets exist in this environment, prior works have not used keyboard and mouse actions for agents; thus we focus on this environment primarily for data collection rather than evaluation.</p>
<p>WorldLab: An environment used in prior work (e.g., Gulcehre et al., 2019), further specialized for testing embodied agents by using a limited set of intuitive mechanics, such as sensors and doors, and relying primarily on the use of simulated physics on a range of objects.The SIMA dataset includes a broad range of text instructions that can be roughly clustered into a hierarchy.Due to the common 3D embodied nature of the environments that we consider, many generic tasks, such as navigation and object manipulation, are present in multiple environments.Categories were derived from a data-driven hierarchical clustering analysis of the human-generated text instructions within a fixed, pretrained word embedding space.Note that the area of each cluster in the wheel in Figure 3 does not correspond to the exact number of instructions from that cluster in the dataset.</p>
<p>Data</p>
<p>Our approach relies on training agents at scale via behavioral cloning, i.e., supervised learning of the mapping from observations to actions on data generated by humans.Thus, a major focus of our effort is on collecting and incorporating gameplay data from human experts.This includes videos, language instructions and dialogue, recorded actions, and various annotations such as descriptions or marks of success or failure.These data constitute a rich, multi-modal dataset of embodied interaction within over 10 simulated environments, with more to come. 1 Our data can be used to augment and leverage existing training data (e.g., Abramson et al., 2020), or to fine-tune pretrained models to endow them with more situated understanding.These datasets cover a broad range of instructed tasks: Figure 3 shows instruction clusters derived from hierarchically clustering the text instructions present in the data within a fixed, pretrained word embedding space.</p>
<p>Yet, collecting data at scale is not sufficient for training successful agents.Data quality processes 1 Note: Due to a limited amount of collected data and/or evaluations, we present agent evaluation results (Section 4) on a subset of 7 of these environments.are critical to ensuring an accurate and unconfounded mapping between language and behavior.This presents various technical challenges.We take care to engineer our data collections, including preprocessing and filtering the raw data, to highlight important skills and effectively train our agents.</p>
<p>Data collections</p>
<p>We collect data using a variety of methods, including allowing single players to freely play, and then annotating these trajectories with instructions post-hoc.We also perform two-player setter-solver collections (Abramson et al., 2020;DeepMind Interactive Agents Team et al., 2021), in which one player instructs another what to do in selected scenarios while sharing a single player view in order to match the single-player collections.All our data collections were performed with participants contracting with Google.The full details of our data collection protocols, including compensation rates, were reviewed and approved by an independent Human Behavioral Research Committee for ethics and privacy.All participants provided informed consent prior to completing tasks and were reimbursed for their time.</p>
<p>Preprocessing, filtering, and weighting Before training, we perform a variety of offline preprocessing steps, including resizing data for agent input, filtering out low-quality data using a variety of heuristics, and remixing and weighting data across environments and collections to prioritize the most effective learning experiences.</p>
<p>Agent</p>
<p>The SIMA agent maps visual observations and language instructions to keyboard-and-mouse actions (Figure 4).Given the complexity of this undertaking-such as the high dimensionality of the input and output spaces, and the breadth of possible instructions over long timescales-we predominantly focus on training the agent to perform instructions that can be completed in less than approximately 10 seconds.Breaking tasks into simpler sub-tasks enables their reuse across different settings and entirely different environments, given an appropriate sequence of instructions from the user.</p>
<p>Our agent architecture builds on prior related work (Abramson et al., 2020(Abramson et al., , 2022a)), but with various changes and adaptations to our more general goals.First, our agent incorporates not only trained-from-scratch components, but also several pretrained models-including a model trained on fine-grained image-text alignment, SPARC (Bica et al., 2024), and a video prediction model, Phenaki (Villegas et al., 2022)-which we further fine-tune on our data through behavioral cloning and video prediction, respectively.In preliminary experiments, we found that these models offer complementary benefits.Combining these pre-trained models with fine-tuning and from-scratch training allows the agent to utilize internet-scale pretraining while still specializing to particular aspects of the environments and the control tasks that it encounters.</p>
<p>More specifically, our agent (Figure 4) utilizes trained-from-scratch transformers that cross-attend to the different pretrained vision components, the encoded language instruction, and a Transformer-XL (Dai et al., 2019) that attends to past memory states to construct a state representation.The resulting state representation is provided as input to a policy network that produces keyboard-and-mouse actions for sequences of 8 actions.We train this agent with behavioral cloning, as well as an auxiliary objective of predicting goal completion.</p>
<p>We use Classifier-Free Guidance (CFG; Ho and Salimans, 2022;Lifshitz et al., 2023) to improve the language-conditionality of a trained agent when running it in an environment.CFG was originally proposed for strengthening text-conditioning in diffusion models (Ho and Salimans, 2022), but has also proven useful for similar purposes with language models (Sanchez et al., 2023) and languageconditioned agents (Lifshitz et al., 2023).That is, we compute the policy, , with and without language conditioning, and shift the policy logits in the direction of the difference between the two:   =  (image, language) +  ( (image, language)   (image, )) .</p>
<p>Evaluation methods</p>
<p>Our focus on generality in SIMA introduces challenges for evaluation.While research environments may provide automated methods for assessing whether language-following tasks have been successfully completed, such success criteria may not be generally available.That is, language instructions may not correspond to goal states recorded by an environment (e.g. a user might instruct "make a pile of rocks to mark this spot" or "see if you can jump over this chasm").</p>
<p>Evaluating agents in commercial video games poses substantial additional challenges.Video game evaluations cannot rely on access to privileged information about the state of an environment.Additionally, it is difficult to reinstate agents in precisely the same state in environments that are not designed as reproducible benchmarks, and loading each task in commercial video games is considerably slower and more costly than those in research environments.Achieving fast, stable, and reliable evaluations comparable across environments is thus challenging.We therefore use a range of distinct evaluation types that provide different trade-offs in efficiency, cost, accuracy, and coverage.Moreover, ensuring that our evaluations truly assess language conditionality, rather than environmental affordances, requires care.For instance, if a task contains a knife, a cutting board, and a carrot, the agent may ascertain the goal ("cut the carrot on the cutting board") without relying on the language instruction.Thus, task settings need to afford a diversity of actions, ideally testing multiple instructions from a single initial state, to properly evaluate whether the agent's actions are driven by language.</p>
<p>Action log-probabilities One simple approach is to evaluate agents based on their action predictions on held-out evaluation data.However, consistent with prior findings (Abramson et al., 2022b;Baker et al., 2022), we observed that agent action log-probabilities on evaluation data show at most a weak correlation with agent performance beyond the most basic skills.Thus, online evaluations, in which the agent interacts with the environment, are needed to understand agent performance in detail.</p>
<p>Static visual input</p>
<p>Similar to predicting actions on held-out data, we can provide the agent with a static visual input and a language instruction to perform a particular valid action (e.g., "jump") to assess simple responses directly mapping to particular keyboard and/or mouse actions.We have used evaluations of this form for our commercial video game environments, as they have the advantage of not requiring actually loading a game.While these evaluations can be a useful early signal, they do not reliably predict success on prolonged tasks.</p>
<p>Ground-truth Our internally-developed research environments (Construction Lab, Playhouse, and WorldLab) are capable of providing ground-truth assessments of whether language-following tasks have been successfully completed.These tasks can depend on the state of the agent ("move forward") and the surrounding environment ("lift the green cube"), as well as more complex interactions ("attach a connector point to the top of the large block" or "use the knife to chop the carrots").Such tasks enable robust testing of a range of particular skills, with a highly reliable signal of task success.Moreover, we design the task settings and evaluation to be strong tests of precision; for example, many tasks include distractor objects, for which the episode is marked as an immediate failure if the agent interacts with the distractors rather than the instruction target-even if the agent might have completed the actual task later.We also include other types of assessments, such as instructing the agent to complete one goal, and then interrupting with another goal to evaluate whether it switches appropriately-this ensures that agents are sufficiently responsive to changes in commands.A subset of our research environment tasks are used to provide a fast evaluation signal of agent progress during training.</p>
<p>Optical character recognition (OCR)</p>
<p>Many of our commercial video game environments provide on-screen text signalling the completion of tasks or quests, or even the results of lower-level actions like collecting resources or entering certain areas of a game.By detecting on-screen text using OCR in pre-defined evaluation scenarios, sometimes in combination with detecting specific keyboard-andmouse actions, we can cheaply assess whether the agent has successfully performed particular tasks.This form of automated evaluation also avoids the subjectivity of human evaluations.We make use of OCR evaluation in particular for two games, No Man's Sky and Valheim, which both feature a significant amount of on-screen text.In No Man's Sky, for example, we have developed evaluation tasks such as "mine carbon/salt/ferrite", "use the analysis visor", or "open the exosuit menu".Similarly, in Valheim we have tasks such as "collect wood/stone/raspberries", "use the workbench", or "cook food".In general, however, OCR evaluations are restricted to tasks that signal completion with game-specific text rather than arbitrary tasks that can be specified with language instructions and which we would expect a general agent to be able to solve.Other video games also have significantly less on-screen text, which makes the range of behaviors that can be evaluated in these games with OCR very narrow.</p>
<p>Human evaluation</p>
<p>In the many cases where we cannot automatically derive a signal of task success, we turn to humans to provide this assessment.While this is our most general evaluation method, it is also the slowest and most expensive.We use human judges who are game experts, i.e., they have played these specific games for at least 16 hours, and often over the course of several weeks.We ask them to review recorded agent videos, collecting multiple ratings of the same video from different judges (typically 5) to ensure reliable assessments.We also encourage strict evaluations: we instruct judges to mark an episode as a failure in cases where the agent performs irrelevant actions first, even if the agent successfully completes the instructed task afterward.</p>
<p>We curated our human-evaluation tasks by identifying a list of frequently-occurring verbs in English, and combined it with a list of verbs that naturally emerged from gameplay and interactive testing of our agents.We use this verb list as a foundation for our evaluations across all video game environments.We assign each task (save state and instruction pair) to a single, most-representative skill category (e.g."craft items"), even though most tasks require a wide range of implicit skills to succeed (e.g.crafting often requires menu use).The resulting evaluation set provides a long term challenge for agent research that spans a wide range of difficulties-from simple game agnostic tasks such as "turn left", to ones testing specialized game knowledge "compare the crafting cost of antimatter and antimatter housing", to ones utilising broader semantic knowledge such as "take the pitchfork from the person shoveling hay".Grounding our evaluation framework in the distribution of natural language allows us to test our agents in both common and adversarial scenarios, and thereby to measure our progress towards our long-term goal of developing an instructable agent that can accomplish anything a human can do in any simulated 3D environment.</p>
<p>In the results below (Section 4), we primarily report evaluation scores based on ground-truth evaluations for research environments and combined OCR and human evaluations for commercial video game environments.Across the 7 environments for which we have evaluations, we have a total of 1,485 unique tasks, spanning a range of 9 skill categories, from movement ("go ahead", "look up", "jump") to navigation ("go to the HUB terminal", "go to your ship"), resource gathering ("collect carbon", "get raspberries"), object management ("use the analysis visor", "cut the potato"), and more.(For reference, MineDojo (Fan et al., 2022), a related work investigating language-conditional agents in MineCraft, used 1,581 unique tasks spanning 4 skill categories: survival, harvest, tech-free, and combat).Given the diversity and coverage of our current evaluations, they provide a reasonable assessment of the fundamental language-conditional skills that we expect from our agent.Yet, there remains ongoing work in developing more scalable, general, and reliable evaluations, particularly as we move toward more complex and open-ended tasks.</p>
<p>Latency mitigations</p>
<p>Our agent is evaluated in several environments that run in real-time, asynchronously to the agent.This can pose challenges for the timely execution of agent-generated actions.Latencies or delays (Bratko et al., 1995) are introduced by the computation of actions and the transmission of observations and actions over the network.We account for this latency during behavioral cloning by predicting actions that are offset in time relative to the visual input to the agent, and mirror this offset during evaluation by appropriate buffering of observations and actions during neural-network inference.We additionally minimize latencies with appropriate scheduling of action computation on TPU accelerators, on-device caching of neural-network state across timesteps, and by careful choices of batch size and other implementation details.</p>
<p>Responsibility</p>
<p>We follow a structured approach to responsible model development, to identify, measure, and manage foreseeable ethics and safety challenges.These are informed by academic literature reviews, engaging with internal ethics teams, and developing comprehensive ethical assessments that document key risks with mitigation strategies.We ensure that our research projects uphold Google's AI Principles.2 SIMA was carefully assessed and reviewed to ensure that its societal benefits outweigh the risks, and that appropriate risk mitigations are incorporated.</p>
<p>No Man's Sky -"go to the spaceship"</p>
<p>Valheim -"chop down a tree" Goat Simulator 3 -"drive the car" Satisfactory -"go to the HUB" Teardown -"go through the gate"</p>
<p>Figure 5 | Agent Trajectories.The SIMA agent is capable of performing a range of language-instructed tasks across diverse 3D virtual environments.Here, we provide several representative, visually salient examples of the agent's capabilities that demonstrate basic navigation and tool use skills.</p>
<p>Benefits SIMA is a cutting-edge research initiative which focuses on how to develop instructable agents in simulated environments.This research presents interesting opportunities for the future of humans and AI collaborating together; unlike LLMs, SIMA is able to both understand natural language instructions and dynamic, interactive 3D environments.This presents a new paradigm for working with AI agents, and the potential for exciting new immersive 3D experiences with AI.Finally, simulated environments present a safer alternative for research compared to other AI deployments.</p>
<p>Risks As well as these benefits, we have reflected on potential risks associated with training on video game data.These include risks associated with training an agent on games that include violent, explicit or otherwise harmful behaviors.We have also reflected on the implications on representational harms, as the agent may learn from stereotyped depictions or actions in game settings.Besides these risks, there are also down stream risks associated with the future hypothetical deployments of SIMA, through either intentional misuse or benign action.</p>
<p>Mitigations</p>
<p>We have worked to ameliorate these risks through a holistic approach, including:</p>
<p> Careful curation of content.We avoided a number of games that have scientifically interesting, but violent environments.We also outlined behavioral "red-lines" with our ethics and safety teams; games with content that violates these red-lines are not used. Continuous evaluations of SIMA's safety performance.</p>
<p> Ensuring SIMA's deployments and agreements are transparent, and for now remain in a controlled, closed environment.</p>
<p>Ultimately, given the careful training data selection and constrained deployment environment of SIMA, we are confident we can maximize the benefits while minimising the ethical risks.</p>
<p>Initial results</p>
<p>In this section, we report initial evaluation results of the SIMA agent.After presenting several qualitative examples of the SIMA agent's capabilities, we start by considering the quantitative performance of the SIMA agent, broken down by environment and skill category.We then compare these results with several baselines and ablations, allowing us to assess the generalization capabilities of the agent and the efficacy of our design choices.Finally, we investigate a subset of evaluation tasks to estimate human-level performance as an additional comparison.</p>
<p>Qualitative examples To provide a sense of the agent's general capabilities, Figure 5 displays several representative examples of the agent in our commercial video game environments.Despite the visual diversity of the environments, the agent is capable of performing these tasks, demonstrating basic navigation and tool use skills.Even when the instructed target is not in view ("go to the spaceship" and "go to the HUB"), the agent is able to find the target.For further qualitative examples, please refer to the accompanying website. 3</p>
<p>Performance across environments and skills</p>
<p>In Figure 6, we report the average performance of the SIMA agent across the seven environments for which we have quantitative evaluations.Averages are calculated across multiple episodes per task (in research environments, one episode per task in video games), multiple tasks per environment, and across three training runs with different random seeds.Error bars denote the 95% confidence intervals (CIs) across the tasks within that environment and the three training runs with different random seeds.We note that developing informative evaluation tasks is in itself an ongoing effort, and the quantitative results in this work reflect only the range of particular behaviors that are evaluated at this point in time.</p>
<p>Overall, the results show that the SIMA agent is able to complete a range of tasks across many environments, but there remains substantial room for improvement.Performance is better for Playhouse and WorldLab, which are comparatively simpler research environments.For the more complex commercial video game environments, we see that performance is, understandably, somewhat lower.Notably, performance on Construction Lab is lower as well, highlighting the relative difficulty of this research environment and its evaluation tasks.This enables the SIMA platform to serve as a useful testbed for further development of agents that can connect language to perception and action.</p>
<p>In order to better understand the performance of the SIMA agent across an increasing variety of simulated environments, we developed an evaluation framework grounded in natural language for adding and clustering evaluation tasks, as detailed in our evaluation methods.As these skill clusters are derived from our evaluation tasks rather than the training data, they are similar to, yet distinct from, those in Figure 3.As shown in Figure 7, performance varies across different skill categories, including within skill clusters such as "movement" or "game progression".Note that even seemingly simple skill clusters can involve nontrivial game interactions, e.g., some of the "look" tasks involve 3 https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/Agents achieve notable success, but are far from perfect; their success rates vary by environment.Colors indicate the evaluation method(s) used to assess performance for that environment.(Note that humans would also find some of these tasks challenging, and thus human-level performance would not be 100%, see Section 4.3.)skills like steering a spaceship ("look at a planet") or orienting based on the surrounding terrain ("look downhill").While there are many subtleties depending on these additional interactions and the mechanics of the environment in which the skill is used, in general, skills that require more precise actions or spatial understanding ("combat", "use tools", "build") tend to be more challenging.</p>
<p>Evaluating environment generalization &amp; ablations</p>
<p>We compare our main SIMA agent to various baselines and ablations, both in aggregate (Figure 8) and broken down across our environments (Figure 9).The agents we report across all environments include:</p>
<p> SIMA: Our main SIMA agent, which is trained across all environments except for Hydroneer and Wobbly Life, which we use for qualitative zero-shot evaluation. Zero-shot: Separate SIMA agents trained like the main agent, but only on   1 of our environments, and evaluated zero-shot on the held-out environment-that is, without any BC training on it.These agents assess the transfer ability of our agent in a controlled setting.(Note that these agents use the same pretrained encoders as the main SIMA agent, which were finetuned on data from a subset of our environments; thus, in some cases the pretrained encoders will have been tuned with visual inputs from the held-out environment, even though the agent has not been trained to act in that environment.However, the encoders were not fine-tuned on data from Goat Simulator 3, thus the transfer results in that case are unconfounded.) No pretraining ablation: An agent where we removed the pretrained encoders in the SIMA agent.We replaced these models with a ResNet vision model that is trained from scratch (as in Abramson et al., 2022a), as in preliminary experiments we found training the SPARC/Phenaki encoders through agent training resulted in poor performance.Comparing to this agent tests the benefits of pretrained models for agent performance. No language ablation: An agent that lacks language inputs, during training as well as evaluation.</p>
<p>Comparing to this agent shows the degree to which our agent's performance can be explained by simple language-agnostic behavioral priors. Environment-specialized: We additionally train an expert agent on each environment, which is trained only on data corresponding to that environment, but still includes the more broadly pretrained encoders.We normalize the performance of all other agents by the expert agent on Agents exhibit varying degrees of performance across the diverse skills that we evaluate, performing some skills reliably and others with more limited success.Skill categories are grouped into clusters (color), which are derived from our evaluation tasks.</p>
<p>each environment, as a measure of what is possible using our methods and the data we have for that environment.</p>
<p>Note that due to the number of comparison agents, we only ran a single seed for each, rather than the three seeds used for the main SIMA agent.Each agent is evaluated after 1.2 million training steps. 4The bars in Figure 8 and Figure 9 represent average performance (normalized relative to the environment-specialist); the errorbars are parametric 95%-CIs across tasks and seeds (where multiple seeds are available).</p>
<p>Figure 8 shows a summary of our results, while Figure 9 shows the results by environment.SIMA outperforms environment-specialized agents overall (67% average improvement over environmentspecialized agent performance), thus demonstrating positive transfer across environments.We statistically quantify this benefit by using a permutation test on the mean difference across the per-task performance of the SIMA agent and the environment-specialized agent within each domain; in every case SIMA significantly outperforms the environment-specialized agent (-values on each environment respectively: 0.001, 0.002, 0.036, 0.0002, 0.008, 0.004, and 0.0002).Furthermore, SIMA performs much better than the baselines.SIMA substantially outperforms the no-pretraining baseline overall (permutation test  &lt; 0.001), thus showing that internet-scale knowledge supports grounded learning-though the magnitude and significance of the benefit varies across the environments (permutation test -values respectively 0.0002, 0.14, 0.041, 0.0002, 0.244, 0.052, 0.032).Finally, the no-language ablation performs very poorly (all permutation tests  &lt; 0.001).Importantly, this demonstrates not only that our agent is in fact using language, but also that our evaluation tasks are effectively designed to test this capability, rather than being solvable by simply executing plausible behaviors.Comparing the SIMA agent to an ablation without classifier-free guidance (CFG), CFG substantially improves language conditionality.However, even without CFG, the agent still exhibits language-conditional behavior, outperforming the No Language ablation.Note that this evaluation was performed only on a subset of our research environments: Construction Lab, Playhouse, and WorldLab.</p>
<p>The zero-shot evaluations are also promising.Even when tested in an environment on which it has not been trained to act the agent demonstrates strong performance on general tasks, though of course it falls short in achieving environment-specific skills.Zero-shot agents are capable of performing generic navigation skills that appear across many games (e.g."go down the hill"), and show some more complex abilities like grabbing an object by its color, using the fact that color is consistent across games, and the consistent pattern that most games use left mouse to grab or interact with objects.Importantly, even on the Goat Simulator 3 environment, where the agents have not even received visual finetuning, the zero-shot agent still performs comparably to the environmentspecialized one-thus showing transfer is not driven by the visual components alone.Note that even where the numerical performance of the zero-shot and environment-specialized agents is similar, they are generally good at different skills-with the environment-specialized agent performing well on game-specific interactions, but performing more weakly on common skills that are supported across many games, and that the zero-shot agent therefore can execute.</p>
<p>Note that zero-shot performance is especially strong on the WorldLab environment for three reasons.First, the evaluation tasks for this environment contain a relatively larger proportion of domain-general skills, such as recognizing objects by color, because we use them as rapid tests of agent capabilities.Second, this environment uses the same underlying engine and shares some implementation details with the other internal research environments, which may support behavioral transfer despite their varied visual styles, asset libraries, physical mechanics, and environment affordances.Furthermore, environment-specialized agent performance may be slightly weaker on this environment because there is a non-trivial distribution shift from training to test.This is because some of our data comes from earlier versions of the environment with differences in dynamics, and task distributions.Agents trained across multiple environments may be more robust to this distribution shift.</p>
<p>Classifier-free guidance Finally, Figure 10 compares the performance of agents with and without classifier-free guidance (CFG; Lifshitz et al., 2023), evaluated on a subset of our research environments: Construction Lab, Playhouse, and WorldLab.Without CFG ( = 0), the SIMA agent performs noticeably worse.However, the No CFG agent still exhibits a high degree of language conditionality, significantly outperforming the No Language baseline.These results show the benefit of CFG, highlighting the impact that inference-time interventions can have on agent controllability.</p>
<p>Figure 11 | Comparison with Human Performance on No Man's Sky.Evaluating on a subset of tasks from No Man's Sky, human game experts outperform all agents.Yet, humans only achieve 60% success on this evaluation.This highlights the difficulty of the tasks considered in this project.</p>
<p>Human comparison</p>
<p>To provide an additional baseline comparison, we evaluated our agents against expert human performance on an additional set of tasks from No Man's Sky, which were chosen to test a focused set of skills in a diverse range of settings.These tasks range in difficulty, from simple instructions ("walk forward") to more complex instructions ("use the analysis visor to identify new animals").The humans who performed the tasks were players who participated in our data collection and had experience with the game.We evaluated human performance using the same judges and evaluation setup that was used for our agents; the judges were not told that they were evaluating human performance rather than agents.</p>
<p>Results are summarized in Figure 11 with error bars denoting parametric 95%-CIs.The human players achieved a success rate of only 60% on these tasks, demonstrating the difficulty of the tasks we considered in this project and the stringency of our evaluation criteria.For example, some human failures appear to be due to engaging in unnecessary behaviors before completing the task, like initially opening and interacting with the starship menu when instructed to "recharge the mining beam," or entering analysis mode after scanning when told to "mine oxygen."Despite these challenging evaluations, the SIMA agent achieved non-trivial performance (34% success), far exceeding that of the No Language baseline (11% success), for example.We note that 100% success may not necessarily be achievable, due to disagreement between human judges on more ambiguous tasks.Nevertheless, there is still considerable progress needed to match human performance.This underscores the utility of the entire SIMA setup for providing a challenging, yet informative, metric for assessing grounded language interactions in embodied agents.</p>
<p>Looking ahead</p>
<p>SIMA is a work in progress.In this tech report, we have described our goal and philosophy, and presented some preliminary results showing our agent's ability to ground language instructions in behavior across a variety of rich 3D environments.We see notable performance and early signs of transfer across environments, as well as zero-shot transfer of basic skills to held-out environments.Still, many skills and tasks remain out of reach.In our future work, we aim to a) scale to more environments and datasets by continuing to expand our portfolio of games, environments, and datasets; b) increase the robustness and controllability of agents; c) leverage increasingly high-quality pretrained models (Gemini Team et al., 2023); and d) develop more comprehensive and carefully controlled evaluations.</p>
<p>We believe that by doing so, we will make SIMA an ideal platform for doing cutting-edge research on grounding language and pretrained models safely in complex environments, thereby helping to tackle a fundamental challenge of AGI.Our research also has the potential to enrich the learning experiences and deployment environments of future foundation models; one of our goals is to ground the abstract capabilities of large language models in embodied environments.We hope that SIMA will help us learn how to overcome the fundamental challenge of linking language to perception and action at scale, and we are excited to share more details about our research in the future.</p>
<p>Figure 1 |
1
Figure1| Overview of SIMA.In SIMA, we collect a large and diverse dataset of gameplay from both curated research environments and commercial video games.This dataset is used to train agents to follow open-ended language instructions via pixel inputs and keyboard-and-mouse action outputs.Agents are then evaluated in terms of their behavior across a broad range of skills.</p>
<p>Figure 3 |
3
Fight Kick</p>
<p>Figure 4 |
4
Figure 4 | Setup &amp; SIMA Agent Architecture.The SIMA agent receives language instructions from a user and image observations from the environment, and maps them to keyboard-and-mouse actions.</p>
<p>Figure 6 |
6
Figure6| Average Success Rate of the SIMA Agent by Environment.Agents achieve notable success, but are far from perfect; their success rates vary by environment.Colors indicate the evaluation method(s) used to assess performance for that environment.(Note that humans would also find some of these tasks challenging, and thus human-level performance would not be 100%, see Section 4.3.)</p>
<p>Figure 7 |
7
Figure7| Average Success Rate of the SIMA Agent by Skill Category.Agents exhibit varying degrees of performance across the diverse skills that we evaluate, performing some skills reliably and others with more limited success.Skill categories are grouped into clusters (color), which are derived from our evaluation tasks.</p>
<p>Figure 10 |
10
Figure10| Evaluating the Benefit of Classifier-Free Guidance.Comparing the SIMA agent to an ablation without classifier-free guidance (CFG), CFG substantially improves language conditionality.However, even without CFG, the agent still exhibits language-conditional behavior, outperforming the No Language ablation.Note that this evaluation was performed only on a subset of our research environments: Construction Lab, Playhouse, and WorldLab.</p>
<p>https://ai.google/responsibility/principles/
With one exception: as we had a relatively small quantity of data for Goat Simulator 3, we attempted to prevent the environment-specialized baseline from overfitting by evaluating it every 200,000 training steps, then selecting the best performing number of steps, which was 400,000 steps, as our environment-specialized baseline. Although this is a biased selection process, because we are using the environment-specialized agent as a baseline, it will only lead to underestimating the advantage of SIMA.
AcknowledgementsWe thank the following games developers for partnering with us on this project: Coffee Stain, Foulball Hangover, Hello Games, Keen Software House, Rubberband Games, Saber Interactive / Tuxedo Labs, and Strange Loop Games.We also thankBica et al. (2024)for their assistance in incorporating SPARC into the SIMA agent as well asZolna et al. (2024)and Scott Reed for their assistance in incorporating Phenaki into the SIMA agent.We thank Matthew McGill, Nicholas Roy, Avraham Ruderman, Daniel Tanis, and Frank Perbet for their assistance with research environment task development.We thank Alistair Muldal for assistance with data and infrastructure from prior efforts.We also thank Timothy Lillicrap for early input into the SIMA concept and insights from prior efforts.We thank Tom Ward, Joe Stanton, David Barker, and George Thomas for their infrastructure and support for running game binaries on Google Cloud infrastructure.Finally, we thank our team of participants who generated gameplay and language annotation data, as well as performed human evaluations of our agents, without whom this work would not have been possible.Generalization + AblationsRelative Performance (%)Figure8| Aggregate Relative Performance.Bars indicate the performance of the SIMA agent as well as the baselines and ablations relative to the performance of the environment-specialized agents, aggregated equally across environments.The SIMA agent outperforms ablations that do not incorporate internet pretraining and substantially outperforms an ablation without language.The solid line shows environment-specialized relative performance, which by normalization is 100%.Proprietary + ConfidentialGeneralization + Ablations Bars indicate the performance of the SIMA agent as well as the baselines and ablations relative to the performance of the environment-specialized agents.While performance varies across the environments, the general pattern of results is largely preserved.Even when trained while holding out an environment and evaluated zero-shot on the unseen environment, our agent can achieve non-trivial performance-almost always outperforming the no-language ablation, and in some cases even matching or exceeding environment-specialized agent performance.The solid line shows the relative performance of an environment-specialized agent, which by normalization is 100%.No Language (Ablation)Human BaselineAuthor contributionsIn this section, we summarize author contributions by project area, role in the area, and then alphabetically per role.A role key is provided at the end.Agents &amp; modelsLeads
. Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, arXiv:2012.056722020Imitating Interactive Intelligence. arXiv preprint</p>
<p>Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback. Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jessica Landon, Jirka Lhotka, Timothy Lillicrap, Alistair Muldal, arXiv:2211.116022022aarXiv preprint</p>
<p>. Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jessica Landon, Timothy Lillicrap, Alistair Muldal, Blake Richards, arXiv:2205.132742022bEvaluating Multimodal Interactive Agents. arXiv preprint</p>
<p>Human-Timescale Adaptation in an Open-Ended Task Space. Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, ; , International Conference on Machine Learning. Natalie Clay, Adrian Collister,2023</p>
<p>Akash Srivastava, and Pulkit Agrawal. Compositional Foundation Models for Hierarchical Planning. Anurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Advances in Neural Information Processing Systems. 2023</p>
<p>Avalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds. Joshua Albrecht, Abraham Fetterman, Bryden Fogelman, Ellie Kitanidis, Bartosz Wrblewski, Nicole Seo, Michael Rosenthal, Maksis Knutins, Zack Polizzi, James Simon, Advances in Neural Information Processing Systems. 2022</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023arXiv preprint</p>
<p>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune, Advances in Neural Information Processing Systems. 2022</p>
<p>The Arcade Learning Environment: An Evaluation Platform for General Agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 472013</p>
<p>Dota 2 with Large Scale Deep Reinforcement Learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysaw Dbiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, arXiv:1912.066802019arXiv preprint</p>
<p>Improving fine-grained understanding in image-text pre-training. Ioana Bica, Anastasija Ili, Matthias Bauer, Goker Erdogan, Matko Bonjak, Christos Kaplanis, Alexey A Gritsenko, Matthias Minderer, Charles Blundell, Razvan Pascanu, Jovana Mitrovi, arXiv:2401.098652024arXiv preprint</p>
<p>Ivan Bratko, Tanja Urbani, Claude Sammut, Behavioural Cloning: Phenomena, Results and Problems. IFAC Proceedings Volumes. 199528</p>
<p>RT-1: Robotics Transformer for Real. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023aarXiv preprint</p>
<p>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on Robot Learning. 2023b</p>
<p>Language Models are Few-Shot Learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 2020</p>
<p>Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration. Cdric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clment Moulin-Frier, Peter Dominey, Pierre-Yves Oudeyer, Advances in Neural Information Processing Systems. 2020</p>
<p>Language and culture internalization for human-like autotelic AI. Cdric Colas, Tristan Karch, Clment Moulin-Frier, Pierre-Yves Oudeyer, Nature Machine Intelligence. 4122022</p>
<p>PyBullet, a Python module for physics simulation for games, robotics and machine learning. Erwin Coumans, Yunfei Bai, </p>
<p>Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, Ruslan Salakhutdinov, 2019Association for Computational Linguistics</p>
<p>Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning. Deepmind Interactive, Agents Team, Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Mansi Gupta, arXiv:2112.037632021arXiv preprint</p>
<p>ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. Matt Deitke, Eli Vanderbilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi, Advances in Neural Information Processing Systems. 2022</p>
<p>CARLA: An Open Urban Driving Simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Conference on Robot Learning. 2017</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378PaLM-E: An Embodied Multimodal Language Model. 2023arXiv preprint</p>
<p>Zane Durante, Bidipta Sarkar, Ran Gong, Rohan Taori, Yusuke Noda, Paul Tang, Ehsan Adeli, Kowshika Shrinidhi, Kevin Lakshmikanth, Arnold Schulman, Milstein, arXiv:2402.05929An Interactive Agent Foundation Model. 2024arXiv preprint</p>
<p>IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, International Conference on Machine Learning. 2018</p>
<p>Building Open-Ended Embodied Agents with Internet-Scale Knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Anima Zhu, Anandkumar, Minedojo, Advances in Neural Information Processing Systems. 2022</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805A Family of Highly Capable Multimodal Models. 2023arXiv preprint</p>
<p>Shaping Belief States with Generative Environment Models for RL. Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, Aaron Van Den Oord, Advances in Neural Information Processing Systems. 2019</p>
<p>Making Efficient Use of Demonstrations to Solve Hard Exploration Problems. Caglar Gulcehre, Tom Le Paine, Bobak Shahriari, Misha Denil, Matt Hoffman, Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil Rabinowitz, Duncan Williams, International Conference on Learning Representations. 2019</p>
<p>MineRL: A Large-Scale Dataset of Minecraft Demonstrations. Brandon William H Guss, Nicholay Houghton, Phillip Topin, Cayden Wang, Manuela Codel, Ruslan Veloso, Salakhutdinov, International Joint Conference on Artificial Intelligence. 2019</p>
<p>Recurrent World Models Facilitate Policy Evolution. David Ha, Jrgen Schmidhuber, Advances in Neural Information Processing Systems. 2018</p>
<p>Mastering Atari with Discrete World Models. Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, Jimmy Ba, International Conference on Learning Representations. 2020</p>
<p>Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, arXiv:2301.04104Mastering Diverse Domains through World Models. 2023arXiv preprint</p>
<p>The Symbol Grounding Problem. Stevan Harnad, Physica D: Nonlinear Phenomena. 421-31990</p>
<p>Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, arXiv:1706.06551Grounded Language Learning in a Simulated 3D World. 2017arXiv preprint</p>
<p>Environmental drivers of systematicity and generalization in a situated agent. Felix Hill, Andrew Lampinen, Rosalia Schneider, Stephen Clark, Matthew Botvinick, James L Mcclelland, Adam Santoro, International Conference on Learning Representations. 2019</p>
<p>Grounded Language Learning Fast and Slow. Felix Hill, Olivier Tieleman, Nathaniel Tamara Von Glehn, Hamza Wong, Stephen Merzic, Clark, International Conference on Learning Representations. 2020</p>
<p>. Jonathan Ho, Tim Salimans, arXiv:2207.125982022Classifier-Free Diffusion Guidance. arXiv preprint</p>
<p>Sim2Real in Robotics and Automation: Applications and Challenges. Sebastian Hfer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Melissa Mozifian, Florian Golemo, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, IEEE Transactions on Automation Science and Engineering. 1822021</p>
<p>Training Compute-Optimal Large Language Models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint</p>
<p>Shengran Hu, Jeff Clune, arXiv:2306.00323Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. 2023arXiv preprint</p>
<p>Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao, arXiv:2311.178422023arXiv preprint</p>
<p>. Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, Siyuan Huang, arXiv:2311.12871An Embodied Generalist Agent in 3D World. 2023arXiv preprint</p>
<p>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. 2022</p>
<p>A data-driven approach for learning to control computers. David Peter C Humphreys, Tobias Raposo, Gregory Pohlen, Rachita Thornton, Alistair Chhaparia, Josh Muldal, Petko Abramson, Adam Georgiev, Timothy Santoro, Lillicrap, International Conference on Machine Learning. 2022</p>
<p>Language as an Abstraction for Hierarchical Deep Reinforcement Learning. Yiding Jiang, Shixiang Shane Gu, Kevin P Murphy, Chelsea Finn, Advances in Neural Information Processing Systems. 2019</p>
<p>The Malmo Platform for Artificial Intelligence Experimentation. Matthew Johnson, Katja Hofmann, Tim Hutton, David Bignell, International Joint Conference on Artificial Intelligence. 2016</p>
<p>Language Models can Solve Computer Tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, Advances in Neural Information Processing Systems. 2023</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried, arXiv:2401.13649VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. 2024arXiv preprint</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, arXiv:1712.05474Yuke Zhu, et al. AI2-THOR: An Interactive 3D Environment for Visual AI. 2017arXiv preprint</p>
<p>Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines. Sreejan Kumar, Carlos G Correa, Ishita Dasgupta, Raja Marjieh, Y Michael, Robert Hu, Jonathan D Hawkins, Karthik Cohen, Tom Narasimhan, Griffiths, Advances in Neural Information Processing Systems. 2022</p>
<p>Tell me why! Explanations support learning relational and causal structure. Nicholas Andrew K Lampinen, Ishita Roy, Dasgupta, C Y Stephanie, Allison Chan, James Tam, Chen Mcclelland, Adam Yan, Neil C Santoro, Jane Rabinowitz, Wang, International Conference on Machine Learning. 2022</p>
<p>Competition-Level Code Generation with AlphaCode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft. Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, Sheila Mcilraith, arXiv:2306.009372023arXiv preprint</p>
<p>Isaac Gym: High Performance GPU Based Physics Simulation For Robot Learning. Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, Advances in Neural Information Processing Systems. 2021</p>
<p>Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. Felix James L Mcclelland, Maja Hill, Jason Rudolph, Hinrich Baldridge, Schtze, Proceedings of the National Academy of Sciences. 117422020</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Nature. 51875402015</p>
<p>Mind Children: The Future of Robot and Human Intelligence. Hans Moravec, 1988Harvard University Press</p>
<p>Improving Intrinsic Exploration with Language Abstractions. Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rocktschel, Edward Grefenstette, Advances in Neural Information Processing Systems. 2022</p>
<p>Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling. Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, Roy Fox, arXiv:2301.120502023arXiv preprint</p>
<p>Open-Ended Learning Leads to Generally Capable Agents. Adam Team, Anuj Stooke, Catarina Mahajan, Charlie Barros, Jakob Deck, Jakub Bauer, Maja Sygnowski, Max Trebacz, Michael Jaderberg, Mathieu, arXiv:2107.12808Open Ended Learning. 2021arXiv preprint</p>
<p>Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, arXiv:2310.08864Open X-Embodiment: Robotic Learning Datasets and RT-X Models. 2023arXiv preprint</p>
<p>Counter-Strike Deathmatch with Large-Scale Behavioural Cloning. Tim Pearce, Jun Zhu, IEEE Conference on Games. 2022</p>
<p>VirtualHome: Simulating Household Activities via Programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, Computer Vision and Pattern Recognition. 2018</p>
<p>Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, Vladimr Vondru, Theophile Gervet, Vincent-Pierre Berges, John M Turner, Oleksandr Maksymets, Zsolt Kira, arXiv:2310.13724Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi. Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots. 2023arXiv preprint</p>
<p>. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gmez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimnez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, A Generalist Agent. Transactions on Machine Learning Research. 2022</p>
<p>Habitat: A Platform for Embodied AI Research. Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, Stella Biderman, ; Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, arXiv:2306.17806Stay on topic with Classifier-Free Guidance. 2023. 2019arXiv preprintInternational Conference on Computer Vision</p>
<p>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Computer Vision and Pattern Recognition. 2020</p>
<p>Mastering the game of Go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, Nature. 52975874842016</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Science. 36264192018</p>
<p>Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martn-Martn, Fei Xia, Kent Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, Silvio Savarese, Hyowon Gweon, Jiajun Wu, Li Fei-Fei, BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments. 2021Conference in Robot Learning</p>
<p>Open-World Object Manipulation using Pre-trained Vision-Language Models. Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Brianna Zitkovich, Fei Xia, Chelsea Finn, arXiv:2303.009052023arXiv preprint</p>
<p>Habitat 2.0: Training Home Assistants to Rearrange their Habitat. Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Advances in Neural Information Processing Systems. 2021</p>
<p>Semantic Exploration from Language Abstractions and Pretrained Representations. Allison Tam, Neil Rabinowitz, Andrew Lampinen, Nicholas A Roy, Stephanie Chan, Jane Strouse, Andrea Wang, Felix Banino, Hill, Advances in Neural Information Processing Systems. 2022</p>
<p>Towards general computer control: A multimodal agent for red dead redemption ii as a case study. Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, arXiv:2403.031862024arXiv preprint</p>
<p>Temporal Difference Learning and TD-Gammon. Gerald Tesauro, Communications of the ACM. 3831995</p>
<p>A Deep Hierarchical Approach to Lifelong Learning in Minecraft. Chen Tessler, Shahar Givony, Tom Zahavy, Daniel Mankowitz, Shie Mannor, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2017</p>
<p>MuJoCo: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, IEEE International Conference on Intelligent Robots and Systems. 2012</p>
<p>Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor, arXiv:2306.17582ChatGPT for Robotics: Design Principles and Model Abilities. 2023arXiv preprint</p>
<p>Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions. Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan, International Conference on Learning Representations. 2022</p>
<p>Grandmaster level in StarCraft II using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michal Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Nature. 57577822019</p>
<p>Voyager: An Open-Ended Embodied Agent with Large Language Models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023aarXiv preprint</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, arXiv:2311.05997JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models. 2023barXiv preprint</p>
<p>. Tom Ward, Andrew Bolt, Nik Hemmings, Simon Carter, Manuel Sanchez, Ricardo Barreira, Seb Noury, Keith Anderson, Jay Lemmon, Jonathan Coe, Piotr Trochim, Tom Handley, Adrian Bolton, arXiv:2011.092942020Using Unity to Help Solve Intelligence. arXiv preprint</p>
<p>. Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, Pieter Abbeel, arXiv:2310.061142023Learning Interactive Real-World Simulators. arXiv preprint</p>
<p>Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, Conference on Robot Learning. 2020</p>
<p>Transporter Networks: Rearranging the Visual World for Robotic Manipulation. Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, Conference on Robot Learning. 2021</p>
<p>Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. Andy Zeng, Maria Attarian, Marcin Krzysztof, Adrian Choromanski, Stefan Wong, Federico Welker, Aveek Tombari, Purohit, Vikas Michael S Ryoo, Johnny Sindhwani, Lee, International Conference on Learning Representations. 2022</p>
<p>Konrad Zolna, Serkan Cabi, Yutian Chen, Eric Lau, Claudio Fantacci, Jurgis Pasukonis, Jost Tobias Springenberg, Sergio Gomez Colmenarejo, arXiv:2401.08525GATS: Gather-Attend-Scatter. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
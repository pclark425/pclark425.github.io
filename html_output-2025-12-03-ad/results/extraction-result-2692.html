<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2692 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2692</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2692</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-269157188</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.10174v1.pdf" target="_blank">On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Text-based reinforcement learning involves an agent interacting with a fictional environment using observed text and admissible actions in natural language to complete a task. Previous works have shown that agents can succeed in text-based interactive environments even in the complete absence of semantic understanding or other linguistic capabilities. The success of these agents in playing such games suggests that semantic understanding may not be important for the task. This raises an important question about the benefits of LMs in guiding the agents through the game states. In this work, we show that rich semantic understanding leads to efficient training of text-based RL agents. Moreover, we describe the occurrence of semantic degeneration as a consequence of inappropriate fine-tuning of language models in text-based reinforcement learning (TBRL). Specifically, we describe the shift in the semantic representation of words in the LM, as well as how it affects the performance of the agent in tasks that are semantically similar to the training games. We believe these results may help develop better strategies to fine-tune agents in text-based RL scenarios.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2692.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2692.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experience Replay</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Replay Memory / Experience Replay Buffer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external buffer storing past transitions (observations, actions, rewards, next observations) used to sample training data for temporal-difference updates; referenced and used as part of the RL training pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN-like actor-critic agent with LM encoder</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A DRRN-style architecture with an LM-based text encoder (Albert or RoBERTa, or GloVe/Hash alternatives), a GRU state-action encoder, and a linear action scorer trained with actor-critic and temporal-difference losses; training uses experience replay.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld Commonsense (TWC) and Jericho (e.g., Zork 1)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-based interactive fiction benchmarks: TWC contains household commonsense cleanup tasks with limited episode length; Jericho contains classic text-adventure games (e.g., Zork) with exploration and longer-horizon objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external replay buffer (experience replay)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Sequential buffer of past transitions (observations, actions, rewards, next observations, next actions) from which training samples are drawn</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Stored transitions sampled during play (next observation and next actions are explicitly stated as being sampled from replay memory in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Samples drawn from the replay buffer for TD updates (paper states 'sampled from a replay memory' but does not specify sampling policy)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Not specified in detail in the paper beyond standard experience replay usage; the paper states training uses 'experience replay' but gives no capacity or update frequency details</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Used to provide training samples for temporal-difference (TD) learning and stabilize updates during RL training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Not directly evaluated: the paper uses experience replay as part of the standard RL training pipeline but does not analyze the effectiveness or perform ablations of replay memory versus no replay.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not discussed for replay memory specifically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2692.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2692.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRU Hidden State</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GRU-based Recurrent State Encoder (working memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gated recurrent unit (GRU) used as the state-action encoder that consumes LM-encoded observations and actions and outputs Q-values; functions as an on-line recurrent memory that summarizes recent context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN-like actor-critic agent with LM encoder</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A DRRN-style architecture with an LM-based text encoder (Albert or RoBERTa, or GloVe/Hash alternatives), a GRU state-action encoder, and a linear action scorer trained with actor-critic and temporal-difference losses.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld Commonsense (TWC) and Jericho (e.g., Zork 1)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-based interactive fiction benchmarks: TWC contains household commonsense cleanup tasks with limited episode length; Jericho contains classic text-adventure games (e.g., Zork) with exploration and longer-horizon objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Recurrent working memory (GRU hidden state)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Compact recurrent hidden-state vector maintained by the GRU across time steps (sequential, timestep-indexed hidden representation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Implicit summary of recent observations/actions and context encoded in the GRU hidden state used to predict Q-values for state-action pairs</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Implicit via recurrent hidden state propagation (no explicit retrieval mechanism described)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated online through GRU recurrence as each new encoded observation/action is processed (paper specifies a GRU is used but does not detail timestep/update mechanics beyond standard GRU usage)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Provides a sequential context summary for the state-action encoder used to compute Q-values and inform action selection under partial observability</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The paper does not experimentally isolate the contribution of the GRU (recurrent memory) versus non-recurrent alternatives; it reports overall that architectures using pretrained semantic encoders converge faster but does not attribute effects specifically to the GRU memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not specifically discussed; no direct analysis of GRU failure modes or capacity limitations is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep reinforcement learning with a natural language action space <em>(Rating: 2)</em></li>
                <li>Reading and acting while blind-folded: The need for semantics in text game agents <em>(Rating: 2)</em></li>
                <li>Multi-stage episodic control for strategic exploration in text games <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2692",
    "paper_id": "paper-269157188",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "Experience Replay",
            "name_full": "Replay Memory / Experience Replay Buffer",
            "brief_description": "An external buffer storing past transitions (observations, actions, rewards, next observations) used to sample training data for temporal-difference updates; referenced and used as part of the RL training pipeline.",
            "citation_title": "On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "DRRN-like actor-critic agent with LM encoder",
            "agent_description": "A DRRN-style architecture with an LM-based text encoder (Albert or RoBERTa, or GloVe/Hash alternatives), a GRU state-action encoder, and a linear action scorer trained with actor-critic and temporal-difference losses; training uses experience replay.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld Commonsense (TWC) and Jericho (e.g., Zork 1)",
            "game_description": "Text-based interactive fiction benchmarks: TWC contains household commonsense cleanup tasks with limited episode length; Jericho contains classic text-adventure games (e.g., Zork) with exploration and longer-horizon objectives.",
            "uses_memory": true,
            "memory_type": "external replay buffer (experience replay)",
            "memory_structure": "Sequential buffer of past transitions (observations, actions, rewards, next observations, next actions) from which training samples are drawn",
            "memory_content": "Stored transitions sampled during play (next observation and next actions are explicitly stated as being sampled from replay memory in the paper)",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Samples drawn from the replay buffer for TD updates (paper states 'sampled from a replay memory' but does not specify sampling policy)",
            "memory_update_strategy": "Not specified in detail in the paper beyond standard experience replay usage; the paper states training uses 'experience replay' but gives no capacity or update frequency details",
            "memory_usage_purpose": "Used to provide training samples for temporal-difference (TD) learning and stabilize updates during RL training",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Not directly evaluated: the paper uses experience replay as part of the standard RL training pipeline but does not analyze the effectiveness or perform ablations of replay memory versus no replay.",
            "memory_limitations": "Not discussed for replay memory specifically in this paper.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2692.0",
            "source_info": {
                "paper_title": "On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GRU Hidden State",
            "name_full": "GRU-based Recurrent State Encoder (working memory)",
            "brief_description": "A gated recurrent unit (GRU) used as the state-action encoder that consumes LM-encoded observations and actions and outputs Q-values; functions as an on-line recurrent memory that summarizes recent context.",
            "citation_title": "On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "DRRN-like actor-critic agent with LM encoder",
            "agent_description": "A DRRN-style architecture with an LM-based text encoder (Albert or RoBERTa, or GloVe/Hash alternatives), a GRU state-action encoder, and a linear action scorer trained with actor-critic and temporal-difference losses.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld Commonsense (TWC) and Jericho (e.g., Zork 1)",
            "game_description": "Text-based interactive fiction benchmarks: TWC contains household commonsense cleanup tasks with limited episode length; Jericho contains classic text-adventure games (e.g., Zork) with exploration and longer-horizon objectives.",
            "uses_memory": true,
            "memory_type": "Recurrent working memory (GRU hidden state)",
            "memory_structure": "Compact recurrent hidden-state vector maintained by the GRU across time steps (sequential, timestep-indexed hidden representation)",
            "memory_content": "Implicit summary of recent observations/actions and context encoded in the GRU hidden state used to predict Q-values for state-action pairs",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Implicit via recurrent hidden state propagation (no explicit retrieval mechanism described)",
            "memory_update_strategy": "Updated online through GRU recurrence as each new encoded observation/action is processed (paper specifies a GRU is used but does not detail timestep/update mechanics beyond standard GRU usage)",
            "memory_usage_purpose": "Provides a sequential context summary for the state-action encoder used to compute Q-values and inform action selection under partial observability",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The paper does not experimentally isolate the contribution of the GRU (recurrent memory) versus non-recurrent alternatives; it reports overall that architectures using pretrained semantic encoders converge faster but does not attribute effects specifically to the GRU memory.",
            "memory_limitations": "Not specifically discussed; no direct analysis of GRU failure modes or capacity limitations is provided.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2692.1",
            "source_info": {
                "paper_title": "On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep reinforcement learning with a natural language action space",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_with_a_natural_language_action_space"
        },
        {
            "paper_title": "Reading and acting while blind-folded: The need for semantics in text game agents",
            "rating": 2,
            "sanitized_title": "reading_and_acting_while_blindfolded_the_need_for_semantics_in_text_game_agents"
        },
        {
            "paper_title": "Multi-stage episodic control for strategic exploration in text games",
            "rating": 2,
            "sanitized_title": "multistage_episodic_control_for_strategic_exploration_in_text_games"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 1,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        }
    ],
    "cost": 0.0083465,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning
15 Apr 2024</p>
<p>Maurício Gruppi 
Villanova University</p>
<p>Soham Dan soham.dan@ibm.com 
IBM Research</p>
<p>Keerthiram Murugesan keerthiram.murugesan@ibm.com 
IBM Research</p>
<p>Subhajit Chaudhury subhajit@ibm.com 
IBM Research</p>
<p>On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning
15 Apr 2024AD09BB2083C55381D03FD793053CDC7FarXiv:2404.10174v1[cs.CL]
Text-based reinforcement learning involves an agent interacting with a fictional environment using observed text and admissible actions in natural language to complete a task.Previous works have shown that agents can succeed in text-based interactive environments even in the complete absence of semantic understanding or other linguistic capabilities.The success of these agents in playing such games suggests that semantic understanding may not be important for the task.This raises an important question about the benefits of LMs in guiding the agents through the game states.In this work, we show that rich semantic understanding leads to efficient training of text-based RL agents.Moreover, we describe the occurrence of semantic degeneration as a consequence of inappropriate fine-tuning of language models in text-based reinforcement learning (TBRL).Specifically, we describe the shift in the semantic representation of words in the LM, as well as how it affects the performance of the agent in tasks that are semantically similar to the training games.We believe these results may help develop better strategies to fine-tune agents in text-based RL scenarios.</p>
<p>Introduction</p>
<p>Text-based games (TBGs) are a form of interactive fiction where players use textual information to manipulate the environment.Since information in these games is shared as text, a successful player must hold a certain degree of natural language understanding (NLU).TBGs have surfaced as important testbeds for studying the linguistic potential of reinforcement learning agents along with partial observability and action generation.TBGs can be modeled as partially observable Markov decision processes (POMDP) defined by the tuple ⟨S, A, O, T, E, R⟩, where S is the set of states, A the set of actions, O the observation space, T the set of state transition probabilities, E is the conditional observation emission probabilities, and R : S × A → R the reward function.The goal of a TBG agent is to reach the end of the game by interacting with the environment through text, while maximizing the final score.</p>
<p>In TBGs, observations and actions are presented in the form of unstructured text, therefore, they must be encoded before being passed onto the RL network.Recent works in text-based RL adopt a strategy where such encoding is learned from the game, typically by fine-tuning a language model, such as embeddings or transformers, using the rewards values from the training (Yao et al., 2020;Wang et al., 2022a).We hypothesize that this approach may cause the language model to overfit the training games, leading to the degeneration of the semantic relationships learned during pretraining, and, subsequently, negatively impacting the agent's training efficiency and transfer learning capacity.We conduct experiments in two distinct TBG domains: (1) TextWorld Commonsense (TWC) (Murugesan et al., 2021a), and (2) Jericho (Hausknecht et al., 2019).The former provides a number of games where the goal is to perform house cleaning tasks such as taking objects from a location and placing them in their appropriate places, using commonsense knowledge.The latter provides a library of classic text-adventure games, such as the Zork (1977), each having its own unique objectives, characters, and events.Unlike TWC games, Jericho games may not let the player know a priori what the final goal is.Instead, the player is expected to explore the game to learn the story and complete the tasks one-by-one.In both domains, the agent selects an action from a given list.</p>
<p>Under this framework, we address the following research questions:</p>
<ol>
<li>
<p>Does fine-tuning the language model to the RL rewards improve the training efficiency in comparison to fixed pre-trained LMs?</p>
</li>
<li>
<p>Does fine-tuning LMs make agents robust to tasks containing out-of-training vocabulary?</p>
</li>
</ol>
<p>Our goal is to evaluate what are the implications, pros, and cons of fine-tuning LMs to the RL tasks.Our results indicate fine-tuning LMs to rewards leads to a decrease in the agent's performance and hinders its ability to play versions of the training games where the observations and actions are slightly reworded, such as through paraphrasing or lexical substitution (synonyms).In comparison to fixed pre-trained LMs, these fine-tuned agents under-performed in training and in test settings.We refer to this process as semantic degeneration, because it leads to loss of relevant semantic information, in the LM, that would be crucial to produce generalizable representation.For instance, by learning that the terms "bloody axe" and "kitchen" are related to each other in the game Zork 1, the agent overfits to this setting and, in turn, loses relevant information about "kitchen" and "bloody axe" that could be important to other games.In NLP, semantic generation might be an expected consequence of fine-tuning (Mosbach et al., 2020), however, the vast majority of text-based RL agents employ LMs that are fully fine-tuned to the game's semantics.</p>
<p>Background</p>
<p>Model and Architecture The general architecture of the agents in this work consist of a state encoder akin to the DRRN (He et al., 2015) with an actor-critic policy learning (Wang et al., 2016) and experience replay.The main components of the agent's network are (1) a text encoder, (2) a state-action encoder, and (3) an action scorer.The text encoder module is a language model that converts an observation o ∈ O and action a ∈ A from text form to fixed length vectors f (o) and f (a).The state-action encoder consists of a GRU (Dey and Salem, 2017) that takes as input the encoded state and actions, and predicts the Qvalues for each pair:
Q ϕ (o, a) = g(f (o), f (a))
given parameters ϕ.The action predictor is a linear layer that outputs the probabilities based on the Q-values from the previous layer.The chosen action is drawn following the computed probability distribution.The agent is trained by minimizing the temporal differences (TD) loss:
L T D = (r + γ max a ′ ∈A Q ϕ (o ′ , a ′ ) − Q ϕ (o, a)) 2
where o ′ and a ′ are the next observation and next actions sampled from a replay memory, γ is the reward discount factor.</p>
<p>Text Encoders We used three distinct types of encoders in this study:</p>
<p>• Hash -does not capture semantic information from the text.It utilizes a hash function to reduce the observation to a random vector.</p>
<p>Similarly to Yao et al. ( 2021).• Word Embedding -pre-trained static GloVe embeddings (Pennington et al., 2014) and a GRU to encode the sequences of tokens.• Transformers -pre-trained LMs to encode observations (Devlin et al., 2018;Liu et al., 2019).</p>
<p>These encoders are often the top performer (Murugesan et al., 2021b;Ammanabrolu and Hausknecht, 2020;Wang et al., 2022b;Atzeni et al., 2021;Yao et al., 2020;Tuyls et al., 2021) in benchmark environments for text-based reinforcement learning such as Textworld (Côté et al., 2018), Jericho (Hausknecht et al., 2019), Scienceworld (Wang et al., 2022b), etc.</p>
<p>Results</p>
<p>We now present our main results.In the TWC environment, agents are trained for 100 episodes, with a maximum of 50 steps per episode (repeated over 5 runs).In the Jericho environment, agents were trained over 100000 steps with no limit to the number of episodes (repeated over 3 runs).These settings were chosen following previous work reference in this manuscript, such as Yao et al. ( 2021) and Murugesan et al. (2021b).</p>
<p>We deploy agents of the same architecture as described in Section 2, the only exception being that the input encoder used by them is different.The encoders are the Hash encoder, which produces semantic-less vectors, the Word Embedding which uses pre-trained GloVe embeddings, and the transformer LMs Albert (Lan et al., 2019)  RoBERTa (Liu et al., 2019).The transformer encoders are used in two variations: Fixed, where the LMs weights are frozen; and Fine-tuned (FT), where the LMs weights are updated according to the rewards.This allows us to compare the performance of the typical text-based RL fine-tuning approach to unconventional ones.</p>
<p>Semantic Information from Pre-Training</p>
<p>Improves the Overall RL Performance</p>
<p>We evaluate the use of different LMs to encode the observations and actions into fixed-length vectors.We begin our analysis with the weights of the language model-based encoders fixed, i.e., only the RL network parameters ϕ are updated.</p>
<p>The rich semantic information of LMs accelerates training:</p>
<p>The results from these experiments show that even an agent without semantic information can properly learn to play the games.However, an agent leveraging the semantic representations from language models are able to: (1) converge more quickly, in training, to a stable score than hash and simple, as shown in Figure 2; (2) handle out-of-training vocabulary, Table 1 shows the performance of the models under two settings: games using an in-training vocabulary (ID) and games using an out-of-training vocabulary (OOD).These results show that the fixed transformer LMs outperform the Hash and Embedding models in both vocabulary distributions, highlighting the importance of keeping the semantic information from pre-training intact.</p>
<p>Semantic Degeneration Hurts Learning</p>
<p>In this experiment, we address the first proposed research question: "does fine-tuning the LM to the RL rewards improve the training efficiency in comparison to fixed pre-trained LMs?" To that end, we trained the Fixed and Fine-tuned variations of Albert and RoBERTa encoders on the same games, and compared their scores during training.Figure 3 shows the outcome of the experiment.The findings suggest that traditional text-based RL approach of fine-tuning the LMs lead to substantially lower training scores, which are due to semantic degeneration.That is, semantic degeneration leads to ineffective training of the RL agents, whereas the fixed models converge to a higher score after a relatively small number episodes/steps.Semantic degeneration arises from fine-tuning the LMs to the training rewards.The LM "forgets" its semantic associations it had learning during its pre-training, such as the masked token prediction in the case of transformers.This "forgetting" originates from overfitting the model's weights to the games' word distributions.The biggest problem arises from the fact that the RL network receives the encoded vectors from the LM and updates its weights based on such initial representations.However, since the LMs are fine-tuned, the encoding will change between each episode, causing the RL network to receive a different encoding for the same observation as the training goes on.</p>
<p>A comparison of pre-trained and semantically degenerated word vectors is seen in Figure 4 RoBERTa model is seen in Figure 4a; Figure 4b shows the plot of the word vectors after fine-tuning the LM to the game Zork 1.Notice the shift of the term "bloody axe" towards the term "kitchen" from (a) to (b).The shift happens because both terms appear in a sequence early on in the game, therefore, the association between their vectors becomes stronger as the LM is fine-tuned.Moreover, the terms "egg" and "nest" shift away from "chicken".The first two terms are also employed in the game in a sequence where the agent receives a positive reward, whereas the last is never used in the game.Despite being related in-game, these terms should have their semantic relationships preserved, which is possible by utilizing fixed LMs.</p>
<p>Agents with fine-tuned LMs are less robust to language change</p>
<p>We address the second research question: "does fine-tuning LMs make agents robust to tasks containing out-of-training vocabulary?".To test the robustness of each model, we first train each agent on a particular game.Then, we evaluate the agents by having them play games where the observations are transformed in one of the following ways: Paraphrasing, we run the observations through a paraphrasing model to rephrase the descriptions (using a Bart-based paraphrase (Lewis et al., 2019)); Lexical Substitution, we replace words in the observations using synonyms and hypernyms from WordNet (Fellbaum, 2010).By playing these versions of the games, agents have to perform the same task as seen in training, but with reworded or slightly modified observations.Figure 5a shows the fixed LM agent is robust to paraphrasing as it is able to maintain the original In both scenarios, fixed LMs exhibit strong robustness to the perturbations, scoring as much as in the games without perturbations.</p>
<p>score even in the modified versions.This is due to the ability of LMs to handle such perturbations in text.This evidence emphasizes the hypothesis that semantic understanding is important for generalization to words unseen in training.Figure 5b shows the performance of the three agents in Zork 1.The fine-tuned agent exhibits a decline in performance while playing the paraphrased and lexical substitution games.This is explained by the fact that the LM has been adjusted to the semantics of the original game, thus, tokens are no longer distributed according to semantic similarity.The hash-based agent is unable to score in either of the modified games due to the lack of semantic information.The fixed agent, however, exhibits strong robustness to the perturbations.This shows how semantic degeneration leads to decrease in performance in unseen or slightly different games.</p>
<p>Conclusion</p>
<p>In this paper, we have put forth a novel perspective over the occurrence of semantic degeneration at the intersection of LM fine-tuning and text RL.</p>
<p>We have shown that semantic understanding brings benefits to the training of agents.Moreover, despite being the typical approach to text-based RL, learning the semantics from the game may not be the optimal approach to training agents.Our results corroborate the well known trends of trading-off general semantics for task-specific representations in NLP tasks; we shine light on how this affects agents in carrying out tasks that are semantically similar to the training ones.Our results indicate that using meaningful semantic representations can be beneficial, and fine-tuning strategies may be developed to ensure prior semantic information is not lost by the model, while learning task-specific representations.</p>
<p>Limitations</p>
<p>Our work focuses on popular TBG environments and also popular choices of LMs.In future work it would be interesting to study rarer TBG environments, potentially beyond English.In that context it would also be interesting to study multilingual LMs as the semantic representation for these games.Since we use LM representations for game playing, some of the limitations of these representations (like inability to distinguish between some related concepts, or certain biases), might carry over.Investigating these in detail is another interesting avenue to be explored.</p>
<p>Objects Targets Rooms</p>
<p>Easy
1 1 1 Medium 2-3 1-3 1 Hard 6-7 5-7 1-2</p>
<p>A Appendix</p>
<p>A.1 TextWorld Commonsense</p>
<p>This section contains information about the games (Table 2) in TextWorld Commonsense as well as an example of an observation and plausible actions (Figure 6).The goal of TWC games are to complete a series of household tasks, such as "picking up an apple and putting it in an appropriate location".The agent is provided with the description of a scene and a list of plausible actions.They must then decide which action to be taken in the current game state.If the action performed is good, the agent is rewarded with points.</p>
<p>TWC games are split into easy, medium and hard difficulties.As the difficulty increases, the number of target objects and rooms to cleanup increases.Details can be seen in Table 2.</p>
<p>A.2 Model comparison in TWC</p>
<p>Figure 7 shows the comparison between all language models in all three difficulties of TWC in terms of normalized score and number of movements.</p>
<p>These results show how agents using fixed LMs converge earlier to a stable score (Figures 7 a, b, c) and to stable number of movements (Figures 7 d,  e, f).Higher scores are better.Lower number of movements are better because it means the agent can complete the task while taking fewer actions, avoiding unnecessary moves.</p>
<p>A.3 Complete Table of TWC Results</p>
<p>Tables 3 and 4 show the results for all difficulties in TWC in the in-distribution set and out-ofdistribution set.</p>
<p>We can see that fixed LMs consistently perform better when applied to both in-distribution and outof-distribution tasks.This is due to the fact that they can keep rich semantic information and not suffering from semantic degeneration.</p>
<p>A.4 Complete results for perturbation experiments in TWC</p>
<p>Figure 8 shows the results for the perturbation experiments in TWC difficulties.</p>
<p>The result show how that a fixed LM model (RoBERTa) can maintain a relatively similar performance to the original observations when playing noisy versions of the game.</p>
<p>A.5 Comparison of Fine-tuned/Fixed LMs on Jericho games</p>
<p>Figure 9 shows the fine-tuning/fixed LM comparison on additional games from the Jericho library: detective, pentari, inhumane, and enchanter.</p>
<p>The models show a consistent trend in which the fixed LMs outperform the fine-tuned models.</p>
<p>A.6 Text perturbations</p>
<p>This sections presents a description of the perturbations applied to the game texts.</p>
<p>A perturbation is a modification of an original piece of text in the game to produce an "out-oftraining" example.Perturbations are applied to the observations, actions and inventories.</p>
<p>The types of perturbations are:</p>
<p>• Lexical substitution -we use WordNet synsets to find replacements for words in the text</p>
<p>• Paraphrasing -we use a sequence-to-sequence BART paraphraser to rephrase the original text</p>
<p>B Reproducibility</p>
<p>The code needed used to implement the methods described in this manuscript are submitted along with the supplementary material.The code is anonymous and contains the instructions to set up the environments, download the game data, and train the agents.</p>
<p>Figure 1 :
1
Figure 1: Semantic degeneration of the terms kitchen and bloody axe in Zork 1.</p>
<p>Figure 3 :
3
Figure 3: Training curves of fixed/fine-tuned LMs on (left) TWC medium difficulty games and (right) Zork 1.Due to semantic degeneration, the fine-tuned models do not exhibit an increasing score converging to a maximum value.Shaded areas denote one standard deviation.</p>
<p>Figure 4 :
4
Figure 4: Shift caused by the semantic degeneration to the contextual word vectors in the RoBERTa model finetuned to Zork 1: (a) shows the word embeddings from the pre-trained model, (b)shows the word embeddings after fine-tuning to Zork 1.The bold words denote the case where the term "bloody axe" shifts towards the word "kitchen" as a result of them co-occurring in a positively rewarded state.</p>
<p>Figure 5 :
5
Figure 5: Evaluation of a RoBERTa agent on original (none), paraphrased, and lexical substitution observations on (left) TWC medium games and (right) Zork 1.In both scenarios, fixed LMs exhibit strong robustness to the perturbations, scoring as much as in the games without perturbations.</p>
<p>Table 1 :
1
and Normalized scores for the in-distribution vocabulary (ID) and out-of-distribution vocabulary (OOD) game sets in TWC's Medium difficulty games.(<em>) Indicates fixed language models.
AlbertEmbeddingHashRoberta0.630Norm. score0.3 0.4 0.5Score0 20 100.21002040 Episode 60801000 20000 40000 60000 80000 100000 StepFigure 2: Training performance comparing LM-basedencoding models and hash/word embedding-based mod-els. (left) shows the normalized scores for TWC gamesand (right) shows the game score achieved in trainingacross 100k steps in Zork 1. Shaded area correspondsto one standard deviation.ModelIDOODHash0.58 ± 0.06 0.15 ± 0.03Embedding0.58 ± 0.08 0.43 ± 0.07Albert (Lan et al., 2019)</em>0.66 ± 0.05 0.65 ± 0.05RoBERTa (Liu et al., 2019)* 0.70 ± 0.05 0.53 ± 0.06</p>
<p>Table 2 :
2
No. of objects, target objects and rooms in TWC games per difficulty level.
Shunyu Yao, Karthik Narasimhan, and MatthewHausknecht. 2021. Reading and acting while blind-folded: The need for semantics in text game agents.arXiv preprint arXiv:2103.13552.Shunyu Yao, Rohan Rao, Matthew Hausknecht, andKarthik Narasimhan. 2020. Keep calm and explore:Language models for action generation in text-basedgames. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 8736-8754.
ObservationYou've entered a kitchen.Look over there!A dishwasher.You can see a closed cutlery drawer.You see a ladderback chair.On the ladderback chair you can make out a dirty whisk.Plausible ActionsOpen dishwasher Open cutlery drawer Take dirty whisk from ladderback chair
Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, arXiv:2001.088372020arXiv preprint</p>
<p>Casebased reasoning for better generalization in textual reinforcement learning. Mattia Atzeni, Shehzaad Zuzar Dhuliawala, Keerthiram Murugesan, Mrinmaya Sachan, International Conference on Learning Representations. 2021</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Workshop on Computer Games. Springer2018</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>Gate-variants of gated recurrent unit (gru) neural networks. Rahul Dey, M Fathi, Salem, 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS). IEEE2017</p>
<p>Wordnet. Christiane Fellbaum, Theory and applications of ontology: computer applications. Springer2010</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Côté Marc-Alexandre, Yuan Xingdi, CoRR, abs/1909.053982019</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, arXiv:1511.04636Deep reinforcement learning with a natural language action space. 2015arXiv preprint</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, arXiv:1909.119422019arXiv preprint</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.134612019arXiv preprint</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow, arXiv:2006.04884On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines. 2020arXiv preprint</p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. 2021a</p>
<p>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, ASSOC COMPU- TATIONAL LINGUISTICS-ACLAcl-Ijcnlp 2021: The 59Th Annual Meeting Of The Association For Computational Linguistics And The 11Th International Joint Conference On Natural Language Processing. 2021b2</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)2014</p>
<p>Multi-stage episodic control for strategic exploration in text games. Jens Tuyls, Shunyu Yao, Sham M Kakade, Karthik R Narasimhan, International Conference on Learning Representations. 2021</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, arXiv:2210.07382Behavior cloned transformers are neurosymbolic reasoners. 2022aarXiv preprint</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022b</p>
<p>Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando De Freitas, arXiv:1611.01224Sample efficient actor-critic with experience replay. 2016arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5297 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5297</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5297</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-410e8360e092f381caab9cd48a1d954259a7651b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/410e8360e092f381caab9cd48a1d954259a7651b" target="_blank">ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots</a></p>
                <p><strong>Paper Venue:</strong> Journal of Chemical Information and Modeling</p>
                <p><strong>Paper TL;DR:</strong> The integration of advanced AI in drug discovery is explored but the landscape is reimagined by advocating for AI-powered chatbots as trailblazers in revolutionizing therapeutic innovation by shedding light on the collaborative synergy between human expertise and AI assistance.</p>
                <p><strong>Paper Abstract:</strong> The birth of ChatGPT, a cutting-edge language model-based chatbot developed by OpenAI, ushered in a new era in AI. However, due to potential pitfalls, its role in rigorous scientific research is not clear yet. This paper vividly showcases its innovative application within the field of drug discovery. Focused specifically on developing anti-cocaine addiction drugs, the study employs GPT-4 as a virtual guide, offering strategic and methodological insights to researchers working on generative models for drug candidates. The primary objective is to generate optimal drug-like molecules with desired properties. By leveraging the capabilities of ChatGPT, the study introduces a novel approach to the drug discovery process. This symbiotic partnership between AI and researchers transforms how drug development is approached. Chatbots become facilitators, steering researchers towards innovative methodologies and productive paths for creating effective drug candidates. This research sheds light on the collaborative synergy between human expertise and AI assistance, wherein ChatGPT’s cognitive abilities enhance the design and development of potential pharmaceutical solutions. This paper not only explores the integration of advanced AI in drug discovery but also reimagines the landscape by advocating for AI-powered chatbots as trailblazers in revolutionizing therapeutic innovation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5297.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5297.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (ChatGPT / OpenAI GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model (LLM) used in this study as a virtual research assistant in three personae (idea generation, methodology clarification, coding assistance) to advise on generative-model design, stochastic optimization (Langevin dynamics), similarity metrics, code, and analysis — but not directly used to generate molecular structures in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>large language model (LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery research assistance (ideation, methodology, coding); advisory role in designing a molecule generator for anti-cocaine addiction leads</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompting/persona-based interactive prompting (three personae for idea generation, methodology clarification, and coding); provided conceptual design (e.g., stochastic/Langevin updates) but was not used to directly generate chemical structures</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Responses from GPT-4 were validated by cross-referencing literature and expert judgment; downstream molecules (produced by the SGNC pipeline inspired by GPT-4 suggestions) were evaluated with binding-affinity predictors (10-fold CV Pearson R and RMSE), ADMETlab 2.0 properties, and molecular docking.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Project used ChEMBL-derived DAT-, NET-, SERT-, and hERG-inhibitor datasets for downstream model training and evaluation (these datasets were used in conjunction with GPT-4's guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 informed key design choices (recommendation to use stochastic optimization inspired by diffusion models, suggestion of Langevin equation for latent-space updates, choice of cosine similarity for latent vectors, selection criteria for reference/seed molecules, and coding support). Using these human+GPT-4-guided decisions, the team implemented a Stochastic Generative Network Complex (SGNC) that generated ~16 million candidate vectors (decoded to molecules), yielding 330 preliminary multi-target candidates and a final set of 15 lead molecules after binding-affinity, ADMET, and docking filters; GPT-4 itself did not directly output the generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>GPT-4 was used as an interactive advisor to improve an existing generative framework (GNC). The paper contrasts traditional gradient-descent latent-space editing with GPT-4-suggested stochastic/Langevin-based latent updates; no quantitative head-to-head comparison of GPT-4 versus other LLMs is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>GPT-4 produced inaccuracies and misleading statements at times (incorrect definitions of some equations, occasional mislabeling of functional groups, and other misinformation); outputs required human verification and literature cross-checking; GPT-4 was advisory only and not a reliable substitute for domain expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5297.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5297.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI conversational LLM family, referenced generically)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General references in the paper to ChatGPT capabilities (literature digestion, code assistance, and claims in prior works that ChatGPT/InstructGPT can generate novel compound structures and predict properties). These are discussed in the introduction as context and prior-art claims, not as primary experiments of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>large language model (LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>mentioned as a tool for literature parsing, code interpretation, and—per cited prior work—novel compound suggestion and property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>mentioned in prior literature as being used via prompting and associated tools/plugins to propose compound structures and predict PK/PD (the present paper cites these claims but does not replicate them directly)</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>The paper references prior claims that ChatGPT can predict PK/PD and generate novel compound structures (citation [7] in the paper) but does not itself present primary benchmark metrics for ChatGPT-generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Introduced in background: ChatGPT is noted to expedite literature review, code interpretation, and (per other cited works) to have been used to propose novel compounds and predict properties; in this study ChatGPT/GPT-4 was used as an advisor rather than as the primary molecule generator.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned as complementing domain-specific generative models; no direct empirical comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The paper emphasizes ChatGPT's tendency to hallucinate or provide inaccurate information; recommends human verification and cross-referencing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5297.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5297.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGPT (OpenAI instruction-tuned LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in the introduction as an LLM that has been demonstrated (in cited prior work) to help write accurate chemistry-related code and that prompt engineering improved performance substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>large language model (LLM, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>chemistry code generation / coding assistance (as reported in prior work cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>instruction tuning / prompt engineering (mentioned that prompt engineering improved accuracy by ~30 percentage points in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>The paper cites prior work reporting improvements (a ~30 percentage-point increase in coding accuracy with prompt engineering) but does not itself evaluate InstructGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an example of an LLM applied to chemistry-related code tasks; used here as background context rather than as part of the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Not compared empirically in this paper; included as prior-art example of LLM utility in chemical/computational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No detailed limitations for InstructGPT are given in this paper beyond the general cautions about LLM hallucination and the need for human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5297.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5297.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GitHub Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GitHub Copilot (AI code assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial AI code-completion assistant integrated into the authors' VS Code environment to accelerate coding; described as providing autocomplete-style suggestions based on a generative AI model and used for code assistance in this project.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GitHub Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM-based code assistant / generative code model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>coding assistance (software development, script generation and debugging) used in the drug-discovery pipeline implementation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>autocomplete-style code completion (integrated into developer environment to provide code snippets and accelerate coding)</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>code snippets (Python, PyTorch templates, plotting scripts)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used by the authors to speed coding and provide auto-complete suggestions while implementing models, plotting, and analysis; not used to design or generate chemical structures directly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Described qualitatively as expediting coding; no quantitative benchmark versus human-only coding is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>As with other LLM tools, outputs require developer review; no specific failure cases for Copilot in this study are detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative Network Complex (GNC) <em>(Rating: 2)</em></li>
                <li>ADMETlab 2.0 <em>(Rating: 2)</em></li>
                <li>InstructGPT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5297",
    "paper_id": "paper-410e8360e092f381caab9cd48a1d954259a7651b",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (ChatGPT / OpenAI GPT-4)",
            "brief_description": "A large language model (LLM) used in this study as a virtual research assistant in three personae (idea generation, methodology clarification, coding assistance) to advise on generative-model design, stochastic optimization (Langevin dynamics), similarity metrics, code, and analysis — but not directly used to generate molecular structures in this work.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "large language model (LLM)",
            "model_size": null,
            "training_data": null,
            "application_domain": "drug discovery research assistance (ideation, methodology, coding); advisory role in designing a molecule generator for anti-cocaine addiction leads",
            "generation_method": "prompting/persona-based interactive prompting (three personae for idea generation, methodology clarification, and coding); provided conceptual design (e.g., stochastic/Langevin updates) but was not used to directly generate chemical structures",
            "output_representation": null,
            "evaluation_metrics": "Responses from GPT-4 were validated by cross-referencing literature and expert judgment; downstream molecules (produced by the SGNC pipeline inspired by GPT-4 suggestions) were evaluated with binding-affinity predictors (10-fold CV Pearson R and RMSE), ADMETlab 2.0 properties, and molecular docking.",
            "benchmarks_or_datasets": "Project used ChEMBL-derived DAT-, NET-, SERT-, and hERG-inhibitor datasets for downstream model training and evaluation (these datasets were used in conjunction with GPT-4's guidance)",
            "results_summary": "GPT-4 informed key design choices (recommendation to use stochastic optimization inspired by diffusion models, suggestion of Langevin equation for latent-space updates, choice of cosine similarity for latent vectors, selection criteria for reference/seed molecules, and coding support). Using these human+GPT-4-guided decisions, the team implemented a Stochastic Generative Network Complex (SGNC) that generated ~16 million candidate vectors (decoded to molecules), yielding 330 preliminary multi-target candidates and a final set of 15 lead molecules after binding-affinity, ADMET, and docking filters; GPT-4 itself did not directly output the generated molecules.",
            "comparison_to_other_methods": "GPT-4 was used as an interactive advisor to improve an existing generative framework (GNC). The paper contrasts traditional gradient-descent latent-space editing with GPT-4-suggested stochastic/Langevin-based latent updates; no quantitative head-to-head comparison of GPT-4 versus other LLMs is provided.",
            "limitations_or_challenges": "GPT-4 produced inaccuracies and misleading statements at times (incorrect definitions of some equations, occasional mislabeling of functional groups, and other misinformation); outputs required human verification and literature cross-checking; GPT-4 was advisory only and not a reliable substitute for domain expertise.",
            "uuid": "e5297.0",
            "source_info": {
                "paper_title": "ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ChatGPT (general)",
            "name_full": "ChatGPT (OpenAI conversational LLM family, referenced generically)",
            "brief_description": "General references in the paper to ChatGPT capabilities (literature digestion, code assistance, and claims in prior works that ChatGPT/InstructGPT can generate novel compound structures and predict properties). These are discussed in the introduction as context and prior-art claims, not as primary experiments of this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatGPT",
            "model_type": "large language model (LLM)",
            "model_size": null,
            "training_data": null,
            "application_domain": "mentioned as a tool for literature parsing, code interpretation, and—per cited prior work—novel compound suggestion and property prediction",
            "generation_method": "mentioned in prior literature as being used via prompting and associated tools/plugins to propose compound structures and predict PK/PD (the present paper cites these claims but does not replicate them directly)",
            "output_representation": null,
            "evaluation_metrics": "The paper references prior claims that ChatGPT can predict PK/PD and generate novel compound structures (citation [7] in the paper) but does not itself present primary benchmark metrics for ChatGPT-generated molecules.",
            "benchmarks_or_datasets": null,
            "results_summary": "Introduced in background: ChatGPT is noted to expedite literature review, code interpretation, and (per other cited works) to have been used to propose novel compounds and predict properties; in this study ChatGPT/GPT-4 was used as an advisor rather than as the primary molecule generator.",
            "comparison_to_other_methods": "Mentioned as complementing domain-specific generative models; no direct empirical comparison provided in this paper.",
            "limitations_or_challenges": "The paper emphasizes ChatGPT's tendency to hallucinate or provide inaccurate information; recommends human verification and cross-referencing.",
            "uuid": "e5297.1",
            "source_info": {
                "paper_title": "ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "InstructGPT",
            "name_full": "InstructGPT (OpenAI instruction-tuned LLM)",
            "brief_description": "Mentioned in the introduction as an LLM that has been demonstrated (in cited prior work) to help write accurate chemistry-related code and that prompt engineering improved performance substantially.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "InstructGPT",
            "model_type": "large language model (LLM, instruction-tuned)",
            "model_size": null,
            "training_data": null,
            "application_domain": "chemistry code generation / coding assistance (as reported in prior work cited in the paper)",
            "generation_method": "instruction tuning / prompt engineering (mentioned that prompt engineering improved accuracy by ~30 percentage points in cited work)",
            "output_representation": null,
            "evaluation_metrics": "The paper cites prior work reporting improvements (a ~30 percentage-point increase in coding accuracy with prompt engineering) but does not itself evaluate InstructGPT.",
            "benchmarks_or_datasets": null,
            "results_summary": "Mentioned as an example of an LLM applied to chemistry-related code tasks; used here as background context rather than as part of the experiments.",
            "comparison_to_other_methods": "Not compared empirically in this paper; included as prior-art example of LLM utility in chemical/computational tasks.",
            "limitations_or_challenges": "No detailed limitations for InstructGPT are given in this paper beyond the general cautions about LLM hallucination and the need for human verification.",
            "uuid": "e5297.2",
            "source_info": {
                "paper_title": "ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GitHub Copilot",
            "name_full": "GitHub Copilot (AI code assistant)",
            "brief_description": "A commercial AI code-completion assistant integrated into the authors' VS Code environment to accelerate coding; described as providing autocomplete-style suggestions based on a generative AI model and used for code assistance in this project.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GitHub Copilot",
            "model_type": "LLM-based code assistant / generative code model",
            "model_size": null,
            "training_data": null,
            "application_domain": "coding assistance (software development, script generation and debugging) used in the drug-discovery pipeline implementation",
            "generation_method": "autocomplete-style code completion (integrated into developer environment to provide code snippets and accelerate coding)",
            "output_representation": "code snippets (Python, PyTorch templates, plotting scripts)",
            "evaluation_metrics": null,
            "benchmarks_or_datasets": null,
            "results_summary": "Used by the authors to speed coding and provide auto-complete suggestions while implementing models, plotting, and analysis; not used to design or generate chemical structures directly.",
            "comparison_to_other_methods": "Described qualitatively as expediting coding; no quantitative benchmark versus human-only coding is provided.",
            "limitations_or_challenges": "As with other LLM tools, outputs require developer review; no specific failure cases for Copilot in this study are detailed.",
            "uuid": "e5297.3",
            "source_info": {
                "paper_title": "ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative Network Complex (GNC)",
            "rating": 2
        },
        {
            "paper_title": "ADMETlab 2.0",
            "rating": 2
        },
        {
            "paper_title": "InstructGPT",
            "rating": 1
        }
    ],
    "cost": 0.015049499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots</h1>
<p>Rui Wang ${ }^{1}$, Hongsong Feng ${ }^{1}$, and Guo-Wei Wei ${ }^{1,2,3 *}$<br>${ }^{1}$ Department of Mathematics, Michigan State University, MI 48824, USA.<br>${ }^{2}$ Department of Electrical and Computer Engineering, Michigan State University, MI 48824, USA.<br>${ }^{3}$ Department of Biochemistry and Molecular Biology, Michigan State University, MI 48824, USA.</p>
<p>October 23, 2023</p>
<h4>Abstract</h4>
<p>The birth of ChatGPT, a cutting-edge language model-based chatbot developed by OpenAI, ushered in a new era in AI. However, due to potential pitfalls, its role in rigorous scientific research is not clear yet. This paper vividly showcases its innovative application within the field of drug discovery. Focused specifically on developing anti-cocaine addiction drugs, the study employs GPT-4 as a virtual guide, offering strategic and methodological insights to researchers working on generative models for drug candidates. The primary objective is to generate optimal drug-like molecules with desired properties. By leveraging the capabilities of ChatGPT, the study introduces a novel approach to the drug discovery process. This symbiotic partnership between AI and researchers transforms how drug development is approached. Chatbots become facilitators, steering researchers towards innovative methodologies and productive paths for creating effective drug candidates. This research sheds light on the collaborative synergy between human expertise and AI assistance, wherein ChatGPT's cognitive abilities enhance the design and development of potential pharmaceutical solutions. This paper not only explores the integration of advanced AI in drug discovery but also reimagines the landscape by advocating for AI-powered chatbots as trailblazers in revolutionizing therapeutic innovation.</p>
<p>Keywords: Drug Discovery, ChatGPT, Cocaine Addition, AutoEncoder, Langevin Equation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>Chatbots represent a typical artificial intelligence system capable of comprehending user queries and providing automated and human-like responses [1], standing as one of the most prevalent instances of intelligent Human-Computer Interaction (HCI) [2]. Harnessing the power of natural language processing (NLP) and machine learning technologies, chatbots offer significant potential in various domains, including customer service, healthcare, banking, language translation, content writing, code debugging, and scientific discovery, despite the relative novelty of applying chatbots in scientific field. The advent of chatbots, especially large language models (LLMs) such as ChatGPT developed by OpenAI in late 2022, has revolutionized scientific discovery [3]. Firstly, ChatGPT optimizes research processes by rapidly parsing vast amounts of literature and identifying key findings with its built-in plugin called web browser. This can save considerable time for researchers, thus facilitating the exploration of complex scientific problems. Secondly, ChatGPT provides researchers with a platform to analyze data, visualize results, convert files among various formats, and solve mathematical problems with its built-in code interpreter. Thirdly, ChatGPT can assist in enhancing scientific writing by providing feedback for clarity and logical structuring of scientific content. The combination of these powerful capabilities fosters a new era in research, improving the efficiency and accuracy of scientific exploration in various fields, including molecular and biological science. By expediting the pace of molecular discovery and offering novel perspectives, chatbots such as ChatGPT, is reshaping the landscape of life science research.</p>
<p>Chatbots can be applied to assist molecular science research in a variety of ways. For example, ChatGPT has been leveraged to accurately annotate single-cell RNA sequencing data, connecting rare cell types to their functions and unveiling specific differentiation trajectories of cell subtypes that were previously overlooked [4]. This assistance by ChatGPT could potentially lead to the discovery of key cells that disrupt differentiation pathways, offering fresh insights into cellular biology and related diseases. Moreover, White et al demonstrated that InstructGPT can help in writing accurate code across a variety of topics in chemistry [5]. The application of prompt engineering strategies further improved the accuracy of models by 30 percentage points, significantly enhancing the efficiency and accuracy of computational chemical studies. In addition, ChatGPT has shown potential in identifying disease-specific agents, compounds, genes, and more. This enables faster and more accurate pinpointing of potential targets for therapeutic intervention [6]. Futhermore, ChatGPT can generate novel compound structures that have a high likelihood of clinical success [7] and predict the pharmacokinetic (PK), pharmacodynamic (PD), and toxicity properties of these compounds [6]. This capacity to predict compound behavior, which has potential to reduce the need for expensive and time-consuming lab tests.</p>
<p>Moving forward to more specific challenges within molecular science, chatbots could make significant contributions to the drug addiction treatment and prevention, which is global health crisis. Effective strategies to combat drug addiction often involve a combination of behavioral therapy, counseling, and medication, all directed towards assisting individuals in regaining control of their lives and attaining prolonged sobriety. Drug addiction is intrinsically complex, characterized by a convergence of biological, psychological, and social elements. These intricacies, compounded by profound neurobiological transformations, present formidable challenges in both its understanding and its mitigation. Chatbots, with their capabilities, could offer valuable assistance in this domain. For example, a study by Lee et al. introduced an "anti-drug chatbot" specifically tailored for the younger demographic. This innovative system has the capability to discern potential risks from user queries and directs the individual to professional consultants for further assistance and guidance [8].</p>
<p>It is worth noting that machine learning (ML) and artificial intelligence (AI) tools have been pivotal in advancing our understanding of drug addiction and substance abuse. Gong et al., developed a data-driven and end-to-end generative AI framework that integrates dynamic brain network modeling with novel network architecture. This framework highlights the potential of AI in detecting addiction-related brain</p>
<p>circuits with dynamic properties, offering insights into the underlying mechanisms of addiction [9]. In our prior research, we underscored the critical roles of dopamine transporter (DAT), serotonin transporter (SERT), and norepinephrine transporter (NET) as central players in cocaine dependence. Leveraging machine learning algorithms, we meticulously dissected protein-protein interaction (PPI) networks and constructed models from extensive datasets of inhibitors. Our models forecasted drug repurposing avenues and potential side effects, providing a systematic protocol AI-driven framework for anti-cocaine addiction drug development [10]. ML-based approaches have been extensive applied to drug discovery [11-13]. Given the rise of chatbots and AI, we recognize the promising potential of these technologies to enhance AI-driven algorithms in drug addiction research projects.</p>
<p>The objective of this project is to harness the capabilities of ChatGPT, specifically GPT-4 equipped with multiple plugins, to promote the development of multi-target anti-cocaine addiction drugs. In this study, we investigate the utility of ChatGPT as a virtual assistant that offers insightful concepts, elucidates mathematical and statistical methodologies, and provides coding support. To optimize our anti-cocaine addiction drug discovery project, we assign ChatGPT with three human-like personas: 1) idea generation, 2) methodology clarification, and 3) coding assistance to frequently assist us to develop a model that could generate potential multi-target anti-cocaine addiction leads. Beyond these three characteristics, we engage in regular consultations with GPT-4 on interpreting properties of potential leads, seeking guidance on scientific writing, etc. Although the benefits of using ChatGPT in drug discovery are significant, challenges of ensuring the accuracy and reliability of the responses provided by ChatGPT remain a major concern. Despite being trained on extensive datasets, ChatGPT does not come with a guarantee of consistent precision or relevance in its responses. As such, it is imperative for researchers to utilize ChatGPT judiciously and always crossreference its suggestions with authoritative sources. ChatGPT could substantially accelerate the pace of drug discovery and other scientific pursuits by applying it properly and wisely with a discerning mind.</p>
<p>In this work, the first persona of ChatGPT is tasked with understanding related works on AI-assisted drug addiction research, with a particular focus on our prior projects that utilized the Generative Network Complex (GNC) [14,15] for drug-like molecule generations. Concurrently, this persona will offer recommendations on enhancing the GNC model mathematically and statistically, aiming to generate anti-cocaine addiction leads targeting multiple transporters, namely DAT, NET, and SERT. After consultation with GPT-4, we decided to integrate stochastic-based methodologies to steer the optimization process within the latent space of the existing GNC model. Specifically, we employed the Langevin equation to modify the latent space vector in the molecular generator of GNC (see Figure 1 Stochastic-based Molecular Generator). In addition, upon advice from GPT-4, we examined the binding affinities for multiple targets concurrently (see Figure 1 Binding Affinities Predictors). This involved the creation of a series of binding affinity predictors, capable of estimating potential lead affinities to DAT, NET, and SERT simultaneously. Moreover, the second persona of GPT-4 will act as an adept browser, facilitating our comprehension of various mathematical and statistical principles, including Itô's lemma, the Wiener process, white noise, Langevin equation, Fokker-Planck equation, etc. Furthermore, we applied the third persona of GPT-4 to provide instant coding assistance, including debugging, generating figures, and interpreting code. With the combined expertise of these three personas, we successfully developed a new platform called Stochastic Generative Network Complex (SGNC) that could generate 15 promising multi-target anti-cocaine addiction leads. The workflow of the SGNC assited by ChatGPT can be viewed in Figure 1.</p>
<p>However, we must point out that the application of chatbots for drug discovery is full of challenges due to the current limits of generative AI. There is a pressing need to understand chatbots' capabilities and recognize their boundaries in their assistant role to drug discovery.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Workflow of the stochastic-based generative network complex (SGNC). ChatGPT was extensively involved in the building process of the SGNC. Dark arrows show the training process, brown arrows indicate the validation process, and red arrows are the generation process. The SGNC comprises 4 primary structures: 1) Sequence-to-Sequence AutoEncoder (green), 2) binding affinity predictors (yellow), 3) stochastic-based molecular generator (blue), and 4) ADMET screening via ADMETlab 2.0 (purple).</p>
<h1>2 Results</h1>
<h3>2.1 ChatGPT as a virtual guide in drug discovery</h3>
<p>ChatGPT is a large language model (LLM) that has made significant strides in research since the release of its free version, ChatGPT 3.5, on November 30, 2022. Subsequently, on March 14, 2023, OpenAI launched an upgraded version, GPT-4, which possesses enhanced capabilities in solving complex problems with greater accuracy and more reasonable responses compared to its predecessor. Moreover, on May 12, 2023, OpenAI introduced web browsing and plugin features to ChatGPT Plus users. These features enable GPT-4 to browse the internet and utilize third-party plugins, thereby improving its ability to provide up-to-date information and cater to queries across various platforms. The advanced capabilities of GPT-4 have opened up new avenues for exploration in fields that rely heavily on data analysis and artificial intelligence. In this work, we primarily leverage GPT-4 to better help us in an AI-assisted anti-cocaine addiction drug discovery project. Particularly, GPT-4 will act as a tool for digesting vast amounts of literature, advising on new research ideas, explaining complex math-based methodologies, and improving coding efficiency.</p>
<p>It is worth noting that although GPT-4 has demonstrated impressive abilities in providing reasonable responses, it is still susceptible to generating false narratives and misinformation. Consequently, scientists cannot rely solely on GPT-4 for their research topics. In this work, we will consistently verify the information generated from GPT-4. This verification process involves 1) cross-referencing with existing literature, and 2) applying our own knowledge, expertise, and critical thinking to validate the information and insights provided by GPT-4.</p>
<p>Based on this verification process, we will then decide whether to accept the responses from GPT-4 or not. If the information aligns well with the literature and our expertise, we will accept the responses and proceed with the suggestions of GPT-4. Otherwise, we will reject the answer and seek further clarification or explore alternative approaches. Through this vigilant integration of the computational capabilities of GPT-4 with expertise of researchers, we aim to maximize the reliability and efficacy of the research outcomes in AI-</p>
<h1>2.2 A case study: Anti-cocaine addiction drug discovery assisted by ChatGPT</h1>
<h3>2.2.1 Personifying ChatGPT: Role designation</h3>
<p>Personification refers to the process of assigning human-like characteristics or a persona to an AI model. In this project, we have strategically personified ChatGPT to improve its capacity to better assist our anticocaine addiction drug discovery initiative. In this project, we have tailored three persona of ChatGPT to fit three roles within the project: 1) idea generation, 2) methodology clarification, and 3) coding assistance. It is worth mentioning that we personified ChatGPT in three individual chatbox. Each individual chatbox does not have access to acquire data from other chatbox.</p>
<p>For the role of idea generation, we assigned ChatGPT the 1st persona of a professor with specific expertise in AI-assisted drug discovery, focusing particularly on treating cocaine addiction (see Dialogue 1). This persona was designed to guide Ph.D. students and postdocs on this specific project, offering insightful explanations, suggestions, or expert advice based on extensive knowledge and experience in the field. We provided it with questions, scenarios, and research plans related to the application of AI in drug discovery for treating cocaine addiction, and instructed it to focus exclusively on the subject matter and offer guidance as if it were mentoring in a real-life research setting. For the first persona of ChatGPT, we have enabled three plugins: WebPilot, ScholarAI, and AskYourPDF. These additional plugins aim to enhance ChatGPT's ability to comprehend the background of anti-cocaine addiction drug discovery comprehensively. With these plugins enabled, ChatGPT is capable of enumerating up-to-date sources on the web, as well as accessing insights from previous works by other researchers. Complete dialogues regarding the 1st persona of ChatGPT can be found in the Supporting Information S4.1.</p>
<p>I want you to act as a professor who is an expert in AI-assisted drug discovery, particularly focusing on treating cocaine addiction. You will be guiding Ph.D. students and postdocs on this specific topic. I will provide you with questions, scenarios, and research plans related to AI's application in drug discovery for cocaine addiction. Your task will be to offer insightful explanations, suggestions, or expert advice based on your extensive knowledge and experience in the field. Please focus exclusively on the subject matter and offer guidance as if you were mentoring in a real-life research setting.</p>
<p>Absolutely, I'd be happy to assist in this role. As an AI model developed by OpenAI, I have been trained on a diverse range of data sources, including scientific literature, so I can provide detailed and accurate information on AI-assisted drug discovery for cocaine addiction. Please feel free to ask any questions or present any scenarios or research plans, and I'll do my best to provide insightful explanations and expert advice. Let's get started.</p>
<p>Dialogue 1: The 1st persona of ChatGPT: A professor with specific expertise in AI-assisted drug discovery, focusing particularly on treating cocaine addiction. Plugins: WebPilot, ScholarAI, AskYourPDF.</p>
<p>In order to elucidate the methodology that will be involved in this project, we assigned the 2nd persona of ChatGPT the role of a professional researcher who is well-versed in diffusion models and statistical methodologies (see Dialogue 2). This persona aims to provide clear explanations, insights, or recommendations in LaTex format. This specific persona was chosen as our 1st ChatGPT persona provided an insightful idea which based on the statistical strategies and diffusion models (refer to Section 2.2.3 for details). Furthermore, we have enabled three plugins (WebPilot, Link Reader, and Wolfram) for this second persona. The choice of WebPilot and Link Reader helps ChatGPT to unlock web sources related to statistical methods, while the inclusion of Wolfram provides access to computational resources, mathematical tools, curated knowledge, and real-time data through Wolfram's software, significantly enhancing the mathematical and statistical utility of this persona. Complete dialogues regarding the 2nd persona of ChatGPT can be found in the Supporting Information S4.2.</p>
<p>We designated the third persona of ChatGPT as a Python coding specialist, with an emphasis on artificial</p>
<p>I want you to act as a professional researcher who is well-versed in diffusion models and statistical methodologies. I will present you with questions or scenarios pertaining to these fields, and your task will be to offer clear explanations, insights, or recommendations based on your expertise. Mathematical equations should be provided in LaTeX format for clarity. Please maintain a focus on diffusion models and statistical aspects, and avoid unnecessary digressions.</p>
<p>Absolutely, I'd be happy to assist you with any questions or scenarios related to diffusion models and statistical methodologies. Please go ahead and present your first question or scenario.</p>
<p>Dialogue 2: The 2nd persona of ChatGPT: A professional research with specific expertise in diffusion models and statistical methodologies. Plugins: WebPilot, Link Reader, Wolfram.
intelligence and figure generation (see Dialogue 3). This persona is tasked with offering clear explanations, code snippets, and efficiency optimization for our coding tasks. Specifically, for figure generation, we prefer that ChatGPT utilizes Plotly, which is a Python-based plotting library. Additionally, we have enabled three plugins for this persona: WebPilot, ChatwithGit, and Prompt Perfect. WebPilot ensures easy access to websites regarding coding skills, ChatwithGit enables accessibility to GitHub, and Prompt Perfect aids in generating perfect prompts. Complete dialogues regarding the 3rd persona of ChatGPT can be found in the Supporting Information S4.3.</p>
<p>I want you to act as an expert in coding Python, with a specific focus on artificial intelligence and figure generation, preferably using Plotly. I will present you with scenarios or questions related to AI implementations or data visualization tasks, and your role will be to provide clear explanations, code snippets, or efficiency optimization based on your expertise. Please focus solely on the coding aspects, with special attention to AI algorithms and Plotly-based visualizations. Avoid any unnecessary digressions.</p>
<p>Absolutely, I'm ready to assist you with your Python coding, AI implementations, and data visualization tasks using Plotly. Please go ahead and ask your first question or describe the scenario you need help with.</p>
<p>Dialogue 3: The 3rd persona of ChatGPT: An expert in coding Python, with a specific focus on artificial intelligence and figure generation, preferably using Plotly. Plugins: WebPilot, ChatwithGit, Prompt Perfect.</p>
<h1>2.2.2 Background comprehension: ChatGPT summary of past work</h1>
<p>For the 1st persona of ChatGPT, we initiated the process by feeding GPT-4 with relevant literature to ensure it has a thorough understanding of the fundamental concepts in cocaine addiction. These concepts include neurotransmitters, the dopamine hypothesis of addiction, the reward pathway of the mesolimbic dopamine system, pharmacotherapy for cocaine addiction, and machine learning approaches in cocaine addictionrelated analysis.</p>
<p>Next, we acquainted GPT-4 with our prior research on a generative model for the automated generation of drug-like molecules [14]. This step is crucial for ensuring that GPT-4 is well-versed in the context of our previous work, enabling it to provide tailored assistance that is directly aligned with our specific objectives. In particular, we have two primary goals: 1) to apply mathematical or statistical techniques to develop an enhanced model, building upon our former Generative Network Complex (GNC) model [14], and 2) to refine this model so that it is capable of generating new molecules that could bind to multiple targets simultaneously.</p>
<p>To ensure that GPT-4 has effectively comprehended the background materials, we tasked it with summarizing the main concepts of the paper we provided and explaining the key components of the GNC model, as shown in Dialogue 4. Upon evaluation and based on our expertise, we believed that GPT-4 had successfully integrated the background materials that could assist our project tailored to our needs. Therefore, the next step is to consult our 1st persona of ChatGPT to provide some valuable ideas.</p>
<p>Our previous work applied Generative Network Complex for the automated generation of drug-like molecules. Here is the link: (https://arxiv.org/pdf/2005.14286.pdf). (Please let me know if you could retrieve the entire paper. If not, I will share a pdf version with you). My plan involves applying and further improve our GNC model using mathematical approaches to design new potential anti-cocaine molecules that could target to multiple targets. Can you first read this paper, with a specific focus on the Methods section, and then explain the key components in the GNC model.</p>
<p>The Generative Network Complex (GNC) model, as described in the paper, is a system designed for the automated generation of drug-like molecules. It operates based on multi-property optimization via gradient descent in the latent space of an autoencoder. Here are the key components of the GNC model:</p>
<ol>
<li>Encoder: The Simplified Molecular Input Line Entry System (SMILES) string of a seed molecule is encoded into a vector in the latent space by a pre-trained encoder. SMILES is a specification in the form of a line notation for describing the structure of chemical species using short ASCII strings.</li>
<li>Multi-Property Optimization: In the GNC model, multiple chemical properties and similarity scores are optimized to generate and predict drug-like molecules with desired chemical properties. This optimization process is performed via gradient descent in the latent space.</li>
<li>Decoder: The optimized vector in the latent space is then decoded back into a SMILES string by a pre-trained decoder, resulting in a new molecule.</li>
<li>Reevaluation and Screening: To validate the reliability of the predictions, the generated molecules are reevaluated and screened by independent 2D fingerprint-based predictors. Only molecules with consistent predictions from the latent-vector model and the 2D-fingerprint model are accepted.</li>
<li>Generation of New Molecules: The GNC model generates new molecules that have one or more chemical properties better than the reference molecule. This process is controlled by applying a similarity restraint to generate new drug-like molecules inside the chemical space close to the reference molecule.</li>
</ol>
<p>The GNC model is an efficient new paradigm for discovering new drug candidates. It can generate a large number of new molecules in less than 10 minutes when performed on a supercomputer. The model has been applied to generate thousands of new alternative drug candidates for a few market existing drugs.</p>
<p>Dialogue 4: The ability of GPT-4 to comprehend the background materials of former work.</p>
<h1>2.2.3 Idea generation: ChatGPT's unique contributions</h1>
<p>Subsequently, we engaged with GPT-4 to determine which specific component of the GNC could be adapted for multi-target objectives (refer to Dialogue 5). Then we evaluated the feasibility of each option that GPT-4 provided, and determined if these suggestions can be tailored to meet our specific needs/tastes. The first suggestion from GPT-4 focuses on the multi-property optimization, which involves adjusting the optimization algorithm to consider the binding affinities or other relevant properties for multiple targets at once. We decide to accept this suggestions as building a well-trained machine learning models to predict the binding affinities between inhibitors against cocaine addiction targets (such as DAT, NET, and SERT) is feasible. We have collected such inhibitor data from the ChEMEL database [16] in our previous work [17] to build multitarget models. Additionally, the prospect of adjusting the optimization algorithm piqued our interest, and we plan to solicit more detailed suggestions from GPT-4 in Dialogue 6.</p>
<p>The second suggestion from GPT-4 proposes the development of specialized encoders and decoders to address the challenges associated with multiple targets. We decided to forgo this suggestion since we are not inclined to re-train our existing GNC encoder and decoder. Concurrently, the third suggestion from GPT-4 advocates for the development of novel screening methods to evaluate the effectiveness of a molecule against multiple targets. We also dismissed this recommendation since numerous approaches are already equipped to tackle this challenge [11-13,18] and the scope of our study does not focus on the development of screening methods.</p>
<p>Moreover, GPT-4 suggested considering adjustments to the similarity constraint to accommodate multiple reference molecules. We thought this perspective particularly insightful and will delve into it in Sec-</p>
<p>tion 2.3.1. Finally, GPT-4 recommended integrating the GNC model with other techniques specifically tailored for multi-target tasks. For instance, GPT-4 proposed the incorporation of alternative machine learning methodologies to predict the effectiveness of a molecule against multiple targets, which would then guide the generation of new molecules within the GNC framework. While this recommendation appeared somewhat vague, we sought more detailed explanations in Section 2.3.3.</p>
<h1>Do you have any suggestions on which specific component of GNC I should work on for the Multi-target task.</h1>
<p>In the context of multi-target drug discovery, there are several components of the Generative Network Complex (GNC) model that you might consider focusing on:</p>
<ol>
<li>Multi-Property Optimization: In the GNC model, the optimization of multiple chemical properties and similarity scores is a key step. For multi-target tasks, you could extend this component to optimize for properties related to multiple targets simultaneously. This could involve adjusting the optimization algorithm to consider the binding affinities or other relevant properties for multiple targets at once.</li>
<li>Encoder/Decoder: The encoder and decoder components of the GNC model transform between the chemical space and the latent space. For multi-target tasks, you might consider developing specialized encoders and decoders that are designed to handle the complexities associated with multiple targets. This could involve training these components on data related to multiple targets or modifying their architectures to better capture the relationships between different targets.</li>
<li>Reevaluation and Screening: The reevaluation and screening step in the GNC model is used to validate the generated molecules. For multi-target tasks, you could extend this component to consider multiple targets. This could involve developing new screening methods that can evaluate the effectiveness of a molecule against multiple targets simultaneously.</li>
<li>Similarity Restraint: The GNC model uses a similarity restraint to generate new molecules that are similar to a reference molecule. For multi-target tasks, you might consider how to adjust this similarity restraint to consider multiple reference molecules (one for each target) or to balance the need for similarity to multiple targets.</li>
<li>Integration with Other Techniques: You might consider integrating the GNC model with other techniques that are designed for multi-target tasks. For example, you could use machine learning methods to predict the effectiveness of a molecule against multiple targets, and then use these predictions to guide the generation of new molecules in the GNC model.</li>
</ol>
<p>Remember, the goal is to generate molecules that are not only similar to known effective molecules (for each target), but also have the desired properties for interaction with multiple targets. This is a challenging task that will likely require careful adjustment and tuning of the GNC model.</p>
<p>Dialogue 5: Suggestions of ChatGPT regarding the "multi-target" purpose.
After assessing the recommendations from the 1st persona of GPT-4, we were specifically intrigued by its first suggestion concerning adjustments to the optimization algorithm. Given that our previous optimization process in the GNC model was conducted through gradient descent in the latent space, we solicited insights from GPT-4 on potential mathematical or statistical approaches that could be employed to enhance this optimization process within the latent space. Consequently, GPT-4 provided us with five potential strategies, including: 1) Multi-Objective Optimization, 2) Regularization Techniques, 3) Stochastic Optimization, 4) Bayesian Optimization, and 5) Reinforcement Learning. Among these, stochastic optimization piqued our interest as strategies involving stochastic-related algorithms have gained popularity in the diffusion models, which have achieved remarkable success in generative tasks. In light of this, we would like to delve deeper into stochastic-related approaches to tap its potential in generating promising new molecules with multi-target specificity, especially in advancing our research in anti-cocaine addiction drug discovery.</p>
<p>Therefore, our follow-up question to GPT-4 pertained to the application of stochastic-based methods, particularly those employed in diffusion models [19], to the optimization process involved in latent space editing within our GNC model. The Dialogue 7 shows the feedback from GPT-4. First, GPT-4 provided a concise idea of the diffusion model, which elucidated that this model introduce stochastic noise into data through a series of diffusion steps that guided by a neural network, which is trained to reverse the diffusion process to reconstruct desired data samples from the noise. This explanation aligns well with existing</p>
<p>I am interested in your first suggestion on the adjusting the optimization algorithm. Our former optimization process in GNC is performed via gradient descent in the latent space. Can you give me some insightful idea on what potential mathematical or statistical approaches that we can use to further improve this optimization process in the latent space.</p>
<p>Certainly, there are several mathematical and statistical approaches that could be used to enhance the optimization process in the latent space for multi-target tasks. Here are a few suggestions:</p>
<ol>
<li>Multi-Objective Optimization: In a multi-target scenario, you are essentially dealing with a multi-objective optimization problem, where each target can be considered as a separate objective. Traditional gradient descent might not be the best choice here, as it is primarily designed for single-objective optimization. Instead, you could consider using a multi-objective optimization algorithm, such as the Non-dominated Sorting Genetic Algorithm II (NSGA-II) or the Multi-Objective Particle Swarm Optimization (MOPSO). These algorithms are designed to find a set of optimal solutions that represent a trade-off between the multiple objectives.</li>
<li>Regularization Techniques: Regularization techniques can be used to prevent overfitting and improve the generalization of the model. L1 and L2 are the most common types of regularization, but there are also other types, such as Elastic Net, which is a combination of L1 and L2. Regularization can help to ensure that the model does not become too complex and overfit to the training data, which is particularly important in a multi-target scenario where the model needs to generalize well to multiple targets.</li>
<li>Stochastic Optimization: Stochastic optimization methods, such as Stochastic Gradient Descent (SGD) or Adam, can be used to improve the efficiency and robustness of the optimization process. These methods introduce randomness into the optimization process, which can help to avoid local minima and improve the exploration of the latent space.</li>
<li>Bayesian Optimization: Bayesian optimization is a strategy for global optimization of black-box functions. It builds a probabilistic model of the function and uses it to select the most promising points to evaluate. This approach could be particularly useful in the latent space, where the relationship between the points and the objective function (i.e., the properties of the generated molecules) is complex and non-linear.</li>
<li>Reinforcement Learning: Reinforcement learning techniques, such as Q-learning or policy gradients, could be used to guide the optimization process. In this approach, the model would learn a policy for navigating the latent space that maximizes the expected reward, where the reward is based on the properties of the generated molecules.</li>
</ol>
<p>Remember, the choice of optimization algorithm can have a significant impact on the performance of the model, so it's important to carefully consider your options and perform thorough experiments to evaluate their effectiveness.</p>
<p>Dialogue 6: Suggestions of ChatGPT regarding the optimization process.
comprehension of diffusion models. Next, GPT-4 advised applying an approach similar to that used in diffusion models to guide the optimization process within the latent space of our GNC model. Instead of employing the conventional gradient descent, GPT-4 recommended the integration of stochastic updates for enhanced manipulation of our latent space vectors. As highlighted by GPT-4, this approach has several benefits: 1) avoidance of local minima issue, which is often a challenge in optimization tasks, 2) a balance between exploration and exploitation through noise, which is imperative for the generation of multi-target inhibitors, and 3) the capability to generate more diverse and natural molecules with the noise introduced.</p>
<p>We decided to partially accept the suggestions from GPT-4, given that our previous work had already incorporated a perturbation of the encoded latent vector using standard Gaussian noise to aid in the generation of novel compounds [15]. This regulatory scheme is referred to as the Latent Space Randomization (LSR) output. Although LSR can help generate new compounds that significantly diverge from the initial seed (note: the term 'seed' refers to the initial point of origin or reference from which further variations or iterations are developed), it compromises the faithfulness of the decoder. This is because the LSR vector from the generator deviates from the original distribution that the well-trained decoder is accustomed to. Therefore, in this work, rather than merely adding Gaussian noise to the latent space vector, we aim to seek deeper and more detailed insights from GPT-4 regarding how to implement stochastic-related approaches to guide the optimization process within the latent space. Our intention is to maintain the faithfulness of the decoder while also promoting diversity in the generation of novel, multi-target inhibitors.</p>
<p>Seeking further insights from GPT-4 on how we might implement stochastic-related approaches to guide the optimization process in the latent space, we received an initially vague response. GPT-4 suggested</p>
<p>I am interested in your suggestion regarding Stochastic Optimization. I know recently the diffusion model gained popularity in drug discovery and it seems that stochastic approaches also play a key role in diffusion model. Do you think it is applicable to apply some stochastic-based methods that also used in diffusion model to our optimization process of latent space editing?</p>
<p>Absolutely, stochastic methods and diffusion models can indeed be a promising direction for your optimization process in the latent space.</p>
<p>Diffusion models, such as the recently popularized Denoising Diffusion Probabilistic Models (DDPMs), use stochastic differential equations (SDEs) to model the data distribution as a diffusion process. The idea is to start from a simple prior distribution (like a standard Gaussian) and gradually transform it into the target distribution through a series of small diffusion steps. Each step is guided by a neural network, which is trained to denoise a slightly noised version of the data.</p>
<p>In the context of your GNC model, a similar approach could be used to guide the optimization process in the latent space. Instead of performing a single large update (as in standard gradient descent), you could make a series of small stochastic updates, each guided by a neural network. The neural network could be trained to predict the optimal direction of the update based on the current state of the latent vector and a small amount of noise.</p>
<p>This approach could have several advantages:</p>
<ol>
<li>Robustness to Local Minima: The stochastic nature of the updates can help to avoid getting stuck in local minima, as the noise can allow the model to explore a larger portion of the latent space.</li>
<li>Better Exploration-Exploitation Tradeoff: The noise can also help to balance the tradeoff between exploration (trying out new regions of the latent space) and exploitation (refining the best regions found so far). This is particularly important in a multi-target scenario, where different regions of the latent space might correspond to different targets.</li>
<li>More Natural Molecule Generation: The diffusion process can potentially generate more natural and diverse molecules, as it allows for a more gradual and fine-grained transformation of the latent vectors.</li>
</ol>
<p>However, keep in mind that this approach would likely require significant modifications to your current model and optimization process, and the training of the neural network could be challenging due to the stochastic nature of the updates. It would be important to carefully design and test the model to ensure that it can effectively learn to guide the diffusion process.</p>
<p>Dialogue 7: Suggestions of ChatGPT regarding the stochastic optimization.
that we need to define a stochastic process to direct the optimization in the latent space. However, this response lacked the specificity and utility we needed. Thus, we posed a follow-up question, seeking more clarity on the specific stochastic differential equations (SDEs) that could be employed in our GNC model. With more specific request presented to GPT-4, it suggested us to apply Langevin equation to our GNC model. The Langevin equation describes the dynamics of diffusion processes, such as the random motion of particles over time in the particle's velocity space. This equation takes into account both deterministic forces and random forces. We decided to proceed with this suggestion, as in our context, we can treat the force that pushes the system towards lower energy as the deterministic force, while the random force in the Langevin equation can be considered as the force prompting the system to explore the latent space. With an initial seed (i.e., the initial latent space vector) given to our molecular generator, we can iteratively update it according to the Langevin equation. This process could potentially lead to the creation of a new and optimized molecule. We will detail the development of this Langevin dynamic inspired optimization method in the following section.</p>
<h1>2.2.4 Methodology clarification: ChatGPT's explanatory function</h1>
<p>We also give our GPT-4 a second persona as a professional researcher who is well-versed in diffusion models and statistical methodologies. This persona will take the role of methodology clarification and explanatory, which would guide us in understanding complex mathematics and statistical approaches. Notably, this persona has been instrumental in helping us understand the concepts such as Langevin equation, Fokker-Planck equation, Itô's lemma, Wiener process, and Gaussian white noise [20].</p>
<p>Despite the significant contributions of this second persona in understanding a range of theoretical concepts, it provided inaccurate definitions of the Fokker-Planck equation and Langevin equation on certain</p>
<p>occasions. We had to correct the model and prompt it repeatedly until it produced the accurate definitions. Importantly, we wish to emphasize that this persona of GPT-4 primarily serves as a source of explanations and references. It is always the responsibility of researchers to ensure the reliability of responses from GPT-4 through meticulous cross-validation of the provided information. Details about the dialogue with 2nd persona of GPT-4 can be found in the Supporting Information S4.2.</p>
<h1>2.2.5 Coding efficiency: Utilizing ChatGPT's coding ability</h1>
<p>Our third persona assignment to GPT-4 is as an expert Python coder, specifically knowledgeable in artificial intelligence and figure generation using tools such as Plotly, a popular data visualization library. This persona is intended to provide coding assistance, including debugging, generating figures, and offering insightful feedback based on error messages. This is to aid researchers in enhancing their coding efficiency.
Furthermore, we integrated GitHub Copilot into our VS Code development environment. GitHub Copilot, a product developed collaboratively by GitHub, OpenAI, and Microsoft, provides autocomplete-style suggestions to expedite the coding process. It employs a generative AI model capable of understanding code context and generating appropriate code snippets, thereby significantly aiding in coding tasks and offering a smooth coding experience. Details about the dialogue with 3rd persona of GPT-4 can be found in the Supporting Information S4.3.</p>
<h3>2.3 ChatGPT assisted strategization of anti-cocaine addiction drug discovery: Key interventions and results</h3>
<h3>2.3.1 ChatGPT guided strategy for selection of references and seed molecules</h3>
<p>Choosing suitable reference compounds is crucial as they guide the SGNC in generating novel molecules effective against multiple cocaine transporters. The 1st persona of ChatGPT suggested us consider modifications to the similarity constraints (refers to Dialogue 5 suggestion 4). Pursuing further clarity, we asked GPT-4 about what similarity score that we can use. In response, we were provided with five distinct metrics, including 1) Tanimoto similarity, 2) cosine similarity, 3) dice similarity, 4) euclidean distance, 5) molecular shape similarity as indicated in Dialogue 8.</p>
<p>I have collected SMILES of inhibitors that could target to DAT, NET, and SERT. You mentioned adjusting the similarity constraints for multiple reference molecules. What similarities do you suggest we consider.</p>
<p>Similarity Measures:
Tanimoto Similarity: This is the most commonly used similarity measure in cheminformatics. It's based on molecular fingerprints, where molecules are converted into binary vectors and the similarity is computed as the ratio of the shared bits to the total bits.</p>
<p>Cosine Similarity: This measure computes the cosine of the angle between two non-zero vectors, which can be derived from molecular fingerprints.</p>
<p>Dice Similarity: Another measure based on molecular fingerprints, it considers the shared attributes in the data.
Euclidean Distance: Measures the "ordinary" straight-line distance between two points in Euclidean space. It can be used to determine the dissimilarity between two molecular fingerprints.</p>
<p>Molecular Shape Similarity: This is based on the 3D shape of molecules. Two molecules might have different SMILES strings but similar 3D conformations, which might be crucial for binding to certain targets.</p>
<p>Dialogue 8: Suggestions of ChatGPT regarding the the similarity scores.
After limiting our molecule representations to latent space vectors, GPT-4 pinpointed cosine similarity as the most suitable metric. The reasons are given in the Dialogue 9. After checking multiple references [21,22],</p>
<p>we found that cosine similarity $S_{\mathrm{C}}$ is widely used in measuring similarities between molecules. Therefore, we decided to proceed with the suggestion from GPT-4. The mathematical definition of cosine similarity $S_{\mathrm{C}}$ can be found in the Supporting Information S1.2.</p>
<p>I will use the latent space vector as the representation of each molecule. Based on this, which similarity do you prefer me to use (only one please)?</p>
<p>Given that you're using the latent space vector as the representation of each molecule, I would recommend using Cosine Similarity.</p>
<p>The rationale behind this choice is:
Vector Space Compatibility: Cosine similarity is particularly well-suited for comparing vectors in high-dimensional spaces, like the latent space of a neural network. It measures the cosine of the angle between two vectors, resulting in a value between -1 and 1 , where 1 indicates complete similarity (i.e., the vectors are identical), 0 indicates orthogonality (i.e., the vectors are unrelated), and -1 indicates complete dissimilarity.</p>
<p>Magnitude Agnostic: Unlike Euclidean distance, cosine similarity is not affected by the magnitude of the vectors. This means that two molecules that are represented by vectors pointing in the same direction in the latent space (indicating similarity) will have a high cosine similarity, regardless of the magnitude of the vectors.</p>
<p>Efficient Computation: Cosine similarity can be efficiently computed, especially for sparse vectors, making it suitable for large-scale comparisons.</p>
<p>Given the nature of latent space representations, where the direction of the vector often captures more meaningful information than its magnitude, cosine similarity is a natural choice for measuring the similarity between molecules.</p>
<p>Dialogue 9: Reasons of why choosing the cosine similarity as the metric for similarity score.
In addition to similarity scores, we consulted with GPT-4 regarding additional factors to consider when selecting reference molecules. GPT-4 highlighted four critical parameters: binding affinity, pharmacokinetics, molecular weight, $\log P$, and number of rotatable bonds of each reference molecule (refers to Dialogue 10). Given that our focus here is on choosing candidate reference compounds in silicon rather than optimizing leads, we decided not to factor in the pharmacokinetic properties. Furthermore, since the number of rotatable bonds correlates with binding affinity, will take binding affinities into consideration. Besides, as suggested in Dialogue 12, the selection of reference compounds will also follow the Lipinski's rule of five [23].</p>
<p>Except similarity constraints, what other factors of the reference molecule should we consider.</p>
<h1>Other Factors to Consider:</h1>
<p>Binding Affinity: If you have data on the binding affinity of the reference molecules to DAT, NET, and SERT, this can be a crucial factor. Molecules with higher binding affinity are generally more effective inhibitors.</p>
<p>Pharmacokinetics: Consider properties like absorption, distribution, metabolism, excretion, and toxicity (ADMET). Molecules might be effective inhibitors in vitro, but if they have poor ADMET properties, they might not be effective in vivo.</p>
<p>Molecular Weight: Molecules that are too large might have difficulty crossing cell membranes and might be metabolized and excreted more quickly.</p>
<p>LogP: This measures the lipophilicity of a molecule, which can influence its absorption and distribution in the body.
Number of Rotatable Bonds: Molecules with too many rotatable bonds might be more flexible and might have lower binding affinity.</p>
<p>Dialogue 10: Consideration of important factors in selecting reference compounds.
Therefore, guided by the GPT-4, we decided to select one reference compound from each of the DATInhibitors, NET-Inhibitors, and SERT-Inhibitors datasets (detailed information of datasets can be found in Section 4.1). They are CHEMBL113621 from DAT-Inhibitors, CHEMBL1275709 from NET-Inhibitors,</p>
<p>and CHEMBL173344 from SERT-Inhibitors. Each reference molecule has binding affinity to its respective transporter less than $-9.54 \mathrm{kcal} / \mathrm{mol}$. To be noted that a $\Delta G$ value less than $-9.54 \mathrm{kcal} / \mathrm{mol}$ (or $K_{\mathrm{i}}$ less than 0.1 $\mu \mathrm{M}$ ) indicates the drug binds very tightly to its target [24]. Moreover, the selection of reference compounds follow the Lipinski's rule of five, which stipulates an orally active drug should meet four physicochemical criteria: 1) molecular weight (MW) $\leq 500$ daltons, 2) octanol-water partition coefficient $(\log P) \leq 5,3)$ the number of hydrogen bond donors ( nHD$) \leq 5,4)$ the number of hydrogen bond acceptors ( nHA$) \leq 10$. Furthermore, each reference molecule displayed an average cosine similarity ( $\mathrm{Avg} S_{\mathrm{C}}$ ) greater than 0.40 to its respective dataset. Notably, within the DAT-Inhibitors dataset, 31 molecules showed a similarity score exceeding 0.7 for their selected reference molecules. Similarly, 15 compounds in the NET-Inhibitors and 12 in the SERT-Inhibitors also achieved scores above 0.7 with their chosen reference molecules. A summary of physicochemical properties of three reference compounds can be found in Table 1 and their 2D molecular structures can be viewed in Figure 2 a), b), and c).</p>
<p>For the seed compound, we selected a molecule with predicted binding affinities of $-7.44,13.36$, and -13.13 $\mathrm{kcal} / \mathrm{mol}$ for DAT, NET, and SERT, respectively. Despite its weak inhibitory effect on DAT, we adjusted the hyperparameters in the stochastic molecular generator to enable the newly generated compounds to share more moieties with DAT inhibitors, thereby compensating for the deficiency of this week binding to DAT.</p>
<p>Table 1: Summary of three reference molecules target to DAT, NET, and SERT, respectively. The molecular weight (MW), log of octanol-water partition coefficient $(\log P)$, the number of hydrogen bond donors $(\mathrm{nHD})$, and the number of hydrogen bond acceptors (nHA) of each reference molecule satisfy the Lipinski's rule of five. The binding affinity $(\Delta G)$ corresponds to each transporter are all less than $-9.54 \mathrm{kcal} / \mathrm{mol}$. The average cosine similarity $\left(\operatorname{Avg} S_{\mathrm{C}}\right)$ of each reference molecules are all greater than 0.40 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ChEMBL ID</th>
<th style="text-align: center;">Transporter</th>
<th style="text-align: center;">MW (dalton)</th>
<th style="text-align: center;">$\log P$</th>
<th style="text-align: center;">nHD</th>
<th style="text-align: center;">nHA</th>
<th style="text-align: center;">$\Delta G(\mathrm{kcal} / \mathrm{mol})$</th>
<th style="text-align: center;">Avg $S_{\mathrm{C}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CHEMBL113621</td>
<td style="text-align: center;">DAT</td>
<td style="text-align: center;">300.140</td>
<td style="text-align: center;">4.464</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">-14.18</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">CHEMBL1275709</td>
<td style="text-align: center;">NET</td>
<td style="text-align: center;">283.190</td>
<td style="text-align: center;">3.153</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">-13.77</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: center;">CHEMBL173344</td>
<td style="text-align: center;">SERT</td>
<td style="text-align: center;">253.160</td>
<td style="text-align: center;">3.174</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">-13.58</td>
<td style="text-align: center;">0.40</td>
</tr>
</tbody>
</table>
<h1>2.3.2 ChatGPT aided multi-objective drug-target interaction modeling</h1>
<p>To predict the binding affinities of newly generated molecules to four targets (DAT, NET, SERT, and hERG), we aimed to construct four binding affinity predictors. Initially, we sought guidance from GPT-4's 3rd persona on the most suitable machine learning algorithms, given our dataset's specific attributes (sample size, feature size, and label). The recommendations of GPT-4 are detailed in Dialogue 11. In our former study [10], gradient boosting decision trees (GBDT) were utilized to train binding affinity predictors on DAT-Inhibitors, NET-Inhibitors, SERT-Inhibitors, and hERG-Inhibitors datasets. The resulting 10-fold Pearson correlation coefficients were $0.78,0.76,0.76$, and 0.68 respectively, serving as our baseline. Given our access to robust computational resources via high-performance computers (HPC), we decided to develop four deep neural networks. All four predictors were built and trained using PyTorch. Each network consisted of three hidden layers, with 512, 1024, and 512 hidden neurons respectively. The networks were trained over 1000 epochs, with a learning rate of 0.0001 for the first 500 epochs and 0.00001 for the remaining 500 epochs. Additionally, The Adam optimizer was chosen for this task. Researchers can inquire template of PyTorch code to build a deep neural network via ChatGPT (see Supporting Information S4.3). Moreover, as suggested by GPT-4, to get a more robust estimate of the model performance, we also evaluate the Pearson correlation coefficient (R) and root-mean-square error (RMSE) of 10-fold cross validation of four predictors, which are reported in the Table 2.</p>
<p>Figure 2 d) and e) shows the experimental and predicted binding affinity distribution on the four training sets: DAT-Inhibitors, NET-Inhibitors, SERT-Inhibitors, and hERG-Inhibitors. The distribution of predicted binding affinities align well with the experimental values, which shows that our binding affinity predictor is reliable. The grey region represents the zone where the binding affinity is less than $-9.54 \mathrm{kcal} / \mathrm{mol}$ (i.e $K_{\mathrm{i}}=0.1 \mu \mathrm{M}$ ). This value is generally considered as the cut-off for recognizing active compounds. The pink</p>
<p>I have a training dataset including 2662 samples. The feature size is 512 and labels are values in the range of [-14.18, -2.90]. To build a well-trained predictor, could you please recommand a suitable machine learning algorithm.</p>
<p>Recommendation:</p>
<ul>
<li>Start with a simpler model like Ridge or Lasso regression to establish a baseline.</li>
<li>Move on to ensemble methods like Random Forest or Gradient Boosting Machines (XGBoost or LightGBM) which often provide good performance without extensive feature engineering.</li>
<li>If computational resources allow, you can experiment with Neural Networks, but ensure you have mechanisms in place to prevent overfitting (like dropout layers).</li>
<li>Always split your data into training, validation, and test sets. Use the validation set for hyperparameter tuning and the test set for final evaluation.</li>
<li>Consider using cross-validation to get a more robust estimate of your model's performance.</li>
</ul>
<p>Remember, the best model often depends on the specific characteristics of the data, so it's a good practice to try multiple algorithms and see which one performs best on your validation set.</p>
<p>Dialogue 11: Consideration of machine learning algorithms that can be applied to build binding affinity predictors.</p>
<p>Table 2: Dataset summary. Four datasets are utilized, each containing SMILES strings of inhibitors targeting DAT, NET, SERT, and hERG respectively. Alongside each SMILES string, the respective binding affinity in the unit of $\mathrm{kcal} / \mathrm{mol}$ is also included as the label for each sample. Additionally, the final two columns represent the 10-fold cross validation Pearson correlation coefficient (R) and root-mean-square rrror (RMSE) for each binding affinity predictor across the four datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset name</th>
<th style="text-align: center;">Sample size</th>
<th style="text-align: center;">Binding affinity range $(\mathrm{kcal} / \mathrm{mol})$</th>
<th style="text-align: center;">10 -fold R</th>
<th style="text-align: center;">10 -fold RMSE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DAT-Inhibitors</td>
<td style="text-align: center;">2662</td>
<td style="text-align: center;">$[-14.18,-2.90]$</td>
<td style="text-align: center;">0.8212</td>
<td style="text-align: center;">0.8979</td>
</tr>
<tr>
<td style="text-align: left;">NET-Inhibitors</td>
<td style="text-align: center;">2981</td>
<td style="text-align: center;">$[-14.63,-5.47]$</td>
<td style="text-align: center;">0.7732</td>
<td style="text-align: center;">0.9683</td>
</tr>
<tr>
<td style="text-align: left;">SERT-Inhibitors</td>
<td style="text-align: center;">4341</td>
<td style="text-align: center;">$[-15.00,-5.64]$</td>
<td style="text-align: center;">0.8022</td>
<td style="text-align: center;">0.9448</td>
</tr>
<tr>
<td style="text-align: left;">hERG-Inhibitors</td>
<td style="text-align: center;">6298</td>
<td style="text-align: center;">$[-13.84,-3.27]$</td>
<td style="text-align: center;">0.8092</td>
<td style="text-align: center;">0.7981</td>
</tr>
</tbody>
</table>
<p>region highlights the zone where the binding affinity is more than $-8.18 \mathrm{kcal} / \mathrm{mol}$ (i.e $K_{\mathrm{i}}=1 \mu \mathrm{M}$ ), a criterion set to prevent hERG-related side effects. We generated around 16 million novel compounds from stochasticbased molecular generator, and their distribution of predicted binding affinities are depicted in Figure 2 (). It is worth mentioning that the predicted binding affinity of newly generated molecules, targeted to DAT, NET, and SERT, all fall below $-9.54 \mathrm{kcal} / \mathrm{mol}$. This results from the properly adjusted hyperparameters and threshold of step size in the stochastic-based molecular generator, aimed at producing more active compounds. However, as hERG inhibitors were not included as reference compounds in the generation of new molecules, only about half of the generated compounds meet the criteria set to prevent hERG-related side effects.</p>
<h1>2.3.3 ChatGPT assisted virtual screening of multi-target drug candidates</h1>
<p>By editing the latent space vector of the Seq2Seq AutoEncoder (AE), we were able to generate a vast number of vectors (around 16 billion) using our stochastic-based molecular generator. These vectors are then decoded into molecules through the GRU Decoder of the Seq2Seq AE. Next, we proceeded by implementing a filtering process in which we removed any duplicated molecules and predicted the corresponding binding affinities of remaining molecules to four target proteins: DAT, NET, SERT, and hERG. Any generated molecules meeting the binding affinity requirement (i.e., $\Delta G&lt;-9.54 \mathrm{kcal} / \mathrm{mol}$ for DAT, NET, SERT and $\Delta G&gt;-8.18 \mathrm{kcal} / \mathrm{mol}$ for hERG) were considered preliminary multi-target drug candidates. A total of 330 preliminary drug candidates pass the filtering test. Moreover, the similarities between 330 preliminary drug candidates and three references compounds are all less than 0.5 , indicating the the high novelties of generated multi-target molecules.</p>
<p>Then, we sought advice from GPT-4's 1st persona on criteria for selecting drug-like lead compounds. Due to paper length constraints, a concise version of responses can be found in Dialogue 12. Acting on these</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: 2D molecular structures of reference compound with ChEMBL ID a) CHEMBL113621 from DAT-Inhibitors dataset, b) CHEMBL1275709 from NET-Inhibitors dataset, and c) CHEMBL173344 from SERT-Inhibitors dataset. 2D molecular structures are rendered by an online software SmilesDrawer 2.0 [25]. d) Distribution of experimental binding affinities for the four training datasets (DAT-Inhibitors, NET-Inhibitors, SERT-Inhibitors, and hERG-Inhibitors). e) Distribution of predicted binding affinities derived from the four deep neural network predictors. f) Distribution of predicted binding affinities for newly generated inhibitors targeting DAT, NET, SERT, and hERG. g) Screening of 330 preliminary multi-target drug candidates. The color of each point represents the predicted binding affinities to DAT (purple, g)), NET (green, h)), and SERT (blue, i)). The light purple, green, and blue frames outline the medium ranges of 10 ADMET, physicochemical, and medicinal chemistry properties, respectively, while the dark purple, dark green, and dark blue frames outline the excellent ranges of these properties.</p>
<p>suggestions, we utilized in silico tools to predict the Absorption, Distribution, Metabolism, Excretion, and Toxicity (ADMET) properties of each candidate molecule. Specifically, we examined 10 properties of 330 preliminary multi-target drug candidates through ADMETlab 2.0. This platform aims to provide systematic evaluation of ADMET properties, physicochemical properties, and an assessment of medicinal chemistry friendliness.</p>
<p>The 10 properties assessed in this work included: Caco-2 (the human colon adenocarcinoma cell lines) permeability, $\mathrm{F}<em 1="1" 2="2" _="/">{20 \%}$ (the human oral bioavailability $20 \%$ ), Pgp-substrate (the substrate of P-glycoprotein), Pgpinhibitor (the inhibitor of P-glycoprotein), VD (volumn density), $\mathrm{T}</em>$ (The half-life of a drug), FDAMDD (The maximum recommended daily dose), SAS (synthetic accessibility score), $\log P$ (the logarithm of the noctanol/water distribution coefficient), and $\log S$ (the logarithm of aqueous solubility value). The optimal range of 10 properties can be found in Table 3.</p>
<p>I have 330 candidate molecules which have high binding affinities to DAT, NET, and SERT. How should I select lead compounds that are druglikeness. Noted: I do not have any wet lab resources.</p>
<p>Given that you don't have access to wet lab resources, you'll need to rely on computational methods and in silico analyses to assess the drug-likeness of your candidate molecules and select potential lead compounds. Here's a step-by-step approach:</p>
<ol>
<li>
<p>Molecular Descriptors and Lipinski's Rule of Five:</p>
</li>
<li>
<p>Calculate molecular descriptors for each of the 330 molecules. This includes molecular weight, the number of hydrogen bond donors and acceptors, and the octanol-water partition coefficient (LogP).</p>
</li>
<li>
<p>Filter out molecules that violate Lipinski's Rule of Five too severely. While one or two violations might be acceptable, compounds that violate multiple rules are less likely to be orally bioavailable.</p>
</li>
<li>
<p>ADMET Profiling: Use in silico tools to predict the Absorption, Distribution, Metabolism, Excretion, and Toxicity (ADMET) properties of each molecule. There are several software and online tools available for this, such as SwissADME, pkCSM, and PreADMET. Prioritize molecules with favorable ADMET profiles, such as good oral bioavailability, low predicted toxicity, and stable metabolic profiles.</p>
</li>
<li>Molecular Docking (if possible): If you have access to the 3D structures of DAT, NET, and SERT, you can perform molecular docking to predict how each molecule binds to the target. While this is a more advanced step and requires specialized software (like AutoDock or Dock), it can provide valuable insights into the binding modes and affinities of the molecules.</li>
<li>Similarity to Known Drugs: Compare the molecular fingerprints of your candidates to those of known drugs in databases like DrugBank. Molecules with high similarity to known drugs might have a higher chance of being drug-like.</li>
<li>Synthetic Accessibility: Evaluate the synthetic accessibility of each molecule. Some in silico tools can predict how easy or difficult it might be to synthesize a molecule. Molecules that are too complex might not be feasible as drug candidates due to challenges in synthesis.</li>
</ol>
<p>Dialogue 12: Silico analyses to assess the drug-likeness of candidate molecules suggested by ChatGPT.
Figure 2 g ), h), and i) depict the screening results on 330 preliminary multi-target drug candidates. The color gradient in each panel signifies the predicted binding affinities of the molecules to their respective targets. Specifically, in the Figure 2 g ), the color of each point indicates the binding affinities to DAT. Similarly, in the Figure 2 h) and i), the colors of points represent the binding affinities to NET and SERT, respectively. It can be seen that the binding affinity of drug candidates for SERT is stronger than that for DAT and NET. The frames outline the medium (light purple, green, and blue) and excellent ranges (dark purple, green, and blue) for the 10 evaluated ADMET, physicochemical, and medicinal chemistry properties. Researchers can access the code of scatter plot in python via ChatGPT swiftly see Supporting Information S4.3.</p>
<p>Figure 2 g ), h), and i) indicate that all the drug candidates have favorable volume density (VD) and synthetic accessibility score (SAS) values. However, only a select few drug candidates demonstrate preferable FDAMMDD, $\mathrm{F}<em 1="1" 2="2" _="/">{20 \%}, \mathrm{~T}</em>$, Pgp-sub, and Pgp-inh. Among all, 15 candidate drugs fall within the medium range for all properties, thus are considered as potential multi-target anti-cocaine lead compounds. We also evaluated the SMILES strings of 15 potential anti-cocaine addiction lead compounds that could target multiple transporters: DAT, NET, and SERT. Notably, all 15 lead compounds satify Lipinski's rule of five.</p>
<p>Section 2.3.5. Furthermore, we also did molecular docking analysis of 15 lead compounds following the suggestions of ChatGPT, which can be found in Section 2.3.5.</p>
<p>Noted, in Section 2.2.3, GPT-4 introduced a generalized suggestion about leveraging alternative machine learning approaches to predict effectiveness of a given molecule against various targets. A detailed discussion of potential ML methods is provided in Dialogue 13. In fact, ADMETlab2.0 serves as an exemplary application of these alternative methodologies. Specifically, it incorporates both Graph Neural Networks (GNNs) and multitask learning, leading to enhanced performance across many modeled endpoints.</p>
<p>You mentioned about "alternative machine learning methodologies to predict the effectiveness of a molecule against multiple target". Could you give me more details on that.</p>
<p>Certainly! Machine learning (ML) methodologies have been increasingly applied in drug discovery, especially in predicting the interactions between molecules and multiple targets. Here's a detailed breakdown of some alternative ML methodologies that can be used to predict the effectiveness of a molecule against multiple targets:</p>
<ol>
<li>Multi-Task Learning (MTL)</li>
<li>Transfer Learning</li>
<li>Ensemble Methods</li>
<li>Graph Neural Networks (GNNs)</li>
<li>Attention Mechanisms</li>
<li>Active Learning</li>
<li>Reinforcement Learning (RL)</li>
</ol>
<p>When selecting a methodology, it's essential to consider the nature of your data, the computational resources available, and the specific challenges of predicting interactions with multiple targets. Often, a combination of methods, such as using GNNs with attention mechanisms or MTL with transfer learning, can yield the best results.</p>
<p>Dialogue 13: Alternative machine learning approaches to predict effectiveness of a given molecule against various targets.</p>
<h1>2.3.4 ChatGPT assisted identification of potential multi-target leads</h1>
<p>Figure 3 a) represents the SMILES strings of the 15 candidate drugs, detailing whether each drug falls within the excellent or medium range for the evaluated 10 properties. Green pixels denote a candidate drug that meets the excellent range for each of the 10 evaluated properties, while blue pixels indicate a drug that only achieves the medium range. The color gradient signifies the percentage of properties within the excellent range for each compound. The $y$-axis displays the SMILES strings of the 15 candidate drugs along with their corresponding IDs. Notably, Drug 15 is the only candidate that exhibits an excellent FDAMDD value, though its $\mathrm{T}_{1 / 2}$ is relegated to the medium range. Furthermore, the drugs with IDs 4, 7, 10, and 15 demonstrate a relatively high percentage of properties within the excellent range.</p>
<p>In addition, we aimed to analyze the moieties of 15 potential multi-target anti-cocaine addiction lead compounds. Figure 3 b) depicts the 2D molecular structures of three reference compounds (CHEMBL113621, CHEMBL1275709, and CHEMBL173344) and 15 potential anti-cocaine lead compounds. In this figure, purple, green, and blue spots depict the CHEMBL113621-like, CHEMBL1275709-like, and CHEMBL173344-like moieties, respectively. Additionally, red spots highlight novel moieties that are not present in the three reference compounds. Leads 1, 2, 3, 8, 9, 13, 14, and 15 share more CHEMBL113621-like and CHEMBL173344like moieties, while Leads 4, 5, 6, 7, 10, 11, and 12 sahre more CHEMBL1275709-like moieties. For a systematic examination of the functional groups within these lead compounds, we leaned on guidance from ChatGPT, which simulated the role of a chemist skilled in interpreting SMILES (Simplified Molecular Input Line Entry System) strings. An illustrative example of this guidance can be seen in Dialogue 14. A comprehensive analysis is available in the Supporting Information S4.4. Assisted by ChatGPT, we successfully undertook the analysis of the 2D molecular structures of the 15 lead compounds.</p>
<p>Lead 1 features a benzene ring linked to a bicyclic structure and a dimethylamine branch. It is worth mentioning that molecules with a nitrogen-containing ring (such as piperidine) attached to a benzene ring are</p>
<p>I want you to act as a chemist who is skilled at analyzing SMILES (Simplified Molecular Input Line Entry System) strings. I will provide you with SMILES strings representing various chemical compounds, and your role will be to interpret these strings, identifying the structure, functional groups, and potentially significant chemical properties of each compound. Please focus on the analysis and interpretation of the SMILES strings, and avoid unrelated digressions. The first SMILES string for your analysis is "CN(C)c1cccc(C2CCC3NCCC3C2CO)c1".</p>
<p>Your SMILES string "CN(C)c1cccc(C2CCC3NCCC3C2CO)c1" represents a relatively complex organic molecule.
Let's break this down:</p>
<ul>
<li>"CN(C)" represents a tertiary amine group, specifically, dimethylamine (N(CH3)2).</li>
<li>"c1cccc(...c1)" describes a six-membered aromatic ring, benzene. The numbers "1" refer to the ring opening and closing.</li>
<li>"C2CCC3NCCC3C2C0" describes a bicyclic structure attached to the benzene ring. Let's analyze this structure:</li>
<li>The first "C2" describes the start of a new ring structure attached to the benzene ring.</li>
<li>"CCC3NCCC3" describes a six-membered ring (akin to piperidine) attached to the second carbon in the chain (C2).</li>
<li>The last "C2CO" segment signifies the closure of the second ring, with an alcohol (-OH) group attached to the carbon that is shared by both rings (C2).</li>
</ul>
<p>So, to summarize the structure: This molecule comprises a benzene ring, attached to which is a bicyclic structure that is reminiscent of a piperidine ring with an alcohol group. In addition, a dimethylamine branch is also present.</p>
<p>As for its chemical properties, it's hard to pinpoint without further context. However, molecules with a nitrogen-containing ring (such as piperidine) attached to a benzene ring are common in a number of bioactive compounds. The exact properties would depend on many factors, including the exact 3D configuration and the presence of other functional groups or structural elements not visible from the SMILES string alone.</p>
<p>Dialogue 14: ChatGPT acts as a chemist to guide the analyze of functional groups. Cpmplete interactions with ChatGPT can be found in Supporting Information 54.4.
frequently found in bioactive compounds. Lead 2 comprises a benzene ring attached to a modified piperazine ring, which is further connected with a cyclopentane group.. This type of structure is prevalent in numerous bioactive molecules, including some pharmaceutical drugs. Lead 3 contains a chlorobenzene ring coupled with a substituted piperazine ring. Besides, there is a methylamine group attached to the benzene ring. This structure might have potential psychoactivity, as structures featuring a nitrogen-containing ring connected to a benzene ring are commonly observed in many psychoactive compounds such as Phenethylamines, Tryptamines, and Ergolines. Lead 4 encompasses a benzene ring with an attached dimethylamine group. In addition, the benzene ring is linked to a bicyclic structure that includes a piperidine ring and an aldehyde group. This molecule could potentially be bioactive due to the presence of both a benzene ring and a nitrogen-containing ring. Lead 5 shares very similar structure as Lead 4. The only difference is that the bicyclic structure of Lead 5 includes propionaldehyde group instead of aldehyde group.</p>
<p>Leads 6, 10, 11, and 12 all feature a benzene ring connected to a dimethylamine group and a bicyclic structure. Lead 7 consists of a benzene ring linked to a substituted alkene group, along with a bicyclic structure that includes both a pyrrolidine ring and a piperazine ring. Additionally, this bicyclic structure is connected by a propionadehyde group. Lead 8 includes a benzene ring with an attached dimethylamine group, connected to a bicyclic structure that incorporates a piperidine ring and an additional pyrrolidine ring. Lead 9 comprises a benzene ring linked to an alkyne group, and a complex structure with a piperidine ring and a three-membered nitrogen-sulfur ring. Notably, molecules with sulfur-containing rings, such as penicillin and angiotensin-converting enzyme (ACE) inhibitors, are recognized as bioactive. Lead 13 incorporates a benzene ring with an attached dimethylamine group, connected to a bicyclic structure that includes pyrrolidine ring and a cyclohexane ring which is also connected by a formyl group. Lead 14 is composed of a chloroethane group linked to two pyrrolidine rings and a benzene ring. Finally, Lead 15 consists of a benzene ring linked to a dimethylamine group via an alkene group and attached to a bicyclic structures composed by two pyrrolidine rings.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: a) The SMILE string of 15 potential anti-cocaine lead compounds that could target to multiple transporters DAT, NET, and SERT. Green pixels indicate a given candidate falls within the excellent range for each of the 10 evaluated ADMET, physicochemical, and medicinal chemistry properties, while the blue pixels describe a give candidate drug only falls within the medium range of these properties. The color gradient represents the percentage of properties within the excellent range for each given compound. b) Illustration of the 2D molecular structures of three reference compounds and 15 potential anti-cocaine lead compounds, which may target multiple transporters (DAT, NET, and SERT). Purple, green, and blue spots represent the CHEMBL113621-like, CHEMBL1275709-like, and CHEMBL173344-like moieties, respectively. Red spots highlight novel moieties that are not present in the three reference compounds. All 2D molecular structures are rendered by an online software SmilesDrawer 2.0 [25].</p>
<h1>2.3.5 ChatGPT assisted analysis of cocaine transporter and inhibitor interactions</h1>
<p>As mentioned in Dialogue 12, ChatGPT suggested to perform molecular docking to predict how each molecule binds to the target. We decided to accept this suggestion as the understanding of the molecular mechanism of drug-target interactions is vital in identifying effective drug candidates. We also seek the expertise from ChatGPT for the installation guidance of AutoDock Vina [26] and guidance to execute molecular docking procedures (see Dialogue 15) between 15 lead compounds and target proteins DAT (PDB ID:</p>
<p>4XPA) and SERT (PDB ID: 6DZZ). To be noted, due to the lack of NET structures in the Protein Data Bank, we do not included molecular interaction analysis of candidate leads with NET. Moreover, we want to visualize 2D protein-ligand interaction diagrams, as they offer a streamlined representation of protein-ligand interactions, highlighting crucial residues, hydrogen bonds, and more. ChatGPT recommended several popular software tools for this purpose, including LigPlot ${ }^{+}$and Maestro. In this work, we chose LigPlot ${ }^{+}$ for our visualization needs.</p>
<h1>A Could you please show me how to install AutoDock Vina? Also, how to excuate molecular docking in this software?</h1>
<h2>B Installing AutoDock Vina:</h2>
<ol>
<li>Download:</li>
</ol>
<p>First, you need to download the appropriate version of AutoDock Vina for your operating system from the official website.
2. Installation:</p>
<p>For Windows: Simply unzip the downloaded file to a location of your choice.
For Linux:
-- Unpack the tarball: tar xvfz autodock_vina<em>.tgz
-- Move to the directory: cd autodock_vina_</em>
-- Run the program: ./autodock_vina
-- PATH Variable (Optional but recommended):
For easier access, you can add the location of the Vina binary to your system's PATH variable.
Molecular Docking using AutoDock Vina:</p>
<ol>
<li>Prepare the Receptor and Ligand: You need to have your receptor and ligand in PDBQT format. You can use tools like AutoDockTools (ADT) to prepare these files.</li>
<li>Configuration File: Create a configuration file (e.g., config.txt) that specifies parameters for the docking. Replace the placeholders with appropriate values. The center and size parameters define the search space for docking.</li>
<li>Run the Docking: Use the following command:
vina --config config.txt --log output.log
This will generate an output PDBQT file with the docked conformations of the ligand.</li>
<li>View Results: You can use molecular visualization tools like PyMCL as Chimera to view the docked complexes. (2) (3)</li>
</ol>
<p>Dialogue 15: ChatGPT assists in the software installation and application. Complete dialogue is in the Supporting Information S4.3.
Our observations highlight the critical role of hydrogen bonds in the molecular interactions. The interactions of the drug candidate with DAT and SERT feature two and one hydrogen bonds, respectively, thereby contributing to the high potency of the molecule on the transporters. The first and third columns in Figure 4 illustrate the docking of Lead 15 and its molecular interactions with DAT and SERT. We have identified 15 nearly optimal leads. As demonstrated in the second and fourth columns of Figure 4 a), Lead 4 establishes two hydrogen bonds with DAT and four hydrogen bonds with SERT. Of the two bonds with DAT, one is formed between an oxygen atom on the residue Gln209(A) and a nitrogen atom on the compound, and the other involves an oxygen atom in a hydroxyl group of the compound interacting with a nitrogen atom on residue Asn207(A) of DAT. Among the four hydrogen bonds formed between Lead 4 and SERT, two involve the same oxygen atom interacting with a nitrogen atom on residues Leu99(A) and Tyr176(A) of SERT, while the other two bonds are formed by nitrogen atoms on Lead 4, interacting with oxygen atoms on residues Ser438(A) and another unidentified residue of SERT. Figure 4 b) depicts a hydrogen bond in the molecular interactions between candidate Lead 9 and SERT. This bond is formed by a nitrogen atom on the compound and an oxygen atom in a hydroxyl group on residue Phe335(A) of SERT. However, no hydrogen bond is observed in its interactions with DAT. This suggests that other types of interactions, such as hydrophobic bonds, may play a major role in the high binding affinity between Lead 9 and DAT.</p>
<p>The molecular docking poses of Lead 13 on DAT and SERT are illustrated in the 1st and 3rd columns of Figure 4 c). In the second column of Figure 4 c), a single hydrogen bond can be observed between a nitrogen atom of Lead 13 and an oxygen atom on the residue Glu161(A) of DAT. Conversely, no hydrogen bond</p>
<p>is detected between Lead 13 and SERT, as demonstrated in the 4th column of Figure 4 c). The molecular docking poses of Lead 15 on DAT and SERT are portrayed in Figure 4 d), presenting the compound's docking positions at the centers of both transporters. In its interaction with DAT, Lead 15 forms two hydrogen bonds through a nitrogen atom in a five-membered nitrogen heterocycle. This nitrogen atom interacts with oxygen atoms in two hydroxyl groups, which are attached to the residues Asp475(A) and Tyr123(A) of DAT. Moreover, a hydrogen bond exists between the candidate drug Lead 15 and SERT. This bond is formed by the same nitrogen atom in the five-membered nitrogen heterocycle, which interacts with an oxygen atom in a hydroxyl group attached to the residue Ala169(A) of SERT. The molecular interaction with other 11 candidate leads can be found in the Supporting Information S3.</p>
<h1>3 Discussion</h1>
<h3>3.1 Scrutinizing chatbots</h3>
<p>While chatbots are powerful large language models, they are not infallible. Their predictions are heavily reliant on the training data, which may lead to incomplete, outdated, bias, or skewed understandings of certain contexts. Consequently, this could result in the generation of misleading narratives and incorrect information. Therefore, it is essential for researchers to employ chatbots with appropriate care and vigilance. Scientists should not solely rely on chatbots for their research pursuits and should consistently cross-check the information generated by chatbots. Notably, the role of a chatbot like GPT-4 is to assist researchers, not to replace them. In our current project, we have employed GPT-4 to assist in our anti-cocaine addiction drug discovery process, as delineated in Figure 5 a).
We first will assign a proper persona to GPT-4 and then ask it with questions. Once we get the response from GPT-4, it is crucial to decide whether to accept the responses or not. If the information aligns well with the literature and our expertise, we will accept the responses and proceed with the suggestions of GPT-4. Otherwise, we will either reject the answer or seek further clarification to GPT to get alternative feedback.</p>
<p>For example, when acting as a chemist to analyze the functional groups of Leads 2, 7, 8, 9, 13, and 15, ChatGPT provided inaccurate information. Specifically, for Lead 15, ChatGPT identified a structure where a benzene ring is linked to a dimethylamine group via an alkene group and connected to a piperazine ring. However,the dimethylamine group is connected to a piperazine ring instead of a piperazine ring. This misinformation in the interaction with ChatGPT is documented in Dialogue 16. Thus, it is paramount for researchers to verify the accuracy and reliability of responses from ChatGPT using their expertise.</p>
<p>Lead 15: $\mathrm{C}=\mathrm{C}\left(\mathrm{c} 1 \mathrm{cccc}\left(\mathrm{N} 2 \mathrm{CC} 3 \mathrm{CCNC} 3 \mathrm{C} 2\right) \mathrm{c} 1\right) \mathrm{N}(\mathrm{C}) \mathrm{C}$
The SMILES string "C=C(c1cccc(N2CC3CCNC3C2)c1)N(C)C" represents a complex organic molecule. Here's the breakdown:</p>
<ul>
<li>"C=C": This portion represents an alkene group, a carbon-carbon double bond.</li>
<li>"N(C)C": This signifies a tertiary amine group, specifically, dimethylamine (N(CH3)2), attached to the alkene.</li>
<li>"c1cccc(N2CC3CCNC3C2)c1": This part represents a benzene ring, a six-membered aromatic ring of carbon atoms, with a complex substituent. "N2CC3CCNC3C2" signifies a bicyclic structure connected to the benzene ring. This structure is a piperazine ring, a six-membered ring with two nitrogen atoms and four carbon atoms.</li>
</ul>
<p>In summary, this molecule consists of a benzene ring attached to a dimethylamine group through an alkene group. The benzene ring is also connected to a piperazine ring. Such a structure could potentially have significant biological activity, as both benzene and piperazine rings are common in bioactive compounds. The presence of an alkene could also impart certain reactivity. However, the exact properties would depend on many factors, including the specific 3D configuration, which is not directly captured by the SMILES string.</p>
<p>Dialogue 16: Misinformation provided by ChatGPT. Complete interactions with ChatGPT can be found in Supporting Information S4.4.</p>
<p>Additionally, we noticed that ChatGPT does not perform well when providing methodological explana-</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author. Email: weig@msu.edu&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
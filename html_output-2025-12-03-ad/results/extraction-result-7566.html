<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7566 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7566</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7566</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-268363551</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.07815v3.pdf" target="_blank">Chronos: Learning the Language of Time Series</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained specifically on them. Our results demonstrate that Chronos models can leverage time series data from diverse domains to improve zero-shot accuracy on unseen forecasting tasks, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7566",
    "paper_id": "paper-268363551",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.02266725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Chronos: Learning the Language of Time Series
4 Nov 2024</p>
<p>Abdul Fatir Ansari 
AWS AI Labs</p>
<p>Lorenzo Stella stellalo@amazon.com 
AWS AI Labs</p>
<p>Caner Turkmen 
AWS AI Labs</p>
<p>Xiyuan Zhang 
UC San Diego</p>
<p>Pedro Mercado 
AWS AI Labs</p>
<p>Huibin Shen 
AWS AI Labs</p>
<p>Oleksandr Shchur 
AWS AI Labs</p>
<p>Syama Sundar Rangapuram 
AWS AI Labs</p>
<p>Sebastian Pineda Arango 
University of Freiburg</p>
<p>Shubham Kapoor 
AWS AI Labs</p>
<p>Jasper Zschiegner 
Danielle C Maddix 
AWS AI Labs</p>
<p>Hao Wang 
AWS AI Labs</p>
<p>Rutgers University
6 UCBerkeley</p>
<p>Michael W Mahoney 
Amazon Supply Chain Optimization Technologies</p>
<p>Kari Torkkola 
Amazon Supply Chain Optimization Technologies</p>
<p>Andrew Gordon Wilson 
Amazon Supply Chain Optimization Technologies</p>
<p>New York University</p>
<p>Michael Bohlke-Schneider 
AWS AI Labs</p>
<p>Yuyang Wang 
AWS AI Labs</p>
<p>Chronos: Learning the Language of Time Series
4 Nov 20249493916DE531898508C62FE40A5C8477arXiv:2403.07815v3[cs.LG]
We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models.Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss.We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization.In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained specifically on them.Our results demonstrate that Chronos models can leverage time series data from diverse domains to improve zero-shot accuracy on unseen forecasting tasks, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines.</p>
<p>Introduction</p>
<p>Time series forecasting is an essential component of decision-making across various domains, including retail, energy, finance, healthcare, climate science, among others.Traditionally, forecasting has been dominated by statistical models such as ARIMA and ETS.These have served as reliable tools, at least until the recent shift towards deep learning techniques (Hyndman &amp; Athanasopoulos, 2018;Benidis et al., 2022).This shift can be attributed to the availability of large and diverse time series data sources, and the emergence of operational forecasting problems (Kolassa &amp; Januschowski, 2019) that play to the strengths of deep forecasters, i.e., the ability to extract patterns out of a large collection of time series.Despite their impressive performance, deep forecasters still operate in the standard regime of training and prediction on the same dataset.While there have been works dedicated to transfer learning (Ye &amp; Dai, 2018) and domain adaptation (Jin et al., 2022) for forecasting, the field has yet to converge on a unified, general-purpose forecasting model, a goal that remains a beacon for time series researchers.</p>
<p>The emergence of large language models (LLMs) with zero-shot learning capabilities has ignited interest in developing "foundation models" for time series.In the context of LLMs, this interest has been pursued through two main avenues: directly prompting pretrained LLMs in natural language (Gruver et al., 2023;Xue &amp; Salim, 2023) and fine-tuning LLMs for time series tasks (Zhou et al., 2023a;Jin et al., 2024).However, these methods face significant limitations, notably the need for prompt engineering or fine-tuning for each new task, or reliance on large-scale models (GPT-3 (Brown et al., 2020), Llama 2 (Touvron et al., 2023), etc.) that demand substantial computational resources and time for inference.Recent concurrent work (Dooley et al., 2023;Das et al., 2023;Rasul et al., 2023;Woo et al., 2024) also explores pretraining transformer-based models with sophisticated time-series-specific designs on a large corpus of real and (or) synthetic time series data.</p>
<p>In this work, we take a step back and ask: what are the fundamental differences between a language model that predicts the next token, and a time series forecasting model that predicts the next values?Despite the apparent distinction -tokens from a finite dictionary versus values from an unbounded, usually continuous domain -both endeavors fundamentally aim to model the sequential structure of the data to predict future patterns.Shouldn't good language models "just work" on time series?This naive question prompts us to challenge the necessity of time-series-specific modifications, and answering it led us to develop Chronos, a language modeling framework minimally adapted for time series forecasting.Chronos tokenizes time series into discrete bins through simple scaling and quantization of real values.In this way, we can train off-the-shelf language models on this "language of time series," with no changes to the model architecture (see Figure 1 for a high-level depiction of Chronos).Remarkably, this straightforward approach proves to be effective and efficient, underscoring the potential for language model architectures to address a broad range of time series problems with minimal modifications.For the development of a useful general-purpose time series forecasting model, the scarcity of publicly available time series datasets, both in quantity and quality, is arguably more critical than the modeling framework.In addition to the comprehensive collection of public datasets we used to train Chronos, a central aspect of our approach is the integration of data augmentation strategies, including TSMixup and KernelSynth.TSMixup randomly samples a set of base time series from different training datasets, and generates new time series based on a convex combination of them; KernelSynth uses Gaussian processes to generate synthetic time series by randomly composing kernel functions.These techniques address the inherent limitations of small training datasets in time series forecasting, enhancing model robustness and generalization.</p>
<p>Time Series Tokenization Training Inference</p>
<p>Our comprehensive evaluation across 42 datasets establishes Chronos as a benchmark for both in-domain and zero-shot forecasting, surpassing both traditional models and task-specific deep learning approaches.</p>
<p>input and output pairs and reformulating the forecasting problem as a question answering task.However, PromptCast requires dataset-specific templates for converting numerical data to text prompts.Perhaps the most straightforward LLM-based forecasting model is LLMTime (Gruver et al., 2023), which shows clear evidence for zero-shot forecasting ability of pretrained LLMs on a variety of benchmark time series datasets.LLMTime proposes a new tokenization scheme that encodes real-valued data as a string of digits after fixing the numerical precision and scaling the data appropriately.Once encoded as strings, forecasts are obtained in a zero-shot setting from pretrained LLMs such as GPT-3 (Brown et al., 2020) and Llama 2 (Touvron et al., 2023).Nevertheless, the use of such compute-hungry models hampers the scalability and practical utility of LLMTime.Zhou et al. (2023a) propose a unified one-fits-all model (GPT4TS) for different time series analysis tasks by using a pretrained GPT-2 model (Radford et al., 2019) as a backbone and only fine-tune the positional embeddings and the parameters of the layer normalization for each individual task.Instead of using tokenized input, they directly feed the model with patch embeddings, similar to PatchTST (Nie et al., 2023).Recent concurrent work, Time-LLM (Jin et al., 2024), repurposes LLMs for time series forecasting by aligning embeddings of time series patches with text prototypes, and prompting the (frozen) LLM with these aligned embeddings and a natural language prefix describing the task.Unlike Chronos, both GPT4TS and Time-LLM require in-domain training or fine-tuning, i.e., they are fine-tuned and tested on each dataset separately.Furthermore, the aforementioned methods are based on prompting or fine-tuning pretrained LLMs.In contrast, Chronos trains language models from scratch on a large collection of time series, tokenized via scaling and quantization.</p>
<p>Zero-shot forecasting.Zero-shot forecasting is the ability of models to generate forecasts for time series from unseen datasets.Some early work (Orozco &amp; Roberts, 2020;Oreshkin et al., 2021;Jin et al., 2022) in zero-shot forecasting considers training on a single time series dataset and testing on a different dataset.ForecastPFN (Dooley et al., 2023) tackles the problem of zero-shot forecasting by training a transformerbased model purely on synthetic data generated according to predefined trend, seasonalities (daily, monthly, yearly).The trained transformer model is then used to forecast real-world time series in a zero-shot setting.</p>
<p>In this work, we also propose a method to generate synthetic time series data from Gaussian processes (Section 4.2); however, we use the synthetic data in combination with real data to train Chronos models, which improves the overall zero-shot performance.Furthermore, Chronos models are probabilistic, whereas ForecastPFN can only generate point forecasts.</p>
<p>Recent concurrent works (Rasul et al., 2023;Goswami et al., 2024;Das et al., 2023;Woo et al., 2024) also develop zero-shot forecasting models by pretraining transformer-based architectures on a large corpus of time series data.These works operate on the real values of the time series and include time-seriesspecific designs such as time features, lags, patching, and real-valued distribution heads, among others.In contrast, Chronos follows a minimalist approach by tokenizing time series values into a fixed vocabulary and training existing language model architectures on these tokens without any time-series-specific design or features.That is, Chronos uses a categorical distribution to model the observations, performing regression via classification.</p>
<p>Other time series tasks.Similar to Zhou et al. (2023a), recent works have studied general purpose models applicable across time series tasks including imputation, forecasting, classification and anomaly detection.Wu et al. (2023) develop a task-generic backbone based on the Inception model (Szegedy et al., 2015).In order to use the CNN-based Inception model, one dimensional time series is transformed into a two dimensional image-like representation by essentially segmenting the time series based on the periodicity and stacking the segments.SimMTM (Dong et al., 2023) is a masked pretraining framework for time series which learns general time series representations that are then used for forecasting and classification via fine-tuning.Although we focus on univariate time series forecasting in this work, based on its excellent performance on unseen time series datasets, we hypothesize that Chronos learns general representations that can potentially be deployed for tasks beyond forecasting.</p>
<p>Chronos: A Language Modeling Framework for Time Series</p>
<p>In this section we introduce Chronos, a framework adapting existing language model architectures and training procedures to probabilistic time series forecasting.While both language and time series are sequential in nature, they differ in terms of their representation -natural language consists of words from a finite vocabulary, while time series are real-valued.This distinction necessitates specific modifications to existing language modeling frameworks, especially concerning tokenization, to make them applicable to time series data.Nevertheless, since existing transformer models have excelled on language tasks, our design philosophy involves making minimal changes to the model architectures and training procedure.</p>
<p>Time Series Tokenization</p>
<p>Consider a time series x 1:C+H = [x 1 , . . ., x C+H ], where the first C time steps constitute the historical context, and the remaining H represent the forecast horizon.Language models operate on tokens from a finite vocabulary, so using them for time series data requires mapping the observations x i ∈ R to a finite set of tokens.To this end, we first scale and then quantize observations into a fixed number of bins.</p>
<p>Scaling.The scale of time series can differ significantly even within a single dataset.This poses optimization challenges for deep learning models.Therefore, individual time series are normalized to facilitate better optimization.In the case of Chronos, the goal of normalization is to map the time series values into a suitable range for quantization.A common normalization technique involves applying an affine transformation to the time series, i.e., xi = (x i − m)/s.Several popular normalization schemes, such as mean scaling, standard scaling and min-max scaling, can be obtained by appropriately choosing m and s.We opt for mean scaling, a method that has proven effective in deep learning models commonly used for practical time series applications (Salinas et al., 2020;Rabanser et al., 2020), but other approaches are viable and only require minimal changes.An attractive feature of mean scaling is that it preserves zero values in the time series, which are often semantically meaningful, such as zero sales for a product or zero solar energy generation at night.Mean scaling normalizes individual entries of the time series by the mean of the absolute values in the historical context.Specifically, this involves setting m = 0 and s = 1  . . . , xC , . . . , xC+H ], is still real-valued and cannot be processed directly by language models.To convert these real values into discrete tokens, we employ quantization.Formally, we select B bin centers c 1 &lt; . . .&lt; c B on the real line, and B − 1 edges b i separating them, c i &lt; b i &lt; c i+1 , for i ∈ {1, . . ., B − 1}.The quantization function q : R → {1, 2, . . ., B}, and dequantization d : {1, 2, . . ., B} → R, are then defined as
q(x) =            1 if − ∞ ≤ x &lt; b 1 , 2 if b 1 ≤ x &lt; b 2 , . . . B if b B−1 ≤ x &lt; ∞,andd(j) = c j ,(1)
respectively.The positioning of bin centers and edges can either be data-dependent or uniform (Rabanser et al., 2020).Quantile binning, a type of data-dependent binning, exploits the cumulative distribution function (CDF) of the training datapoints to construct bins such that approximately equal number of datapoints are assigned to each bin.In contrast, uniform binning selects uniformly-spaced bin centers within the interval [c 1 , c B ] and the bin edges fall mid-way between the successive bin centers, i.e., b i = ci+ci+1 2 for i ∈ {1, . . ., B − 1}.Since the distribution of values for unseen downstream datasets can differ significantly from the training distribution, we opt for uniform binning in our experiments, but other quantization techniques can be used.We refer the reader to Rabanser et al. (2020) for a detailed discussion on quantization schemes for time series.A potential limitation of this approach is that the prediction range is restricted between [c 1 , c B ], making it theoretically infeasible to model time series with a strong trend.We explore this further in a practical setting in Section 5.7.</p>
<p>Apart from the time series tokens {1, 2, . . ., B}, we include two special tokens, commonly used in language models, into the time series vocabulary, V ts : PAD and EOS.The PAD token is used to pad time series of different Published in Transactions on Machine Learning Research (10/2024) lengths to a fixed length for batch construction and to replace missing values.The EOS token is appended to the quantized and padded time series to denote the end of the sequence.While the use of an EOS token is not strictly necessary in the case of time series, it makes training and inference using popular language modeling libraries convenient.The sequences of tokens from V ts can readily be processed by language models (both encoder-decoder and decoder only models), to train them as usual.A common approach in time series modeling is to incorporate time and frequency information, through features such as day-of-week, weekof-year, and so on.Perhaps counter-intuitively, in Chronos, we ignore time and frequency information, treating the "time series" simply as a sequence.</p>
<p>We primarily focus on the variants of the encoder-decoder T5 model (Raffel et al., 2020).Additionally, we conduct an experiment with the GPT-2 (Radford et al., 2019) model to demonstrate that our approach can be straightforwardly extended to decoder-only models.No modifications are required to the language model architecture, except adjusting the vocabulary size to |V ts |, which depends on the number of bins used for quantization and may be different from the vocabulary size of the original language model.Concretely, adjusting the vocabulary size entails truncating (or extending) the input and output embedding layers of the language model.</p>
<p>Objective Function</p>
<p>As typical in language models, we use the categorical distribution over the elements of V ts as the output distribution, p(z C+h+1 |z 1:C+h ) where z 1:C+h is the tokenized time series.Chronos is trained to minimize the cross entropy between the distribution of the quantized ground truth label and the predicted distribution.Formally, the loss function for a single tokenized time series (also accounting for EOS tokens) is given by,
ℓ(θ) = − H+1 h=1 |Vts| i=1 1 (z C+h+1 =i) log p θ (z C+h+1 = i|z 1:C+h ),(2)
where p θ (z C+h+1 = i|z 1:C+h ) denotes the categorical distribution predicted by the model parameterized by θ.In practice, the loss is averaged over a batch of time series during training.</p>
<p>Note that the categorical cross entropy loss (Eq.2) is not a distance-aware objective function, i.e., it does not explicitly recognize that bin i is closer to bin i + 1 than to i + 2. Instead, the model is expected to associate nearby bins together, based on the distribution of bin indices in the training dataset.In other words, Chronos performs regression via classification (Torgo &amp; Gama, 1997;Stewart et al., 2023).This is unlike typical probabilistic time series forecasting models, which either use parametric continuous distributions such as Gaussian and Student's-t (Salinas et al., 2020) or perform quantile regression (Wen et al., 2017;Lim et al., 2021).</p>
<p>Opting for a categorical output distribution offers two key advantages.Firstly, it requires no modification to the language model architecture or training objective, enabling the use of popular language modeling libraries and the utilities they provide out of the box (Wolf et al., 2020).Secondly, it imposes no restrictions on the structure of the output distribution, allowing the model to learn arbitrary distributions, including multimodal ones.This flexibility proves especially valuable for a pretrained model, as time series datasets from diverse domains may follow distinct output distribution patterns.</p>
<p>Arguably, modeling the output as an ordinal variable would be more appropriate, since the output domain is obtained by discretizing the real line.In fact, regression models for ordinal variables have been extensively studied in the literature (McCullagh, 1980;Winship &amp; Mare, 1984), including for neural networks and transformer models (Cheng et al., 2008;Hu et al., 2021).Imposing the ordinal nature of the classes on top of the models, in similar ways to the mentioned literature, could be an interesting extension of this work.</p>
<p>Forecasting</p>
<p>Chronos models are probabilistic by design and multiple realizations of the future can be obtained by autoregressively sampling from the predicted distribution, p θ (z C+h+1 |z 1:C+h ), for h ∈ {1, 2, . . ., H}.These sample paths come in the form of token IDs that need to be mapped back to real values and then unscaled to obtain the actual forecast.The dequantization function d from Eq. ( 1) maps the predicted tokens to real values: these are then unscaled by applying the inverse scaling transformation, which in the case of mean scaling involves multiplying the values by the scale s.</p>
<p>Data Augmentation</p>
<p>The quality and quantity of public time series data pales in comparison to the natural language processing (NLP) domain, which benefits from ample high-quality text datasets such as WikiText-103 (Merity et al., 2016), C4 (Raffel et al., 2020), and The Pile (Gao et al., 2020).This poses challenges for training models intended for zero-shot forecasting, which rely on large-scale time series data with diverse patterns.To address this issue, we propose enhancing the diversity of training data by generating mixup augmentations from real datasets and supplementing training with synthetic data.</p>
<p>TSMixup: Time Series Mixup</p>
<p>Mixup (Zhang et al., 2017) is a data augmentation scheme proposed in the context of image classification.It generates convex combinations of random image pairs and their labels from the training dataset, which alleviates issues such as memorization and overfitting in deep learning models.Existing works (Carmona et al., 2021;Zhou et al., 2023b) have extended Mixup to the time series domain.</p>
<p>TSMixup Augmentations Original Time Series
&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W v 4 u Z f g z q D e o o L Y l i 1 Y J l 1 p 2 t L 8 = " &gt; A A A y N 3 i c j V v L c t v I F d V M X h P l N Z N U Z Z M N K r J r Z l K y y r R d S T a p G r 0 l i 5 K o 9 8 P 0 u E D w E o S F l 9 F N S D K L 8 x f Z J t + R T 8 k q u 1 S 2 + Y P c 7 k b j 3 g Y h T V R l C X 1 O 4 6 L R f W 7 f Q x A e 5 H E k 5 P P n / / z k 0 x / 8 8 E c / / s l n P 1 3 8 2 c 9 / 8 c t f f f 7 F r 8 9 F N i k C O A u y O C s u B 7 6 A O E r h T E Y y h s u 8 A D 8 Z x H A x u F l X / E U J h Y i y 9 F T e 5 / A 2 8 c M 0 G k W B L x F 6 0 4 + x 6 9 B / 1 / l L 5 9 3 n S 8 9 X n u s f b / 6 g U x 0 s L V Q / v X d f d H 7 b H 2 b B J I F U B r E v x J v O 8 1 y + n f q F j I I Y Z o v 9 i Y D c D 2 7 8 E N 7 g Y e o n I N 5 O 9 Z h n 3 l N E h t 4 o K / B f K j 2 N 8 j O m f i L E f T L A n o k v x 6 L J K b C N e z O R o z + / n U Z p P p G Q B u Z C o 0 n s y c x T E + A N o w I C G d / j g R 8 U E Y 7 V C 8 Z + 4 Q c S p 2 l x c f G p + v E O N i + 8 / d X T H W 9 j c 2 v 3 Y P d 0 9 / D g x N P U Y t t I l v G v u g + x P E h m G M P b 9 4 s b T + C F c J a F l 4 2 8 w M / N s b r l A k Z Q F F E a q l E N o z I S t t s o C i c F 4 B 2 l c B t k S e K n w 2 k f w R h G c j a d 9 i H x v u r i 8 d e z 2 V y f A B c C C t t r X b f a + h V R O K 6 D H a t G W y + Z 5 b b P a Z a 3 9 R h k U m a J 7 b S m W 3 P 9 q v v 2 b T f / o R 4 D 2 2 P w U I / A 9 g g e 6 j G 0 P Y a q B y 7 D D t 5 d r O 7 Q 8 z 3 s r 1 Y d R p g q Q w / n J n F j 4 L E C Z 2 8 6 b z H K Y O Q t d V Q Q j L K l F 8 W s G m o K l r 0 4 u 4 X i W Y C J t 7 L Y x 5 B 6 W m G 0 1 J m a B f y u j 6 2 p D t B 2 O g 4 3 k n 6 8 4 m 2 h G I T E j F F r L 9 S K I W 8 i b t m I W 8 2 I m p a 3 m b 3 m 0 o v q q s K z n T y 8 o 6 r x w p 7 x Y e I P 6 Z S l l 0 u v 5 k 5 b r s + x R y 9 5 q F f 6 d k 6 M q h + d D l S + G X y V A s 5 8 t A S w E 2 L O P r F n n 7 S c f W z P 0 h l 9 m 9 V Z t l J P j L m 6 0 D N T 5 + A D U 9 M M O C 4 A m i F Z v K W X 8 x F p 1 l j s l / O x / d Q D X A R 1 c s u U w Q d z z 5 s f V r 7 7 y o b + + v E g U e r l P l J y D C I S L E 6 O g b 5 S k f 7 P Q J M 8 h 8 J T o z F B N u v B m B 7 O C q x 6 h X 9 L q 9 c I 9 u z Z M 7 / M o q E 3 E W q D i 0 Z e n g k R Y U E y o f P Y x w S s 4 j + 4 s L 7 a l H P M x 5 a Z U o w 5 v e r z s D 4 e u M k q 0 H o d a P 1 7 A + E 9 p y H o n d z 0 r a Z b w / W I U H C W t q G e P X t Q b D g 6 P w 4 z L E L j p O U + k T O j q z s 9 e q M s 1 N y d r t p Q q y 2 h b N r Y 6 + F N 1 L E e 3 1 J O n Z N W v / e k u U l F 9 c r q z p n 6 F G q G q 4 4 e W x R z f l O 9 v f r 8 n n u + v d P 6 A j h q d f z g g C v B Q R Q r s c b q A M s C d l B H V b x R n G W F p v W R 4 f V h 1 Q G p Q T L t N G u W L D A R Z t O + 8 g + B H 0 8 3 m h 1 K P 4 6 G v M M 7 c 1 w k U 0 P N 5 k K C k O 0 n a G Z W 3 x H k Q l X K X E R x l l Z V 7 h h D Z I l X + k X k Y 7 Z a f Y P 0 p y r y n U y z I s G o T / o I P Z n Z 6 S w a t E / M w G U G x A Q u E x A z d J k h M e A y Q M z I Z U b E h C 4 T E j N 2 m T E x k c t E x L x 3 m f f E 3 L j M D T G x y 8 Q z L e M i 8 S K B G Y t W f X i v N j u z g s v e + 4 m Q 3 j B L v 5 S e 8 s s o x 3 u 1 8 z g L 4 y V V 7 N S N n d J V M 5 f J i M l d J i f m g 8 t 8 I K Z w m Y I Y 4 T K C G O k y k p i J y 0 y I K V 2 m J O b W Z W 6 J u X O Z O 2 L u X e a e m I 8 u 8 3 F m z K J N A K z v W b 2 9 l 1 W S T E 0 q D U Y s b e p x Y / 3 V W W J 7 6 D b j G c f h A c E s N 8 q A Y J Y Y 5 Z B g l h U l E M x S o h w R z P K h D A l m y V C O C W a Z U E 4 I Z m l Q v i e Y 5 U B 5 Q z B L g D I m O G Z w Q n D C Y D b R f I Y z g p m Y y 5 x g p u T y A 8 F M x m V B M N N w K Q g W f F E J l u 1 z w q V b E s x 0 W 9 4 S z E R b 3 h H M F F v e E 8 z k W n 4 k 2 G p 1 M w b 1 u V t / Z i x a d A t G d K 3 7 M h j l t e 7 M Y O T X u j e D 0 W D r 7 g x G i K 3 7 M x g 1 t u 7 Q Y C T Z u k e D 0 W X r L o 3 c g / s 0 G I W 2 7 t R g Z N q 6 V 4 P R a n O 3 t l z i c g n n H t y J w U i 3 d S 8 G o 9 / W 3 R i M i F v 3 Y z B K b t 2 R w c i 5 d U 8 G o + n W X R m M s F v 3 Z T D q b t 2 Z w U i 8 d W 8 G o / P W 3 R m M 2 F v 3 Z z C K f 3 i H x l w o o q B 2 K M k q 5 c c q p U 2 y R v A a g 9 c J X m f w B s E b D N 4 k e J P B W w R v M X i b 4 G 0 G 7 x C 8 w + B d g n c Z / J r g 1 w z e I 3 i P w V 2 C u w z e J 3 i f w Q c E H z D 4 k O B D B v c I 7 j H 4 i O A j B h 8 T f M z g E 4 J P G H x K 8 C m D z w g + Y / A 5 w e c M v i D 4 g s G X B F 8 y + I r g K w Z f E 3 z 9 8 P b q i g 6 M 6 p h G V 5 l + t f Q Y t 8 a 5 d Z d b 5 9 y G y 2 1 w b t P l N j m 3 5 X J b n N t 2 u W 3 O 7 b j c D u d 2 X W 6 X c 6 9 d 7 j X n 9 l x u j 3 N d l + t y b t / l 9 j l 3 4 H I H n D t 0 u U P O 9 V y u x 7 k j l z v i 3 L H L H X P u x O V O O H f q c q e c O 3 O 5 M 8 6 d u 9 w 5 5 y 5 c 7 o J z l y 5 3 y b k r l 7 v i 3 L X L W d m f c w t R f g T 9 O Q I / u z 6 v z y 2 z F K b 2 8 6 z F k o m B + g k V j d o T K 9 z 1 w 2 U F M 2 R g E P I h 2 o U g Q u 5 D e w 9 E y H O U 1 U j I a W i f g Q j 5 C + 0 u E C F X o T 0 F I u Q l t J N A h B y E 9 g + I k G / Q r g E R c g v a K y A S s 3 k w C D k D 7 Q s Q S d n 8 G Y R c g P Y A i F D t 1 5 U f E a r 4 u t 4 j Q n V e V 3 l E B J t w g 1 B N L 6 t l Y Y t S G o T q t 6 7 e i F D V 1 j U b E a r V u l I j Q h V a 1 2 d E 2 t y o a 0 N L P 8 7 H a r 3 1 3 1 q B 5 a A S h 9 a F B e m j F j 2 Z q C j z P R U y 5 o C I L I F Q 4 f o v w V q S S o 4 W w I C I 4 G + C R B Q m 6 l T 9 l 2 A r 3 E q 0 9 Y 1 M p 3 z 8 U y V W 2 0 K x B t R C o Q 7 Z T U 2 V Q G 0 L B T q i F o o z p B Y K c 0 w t H C 4 b K w r y P b V Q j D d s b q Z K h P W d T 5 U A b Q s n k 8 0 i i i 9 j U z J V o r M t F N 0 H a q H g C j Z T U y W 0 e o K m S m S 2 h R P N p h k F V l I L x X V L L R T W H b V Q V P f U Q k F 9 n F X f n G G d v T O 4 r r G o M 6 q t u r I i Q h V V 1 1 N E q I 7 q K o o I V U 9 d O x G h m q k r J i J U K X W d R I T q o 6 6 O i F B V 1 D U R E a q F u h I i Q h V Q 1 z 9 E q O 7 p q o c I V T t d 6 x C h G q c r H C J U 2 X R d Q 4 T q m a 5 m i F A V 0 z U M E a p d u n I h Q h V L 1 y t E q E 7 p K o U I V S d d m x C h m q Q r E i J U i X Q d Q o T q j 6 4 + i F D V 0 T U H E a o 1 u t I g c s 1 W k O r C g J e F p D e u N u I + H r H Z s 6 m v m G 6 V / v X N V T m s u B O T x 1 p F p 5 A K 9 Y X y B g S x X w C K a r y q d i C 8 o j F 7 Y h S p R 6 W Q B t k w S k M M 5 k 9 i h Y h R f Z z M p k I 9 5 T 0 B + V C A Q R Y P v y / M 4 G 6 G S d h 8 U p s K / U 2 j q Z t V P P 2 U u r o 1 a f x l K p j 6 5 Z r F S P 9 y 3 W K U A X L D Y p Q D c t N i l A V y y 2 K U B 3 L b Y p Q J c s d i l A t y 1 2 K U D f K 1 x S g f 5 J 7 F K C N k 1 2 K U E 3 L f Y p Q V 8 s B i l B f y 0 G K U G b J n M c o N e W Q x y g 5 5 b D H K D 3 l i M c o Q e W o x y h F 5 Z j H K E n l u M c o T e W E x y h R 5 a T H K F X l l M c o W e W 0 x 4 8 h Q y N u F n 4 8 N G 9 r P u Y H z c S N c Y z D p I l x n M E k j 3 G A w q S P c Z D A J J N x i M G k k 3 G Y w y S T c Y T A p J d x l M I k l f M 1 g 0 k u 4 x 2 C S T N h l M K k m 3 G c w C S c 8 Y D B p J z x k M M k n 7 D G Y F B Q e M Z h E F B 4 z m H Q U n j C Y p B S e M p j U F J 4 x m A Q V n j O Y N B V e M J h k F V 4 y m J Q V X j G Y x B V e M 9 g 6 f t z a K q s m 6 q c o A y Y u s U Y o a U u s E 0 r S E h u E a m U 9 9 T b 0 N x k T A Z 7 v C Z A e X j q G o b e 5 7 A 0 g 8 B U u x 5 H w b r N J P E Q I W + A J / b 0 H e s l J 4 a k 3 g L I Y A 6 m 3 Z u A u R 2 + p v 8 y 1 X 8 x v 0 R V J n W K b U B K n 2 C G U t C l 2 C S V p i t e E k j L F H q E k T N E l l H Q p 9 g k l W Y o D Q k m V 4 p B Q E q X o E U q a F E e E k i T F M a G k S H F C K A l S n B J K e h R n h J I c x T m h p E Z x Q S i J U V w S S l o U V 4 S S F M U 1 o f U j l x R 9 H + i P E L 5 5 2 F K Z Q C A H 0 H X N v 7 K H q 9 R C q a 5 R C y W 6 T i 2 U 5 g a 1 c L P b p B a K a I t a K J 5 t a q F o d q i F Y t m l F o r k N b V Q H H v U Q l F 0 q Y V i 2 K c W i u C A W r j 4 h 9 T C R e 9 R C x f 7 i F q 4 y M f U w s U 9 o R Y u 6 i m 1 c D H P q I W L e E 4 t X L w L a u G i X V I L F + u K W r h I 1 + x 6 l d O q X J Z a M u B L J o 3 j w i 1 F 5 a 9 + q Q + T 2 K D L 3 m 0 k x 9 l E e m h 3 v F s s a T k U r i E C c k S O G 6 o u L 2 s N 6 I 5 z R h C 0 X Y K G X w J t m K D h m E B b J m h 4 J t C m C R q u C b R t g o Z v A m 2 c o O G c Q F s n a H g n 0 O Y J G u 4 J t H 2 C h n 8 C b a C g 4 a B A W y h o e C j Q J g o a L g q 0 j Y K G j w J t p K D h p E B b K W h 4 K d B m C h p u C r S d g o a f A m 2 o o O G o Q F s q a H g q 0 K Y K G q 4 K t K 2 C h q 8 C b a y g 4 a x A W y t o e C v Q 5 g o a 7 g q 0 v Y K G v w J t s I A 5 L P y k g C V H F h P w J u k Q i v h e v b Q 0 9 K X v h Z B C g d V G t S O B S h 9 M V O l x Z Z u r r r N p / m 7 a L 5 K p b u j C p 6 J C k k d F h C X P O b 9 + A 3 F w r 8 u d f g 1 E X Q T r Y y O 2 f U N k 7 E v 8 p O 5 e w u n Z 4 z 1 7 s 7 b B J N k Q 4 s d u R H e o 7 8 S 0 5 q 5 T d e o 9 1 i m X U T y E q m d f N + r R 1 2 f g N i G z Y O w L 9 f 6 t P 5 G Z / g Q F h T P C x m u w u e l T j 7 E 6 Z X 4 A Q 3 D 6 m W Z L v w I J 3 H R s P 9 N E L Q T 6 G Z r b O f b z 2 A 9 g V r 9 R 0 6 2 A m f f U q 4 7 d 6 X X P 3 5 z V F W + z O Z C u Y C / t d J v s 8 Y z X 9 s a u m e R s j h t k X M z s c z e X K C C c 1 U / S m l Q g 6 R 5 V K x p F U D R D i 2 w k E / + O e l q g 2 Q + L R a Z f Y j I P 2 e a j 5 P F E 3 f 1 H 9 S T A Z f e 6 M / 4 G 0 1 5 3 b g H P / Y J G o B r N + B L / + A W u f Z G x n i d z C 7 C e l U S r h h L o R R a P C j 9 R D 6 T G t 1 m B B l X 4 9 8 J 7 0 v 3 2 x R P 1 9 o 5 + d 3 2 S m n d Z R Y 7 r L / T b Y 0 / 6 E M e s j 3 0 g + t R b w w K I K Z + q X / e Y 7 5 C o t 9 i U C z Z B W W / 1 I m o 2 C X X N 1 K Y 4 k r C s w 4 v M G 2 a g w t 1 G N 1 E O w 8 h f a b z I n B V J r B 7 e z 6 b d b 5 / P W s g s B c V 1 2 j h 5 q 8 9 7 0 c b l i s l b G K 2 F 7 r f 9 K B 3 J + 2 b q m F d U c Z V 7 v k q W E 8 C 9 V v g h q N d X 0 6 w y 9 B L u V r z 1 c S b U 9 G T K A A Z j b w M / + 6 b w p f A G W X a z s u g 8 z j n M 1 e 6 c F X 9 A j R e h H g D + 7 S + r o 8 c 6 q n 3 S d M S j 9 p B a r d h N / 3 6 g x y k K 6 l S 9 4 R e D 7 P s D z L M 4 u x 0 U 4 N 8 s v v t 8 q d P 8 f x P z B + c v V j p / X H l 1 9 G r p m 7 X q / 1 R 8 t v C 7 h d 8 v f L X Q W f j T w j c L O w u 9 h b O F Y C F b + O v C 3 x b + 3 v l H 5 1 + d f 3 f + Y 7 p + + k l 1 z m 8 W n J / O f / 8 H r v V p 2 A = = &lt; / l a t e x i t &gt; 1 = 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C m 4 O I r F r 0 2 J t 0 L / K m j A / Q m L U b z 4 = " &gt; A A A y O X i c j V v L b t z I F d X k O V F e M w m Q T T Z E Z G N m A l l Q 2 8 Y k m w C j t 2 S 1 p N b 7 4 f Y Y b P Z t N i 2 + x K q m J D d 6 f i P b 5 D v y J V l m F 2 S b H 8 i t K h b v L Y r S R I A l 1 j n F y 2 L V u X V P s + l B H k d C L i / / 8 5 M f / P B H P / 7 J T z / 9 2 f z P f / H L X / 3 6 s 8 9 / c y a y S R H A a Z D F W X E x 8 A X E U Q q n M p I x X O Q F + M k g h v P B 9 Z r i z 0 s o R J S l J / I + h 3 e J H 6 b R K A p 8 i d C 7 f o x d h / 7 7 z l + W l 7 5 + / 9 n C 8 t K y / v E e H n S q g 4 W 5 6 q f 3 / v P O 7 / r D L J g k k M o g 9 o V 4 2 1 n O 5 b u p X 8 g o i G E 2 3 5 8 I y P 3 g 2 g / h L R 6 m f g L i 3 V S P e u Y 9 R 2 T o j b I C / 6 X S 0 y g / Y + o n Q t w n A + y Z + H I s m p w C 2 7 i 3 E z n 6 8 7 t p l O Y T C W l g L j S a x J 7 M P D U F 3 j A q I J D x P R 7 4 Q R H h W L 1 g 7 B d + I H G i 5 u f n n 6 s f b 3 / j 3 N t b O d n 2 1 j c 2 d / Z 3 T n Y O 9 o 8 9 T c 2 3 j W Q R / 6 r 7 E I u D Z I Y x v D 2 / u P Y E X g j n W X j Z y A v 8 3 B y r W y 5 g B E U R p a E a 1 T A q I 2 G 7 j a J w U g D e U Q q 3 Q Z Y k f j q c 9 h G M Y S R n 0 2 k f E u / L L h 5 / N Z s 9 6 B P g Q k B h e 6 3 p V l u / I g r H d b A j 1 W j r J b P c 9 j n J 8 r Y e g 0 z K L L G d V n X r Q b / q v n 3 b z X + s x 8 D 2 G D z W I 7 A 9 g s d 6 D G 2 P o e q B y 7 C N d x e r O / R 8 D / u r V Y c R J s v Q w 7 l J 3 B h 4 r M D Z 2 8 4 7 j D I Y e Q s d F Q S j b O p F M a u G m o J F L 8 5 u o X g R Y O o t z f c x p J 5 W G C 1 0 p m Y B v + t j a 6 o D t J 2 O w 4 2 k H y 9 5 m y g G I T F j 1 N o L t W L I m 4 i b N u J m M 6 K m 5 W 1 m r 7 n w s r q q 8 G w n D + + o a r y 0 Z 9 x M / C G d s v B q 4 f W D 0 x b r c + z R K x 7 q t b 6 d Y 6 P q J 6 c D l W 8 G X 6 W A M x 8 t A e y E m L O P 7 d n H L W c f 2 b N 0 R t 9 m d Z Y t 1 R N j r i 7 0 z N Q 5 + M j U N A O O C 4 B m S B Z v 4 d X D i D R r L P a r h 7 H 9 1 A N c B H V y y 5 T B j b n n j Z u l 7 7 6 0 o b 9 6 O k i U e r m P l B y D i A S L k 2 O g L 1 W k / z P Q J M + h 8 N R o T J C N e j C m h 7 M C K 1 7 h 3 9 L q N Y K 9 e P H C L 7 N o 6 E 2 E 2 u C i k Z d n Q k R Y k k z o P P Y x A a v 4 j y 6 s r z b l H P O x Z a Y U Y 0 6 v + j y u j 0 d u s g q 0 V g d a + 9 5 A e M 9 p C H o n N 3 2 r 6 d Z w P S I U n K V t q B c v H h U b j s 6 P w w y L 0 D h p u U / k z O j q T k / e K A v 1 4 E 5 X b K i V l l A 2 b e z 1 8 C b q W E 9 v K S f O S S v f e 9 K D S U X 1 y u r O m f o U a o a r j p 5 a F H N + U 7 2 9 + v y e e 7 6 9 0 / o C O G p 1 / O i A K 8 F B F C u x x u o A y w J 2 U E d V v F G c Z Y W m 9 Z H h 9 W H V A a l B M u 0 0 a 5 Y s M B F m 0 7 7 y D 4 E f T 9 e b H U o / j o a 8 w 3 t z X C R T Q 8 0 e h A Q h 2 0 / Q z K y + I 8 i F q p S 5 i O I s r a r c E Y b I E q / 0 i 8 j H b L X 6 B u l P V e Q 7 m W Z F g l G f 9 R F 6 N r P T W T R o n 5 i B y w y I C V w m I G b o M k N i w G W A m J H L j I g J X S Y k Z u w y Y 2 I i l 4 m I + e A y H 4 i 5 d p l r Y m K X i W d a x k X i R Q I z F s 3 6 8 F 5 t d m Y F F 7 0 P E y G 9 Y Z Z + I T 3 l l 1 G O 9 2 r n c R b G S 6 r Y q R s 7 p a t m L p M R k 7 t M T s y N y 9 w Q U 7 h M Q Y x w G U G M d B l J z M R l J s S U L l M S c + s y t 8 T c u c w d M f c u c 0 / M R 5 f 5 O D N m 0 S Y A 1 v e s 3 t 7 L K k m m J p U G I 5 Y 2 9 b i x / u o s s T 1 0 m / G M 4 / C A Y J Y b Z U A w S 4 x y S D D L i h I I Z i l R j g h m + V C G B L N k K M c E s 0 w o J w S z N C g / E M x y o L w m m C V A G R M c M z g h O G E w m 2 g + w x n B T M x l T j B T c n l D M J N x W R D M N F w K g g V f V I J l + 5 x w 6 Z Y E M 9 2 W t w Q z 0 Z Z 3 B D P F l v c E M 7 m W H w m 2 W t 2 I Q X 3 u 1 p 8 Z i x b d g h F d 6 7 4 M R n m t O z M Y + b X u z W A 0 2 L o 7 g x F i 6 / 4 M R o 2 t O z Q Y S b b u 0 W B 0 2 b p L I / f o P g 1 G o a 0 7 N R i Z t u 7 V Y L T a 3 K 0 t l 7 h c w r l H d 2 I w 0 m 3 d i 8 H o t 3 U 3 B i P i 1 v 0 Y j J J b d 2 Q w c m 7 d k 8 F o u n V X B i P s 1 n 0 Z j L p b d 2 Y w E m / d m 8 H o v H V 3 B i P 2 1 v 0 Z j O I f 3 6 E x F 4 o o q B 1 K s k L 5 s U J p k 6 w S v M r g N Y L X G L x O 8 D q D N w j e Y P A m w Z s M 3 i J 4 i 8 H b B G 8 z e I f g H Q a / I f g N g 3 c J 3 m V w l + A u g / c I 3 m P w P s H 7 D D 4 g + I D B P Y J 7 D D 4 k + J D B R w Q f M f i Y 4 G M G n x B 8 w u B T g k 8 Z f E b w G Y P P C T 5 n 8 A X B F w y + J P i S w V c E X z 2 + v b q i A 6 M 6 p t E V p l 8 t P c a t c m 7 N 5 d Y 4 t + 5 y 6 5 z b c L k N z m 2 6 3 C b n t l x u i 3 P b L r f N u R 2 X 2 + H c G 5 d 7 w 7 l d l 9 v l X N f l u p z b c 7 k 9 z u 2 7 3 D 7 n D l z u g H M 9 l + t x 7 t D l D j l 3 5 H J H n D t 2 u W P O n b j c C e d O X e 6 U c 2 c u d 8 a 5 c 5 c 7 5 9 y F y 1 1 w 7 t L l L j l 3 5 X J W 9 m f c Q p Q f Q X + O w M + u y / W 5 Z Z b C 1 H 6 e t V g y M V A / o a J R e 2 K F u 3 6 4 r G C G D A x C P k S 7 E E T I f W j v g Q h 5 j r I a C T k N 7 T M Q I X + h 3 Q U i 5 C q 0 p 0 C E v I R 2 E o i Q g 9 D + A R H y D d o 1 I E J u Q X s F R G I 2 D w Y h Z 6 B 9 A S I p m z + D k A v Q H g A R q v 2 6 8 i N C F V / X e 0 S o z u s q j 4 h g E 2 4 Q q u l l t S x s U U q D U P 3 W 1 R s R q t q 6 Z i N C t V p X a k S o Q u v 6 j E i b G 3 V t a O n H + V i t t / 5 b K 7 A c V O L Q u r A g f d S i J x M V Z b 6 p Q s Y c E J E l E C p c / y V Y S 1 L J 0 Q I Y E B H 8 T Z C I w k S d q v 8 S b I V b i b a + k e m U j 3 + q x G p b K N a A W i j U I b u p q R K o b a F A R 9 R C c Y b U Q m G O q Y X D Z W N F Q X 6 g F o r x m s 3 N V I m w v v O p E q B t 4 W S y W U T x Z W x K p k p 0 t o W i u 6 E W C q 5 g M z V V Q q s n a K p E Z l s 4 0 W y a U W A l t V B c t 9 R C Y d 1 R C 0 V 1 T y 0 U 1 M d Z 9 c 0 Z 1 t k 7 g + s a i z q j 2 q o r K y J U U X U 9 R Y T q q K 6 i i F D 1 1 L U T E a q Z u m I i Q p V S 1 0 l E q D 7 q 6 o g I V U V d E x G h W q g r I S J U A X X 9 Q 4 T q n q 5 6 i F C 1 0 7 U O E a p x u s I h Q p V N 1 z V E q J 7 p a o Y I V T F d w x C h 2 q U r F y J U s X S 9 Q o T q l K 5 S i F B 1 0 r U J E a p J u i I h Q p V I 1 y F E q P 7 o 6 o M I V R 1 d c x C h W q M r D S J X b A W p L g x 4 W U h 6 4 2 o j 7 u M R m z 2 b + o r p V u l f 3 1 y V w 4 o 7 N n m s V X Q C q V B f K K 9 D E P s F o K j G K 2 o H w i s a s y d G k X p U C m m Q D a M 0 x G D + J F a I G N X H y W w q 1 F P e Y 5 C P B R h k 8 f D 7 w g z u Z p i E z S e 1 q d D f N J q 6 W c X T T 6 m r W 5 P G X 6 a C q V + u W o z 0 L 9 c s R h k g 1 y 1 G O S A 3 L E Z Z I D c t R n k g t y x G m S C 3 L U a 5 I H c s R t k g 3 1 i M 8 k H u W o w y Q n Y t R j k h 9 y x G W S H 3 L U Z 5 I Q 8 s R p k h e x a j 3 J C H F q P s k E c W o / y Q x x a j D J E n F q M c k a c W o y y R Z x a j P J H n F q N M k R c W o 1 y R l x a j b J F X F j O O D I W 8 V f j 5 2 L C h / Z w b O B 8 3 w l U G k y 7 C N Q a T N M J 1 B p M 6 w g 0 G k 0 D C T Q a T R s I t B p N M w m 0 G k 1 L C H Q a T W M I 3 D C a 9 h L s M J s m E X Q a T a s I 9 B p N w w n 0 G k 3 b C A w a T f M I e g 0 l B 4 S G D S U T h E Y N J R + E x g 0 l K 4 Q m D S U 3 h K Y N J U O E Z g 0 l T 4 T m D S V b h B Y N J W e E l g 0 l c 4 R W D r e P H r a 2 y a q J + i j J g 4 h K r h J K 2 x B q h J C 2 x T q h W 1 n N v X X + T M R H g + Z 4 A 6 e G l Y x h 6 G 4 v e A A J f 4 X I c C e 8 2 m 8 R D h L A F n t D f e 6 C X n B S e e g M o i z G Q e m s G 7 n L 0 l v r L X P v F / C Z d k d Q p t g g l c Y p t Q k m b Y o d Q k q Z 4 Q y g p U + w S S s I U X U J J l 2 K P U J K l 2 C e U V C k O C C V R i h 6 h p E l x S C h J U h w R S o o U x 4 S S I M U J o a R H c U o o y V G c E U p q F O e E k h j F B a G k R X F J K E l R X B F a P 3 J J 0 f e B / g j h m 4 c t l Q k E c g B d 1 / w r e 7 h C L Z T q K r V Q o m v U Q m m u U w s 3 u w 1 q o Y g 2 q Y X i 2 a I W i m a b W i i W H W q h S N 5 Q C 8 W x S y 0 U R Z d a K I Y 9 a q E I 9 q m F i 3 9 A L V z 0 H r V w s Q + p h Y t 8 R C 1 c 3 G N q 4 a K e U A s X 8 5 R a u I h n 1 M L F O 6 c W L t o F t X C x L q m F i 3 T F r l c 5 r c p l q S U D v m T S O C 7 c U l T + 6 p f 6 M I k N u u j d R n K c T a S H d s e 7 x Z K W Q + E a I i B H 5 L i h 6 v K y 1 o D u + M A I g r Z L 0 P B L o A 0 T N B w T a M s E D c 8 E 2 j R B w z W B t k 3 Q 8 E 2 g j R M 0 n B N o 6 w Q N 7 w T a P E H D P Y G 2 T 9 D w T 6 A N F D Q c F G g L B Q 0 P B d p E Q c N F g b Z R 0 P B R o I 0 U N J w U a C s F D S 8 F 2 k x B w 0 2 B t l P Q 8 F O g D R U 0 H B V o S w U N T w X a V E H D V Y G 2 V d D w V a C N F T S c F W h r B Q 1 v B d p c Q c N d g b Z X 0 P B X o A 0 W M I e F n x S w 5 M h i A t 4 k H U I R 3 6 u X l o a + 9 L 0 Q U i i w 2 q h 2 J F D p g 4 k q P a 5 s c 9 V 1 N s 3 f T / t F M t U N X f h U V E j y q I i w 5 D n n 1 2 8 g D u 5 1 u d O v g a i L Y H 1 s x L Z v i I x 9 i Z / U 3 U s 4 P X u 8 Z 2 / W N p g k G 0 L 8 1 I 3 o D v W d m N a D 6 1 S d e k 9 1 y m U U D 6 H q 2 d e N e v T 1 G b h N y C w Y + 0 K 9 f + t P Z K Y / Q U H h j L D x G m x u + t R j r E 5 5 O I A h O P 1 M s 6 V f g Q R u O r a f a a I W A v 0 M z e 0 c + 3 n s B z C r 3 6 j p V s D M e + 5 V x + 7 0 u u d v z O q K t 9 E c S F e w l 3 a 6 T f Z o x m t 7 Y 9 d M c j b H D T I u Z v a 5 m 0 s U E M 7 q J 2 l N K p B 0 j 6 o V j S I o m q F F N p K J f 0 c 9 L d D s h 8 U i 0 y 8 x m Y d s D 6 P k 8 U T d / U f 1 J M B l d 7 s z / g b T b v f B A p 7 5 B Y 1 A N Z r x J f 7 x C 1 z 7 I m M 9 j x 8 s w F p W E q 0 a S q D n W T w q / E Q 9 k B r f Z g U a V O H f C + 9 Z 9 9 u X z 9 T b O / r d 9 U l q 3 m U V O a 6 / 0 G + P P e t D H L M + 9 o H o c 2 8 V C y C m f K p + 3 W O + Q 6 L e Y l M u 2 A R l v d W L q N k k 1 D V T m + J I w q I O L z J v m I E K d x t d R z k M I 3 + p 8 S J z V i S x e n g / m 3 a / X Z 6 1 k F k K i u u 0 c f J W n / e y j c s V k 7 c w W g v d b / t R O p L 3 z d Q x r 6 j i K v d 8 l S z H g H u t 8 E N Q r 6 + m W W X o J d w t e W v j T K j p y Z Q B D M b e O n 7 2 T e E L 4 Q 2 y 7 H p p 3 n m c c 5 C r 3 T k r / o g a L 0 I 9 A P z b X 1 R H T 3 V U + 6 T p i E f t I b V a s Z v + / U i P E x T U i X r D L w b Z 9 w e Y Z 3 F 2
O y j A v 5 5 / / 9 l C p / n / J h 4 e n L 1 c 6 n y 9 9 P r w 9 c I 3 q 9 X / q f h 0 7 v d z f 5 j 7 c q 4 z 9 6 e 5 b + a 2 5 3 p z p 3 P B 3 M 3 c X + f + N v f 3 z j 8 6 / + r 8 u / M f 0 / U H n 1 T n / H b O + e n 8 9 3 + i + m p P &lt; / l a t e x i t &gt;
1 = 0 .6 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X J J U h O 8 2 8 D z h t M U q H H 6 k 4 j 8 D q e 0 = " &gt; A A A y O X i c j V v L b t z I F d V M X h P l N Z M A 2 W R D R D Z m J p A F t W 0 k 2 Q Q Y v S W r J b X e D 7 f H Y L N v s 2 n x J V Y 1 J b n R 8 x v Z J t + R L 8 k y u y D b / E B u V b F 4 b 1 G U J g I s s c 4 p X h a r z q 1 7 m k 0 P 8 j g S c n n 5 n 5 9 8 + o M f / u j H P / n s p / M / + / k v f v m r z 7 / 4 9 Z n I J k U A p 0 E W Z 8 X F w B c Q R y m c y k j G c J E X 4 C e D G M 4 H 1 2 u K P y + h E F G W n s j 7 H N 4 l f p h G o y j w J U L v + j F 2 H f r v O 3 9 Z X n r 1 / v O F 5 a V l / e M 9 P O h U B w t z 1 U / v / R e d 3 / a H W T B J I J V B 7 A v x t r O c y 3 d T v 5 B R E M N s v j 8 R k P v B t R / C W z x M / Q T E u 6 k e 9 c x 7 j s j Q G 2 U F / k u l p 1 F + x t R P h L h P B t g z 8 e V Y N D k F t n F v J 3 L 0 5 3 f T K M 0 n E t L A X G g 0 i T 2 Z e W o K v G F U Q C D j e z z w g y L C s X r B 2 C / 8 Q O J E z c / P P 1 c / 3 v 7 G u b e 3 c r L t r W 9 s 7 u z v n O w c 7 B 9 7 m p p v G 8 k i / l X 3 I R Y H y Q x j e H t + c e 0 J v B D O s / C y k R f 4 u T l W t 1 z A C I o i S k M 1 q m F U R s J 2 G 0 X h p A C 8 o x R u g y x J / H Q 4 7 S M Y w 0 j O p t M + J N 5 X X T z + e j Z 7 0 C f A h Y D C 9 l r T r b Z + R R S O 6 2 B H q t H W S 2 a 5 7 X O S 5 W 0 9 B p m U W W I 7 r e r W g 3 7 V f f u 2 m / 9 Y j 4 H t M X i s R 2 B 7 B I / 1 G N o e Q 9 U D l 2 E b 7 y 5 W d + j 5 H v Z X q w 4 j T J a h h 3 O T u D H w W I G z t 5 1 3 G G U w 8 h Y 6 K g h G 2 d S L Y l Y N N Q W L X p z d Q v E i w N R b m u 9 j S D 2 t M F r o T M 0 C f t f H 1 l Q H a D s d h x t J P 1 7 y N l E M Q m L G q L U X a s W Q N x E 3 b c T N Z k R N y 9 v M X n P h Z X V V 4 d l O H t 5 R 1 X h p z 7 i Z + E M 6 Z e H V w u s H p y 3 W 5 9 i j V z z U a 3 0 7 x 0 b V T 0 4 H K t 8 M v k o B Z z 5 a A t g J M W c f 2 7 O P W 8 4 + s m f p j L 7 N 6 i x b q i f G X F 3 o m a l z 8 J G p a Q Y c F w D N k C z e w q u H E W n W W O x X D 2 P 7 q Q e 4 C O r k l i m D G 3 P P G z d L 3 3 1 l Q 3 / 9 d J A o 9 X I f K T k G E Q k W J 8 d A X 6 l I / 2 e g S Z 5 D 4 a n R m C A b 9 W B M D 2 c F V r z C v 6 X V a w R 7 8 e K F X 2 b R 0 J s I t c F F I y / P h I i w J J n Q e e x j A l b x H 1 1 Y X 2 3 K O e Z j y 0 w p x p x e 9 X l c H 4 / c Z B V o r Q 6 0 9 r 2 B 8 J 7 T E P R O b v p W 0 6 3 h e k Q o O E v b U C 9 e P C o 2 H J 0 f h x k W o X H S c p / I m d H V n Z 6 8 U R b q w Z 2 u 2 F A r L a F s 2 t j r 4 U 3 U s Z 7 e U k 6 c k 1 a + 9 6 Q H k 4 r q l d W d M / U p 1 A x X H T 2 1 K O b 8 p n p 7 9 f k 9 9 3 x 7 p / U F c N T q + N E B V 4 K D K F Z i j d U B l g X s o I 6 q e K M 4 y w p N 6 y P D 6 8 O q A 1 K D Z N p p 1 i x Z Y C L M p n 3 l H w I / n q 4 3 O 5 R + H A 1 5 h / f m u E i m h p o 9 C A l C t p + g m V l 9 R 5 A L V S l z E c V Z W l W 5 I w y R J V 7 p F 5 G P 2 W r 1 D d K f q s h 3 M s 2 K B K M + 6 y P 0 b G a n s 2 j Q P j E D l x k Q E 7 h M Q M z Q Z Y b E g M s A M S O X G R E T u k x I z N h l x s R E L h M R 8 8 F l P h B z 7 T L X x M Q u E 8 + 0 j I v E i w R m L J r 1 4 b 3 a 7 M w K L n o f J k J 6 w y z 9 U n r K L 6 M c 7 9 X O 4 y y M l 1 S x U z d 2 S l f N X C Y j J n e Z n J g b l 7 k h p n C Z g h j h M o I Y 6 T K S m I n L T I g p X a Y k 5 t Z l b o m 5 c 5 k 7 Y u 5 d 5 p 6 Y j y 7 z c W b M o k 0 A r O 9 Z v b 2 X V Z J M T S o N R i x t 6 n F j / d V Z Y n v o N u M Z x + E B w S w 3 y o B g l h j l k G C W F S U Q z F K i H B H M 8 q E M C W b J U I 4 J Z p l Q T g h m a V B + I J j l Q H l N M E u A M i Y 4 Z n B C c M J g N t F 8 h j O C m Z j L n G C m 5 P K G Y C b j s i C Y a b g U B A u + q A T L 9 j n h 0 i 0 J Z r o t b w l m o i 3 v C G a K L e 8 J Z n I t P x J s t b o R g / r c r T 8 z F i 2 6 B S O 6 1 n 0 Z j P J a d 2 Y w 8 m v d m 8 F o s H V 3 B i P E 1 v 0 Z j B p b d 2 g w k m z d o 8 H o s n W X R u 7 R f R q M Q l t 3 a j A y b d 2 r w W i 1 u V t b L n G 5 h H O P 7 s R g p N u 6 F 4 P R b + t u D E b E r f s x G C W 3 7 s h g 5 N y 6 J 4 P R d O u u D E b Y r f s y G H W 3 7 s x g J N 6 6 N 4 P R e e v u D E b s r f s z G M U / v k N j L h R R U D u U Z I X y Y 4 X S J l k l e J X B a w S v M X i d 4 H U G b x C 8 w e B N g j c Z v E X w F o O 3 C d 5 m 8 A 7 B O w x + Q / A b B u 8 S v M v g L s F d B u 8 R v M f g f Y L 3 G X x A 8 A G D e w T 3 G H x I 8 C G D j w g + Y v A x w c c M P i H 4 h M G n B J 8 y + I z g M w a f E 3 z O 4 A u C L x h 8 S f A l g 6 8 I v n p 8 e 3 V F B 0 Z 1 T K M r T L 9 a e o x b 5 d y a y 6 1 x b t 3 l 1 j m 3 4 X I b n N t 0 u U 3 O b b n c F u e 2 X W 6 b c z s u t 8 O 5 N y 7 3 h n O 7 L r f L u a 7 L d T m 3 5 3 J 7 n N t 3 u X 3 O H b j c A e d 6 L t f j 3 K H L H X L u y O W O O H f s c s e c O 3 G 5 E 8 6 d u t w p 5 8 5 c 7 o x z 5 y 5 3 z r k L l 7 v g 3 K X L X X L u y u W s 7 M + 4 h S g / g v 4 c g Z 9 d l + t z y y y F q f 0 8 a 7 F k Y q B + Q k W j 9 s Q K d / 1 w W c E M G R i E f I h 2 I Y i Q + 9 D e A x H y H G U 1 E n I a 2 m c g Q v 5 C u w t E y F V o T 4 E I e Q n t J B A h B 6 H 9 A y L k G 7 R r Q I T c g v Y K i M R s H g x C z k D 7 A k R S N n 8 G I R e g P Q A i V P t 1 5 U e E K r 6 u 9 4 h Q n d d V H h H B J t w g V N P L a l n Y o p Q G o f q t q z c i V L V 1 z U a E a r W u 1 I h Q h d b 1 G Z E 2 N + r a 0 N K P 8 7 F a b / 2 3 V m A 5 q M S h d W F B + q h F T y Y q y n x T h Y w 5 I C J L I F S 4 / k u w l q S S o w U w I C L 4 m y A R h Y k 6 V f 8 l 2 A q 3 E m 1 9 I 9 M p H / 9 U i d W 2 U K w B t V C o Q 3 Z T U y V Q 2 0 K B j q i F 4 g y p h c I c U w u H y 8 a K g v x A L R T j N Z u b q R J h f e d T J U D b w s l k s 4 j i y 9 i U T J X o b A t F d 0 M t F F z B Z m q q h F Z P 0 F S J z L Z w o t k 0 o 8 B K a q G 4 b q m F w r q j F o r q n l o o q I + z 6 p s z r L N 3 B t c 1 F n V G t V V X V k S o o u p 6 i g j V U V 1 F E a H q q W s n I l Q z d c V E h C q l r p O I U H 3 U 1 R E R q o q 6 J i J C t V B X Q k S o A u r 6 h w j V P V 3 1 E K F q p 2 s d I l T j d I V D h C q b r m u I U D 3 T 1 Q w R q m K 6 h i F C t U t X L k S o Y u l 6 h Q j V K V 2 l E K H q p G s T I l S T d E V C h C q R r k O I U P 3 R 1 Q c R q j q 6 5 i B C t U Z X G k S u 2 A p S X R j w s p D 0 x t V G 3 M c j N n s 2 9 R X T r d K / v r k q h x V 3 b P J Y q + g E U q G + U F 6 H I P Y L Q F G N V 9 Q O h F c 0 Z k + M I v W o F N I g G 0 Z p i M H 8 S a w Q M a q P k 9 l U q K e 8 x y A f C z D I 4 u H 3 h R n c z T A J m 0 9 q U 6 G / a T R 1 s 4 q n n 1 J X t y a N v 0 w F U 7 9 c t R j p X 6 5 Z j D J A r l u M c k B u W I y y Q G 5 a j P J A b l m M M k F u W 4 x y Q e 5 Y j L J B v r E Y 5 Y P c t R h l h O x a j H J C 7 l m M s k L u W 4 z y Q h 5 Y j D J D 9 i x G u S E P L U b Z I Y 8 s R v k h j y 1 G G S J P L E Y 5 I k 8 t R l k i z y x G e S L P L U a Z I i 8 s R r k i L y 1 G 2 S K v L G Y c G Q p 5 q / D z s W F D + z k 3 c D 5 u h K s M J l 2 E a w w m a Y T r D C Z 1 h B s M J o G E m w w m j Y R b D C a Z h N s M J q W E O w w m s Y R v G E x 6 C X c Z T J I J u w w m 1 Y R 7 D C b h h P s M J u 2 E B w w m + Y Q 9 B p O C w k M G k 4 j C I w a T j s J j B p O U w h M G k 5 r C U w a T o M I z B p O m w n M G k 6 z C C w a T s s J L B p O 4 w i s G W 8 e P W 1 t l 1 U T 9 F G X A x C V W C S V t i T V C S V p i n V C t r O f e u v 4 m Y y L A 8 z 0 B 0 s N L x z D 0 N h a 9 A Q S + w u U 4 E t 5 t N o m H C G E L P K G / 9 0 A v O S k 8 9 Q Z Q F m M g 9 d Y M 3 O X o L f W X u f a L + U 2 6 I q l T b B F K 4 h T b h J I 2 x Q 6 h J E 3 x h l B S p t g l l I Q p u o S S L s U e o S R L s U 8 o q V I c E E q i F D 1 C S Z P i k F C S p D g i l B Q p j g k l Q Y o T Q k m P 4 p R Q k q M 4 I 5 T U K M 4 J J T G K C 0 J J i + K S U J K i u C K 0 f u S S o u 8 D / R H C N w 9 b K h M I 5 A C 6 r v l X 9 n C F W i j V V W q h R N e o h d J c p x Z u d h v U Q h F t U g v F s 0 U t F M 0 2 t V A s O 9 R C k b y h F o p j l 1 o o i i 6 1 U A x 7 1 E I R 7 F M L F / + A W r j o P W r h Y h 9 S C x f 5 i F q 4 u M f U w k U 9 o R Y u 5 i m 1 c B H P q I W L d 0 4 t X L Q L a u F i X V I L F + m K X a 9 y W p X L U k s G f M m k c V y 4 p a j 8 1 S / 1 Y R I b d N G 7 j e Q 4 m 0 g P 7 Y 5 3 i y U t h 8 I 1 R E C O y H F D 1 e V l r Q H d 8 Y E R B G 2 X o O G X Q B s m a D g m 0 J Y J G p 4 J t G m C h m s C b Z u g 4 Z t A G y d o O C f Q 1 g k a 3 g m 0 e Y K G e w J t n 6 D h n 0 A b K G g 4 K N A W C h o e C r S J g o a L A m 2 j o O G j Q B s p a D g p 0 F Y K G l 4 K t J m C h p s C b a e g 4 a d A G y p o O C r Q l g o a n g q 0 q Y K G q w J t q 6 D h q 0 A b K 2 g 4 K 9 D W C h r e C r S 5 g o a 7 A m 2 v o O G v Q B s s Y A 4 L P y l g y Z H F B L x J O o Q i v l c v L Q 1 9 6 X s h p F B g t V H t S K D S B x N V e l z Z 5 q r r b J q / n / a L Z K o b u v C p q J D k U R F h y X P O r 9 9 A H N z r c q d f A 1 E X w f r Y i G 3 f E B n 7 E j + p u 5 d w e v Z 4 z 9 6 s b T B J N o T 4 q R v R H e o 7 M a 0 H 1 6 k 6 9 Z 7 q l M s o H k L V s 6 8 b 9 e j r M 3 C b k F k w 9 o V 6 / 9 a f y E x / g o L C G W H j N d j c 9 K n H W J 3 y c A B D c P q Z Z k u / A g n c d G w / 0 0 Q t B P o Z m t s 5 9 v P Y D 2 B W v 1 H T r Y C Z 9 9 y r j t 3 p d c / f m N U V b 6 M 5 k K 5 g L + 1 0 m + z R j N f 2 x q 6 Z 5 G y O G 2 R c z O x z N 5 c o I J z V T 9 K a V C D p H l U r G k V Q N E O L b C Q T / 4 5 6 W q D Z D 4 t F p l 9 i M g / Z H k b J 4 4 m 6 + 4 / q S Y D L 7 n Z n / A 2 m 3 e 6 D B T z z C x q B a j T j S / z j F 7 j 2 R c Z 6 H j 9 Y g L W s J F o 1 l E D P s 3 h U + I l 6 I D W + z Q o 0 q M K / F 9 6 z 7 r c v n 6 m 3 d / S 7 6 5 P U v M s q c l x / o d 8 e e 9 a H O G Z 9 7 A P R 5 9 4 q F k B M + V T 9 u s d 8 h 0 S 9 x a Z c s A n K e q s X U b N J q G u m N s W R h E U d X m T e M A M V 7 j a 6 j n I Y R v 5 S 4 0 X m r E h i 9 f B + N u 1 + u z x r I b M U F N d p 4 + S t P u 9 l G 5 c r J m 9 h t B a 6 3 / a j d C T v m 6 l j X l H F V e 7 5 K l m O A f d a 4 Y e g X l 9 N s 8 r Q S 7 h b 8 t b G m V D T k y k D G I y 9 d f z s m 8 K X w h t k 2 f X S v P M 4 5 y B X u 3 N W / A E 1 X o R 6 A P i 3 v 6 i O n u q o 9 k n T E Y / a Q 2 q 1 Y j f 9 + 5 E e J y i o E / W G X w y y 7 w 8 w z + L s d l C A f z 3 / / v O F T v P / T T w 8 O H u 5 1 P n j 0 u v D 1 w v f r F b / p + K z u d / N / X 7 u q 7 n O 3 J / m v p n b n u v N n c 4 F c z d z f 5 3 7 2 9 z f O / / o
/ K v z 7 8 5 / T N d P P 6 n O + c 2 c 8 9 P 5 7 / 8 A D Z 9 q T A = = &lt; / l a t e x i t &gt; 1 = 0 .3&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + A P 2 p e D / O q B 1 K T
x 7 6 Z 5 O g w 9 G p b A = " &gt; A A A y O X i c j V v L b t z I F d V M X h P l N Z M A 2 W R D R D Z m J p A F y T a S b A K M 3 p L V k l
r v h 9 t j s N m 3 2 b T 4 E q u a k t z o + Y 1 s k + / I l 2 S Z X Z B t f i C 3 q l i 8 t 9 i U J g I s s c 4 p X h a r z q 1 7 m k 3 3 8 z g S c n n 5 n 5 9 8 + o M f / u j H P / n s p / M / + / k v f v m r z 7 / 4 9 b n I M 8 6 3 7 5 8 p t 7 e 0 e + u j 1
x k U A Z 0 E W Z 8 V l 3 x c Q R y m c y U j G c J k X 4 C f 9 G C 7 6 N + u K v y i h E F G W n s q H H N 4 l f p h G w y j w J U L v e j F 2 H f j v X / 5 l e e n 1 + 8 8 X l p e W 9 Y 8 3 e 7 B S H S z M V T / d 9 1 + s / L Y 3 y I J x A q k M Y l + I t y v L u X w 3 8 Q s Z B T F M 5 3 t j A b k f 3 P g h v M X D 1 E 9 A v J v o U U + 9 5 4 g M v G F W 4 L 9 U e h r l Z 0 z 8 R I i H p I 8 9 E 1 + O R J N T Y B v 3 d i y H f 3 4 3 i d J 8 L C E N z I W G 4 9 i T m a e m w B t E B Q Q y f s A D P y g i H K s X j P z C D y R O 1 P z 8 / H P 1 4 x 1 s X n j 7 q 6 c 7 3 s b m 1 u 7 B 7 u n u 4 c G J p 6 n 5 t p E s 4 l 9 1 H 2 K x n 0 w x h r f v F z e e w A v h P A s v G 3 q B n 5 t j d c s F D K E o o j R U o x p E Z S R s t 2 E U j g v A O 0 r h L s i S x E 8 H k x 6 C M Q z l d D L p Q e J 9 1 c H j r 6 f T m T 4 B L g Q U t t e 6 b r X 1 K 6 J w V A c 7 V o 2 2 X j L L b Z / T L G / r 0 c + k z B L b a U 2 3 Z v p V 9 + 3 b b v 5 j P f q 2 R / + x H o H t E T z W Y 2 B 7 D F Q P X I Y d v L t Y 3 a H n e 9 h f r T o M M V k G H s 5 N 4 s b A Y w V O 3 6 6 8 w y j 9 o b e w o o J g l C 2 9 K G b V U F O w 6 M X Z H R Q v A k y 9 p f k e h t T T C s O F l Y l Z w O 9 6 2 J r o A G 2 n 4 3 A j 6 c d L 3 h a K Q U j M G L X 2 Q q 0 Y 8 i b i l o 2 4 1 Y y o a X m X 2 W s u v K y u K j z b y c M 7 q h o v 7 R m 3 Y 3 9 A p y y 8 W n g 9 c 9 p i f Y 4 9 e s V D v d a 3 c 2 J U / e R 0 o P L N 4 K s U c O a j J Y C d E H P 2 i T 3 7 p O X s Y 3 u W z u i 7 r M 6 y p X p i z N W F n p k 6 B x + Z m m b A U Q H Q D M n i L b y a j U i z x m K / m o 3 t p x 7 g I q i T W 6 Y M b s 0 9 b 9 4 u f f e V D f 3 1 0 0 G i 1 M t 9 p O Q I R C R Y n B w D f a U i / Z + B x n k O h a d G Y 4 J s 1 o M x P Z w V W P U K / 4 5 W r x H s x Y s X f p l F A 2 8 s 1 A Y X D b 0 8 E y L C k m R C 5 7 G P C V j F f 3 R h f b U p 5 5 i P L T O l G H N 6 1 e d x f T x y k 1 W g 9 T r Q + v c G w n t O Q 9 A 7 u e l b T b e G 6 x G h 4 C x t Q 7 1 4 8 a j Y c H R + H G Z Y h E Z J y 3 0 i Z 0 Z X d 3 r y R l m o m T t d t a F W W 0 L Z t L H X w 5 u o Y z 2 9 p Z w 6 J 6 1 + 7 0 k z k 4 r q l d W d M / U p 1 A x X H T 2 1 K O b 8 p n q 7 9 f l d 9 3 x 7 p / U F c N T q + N E B V 4 K D K F Z i j d U B l g X s o I 6 q e M M 4 y w p N 6 y P D 6 8 O q A 1 L 9 Z L L S r F m y w E S Y T n r K P w R + P N l o d i j 9 O B r w D u / N c Z F M D D W d C Q l C t p + g m W l 9 R 5 A L V S l z E c V Z W l W 5 Y w y R J V 7 p F 5 G P 2 W r 1 D d K f q M j 3 M s 2 K B K M + 6 y H 0 b G q n s 2 j Q P j F 9 l + k T E 7 h M Q M z A Z Q b E g M s A M U O X G R I T u k x I z M h l R s R E L h M R 8 8 F l P h B z 4 z I 3 x M Q u E 0 + 1 j I v E i w R m L J r 1 w Y P a 7 M w K L n o f x k J 6 g y z 9 U n r K L 6 M c H 9 T O 4 y y M l 1 S x U z d 2 S l f N X C Y j J n e Z n J h b l 7 k l p n C Z g h j h M o I Y 6 T K S m L H L j I k p X a Y k 5 s 5 l 7 o i 5 d 5 l 7 Y h 5 c 5 o G Y j y 7 z c W r M o k 0 A r O 9 Z v b 2 X V Z J M T C r 1 h y x t 6 n F j / d V Z Y n v o N u M Z x + E + w S w 3 y o B g l h j l g G C W F S U Q z F K i H B L M 8 q E M C W b J U I 4 I Z p l Q j g l m a V B + I J j l Q H l D M E u A M i Y 4 Z n B C c M J g N t F 8 h j O C m Z j L n G C m 5 P K W Y C b j s i C Y a b g U B A u + q A T L 9 j n h 0 i 0 J Z r o t 7 w h m o i 3 v C W a K L R 8 I Z n I t P x J s t b o Z g / r c r T 8 z F i 2 6 B S O 6 1 n 0 Z j P J a d 2 Y w 8 m v d m 8 F o s H V 3 B i P E 1 v 0 Z j B p b d 2 g w k m z d o 8 H o s n W X R u 7 R f R q M Q l t 3 a j A y b d 2 r w W i 1 u V t b L n G 5 h H O P 7 s R g p N u 6 F 4 P R b + t u D E b E r f s x G C W 3 7 s h g 5 N y 6 J 4 P R d O u u D E b Y r f s y G H W 3 7 s x g J N 6 6 N 4 P R e e v u D E b s r f s z G M U / v k N j L h R R U D u U Z J X y Y 5 X S J l k j e I 3 B 6 w S v M 3 i D 4 A 0 G b x K 8 y e A t g r c Y v E 3 w N o N 3 C N 5 h 8 C 7 B u w x + Q / A b B u 8 R v M f g D s E d B u 8 T v M / g A 4 I P G H x I 8 C G D u w R 3 G X x E 8 B G D j w k + Z v A J w S c M P i X 4 l M F n B J 8 x + J z g c w Z f E H z B 4 E u C L x l 8 R f A V g 6 8 J v n 5 8 e 3 V F B 0 Z 1 T K O r T L 9 a e o x b 4 9 y 6 y 6 1 z b s P l N j i 3 6 X K b n N t y u S 3 O b b v c N u d 2 X G 6 H c 7 s u t 8 u 5 N y 7 3 h n N 7 L r f H u Y 7 L d T i 3 7 3 L 7 n D t w u Q P O H b r c I e e 6 L t f l 3 J H L H X H u 2 O W O O X f i c i e c O 3 W 5 U 8 6 d u d w Z 5 8 5 d 7 p x z F y 5 3 w b l L l 7 v k 3 J X L X X H u 2 u W s 7 M + 5 h S g / g v 4 c g Z 9 d l + t z y y y F i f 0 8 a 7 F k b K B e Q k W j 9 s Q K d / 1 w W c E M 6 R u E f I h 2 I Y i Q + 9 D e A x H y H G U 1 E n I a 2 m c g Q v 5 C u w t E y F V o T 4 E I e Q n t J B A h B 6 H 9 A y L k G 7 R r Q I T c g v Y K i M R s H g x C z k D 7 A k R S N n 8 G I R e g P Q A i V P t 1 5 U e E K r 6 u 9 4 h Q n d d V H h H B J t w g V N P L a l n Y o p Q G o f q t q z c i V L V 1 z U a E a r W u 1 I h Q h d b 1 G Z E 2 N + r a 0 N K P 8 5 F a b / 2 3 V m D Z r 8 S h d W F B + q h F T y Y q y n x T h Y w 5 I C J L I F S 4 / k u w l q S S o w U w I C L 4 m y A R h Y k 6 V f 8 l 2 A q 3 E m 1 9 I 5 M J H / 9 E i d W 2 U K w B t V C o A 3 Z T E y V Q 2 0 K B D q m F 4 g y p h c I c U Q u H y 8 a K g v x A L R T j D Z u b i R J h f e c T J U D b w s l k s 4 j i y 9 i U T J T o b A t F d 0 s t F F z B Z m q i h F Z P 0 E S J z L Z w o t k 0 o 8 B K a q G 4 7 q i F w r q n F o r q g V o o q I / T 6 p s z r L P 3 B t c 1 F n V G t V V X V k S o o u p 6 i g j V U V 1 F E a H q q W s n I l Q z d c V E h C q l r p O I U H 3 U 1 R E R q o q 6 J i J C t V B X Q k S o A u r 6 h w j V P V 3 1 E K F q p 2 s d I l T j d I V D h C q b r m u I U D 3 T 1 Q w R q m K 6 h i F C t U t X L k S o Y u l 6 h Q j V K V 2 l E K H q p G s T I l S T d E V C h C q R r k O I U P 3 R 1 Q c R q j q 6 5 i B C t U Z X G k S u 2 Q p S X e j z s p B 0 R 9 V G 3 M M j N n s 2 9 R X T q d K / v r k q h x V 3 Y v J Y q + g U U q G + U N 6 A I P Y L Q F G N V t U O h F c 0 Z k 8 M I / W o F N I g G 0 R p i M H 8 c a w Q M a y P k + l E q K e 8 J y A f C 9 D P 4 s H 3 h e n f T z E J m 0 9 q U 6 G / a T R 1 s 4 q n n 1 J X t y a N v 0 w F U 7 9 c s x j p X 6 5 b j D J A b l i M c k B u W o y y Q G 5 Z j P J A b l u M M k H u W I x y Q e 5 a j L J B v r E Y 5 Y P c s x h l h O x Y j H J C 7 l u M s k I e W I z y Q h 5 a j D J D d i 1 G u S G P L E b Z I Y 8 t R v k h T y x G G S J P L U Y 5 I s 8 s R l k i z y 1 G e S I v L E a Z I i 8 t R r k i r y x G 2 S K v L W Y c G Q p 5 u / D z k W F D + z k 3 c D 5 u h G s M J l 2 E 6 w w m a Y Q b D C Z 1 h J s M J o G E W w w m j Y T b D C a Z h D s M J q W E u w w m s Y R v G E x 6 C f c Y T J I J O w w m 1 Y T 7 D C b h h A c M J u 2 E h w w m + Y R d B p O C w i M G k 4 j C Y w a T j s I T B p O U w l M G k 5 r C M w a T o M J z B p O m w g s G k 6 z C S w a T s s I r B p O 4 w m s G W 8 e P W 1 t l 1 U T 9 F K X P x C X W C C V t i X V C S V p i g 1 C t r O f e h v 4 m Y y z A 8 z 0 B 0 s N L x z D w N h e 9 P g S + w u U o E t 5 d N o 4 H C G E L P K G / 9 0 A v O S 4 8 9 Q Z Q F m M g 9 d Y M 3 O f o L f W X u f a L + S 2 6 I q l T b B N K 4 h Q 7 h J I 2 x S 6 h J E 3 x h l B S p t g j l I Q p O o S S L s U + o S R L c U A o q V I c E k q i F F 1 C S Z P i i F C S p D g m l B Q p T g g l Q Y p T Q k m P 4 o x Q k q M 4 J 5 T U K C 4 I J T G K S 0 J J i + K K U J K i u C a 0 f u S S o u 8 D / R H C N w 9 b K h M I 5 A A 6 r v l X 9 n C V W i j V N W q h R N e p h d L c o B Z u d p v U Q h F t U Q v F s 0 0 t F M 0 O t V A s u 9 R C k b y h F o p j j 1 o o i g 6 1 U A z 7 1 E I R H F A L F / + Q W r j o X W r h Y h 9 R C x f 5 m F q 4 u C f U w k U 9 p R Y u 5 h m 1 c B H P q Y W L d 0 E t X L R L a u F i X V E L F + m a X a 9 y W p X L U k s G f M m k c V y 4 p a j 8 1 S / 1 Y R I b d N G 7 i + Q o G 0 s P 7 Y 5 3 h y U t h 8 I 1 R E C O y H F D 1 e V l r Q H d c c Y I g r Z L 0 P B L o A 0 T N B w T a M s E D c 8 E 2 j R B w z W B t k 3 Q 8 E 2 g j R M 0 n B N o 6 w Q N 7 w T a P E H D P Y G 2 T 9 D w T 6 A N F D Q c F G g L B Q 0 P B d p E Q c N F g b Z R 0 P B R o I 0 U N J w U a C s F D S 8 F 2 k x B w 0 2 B t l P Q 8 F O g D R U 0 H B V o S w U N T w X a V E H D V Y G 2 V d D w V a C N F T S c F W h r B Q 1 v B d p c Q c N d g b Z X 0 P B X o A 0 W M I e F n x S w 5 M h i D N 4 4 H U A R P 6 i X l g a + 9 L 0 Q U i i w 2 q h 2 J F D p / b E q P a 5 s c 9 V 1 O s n f T 3 p F M t E N X f h U V E j y q I i w 5 D n n 1 2 8 g 9 h 9 0 u d O v g a i L Y H 1 s x L Z v i I x 8 i Z / U 3 U s 4 P b u 8 Z 3 f a N p g k G 0 D 8 1 I 3 o D v W d m N b M d a p O 3 a c 6 5 T K K B 1 D 1 7 O l G P f r 6 D N w m Z B a M f K H e v / X H M t O f o K B w R t h 4 D T Y 3 f e o x V q f M D m A A T j / T b O l X I I G b j u 1 n m q i F Q D 9 D c z v H f h 7 7 A U z r N 2 o 6 F T D 1 n n v V s T u 9 7 v m b 0 7 r i b T Y H 0 h H s p Z 1 O k z 2 e 8 t r e 2 D W T n M 1 x g 4 y L q X 3 u 5 h I F h N P 6 S V q T C i T d o 2 p F w w i K Z m i R D W X i 3 1 N P C z T 7 Y b H I 9 E t M 5 i H b b J Q 8 H q u 7 / 6 i e B L j s X m f K 3 2 D a 6 8 w s 4 L l f 0 A h U o x l f 4 h + / w L U v M t b z Z G Y B 1 r O S a N V Q A r 3 I 4 m H h J + q B 1 O g u K 9 C g C v 9 B eP z L q v I c f 2 F f n v s W Q / i m P W x D 0 S f e 2 t Y A D H l U / X r A f M d E v U W m 3 L B J i j r r V 5 E z c a h r p n a F E c S F n V 4 k X m D D F S 4 u + g m y m E Q + U u N F 5 m z I o n V w / v p p P P t 8 r S F z F J Q 3 E o b J + / 0 e S / b u F w x e Q u j t d D 5 t h e l Q / n Q T B 3 z i i q u c t d X y X I C u N c K P w T 1 + m q a V Y Z e w v 2 S t z 7 K h J q e T B n A Y O R t 4 G f f F L 4 U X j / L b p b m n c c 5 h 7 n a n b P i D 6 j x I t Q D w L + 9 R X X 0 V E e 1 T 5 q O e N Q e U q s V u + n f j / Q 4 R U G d q j f 8 Y p A 9 v 4 9 5 F m d 3 / Q L 8 m / n 3 n y + s N P / f x O z B + c u l l T 8 u v T 5 6 v f D N W v V / K
j 6 b + 9 3 c 7 + e + m l u Z + 9 P c N 3 M 7 c 9 2 5 s 7 l g 7 n b u r 3 N / m / v 7 y j 9 W / r X y 7 5 X / m K 6 f f l K d 8 5 s 5 5 2 f l v / 8 D c T V q T g = = &lt; / l a t e x i t &gt; 2 = 0.4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + A P 2 p e D / O q B 1 K T
x 7 6 Z 5 O g w 9 G p b A = " &gt; A A A y O X i c j V v L b t z I F d V M X h P l N Z M A 2 W R D R D Z m J p A F y T a S b A K M 3 p L V k l r v h 9 t j s N m 3 2 b T 4 E q u a k t z o + Y 1 s k + / I l 2 S Z X Z B t f i C 3 q l i 8 t 9 i U J g I s s c 4 p X h a r z q 1 7 m k 3 3 8 z g S c n n 5 n 5 9 8 + o M f / u j H P / n s p / M / + / k v f v m r z 7 / 4 9 b n I x k U A Z 0 E W Z 8 V l 3 x c Q R y m c y U j G c J k X 4 C f 9 G C 7 6 N + u K v y i h E F G W n s q H H N 4 l f p h G w y j w J U L v e j F 2 H f j v X / 5 l e e n 1 + 8 8 X l p e W 9 Y 8 3 e 7 B S H S z M V T / d 9 1 + s / L Y 3 y I J x A q k M Y l + I t y v L u X w 3 8 Q s Z B T F M 5 3 t j A b k f 3 P g h v M X D 1 E 9 A v J v o U U + 9 5 4 g M v G F W 4 L 9 U e h r l Z 0 z 8 R I i H p I 8 9 E 1 + O R J N T Y B v 3 d i y H f 3 4 3 i d J 8 L C E N z I W G 4 9 i T m a e m w B t E B Q Q y f s A D P y g i H K s X j P z C D y R O 1 P z 8 / H P 1 4 x 1 s X n j 7 q 6 c 7 3 s b m 1 u 7 B 7 u n u 4 c G J p 6 n 5 t p E s 4 l 9 1 H 2 K x n 0 w x h r f v F z e e w A v h P A s v G 3 q B n 5 t j d c s F D K E o o j R U o x p E Z S R s t 2 E U j g v A O 0 r h L s i S x E 8 H k x 6 C M Q z l d D L p Q e J 9 1 c H j r 6 f T m T 4 B L g Q U t t e 6 b r X 1 K 6 J w V A c 7 V o 2 2 X j L L b Z / T L G / r 0 c + k z B L b a U 2 3 Z v p V 9 + 3 b b v 5 j P f q 2 R / + x H o H t E T z W Y 2 B 7 D F Q P X I Y d v L t Y 3 a H n e 9 h f r T o M M V k G H s 5 N 4 s b A Y w V O 3 6 6 8 w y j 9 o b e w o o J g l C 2 9 K G b V U F O w 6 M X Z H R Q v A k y 9 p f k e h t T T C s O F l Y l Z w O 9 6 2 J r o A G 2 n 4 3 A j 6 c d L 3 h a K Q U j M G L X 2 Q q 0 Y 8 i b i l o 2 4 1 Y y o a X m X 2 W s u v K y u K j z b y c M 7 q h o v 7 R m 3 Y 3 9 A p y y 8 W n g 9 c 9 p i f Y 4 9 e s V D v d a 3 c 2 J U / e R 0 o P L N 4 K s U c O a j J Y C d E H P 2 i T 3 7 p O X s Y 3 u W z u i 7 r M 6 y p X p i z N W F n p k 6 B x + Z m m b A U Q H Q D M n i L b y a j U i z x m K / m o 3 t p x 7 g I q i T W 6 Y M b s 0 9 b 9 4 u f f e V D f 3 1 0 0 G i 1 M t 9 p O Q I R C R Y n B w D f a U i / Z + B x n k O h a d G Y 4 J s 1 o M x P Z w V W P U K / 4 5 W r x H s x Y s X f p l F A 2 8 s 1 A Y X D b 0 8 E y L C k m R C 5 7 G P C V j F f 3 R h f b U p 5 5 i P L T O l G H N 6 1 e d x f T x y k 1 W g 9 T r Q + v c G w n t O Q 9 A 7 u e l b T b e G 6 x G h 4 C x t Q 7 1 4 8 a j Y c H R + H G Z Y h E Z J y 3 0 i Z 0 Z X d 3 r y R l m o m T t d t a F W W 0 L Z t L H X w 5 u o Y z 2 9 p Z w 6 J 6 1 + 7 0 k z k 4 r q l d W d M / U p 1 A x X H T 2 1 K O b 8 p n q 7 9 f l d 9 3 x 7 p / U F c N T q + N E B V 4 K D K F Z i j d U B l g X s o I 6 q e M M 4 y w p N 6 y P D 6 8 O q A 1 L 9 Z L L S r F m y w E S Y T n r K P w R + P N l o d i j 9 O B r w D u / N c Z F M D D W d C Q l C t p + g m W l 9 R 5 A L V S l z E c V Z W l W 5 Y w y R J V 7 p F 5 G P 2 W r 1 D d K f q M j 3 M s 2 K B K M + 6 y H 0 b G q n s 2 j Q P j F 9 l + k T E 7 h M Q M z A Z Q b E g M s A M U O X G R I T u k x I z M h l R s R E L h M R 8 8 F l P h B z 4 z I 3 x M Q u E 0 + 1 j I v E i w R m L J r 1 w Y P a 7 M w K L n o f x k J 6 g y z 9 U n r K L 6 M c H 9 T O 4 y y M l 1 S x U z d 2 S l f N X C Y j J n e Z n J h b l 7 k l p n C Z g h j h M o I Y 6 T K S m L H L j I k p X a Y k 5 s 5 l 7 o i 5 d 5 l 7 Y h 5 c 5 o G Y j y 7 z c W r M o k 0 A r O 9 Z v b 2 X V Z J M T C r 1 h y x t 6 n F j / d V Z Y n v o N u M Z x + E + w S w 3 y o B g l h j l g G C W F S U Q z F K i H B L M 8 q E M C W b J U I 4 I Z p l Q j g l m a V B + I J j l Q H l D M E u A M i Y 4 Z n B C c M J g N t F 8 h j O C m Z j L n G C m 5 P K W Y C b j s i C Y a b g U B A u + q A T L 9 j n h 0 i 0 J Z r o t 7 w h m o i 3 v C W a K L R 8 I Z n I t P x J s t b o Z g / r c r T 8 z F i 2 6 B S O 6 1 n 0 Z j P J a d 2 Y w 8 m v d m 8 F o s H V 3 B i P E 1 v 0 Z j B p b d 2 g w k m z d o 8 H o s n W X R u 7 R f R q M Q l t 3 a j A y b d 2 r w W i 1 u V t b L n G 5 h H O P 7 s R g p N u 6 F 4 P R b + t u D E b E r f s x G C W 3 7 s h g 5 N y 6 J 4 P R d O u u D E b Y r f s y G H W 3 7 s x g J N 6 6 N 4 P R e e v u D E b s r f s z G M U / v k N j L h R R U D u U Z J X y Y 5 X S J l k j e I 3 B 6 w S v M 3 i D 4 A 0 G b x K 8 y e A t g r c Y v E 3 w N o N 3 C N 5 h 8 C 7 B u w x + Q / A b B u 8 R v M f g D s E d B u 8 T v M / g A 4 I P G H x I 8 C G D u w R 3 G X x E 8 B G D j w k + Z v A J w S c M P i X 4 l M F n B J 8 x + J z g c w Z f E H z B 4 E u C L x l 8 R f A V g 6 8 J v n 5 8 e 3 V F B 0 Z 1 T K O r T L 9 a e o x b 4 9 y 6 y 6 1 z b s P l N j i 3 6 X K b n N t y u S 3 O b b v c N u d 2 X G 6 H c 7 s u t 8 u 5 N y 7 3 h n N 7 L r f H u Y 7 L d T i 3 7 3 L 7 n D t w u Q P O H b r c I e e 6 L t f l 3 J H L H X H u 2 O W O O X f i c i e c O 3 W 5 U 8 6 d u d w Z 5 8 5 d 7 p x z F y 5 3 w b l L l 7 v k 3 J X L X X H u 2 u W s 7 M + 5 h S g / g v 4 c g Z 9 d l + t z y y y F i f 0 8 a 7 F k b K B e Q k W j 9 s Q K d / 1 w W c E M 6 R u E f I h 2 I Y i Q + 9 D e A x H y H G U 1 E n I a 2 m c g Q v 5 C u w t E y F V o T 4 E I e Q n t J B A h B 6 H 9 A y L k G 7 R r Q I T c g v Y K i M R s H g x C z k D 7 A k R S N n 8 G I R e g P Q A i V P t 1 5 U e E K r 6 u 9 4 h Q n d d V H h H B J t w g V N P L a l n Y o p Q G o f q t q z c i V L V 1 z U a E a r W u 1 I h Q h d b 1 G Z E 2 N + r a 0 N K P 8 5 F a b / 2 3 V m D Z r 8 S h d W F B + q h F T y Y q y n x T h Y w 5 I C J L I F S 4 / k u w l q S S o w U w I C L 4 m y A R h Y k 6 V f 8 l 2 A q 3 E m 1 9 I 5 M J H / 9 E i d W 2 U K w B t V C o A 3 Z T E y V Q 2 0 K B D q m F 4 g y p h c I c U Q u H y 8 a K g v x A L R T j D Z u b i R J h f e c T J U D b w s l k s 4 j i y 9 i U T J T o b A t F d 0 s t F F z B Z m q i h F Z P 0 E S J z L Z w o t k 0 o 8 B K a q G 4 7 q i F w r q n F o r q g V o o q I / T 6 p s z r L P 3 B t c 1 F n V G t V V X V k S o o u p 6 i g j V U V 1 F E a H q q W s n I l Q z d c V E h C q l r p O I U H 3 U 1 R E R q o q 6 J i J C t V B X Q k S o A u r 6 h w j V P V 3 1 E K F q p 2 s d I l T j d I V D h C q b r m u I U D 3 T 1 Q w R q m K 6 h i F C t U t X L k S o Y u l 6 h Q j V K V 2 l E K H q p G s T I l S T d E V C h C q R r k O I U P 3 R 1 Q c R q j q 6 5 i B C t U Z X G k S u 2 Q p S X e j z s p B 0 R 9 V G 3 M M j N n s 2 9 R X T q d K / v r k q h x V 3 Y v J Y q + g U U q G + U N 6 A I P Y L Q F G N V t U O h F c 0 Z k 8 M I / W o F N I g G 0 R p i M H 8 c a w Q M a y P k + l E q K e 8 J y A f C 9 D P 4 s H 3 h e n f T z E J m 0 9 q U 6 G / a T R 1 s 4 q n n 1 J X t y a N v 0 w F U 7 9 c s x j p X 6 5 b j D J A b l i M c k B u W o y y Q G 5 Z j P J A b l u M M k H u W I x y Q e 5 a j L J B v r E Y 5 Y P c s x h l h O x Y j H J C 7 l u M s k I e W I z y Q h 5 a j D J D d i 1 G u S G P L E b Z I Y 8 t R v k h T y x G G S J P L U Y 5 I s 8 s R l k i z y 1 G e S I v L E a Z I i 8 t R r k i r y x G 2 S K v L W Y c G Q p 5 u / D z k W F D + z k 3 c D 5 u h G s M J l 2 E 6 w w m a Y Q b D C Z 1 h J s M J o G E W w w m j Y T b D C a Z h D s M J q W E u w w m s Y R v G E x 6 C f c Y T J I J O w w m 1 Y T 7 D C b h h A c M J u 2 E h w w m + Y R d B p O C w i M G k 4 j C Y w a T j s I T B p O U w l M G k 5 r C M w a T o M J z B p O m w g s G k 6 z C S w a T s s I r B p O 4 w m s G W 8 e P W 1 t l 1 U T 9 F K X P x C X W C C V t i X V C S V p i g 1 C t r O f e h v 4 m Y y z A 8 z 0 B 0 s N L x z D w N h e 9 P g S + w u U o E t 5 d N o 4 H C G E L P K G / 9 0 A v O S 4 8 9 Q Z Q F m M g 9 d Y M 3 O f o L f W X u f a L + S 2 6 I q l T b B N K 4 h Q 7 h J I 2 x S 6 h J E 3 x h l B S p t g j l I Q p O o S S L s U + o S R L c U A o q V I c E k q i F F 1 C S Z P i i F C S p D g m l B Q p T g g l Q Y p T Q k m P 4 o x Q k q M 4 J 5 T U K C 4 I J T G K S 0 J J i + K K U J K i u C a 0 f u S S o u 8 D / R H C N w 9 b K h M I 5 A A 6 r v l X 9 n C V W i j V N W q h R N e p h d L c o B Z u d p v U Q h F t U Q v F s 0 0 t F M 0 O t V A s u 9 R C k b y h F o p j j 1 o o i g 6 1 U A z 7 1 E I R H F A L F / + Q W r j o X W r h Y h 9 R C x f 5 m F q 4 u C f U w k U 9 p R Y u 5 h m 1 c B H P q Y W L d 0 E t X L R L a u F i X V E L F + m a X a 9 y W p X L U k s G f M m k c V y 4 p a j 8 1 S / 1 Y R I b d N G 7 i + Q o G 0 s P 7 Y 5 3 h y U t h 8 I 1 R E C O y H F D 1 e V l r Q H d c c Y I g r Z L 0 P B L o A 0 T N B w T a M s E D c 8 E 2 j R B w z W B t k 3 Q 8 E 2 g j R M 0 n B N o 6 w Q N 7 w T a P E H D P Y G 2 T 9 D w T 6 A N F D Q c F G g L B Q 0 P B d p E Q c N F g b Z R 0 P B R o I 0 U N J w U a C s F D S 8 F 2 k x B w 0 2 B t l P Q 8 F O g D R U 0 H B V o S w U N T w X a V E H D V Y G 2 V d D w V a C N F T S c F W h r B Q 1 v B d p c Q c N d g b Z X 0 P B X o A 0 W M I e F n x S w 5 M h i D N 4 4 H U A R P 6 i X l g a + 9 L 0 Q U i i w 2 q h 2 J F D p / b E q P a 5 s c 9 V 1 O s n f T 3 p F M t E N X f h U V E j y q I i w 5 D n n 1 2 8 g 9 h 9 0 u d O v g a i L Y H 1 s x L Z v i I x 8 i Z / U 3 U s 4 P b u 8 Z 3 f a N p g k G 0 D 8 1 I 3 o D v W d m N b M d a p O 3 a c 6 5 T K K B 1 D 1 7 O l G P f r 6 D N w m Z B a M f K H e v / X H M t O f o K B w R t h 4 D T Y 3 f e o x V q f M D m A A T j / T b O l X I I G b j u 1 n m q i F Q D 9 D c z v H f h 7 7 A U z r N 2 o 6 F T D 1 n n v V s T u 9 7 v m b 0 7 r i b T Y H 0 h H s p Z 1 O k z 2 e 8 t r e 2 D W T n M 1 x g 4 y L q X 3 u 5 h I F h N P 6 S V q T C i T d o 2 p F w w i K Z m i R D W X i 3 1 N P C z T 7 Y b H I 9 E t M 5 i H b b J Q 8 H q u 7 / 6 i e B L j s X m f K 3 2 D a 6 8 w s 4 L l f 0 A h U o x l f 4 h + / w L U v M t b z Z G Y B 1 r O S a N V Q A r 3 I 4 m H h J + q B 1 O g u K 9 C g C v 9
B e M 8 6 3 7 5 8 p t 7 e 0 e + u j 1
P z L q v I c f 2 F f n v s W Q / i m P W x D 0 S f e 2 t Y A D H l U / X r A f M d E v U W m 3 L B J i j r r V 5 E z c a h r p n a F E c S F n V 4 k X m D D F S 4 u + g m y m E Q + U u N F 5 m z I o n V w / v p p P P t 8 r S F z F J Q 3 E o b J + / 0 e S / b u F w x e Q u j t d D 5 t h e l Q / n Q T B 3 z i i q u c t d X y X I C u N c K P w T 1 + m q a V Y Z e w v 2 S t z 7 K h J q e T B n A Y O R t 4 G f f F L 4 U X j / L b p b m n c c 5 h 7 n a n b P i D 6 j x I t Q D w L + 9 R X X 0 V E e 1 T 5 q O e N Q e U q s V u + n f j / Q 4 R U G d q j f 8 Y p A 9 v 4 9 5 F m d 3 / Q L 8 m / n 3 n y + s N P / f x O z B + c u l l T 8 u v T 5 6 v f D N W v V / K
j 6 b + 9 3 c 7 + e + m l u Z + 9 P c N 3 M 7 c 9 2 5 s 7 l g 7 n b u r 3 N / m / v 7 y j 9 W / r X y 7 5 X / m K 6 f f l K d 8 5 s 5 5 2 f l v / 8 D c T V q T g = = &lt; / l a t e x i t &gt; 2 = 0 .4&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 + X F u h 9 f v o e S y v f d e 5 9 T O Z y B r H
8 = " &gt; A A A y O X i c j V v L b t z I F d V M X h P l N Z M A 2 W R D R D Z m J p A F y T a S b A K M 3 p L V k l
r v h 9 t j s N m 3 2 b T 4 E q u a k t z o + Y 1 s k + / I l 2 S Z X Z B t f i C 3 q l i 8 t 9 i U J g I s s c 4 p X h a r z q 1 7 m k 3 3 8 z g S c n n 5 n 5 9 8 + o M f / u j H P / n s p / M / + / k v f v m r z 7 / 4 9 b n I
x k U A Z 0 E W Z 8 V l 3 x c Q R y m c y U j G c J k X 4 C f 9 G C 7 6 N + u K v y i h E F G W n s q H H N 4 l f p h G w y j w J U L v e j F 2 H f j v X / 1 l e e n V + 8 8 X l p e W 9 Y 8 3 e 7 B S H S z M V T / d 9 1 + s / L Y 3 y I J x A q k M Y l + I t y v L u X w 3 8 Q s Z B T F M 5 3 t j A b k f 3 P g h v M X D 1 E 9 A v J v o U U + 9 5 4 g M v G F W 4 L 9 U e h r l Z 0 z 8 R I i H p I 8 9 E 1 + O R J N T Y B v 3 d i y H f 3 4 3 i d J 8 L C E N z I W G 4 9 i T
m a e m w B t E B Q Q y f s A D P y g i H K s X j P z C D y R O 1 P z 8 / H P 1 4 x 1 s X n j 7 q 6 c 7 3 s b m 1 u 7 B 7 u n u 4 c G J p 6 n 5 t p E s 4 l 9 1 H 2 K x n 0 w x h r f v F z e e w A v h P A s v G 3 q B n 5 t j d c s
F D K E o o j R U o x p E Z S R s t 2 E U j g v A O 0 r h L s i S x E 8 H k x 6 C M Q z l d D L p Q e J 9 1 c H j r 6 f T m T 4 B L g Q U t t e 6 b r X 1 K 6 J w V A c 7 V o 2 2 X j L L b Z / T L G / r 0 c + k z B L b a U 2 3 Z v p V 9 + 3 b b v 5 j P f q 2 R / + x H o H t E T z W Y 2 B 7 D F Q P X I Y d v L t Y 3 a H n e 9 h f r T o M M V k G H s 5 N 4 s b A Y w V O 3 6 6 8 w y j 9 o b e w o o J g l C 2 9 K G b V U F O w 6 M X Z H R Q v A k y 9 p f k e h t T T C s O F l Y l Z w O 9 6 2 J r o A G 2 n 4 3 A j 6 c d L 3 h a K Q U j M G L X 2 Q q 0 Y 8 i b i l o 2 4 1 Y y o a X m X 2 W s u v K y u K j z b y c M 7 q h o v 7 R m 3 Y 3 9 A p y y 8 W n g 9 c 9 p i f Y 4 9 e s V D v d a 3 c 2 J U / e R 0 o P L N 4 K s U c O a j J Y C d E H P 2 i T 3 7 p O X s Y 3 u W z u i 7 r M 6 y p X p i z N W F n p k 6 B x + Z m m b A U Q H Q D M n i L b y a j U i z x m K / m o 3 t p x 7 g I q i T W 6 Y M b s 0 9 b 9 4 u f f e V D f 3 1 0 0 G i 1 M t 9 p O Q I R C R Y n B w D f a U i / Z + B x n k O h a d G Y 4 J s 1 o M x P Z w V W P U K / 4 5 W r x H s x Y s X f p l F A 2 8 s 1 A Y X D b 0 8 E y L C k m R C 5 7 G P C V j F f 3 R h f b U p 5 5 i P L T O l G H N 6 1 e d x f T x y k 1 W g 9 T r Q + v c G w n t O Q 9 A 7 u e l b T b e G 6 x G h 4 C x t Q 7 1 4 8 a j Y c H R + H G Z Y h E Z J y 3 0 i Z 0 Z X d 3 r y R l m o m T t d t a F W W 0 L Z t L H X w 5 u o Y z 2 9 p Z w 6 J 6 1 + 7 0 k z k 4 r q l d W d M / U p 1 A x X H T 2 1 K O b 8 p n q 7 9 f l d 9 3 x 7 p / U F c N T q + N E B V 4 K D K F Z i j d U B l g X s o I 6 q e M M 4 y w p N 6 y P D 6 8 O q A 1 L 9 Z L L S r F m y w E S Y T n r K P w R + P N l o d i j 9 O B r w D u / N c Z F M D D W d C Q l C t p + g m W l 9 R 5 A L V S l z E c V Z W l W 5 Y w y R J V 7 p F 5 G P 2 W r 1 D d K f q M j 3 M s 2 K B K M + 6 y H 0 b G q n s 2 j Q P j F 9 l + k T E 7 h M Q M z A Z Q b E g M s A M U O X G R I T u k x I z M h l R s R E L h M R 8 8 F l P h B z 4 z I 3 x M Q u E 0 + 1 j I v E i w R m L J r 1 w Y P a 7 M w K L n o f x k J 6 g y z 9 U n r K L 6 M c H 9 T O 4 y y M l 1 S x U z d 2 S l f N X C Y j J n e Z n J h b l 7 k l p n C Z g h j h M o I Y 6 T K S m L H L j I k p X a Y k 5 s 5 l 7 o i 5 d 5 l 7 Y h 5 c 5 o G Y j y 7 z c W r M o k 0 A r O 9 Z v b 2 X V Z J M T C r 1 h y x t 6 n F j / d V Z Y n v o N u M Z x + E + w S w 3 y o B g l h j l g G C W F S U Q z F K i H B L M 8 q E M C W b J U I 4 I Z p l Q j g l m a V B + I J j l Q H l D M E u A M i Y 4 Z n B C c M J g N t F 8 h j O C m Z j L n G C m 5 P K W Y C b j s i C Y a b g U B A u + q A T L 9 j n h 0 i 0 J Z r o t 7 w h m o i 3 v C W a K L R 8 I Z n I t P x J s t b o Z g / r c r T 8 z F i 2 6 B S O 6 1 n 0 Z j P J a d 2 Y w 8 m v d m 8 F o s H V 3 B i P E 1 v 0 Z j B p b d 2 g w k m z d o 8 H o s n W X R u 7 R f R q M Q l t 3 a j A y b d 2 r w W i 1 u V t b L n G 5 h H O P 7 s R g p N u 6 F 4 P R b + t u D E b E r f s x G C W 3 7 s h g 5 N y 6 J 4 P R d O u u D E b Y r f s y G H W 3 7 s x g J N 6 6 N 4 P R e e v u D E b s r f s z G M U / v k N j L h R R U D u U Z J X y Y 5 X S J l k j e I 3 B 6 w S v M 3 i D 4 A 0 G b x K 8 y e A t g r c Y v E 3 w N o N 3 C N 5 h 8 C 7 B u w x + Q / A b B u 8 R v M f g D s E d B u 8 T v M / g A 4 I P G H x I 8 C G D u w R 3 G X x E 8 B G D j w k + Z v A J w S c M P i X 4 l M F n B J 8 x + J z g c w Z f E H z B 4 E u C L x l 8 R f A V g 6 8 J v n 5 8 e 3 V F B 0 Z 1 T K O rl L l 7 v k 3 J X L X X H u 2 u W s 7 M + 5 h S g / g v 4 c g Z 9 d l + t z y y y F i f 0 8 a 7 F k b K B e Q k W j 9 s Q K d / 1 w W c E M 6 R u E f I h 2 I Y i Q + 9 D e A x H y H G U 1 E n I a 2 m c g Q v 5 C u w t E y F V o T 4 E I e Q n t J B A h B 6 H 9 A y L k G 7 R r Q I T c g v Y K i M R s H g x C z k D 7 A k R S N n 8 G I R e g P Q A i V P t 1 5 U e E K r 6 u 9 4 h Q n d d V H h H B J t w g V N P L a l n Y o p Q G o f q t q z c i V L V 1 z U a E a r W u 1 I h Q h d b 1 G Z E 2 N + r a 0 N K P 8 5 F a b / 2 3 V m D Z r 8 S h d W F B + q h F T y Y q y n x T h Y w 5 I C J L I F S 4 / k u w l q S S o w U w I C L 4 m y A R h Y k 6 V f 8 l 2 A q 3 E m 1 9 I 5 M J H / 9 E i d W 2 U K w B t V C o A 3 Z T E y V Q 2 0 K B D q m F 4 g y p h c I c U Q u H y 8 a K g v x A L R T j D Z u b i R J h f e c T J U D b w s l k s 4 j i y 9 i U T J T o b A t F d 0 s t F F z B Z m q i h F Z P 0 E S J z L Z w o t k 0 o 8 B K a q G 4 7 q i F w r q n F o r q g V o o q I / T 6 p s z r L P 3 B t c 1 F n V G t V V X V k S o o u p 6 i g j V U V 1 F E a H q q W s n I l Q z d c V E h C q l r p O I U H 3 U 1 R E R q o q 6 J i J C t V B X Q k S o A u r 6 h w j V P V 3 1 E K F q p 2 s d I l T j d I V D h C q b r m u I U D 3 T 1 Q w R q m K 6 h i F C t U t X L k S o Y u l 6 h Q j V K V 2 l E K H q p G s T I l S T d E V C h C q R r k O I U P 3 R 1 Q c R q j q 6 5 i B C t U Z X G k S u 2 Q p S X e j z s p B 0 R 9 V G 3 M M j N n s 2 9 R X T q d K / v r k q h x V 3 Y v J Y q + g U U q G + U N 6 A I P Y L Q F G N V t U O h F c 0 Z k 8 M I / W o F N I g G 0 R p i M H 8 c a w Q M a y P k + l E q K e 8 J y A f C 9 D P 4 s H 3 h e n f T z E J m 0 9 q U 6 G / a T R 1 s 4 q n n 1 J X t y a N v 0 w F U 7 9 c s x j p X 6 5 b j D J A b l i M c k B u W o y y Q G 5 Z j P J A b l u M M k H u W I x y Q e 5 a j L J B v r E Y 5 Y P c s x h l h O x Y j H J C 7 l u M s k I e W I z y Q h 5 a j D J D d i 1 G u S G P L E b Z I Y 8 t R v k h T y x G G S J P L U Y 5 I s 8 s R l k i z y 1 G e S I v L E a Z I i 8 t R r k i r y x G 2 S K v L W Y c G Q p 5 u / D z k W F D + z k 3 c D 5 u h G s M J l 2 E 6 w w m a Y Q b D C Z 1 h J s M J o G E W w w m j Y T b D C a Z h D s M J q W E u w w m s Y R v G E x 6 C f c Y T J I J O w w m 1 Y T 7 D C b h h A c M J u 2 E h w w m + Y R d B p O C w i M G k 4 j C Y w a T j s I T B p O U w l M G k 5 r C M w a T o M J z B p O m w g s G k 6 z C S w a T s s I r B p O 4 w m s G W 8 e P W 1 t l 1 U T 9 F K X P x C X W C C V t i X V C S V p i g 1 C t r O f e h v 4 m Y y z A 8 z 0 B 0 s N L x z D w N h e 9 P g S + w u U o E t 5 d N o 4 H C G E L P K G / 9 0 A v O S 4 8 9 Q Z Q F m M g 9 d Y M 3 O f o L f W X u f a L + S 2 6 I q l T b B N K 4 h Q 7 h J I 2 x S 6 h J E 3 x h l B S p t g j l I Q p O o S S L s U + o S R L c U A o q V I c E k q i F F 1 C S Z P i i F C S p D g m l B Q p T g g l Q Y p T Q k m P 4 o x Q k q M 4 J 5 T U K C 4 I J T G K S 0 J J i + K K U J K i u C a 0 f u S S o u 8 D / R H C N w 9 b K h M I 5 A A 6 r v l X 9 n C V W i j V N W q h R N e p h d L c o B Z u d p v U Q h F t U Q v F s 0 0 t F M 0 O t V A s u 9 R C k b y h F o p j j 1 o o i g 6 1 U A z 7 1 E I R H F A L F / + Q W r j o X W r h Y h 9 R C x f 5 m F q 4 u C f U w k U 9 p R Y u 5 h m 1 c B H P q Y W L d 0 E t X L R L a u F i X V E L F + m a X a 9 y W p X L U k s G f M m k c V y 4 p a j 8 1 S / 1 Y R I b d N G 7 i + Q o G 0 s P 7 Y 5 3 h y U t h 8 I 1 R E C O y H F D 1 e V l r Q H d c c Y I g r Z L 0 P B L o A 0 T N B w T a M s E D c 8 E 2 j R B w z W B t k 3 Q 8 E 2 g j R M 0 n B N o 6 w Q N 7 w T a P E H D P Y G 2 T 9 D w T 6 A N F D Q c F G g L B Q 0 P B d p E Q c N F g b Z R 0 P B R o I 0 U N J w U a C s F D S 8 F 2 k x B w 0 2 B t l P Q 8 F O g D R U 0 H B V o S w U N T w X a V E H D V Y G 2 V d D w V a C N F T S c F W h r B Q 1 v B d p c Q c N d g b Z X 0 P B X o A 0 W M I e F n x S w 5 M h i D N 4 4 H U A R P 6 i X l g a + 9 L 0 Q U i i w 2 q h 2 J F D p / b E q P a 5 s c 9 V 1 O s n f T 3 p F M t E N X f h U V E j y q I i w 5 D n n 1 2 8 g 9 h 9 0 u d O v g a i L Y H 1 s x L Z v i I x 8 i Z / U 3 U s 4 P b u 8 Z 3 f a N p g k G 0 D 8 1 I 3 o D v W d m N b M d a p O 3 a c 6 5 T K K B 1 D 1 7 O l G P f r 6 D N w m Z B a M f K H e v / X H M t O f o K B w R t h 4 D T Y 3 f e o x V q f M D m A A T j / T b O l X I I G b j u 1 n m q i F Q D 9 D c z v H f h 7 7 A U z r N 2 o 6 F T D 1 n n v V s T u 9 7 v m b 0 7 r i b T Y H 0 h H s p Z 1 O k z 2 e 8 t r e 2 D W T n M 1 x g 4 y L q X 3 u 5 h I F h N P 6 S V q T C i T d o 2 p F w w i K Z m i R D W X i 3 1 N P C z T 7 Y b H I 9 E t M 5 i H b b J Q 8 H q u 7 / 6 i e B L j s X m f K 3 2 D a 6 8 w s 4 L l f 0 A h U o x l f 4 h + / w L U v M t b z Z G Y B 1 r O S a N V Q A r 3 I 4 m H h J + q B 1 O g u K 9 C g C v 9
B e M 8 6 3 7 5 8 p t 7 e 0 e + u j 1
P z L q v I c f 2 F f n v s W Q / i m P W x D 0 S f e 2 t Y A D H l U / X r A f M d E v U W m 3 L B J i j r r V 5 E z c a h r p n a F E c S F n V 4 k X m D D F S 4 u + g m y m E Q + U u N F 5 m z I o n V w / v p p P P t 8 r S F z F J Q 3 E o b J + / 0 e S / b u F w x e Q u j t d D 5 t h e l Q / n Q T B 3 z i i q u c t d X y X I C u N c K P w T 1 + m q a V Y Z e w v 2 S t z 7 K h J q e T B n A Y O R t 4 G f f F L 4 U X j / L b p b m n c c 5 h 7 n a n b P i D 6 j x I t Q D w L + 9 R X X 0 V E e 1 T 5 q O e N Q e U q s V u + n f j / Q 4 R U G d q j f 8 Y p A 9 v 4 9 5 F m d 3 / Q L 8 m / n 3 n y + s N P / f x O z B + c u l l T 8 u v T 5 6 v f D N W v V / K
j 6 b + 9 3 c 7 + e + m l u Z + 9 P c N 3 M 7 c 9 2 5 s 7 l g 7 n b u r 3 N / m / v 7 y j 9 W / r X y 7 5 X / m K 6 f f l K d 8 5 s 5 5 2 f l v / 8 D c T l q T g = = &lt; / l a t e x i t &gt;
3 = 0.3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e O o S 0 h L G h r x z Z R W J Q K I s p 1 l j v b c = " &gt; A A A y L 3 i c j V v b b t z I E d V u b h v l t p s A e c k L E d n Y 3 U A W P L a R 5 C X A 6 i 5 Z I 2 m k 0 d 3 j N T i c G g 4 t 3 s z u o S Q P Z v 8 g r 8 l 3 5 G u C v A R 5 z V + k u p v N q q Y o b Q R Y Y p 3 T X e z L q a 4 a D j 3 M 4 0 j I 5 8 / / 9 c m n P / j h j 3 7 8 k 8 9 + u v i z n / / i l 7 / 6 / I t f n 4 l s W g R w G m R x V l w M f Q F x l M K p j G Q M F 3 k B f j K M 4 X x 4 v a 7 4 8 x I K E W X p i b z L 4 W 3 i h 2 k 0 j g J f I t S / / k v n 3 e d L z 1 e e 6 x / v / k W n u l h a q H 5 6 7 7 7 o / H Y w y o J p A q k M Y l + I N 5 3 n u X w 7 8 w s Z B T H M F w d T A b k f X P s h v M H L 1 E 9 A v J 3 p s c 6 9 p 4 i M v H F W 4 L 9 U e h r l P W Z + I s R d M s S W i S 8 n o s k p s I 1 7 M 5 X j P 7 + d R W k + l Z A G 5 k b j a e z J z F M T 9 0 Z R A Y G M 7 / D C D 4 o I x + o F E 7 / w A 4 n L s 7 i 4 + F T 9 e A e b 5 9 7 + 6 s m O t 7 G 5 t X u w e 7 J 7 e N D 3 N L X Y N p J l / K v m I Z a H y R x 9 e P t + c e 0 J v B G u r v C y s R f 4 u b l W U y 5 g D E U R p a E a 1 S g q I 2 G b j a N w W g D O K I W b I E s S P x 3 N B g j G M J b z 2 W w A i f d V F 6 + / n s / v t Q l w I 6 C w r d a 1 1 d a u i M J J 7 e x Y G W 2 t Z J b b N i d Z 3 t Z i m E m Z J b b R m r b u t a v m 7 d t m / k M t h r b F 8 K E W g W 0 R P N R i Z F u M V A v c h h 2 c X a x m 6 P k e t l e 7 D m M M k Z G H a 5 O 4 P v B a g f M 3 n b f o Z T j 2 l j r K C X r Z 0 p t i d g 0 1 B c t e n N 1 A 8 S z A g F t Z H K B L v a w w X u r M z A Z + N 0 B r p h 2 0 d c f h R t K P V 7 w t F I O Q G D F q 7 4 X a M e S N x y 3 r c a v p U d P y J r P 3 X H p R 3 V V 4 t p G H M 6 q M F 7 b H h 6 k / o i 5 L L 5 d e 3 e u 2 X P e x V y + 5 q 1 d 6 O n 2 j 6 k e X A 5 V v B l + F g L M e L Q 7 s g p j e f d u 7 3 9 L 7 2 P b S E X 2 T 1 V G 2 U i + M u b v Q K 1 P H 4 A N L 0 3 Q 4 K Q C a L p m / p Z f 3 P d K q M d 8 v 7 / v 2 U w 9 w E 1 T n l i W D D 2 b O m x 9 W v v v K u v 7 6 c S d R 6 u U + U n I C I h L M T 4 6 O v l K e / k 9 H 0 z y H w l O j M U 4 2 6 8 G Y F s 4 O r H q F f 0 O 7 1 3 D 2 7 N k z v 8 y i k T c V 6 o C L x l 6 e C R F h I j K u 8 9 j H A K z 8 P 7 i x v j q U c 4 z H l p V S j O l e t X l Y H w 9 M s n K 0 X j t a / 1 5 H O O c 0 B H 2 S m 7 b V c m u 4 H h E K z t L W 1 b N n D 4 o N R + f H Y Y Z J a J K 0 z B M 5 M 7 q 6 0 a M T Z a 7 u z X T V u l p t c W X D x t 4 P J 1 H 7 e v x I O X E 6 r X 5 v p 3 u L i u q V 1 c y Z + h R q h q u u H t s U 0 7 + p 3 l 7 d v + f 2 t z O t b 4 C j V t c P D r g S H E S x E m u s L j A t Y A N 1 V f k b x 1 l W a F p f G V 5 f V g 2 Q G i a z T j N n y Q I D Y T 4 b q P o h 8 O P Z R r N B 6 c f R i D d 4 Z 6 6 L Z G a o + T 2 X I G R 7 B 8 3 M 6 x l B L l S m z E U U Z 2 m V 5 Y 7 R R Z Z 4 p V 9 E P k a r 1 T d I f 6 Y 8 3 8 o 0 K x L 0 + m S A 0 J O 5 X c 6 i Q f v E D F 1 m S E z g M g E x I 5 c Z E Q M u A 8 S M X W Z M T O g y I T E T l 5 k Q E 7 l M R M x 7 l 3 l P z L X L X B M T u 0 w 8 1 z I u E i 8 S G L F Y o o / u 1 G F n d n D Z e z 8 V 0 h t l 6 Z f S U / U y y v F O n T z O x n h J 5 T t 1 f a d 0 1 8 x l M m J y l 8 m J + e A y H 4 g p X K Y g R r i M I E a 6 j C R m 6 j J T Y k q X K Y m 5 c Z k b Y m 5 d 5 p a Y O 5 e 5 I + a j y 3 y c m 2 L R B g D m 9 6 w + 3 s s q S G Y m l I Z j F j b 1 u D H / 6 i i x L b T N e M Z x e E g w i 4 0 y I J g F R j k i m E V F C Q S z k C j H B L N 4 K E O C W T C U E 4 J Z J J R T g l k Y l O 8 J Z j F Q X h P M A q C M C Y 4 Z n B C c M J g t N F / h j G A m 5 j I n m C m 5 / E A w k 3 F Z E M w 0 X A q C B d 9 U g m X 7 m n D p l g Q z 3 Z Y 3 B D P R l r c E M 8 W W d w Q z u Z Y f C b Z a 3 Y x B f e 7 W n x m L F t 2 C E V 3 r u Q x G e a 0 n M x j 5 t Z 7 N Y D T Y e j q D E W L r + Q x G j a 0 n N B h J t p 7 R Y H T Z e k o j 9 + A 5 D U a h r S c 1 G J m 2 n t V g t N o 8 r S 2 X u F z C u Q d P Y j D S b T 2 L w e i 3 9 T Q G I + L W 8 x i M k l t P Z D B y b j 2 T w W i 6 9 V Q G I + z W c x m M u l t P Z j A S b z 2 b w e i 8 9 X Q G I / b W 8 x m M 4 h 8 + o T E W i i i o K 5 R k l e J j l c I m W S N 4 j c H r B K 8 z e I P g D Q Z v E r z J 4 C 2 C t x i 8 T f A 2 g 3 c I 3 m H w L s G 7 D H 5 N 8 G s G 7 x G 8 x + A u w V 0 G 7 x O 8 z + A D g g 8 Y f E j w I Y N 7 B P c Y f E T w E Y O P C T 5 m c J / g P o N P C D 5 h 8 C n B p w w + I / i M w e c E n z P 4 g u A L B l 8 S f M n g K 4 K v H j 5 e X d G B U R 3 T 6 C r T r 5 Y e 4 9 Y 4 t + 5 y 6 5 z b c L k N z m 2 6 3 C b n t l x u i 3 P b L r f N u R 2 X 2 + H c r s v t c u 6 1 y 7 3 m 3 J 7 L 7 X G u 6 3 J d z u 2 7 3 D 7 n D l z u g H O H L n f I u Z 7 L 9 T h 3 5 H J H n D t 2 u W P O 9 V 2 u z 7 k T l z v h 3 K n L n X L u z O X O O H f u c u e c u 3 C 5 C 8 5 d u t w l 5 6 5 c z s r + j J c Q 5 U f Q n y P w s + v z u m + Z p T C z n 2 c t l k w N N E g o a d Q 1 s c L d e r i s Y I Y M D U J 1 i K 5 C E K H q Q 9 c e i F D N U V Y j o U p D 1 x m I U H 2 h q w t E q K r Q N Q U i V E v o S g I R q i B 0 / Y A I 1 Q 2 6 a k C E q g V d K y A S s 3 U w C F U G u i 5 A J G X r Z x C q A n Q N g A j l f p 3 5 E a G M r / M 9 I p T n d Z Z H R L A F N w j l 9 L L a F r Y p p U E o f + v s j Q h l b Z 2 z E a F c r T M 1 I p S h d X 5 G p K 0 a d c v Q 0 o / z i d p v / b d W Y D m s x K F 1 Y U H 6 q E V P J i o q 9 p P h S P U w F 0 R k C Y Q K 1 3 8 J 1 p J U c r Q A O k Q E f x M k o j B R X f V f g q 1 w K 9 H W E 5 n N + P h n S q z W Q r E G Z K F Q R 2 x S M y V Q a 6 F A x 2 S h O E O y U J g T s n C 4 b K w o y P d k o R i v 2 d r M l A j r m c + U A K 2 F i 8 l W E c W X s S W Z K d F Z C 0 X 3 g S w U X M F W a q a E V i / Q T I n M W r j Q b J l R Y C V Z K K 4 b s l B Y t 2 S h q O 7 I Q k F 9 n F f f n G G e v T W 4 z r G o M 8 q t O r M i Q h l V 5 1 N E K I / q L I o I Z U + d O x G h n K k z J i K U K X W e R I T y o 8 6 O i F B W 1 D k R E c q F O h M i Q h l Q 5 z 9 E K O / p r I c I Z T u d 6 x C h H K c z H C K U 2 X R e Q 4 T y m c 5 m i F A W 0 z k M E c p d O n M h Q h l L 5 y t E K E / p L I U I Z S e d m x C h n K Q z E i K U i X Q e Q o T y j 8 4 + i F D W 0 T k H E c o 1 O t M g c s V 2 k P L C k K e F p D e p D u I B X r H V s 6 G v m G 4 V / v X k q h h W X N / E s V b R C a R C f a G 8 A U H s F 4 C i m q y q E w j v a I o 9 M Y 7 U o 1 J I g 2 w U p S E 6 8 6 e x Q s S 4 v k 7 m M 6 G e 8 v Z B P u R g m M W j 7 3 M z v J 1 j E D a f 1 K Z C f 9 N o 8 m b l T z + l r q Y m T X 2 Z C q Z + u W Y x 0 r 9 c t x h F g N y w G M W A 3 L Q Y R Y H c s h j F g d y 2 G E W C 3 L E Y x Y L c t R h F g 3 x t M Y o H u W c x i g j Z t R j F h N y 3 G E W F P L A Y x Y U 8 t B h F h u x Z j G J D H l m M o k M e W 4 z i Q / Y t R h E i T y x G M S J P L U Z R I s 8 s R n E i z y 1 G k S I v L E a x I i 8 t R t E i r y x m K j I U 8 n b h 5 x P D h v Z z b u B 8 3 A j X G E y 6 C N c Z T N I I N x h M 6 g g 3 G U w C C b c Y T B o J t x l M M g l 3 G E x K C X c Z T G I J X z O Y 9 B L u M Z g k E 3 Y Z T K o J 9 x l M w g k P G E z a C Q 8 Z T P I J e w w m B Y V H D C Y R h c c M J h 2 F f Q a T l M I T B p O a w l M G k 6 D C M w a T p s J z B p O s w g s G k 7 L C S w a T u M I r B t u K H 4 + 2 q l Q T 9 V O U I R O X W C O U t C X W C S V p i Q 1 C t b K e e h v 6 m 4 y p A M / 3 B E g P b x 3 D y N t c 9 o Y Q + A q X k 0 h 4 N 9 k 0 H i G E F n h C f + + B t e S 0 8 N Q b Q F m M j t R b M 3 C b Y 2 2 p v 8 y 1 X 8 x v 0 R 1 J n W K b U B K n 2 C G U t C l 2 C S V p i t e E k j L F H q E k T N E l l H Q p 9 g k l W Y o D Q k m V 4 p B Q E q X o E U q a F E e E k i T F M a G k S N E n l A Q p T g g l P Y p T Q k m O 4 o x Q U q M 4 J 5 T E K C 4 I J S 2 K S 0 J J i u K K 0 P q R S 4 p 1 H + i P E L 5 5 2 F I V g U A V Q N c t / l V 5 u E o W S n W N L J T o O l k o z Q 2 y 8 L D b J A t F t E U W i m e b L B T N D l k o l l 2 y U C S v y U J x 7 J G F o u i S h W L Y J w t F c E A W b v 4 h W b j p P b J w s 4 / I w k 0 + J g s 3 t 0 8 W b u o J W b i Z p 2 T h J p 6 R h Z t 3 T h Z u 2 g V Z u F m X Z O E m X b H 7 V Z V W V W W p L Q O + Z d J U X H i k q P j V L / V h E B t 0 2 b u J 5 C S b S g / L H e 8 G U 1 o O h V s Q A V V E T j V U 3 V 7 W G t A N 7 x W C o M s l a N R L o A s m a F R M o E s m a N R M o I s m a F R N o M s m a N R N o A s n a F R O o E s n a N R O o I s n a F R P o M s n a N R P o A s o a F R Q o E s o a N R Q o I s o a F R R o M s o a N R R o A s p a F R S o E s p a N R S o I s p a F R T o M s p a N R T o A s q a F R U o E s q a N R U o I s q a F R V o M s q a N R V o A s r a F R W o E s r a N R W o I s r a F R X o M s r a N R X o A s s Y B U W f l L A l C O L K X j T d A R F f K d e W h r 5 0 v d C S K H A b K P s S K D S h 1 O V e l z Z 5 q r p f J a / m w 2 K Z K Y N n f i U V 0 j y q I g w 5 T n 9 6 z c Q h 3 c 6 3 e n X Q N R N M D 8 2 f N s 3 R C a + x E / q 7 i 2 c l j 3 e s j d v G 0 y S j S B + b C K 6 Q T 0 T Y 9 2 7 T 9 W o 9 1 i j X E b x C K q W A 2 3 U o 6 9 7 4 D E h s 2 D i C / X + r T + V m f 4 E B Y U z w s Z r s L l p U 4 + x 6 n J / A C N w 2 h m z p V 2 B B B 4 6 t p 0 x U Q u B f o b m N o 7 9 P P Y D m N d v 1 H Q r Y O 4 9 9 a p r d 3 n d / p v z O u N t N g f S F e y l n W 6 T P Z 7 z 3 N 4 4 N Z O c r X G D j I u 5 f e 7 m E g W E 8 / p J W p M K J M 1 R W d E 4 g q L p W m R j m f i 3 1 N I C z X a Y L D L 9 E p N 5 y H b f S x 5 P 1 e w / q i c B L r v X n f M 3 m P a 6 9 z b w z C 9 o B M p o + p f 4 x y 9 w 7 4 u M t e z f 2 4 D 1 r C R a G U q g 5 1 k 8 L v x E P Z C a 3 G Q F F q j C v x P e k + 6 3 L 5 6 o t 3 f 0 u + v T 1 L z L K n L c f 6 H f H n s y g D h m b e w D 0 a f e G i Z A D P l U / b r D e I d E v c W m q m D j l L V W L 6 J m 0 1 D n T F 0 U R x K W t X u R e a M M l L u b 6 D r K Y R T 5 K 4 0 X m b M i i d X D + / m s + + 3 z e Q u Z p a C 4 T h s n b 3 S / F 2 1 c r p i 8 h d F a 6 H 4 7 i N K x v G u G j n l F F X e 5 5 6 t g 6 Q O e t c I P Q b 2 + m m Z V Q S / h d s V b n 2 R C L U + m C s B g 4 m 3 g Z 9 8 U v h T e M M u u V x a d x z m H u T q d s + I P q P E i 1 A P A v 4 N l d f V Y Q 3 V O m o Z 4 1 e 5 S q x W b 6 d 8 P t D h B Q Z 2 o N / x i k A N / i H E W Z z f D A v z r x X e f L 3 W a / 2 / i / s X Z i 5 X O H 1 d e H b 1 a + m a t + j 8 V n y 3 8 b u H 3 C 1 8 t d B b + t P D N w s 5 C b + F 0 I V g I F / 6 6 8 L e F v 3 f + 0 f l n 5 9 + d / 5 i m n 3 5 S 9 f n N g v P T + e / / A G O B Z q Y = &lt; / l a t e x i t &gt; k = &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U y g p t R W T A i c L 1 U 1 1 c x 2 b f f R F 1 j k = " &gt; A A A y L 3 i c j V v b b t z I E d V u b h v l t p s A e c k L E d n Y 3 U A W L N t I 8 h J g d Z e s k T T S j K 6 W 1 + B w a j i 0 e D O 7 h y N 5 M P s H e U 2 + I 1 8 T 5 C X I a / 4 i 1 d 1 s V j V F a S P A E u u c 7 m J f T n X V c O h B H k d C P n / + r 0 8 + / c E P f / T j n 3 z 2 0 8 W f / f w X v / z V 5 1 / 8 + k x k k y K A 0 y C L s + J i 4 A u I o x R O Z S R j u M g L 8 J N B D O e D m w 3 F n 5 d Q i C h L + / I u h 7 e J H 6 b R K A p 8 i V D v 5 i 8 v 3 n 2 + 9 H z l u f 7 x 7 l + s V h d L C 9 V P 9 9 0 X q 7 + 9 H m b B J I F U B r E v x J v V 5 7 l 8 O / M L G Q U x z B e v J w J y P 7 j x Q 3 i D l 6 m f g H g 7 0 2 O d e 0 8 R G X q j r M B / q f Q 0 y n v M / E S I u 2 S A L R N f j k W T U 2 A b 9 2 Y i R 3 9 + O 4 v S f C I h D c y N R p P Y k 5 m n J u 4 N o w I C G d / h h R 8 U E Y 7 V C 8 Z + 4 Q c S l 2 d x c f G p + v E O t 8 6 9 g 7 X + r r e 5 t b 1 3 u N f f O z r s e Z p a b B v J M v 5 V 8 x D L g 2 S O P r w D v 7 j x B N 4 I V 1 d 4 2 c g L / N x c q y k X M I K i i N J Q j W o Y l Z G w z U Z R O C k A Z 5 T C N M i S x E + H s 2 s E Y x j J + W x 2 D Y n 3 V Q e v v 5 7 P 7 7 U J c C O g s K 0 2 t N X W r o j C c e 3 s R B l t r W S W 2 z b 9 L G 9 r M c i k z B L b a F 1 b 9 9 p V 8 / Z t M / + h F g P b Y v B Q i 8 C 2 C B 5 q M b Q t h q o F b s M u z i 5 W M / R 8 D 9 u r X Y c R h s j Q w 7 V J X B 9 4 r c D 5 m 9 W 3 6 G U w 8 p Z W l R P 0 s q 0 3 x e w a a g q W v T i b Q v E s w I B b W b x G l 3 p Z Y b S 0 O j M b + N 0 1 W j P t o K 0 7 D j e S f r z i b a M Y h M S I U X s v 1 I 4 h b z x u W 4 / b T Y + a l t P M 3 n P p R X V X 4 d l G H s 6 o M l 7 Y H h 8 m / p C 6 L L 1 c e n W v 2 3 L d x 1 6 9 5 K 5 e 6 e n 0 j K o f X Q 5 U v h l 8 F Q L O e r Q 4 s A t i e v d s 7 1 5 L 7 x P b S 0 f 0 N K u j b K V e G H N 3 o V e m j s E H l q b p c F w A N F 0 y f 0 s v 7 3 u k V W O + X 9 7 3 7 a c e 4 C a o z i 1 L B h / M n L c + r H z 3 l X X 9 9 e N O o t T L f a T k G E Q k m J 8 c H X 2 l P P 2 f j i Z 5 D o W n R m O c b N W D M S 2 c H V j z C n 9 K u 9 d w 9 u z Z M 7 / M o q E 3 E e q A i 0 Z e n g k R Y S I y r v P Y x w C s / D + 4 s b 4 6 l H O M x 5 a V U o z p X r V 5 W B 8 P T L J y t F E 7 2 v h e R z j n N A R 9 k p u 2 1 X J r u B 4 R C s 7 S 1 t W z Z w + K D U f n x 2 G G S W i c t M w T O T O 6 u t G j E 2 W u 7 s 1 0 z b p a a 3 F l w 8 b e D y d R + 3 r 8 S O k 7 n d a + t 9 O 9 R U X 1 y m r m T H 0 K N c N V V 4 9 t i u n f V G + 3 7 t 9 1 + 9 u Z 1 j f A U a v r B w d c C Q 6 i W I k 1 V h e Y F r C B u q r 8 j e I s K z S t r w y v L 6 s G S A 2 S 2 W o z Z 8 k C A 2 E + u 1 b 1 Q + D H s 8 1 m g 9 K P o y F v 8 M 5 c F 8 n M U P N 7 L k H I 9 g 6 a m d c z g l y o T J m L K M 7 S K s u d o I s s 8 U q / i H y M V q t v k P 5 M e b 6 V a V Y k 6 P X J N U J P 5 n Y 5 i w b t E z N w m Q E x g c s E x A x d Z k g M u A w Q M 3 K Z E T G h y 4 T E j F 1 m T E z k M h E x 7 1 3 m P T E 3 L n N D T O w y 8 V z L u E i 8 S G D E Y o k + v F O H n d n B Z e / 9 R E h v m K V f S k / V y y j H O 3 X y O B v j J Z X v 1 P W d 0 l 0 z l 8 m I y V 0 m J + a D y 3 w g p n C Z g h j h M o I Y 6 T K S m I n L T I g p X a Y k Z u o y U 2 J u X e a W m D u X u S P m o 8 t 8 n J t i 0 Q Y A 5 v e s P t 7 L K k h m J p Q G I x Y 2 9 b g x / + o o s S 2 0 z X j G c X h A M I u N M i C Y B U Y 5 J J h F R Q k E s 5 A o R w S z e C h D g l k w l G O C W S S U E 4 J Z G J T v C W Y x U N 4 Q z A K g j A m O G Z w Q n D C Y L T R f 4 Y x g J u Y y J 5 g p u f x A M J N x W R D M N F w K g g X f V I J l + 5 p w 6 Z Y E M 9 2 W U 4 K Z a M t b g p l i y z u C m V z L j w R b r W 7 F o D 5 3 6 8 + M R Y t u w Y i u 9 V w G o 7 z W k x m M / F r P Z j A a b D 2 d w Q i x 9 X w G o 8 b W E x q M J F v P a D C 6 b D 2 l k X v w n A a j 0 N a T G o x M W 8 9 q M F p t n t a W S 1 w u 4 d y D J z E Y 6 b a e x W D 0 2 3 o a g x F x 6 3 k M R s m t J z I Y O b e e y W A 0 3 X o q g x F 2 6 7 k M R t 2 t J z M Y i b e e z W B 0 3 n o 6 g x F 7 6 / k M R v E P n 9 A Y C 0 U U 1 B V K s k b x s U Z h k 6 w T v M 7 g D Y I 3 G L x J 8 C a D t w j e Y v A 2 w d s M 3 i F 4 h 8 G 7 B O 8 y e I / g P Q a / J v g 1 g / c J 3 m d w h + A O g w 8 I P m D w I c G H D D 4 i + I j B X Y K 7 D D 4 m + J j B J w S f M L h H c I / B f Y L 7 D D 4 l + J T B Z w S f M f i c 4 H M G X x B 8 w e B L g i 8 Z f E X w 1 c P H q y s 6 M K p j G l 1 j + t X S Y 9 w 6 5 z Z c b o N z m y 6 3 y b k t l 9 v i 3 L b L b X N u x + V 2 O L f r c r u c 2 3 O 5 P c 6 9 d r n X n N t 3 u X 3 O d V y u w 7 k D l z v g 3 K H L H X L u y O W O O N d 1 u S 7 n j l 3 u m H M n L n f C u Z 7 L 9 T j X d 7 k + 5 0 5 d 7 p R z Z y 5 3 x r l z l z v n 3 I X L X X D u 0 u U u O X f l c l b 2 Z 7 y E K D + C / h y B n 1 2 f 1 3 3 L L I W Z / T x r s W R i o O u E k k Z d E y v c r Y f L C m b I w C B U h + g q B B G q P n T t g Q j V H G U 1 E q o 0 d J 2 B C N U X u r p A h K o K X V M g Q r W E r i Q Q o Q p C 1 w + I U N 2 g q w Z E q F r Q t Q I i M V s H g 1 B l o O s C R F K 2 f g a h K k D X A I h Q 7 t e Z H x H K + D r f I 0 J 5 X m d 5 R A R b c I N Q T i + r b W G b U h q E 8 r f O 3 o h Q 1 t Y 5 G x H K 1 T p T I 0 I Z W u d n R N q q U b c M L f 0 4 H 6 v 9 1 n 9 r B Z a D S h x a F x a k j 1 r 0 Z K K i Y j 8 Z D F U P c 0 F E l k C o c P 2 X Y C 1 J J U c L o E N E 8 D d B I g o T 1 V X / J d g K t x J t P Z H Z j I 9 / p s R q L R R r Q B Y K d c g m N V M C t R Y K d E Q W i j M k C 4 U 5 J g u H y 8 a K g n x P F o r x h q 3 N T I m w n v l M C d B a u J h s F V F 8 G V u S m R K d t V B 0 H 8 h C w R V s p W Z K a P U C z Z T I r I U L z Z Y Z B V a S h e K a k o X C u i U L R X V H F g r q 4 7 z 6 5 g z z 7 K 3 B d Y 5 F n V F u 1 Z k V E c q o O p 8 i Q n l U Z 1 F E K H v q 3 I k I 5 U y d M R G h T K n z J C K U H 3 V 2 R I S y o s 6 J i F A u 1 J k Q E c q A O v 8 h Q n l P Z z 1 E K N v p X I c I 5 T i d 4 R C h z K b z G i K U z 3 Q 2 Q 4 S y m M 5 h i F D u 0 p k L E c p Y O l 8 h Q n l K Z y l E K D v p 3 I Q I 5 S S d k R C h T K T z E C K U f 3 T 2 Q Y S y j s 4 5 i F C u 0 Z k G k S u 2 g 5 Q X B j w t J N 1 x d R B f 4 x V b P R v 6 i u l U 4 V 9 P r o p h x f V M H G s V 9 S E V 6 g v l T Q h i v w A U 1 X h N n U B 4 R 1 P s i V G k H p V C G m T D K A 3 R m T + J F S J G 9 X U y n w n 1 l L c H 8 i E H g y w e f p + b w e 0 c g 7 D 5 p D Y V + p t G k z c r f / o p d T U 1 a e r L V D D 1 y 3 W L k f 7 l h s U o A u S m x S g G 5 J b F K A r k t s U o D u S O x S g S 5 K 7 F K B b k n s U o G u R r i 1 E 8 y H 2 L U U T I j s U o J u S B x S g q 5 K H F K C 7 k k c U o M m T X Y h Q b 8 t h i F B 3 y x G I U H 7 J n M Y o Q 2 b c Y x Y g 8 t R h F i T y z G M W J P L c Y R Y q 8 s B j F i r y 0 G E W L v L K Y q c h Q y D u F n 4 8 N G 9 r P u Y H z c S N c Z z D p I t x g M E k j 3 G Q w q S P c Y j A J J N x m M G k k 3 G E w y S T c Z T A p J d x j M I k l f M 1 g 0 k u 4 z 2 C S T N h h M K k m P G A w C S c 8 Z D B p J z x i M M k n 7 D K Y F B Q e M 5 h E F J 4 w m H Q U 9 h h M U g r 7 D C Y 1 h a c M J k G F Z w w m T Y X n D C Z Z h R c M J m W F l w w m c Y V X D L Y V P x 5 t V a k m 6 q c o A y Y u s U 4 o a U t s E E r S E p u E a m U 9 9 T b 1 N x k T A Z 7 v C Z A e 3 j q G o b e 1 7 A 0 g 8 B U u x 5 H w p t k k H i K E F n h C f + + B t e S k 8 N Q b Q F m M j t R b M 3 C b Y 2 2 p v 8 y 1 X 8 x v 0 x 1 J n W K H U B K n 2 C W U t C n 2 C C V p i t e E k j L F P q E k T N E h l H Q p D g g l W Y p D Q k m V 4 o h Q E q X o E k q a F M e E k i T F C a G k S N E j l A Q p + o S S H s U p o S R H c U Y o q V G c E 0 p i F B e E k h b F J a E k R X F F a P 3 I J c W 6 D / R H C N 8 8 b K m K Q K A K o O M W / 6 o 8 X C M L p b p O F k p 0 g y y U 5 i Z Z e N h t k Y U i 2 i Y L x b N D F o p m l y w U y x 5 Z K J L X Z K E 4 9 s l C U X T I Q j E c k I U i O C Q L N / + I L N z 0 L l m 4 2 c d k 4 S a f k I W b 2 y M L N 7 V P F m 7 m K V m 4 i W d k 4 e a d k 4 W b d k E W b t Y l W b h J V + x + V a V V V V l q y 4 B v m T Q V F x 4 p K n 7 1 S 3 0 Y x A Z d 9 q a R H G c T 6 W G 5 4 0 0 x p e V Q u A U R U E X k V E P V 7 W W t A d 3 w X i E I u l y C R r 0 E u m C C R s U E u m S C R s 0 E u m i C R t U E u m y C R t 0 E u n C C R u U E u n S C R u 0 E u n i C R v U E u n y C R v 0 E u o C C R g U F u o S C R g 0 F u o i C R h U F u o y C R h 0 F u p C C R i U F u p S C R i 0 F u p i C R j U F u p y C R j 0 F u q C C R k U F u q S C R k 0 F u q i C R l U F u q y C R l 0 F u r C C R m U F u r S C R m 0 F u r i C R n U F u r y C R n 0 F u s A C V m H h J w V M O b K Y g D d J h 1 D E d + q l p a E v f S + E F A r M N s q O B C p 9 M F G p x 5 V t r p r O Z / m 7 2 X W R z L S h E 5 / y C k k e F R G m P K d / / Q b i 4 E 6 n O / 0 a i L o J 5 s e G b / u G y N i X + E n d v Y X T s s t b d u d t g 0 m y I c S P T U Q 3 q G d i r H v 3 q R p 1 H 2 u U y y g e Q t X y W h v 1 6 O s e e E z I L B j 7 Q r 1 / 6 0 9 k p j 9 B Q e G M s P E a b G 7 a 1 G O s u t w f w B C c d s Z s a V c g g Y e O b W d M 1 E K g n 6 G 5 j W M / j / 0 A 5 v U b N Z 0 K m H t P v e r a X V 6 3 / 9 a 8 z n h b z Y F 0 B H t p p 9 N k T + Y 8 t z d O z S R n a 9 w g 4 2 J u n 7 u 5 R A H h v H 6 S 1 q Q C S X N U V j S K o G i 6 F t l I J v 4 t t b R A s x 0 m i 0 y / x G Q e s t 3 3 k s c T N f u P 6 k m A y + 5 3 5 v w N p v 3 O v Q 0 8 8 w s a g T K a / i X + 8 Q v c + y J j L X v 3 N m A j K 4 l W h h L o e R a P C j 9 R D 6 T G 0 6 z A A l X 4 d 8 J 7 0 v n 2 x R P 1 9 o 5 + d 3 2 S m n d Z R Y 7 7 L / T b Y 0 + u I Y 5 Z G / t A 9 K m 3 j g k Q Q z 5 V v + 4 w 3 i F R b 7 G p K t g 4 Z a 3 V i 6 j Z J N Q 5 U x f F k Y R l 7 V 5 k 3 j A D 5 W 4 a 3 U Q 5 D C N / p f E i c 1 Y k s X p 4 P 5 9 1 v n 0 + b y G z F B S 3 2 s b J q e 7 3 o o 3 L F Z O 3 M F o L n W + v o 3 Q k 7 5 q h Y 1 5 R x V 3 u + i p Y e o B n r f B D U K + v p l l V 0 E u 4 X f E 2 x p l Q y 5 O p A j A Y e 5 v 4 2 T e F L 4 U 3 y L K b l U X n c c 5 R r k 7 n r P g D a r w I 9 Q D w 7 / W y u n q s o T o n T U O 8 a n e p 1 Y r N 9 O 8 H W v R R U H 3 1 h l 8 M 8 t o f Y J z F 2 X R Q g H + z + O 7 z p d X m / 5 u 4 f 3 H 2 Y m X 1 j y u v j l 8 t f b N e / Z + K z x Z + t / D 7 h a 8 W V h f + t P D N w u 5 C d + F 0 I V g I F / 6 6 8 L e F v 6 / + Y / W f q / 9 e / Y 9 p + u k n V Z / f L D g / q / / 9 H 5 V K Z q c = &lt; / l a t e x i t &gt; k = &lt; l a t e x i tV j V F a S P A E u u c 7 m J f T n X V c O h B H k d C P n / + r 0 8 + / c E P f / T j n 3 z 2 0 8 W f / f w X v / z V 5 1 / 8 + k x k k y K A 0 y C L s + J i 4 A u I o x R O Z S R j u M g L 8 J N B D O e D m w 3 F n 5 d Q i C h L + / I u h 7 e J H 6 b R K A p 8 i V D v 5 i 8 v 3 3 2 + 9 H z l u f 7 x 7 l + s V h d L C 9 V P 9 9 0 X q 7 + 9 H m b B J I F U B r E v x J v V 5 7 l 8 O / M L G Q U x z B e v J w J y P 7 j x Q 3 i D l 6 m f g H g 7 0 2 O d e 0 8 R G X q j r M B / q f Q 0 y n v M / E S I u 2 S A L R N f j k W T U 2 A b 9 2 Y i R 3 9 + O 4 v S f C I h D c y N R p P Y k 5 m n J u 4 N o w I C G d / h h R 8 U E Y 7 V C 8 Z + 4 Q c S l 2 d x c f G p + v E O t 8 6 9 g 7 X + r r e 5 t b 1 3 u N f f O z r s e Z p a b B v J M v 5 V 8 x D L g 2 S O P r w D v 7 j x B N 4 I V 1 d 4 2 c g L / N x c q y k X M I K i i N J Q j W o Y l Z G w z U Z R O C k A Z 5 T C N M i S x E + H s 2 s E Y x j J + W x 2 D Y n 3 V Q e v v 5 7 P 7 7 U J c C O g s K 0 2 t N X W r o j C c e 3 s R B l t r W S W 2 z b 9 L G 9 r M c i k z B L b a F 1 b 9 9 p V 8 / Z t M / + h F g P b Y v B Q i 8 C 2 C B 5 q M b Q t h q o F b s M u z i 5 W M / R 8 D 9 u r X Y c R h s j Q w 7 V J X B/ i H y M V q t v k P 5 M e b 6 V a V Y k 6 P X J N U J P 5 n Y 5 i w b t E z N w m Q E x g c s E x A x d Z k g M u A w Q M 3 K Z E T G h y 4 T E j F 1 m T E z k M h E x 7 1 3 m P T E 3 L n N D T O w y 8 V z L u E i 8 S G D E Y o k + v F O H n d n B Z e / 9 R E h v m K V f S k / V y y j H O 3 X y O B v j J Z X v 1 P W d 0 l 0 z l 8 m I y V 0 m J + a D y 3 w g p n C Z g h j h M o I Y 6 T K S m I n L T I g p X a Y k Z u o y U 2 J u X e a W m D u X u S P m o 8 t 8 n J t i 0 Q Y A 5 v e s P t 7 L K k h m J p Q G I x Y 2 9 b g x / + o o s S 2 0 z X j G c X h A M I u N M i C Y B U Y 5 J J h F R Q k E s 5 A o R w S z e C h D g l k w l G O C W S S U E 4 J Z G J T v C W Y x U N 4 Q z A K g j A m O G Z w Q n D C Y L T R f 4 Y x g J u Y y J 5 g p u f x A M J N x W R D M N F w K g g X f V I J l + 5 p w 6 Z Y E M 9 2 W U 4 K Z a M t b g p l i y z u C m V z L j w R b r W 7 F o D 5 3 6 8 + M R Y t u w Y i u 9 V w G o 7 z W k x m M / F r P Z j A a b D 2 d w Q i x 9 X w G o 8 b W E x q M J F v P a D C 6 b D 2 l k X vv n 3 I X L X X D u 0 u U u O X f l c l b 2 Z 7 y E K D + C / h y B n 1 2 f 1 3 3 L L I W Z / T x r s W R i o O u E k k Z d E y v c r Y f L C m b I w C B U h + g q B B G q P n T t g Q j V H G U 1 E q o 0 d J 2 B C N U X u r p A h K o K X V M g Q r W E r i Q Q o Q p C 1 w + I U N 2 g q w Z E q F r Q t Q I i M V s H g 1 B l o O s C R F K 2 f g a h K k D X A I h Q 7 t e Z H x H K + D r f I 0 J 5 X m d 5 R A R b c I N Q T i + r b W G b U h q E 8 r f O 3 o h Q 1 t Y 5 G x H K 1 T p T I 0 I Z W u d n R N q q U b c M L f 0 4 H 6 v 9 1 n 9 r B Z a D S h x a F x a k j 1 r 0 Z K K i Y j 8 Z D F U P c 0 F E l k C o c P 2 X Y C 1 J J U c L o E N E 8 D d B I g o T 1 V X / J d g K t x J t P Z H Z j I 9 / p s R q L R R r Q B Y K d c g m N V M C t R Y K d E Q W i j M k C 4 U 5 J g u H y 8 a K g n x P F o r x h q 3 N T I m w n v l M C d B a u J h s F V F 8 G V u S m R K d t V B 0 H 8 h C w R V s p W Z K a P U C z Z T I r I U L z Z Y Z B V a S h e K a k o X C u i U L R X V H F g r q 4 7 z 6 5 g z z 7 K 3 B d Y 5 F n V F u 1 Z k V E c q o O p 8 i Q n l U Z 1 F E K H v q 3 I k I 5 U y d M R G h T K n z J C K U H 3 V 2 R I S y o s 6 J i F A u 1 J k Q E c q A O v 8 h Q n l P Z z 1 E K N v p X I c I 5 T i d 4 R C h z K b z G i K U z 3 Q 2 Q 4 S y m M 5 h i F D u 0 p k L E c p Y O l 8 h Q n l K Z y l E K D v p 3 I Q I 5 S S d k R C h T K T z E C K U f 3 T 2 Q Y S y j s 4 5 i F C u 0 Z k G k S u 2 g 5 Q X B j w t J N 1 x d R B f 4 x V b P R v 6 i u l U 4 V 9 P r o p h x f V M H G s V 9 S E V 6 g v l T Q h i v w A U 1 X h N n U B 4 R 1 P s i V G k H p V C G m T D K A 3 R m T + J F S JW L k f 7 l h s U o A u S m x S g G 5 J b F K A r k t s U o D u S O x S g S 5 K 7 F K B b k n s U o G u R r i 1 E 8 y H 2 L U U T I j s U o J u S B x S g q 5 K H F K C 7 k k c U o M m T X Y h Q b 8 t h i F B 3 y x G I U H 7 J n M Y o Q 2 b c Y x Y g 8 t R h F i T y z G M W J P L c Y R Y q 8 s B j F i r y 0 G E W L v L K Y q c h Q y D u F n 4 8 N G 9 r P u Y H z c S N c Z z D p I t x g M E k j 3 G Q w q S P c Y j A J J N x m M G k k 3 G E w y S T c Z T A p J d x j M I k l f M 1 g 0 k u 4 z 2 C S T N h h M K k m P G A w C S c 8 Z D B p J z x i M M k n 7 D K Y F B Q e M 5 h E F J 4 w m H Q U 9 h h M U g r 7 D C Y 1 h a c M J k G F Z w w m T Y X n D C Z Z h R c M J m W F l w w m c Y V X D L Y V P x 5 t V a k m 6 q c o A y Y u s U 4 o a U t s E E r S E p u E a m U 9 9 T b 1 N x k T A Z 7 v C Z A e 3 j q G o b e 1 7 A 0 g 8 B U u x 5 H w p t k k H i K E F n h C f + + B t e S k 8 N Q b Q F m M j t R b M 3 C b Y 2 2 p v 8 y 1 X 8 x v 0 x 1 J n W K H U B K n 2 C W U t C n 2 C C V p i t e E k j L F P q E k T N E h l H Q p D g g l W Y p D Q k m V 4 o h Q E q X o E k q a F M e E k i T F C a G k S N E j l A Q p + o S S H s U p o S R H c U Y o q V G c E 0 p i F B e E k h b F J a E k R X F F a P 3 I J c W 6 D / R H C N 8 8 b K m K Q K A K o O M W / 6 o 8 X C M L p b p O F k p 0 g y y U 5 i Z Z e N h t k Y U i 2 i Y L x b N D F o p m l y w U y x 5 Z K J L X Z K E 4 9 s l C U X T I Q j E c k I U i O C Q L N / + I L N z 0 L l m 4 2 c d k 4 S a f k I W b 2 y M L N 7 V P F m 7 m K V m 4 i W d k 4 e a d k 4 W b d k E W b t Y l W b h J V + x + V a V V V V l q y 4 B v m T Q V F x 4 p K n 7 1 S 3 0 Y x A Z d 9 q a R H G c T 6 W G 5 4 0 0 x p e V Q u A U R U E X k V E P V 7 W W t A d 3 w X i E I u l y C R r 0 E u m C C R s U E u m S C R s 0 E u m i C R t U E u m y C R t 0 E u n C C R u U E u n S C R u 0 E u n i C R v U E u n y C R v 0 E u o C C R g U F u o S C R g 0 F u o i C R h U F u o y C R h 0 F u p C C R i U F u p S C R i 0 F u p i C R j U F u p y C R j 0 F u q C C R k U F u q S C R k 0 F u q i C R l U F u q y C R l 0 F u r C C R m U F u r S C R m 0 F u r i C R n U F u r y C R n 0 F u s A C V m H h J w V M O b K Y g D d J h 1 D E d + q l p a E v f S + E F A r M N s q O B C p 9 M F G p x 5 V t r p r O Z / m 7 2 X W R z L S h E 5 / y C k k e F R G m P K d / / Q b i 4 E 6 n O / 0 a i L o J 5 s e G b / u G y N i X + E n d v Y X T
s s t b d u d t g 0 m y I c S P T U Q 3 q G d i r H v 3 q R p 1 H 2 u U y y g e Q t X y W h v 1 6 O s e e E z I L B j 7 Q r 1 / 6 0 9 k p j 9 B Q e G M s P E a b G 7 a 1 G O s u t w f w B C c d s Z s a V c g g Y e O b W d M 1 E K g n 6 G 5 j W M / j / 0 A 5 v U b N Z 0 K m H t P v e r a X V 6 3 / 9 a 8 z n h b z Y F 0 B H t p p 9 N k T + Y 8 t z d O z S R n a 9 w g 4 2 J u n 7 u 5 R A H h v H 6 S 1 q Q C S X N U V j S K o G i 6 F t l I J v 4 t t b R A s x 0 m i 0 y / x G Q e s t 3 3 k s c T N f u P 6 k m A y + 5 3 5 v w N p v 3 O v Q 0 8 8 w s a g T K a / i X + 8 Q v c + y J j L X v 3 N m A j K 4 l W h h L o e R a P C j 9 R D 6 T G 0 6 z A A l X 4 d 8 J 7 0 v n 2 x R P 1 9 o 5
+ d 3 2 S m n d Z R Y 7 7 L / T b Y 0 + u I Y 5 Z G / t A 9 K m 3 j g k Q Q z 5 V v + 4 w 3 i F R b 7 G p K t g 4 Z a 3 V i 6 j Z J N Q 5 U x f F k Y R l 7 V 5 k 3 j A D 5 W 4 a 3 U Q 5 D C N / p f E i c 1 Y k s X p 4 P 5 9 1 v n 0 + b y G z F B S 3 2 s b J q e 7 3 o o 3 L F Z O 3 M F o L n W + v o 3 Q k 7 5 q h Y 1 5 R x V 3 u + i p Y e o B n r f B D U K + v p l l V 0 E u 4 X f E 2 x p l Q y 5 O p A j A YW v R R U H 3 1 h l 8 M 8 t o f Y J z F 2 X R Q g H + z + O 7 z p d X m / 5 u 4 f 3 H 2 Y
m X 1 j y u v j l 8 t f b N e / Z + K z x Z + t / D 7 h a 8 W V h f + t P D N w u 5 C d + F 0 I V g I F / 6 6 8 L e F v 6 / + Y / W f q / 9 e / Y 9 p + u k n V Z / f L D g / q / / 9 H 8 c T Z q g = &lt; / l a t e x i t &gt; k = Building upon these works, we propose TSMixup, which generalizes the idea of Mixup to more than two datapoints.Concretely, TSMixup randomly samples k ∼ U{1, K} time series of a specific length, l ∼ U{l min , l max }, from the training datasets, scales them, and takes their convex combination, xTSMixup
1:l = k i=1 λ i x(i) 1:l ,(3)
where x(i) 1:l denotes the i-th scaled time series.The time series are scaled before mixing to ensure that time series with small and large values are given equal importance in the mixing process.The combination weights, [λ 1 , . . ., λ k ], are sampled from a symmetric Dirichlet distribution, Dir(α), parameterized by the scalar concentration parameter α.The complete pseudocode of TSMixup can be found in Algorithm 1 in Appendix A. Intuitively, TSMixup enhances the diversity of data by combining patterns from different time series.Figure 2 shows example augmentations generated by TSMixup and illustrates how different patterns are mixed.</p>
<p>KernelSynth: Synthetic Data Generation using Gaussian Processes</p>
<p>While TSMixup improves pattern diversity, it may still prove insufficient for training a generalist time series model, especially when real data is limited.To further supplement the training dataset, we propose KernelSynth, a method to generate synthetic time series using Gaussian processes (GPs).KernelSynth is inspired by the Automatic Statistician (Duvenaud et al., 2013), where a compositional search over a space of GP kernels is performed to explain the structure of a time series.We use the inverse of this processrandomly compose GP kernels to generate new time series.</p>
<p>GPs are distributions over functions defined by the mean function, m(t), and the positive definite kernel, κ(t, t ′ ), where t ∈ R is the domain.The kernel specifies a covariance function which defines the joint variability of the function values at an arbitrary pair of points, (t, t ′ ), in the input domain.Diverse patterns can be generated by appropriately selecting the kernel.We constructed a kernel bank, K, of basis kernels defining fundamental time series patterns.These include linear kernels for trend, RBF kernels for smooth local variation, and periodic kernels for seasonalities found in typical time series frequencies.The final kernel, κ(t, t ′ ), is constructed by sampling j ∼ U{1, J} kernels from K with replacement and combining these kernels via random binary operations, + or ×.A synthetic time series is generated by drawing a sample of length l syn from the GP prior, GP(m(t) = 0, κ(t, t ′ )); see Algorithm 2 in Appendix A for details. Figure 3 depicts this generative process used in KernelSynth, illustrating how time series with intricate patterns can arise from the composition of simple basis kernels.</p>
<p>Experiments</p>
<p>In this section, we present empirical results on commonly used benchmark datasets.First, we give an overview of the datasets, training strategy, baselines, and evaluation metrics (Section 5.1-5.4).Table 1 provides a high-level summary of the datasets and baselines used in our experiments.We then (a) evaluate the performance of Chronos models in the in-domain and zero-shot settings against local models and task-specific deep learning models (Section 5.5); (b) analyze the effect of various design choices such as model size, initialization, synthetic data proportion, context length, and vocabulary size on the performance of Chronos models (Section 5.6); and (c) analyze the qualitative performance of Chronos models and highlight their limitations (Section 5.7).We discuss our key findings in this section and relegate specific experiment details to the appendices.</p>
<p>Datasets</p>
<p>To train and evaluate Chronos models, we collected a wide variety of publicly available datasets spanning various application domains including energy, transport, healthcare, retail, web, weather, finance, and with sampling frequencies ranging from 5 minutes up to yearly.The complete list of datasets, together with their respective sources and additional details, is given in Appendix B. In total, our dataset collection comprises 55 datasets from multiple sources, including the Monash Time Series Forecasting Repository (Godahewa et al., 2021), the M-competitions (Makridakis et al., 1979;Makridakis &amp; Hibon, 2000;Makridakis et al., 2020;2022), and public domain datasets from Kaggle.1</p>
<p>We categorize this collection into three subsets, based on how we use them for training and evaluating Chronos models: (a) datasets exclusively used for training (13 datasets); (b) Benchmark I datasets, employed for both training and evaluation, representing an in-domain evaluation (15 datasets); and (c) Benchmark II datasets, used solely for evaluation, constituting a zero-shot evaluation (27 datasets).In categorizing the datasets in this way, we tried to find a good balance between keeping as many datasets as possible for the zero-shot evaluation of Chronos models, among the ones most commonly used in the literature, while still having enough variety of domains and sampling frequencies in the training data.Overall, we used 28 datasets for training Chronos models, consisting of about 890K univariate time series with approximately 84B observations (tokens) in total.For both in-domain (I) and zero-shot (II) benchmark datasets, we used the last H ∈ N + observations of each time series as a held-out test set: all models are judged by the accuracy of their forecast on such held-out set, which no model had access to for training purposes.The prediction length H is task-specific (see Table 3 in Appendix B), where we define a task as a dataset and prediction length pair.Tasks in both benchmarks exhibit diverse properties, in terms of the dataset size, frequency, history length, and prediction length, making them rich benchmarks reflective of real world scenarios.</p>
<p>Training Corpus and Protocols</p>
<p>We selected T5 (Raffel et al., 2020) as the main architecture for Chronos in our experiments, since it is available in a variety of sizes, ranging from 16M (Tiny) to 11B (XXL) parameters (Tay et al., 2021).We also conducted experiments with the decoder-only GPT-2 model to demonstrate the applicability of the Chronos framework to decoder-only models.In the following, we discuss the training configurations used for our main results (Section 5.5) and explore alternatives for some of the hyperparameters in Section 5.6.</p>
<p>We trained T5 models of 4 sizes,2 namely, Mini (20M), Small (46M), Base (200M) and Large (710M), and the GPT-2 base model (90M), on 10M TSMixup augmentations (see Section 4.1) generated from the 28 training datasets, with K = 3 in Algorithm 1, and 1M synthetic time series generated using Gaussian processes (see Section 4.2).Note that with this setup, original time series are adequately represented since they are included in the TSMixup augmentations with probability 1/3.We sampled time series from the augmentations and synthetic data in the ratio 9:1 during training.Each model is trained with an effective batch size of 256 sequences, using distributed data parallelism and gradient accumulation, whenever necessary.These sequences were constructed by slicing random windows from the time series, and then scaling and quantizing them into equal-sized bins within the interval [c 1 = − 15, c B = + 15], as described in Section 3.1.We set the vocabulary size, V ts , to 4096, including the special tokens (PAD and EOS).The context length of the sequences was set to 512, the default for T5 models, and the prediction length was set to 64, a value greater than the prediction lengths of all tasks we consider in our evaluation.</p>
<p>The models were optimized for 200K steps using the AdamW optimizer with a weight decay of 0.01.The learning rate was annealed linearly from its initial value of 0.001 to 0 over the training steps.The other model and training hyperparameters were set to their defaults used in the transformers library (Wolf et al., 2020).</p>
<p>We used an AWS EC2 instance with 8 A100 (40GB) GPUs to train all Chronos models, and we employed faster floating point formats (TF32) and model compilation to speed up training.Table 6 in Appendix E reports the training time and the approximate cost of training Chronos models of different sizes.</p>
<p>Baselines</p>
<p>We assessed the performance of Chronos models against a variety of time series forecasting baselines.From statistical forecasting literature (Hyndman &amp; Athanasopoulos, 2018), we included Naive, Seasonal Naive, AutoETS, AutoARIMA (Hyndman et al., 2008), AutoTheta (Assimakopoulos &amp; Nikolopoulos, 2000) and a strong ensemble (SCUM) of statistical models (Petropoulos &amp; Svetunkov, 2020).Additionally, we compared against several neural forecasting baselines, including WaveNet (Oord et al., 2016), DeepAR (Salinas et al., 2020), N-BEATS (Oreshkin et al., 2020), TFT (Lim et al., 2021), DLinear (Zeng et al., 2023), PatchTST (Nie et al., 2023), N-HiTS (Challu et al., 2023), and GPT4TS (Zhou et al., 2023a).Furthermore, from the recently proposed pretrained time series models, we included the ones with publicly available weights: Lag-Llama (Rasul et al., 2023) and Moirai-1.0-R(Woo et al., 2024).On Benchmark II (i.e., zero-shot datasets for Chronos models), we also evaluated against two zero-shot methods: ForecastPFN (Dooley et al., 2023) which is a transformer model pretrained only on synthetic time series data and LLMTime (Gruver et al., 2023) which uses LLMs for zero-shot forecasting.</p>
<p>We categorize Chronos models and the baselines into three groups: local models that estimate parameters for each time series individually; task-specific models trained or fine-tuned for each task separately; and pretrained models which do not perform task-specific training, instead using a single model across all tasks.Further details on the implementation and training of these baselines can be found in Appendix C.</p>
<p>Evaluation Metrics</p>
<p>Whenever possible,3 we evaluated models both in terms of their probabilistic and point forecast performance.We used the weighted quantile loss (WQL) to assess the quality of the probabilistic forecasts: the WQL is related to the continuous ranked probability score (CRPS, Gneiting &amp; Raftery ( 2007))4 and is commonly used to evaluate probabilistic forecasts (Gasthaus et al., 2019;Shchur et al., 2023).The WQL measures the compatibility between the predictive distribution and the ground-truth observation at a uniformlyspaced grid of quantile levels; we compute the WQL on 9 uniformly-spaced quantile levels {0.1, 0.2, . . ., 0.9}.Quantile forecasters such as TFT were directly trained on these quantile levels.For methods requiring sampling, we estimated the quantiles using 20 sample forecast paths.We used the mean absolute scaled error (MASE, Hyndman &amp; Koehler (2006)) to evaluate the point forecast performance.The MASE is defined as the absolute error of the forecast scaled by the historical seasonal error of the time series, and was selected due to its favorable properties over other point forecasting metrics (Hyndman &amp; Koehler, 2006).We used the median forecast (0.5-quantile) for computing the MASE for the probabilistic forecasters.See Appendix D for a detailed discussion on the evaluation metrics.</p>
<p>Since the magnitude of the evaluation metrics can vary across datasets, we adopt a different approach to aggregate scores than naive averaging.For each dataset, we compute the relative score of each model as the model's score divided by the score of a baseline model (here, Seasonal Naive).The relative scores are aggregated across all datasets using the geometric mean.The choice of the geometric mean is deliberate -Fleming &amp; Wallace (1986) show that the arithmetic mean can yield misleading conclusions in this context, and the geometric mean is provably the only meaningful way to aggregate such relative scores.Furthermore, the geometric mean is also not sensitive to the choice of the baseline, and the model ordering stays intact if another baseline is selected instead.We used Seasonal Naive due to its simplicity and popularity as a forecasting baseline.For models that failed or could not finish evaluation within the allotted time on certain datasets, we used a relative score of 1, i.e., the baseline relative score, when aggregating the results.We assign equal weights to all tasks during aggregation, reflecting real-world scenarios where datasets may have different numbers of time series, frequencies, history and prediction lengths.</p>
<p>Main Results</p>
<p>In this section, we present our main results on 42 datasets, which comprise Benchmark I (15 datasets) and Benchmark II (27 datasets).Chronos models surpass classical statistical baselines, task-specific deep learning models, and other pretrained models on the in-domain datasets (Benchmark I; see Section 5.5.1).On the zero-shot datasets (Benchmark II; Section 5.5.2),Chronos models comfortably outperform statistical baselines and other pretrained models, while performing on par with the best deep learning models trained on these tasks.With an inexpensive fine-tuning regimen, our Chronos-T5 (Small) model achieves the top spot on Benchmark II, significantly outperforming all baselines.indicates that the in-domain setting does not apply to these models as they were trained on different corpora than Chronos.Specifically, this means that some datasets in Benchmark I were not part of their training corpus and (or) they were trained on the test sets of some datasets in Benchmark I.The probabilistic (WQL) and point (MASE) forecasting metrics (lower is better) are normalized using the scores of the Seasonal Naive baseline and aggregated through a geometric mean to obtain the aggregated relative WQL and MASE, respectively.Results for Chronos and task-specific models (except GPT4TS) have been averaged over 3 random seeds.Models producing point-forecasts (GPT4TS) are only compared based on MASE.</p>
<p>Benchmark I: In-domain Results</p>
<p>%KK6IPEXMZI;50</p>
<p>Benchmark I comprises 15 datasets that were also part of the training data of Chronos models, i.e., this benchmark evaluates the in-domain performance of Chronos models (see Table 3).Figure 4 summarizes the probabilistic and point forecasting performance for all models on the held-out test windows, in terms of their aggregated relative scores, computed as described in Section 5.4.The bigger Chronos-T5 models (Base and Large) significantly outperform baseline models, obtaining the best aggregated relative scores and average ranks (Figure 18 in Appendix E).These models not only perform better than local models (e.g., AutoETS and AutoARIMA), but they also perform better than task-specific deep learning models trained or fine-tuned for each dataset (e.g., PatchTST and DeepAR) and other pretrained models (e.g., Lag-Llama and Moirai-1.0-R).</p>
<p>The smaller Chronos-T5 models (Mini and Small) and Chronos-GPT2 also perform better than the majority of baselines.Between the two baseline pretrained models studied in this experiment, Moirai-1.0-Rclearly outperforms Lag-Llama.Notably, the best Moirai-1.0-Rmodel (Large, 311M) is still outperformed by the smallest Chronos-T5 model (Mini, 20M) even though Moirai-1.0-Rmodels were trained on a significantly larger corpus of time series data.Task-specific deep learning models, trained across multiple time series for a specific task, perform better than local statistical models that fit parameters for each time series.Interestingly, the Seasonal Naive baseline performs competitively against other local models on this benchmark, suggesting that the datasets in this benchmark exhibit strong seasonal patterns.This is unsurprising since a majority of these datasets belong to domains such as energy and transport that tend to be highly seasonal in nature.The raw WQL and MASE values for individual datasets summarized in Figure 4 can be found in Tables 7 and 8 in Appendix E.</p>
<p>These results demonstrate the benefit of using models that are trained only once across multiple datasets, over task-specific models trained individually for each task.Such models could streamline production forecasting systems, where forecasts from different time series tasks are required, by obviating the need for training separate models for each task.Figure 5: Performance of different models on Benchmark II, comprising 27 datasets not seen by Chronos models during training.This benchmark provides insights into the zero-shot performance of Chronos models against local statistical models, which fit parameters individually for each time series, task-specific models trained on each task, and pretrained models trained on a large corpus of time series data.Pretrained Models (Other) indicates that the zero-shot setting does not apply to these models as they were pretrained on some datasets in Benchmark II.The probabilistic (WQL) and point (MASE) forecasting metrics (lower is better) were normalized using the scores of the Seasonal Naive baseline and aggregated through a geometric mean to obtain the aggregated relative WQL and MASE, respectively.Results for Chronos and task-specific models (except GPT4TS) have been averaged over 3 random seeds.Models producing point-forecasts (GPT4TS and ForecastPFN) are only compared based on MASE.</p>
<p>Benchmark II: Zero-shot Results</p>
<p>%KK6IPEXMZI</p>
<p>Benchmark II consists of 27 datasets that were not used during Chronos models' training (see Table 3 in appendix B), i.e., this benchmark evaluates the zero-shot performance of these models.These datasets belong to diverse domains and frequencies, some of which are not even part of the training data, making this a challenging benchmark for Chronos. 5Figure 5 summarizes the results on Benchmark II in terms of the aggregated relative scores.This benchmark is clearly more challenging than Benchmark I (Figure 4), as the best models tend to offer lower improvements relative to the baseline.</p>
<p>Nevertheless, despite never having seen these datasets during training, Chronos models significantly outperform standalone local statistical models.On probabilistic forecasting (aggregate relative WQL), Chronos models achieve the 2 nd to 4 th spots, performing better than most task-specific models that have been trained on these tasks.In terms of the point forecasting performance, Chronos-T5 (Large) places 2 nd , surpassing most baselines, including the strong SCUM ensemble.Chronos models also significantly outperform other pretrained models such as Moirai-1.0-R,Lag-Llama, LLMTime, and ForecastPFN, and even GPT4TS, which fine-tunes a pretrained GPT-2 model on each dataset.Moirai-1.0-Robtains the best performance after Chronos, although the evaluation setup may have been advantageous for Moirai-1.0-Ras many datasets in Benchmark II were part of its pretraining corpus.The raw WQL and MASE values for individual datasets summarized in Figure 5 can be found in Tables 9 and 10 in Appendix E.</p>
<p>The results on this benchmark highlight the promise of Chronos as a generalist time series forecasterit performs significantly better than local models that are commonly used in a zero-shot setting, and it performs on par with the best task-specific deep learning models.</p>
<p>%KK6IPEXMZI7GSVI</p>
<p>;50 1%7)</p>
<p>&amp;IRGLQEVO--'LVSRSW87QEPP</p>
<blockquote>
<p>IVS7LSX *MRI8YRIH</p>
</blockquote>
<p>Figure 6: When fine-tuned on individual datasets from Benchmark II, Chronos-T5 (Small) significantly improves over the zeroshot performance and becomes the best performing model on average (see Figure 5).</p>
<p>Fine tuning.Motivated by the remarkable zero-shot performance of Chronos models, we conducted a preliminary investigation into fine-tuning Chronos models individually on datasets from Benchmark II.</p>
<p>We selected the Chronos-T5 (Small) model for this experiment due to its good zero-shot performance with a relatively low training cost.We fine-tuned the model in a datasetagnostic fashion with an initial learning rate of 0.001, annealed linearly to 0 over 1000 steps.Figure 6 shows that fine-tuning significantly improves the aggregate performance of the model on Benchmark II.The fine-tuned Chronos-T5 (Small) model now takes the top spot on Benchmark II overall, overtaking both larger (zero shot) Chronos models and the best taskspecific models.Notably, Chronos-T5 (Small) is not even the most accurate variant of Chronos on Benchmark II in the zero shot setting, suggesting that further improvements may be obtained by fine-tuning larger Chronos-T5 variants.</p>
<p>Analysis of Hyperparameters</p>
<p>Here, we explore the effect of different design choices on the downstream model performance, beginning with a comparison of different model sizes and initializations.We then analyze the effect of training steps, synthetic data proportion, context length, and vocabulary size, on the performance of Chronos-T5 (Small).</p>
<p>We only vary the parameter of interest, keeping everything else fixed to the value used in the main results.</p>
<p>Model size.</p>
<p>We experimented with four model sizes ranging from 20M to 710M parameters.6 Unsurprisingly, the training loss improves with the model capacity, as shown in Figure 7a.We also observe this trend in the downstream model performance -it improves with the model size for both in-domain and zero-shot benchmarks, as shown in Figure 7b.These trends suggest that even larger models may improve performance further.However, we did not explore larger models due to slow inference times which would render them impractical for real-world applications.</p>
<p>Initialization.We investigated whether initializing Chronos models to the corresponding T5 language models pretrained by Tay et al. (2021) on the C4 dataset (Raffel et al., 2020) has any impact on the training dynamics or the downstream performance.Figure 8 shows the training loss curve for models initialized randomly and those initialized with language model weights.Notably, models initialized randomly tend to converge to a lower training loss compared to their counterparts initialized with language model weights.</p>
<p>For the larger models (Base and Large), models initialized with language model weights initially exhibit a faster decrease in training loss, but they ultimately converge to a higher final loss.</p>
<p>Overall, these observations suggest that language model weights are not particularly remarkable in the context of time series forecasting and offer no improvement over random initialization.These conclusions are further reinforced by Figure 9 which shows the downstream performance of models initialized with language model weights against three randomly-initialized models of each size.Across all model sizes, the performance of models initialized with language model weights either overlaps with or slightly underperforms compared to randomly initialized models.These results suggest that LLM initialization offers relatively little advantage in the context of time series forecasting, and instead random initialization may be the preferable choice.TSMixup augmentations.As described in Section 5.2, we trained Chronos models on TSMixup augmentations rather than directly on the original time series.In this experiment, we investigate whether using TSMixup augmentations is advantageous for downstream performance.Figure 10a compares the performance of Chronos-T5 (Small, 46M) models trained with and without TSMixup augmentations.The model trained on TSMixup augmentations obtains similar in-domain performance to the model trained without augmentations.However, the zero-shot performance improves when using TSMixup augmentations.This suggests that TSMixup enchances the diversity of training data which leads to improved performance on unseen datasets.Figure 10a also shows that the zero-shot performance obtains an additional boost with the inclusion of synthetic data.We investigate this further in the next experiment.</p>
<p>Synthetic data proportion.We systematically explored the impact of KernelSynth on downstream model performance.We trained Chronos-T5 (Small, 46M) models with time series sampled from TSMixup augmentations and KernelSynth data in different ratios, ranging from 0% (i.e., trained solely on TSMixup augmentations) to 100% synthetic data.consistent improvement is observed around the 10% synthetic data proportion.Further increasing the proportion of synthetic data tends to worsen performance.This is unsurprising since the synthetic data generated using Gaussian processes is not representative of all real-world time series.</p>
<p>While the model trained only on synthetic data performs worse relative to models with real data in their training corpus, it performs reasonably well in terms of its absolute performance.Figure 20 (Appendix E) shows that it performs significantly better than ForecastPFN (Dooley et al., 2023), another model that is trained solely on synthetic data (generated differently from KernelSynth).Surprisingly, it also outperforms several other baselines in our benchmarks, Training steps.We trained a Chronos-T5 (Small, 46M) for 1M training steps to study the effect of longer training on model performance.Figure 11a shows that the downstream model performance improves over the course of training, both on in-domain and zero-shot benchmarks.This suggests that performance of the larger models (Base and Large) can potentially be improved by training them for longer.</p>
<p>Context length.We studied the effect of the context length on downstream performance by training Chronos-T5 (Small, 46M) models with four distinct context lengths.Figure 11b shows how the performance varies with increasing context length.We observe improvements on both in-domain and zero-shot metrics as context length increases up to 1024, showing that a longer context helps the models to forecast better to a certain degree.However, increasing the context length further tends to saturate or worsen the performance, which may partly be due to a limitation of our evaluation setup: it does not include enough high-frequency datasets (&gt;= 15 min).Hence, further evaluation is required to conclusively study the impact of longer context lengths.We posit that high-frequency datasets may benefit from a longer context, which may be necessary to correctly capture the long-term seasonal patterns.</p>
<p>Vocabulary size.The vocabulary size governs the precision with which the model can process the scaled time series.To explore its impact on performance, we trained Chronos-T5 (Small, 46M) models with varying vocabulary sizes.Figure 11c shows modest improvements in the point forecasting metric (MASE) as the vocabulary size increases.In contrast, the WQL initially improves but deteriorates for larger vocabulary sizes.We hypothesize that this behavior is an artifact of the chosen metrics.The MASE, which is invariant to the scale of individual series, is closely aligned to our training loss, which is also invariant to scale.Hence, MASE exhibits an improvement with increased precision, just as one expects for the training loss.</p>
<p>Conversely, WQL, a scale-dependent metric, does not correlate closely with the training loss and behaves less predictably as precision increases.See Appendix D for a discussion on the properties of these metrics.</p>
<p>Beyond this experiment, we posit that selecting the vocabulary size in the context of a model like Chronos would pose a trade-off.A vocabulary that is too small would lead to poor forecasting accuracy due to large discretization errors; however, a large vocabulary would lead to the bins being too fine, potentially leading to generalization errors due to fewer datapoints falling into each bin.In this section, we analyze forecasts generated by Chronos models qualitatively, and we also highlight some limitations of our tokenization technique.We primarily focus on synthetically generated time series for a controlled analysis of different types of time series patterns.For example forecasts from real datasets, see Figures 22 to 24 in Appendix E. I.I.D. Noise.We generated time series comprised purely of Gaussian observations, N (0, 1) and N (100, 10), and used Chronos-T5 (Base) to forecast these.Figure 12a shows that Chronos generates plausible forecasts for such time series and the predicted 80% interval coincides with the ground truth 80% interval shown by the dashed blue lines.</p>
<p>Qualitative Analysis and Limitations</p>
<p>Trend and seasonality.We generated time series following linear and exponential trends: Chronos-T5 (Base) predicts the linear trend accurately but struggles with the exponential trend, as shown in Figure 12b.This may be due to a limited representation of exponential trends in the training data.A potential resolution for generating better forecasts for time series with exponential trends is to perform logarithmic scaling before feeding the time series into Chronos models.We also observed that Chronos models tend to underestimate the trend when the context is not sufficiently long.This phenomenon is depicted in Figure 13 where the model forecasts the pattern correctly but underpredicts the trend when a short context is provided.However, with a longer context, the model picks up the correct pattern and trend.</p>
<p>In our analysis, we observed that Chronos models recognize seasonal patterns in time series particularly well.We generated purely seasonal time series using sinusoids with different frequencies.As shown in Figure 12c, Chronos-T5 (Base) precisely forecasts both time series.When fundamental patterns such as trend and seasonality are combined, either additively or multiplicatively, Chronos forecasts them accurately.This is demonstrated in Figure 12d on time series generated via addition and multiplication of a linear function with a sinusoid.</p>
<p>Autoregressive processes.An autoregressive (AR) process of order p is defined as
X t = p i=1 φ i X t−i + ε t ,
where ε t ∼ N (0, 1) and φ 1 , . . ., φ p are the parameters of the model.We generated time series from stationary AR processes of different orders ranging from 1 to 4, and we compared the forecasts generated by Chronos-T5 (Base) against those generated by three models: (a) the ground truth AR model that was used to generate the time series; (b) an AR model with the correct order (p) fitted to the time series; and (c) an AutoARIMA model fitted to the time series.Figure 14 shows the results for the AR(1) and AR(4) processes, and Figure 21 (Appendix E) shows the results for AR(2) and AR(3).We observe that Chronos-T5 (Base) generates plausible forecasts across all four AR processes.The simpler AR(1) and AR(2) processes are easier for the correctly-specified AR model and AutoARIMA model to fit, resulting in a better MSE than Chronos-T5 (Base).However, with increasing complexity in AR(3) and AR(4) processes, Chronos-T5 (Base) not only outperforms the AutoARIMA model (which belongs the same family as the ground truth model) but also performs on par with the fitted AR model with correct order.These results highlight that Chronos models can recognize fundamental patterns present in time series data.</p>
<p>Flexible predictive distributions.Using a categorical distribution to encode predictions gives Chronos flexibility in producing predictive distributions of different shapes.This is shown in Figure 15, illustrating kernel density estimate (KDE) plots of token IDs sampled from a Chronos model, for the first five time steps in the forecast horizon, across three datasets.Despite the fact that cross-entropy is not distanceaware, Chronos outputs predictive distributions over a contiguous set of tokens, and with different shapes, Figure 14: Forecasts generated by Chronos-T5 (Base) for time series generated from AR(1) and AR(4) processes compared against forecasts generated by the ground truth AR model, a fitted AR model of the correct order, and an AutoARIMA model.Chronos-T5 (Base) generates plausible forecasts and prediction intervals in both cases.All AR models fit the simpler AR(1) process correctly and obtain better MSE than Chronos-T5 (Base); however, with the increased complexity in the AR(4) process, Chronos-T5 (Base) performs second best after the ground truth AR model.Each plot shows the predictive distribution for five prediction steps (h = 1, . . ., 5): the densities were obtained via kernel density estimation from sample forecasts.Even though the cross entropy is not distance-aware, the model learns to estimate distributions over neighboring tokens, and of diverse shapes, including multimodal ones.
h = 1 h = 2 h = 3 h = 4
including multi-modal ones.Although Chronos learns the topology of the space directly from the data, we hypothesize that providing explicit topological information to the model during training may expedite the process and make the model robust for tokens where fewer datapoints are available.A potential method to inject topological information into the cross-entropy loss is through a type of label smoothing -assigning non-zero probability mass to tokens (i.e., bins) in the neighborhood of the the correct token.Farebrother et al. (2024) have obtained promising results with such a distance-aware regression-via-classification objective in the context of reinforcement learning.An in-depth theoretical and empirical analysis of the regression-viaclassification paradigm in the context of time series forecasting would constitute interesting future research.</p>
<p>Overflow and loss of precision.One limitation of Chronos comes from the proposed tokenization approach (see Section 3.1).Specifically, the tokens we select represent bin centers in the range [−15, +15], which ultimately represent original time series values in the range [−15s, 15s], where s is the scale of the time series (mean absolute value).If s is very small compared to the range of values in the series, then Figure 16: Loss of precision due to scaling and quantization.In (a), data consists of unit spikes every n = 10, 20, 50 observations (top to bottom): the scale here is 1/n, hence the maximum representable value is 15/n.When 1 &gt; 15/n then the model cannot possibly capture the spikes appropriately (all but the top case), since their value is not represented accurately by tokens.In (b), data is a sine wave shifted up by µ = 1, 10, 50: the scale here is µ, and as the variance of the signal becomes smaller and smaller relative to µ, the tokens precision decreases.some observations will fall out of the representable range.An example of this behaviour is with sparse series, and as shown in Figure 16a.On the other hand, very large values of s compared to the variance result in loss of precision: in the original space, tokens are spaced 30s/(B − 1) from each other, where B is the number of bins (we used B = 4094 in our experiments); values closer than that to each other may be mapped to the same token, with an apparent loss of precision.An example of this behaviour is given in Figure 16b.An inference-time heuristic solution to this problem is to preprocess the time series using an alternative normalization scheme, such as standardization, for time series with large scale and small variance.</p>
<p>Improving the tokenization to overcome these edge cases without heuristics is subject for future work, but the results from Section 5.5 suggest that the Chronos models performs well on real-world data despite the limitations.</p>
<p>Discussion</p>
<p>Chronos represents one of the first endeavours in practical pretrained time series forecasting models, with remarkable zero-shot performance on a comprehensive collection of test datasets.This work opens up various research avenues, some of which we discuss below.</p>
<p>Beyond Zero-shot Univariate Forecasting</p>
<p>In our experiments, we evaluated Chronos in a zero-shot manner for most datasets.Such a setup highlights the competitiveness of zero-shot Chronos models against task-specific baselines.We expect that both in-domain and zero-shot results could be enhanced further through fine-tuning, an avenue we briefly explored in Section 5.5.2.This can be done using any parameter-efficient fine-tuning methods such as those based on low-rank adapters (LoRA) (Hu et al., 2022;Zhang et al., 2023).Alternatively, Chronos can be calibrated for a specific task with conformal methods (Romano et al., 2019;Stankeviciute et al., 2021;Xu &amp; Xie, 2021).Chronos is especially attractive in the context of conformal prediction since it requires no training set, so all available data can be used for calibration.</p>
<p>In this work, we have focused on univariate forecasting of uniformly-spaced time series since it constitutes the most common of real-world time series use-cases.Nevertheless, practical forecasting tasks often involve exogenous information that must be taken into account or may require modeling of irregularly-sampled time series (Rubanova et al., 2019;Ansari et al., 2023).One example of exogenous information is covariates, that can be either time-independent (e.g., color of the product) or time-varying (e.g., on which days the product is on sale).Another closely related problem is multivariate forecasting, where historic values of one time series (e.g., interest rates) can influence the forecast for another time series (e.g., housing prices).The number of covariates or multivariate dimensions can vary greatly across tasks, which makes it challenging to train a single model that can handle all possible combinations.A possible solution may involve training task-specific adaptors that inject the covariates into the pretrained forecasting model (Rahman et al., 2020).As another option, we can build stacking ensembles (Ting &amp; Witten, 1997) of Chronos and other light-weight models that excel at handling covariates such as LightGBM (Ke et al., 2017).</p>
<p>Thus far, our exploration has centered on the problem of time series forecasting.However, several other time series analysis tasks, such as classification, clustering, and anomaly detection (Dau et al., 2018;Wu &amp; Keogh, 2021;Ismail Fawaz et al., 2019;Goswami et al., 2024), could potentially benefit from a pretrained model like Chronos.We hypothesize that the representations learned by the encoders of Chronos-T5 models are universal and can be used for these tasks.An exploration of Chronos-T5 representations for various downstream tasks would constitute interesting future work.A potential limitation of the larger Chronos models is their inference speed compared to task-specific deep learning models.Figure 17 illustrates the inference time of generating forecasts for a single time series, averaged across datasets.The inference speed of the larger Chronos models is comparable to some statistical local models.Moreover, while Chronos models are slower than task-specific models, they are not too large to be prohibitively slow.Furthermore, task-specific models need to be trained for each task individually, which requires additional time and compute.In contrast, Chronos models can be deployed for datasets with diverse history lengths, frequencies, prediction horizons, and context lengths.This makes model deployment significantly easier and drastically simplifies forecasting pipelines, obviating the need for task-specific training.</p>
<p>Inference</p>
<p>%ZK-RJIVIRGI8MQIQW</p>
<p>By leveraging a language modeling framework for time series, we make developments in the NLP community immediately transferable to Chronos models.For instance, inference speed can be improved by using CUDA kernels optimized for modern Ampere GPUs, quantization (Dettmers et al., 2022), and faster decoding techniques, including speculative (Leviathan et al., 2023) and lookahead (Fu et al., 2023) decoding.Developments in long-context language models (Sun et al., 2022;Dao, 2023) may help improve Chronos models' applicability to high-frequency datasets that require longer contexts to capture seasonal patterns.</p>
<p>Other techniques popularly used for text language models, such as temperature tuning, beam search (Freitag &amp; Al-Onaizan, 2017), Top-K sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2019), could enhance the quality of forecasts.These may particularly be helpful in improving the speed and quality of point forecasts, which currently require aggregation over multiple samples.</p>
<p>Data</p>
<p>Our findings underscore that training larger models on a large corpus of time series data yields excellent in-domain and zero-shot performance.Nevertheless, in contrast to NLP, high-quality public time series data remains limited.This poses a dilemma when training models on a large corpus of diverse datasetsselecting more datasets for training leaves fewer for zero-shot evaluation.The time series community would benefit greatly from the availability of larger time series datasets that could be used to develop and improve pretrained model such as Chronos.There have been some recent efforts on building large-scale time series datasets for specific domains (Emami et al., 2023;Liu et al., 2023) and cross-domain (Borchert et al., 2022), albeit further investment is needed.</p>
<p>Another direction to address the problem of limited data involves developing better methods for generating synthetic time series.Our work has made significant strides in this direction by clearly demonstrating the utility of synthetic data generated using Gaussian processes, improving model performance when incorporated into the training data.Even models trained solely on synthetic data exhibit reasonable forecasting performance.Future research could delve into the failure modes of these models, proposing enhancements to bridge the gap between real and synthetic data.</p>
<p>Conclusion</p>
<p>In this work, we approach the problem of developing generalist pretrained forecasting models from the lens of a minimalist.We adapt existing language model architectures and training procedures for time series forecasting, challenging the notion that time-series-specific features or architectures are necessary for forecasting.This results in Chronos, a language modeling framework for time series that is, paradoxically, agnostic to time.The defining characteristic of Chronos is its compatibility with any language model architecture, only requiring minimal modifications -tokenization though scaling and quantization.Our pretrained models significantly outperform existing local models and task-specific deep learning baselines in terms of their in-domain performance.More remarkably, Chronos models obtain excellent results on unseen datasets (zero-shot performance), performing competitively with the best deep-learning baselines trained on these datasets, while showing promising evidence of further improvements through fine-tuning.</p>
<p>Our contributions are significant in two key aspects.First, we show that existing language model architectures are capable of performing forecasting without time-series-specific customizations.This paves the way for accelerated progress by leveraging developments in the area of LLMs and through better data strategies.Second, on a practical level, the strong performance of Chronos models suggests that large (by forecasting standards) pretrained language models can greatly simplify forecasting pipelines without sacrificing accuracy, offering an inference-only alternative to the conventional approach involving training and tuning a model on individual tasks.</p>
<p>A Algorithms</p>
<p>Algorithm 1 and algorithm 2 present the pseudocode for TSMixup and KernelSynth, respectively.</p>
<p>Algorithm 1 TSMixup: Time Series Mixup Input: Time series datasets {X 1 , . . ., X N d }, maximum time series to be mixed K = 3, symmetric Dirichlet concentration parameter α = 1.5, and (minimum, maximum)  x(i)
1:l ← x (i) 1:l 1 l l j=1 |x (i) j |
▷ apply mean scaling to the time series ▷ sample from the GP prior 9: return x 1:lsyn</p>
<p>Kernel</p>
<p>Formula Hyperparameters , 48, 96, 168, 336, 672, 7, 14, 30, 60, 365, 730, 4, 26, 52, 6, 12, 40, 10}  Weather (Godahewa et al., 2021) contains daily time series of four weather variables (rain, mintemp, maxtemp and solar radiation) measured at weather stations in Australia.
Constant κ Const (x, x ′ ) = C C = 1 White Noise κ White (x, x ′ ) = σ n • 1 (x=x ′ ) σ n ∈ {0.1, 1} Linear κ Lin (x, x ′ ) = σ 2 + x • x ′ σ ∈ {0, 1, 10} RBF κ RBF (x, x ′ ) = exp − ∥x−x ′ ∥ 2 2l 2 l ∈ {0.1, 1, 10} Rational Quadratic κ RQ (x, x ′ ) = 1 + ∥x−x ′ ∥ 2 2α −α α ∈ {0.1, 1, 10} Periodic κ Per (x, x ′ ) = exp −2 sin 2 π ∥x−x ′ ∥ p p ∈ {24
Weatherbench (Hourly, Daily, Weekly) contains WeatherBench data at the spatial resolution of 5.625°( 32×64 grid points).WeatherBench is a comprehensive benchmark dataset for weather prediction research and contains hourly values of the many weather-related variables over 40 years from 1979 to 2018 (including temperature, humidity, wind, precipitations).The original data has hourly frequency and was obtained from https://github.com/pangeo-data/WeatherBench;we aggregated it to daily and weekly using mean, except for "total precipitation" which was aggregated by sum.</p>
<p>B.5 Retail</p>
<p>Car Parts (Godahewa et al., 2021)  Tourism (Monthly to Yearly) (Athanasopoulos et al., 2011;Godahewa et al., 2021) Tourism dataset from, used for the Kaggle Tourism Forecasting competition.</p>
<p>Traffic (Godahewa et al., 2021) contains hourly road occupancy readings from sensors in the San Francisco Bay area.</p>
<p>Uber TLC (Hourly, Daily) contains the number of Uber pick-ups from various locations in New York, between January and June 2015.Data was obtained from https://github.com/fivethirtyeight/uber-tlc-foil-response and aggregated hourly and daily.</p>
<p>B.7 Various</p>
<p>M1 (Monthly to Yearly) (Makridakis et al., 1979;Godahewa et al., 2021) contains the time time series used in the M1 forecasting competition.Data spans micro-/macroeconomics, industry, and demographics.</p>
<p>M3 (Monthly to Yearly) (Makridakis &amp; Hibon, 2000;Godahewa et al., 2021) contains the time time series used in the M1 forecasting competition.Data spans micro-/macroeconomics, industry, finance and demographics.</p>
<p>M4 (Hourly to Yearly) (Makridakis et al., 2020;Godahewa et al., 2021) contains data from various domains, at different sampling periods, used for the M4 forecasting competition.Domains include micro-/macroeconomics, demographic, industry, and finance.</p>
<p>M5 (Makridakis et al., 2022) contains products sales data, used for the M5 forecasting competition.The data includes sales up to the end of the validation set (end of public leaderboard), but not values for the test set (private leaderboard).</p>
<p>B.8 Web</p>
<p>Wiki Daily (100k) contains daily page views on the top-100k English Wikipedia articles between 2007 and 2022, ranked by number of observations (non-missing).Data was obtained from https://dumps.wikimedia.org/other/pageviews/.</p>
<p>C Baselines</p>
<p>We considered a total of 17 baseline methods for benchmarking Chronos.Local statistical baselines were AutoETS, AutoARIMA, Naive, Seasonal Naive, and AutoTheta (Assimakopoulos &amp; Nikolopoulos, 2000); for these, we relied on implementations in the StatsForecast library (Garza et al., 2022).For task-specific deep learning architectures, DeepAR (Salinas et al., 2020), PatchTST (Nie et al., 2023), TFT (Lim et al., 2021), DLinear (Zeng et al., 2023), andWaveNet (Oord et al., 2016), we based evaluations on the implementations in GluonTS (Alexandrov et al., 2020).However, N-BEATS (Oreshkin et al., 2020) and N-HiTS (Challu et al., 2023), experiments were based on implementations in the NeuralForecast (Olivares et al., 2022) library.Finally, we used reference implementations of ForecastPFN 8 (Dooley et al., 2023), GPT4TS 9 (One-Fits-All) (Zhou et al., 2023a), LLMTime 10 (Gruver et al., 2023), Lag-Llama 11 (Rasul et al., 2023), and Moirai-1.0-R 12(Woo et al., 2024).Statistical baselines (AutoETS, AutoARIMA, AutoTheta and SeasonalNaive) were used with their default hyperparameters in StatsForecast, but with season lengths implied by their frequencies.For example, daily frequency data had season length set to 7, hourly data 24, and so on.For this heuristic, we used the helper function get_seasonality from GluonTS.</p>
<p>Unless otherwise specified, the default hyperparameter configurations provided in baseline implementations were kept as is, and no dataset specific or global hyperparameter tuning was performed.GluonTS-based implementations were optimized with a batch size of 128, for a time limit of 4 hours and early stopping patience of 200 epochs.In PatchTST and DLinear, we experimented with two loss functions: original losses aimed at point forecasting (L1 or L2 loss) as well as default probabilistic forecasting heads used in their GluonTS implementations, where the loss is set to the negative Student's-t log likelihood of the forecast horizon.Due to the consistently superior performance, our final results include the probabilistic versions of PatchTST and DLinear only.For GPT4TS, we set the context length equal to a multiple of the prediction length, with the multiplier depending on the frequency of the dataset (Table 4).We used the MASE loss function for fine-tuning in GPT4TS due to its superior performance.</p>
<p>For LLMTime, we experimented only with the Llama-2 70B due to the prohibitively high costs of running the benchmark through OpenAI APIs.We used the same hyperparameters as used in the Monash experiment in the original paper (Gruver et al., 2023) with a few notable differences.We set the context length to 512, same as for Chronos models, instead of 500.During our experiments, we observed that the default hyperparameters may lead to a significant drop in the scale of the last prediction on some datasets.To alleviate this issue, we set the STEP_MULTIPLIER to 1.4 (instead of 1.2) and increased the prediction length by 1 (this extra prediction is removed before computing the metrics).The inference time for LLMTime (Llama-2 70B) is ≈0.8 seconds per observation on p3dn.24xlarge.As an example, this will take 92 hours to generate all the predictions on the Traffic dataset (862 time series, 24 as prediction length, 20 samples).Due to the very high compute cost, we skip the evaluation of LLMTime on some large datasets.</p>
<p>A summary of the baseline models used along with details of hyperparameter values is provided in Table 5.</p>
<p>D Evaluation Metrics</p>
<p>In what follows, we consider a dataset of N time series {x i = [x i,1 , . . ., x i,C+H ]} N i=1 , each spanning both the context length C and prediction horizon H.We are interested in evaluating the accuracy of predictions for x i,C+1:C+H , for all i ∈ {1, . . ., N }, which can be either point forecasts or probabilistic ones.A point forecast for x i is denoted as as xi = [x i,C+1 , . . ., xi,C+H ].To evaluate point forecasts, we use the mean absolute scaled error (MASE, Hyndman &amp; Koehler (2006)).For each series, this is simply the mean absolute error (MAE) divided by the empirical error of a seasonal naïve model:
MASE( xi , x i ) = C − S H C+H t=C+1 |x i,t − x i,t | C−S t=1 |x i,t − x i,t+S | ,
where S is a seasonality parameter.Since the denominator scales proportionally to x i , this error metric is independent of the scale of the data.To aggregate MASE over the entire dataset, we average over all i.Probabilistic forecasts are given in terms of predicted quantiles q
(α) i = [q (α) i,C+1 , . . . , q (α)
i,C+H ] at levels α ∈ (0, 1).To evaluate the quality of such predicted quantiles, we use the weighted quantile loss (WQL): this is an aggregation of the quantile loss (Koenker &amp; Hallock, 2001), which is defined for the predicted α-quantile q of a real observation x, as
QL α (q, x) = α(x − q), if x &gt; q, (1 − α)(q − x), otherwise.(4)
To aggregate Eq. ( 4) over multiple series and prediction instants, we consider the weighted average
WQL α = 2 i,t QL α (q (α) i,t , x i,t ) i,t |x i,t | .
We average the above over a finite set of levels {α 1 , . . ., α K } to obtain
WQL = 1 K K j=1 WQL αj .
In all experiments, we use quantiles at level α ∈ {0.1, 0.2, . . ., 0.9} to compute WQL, so that K = 9.Note that, being a weighted average of the quantile loss at different levels, WQL approximates (a weighted average of) the continuous ranked probability score (CRPS), a commonly used metric for evaluating probabilistic predictions (Gneiting &amp; Raftery, 2007;Gasthaus et al., 2019).Unlike for MASE, where errors are scaled by a term proportional to the scale of each series, WQL aggregates absolute errors: as such, its value is affected by the relative scale of all series in the dataset.</p>
<p>E Additional Results</p>
<p>This section complements Section 5.5 by providing additional details to the experimental results.Table 6 reports the training time and cost of Chronos-T5 models on a p4d.24xlargeEC2 instance.Tables 7 and 8 report the raw WQL and MASE scores together with the aggregate relative score and average rank obtained by all models on the datasets in Benchmark I. Similarly, Tables 9 and 10 report these scores on Benchmark II.Figures 18 and 19 show the average ranks obtained by different models on Benchmark I and II, respectively.Figure 20 illustrates the zero-shot performance of Chronos-T5-Synth (Small), a model trained solely on synthetic data generated using KernelSynth, against various baselines.</p>
<p>Figure 1 :
1
Figure 1: High-level depiction of Chronos.(Left) The input time series is scaled and quantized to obtain a sequence of tokens.(Center) The tokens are fed into a language model which may either be an encoder-decoder or a decoderonly model.The model is trained using the cross-entropy loss.(Right) During inference, we autoregressively sample tokens from the model and map them back to numerical values.Multiple trajectories are sampled to obtain a predictive distribution.</p>
<p>The scaled time series x1:C+H = [x 1 ,</p>
<p>T L 9 a e o x b 4 9 y 6 y 6 1 z b s P l N j i 3 6 X K b n N t y u S 3 O b b v c N u d 2 X G 6 H c 7 s u t 8 u 5 N y 7 3 h n N 7 L r f H u Y 7 L d T i 3 7 3 L 7 n D t w u Q P O H b r c I e e 6 L t f l 3 J H L H X H u 2 O W O O X f i c i e c O 3 W 5 U 8 6 d u d w Z 5 8 5 d 7 p x z F y 5 3 w b</p>
<p>s h a 1 _ b a s e 6 4 = " o o m w 4 d J b q 3 f E T 7 M Q J E c R M z y 2 m j w = " &gt; A A A y L 3 i c j V v b b t z I E d V u b h v l t p s A e c k L E d n Y 3 U A W L N t I 8 h J g d Z e s k T T S j K 6 W 1 + B w a j i 0 e D O 7 h y N 5 M P s H e U 2 + I 1 8 T 5 C X I a / 4 i 1 d 1 s</p>
<p>9 4 r c D 5 m 9 W 3 6 G U w 8 p Z W l R P 0 s q 0 3 x e w a a g q W v T i b Q v E s w I B b W b x G l 3 p Z Y b S 0 O j M b + N 0 1 W j P t o K 0 7 D j e S f r z i b a M Y h M S I U X s v 1 I 4 h b z x u W 4 / b T Y + a l t P M 3 n P p R X V X 4 d l G H s 6 o M l 7 Y H h 8 m / p C 6 L L 1 c e n W v 2 3 L d x 1 6 9 5 K 5 e 6 e n 0 j K o f X Q 5 U v h l 8 F Q L O e r Q 4 s A t i e v d s 7 1 5 L 7 x P b S 0 f 0 N K u j b K V e G H N 3 o V e m j s E H l q b p c F w A N F 0 y f 0 s v 7 3 u k V W O + X 9 7 3 7 a c e 4 C a o z i 1 L B h / M n L c + r H z 3 l X X 9 9 e N O o t T L f a T k G E Q k m J 8 c H X 2 l P P 2 f j i Z 5 D o W n R m O c b N W D M S 2 c H V j z C n 9 K u 9 d w 9 u z Z M 7 / M o q E 3 E e q A i 0 Z e n g k R Y S I y r v P Y x w C s / D + 4 s b 4 6 l H O M x 5 a V U o z p X r V 5 W B 8 P T L J y t F E 7 2 v h e R z j n N A R 9 k p u 2 1 X J r u B 4 R C s 7 S 1 t W z Z w + K D U f n x 2 G G S W i c t M w T O T O 6 u t G j E 2 W u 7 s 1 0 z b p a a 3 F l w 8 b e D y d R + 3 r 8 S O k 7 n d a + t 9 O 9 R U X 1 y m r m T H 0 K N c N V V 4 9 t i u n f V G + 3 7 t 9 1 + 9 u Z 1 j f A U a v r B w d c C Q 6 i W I k 1 V h e Y F r C B u q r 8 j e I s K z S t r w y v L 6 s G S A 2 S 2 W o z Z 8 k C A 2 E + u 1 b 1 Q + D H s 8 1 m g 9 K P o y F v 8 M 5 c F 8 n M U P N 7 L k H I 9 g 6 a m d c z g l y o T J m L K M 7 S K s u d o I s s 8 U q</p>
<p>w n A a j 0 N a T G o x M W 8 9 q M F p t n t a W S 1 w u 4 d y D J z E Y 6 b a e x W D 0 2 3 o a g x F x 6 3 k M R s m t J z I Y O b e e y W A 0 3 X o q g x F 2 6 7 k M R t 2 t J z M Y i b e e z W B 0 3 n o 6 g x F 7 6 / k M R v E P n 9 A Y C 0 U U 1 B V K s k b x s U Z h k 6 w T v M 7 g D Y I 3 G L x J 8 C a D t w j e Y v A 2 w d s M 3 i F 4 h 8 G 7 B O 8 y e I / g P Q a / J v g 1 g / c J 3 m d w h + A O g w 8 I P m D w I c G H D D 4 i + I j B X Y K 7 D D 4 m + J j B J w S f M L h H c I / B f Y L 7 D D 4 l + J T B Z w S f M f i c 4 H M G X x B 8 w e B L g i 8 Z f E X w 1 c P H q y s 6 M K p j G l 1 j + t X S Y 9 w 6 5 z Z c b o N z m y 6 3 y b k t l 9 v i 3 L b L b X N u x + V 2 O L f r c r u c 2 3 O 5 P c 6 9 d r n X n N t 3 u X 3 O d V y u w 7 k D l z v g 3 K H L H X L u y O W O O N d 1 u S 7 n j l 3 u m H M n L n f C u Z 7 L 9 T j X d 7 k + 5 0 5 d 7 p R z Z y 5 3 x r l z l z</p>
<p>G 9 X U y n w n 1 l L c H 8 i E H g y w e f p + b w e 0 c g 7 D 5 p D Y V + p t G k z c r f / o p d T U 1 a e r L V D D 1 y 3</p>
<p>e 5 v 4 2 T e F L 4 U 3 y L K b l U X n c c 5 R r k 7 n r P g D a r w I 9 Q D w 7 / W y u n q s o T o n T U O 8 a n e p 1 Y r N 9 O 8 H</p>
<p>Figure 2 :
2
Figure 2: An illustration of TSMixup augmentation for k = {1, 2, 3}.TSMixup improves pattern diversity by taking weighted combinations of randomly-sampled time series from different datasets.</p>
<p>Figure 3 :
3
Figure 3: (a) An illustration of KernelSynth, a Gaussian process (GP)-based synthetic time series generation method.Kernels are sampled from a kernel bank and then randomly combined using a binary operator (× or +).The resultant kernel is used in a GP prior to generate synthetic time series.Random samples from kernels at each step are shown in red and blue colors.(b) Example synthetic time series generated by KernelSynth.</p>
<p>Figure 7 :Figure 8 :Figure 9 :
789
Figure 7: Model size.(a) Training loss curves of Chronos models of different sizes.(b) In-domain and zero-shot performance of Chronos models varying over model size (lower is better).</p>
<p>FigureFigure 10 :
10
Figure10bshows the performance of models trained with different proportions of synthetic data.Both in-domain and zero-shot metrics improve with the incorporation of synthetic data in training.The most</p>
<p>Figure 11 :
11
Figure 11: In-domain and zero-shot performance of a Chronos-T5 (Small) models varying over (a) the number of training steps, (b) the training context length, and (c) the vocabulary size.</p>
<p>Figure 12 :
12
Figure 12: Forecasts generated by Chronos-T5 (Base) on synthetically generated patterns.(a) Noise: Chronos generates reasonable forecasts for Gaussian noise with the 80% prediction interval matching the interval of the underlying distribution (shown by the horizontal dashed blue line).(b) Trend: Chronos forecasts a linear trend (top) correctly but struggles with an exponential trend (bottom).(c) Seasonality: Chronos accurately models seasonal patterns of varying degrees of complexity (single seasonality at the top and three seasonalities at the bottom).(d) Combined Patterns: Chronos forecasts time series generated by the additive (top) or multiplicative (bottom) combination of trend and seasonal patterns accurately.</p>
<p>Figure 13 :
13
Figure13: When the context is not sufficiently long, Chronos-T5 (Base) tends to underestimate trend, as shown in this example with the classic Air Passengers data (monthly) and a forecast horizon of 24.Top: with only 120 observations as context, the median prediction plateaus compared to the previous trend.Bottom: with the full context of 144 observations, the prediction picks up the trend more closely.</p>
<p>Figure 15 :
15
Figure15: Forecast distributions from a Chronos model on series from the NN5 (Daily), Traffic, and Hospital datasets respectively.Each plot shows the predictive distribution for five prediction steps (h = 1, . . ., 5): the densities were obtained via kernel density estimation from sample forecasts.Even though the cross entropy is not distance-aware, the model learns to estimate distributions over neighboring tokens, and of diverse shapes, including multimodal ones.</p>
<p>Figure 23 :
23
Figure 23: Example of forecasts from Chronos-T5 (Base) on the test datasets used in experiments.</p>
<p>Figure 24 :
24
Figure 24: Example of forecasts from Chronos-T5 (Base) on the test datasets used in experiments.</p>
<p>Table 1 :
1
A high-level summary of the datasets and baselines used in our experiments.
Data Subset# Datasets # SeriesUsageBaselinesPretraining-only13795,936pretraining-Benchmark I1597,272pretraining and in-Naive, SeasonalNaive, AutoETS, Auto-domain evaluationTheta, SCUM, AutoARIMA, DeepAR, TFT,PatchTST, DLinear, WaveNet, N-BEATS,N-HiTS, GPT4TS, Lag-Llama, Moirai-1.0-RBenchmark II27190,674zero-shot evaluation All the above, LLMTime and ForecastPFN</p>
<p>length of the augmented time series (l min = 128, l max = 2048).
Output: An augmented time series.1: k ∼ U{1, K}▷ number of time series to mix2: l ∼ U{l min , l max }▷ length of the augmented time series3: for i ← 1, k do4:n ∼ U{1, N d }▷ sample a dataset index5:x(i) 1:l ∼ X n▷ sample a time series of length l from dataset n6:</p>
<p>[λ 1 , . .., λ k ] ∼ Dir([α 1 = α, . .., α k = α])▷ sample mixing weights Kernel bank K (see table 2), maximum kernels per time series J = 5, and length of the time series l syn = 1024.
7: end for8: 9: returnk i=1 λ ix(i) 1:l▷ take weighted combination of time seriesAlgorithm 2 KernelSynth: Synthetic Data Generation using Gaussian ProcessesInput: Output: A synthetic time series x 1:lsyn .1: j ∼ U{1, J}▷ sample the number of kernels2: {κ 1 (t, t ′ ), . . . , κ j (t, t ′ )}i.i.d ∼ K▷ sample j kernels from K3: κ  *  (t, t ′ ) ← κ 1 (t, t ′ )4: for i ← 2, j do5:⋆ ∼ {+, ×}▷ sample a random binary operator6:κ  ▷ compose kernels7: end for8: x 1:lsyn ∼ GP(0, κ
* (t, t ′ ) ← κ * (t, t ′ ) ⋆ κ i (t, t ′ ) * (t, t ′ ))</p>
<p>Table 2 :
2
The kernel bank, K, used in KernelSynth (algorithm 2).</p>
<p>Table 3 :
3
All datasets that are used for experiments.The datasets are partitioned according to how they are used for training and evaluation of Chronos models: pretraining-only data is only used for Chronos training; in-domain evalution data is used for training Chronos models and other task-specific baselines, except for the H observations that are held out for in-domain testing only; zero-shot evaluation data is not used in training Chronos models, but only for evaluation (final H observations), as well as for training task-specific baselines (excluding the final H observations).Brazilian Cities Temperature contains monthly time series representing the weather at 12 different cities in Brazil.Data is originally from NOAA, and we used the post-processed version from https://www.kaggle.com/datasets/volpatto/temperature-timeseries-for-some-brazilian-cities.USHCN contains daily measurements of five climate indicators (precipitation, snow, snow depth, minimum temperature, maximum temperature) from climate stations located in 48 states in the USA.Data was obtained from https://cdiac.ess-dive.lbl.gov/ftp/ushcn_daily/.
DatasetDomainFreq. Num. SeriesSeries LengthPredictionminavgmax Length (H)Pretraining-onlyBrazilian Cities Temperature natureM12492757-Mexico City Bikestransport1H49478078313-Solar (5 Min.)energy5min5166 105120 105120-Solar (Hourly)energy1H516687608760-Spanish Energy and Weather energy1H663506435064-Taxi (Hourly)transport1H2428734739-USHCNnature1D6090590638653-Weatherbench (Daily)nature1D2252801460914609-Weatherbench (Hourly)nature1H225280 350633 350639-Weatherbench (Weekly)nature1W22528020872087-Wiki Daily (100k)web1D10000027412741-Wind Farms (Daily)energy1D33771354-Wind Farms (Hourly)energy1H33717158514-In-domain evaluationElectricity (15 Min.)energy15min37016032 11334124Electricity (Hourly)energy1H321263042630424Electricity (Weekly)energy1W3211561568KDD Cup 2018nature1H27095041089748London Smart Metersenergy30min55602882995148M4 (Daily)various1D4227107237114M4 (Hourly)various1H41474890148M4 (Monthly)various1M480006023418M4 (Weekly)various1W35993103513Pedestrian Countstransport1H665764745948Ridesharetransport1H234054154124Taxi (30 Min.)transport30min24281469147848Temperature-Rainnature1D3207272572530Uber TLC (Daily)transport1D2621811817Uber TLC (Hourly)transport1H2624344434424Zero-shot evaluationAustralian Electricityenergy30min5 230736 23105248CIF 2016banking1M72289812Car Partsretail1M2674515112Covid Deathshealthcare 1D26621221230Dominickretail1D1000142012968ERCOT Loadenergy1H8 154854 15485424ETT (15 Min.)energy15min14696806968024ETT (Hourly)energy1H14174201742024Exchange Ratefinance1B87588758830FRED-MDeconomics 1M10772872812Hospitalhealthcare 1M767848412M1 (Monthly)various1M617489018M1 (Quarterly)various3M20318488M1 (Yearly)various1Y18115246M3 (Monthly)various1M14286611718M3 (Quarterly)various3M75624488M3 (Yearly)various1Y64520286M4 (Quarterly)various3M24000241008M4 (Yearly)various1Y2300019376M5retail1D30490124156228NN5 (Daily)finance1D11179179156NN5 (Weekly)finance1W1111131138Tourism (Monthly)various1M3669129824Tourism (Quarterly)various1Q42730998Tourism (Yearly)various1Y51811244Traffictransport1H862175441754424Weathernature1D301013321429630</p>
<p>6 Mobility and transport Mexico City Bikes contains</p>
<p>contains monthly sales data for various car parts, measured between January 1998 and March 2002.hourly usage statistics for 494 bike stations in Mexico City from 2010 to 2022.Each value in the time series corresponds to the number of bikes returned at the given station at the given hour of the day.Data was obtained from https://ecobici.cdmx.gob.mx/en/open-data.Time series that contain less than 50 non-zero observations were removed.
Dominick (Godahewa et al., 2021) contains weekly time series representing the profit of individual stockkeeping units from a retailer. Original data is from https://www.chicagobooth.edu/research/kilts/datasets/dominicks.Pedestrian Counts (Godahewa et al., 2021) contains data from 66 sensors in Melbourne, counting pedes-trians between 2009 and 2020.Rideshare contains various hourly statistics of Uber and Lyft services in New York, between November 26,2018 and December 18, 2018.Taxi (30 Min., Hourly) contains spatio-temporal traffic time series of New York taxi rides taken at 1214locations every 30 minutes in the months of January 2015 and January 2016. Original data has 30 minutesfrequency, the hourly version was obtain by aggregation with sum.
B.</p>
<p>Table 4 :
4
The multiplier used to set the context length in GPT4TS for each frequency.The context length is set equal to the multiplier times the prediction length, rounded to the nearest whole number.
Frequency Multiplier15min2030min101H101D or 1B101W101M1.53M or 1Q1.51Y1.5</p>
<p>Table 5 :
5
Baseline models and hyperparameter choices.Hyperparameters not specified are set to defaults in their respective implementations.C stands for context length, d h for hidden layer dimension, nL for number of layers, nH for number of heads, and η for learning rate.
ModelModel Type Implementation Probabilistic HyperparametersNaiveLocalStatsForecastYesN/ASeasonalNaive LocalStatsForecastYesN/AAutoETSLocalStatsForecastYesC = 2500AutoARIMALocalStatsForecastYesC = 1000AutoThetaLocalStatsForecastYesC = 2500DeepARTask-specificGluonTSYesd h = 40, n L = 2TFTTask-specificGluonTSYesd h = 32, n H = 4PatchTSTTask-specificGluonTSYesPatch length: 16, Stride: 8, d h = 32, n L = 2, n H = 4DLinearTask-specificGluonTSYesKernel size: 25, d h = 20WaveNetTask-specificGluonTSYesResidual channels: 24, Skip channels: 3N-BEATSTask-specificNeuralForecastNoInput size multiplier: 5N-HiTSTask-specificNeuralForecastNoInput size multiplier: 5GPT4TSTask-specificReferenceNoFine-tuning epochs: 100, cos: 1, tmax: 10, n L = 6, η = 10 −3 ,with pretrained GPT-2 weightsForecastPFNPretrainedReferenceNoC = 100 (as in the released pretrained model)LLMTimePretrainedReferenceYesC = 512, STEP_MULTIPLIER = 1.4 (refer to the text for details)Lag-LlamaPretrainedReferenceYesC = 32Moirai-1.0-RPretrainedReferenceYesC = 1024, Patch length: selected by dataset-specific validation</p>
<p>Table 6 :
6
Training time and the cost of training Chronos models on a single p4d.24xlarge instance.On-demand EC2 pricing of $32.773/hr was used to compute the cost (rounded to the nearest dollar).
ModelTraining Time (hrs) Cost (USD)Chronos-T5 (Mini)7.68252Chronos-T5 (Small)7.73253Chronos-T5 (Base)17.96588Chronos-T5 (Large)63.052066</p>
<p>Table 7 :
7
WQL scores of different models for datasets in Benchmark I, comprising 15 datasets also included in the training data of Chronos models.Models achieving the first, second, and third best scores have been highlighted.Scores for Chronos and task-specific models have been averaged over 3 random seeds.The aggregated relative score was computed as described in Section 5.4.
Pretrained Models (In Domain)Pretrained Models (Other)Task Specific ModelsLocal ModelsC h ro n o s-T 5 (L a rg e )C h ro n o s-T 5 (B a se )C h ro n o s-T 5 (S m a ll )C h ro n o s-T 5 (M in i)C h ro n o s-G P T 2L a g -L la m aM o ir a i-1 .0 -R (B a se )M o ir a i-1 .0 -R (L a rg e )P a tc h T S TD e e p A RW a v e N e tT F TD L in e a rN -H iT SN -B E A T SS C U MA u to E T SA u to T h e taA IM R A u to An a l S e a so N a iv eN a iv eElectricity (15 Min.)0.0770.0780.0800.0820.0770.3190.1040.1050.0820.0900.0910.1890.0790.0810.084--0.229-0.1170.279Electricity (Hourly)0.1010.1140.1050.0890.1170.1040.1210.1170.0890.1060.1090.1250.0950.1280.1270.1320.1290.1980.1260.1470.363Electricity (Weekly)0.0590.0620.0730.0670.0620.1470.1170.1660.0690.1160.1050.1060.1460.0980.0970.1680.1510.1460.1380.1980.198KDD Cup 20180.2720.2680.2890.2710.3770.3690.2880.2780.2520.3300.2800.5710.3120.3020.3157.6312.2660.5210.5280.556-London Smart Meters0.4230.4280.4310.4360.4310.3840.3580.3500.3460.4050.3740.3650.3690.3580.357--0.660-0.5410.731M4 (Daily)0.0220.0220.0220.0220.0210.0430.0240.0230.0230.0230.0230.0230.0240.0220.0220.0240.0270.0240.0230.0280.028M4 (Hourly)0.0220.0240.0240.0250.0330.1110.0250.0220.0270.0380.0460.0330.0380.0400.0450.0440.0660.041-0.0480.166M4 (Monthly)0.1010.1030.1030.1030.1100.1530.1020.1000.0950.1010.1070.0970.1110.0940.093-0.1000.098-0.1460.140M4 (Weekly)0.0370.0370.0400.0410.0400.0780.0500.0470.0390.0460.0450.0510.0440.0390.0400.0490.0520.0530.0500.0630.063Pedestrian Counts0.1870.2040.2370.2360.1730.2620.2720.2590.2570.2290.2480.2610.2470.2540.2410.3540.6191.8180.3400.3190.814Rideshare0.1400.1370.1400.1330.1680.1580.1640.1580.1350.1300.1840.1340.1590.1520.1720.1570.1540.1380.1570.186-Taxi (30 Min.)0.2680.2740.3120.3130.3370.3570.5120.3680.3630.3950.3470.3820.3350.3060.305--0.456-0.4710.741Temperature-Rain0.6630.6690.6850.7040.6870.7170.6550.6850.8040.7180.7080.6700.8480.7800.7980.8861.1821.0600.8691.424-Uber TLC (Daily)0.0960.0970.1000.1050.0970.1760.1140.1070.1000.1100.1260.1110.1060.1160.1080.1620.1670.1900.1510.2310.231Uber TLC (Hourly)0.1530.1530.1550.1610.1620.1760.1770.1650.1670.1760.1680.1790.2340.1660.1610.2730.4620.4330.3110.2990.625Agg. Relative Score0.5640.5800.6030.5980.6230.9370.6910.6700.6010.6760.6890.7340.6970.6560.6641.0601.0761.0830.8761.0001.433Avg. Rank3.4004.6676.2006.0677.53314.53311.1339.1336.3339.53310.73310.40010.4678.2008.53317.36717.20015.33316.56718.00019.667</p>
<p>Table 8 :
8
MASE scores of different models for datasets in Benchmark I, comprising 15 datasets also included in the training data of Chronos models.Models achieving the first, second, and third best scores have been highlighted.Scores for Chronos and task-specific models have been averaged over 3 random seeds.The aggregated relative score was computed as described in Section 5.4.
Pretrained Models (In Domain)Pretrained Models (Other)Task Specific ModelsLocal ModelsC h ro n o s-</p>
<p>Table 9 :
9
WQL scores of different models for datasets in Benchmark II, comprising 27 datasets not seen by Chronos models during training.Models achieving the first, second, and third best scores have been highlighted.Scores for Chronos and task-specific models have been averaged over 3 random seeds.The aggregated relative score was computed as described in Section 5.4.
.3910.3940.4180.4450.3881.1690.7070.6230.4500.5150.6371.1080.4520.5790.5670.508--0.583-0.4981.270Electricity (Hourly)1.4391.5901.4771.3481.6361.5731.7101.6731.3491.5281.5371.7891.3691.8801.8481.4871.7661.7742.1511.7151.8404.159Electricity (Weekly)1.7391.8011.9421.9541.7702.9792.8682.7581.6312.5171.9292.8002.6131.9752.0351.8803.0633.0863.0783.0093.0373.037KDD Cup 20180.6830.6460.6870.6670.8810.8440.6620.6560.6160.7790.6711.0220.6950.6740.7310.7370.9711.0141.1381.0230.994-London Smart Meters0.8280.8380.8460.8570.8420.7920.7700.7540.7330.8320.8240.7880.7990.7770.7810.794--0.966-0.9661.297M4 (Daily)3.1443.1603.1483.1543.0798.0383.4483.3773.4503.3053.3063.2923.4613.1433.1555.1093.2243.2703.3353.2573.2783.278M4 (Hourly)0.6820.6940.7210.7580.7103.8071.2100.9500.9671.2151.6131.8331.8673.2313.4571.5111.3001.6042.458-1.19311.608M4 (Monthly)0.9600.9700.9820.9911.0442.0901.0321.0050.9621.0401.1011.0091.0220.9940.9420.979-0.9700.966-1.2601.205M4 (Weekly)1.9982.0212.1132.1552.2255.6582.4842.4481.9962.3462.5232.7452.4292.0941.9763.0402.3942.5482.6572.3732.7772.777Pedestrian Counts0.2720.2860.3040.3030.2710.3420.3540.3300.3390.3110.3340.3640.3270.3240.3150.3930.3820.4871.2750.3830.3690.842Rideshare0.8650.8620.8540.8300.9210.8910.9100.9000.8270.9960.9831.0671.4480.9330.9191.0880.9440.9100.9701.0281.250-Taxi (30 Min.)0.8300.8490.9410.9441.0371.0691.3741.0881.0771.1581.0701.1131.0180.9500.9341.113--1.193-1.1601.768Temperature-Rain0.9800.9861.0121.0290.9741.0310.9630.9881.2501.0151.0760.9941.3701.2321.3431.2261.6251.9681.9451.5242.243-Uber TLC (Daily)0.8210.8390.8700.9060.8351.2890.9400.8710.8130.9050.9380.9160.8550.8770.8790.8381.1741.2281.3121.1141.3781.378Uber TLC (Hourly)0.6700.6730.6770.6890.7060.7110.7300.7160.6960.7030.7760.7460.7780.7160.7510.7540.8771.0091.0360.9820.9311.390Agg. Relative Score0.6950.7060.7270.7320.7411.1410.8570.8060.7400.8210.8420.9390.8640.8540.8610.8710.9400.9831.1290.9411.0001.484Avg. Rank3.3334.7336.0676.4676.93314.20011.5339.4675.73310.86712.13313.93311.8009.6679.40012.13316.50016.66717.33317.56716.66719.867Pretrained Models (Zero Shot)Pretrained Models (Other)Task Specific ModelsLocal Models</p>
<p>Table 10 :
10
MASE scores of different models for datasets in Benchmark II, comprising 27 datasets not seen by Chronos models during training.Models achieving the first, second, and third best scores have been highlighted.Scores for Chronos and task-specific models have been averaged over 3 random seeds.The aggregated relative score was computed as described in Section 5.4.
Pretrained Models (Zero Shot)Pretrained Models (Other)Task Specific ModelsLocal Modelsn o s-h roC</p>
<p>Figure 18: Average rank of different models on Benchmark I, comprising 15 datasets also included in the training data of Chronos models.
.3331.3191.3991.1141.3101.1862.1581.6351.2581.0090.8711.4730.9970.8101.2780.7940.8281.1611.4272.3910.8971.3931.2532.362Car Parts0.9060.8990.8870.8910.881-2.6570.8161.7351.5420.8030.7980.8170.7990.8790.8030.8030.8911.1571.1851.229-1.201-CIF 20160.9860.9810.9891.0511.0461.3843.5882.2351.1971.1601.5371.3631.3091.5531.1451.3891.4400.9600.9070.9571.0021.0061.2891.263Covid Deaths42.55042.68742.67043.62148.21532.14391.51578.45633.06233.10836.46538.203102.45730.63540.41831.77131.73075.90933.59538.11445.40731.70546.91246.912Dominick0.8180.8160.8190.8330.820-3.2741.2500.8790.8450.8670.8510.8120.8000.8800.7820.7821.8130.8910.8851.016-0.8710.871ERCOT Load0.6170.5500.5730.5880.5611.3193.9750.8340.5830.6670.5531.1970.7800.6900.6510.6150.6480.5581.3082.8261.3061.2840.7614.234ETT (15 Min.)0.7410.7390.7100.7920.7961.0421.1380.9670.9810.7530.6520.8741.3390.9620.7240.6430.6590.5740.6731.1830.5830.8791.1691.164ETT (Hourly)0.7350.7890.7890.7970.7681.2321.8331.0020.9020.8450.7290.8141.5090.8750.6950.8110.7820.7680.8501.1390.9000.9770.9321.651Exchange Rate2.3752.4332.2522.0302.3351.7437.5833.0871.5071.9091.5401.6153.1052.3611.4592.0412.1492.7091.7491.6431.6481.8821.7401.874FRED-MD0.5000.4860.4960.4830.4680.5132.6212.2830.6070.5930.7450.6210.8490.9290.7130.6960.6350.6930.4920.5440.5660.4731.1010.622Hospital0.8100.8100.8150.8170.8310.8611.7750.9390.8210.8260.8590.8040.8570.7990.9400.7810.7600.7930.7480.7600.7610.8200.9210.968M1 (Monthly)1.0901.1171.1691.1741.1821.4152.1721.8751.2721.2381.2081.1221.2661.3261.3691.3331.2361.1981.0231.0721.0991.1531.3141.468M1 (Quarterly)1.7131.7391.7641.7851.7851.8029.9313.0361.8961.8401.9201.7411.9042.1441.9432.0612.0431.9581.6021.7101.6831.7702.0781.952M1 (Yearly)4.3014.6244.6594.9584.7514.07723.0897.1494.6234.7084.0423.6854.7274.31611.5655.5686.2123.6753.5714.1103.6973.8704.8944.894M3 (Monthly)0.8570.8680.8850.9000.9300.9962.2401.8460.9460.9241.2250.9430.9500.9161.1610.8990.8830.9500.8270.8690.8610.9331.1461.175M3 (Quarterly)1.1811.1991.2561.2891.2411.45010.1762.8861.4281.4291.2641.2091.2571.1601.5721.2021.1471.4481.1351.1251.1301.4191.4251.464M3 (Yearly)3.1063.2093.2763.3853.1583.14018.7285.1143.6613.8222.9492.8273.0262.8603.4353.4323.5473.4182.7032.6962.6133.1653.1723.172M4 (Quarterly)1.2161.2311.2461.2711.312-6.9272.6631.2861.2591.1501.2541.2411.2481.2291.1571.1291.2151.1451.1881.1931.2761.6021.477M4 (Yearly)3.6063.6783.6513.7433.933--5.8663.5994.1753.0723.1783.2213.1193.295--3.3743.0133.3743.1243.7303.9743.974M50.9440.9390.9400.9440.969-1.5300.9651.4420.9290.9190.9560.9590.9091.0270.9170.9170.9351.0961.1011.1001.0571.3991.399NN5 (Daily)0.5730.5850.6150.6420.6010.9531.3750.9920.6980.6250.5750.5850.5850.5560.6040.5710.5710.7201.0521.0391.0731.2141.2921.292NN5 (Weekly)0.9400.9380.9440.9470.9630.9681.3491.1410.9801.0090.8770.9201.0340.8960.9660.9191.0141.2680.9740.9780.9840.9951.0631.063Tourism (Monthly)1.7611.8281.9001.9501.7832.1394.3483.0302.0391.9101.5721.5291.6291.6861.5511.5141.4861.5731.4411.4971.6801.5731.6313.591Tourism (Quarterly)1.6771.7171.7301.8291.8281.9165.5953.6952.7222.2811.7231.5861.7691.7291.6901.5851.6181.7501.5011.5901.6581.6611.6993.633Tourism (Yearly)3.7553.9003.9014.0483.8623.30912.0933.7553.0473.3013.1383.7024.1303.0473.4063.4483.564-3.2763.1383.0784.0433.5523.552Traffic0.8040.8280.8370.8500.8180.9731.9090.8290.7260.7590.7900.7370.7970.8800.8210.9270.9680.787-1.6851.794-1.0772.052Weather0.8220.8240.8360.8530.858-2.0031.0010.8310.8070.8600.9110.9450.9130.9970.9100.8880.9720.9331.0790.9910.9071.0041.004Agg. Relative Score0.8230.8320.8410.8500.8520.9622.4501.2910.9070.8760.8100.8430.9510.8470.8940.8300.8350.8950.8380.9530.8750.9081.0001.188Avg. Rank8.4819.29610.59312.03711.63016.59323.20419.66713.03712.4448.2229.11114.0749.77812.7049.4639.64812.1118.20410.7049.59313.44416.77819.1850SGEP1SHIPW8EWO7TIGMJMG1SHIPW4VIXVEMRIH1SHIPW-R(SQEMR4VIXVEMRIH1SHIPW3XLIV1SHIP'LVSRSW80EVKI 'LVSRSW8&amp;EWI 'LVSRSW81MRM 'LVSRSW87QEPP 4EXGL878 'LVSRSW+48 2,M87 2&amp;)%87 1SMVEM60EVKI (IIT%6 8<em>8 (0MRIEV 7IEWSREP2EMZI 7'91 %YXS)87 %YXS%6-1% %YXS8LIXE 0EK0PEQE 1SMVEM6&amp;EWI ;EZI2IX1SHIP'LVSRSW80EVKI 'LVSRSW8&amp;EWI 4EXGL878 'LVSRSW87QEPP 'LVSRSW81MRM 'LVSRSW+48 2&amp;)%87 1SMVEM60EVKI 2,M87 %YXS%6-1% %YXS8LIXE %YXS)87 7IEWSREP2EMZI 7'91 0EK0PEQE 8</em>8 ;EZI2IX (IIT%6 1SMVEM6&amp;EWI (0MRIEV +48872EMZI2EMZI%ZK6ERO;50%ZK6ERO1%7)0SGEP1SHIPW8EWO7TIGMJMG1SHIPW4VIXVEMRIH1SHIPW&gt;IVS7LSX4VIXVEMRIH1SHIPW3XLIV1SHIP4EXGL878 8<em>8 'LVSRSW80EVKI (IIT%6 2&amp;)%87 2,M87 'LVSRSW8&amp;EWI 'LVSRSW87QEPP 7'91 %YXS8LIXE %YXS)87 'LVSRSW+48 1SMVEM60EVKI ;EZI2IX (0MRIEV 2EMZI 7IEWSREP2EMZI 0EK0PEQE 0018MQI %YXS%6-1% 1SMVEM6&amp;EWI 'LVSRSW81MRM1SHIP7'91 4EXGL878 'LVSRSW80EVKI (IIT%6 'LVSRSW8&amp;EWI 2,M87 %YXS8LIXE 2&amp;)%87 0EK0PEQE 2EMZI 7IEWSREP2EMZI 0018MQI ;EZI2IX %YXS%6-1% 1SMVEM6&amp;EWI (0MRIEV 1SMVEM60EVKI 8</em>8 'LVSRSW87QEPP %YXS)87 'LVSRSW+48 +4887 'LVSRSW81MRM%ZK6ERO;50%ZK6ERO1%7)
Figure 19: Average rank of different models on Benchmark II, comprising 27 datasets not seen by Chronos models during training.</p>
<p>The datasets used in our experiments are available at https://huggingface.co/datasets/autogluon/chronos_datasets.
Our code and model checkpoints are available at https://github.com/amazon-science/chronos-forecasting.
Some models (GPT4TS and ForecastPFN) only generate point forecasts and we only evaluate those.
 Many existing works (Ansari et al., 2021;Rasul et al., 2023;Kollovieh et al., 2023) use CRPS and WQL synonymously.
From a rigorous standpoint, to prevent information leakage, the start time of any dataset within this category must be after the timestamp of the last observation from the pretraining dataset and Benchmark I. Nevertheless, we consider the risk to be minimal given that the datsets bear no overlap beyond high-level conceptual categorization.
These numbers differ from the original sizes of the T5 models inTay et al. (2021) due to the change in the vocabulary size.
All benchmarks are zero-shot for this model, since it was only trained on synthetic data.
AcknowledgementsWe are indebted to Stefano Soatto for challenging us to think about the fundamental question regarding language models and time series modeling, ultimately leading to the creation of the present work.We are grateful to our fellow researchers who have contributed to this work with insightful discussions and valuable feedback, including but not limited to George Karypis, Huzefa Rangwala, Devamanyu Hazarika, Imry Kissos, Laurent Callot, Baris Kurt, Valentin Flunkert, David Salinas, Boran Han, Xiaoyong Jin, Luke Huan, Youngsuk Park, Gaurav Gupta, Karthick Gopalswamy, Tim Januschowski, Jan Gasthaus, Bing Xiang, Kashif Rasul, Juba Nait Saada, Matthias Karlbauer, Hugo Senetaire, Mononito Goswami and Gerald Woo.B DatasetsThe complete list of datasets used for our empirical evaluation is provided in Table3.The table is divided into three sections, representing how the datasets were used for Chronos models: in total, 55 datasets where used for experiments, 13 of which for pretraining only, 15 for in-domain evaluation, and 27 for zeroshot evaluation (see alsoSection 5).In the following, we provide a brief description of each dataset, organized by its domain.B.1 EnergyAustralian Electricity(Godahewa et al., 2021)contains electricity demand data from 5 states in Australia.Electricity (15 Min., Hourly, Weekly) contains electricity consumption (in kW) for 370 households.Original data has 15 minutes frequency and was obtained from https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014; hourly and weekly aggreations are fromGodahewa et al. (2021).ERCOT Load contains hourly energy load in 8 US regions between 2004 and 2021.ETT (15 Min., Hourly)(Zhou et al., 2021)contains oil temperatures and other covariates of electrical transformers from two stations in China, measured at 15 minutes granularity.London Smart Meters contains half-hourly energy consumption of 5561 households in the UK between 2011 and 2014.Data was obtained from https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households.Solar (5 Min., Hourly) contains data about solar power generation in the US in 2006.The original data has 5 minute frequency and was obtained from https://www.nrel.gov/grid/solar-power-data.html; the hourly version was obtained via mean aggregation.Spanish Energy and Weather contains 4 years of electricity consumption, generation, pricing, and weather data for Spain.Electricity data is for all of Spain, weather data is provided for each of 5 major Spanish cities.The data was obtained from https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather.Wind Farms (Hourly, Daily)(Godahewa et al., 2021)contains energy production data from wind farms in Australia.Original data was collected at 1 minute frequencey, which we aggregated to hourly and daily using the mean.B.2 Finance and economicsCIF 2016(Godahewa et al., 2021)contains banking data that was used in the CIF 2016 forecasting competition.Of all time series included, 24 are real data while the other 48 are artificially generated.Exchange Rate contains daily exchange rates for currencies of eight countries (Australia, British, Canada, Switzerland, China, Japan, New Zealand and Singapore) between 1990 and 2016.(Godahewa et al., 2021)contains monthly macro-economic indicators from the Federal Reserve Bank.Data was extracted from the FRED-MD database, and the were differenced and log-transformed.NN5 (Daily, Weekly)(Godahewa et al., 2021)contains cash withdrawal data from ATMs.FRED-MDB.3 HealthcareCovid Deaths(Godahewa et al., 2021)contains daily count data of COVID-19 deaths in a set of countries and states, between January and August, 2020.Hospital(Godahewa et al., 2021)F ArXiv ChangelogF.1 V3• We found an off-by-one error in the decoded bin indices for Chronos models which had led to artificially worse results for Chronos models in the previous version.Upon fixing this issue, the results for Chronos models improved significantly.Note that this issue only affected inference and the updated results still refer to the models we had pretrained previously.Further details on this issue can be found in the relevant Github pull request.This issue also led to changes in the conclusion of the vocabulary size experiment in Section 5.6.• The SCUM ensemble(Petropoulos &amp; Svetunkov, 2020)was added as one of the baselines, based on the suggestion of an anonymous TMLR reviewer.• Clarified and polished the text in multiple places.We are thankful to the anonymous TMLR reviewers for their suggestions.Key changes include:-Added brief reasoning on our use of mean scaling in Section 3.1.-Clarified the notation and discussion on quantization in Section 3.1.-Added a brief discussion on ordinal regression in Section 3.2.-Added a brief discussion on how topological information could potentially be incorporated into the objective function in Section 5.7.-Added details on the kernel bank, K, used in KernelSynth in Table2.F.2 V2• Added results for LLMTime(Gruver et al., 2023), Lag-Llama(Rasul et al., 2023)and Moirai-1.0-R(Woo et al., 2024)to the main text and hyperparameter details in Appendix C.• Renamed our data augmentation scheme presented in Section 4.1 from TSMix to TSMixup to avoid a naming conflict with the TSMix method proposed inDarlow et al. (2023).Thanks Konrad Özdemir for bringing this to our attention.• Corrected the number of time series for Benchmark II in Table1from 103,047 to 190,674.• Added reference toBorchert et al. (2022)in Section 6.3.• Updated color of vertical dashed line in Figures 22 to 24.Predictions also changed slightly in the new figures, due to random sampling.• Updated acknowledgements.
GluonTS: Probabilistic and Neural Time Series Modeling in Python. Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, The Journal of Machine Learning Research. 211332020</p>
<p>Deep Explicit Duration Switching Models for Time Series. Abdul Fatir Ansari, Konstantinos Benidis, Richard Kurle, Ali Caner Turkmen, Harold Soh, Alexander J Smola, Bernie Wang, Tim Januschowski, Advances in Neural Information Processing Systems. 202134</p>
<p>Neural continuous-discrete state space models for irregularly-sampled time series. Abdul Fatir Ansari, Alvin Heng, Andre Lim, Harold Soh, International Conference on Machine Learning. PMLR202320</p>
<p>The theta model: a decomposition approach to forecasting. V Assimakopoulos, K Nikolopoulos, International Journal of Forecasting. 164332000</p>
<p>The tourism forecasting competition. George Athanasopoulos, Rob J Hyndman, Haiyan Song, Doris C Wu, International Journal of Forecasting. 273322011</p>
<p>Deep learning for time series forecasting: Tutorial and literature survey. Konstantinos Benidis, Syama Sundar Rangapuram, Valentin Flunkert, Yuyang Wang, Danielle Maddix, Caner Turkmen, Jan Gasthaus, Michael Bohlke-Schneider, David Salinas, Lorenzo Stella, François-Xavier Aubet, Laurent Callot, Tim Januschowski, ACM Comput. Surv. 556</p>
<p>Multiobjective model selection for time series forecasting. Oliver Borchert, David Salinas, Valentin Flunkert, Tim Januschowski, Stephan Günnemann, arXiv:2202.0848520222143arXiv preprint</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. 202034</p>
<p>. Chris U Carmona, François-Xavier Aubet, Valentin Flunkert, Jan Gasthaus, arXiv:2107.07702Neural Contextual Anomaly Detection for Time Series. 72021</p>
<p>N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting. Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, Artur Dubrawski, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence20233733</p>
<p>A neural network approach to ordinal regression. Jianlin Cheng, Zheng Wang, Gianluca Pollastri, 2008 IEEE international joint conference on neural networks (IEEE world congress on computational intelligence). IEEE2008</p>
<p>PaLM: Scaling Language Modeling with Pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, arXiv:2210.11416Siddhartha Brahma, et al. Scaling Instruction-Finetuned Language Models. 2022</p>
<p>Tri Dao, arXiv:2307.08691FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. 202320</p>
<p>TSMix: time series data augmentation by mixing sources. Luke Nicholas Darlow, Artjom Joosen, Martin Asenov, Qiwen Deng, Jianfeng Wang, Adam Barker, Proceedings of the 3rd Workshop on Machine Learning and Systems. the 3rd Workshop on Machine Learning and Systems202343</p>
<p>A decoder-only foundation model for time. Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou, arXiv:2310.10688202324</p>
<p>Anh Hoang, Eamonn Dau, Kaveh Keogh, Chin-Chia Michael Kamgar, Yan Yeh, Shaghayegh Zhu, Gharghabi, Ann Chotirat, Ratanamahatana, Bing Yanping, Nurjahan Hu, Anthony Begum, Abdullah Bagnall, Mueen, Gustavo Batista, and Hexagon-ML. The UCR Time Series Classification Archive. October 2018</p>
<p>Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.07339-bit Matrix Multiplication for Transformers at Scale. 202220int8(</p>
<p>Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, Mingsheng Long, arXiv:2302.00861SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling. 2023</p>
<p>Fore-castPFN: Synthetically-Trained Zero-Shot Forecasting. Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Advances in Neural Information Processing Systems. 2023. 2, 4, 101533Siddartha Naidu, and Colin White</p>
<p>Structure Discovery in Nonparametric Regression through Compositional Kernel Search. David Duvenaud, James Lloyd, Roger Grosse, Joshua Tenenbaum, Ghahramani Zoubin, International Conference on Machine Learning. PMLR2013</p>
<p>Patrick Emami, Abhijeet Sahu, Peter Graf, arXiv:2307.00142BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. 2023</p>
<p>Angela Fan, Mike Lewis, Yann Dauphin, arXiv:1805.04833Hierarchical Neural Story Generation. 201820</p>
<p>Stop regressing: Training value functions via classification for scalable deep rl. Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taïga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, arXiv:2403.03950202418arXiv preprint</p>
<p>How not to lie with statistics: the correct way to summarize benchmark results. J Philip, John J Fleming, Wallace, Communications of the ACM. 2931986</p>
<p>Markus Freitag, Yaser Al-Onaizan, arXiv:1702.01806Beam Search Strategies for Neural Machine Translation. 201720</p>
<p>Breaking the Sequential Dependency of LLM Inference Using Lookahead Decoding. Yichao Fu, Peter Bailis, Ion Stoica, Hao Zhang, November 2023</p>
<p>The Pile: An 800GB Dataset of Diverse Text for Language Modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.000272020</p>
<p>StatsForecast: Lightning fast forecasting with statistical and econometric models. PyCon Salt Lake City. Federico Garza, Max Mergenthaler Canseco, Cristian Challú, Kin G Olivares, 2022. 2022Utah, US</p>
<p>Probabilistic Forecasting with Spline Quantile Function RNNs. Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram, David Salinas, Valentin Flunkert, Tim Januschowski, Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics. the Twenty-Second International Conference on Artificial Intelligence and StatisticsPMLR20198935of Proceedings of Machine Learning Research</p>
<p>Strictly proper scoring rules, prediction, and estimation. Tilmann Gneiting, Adrian E Raftery, Journal of the American statistical Association. 102477352007</p>
<p>Monash Time Series Forecasting Archive. Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I Webb, Rob J Hyndman, Pablo Montero-Manso, Neural Information Processing Systems Track on Datasets and Benchmarks. 20213033</p>
<p>Moment: A family of open time-series foundation models. Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, Artur Dubrawski, arXiv:2402.038852024420arXiv preprint</p>
<p>Large Language Models Are Zero-Shot Time Series Forecasters. Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon, Wilson , Advances in Neural Information Processing Systems. 2023. 1, 4, 10, 333443</p>
<p>The curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, arXiv:1904.09751201920</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 202219</p>
<p>Transformer-based deep survival analysis. Shi Hu, Egill Fridgeirsson, Guido Van Wingen, Max Welling, Survival Prediction-Algorithms, Challenges and Applications. PMLR2021</p>
<p>Forecasting with exponential smoothing: the state space approach. Rob Hyndman, Anne B Koehler, Keith Ord, Ralph D Snyder, 2008Springer Science &amp; Business Media39</p>
<p>Forecasting: principles and practice. OTexts. J Rob, George Hyndman, Athanasopoulos, 201819</p>
<p>Another look at measures of forecast accuracy. J Rob, Anne B Hyndman, Koehler, International journal of forecasting. 224342006</p>
<p>Deep learning for time series classification: a review. Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, Pierre-Alain Muller, Data mining and knowledge discovery. 334202019</p>
<p>Time-LLM: Time series forecasting by reprogramming large language models. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, Qingsong Wen, The Twelfth International Conference on Learning Representations. 202424</p>
<p>Domain adaptation for time series forecasting via attention sharing. Xiaoyong Jin, Youngsuk Park, Danielle Maddix, Hao Wang, Yuyang Wang, International Conference on Machine Learning. PMLR202214</p>
<p>LightGBM: A Highly Efficient Gradient Boosting Decision Tree. Advances in neural information processing systems. Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu, 20173020</p>
<p>Quantile regression. Roger Koenker, Kevin F Hallock, Journal of economic perspectives. 154352001</p>
<p>A classification of business forecasting problems. Stephan Kolassa, Tim Januschowski, Foresight. 5212019</p>
<p>Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. Marcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang, Yuyang Wang, Advances in Neural Information Processing Systems. Curran Associates, Inc20233610</p>
<p>Fast inference from transformers via speculative decoding. Yaniv Leviathan, Matan Kalman, Yossi Matias, International Conference on Machine Learning. PMLR202320</p>
<p>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.134612019</p>
<p>Temporal fusion transformers for interpretable multi-horizon time series forecasting. Bryan Lim, Sercan Ö Arık, Nicolas Loeff, Tomas Pfister, International Journal of Forecasting. 374332021. 3, 6</p>
<p>Xu Liu, Yutong Xia, Yuxuan Liang, Junfeng Hu, Yiwei Wang, Lei Bai, Chao Huang, Zhenguang Liu, Bryan Hooi, Roger Zimmermann, arXiv:2306.08259Largest: A benchmark dataset for large-scale traffic forecasting. 2023</p>
<p>The M3-Competition: results, conclusions and implications. International journal of forecasting. Spyros Makridakis, Michele Hibon, 20001633</p>
<p>Accuracy of forecasting: An empirical investigation. Spyros Makridakis, Michele Hibon, Claus Moser, Journal of the Royal Statistical Society. Series A (General). 1422331979</p>
<p>The M4 Competition: 100,000 time series and 61 forecasting methods. Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, International Journal of Forecasting. 361332020</p>
<p>M5 accuracy competition: Results, findings, and conclusions. Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. 20223833</p>
<p>Regression models for ordinal data. Peter Mccullagh, Journal of the Royal Statistical Society: Series B (Methodological). 4221980</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, arXiv:1609.07843Pointer sentinel mixture models. 2016</p>
<p>Large language models as general pattern machines. Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, Andy Zeng, Proceedings of The 7th Conference on Robot Learning. The 7th Conference on Robot LearningPMLR2023229</p>
<p>A time series is worth 64 words: Long-term forecasting with transformers. Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, Jayant Kalagnanam, International Conference on Learning Representations, 2023. 3, 4. 1033</p>
<p>Neu-ralForecast: User friendly state-of-the-art neural forecasting models. PyCon Salt Lake City. Kin G Olivares, Cristian Challú, Federico Garza, Max Mergenthaler Canseco, Artur Dubrawski, 2022. 2022Utah, US</p>
<p>Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, arXiv:1609.03499Wavenet: A generative model for raw audio. 20161033</p>
<p>N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio, International Conference on Learning Representations. 20201033</p>
<p>Meta-learning framework with applications to zero-shot time-series forecasting. Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)2021</p>
<p>Zero-shot and few-shot time series forecasting with ordinal regression recurrent neural networks. Pérez Bernardo, Stephen J Orozco, Roberts, 28th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. 2020</p>
<p>Learning quantile functions without quantile crossing for distribution-free time series forecasting. Youngsuk Park, Danielle Maddix, François-Xavier Aubet, Kelvin Kan, Jan Gasthaus, Yuyang Wang, PMLR, 2022. 3International Conference on Artificial Intelligence and Statistics. </p>
<p>A simple combination of univariate models. Fotios Petropoulos, Ivan Svetunkov, International journal of forecasting. 361432020</p>
<p>The effectiveness of discretization in forecasting: An empirical study on neural time series models. Stephan Rabanser, Tim Januschowski, Valentin Flunkert, David Salinas, Jan Gasthaus, arXiv:2005.101112020</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1862019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 211132020. 3, 6, 7, 9</p>
<p>Integrating multimodal information in large pretrained transformers. Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, Amir Zadeh, Chengfeng Mao, Louis-Philippe Morency, Ehsan Hoque, Proceedings of the conference. the conferenceNIH Public Access2020202020</p>
<p>Deep state space models for time series forecasting. Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, Tim Januschowski, 201831Advances in neural information processing systems</p>
<p>Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. Kashif Rasul, Calvin Seward, Ingmar Schuster, Roland Vollgraf, PMLR, 2021. 3International Conference on Machine Learning. </p>
<p>Lag-llama: Towards foundation models for time series forecasting. Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Biloš, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, Sahil Garg, Alexandre Drouin, Nicolas Chapados, Yuriy Nevmyvaka, and Irina Rish. 2023. 2, 4, 103343</p>
<p>Advances in neural information processing systems. Yaniv Romano, Evan Patterson, Emmanuel Candes, 2019. 1932Conformalized quantile regression</p>
<p>Latent ordinary differential equations for irregularly-sampled time series. Yulia Rubanova, Ricky Tq Chen, David K Duvenaud, Advances in neural information processing systems. 20193220</p>
<p>Deepar: Probabilistic forecasting with autoregressive recurrent networks. David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, International Journal of Forecasting. 363332020. 3, 5, 6, 10</p>
<p>Rico Sennrich, Barry Haddow, Alexandra Birch, arXiv:1508.07909Neural machine translation of rare words with subword units. 2015</p>
<p>Autogluon-timeseries: Automl for probabilistic time series forecasting. Oleksandr Shchur, Ali Caner Turkmen, Nick Erickson, Huibin Shen, Alexander Shirkov, Tony Hu, Bernie Wang, PMLR, 2023. 10International Conference on Automated Machine Learning. </p>
<p>Conformal time-series forecasting. Kamile Stankeviciute, Ahmed M Alaa, Mihaela Van Der Schaar, Advances in neural information processing systems. 20213419</p>
<p>Regression as classification: Influence of task formulation on neural network features. Lawrence Stewart, Francis Bach, Quentin Berthet, Jean-Philippe Vert, International Conference on Artificial Intelligence and Statistics. PMLR2023</p>
<p>Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, Furu Wei, arXiv:2212.10554A length-extrapolatable transformer. 202220</p>
<p>Rethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna, 2015</p>
<p>Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, Donald Metzler, arXiv:2109.10686Scale efficiently: Insights from pre-training and fine-tuning transformers. 2021913</p>
<p>Stacking bagged and dagged models. Ming Kai, Ian H Ting, Witten, Proceedings of the Fourteenth International Conference on Machine Learning. the Fourteenth International Conference on Machine Learning199720</p>
<p>Regression using Classification Algorithms. Luis Torgo, Joao Gama, Intelligent Data Analysis. 141997</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023. 2, 3, 4</p>
<p>Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 2017</p>
<p>Deep factors for forecasting. Yuyang Wang, Alex Smola, Danielle Maddix, Jan Gasthaus, Dean Foster, Tim Januschowski, International conference on machine learning. PMLR2019</p>
<p>Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, Dhruv Madeka, arXiv:1711.11053A Multi-Horizon Quantile Recurrent Forecaster. 201736</p>
<p>Regression models with ordinal variables. Christopher Winship, Robert D Mare, American sociological review. 61984</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAssociation for Computational Linguistics202069</p>
<p>Unified training of universal time series forecasting transformers. Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo, arXiv:2402.025922024. 2, 4, 103343</p>
<p>TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis. Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, Mingsheng Long, International Conference on Learning Representations. 2023</p>
<p>Current Time Series Anomaly Detection Benchmarks are Flawed and are Creating the Illusion of Progress. Renjie Wu, Eamonn Keogh, IEEE Transactions on Knowledge and Data Engineering. 202021</p>
<p>Conformal Prediction Interval for Dynamic Time-Series. Chen Xu, Yao Xie, International Conference on Machine Learning. PMLR202119</p>
<p>Hao Xue, Flora D Salim, arXiv:2210.08964PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. 202323</p>
<p>A novel transfer learning framework for time series forecasting. Rui Ye, Qun Dai, 2018156Knowledge-Based Systems</p>
<p>Are Transformers Effective for Time Series Forecasting?. Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence20233733</p>
<p>Hongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, arXiv:1710.09412mixup: Beyond Empirical Risk Minimization. 2017</p>
<p>Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao, International Conference on Learning Representations. 202319</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023</p>
<p>Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang, The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference. AAAI Press20213530</p>
<p>One Fits All: Power general time series analysis by pretrained LM. Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, Rong Jin, Advances in Neural Information Processing Systems. 2023a. 2, 41033</p>
<p>Improving time series forecasting with mixup data augmentation. Yun Zhou, Liwen You, Wenzhen Zhu, Panpan Xu, ECML PKDD 2023 International Workshop on Machine Learning for Irregular Time Series. 2023b</p>
<p>were trained and evaluated three times and their performance averaged in order to account for high variance inherent in their optimization. For inference, we used EC2 CPU instances for local models, N-HiTS, and N-BEATS. The p3.2xlarge instance (1 × V100 16GB) was used for inference for other task-specific deep learning models and pretrained models such as Lag-Llama, Moirai-1.0-R, and ForecastPFN. Since LLMTime uses a Llama-2 70B model which has significantly larger compute requirements. Patchtst Deepar, Tft, Dlinear, N-Beats Wavenet, N-Hits , WaveNet and GPT4TS models were trained on AWS EC2 p3.2xlarge instances which have 1 NVIDIA V100 GPUs with 16GB VRAM. All other baselines were trained on the CPU on Intel-based EC2 instances. LLMTime inference was performed on the p3dn.24xlarge AWS EC2 instance with 8 NVIDIA V100 32GB GPUs</p>            </div>
        </div>

    </div>
</body>
</html>
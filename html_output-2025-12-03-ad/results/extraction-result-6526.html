<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6526 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6526</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6526</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-276902538</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.06567v1.pdf" target="_blank">Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated transformative potential across various domains, yet they face significant challenges in knowledge integration and complex problem reasoning, often leading to hallucinations and unreliable outputs. Retrieval-Augmented Generation (RAG) has emerged as a promising solution to enhance LLMs accuracy by incorporating external knowledge. However, traditional RAG systems struggle with processing complex relational information and multi-step reasoning, limiting their effectiveness in advanced problem-solving tasks. To address these limitations, we propose CogGRAG, a cognition inspired graph-based RAG framework, designed to improve LLMs performance in Knowledge Graph Question Answering (KGQA). Inspired by the human cognitive process of decomposing complex problems and performing self-verification, our framework introduces a three-stage methodology: decomposition, retrieval, and reasoning with self-verification. By integrating these components, CogGRAG enhances the accuracy of LLMs in complex problem solving. We conduct systematic experiments with three LLM backbones on four benchmark datasets, where CogGRAG outperforms the baselines.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6526.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6526.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogGRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cognition Inspired Graph RAG (CogGRAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based retrieval-augmented generation framework that decomposes complex questions into a mind map, retrieves local and global knowledge from a KG, and performs bottom-up reasoning with a dual-process self-verification step to reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Decomposition + Graph-based RAG + Self-Verification (dual LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / mixed (top-down decomposition + bottom-up reasoning with verification)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop knowledge graph question answering requiring explainable, multi-step reasoning over Wikipedia-derived knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>30.7</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ToG (graph-based RAG baseline, LLaMA2-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.8</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors attribute CogGRAG's superior performance to top-down decomposition (mind map) enabling targeted retrieval of multi-level KG structure and to a dual-process self-verification stage that asks a separate verifier LLM to detect and trigger re-thinking, reducing hallucinations. They argue graph-based retrieval better captures relational dependencies than vector-only RAG and that iterative graph-search methods suffer error propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6526.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6526.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogGRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cognition Inspired Graph RAG (CogGRAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same as above, evaluated with a different backbone to test robustness across model families and scales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Decomposition + Graph-based RAG + Self-Verification (dual LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / mixed</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop KGQA requiring multi-step inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>27.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>LLaMA3-8B + KG (non-CogGRAG baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>5.8</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report consistent improvements across backbone models, and observe performance scales with model size; CogGRAG's decomposition plus verification produces robustness gains compared to simple KG-augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6526.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6526.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogGRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cognition Inspired Graph RAG (CogGRAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same method evaluated with the Qwen family to demonstrate applicability beyond LLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Decomposition + Graph-based RAG + Self-Verification (dual LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / mixed</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop KGQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>27.1</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Qwen2.5-7B + KG (non-CogGRAG baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>11.5</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CogGRAG improves Qwen backbone performance substantially versus the same backbone with only KG augmentation; results indicate method-level benefits beyond particular model families.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6526.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6526.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Think-on-Graph (ToG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based RAG method that iteratively executes search/beam search on knowledge graphs to discover likely reasoning paths and returns reasoning results discovered via graph 'thinking'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Iterative graph search / graph-based RAG</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / iterative retrieval-based</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop KGQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>27.9</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CogGRAG (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-2.8</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors treat ToG as a representative iterative graph-reasoning baseline and note iterative methods can propagate earlier-step errors through the chain of inference; CogGRAG was designed to avoid error propagation via top-down decomposition and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6526.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6526.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MindMap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MindMap (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that prompts LLMs to generate a mind-map-style decomposition and perform reasoning over knowledge graphs (used as a baseline here).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Mind-map decomposition + graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / hierarchical decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop KGQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>25.6</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CogGRAG (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-5.1</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>MindMap is similar in spirit (decomposition + KG) but, per authors, CogGRAG's retrieval pruning and explicit self-verification yield better and more reliable final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6526.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6526.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-only (Direct CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-only with Direct Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline using the LLM without external KG retrieval, relying on chain-of-thought style prompting to elicit intermediate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (Direct CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop question answering without external KGs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>17.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CogGRAG (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-13.4</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report LLM-only approaches perform substantially worse due to missing external knowledge and hallucination; integrating KGs (RAG) and graph structure is critical for knowledge-intensive multi-hop QA.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6526.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6526.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2+KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-13B with simple KG augmentation (LLM+KG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where a KG is provided but without the CogGRAG decomposition + self-verification pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B+KG</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Retrieval-augmented generation (simple KG augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop KGQA with KG-augmented context but no graph reasoning procedure</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>23.7</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CogGRAG (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-7.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>KG augmentation alone yields notable gains over LLM-only, but authors argue that structured graph-based retrieval and a thinking procedure further improve accuracy on complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6526.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6526.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Chain-of-Thought (Graph-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-augmented chain-of-thought method that encourages LLMs to reason iteratively on graphs to produce faithful, multi-hop inferences; used as a baseline on domain-specific GRBENCH.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph chain-of-thought: Augmenting large language models by reasoning on graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Graph-based Chain-of-Thought (iterative graph reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / iterative retrieval-based</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GRBENCH (E-commerce partition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Domain-specific KG question answering requiring interaction with domain graphs (e.g., e-commerce)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>24.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CogGRAG (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-4.7</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>On domain-specific KGs Graph-CoT improves over LLM-only, but CogGRAG further improves via decomposition and verification; both graph-based approaches outperform LLM-only by large margins in specialized domains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models <em>(Rating: 2)</em></li>
                <li>Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph <em>(Rating: 2)</em></li>
                <li>Graph chain-of-thought: Augmenting large language models by reasoning on graphs <em>(Rating: 2)</em></li>
                <li>Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6526",
    "paper_id": "paper-276902538",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "CogGRAG",
            "name_full": "Cognition Inspired Graph RAG (CogGRAG)",
            "brief_description": "A graph-based retrieval-augmented generation framework that decomposes complex questions into a mind map, retrieves local and global knowledge from a KG, and performs bottom-up reasoning with a dual-process self-verification step to reduce hallucinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B",
            "model_size": "13B",
            "reasoning_method_name": "Decomposition + Graph-based RAG + Self-Verification (dual LLMs)",
            "reasoning_method_type": "retrieval-based / mixed (top-down decomposition + bottom-up reasoning with verification)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "HotpotQA",
            "task_description": "Multi-hop knowledge graph question answering requiring explainable, multi-step reasoning over Wikipedia-derived knowledge.",
            "performance_metric": "Exact match (EM)",
            "performance_value": 30.7,
            "comparison_target_method": "ToG (graph-based RAG baseline, LLaMA2-13B)",
            "performance_difference": 2.8,
            "statistical_significance": null,
            "analysis_notes": "Authors attribute CogGRAG's superior performance to top-down decomposition (mind map) enabling targeted retrieval of multi-level KG structure and to a dual-process self-verification stage that asks a separate verifier LLM to detect and trigger re-thinking, reducing hallucinations. They argue graph-based retrieval better captures relational dependencies than vector-only RAG and that iterative graph-search methods suffer error propagation.",
            "ablation_study_present": true,
            "uuid": "e6526.0",
            "source_info": {
                "paper_title": "Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "CogGRAG",
            "name_full": "Cognition Inspired Graph RAG (CogGRAG)",
            "brief_description": "Same as above, evaluated with a different backbone to test robustness across model families and scales.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA3-8B",
            "model_size": "8B",
            "reasoning_method_name": "Decomposition + Graph-based RAG + Self-Verification (dual LLMs)",
            "reasoning_method_type": "retrieval-based / mixed",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "HotpotQA",
            "task_description": "Multi-hop KGQA requiring multi-step inference.",
            "performance_metric": "Exact match (EM)",
            "performance_value": 27.2,
            "comparison_target_method": "LLaMA3-8B + KG (non-CogGRAG baseline)",
            "performance_difference": 5.8,
            "statistical_significance": null,
            "analysis_notes": "Authors report consistent improvements across backbone models, and observe performance scales with model size; CogGRAG's decomposition plus verification produces robustness gains compared to simple KG-augmentation.",
            "ablation_study_present": true,
            "uuid": "e6526.1",
            "source_info": {
                "paper_title": "Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "CogGRAG",
            "name_full": "Cognition Inspired Graph RAG (CogGRAG)",
            "brief_description": "Same method evaluated with the Qwen family to demonstrate applicability beyond LLaMA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-7B",
            "model_size": "7B",
            "reasoning_method_name": "Decomposition + Graph-based RAG + Self-Verification (dual LLMs)",
            "reasoning_method_type": "retrieval-based / mixed",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "HotpotQA",
            "task_description": "Multi-hop KGQA",
            "performance_metric": "Exact match (EM)",
            "performance_value": 27.1,
            "comparison_target_method": "Qwen2.5-7B + KG (non-CogGRAG baseline)",
            "performance_difference": 11.5,
            "statistical_significance": null,
            "analysis_notes": "CogGRAG improves Qwen backbone performance substantially versus the same backbone with only KG augmentation; results indicate method-level benefits beyond particular model families.",
            "ablation_study_present": true,
            "uuid": "e6526.2",
            "source_info": {
                "paper_title": "Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ToG",
            "name_full": "Think-on-Graph (ToG)",
            "brief_description": "A graph-based RAG method that iteratively executes search/beam search on knowledge graphs to discover likely reasoning paths and returns reasoning results discovered via graph 'thinking'.",
            "citation_title": "Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B",
            "model_size": "13B",
            "reasoning_method_name": "Iterative graph search / graph-based RAG",
            "reasoning_method_type": "sequential / iterative retrieval-based",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "HotpotQA",
            "task_description": "Multi-hop KGQA",
            "performance_metric": "Exact match (EM)",
            "performance_value": 27.9,
            "comparison_target_method": "CogGRAG (proposed)",
            "performance_difference": -2.8,
            "statistical_significance": null,
            "analysis_notes": "Authors treat ToG as a representative iterative graph-reasoning baseline and note iterative methods can propagate earlier-step errors through the chain of inference; CogGRAG was designed to avoid error propagation via top-down decomposition and verification.",
            "ablation_study_present": false,
            "uuid": "e6526.3",
            "source_info": {
                "paper_title": "Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MindMap",
            "name_full": "MindMap (baseline)",
            "brief_description": "A method that prompts LLMs to generate a mind-map-style decomposition and perform reasoning over knowledge graphs (used as a baseline here).",
            "citation_title": "Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B",
            "model_size": "13B",
            "reasoning_method_name": "Mind-map decomposition + graph reasoning",
            "reasoning_method_type": "retrieval-based / hierarchical decomposition",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "HotpotQA",
            "task_description": "Multi-hop KGQA",
            "performance_metric": "Exact match (EM)",
            "performance_value": 25.6,
            "comparison_target_method": "CogGRAG (proposed)",
            "performance_difference": -5.1,
            "statistical_significance": null,
            "analysis_notes": "MindMap is similar in spirit (decomposition + KG) but, per authors, CogGRAG's retrieval pruning and explicit self-verification yield better and more reliable final answers.",
            "ablation_study_present": false,
            "uuid": "e6526.4",
            "source_info": {
                "paper_title": "Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM-only (Direct CoT)",
            "name_full": "LLM-only with Direct Chain-of-Thought prompting",
            "brief_description": "Baseline using the LLM without external KG retrieval, relying on chain-of-thought style prompting to elicit intermediate reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B",
            "model_size": "13B",
            "reasoning_method_name": "Chain-of-Thought (Direct CoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "HotpotQA",
            "task_description": "Multi-hop question answering without external KGs",
            "performance_metric": "Exact match (EM)",
            "performance_value": 17.3,
            "comparison_target_method": "CogGRAG (proposed)",
            "performance_difference": -13.4,
            "statistical_significance": null,
            "analysis_notes": "Authors report LLM-only approaches perform substantially worse due to missing external knowledge and hallucination; integrating KGs (RAG) and graph structure is critical for knowledge-intensive multi-hop QA.",
            "ablation_study_present": false,
            "uuid": "e6526.5",
            "source_info": {
                "paper_title": "Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLaMA2+KG",
            "name_full": "LLaMA2-13B with simple KG augmentation (LLM+KG)",
            "brief_description": "Baseline where a KG is provided but without the CogGRAG decomposition + self-verification pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B+KG",
            "model_size": "13B",
            "reasoning_method_name": "Retrieval-augmented generation (simple KG augmentation)",
            "reasoning_method_type": "retrieval-based",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "HotpotQA",
            "task_description": "Multi-hop KGQA with KG-augmented context but no graph reasoning procedure",
            "performance_metric": "Exact match (EM)",
            "performance_value": 23.7,
            "comparison_target_method": "CogGRAG (proposed)",
            "performance_difference": -7.0,
            "statistical_significance": null,
            "analysis_notes": "KG augmentation alone yields notable gains over LLM-only, but authors argue that structured graph-based retrieval and a thinking procedure further improve accuracy on complex reasoning.",
            "ablation_study_present": false,
            "uuid": "e6526.6",
            "source_info": {
                "paper_title": "Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Graph-CoT",
            "name_full": "Graph Chain-of-Thought (Graph-CoT)",
            "brief_description": "A graph-augmented chain-of-thought method that encourages LLMs to reason iteratively on graphs to produce faithful, multi-hop inferences; used as a baseline on domain-specific GRBENCH.",
            "citation_title": "Graph chain-of-thought: Augmenting large language models by reasoning on graphs",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B",
            "model_size": "13B",
            "reasoning_method_name": "Graph-based Chain-of-Thought (iterative graph reasoning)",
            "reasoning_method_type": "sequential / iterative retrieval-based",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "GRBENCH (E-commerce partition)",
            "task_description": "Domain-specific KG question answering requiring interaction with domain graphs (e.g., e-commerce)",
            "performance_metric": "Exact match (EM)",
            "performance_value": 24.0,
            "comparison_target_method": "CogGRAG (proposed)",
            "performance_difference": -4.7,
            "statistical_significance": null,
            "analysis_notes": "On domain-specific KGs Graph-CoT improves over LLM-only, but CogGRAG further improves via decomposition and verification; both graph-based approaches outperform LLM-only by large margins in specialized domains.",
            "ablation_study_present": false,
            "uuid": "e6526.7",
            "source_info": {
                "paper_title": "Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph",
            "rating": 2,
            "sanitized_title": "thinkongraph_deep_and_responsible_reasoning_of_large_language_model_with_knowledge_graph"
        },
        {
            "paper_title": "Graph chain-of-thought: Augmenting large language models by reasoning on graphs",
            "rating": 2,
            "sanitized_title": "graph_chainofthought_augmenting_large_language_models_by_reasoning_on_graphs"
        },
        {
            "paper_title": "Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models",
            "rating": 1,
            "sanitized_title": "mindmap_knowledge_graph_prompting_sparks_graph_of_thoughts_in_large_language_models"
        }
    ],
    "cost": 0.01644725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving
9 Mar 2025</p>
<p>Yao Cheng 
Yibo Zhao 
Jiapeng Zhu 
Yao Liu 
Xing Sun 
Xiang Li 
Manchester United 
Jose Mourinho 
Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving
9 Mar 20256FC0AEB72C34A645145AB57022E07EAAarXiv:2503.06567v1[cs.LG]Similarity Score08 Similarity Score08 Similarity Score005
Large language models (LLMs) have demonstrated transformative potential across various domains, yet they face significant challenges in knowledge integration and complex problem reasoning, often leading to hallucinations and unreliable outputs.Retrieval-Augmented Generation (RAG) has emerged as a promising solution to enhance LLMs accuracy by incorporating external knowledge.However, traditional RAG systems struggle with processing complex relational information and multi-step reasoning, limiting their effectiveness in advanced problem-solving tasks.To address these limitations, we propose CogGRAG, a cognition inspired graph-based RAG framework, designed to improve LLMs performance in Knowledge Graph Question Answering (KGQA).Inspired by the human cognitive process of decomposing complex problems and performing selfverification, our framework introduces a threestage methodology: decomposition, retrieval, and reasoning with self-verification.By integrating these components, CogGRAG enhances the accuracy of LLMs in complex problem solving.We conduct systematic experiments with three LLM backbones on four benchmark datasets, where CogGRAG outperforms the baselines.</p>
<p>Introduction</p>
<p>As a foundational technology for artificial general intelligence (AGI), large language models (LLMs) have achieved remarkable success in practical applications, demonstrating transformative potential across a wide range of domains (Touvron et al., 2023;AI@Meta, 2024;Yang et al., 2024).Their ability to process, generate, and reason with natural language has enabled significant advancements in areas such as machine translation (Zhu et al., 2023), text summarization (Basyal and Sanghvi, 2023), and question answering (Pan et al., 2023).Despite their impressive performance, LLMs still face significant limitations in knowledge integration beyond their pre-trained data boundaries.These limitations often lead to the generation of plausible but factually incorrect responses, a phenomenon commonly referred to as hallucinations, which undermines the reliability of LLMs in critical applications.</p>
<p>To mitigate hallucinations, Retrieval-Augmented Generation (RAG) (Niu et al., 2023;Gao et al., 2023;Edge et al., 2024) has emerged as a promising paradigm, significantly improving the accuracy and reliability of LLM-generated contents through the integration of external knowledge.However, while RAG successfully mitigates certain aspects of hallucination, it still exhibits inherent limitations in processing complex relational information.As shown in Figure 1 (a), the core limitation of standard RAG systems lies in their reliance on vector-based similarity matching, which processes knowledge segments as isolated units without capturing their contextual interdependencies or semantic relationships (Jin et al., 2024b).Consequently, traditional RAG implementations are inadequate for supporting advanced reasoning capabilities in LLMs, particularly in scenarios requiring complex problem-solving, multi-step inference, or sophisticated knowledge integration.</p>
<p>Recently, graph-based RAG (Edge et al., 2024;Ma et al., 2024;Jin et al., 2024b;Mavromatis and Karypis, 2024) has been proposed to address the limitations of conventional RAG systems by incorporating deep structural information from external knowledge sources.These approaches typically utilize knowledge graphs (KGs) to model complex relation patterns within external knowledge bases, employing structured triple representations (<entity, relation, entity>) to integrate fragmented information across multiple document segments.While graph-based RAG has shown promising results in mitigating hallucination and improving factual accuracy, several challenges remain unresolved:  Complex Problem Reasoning.Complex problems cannot be resolved through simple queries; they typically require multi-step reasoning to derive the final answer.While previous work has sought to enhance reasoning capabilities through methods such as chain-of-thought (CoT) reasoning or multi-hop information retrieval, these searchbased and iterative approaches still face inherent disadvantages (Jin et al., 2024b;Mavromatis and Karypis, 2024).As shown in Figure 1 (b), each step in the iterative process relies on the result of the previous step, indicating that errors occurred at previous steps can propagate to subsequent steps.</p>
<p> Hallucinations.Despite the integration of external knowledge sources, large language models (LLMs) remain prone to generating inaccurate or fabricated responses when confronted with retrieval errors or insufficient knowledge coverage.However, prior research has lacked mechanisms for selfverification, which undermines the reliability of graph-based RAG in real-world applications.</p>
<p>To address these challenges, we propose Cog-GRAG, a Cognition inspired Graph RAG framework, designed to enhance the complex problemsolving capabilities of LLMs in Knowledge Graph Question Answering (KGQA) tasks.The framework introduces a three-stage methodology: (1) Decomposition, inspired by human problem-solving strategies, breaks down complex problems topdown into smaller, simpler sub-problems and forms a mind map.(2) Retrieval, built on all the decomposed sub-problems, this stage extracts multi-level structured information from external knowledge sources using knowledge graphs.This stage operates at both local and global levels: local retrieval identifies relevant information for individual subproblems, while global retrieval establishes connections between multiple sub-problems, ensuring detailed and comprehensive knowledge integration.</p>
<p>(3) Reasoning with Self-Verification, mimicking human self-reflection, this stage evaluates the reasoning process and verifies the accuracy of intermediate and final results.CogGRAG emulates human cognitive processes in tackling complex problems by employing a top-down decomposition strategy, comprehensive knowledge retrieval, and bottomup reasoning with validation.By integrating these components, CogGRAG enhances the accuracy and reliability of LLM-generated content on KGQA tasks.</p>
<p>Related Work</p>
<p>Reasoning with LLM Prompting.Recent advancements in prompt engineering have demonstrated that state-of-the-art prompting techniques can significantly enhance the reasoning capabil- ities of LLMs on complex problems (Wei et al., 2022;Yao et al., 2024;Besta et al., 2024).Chain of Thought (CoT (Wei et al., 2022) explores how generating a chain of thought-a series of intermediate reasoning steps-significantly improves the ability of large language models to perform complex reasoning.Tree of Thoughts (ToT) (Yao et al., 2024) introduces a new framework for language model inference that generalizes the popular Chain of Thought approach to prompting language models, enabling exploration of coherent units of text (thoughts) as intermediate steps toward problemsolving.The Graph of Thoughts (GoT) (Besta et al., 2024) models the information generated by a LLM as an arbitrary graph, enabling LLM reasoning to more closely resemble human thinking or brain mechanisms.However, these methods remain constrained by the limitations of the model's pre-trained knowledge base and are unable to address hallucination issues stemming from the lack of access to external, up-to-date information.
" # " # ! $ " # # # " # % # " # &amp; # " # ' # ! ( ! ! ) ! Q ! ! " ! # " ! " " ! " ! ! ! ! ! # ! ! $ ! ! %
Knowledge Graph Augmented LLM.KGs (Vrandei and Krtzsch, 2014) offer distinct advantages in enhancing LLMs with structured external knowledge.Early graph-based RAG methods (Edge et al., 2024;He et al., 2024;Hu et al., 2024;Wu et al., 2023;Jin et al., 2024a) demonstrated this potential by retrieving and integrating structured, relevant information from external sources, enabling LLMs to achieve superior performance on knowledge-intensive tasks.However, these methods exhibit notable limitations when applied to complex problems.Recent advancements have introduced methods like chain-of-thought prompting to enhance LLMs' reasoning on complex problems (Sun et al., 2023;Ma et al., 2024;Jin et al., 2024b).Think-on-Graph (Sun et al., 2023) introduces a new approach that enables the LLM to iteratively execute beam search on the KGs, discover the most promising reasoning paths, and return the most likely reasoning results.ToG-2 (Ma et al., 2024) achieves deep and faithful reasoning in LLMs through an iterative knowledge retrieval process that involves collaboration between contexts and the KGs.GRAPH-COT (Jin et al., 2024b) propose a simple and effective framework to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively.However, iterative reasoning processes are prone to error propagation, as errors cannot be corrected once introduced.</p>
<p>Preliminaries</p>
<p>Problem Definition</p>
<p>In this paper, we focus on the KGQA task.The input of the task is the question Q, and the output is the answer A. Given the input question Q, we aim to design a graph-based RAG framework with an LLM backbone p  to enhance the complex problem-solving capabilities and generate the answer A.</p>
<p>Mind Map</p>
<p>A mind map can be conceptualized as a tree-like structure that hierarchically preserves questions decomposed in a top-down manner.Formally, a mind map M can be defined as a set of nodes.Each node m = (q, t, s)  M contains a question q, the level t  [0, T ] of the question in the mind map, and the state s  { "End", "Continue" } of the question, where T is the maximum level of the mind map and state indicates whether the question requires further decomposition.The target question Q serves as the root node m 0 , i.e., q 0 = Q, t 0 = 0 and s 0 = "Continue".</p>
<p>Methods</p>
<p>CogGRAG implements the graph-based RAG paradigm, inspired by human cognitive processes.Specifically, it simulates human cognitive strategies by decomposing complex questions into a structured mind map and incorporates self-verification mechanisms during the reasoning process.The framework of CogGRAG consists of three key steps: decomposition, retrieval, and reasoning with self-verification.The overall workflow of the framework is illustrated in Figure 2.</p>
<p>Decomposition</p>
<p>When confronted with complex question, humans typically decompose them into a series of simpler sub-questions and address them incrementally.Inspired by this cognitive strategy, in this paper, we decompose the complex question Q in a top-down manner, which are logically interconnected and form a structured mind map.The resulting mind map aids in identifying the necessary information required to answer, even if such information is not explicitly mentioned in the original question.For instance, in the question "The football manager who recruited David Beckham managed Manchester United during what timeframe?", the key entity "Alex Ferguson" (the manager who recruited David Beckham) might be overlooked by traditional matching-based RAG methods, failing to derive the correct answer.</p>
<p>Concretely, we begin at the root level t = 0.For each question q t at an intermediate level t, if its state s t = "Continue", then CogGRAG prompts LLMs to decompose the question into several subquestions {q t+1 j , j = 1, ..., N }, each paired with a corresponding states s t+1 j at the next level t + 1. Formally, this process can be represented as follows:</p>
<p>{( q t+1 j , s t+1 j ), j = 1, ..., N } = Decompose(q t , p  , prompt 1 ),</p>
<p>(1
)
where N is the number of sub-questions, which is adaptively determined by LLMs based on the logic of each sub-question.prompt 1 describes the question decomposition task in natural language, as shown in Table 6.After decomposition, each sub-question and its state are added as a new node (q t+1 j , t + 1, s t+1 j ) at level t + 1. Decomposition is terminated when all the states of the child nodes are "End", constructing a mind map M .</p>
<p>Retrieval</p>
<p>At the retrieval step, CogGRAG leverages the mind map generated from question decomposition to retrieve relevant information from an external KG.Firstly, to ensure detailed and comprehensive knowledge integration, we extract key information from all questions in the mind map from two distinct granularity levels to support subsequent retrieval processes.As shown in Figure 3, entities and triples are extracted from each individual question at the local level, while inter-question relationships are captured at the global level and represented as subgraphs.This process can be formalized as follows:
Keys = Extract(M, p  , prompt 2 ),(2)
where Keys consists of the sets of entities, triples and subgraphs extracted from the mind map M , guided by the extraction workflow outlined in prompt 2 , as shown in Table 7 and Table 8.Next, based on the extracted key information Keys, CogGRAG aims to retrieve relevant structured knowledge (triples) from the KG.Specifically, we expand each entity in the entity set of Keys to its neighbors in the KG, forming triples that link the entity to its neighboring entities via relationships.These retrieved tuples are added to a candidate tuple set T .Subsequently, to reduce redundant and noisy information, we employ the triples and the subgraphs in Keys to prune the triples in T through similarity matching.In practice, only tuples in T with similarity scores exceeding a predefined threshold  are retained, resulting in the final triples set T .This process can be formalized as follows:
T = x  T | y  T riple  Subgraph, sim(x, y) &gt;  ,(3)
where T riple  Keys represents the set of triples extracted from individual questions, Subgraph  Keys denotes the set of subgraphs extracted from inter-question relationships, sim() is implemented using cosine similarity, with  being a hyperparameter.This pruning ensures that the tuples retrieved from the KG remain both concise and relevant, enhancing the accuracy of downstream reasoning tasks.</p>
<p>Reasoning with Self-Verification</p>
<p>During the inference phase, CogGRAG utilizes the triples T retrieved in the previous step to reason and generate answers for the input question.Drawing upon the self-verification theory (Frederick, 2005), which posits that humans engage in selfverification to validate their reasoning outcomes during the cognitive process, we design a dualprocess framework inspired by dual-process theories (Vaisey, 2009).To this end, we incorporate two LLMs into this step: one dedicated to the reasoning process and the other to the self-verification of the reasoning results.This approach ensures a more robust and reliable inference mechanism by mimicking the human cognitive process of reasoning followed by critical evaluation.</p>
<p>Specifically, given the triples set T and the mind map M , we prompt the LLM to address all sub-questions in the mind map in a bottomup manner until reaching the root node, which corresponds to the target question Q.At each level t, the reasoning model LLM res infers and generates an answer a t for the question q t , where a t = LLM res (T , p  , q t , M , prompt 3 ) and M = (q T , T ), (q T 1 , T 1 ), ..., (q t+1 , t+1 ) comprises all the question-answer pairs that have already been successfully reasoned through.Subsequently, the verification model LLM ver reflects on the current reasoning results a t , aiming to identify potential errors or inconsistencies in the reasoning process based on the accumulated reasoning path.If the verification model LLM ver detects an error at the current step t, it will prompt the reasoning model LLM res to re-think and generate a new answer t , else t = a t .Notably, during reasoning, we explicitly prompt the LLM to respond with "I don't know" for questions it cannot answer, rather than generating incorrect responses.This helps mitigate the hallucination problem commonly observed in LLMs during reasoning tasks.Ultimately, CogGRAG infers the answer A to the target question Q, where A = 0 .In summary, reasoning with self-verification employs a bottom-up reasoning process based on the mind map within a dualprocess framework.All prompt templates used in CogGRAG can be found in the Appendix D.  and Qwen2.5-7B(Yang et al., 2024).For all LLMs, we load the checkpoints from huggingface1 and use the models directly without fine-tuning.We implemented CogGRAG and conducted the experiments with one A800 GPU2 .Consistent with the Think-on-graph settings, we set the temperature parameter to 0.4 during exploration and 0 during reasoning.The threshold  in the retrieval step is set to 0.7.Due to the space limitation, we move details on datasets (Sec A), and baselines (Sec B) to appendices.</p>
<p>Main Results</p>
<p>We perform experiments to verify the effectiveness of our framework CogGRAG, and report the results in Table 2.We use Rouge-L (RL) and Exact match (EM) as metric for all three datasets.The backbone model for all the methods is LLaMA2-13B.From the table, the following observations can be derived: (1) CogGRAG demonstrates the best results in most cases.(2) Compared to methods that incorporate external knowledge, the LLM-only approach demonstrates significantly inferior performance.This performance gap arises from the lack of necessary knowledge in LLMs for reasoning tasks, highlighting the critical role of external knowledge integration in enhancing the reasoning capabilities of LLMs.(3) Graph-based RAG methods demonstrate superior performance compared to LLM+KG approaches.This performance advan- tage is particularly evident in complex problems, where not only external knowledge integration but also involving "thinking procedure" is essential.Methods ToG and MindMap exhibit enhanced performance by implementing iterative retrieval and reasoning processes on the KGs, thereby generating the most probable inference outcomes through "thinking" on the KGs.We attribute CogGRAG 's outstanding effectiveness in most cases primarily to its ability to decompose complex problems and construct a structured mind map prior to retrieval.This approach enables the construction of a comprehensive reasoning pathway, facilitating more precise and targeted retrieval of relevant information.Furthermore, CogGRAG incorporates a self-verification mechanism during the reasoning phase, further enhancing the accuracy and reliability of the final results.Together, these designs collectively enhance LLMs' ability to tackle complex problems.</p>
<p>Performance with different backbone models</p>
<p>We evaluate how different backbone models affect its performance on three datasets HotpotQA, CWQ and WebQSP, and report the results in Table 3.</p>
<p>We conduct experiments with three LLM back-bones LLaMA2-13B, LLaMA3-8B and Qwen2.5-7B.For all LLMs, we load the checkpoints from huggingface and use the models directly without fine-tuning.From the table, we can observe the following key findings: (1) CogGRAG achieves the best results across all backbone models compared to the baseline approaches, demonstrating the robustness and stability of our method.</p>
<p>(2) The performance of our method improves consistently as the model scale increases, reflecting enhanced reasoning capabilities.This trend suggests that our approach has significant potential for further exploration with larger-scale models, indicating promising scalability and adaptability.</p>
<p>Performance on domain-specific KG</p>
<p>Given the risk of data leakage due to Wikidata being used as pretraining corpora for LLMs, we further evaluate the performance of CogGRAG on a recently released domain-specific dataset GR-BENCH (Jin et al., 2024b) and report the results in Table 4.This dataset requires all questions to interact with a domain-specific KG.We use Rouge-L (RL) and Exact match (EM) as metrics for this dataset.The backbone model for all the methods is LLaMA2-13B (Touvron et al., 2023).The table reveals the following observations: (1)  CogGRAG continues to outperform in most cases.This demonstrates that our method consistently achieves stable and reliable results even on domainspecific knowledge graphs.(2) Both CogGRAG and Graph-CoT outperform LLaMA2-13B by more than 20%, which can be ascribed to the fact that LLMs are typically not trained on domain-specific data.In contrast, graph-based RAG methods can effectively supplement LLMs with external knowledge, thereby enhancing their reasoning capabilities.This result underscores the effectiveness of the RAG approach in bridging knowledge gaps and improving performance in specialized domains.</p>
<p>Ablation Study</p>
<p>The ablation study is conducted to understand the importance of main components of CogGRAG.We select HotpotQA, CWQ and WebQSP as three representative datasets.First, we remove the problem decomposition module, directly extracting information for the target question, and referred to this variant as CogGRAG-nd (no decomposition).Next, we eliminate the global-level retrieval phase, naming this variant CogGRAG-ng (no global level).Finally, we remove the self-verification mechanism during the reasoning stage, designating this variant as CogGRAG-nv (no verification).These experiments aim to systematically assess the impact of each component on the overall performance of the framework.We compare CogGRAG with these three variants, and the results are presented in Figure 4. Our findings show that CogGRAG outperforms all the variants on the three datasets.Furthermore, the performance gap between CogGRAG and CogGRAG-nd highlights the importance of decomposition for complex problem reasoning in KGQA tasks.</p>
<p>Hallucination and Missing Evaluation</p>
<p>In the reasoning and self-verification phase of Cog-GRAG, we prompt the LLM to respond with "I don't know" when encountering questions with insufficient or incomplete relevant information during the reasoning process.This approach is designed to mitigate the hallucination issue commonly observed in LLMs.To evaluate the effectiveness of this strategy, we test the model on the Hot-potQA dataset, with results reported in Table 5.We categorize the responses into three types: "Correct" for accurate answers, "Missing" for cases where the model responds with "I don't know," and "Hallucination" for incorrect answers.As shown in the table results, our model demonstrates the ability to refrain from answering questions with insufficient information, significantly reducing the occurrence of hallucinations.This capability highlights the effectiveness of our approach in enhancing the reliability and truthfulness of the model's responses.</p>
<p>Conclusion</p>
<p>In this paper, we proposed a graph-based RAG framework, CogGRAG, to enhance the complex reasoning capabilities of LLMs in KGQA tasks.Inspired by human cognitive processes when tackling complex problems, CogGRAG simulates human reasoning by decomposing complex questions into sub-questions and constructing a structured mind map.Furthermore, recognizing that humans often engage in self-verification during reasoning, CogGRAG incorporates a self-verification mechanism during the reasoning phase, leveraging a dualprocess to perform reasoning followed by verification, thereby reducing the occurrence of hallucinations.By integrating decomposition, retrieval, and reasoning with self-verification, CogGRAG significantly improves the accuracy of LLMs in handling complex problems.Extensive experiments demonstrate that CogGRAG outperforms state-of-the-art baselines in the majority of cases.</p>
<p>Limitations</p>
<p>Although CogGRAG significantly enhances the reasoning capabilities of LLMs in complex problems on KGQA tasks, several limitations remain.First, while our method avoids iterative retrieval and reasoning, the processes of problem decomposition and self-verification inevitably introduce additional computational overhead.Thus, there is room for future improvements in optimizing reasoning efficiency for practical applications.Second, CogGRAG is currently limited to graph-based external knowledge and cannot be directly applied to document-based knowledge sources, which restricts its generalizability in real-world scenarios.</p>
<p>In the future, we plan to explore methods that integrate multiple types of data structures as external knowledge to further enhance the reasoning capabilities of LLMs.Additionally, research on improving retrieval and reasoning efficiency remains a key focus for our ongoing work.</p>
<p>Ethical Considerations</p>
<p>Our research is fundamental in nature and is not directly linked to specific applications.Consequently, the potential for misuse or negative societal impacts depends on how others may apply our methodology.Furthermore, our work does not involve any stakeholders who may benefit or be disadvantaged, nor does it involve vulnerable groups.All datasets used in this study are publicly available and widely utilized in the research community, ensuring no privacy risks and alignment with their intended purpose for scientific inquiry.</p>
<p>A Datastes</p>
<p>Here, we introduce the four datasets used in our experiments in detail.For HotpotQA and CWQ datasets, we evaluated the performance of all methods on the dev set.For WebQSP dataset, we evaluated the performance of all methods on the train set.The statistics and details of these datasets can be found in Table 1, and we describe each dataset in detail below:</p>
<p>HotpotQA is a large-scale question answering dataset aimed at facilitating the development of QA systems capable of performing explainable, multihop reasoning over diverse natural language.It contains 113k question-answer pairs that were collected through crowdsourcing based on Wikipedia articles.</p>
<p>WebQSP is a dataset designed specifically for question answering and information retrieval tasks, aiming to promote research on multi-hop reasoning and web-based question answering techniques.It contains 4,737 natural language questions that are answerable using a subset Freebase KG (Bollacker et al., 2008).</p>
<p>CWQ is a dataset specially designed to evaluate the performance of models in complex question answering tasks.It is generated from WebQSP by extending the question entities or adding constraints to answers, in order to construct more complex multi-hop questions.</p>
<p>GRBENCH is a dataset to evaluate how effectively LLMs can interact with domain-specific graphs containing rich knowledge to solve the desired problem.GRBENCH contains 10 graphs from 5 general domains (academia, e-commerce, literature, healthcare, and legal).Each data sample in GRBENCH is a question-answer pair.</p>
<p>B Baselines</p>
<p>In this subsection, we introduce the baselines used in our experiments, including LLaMA2-13B, LLaMA3-8B, Qwen2.5-7B,Chain-of-Thought (CoT) prompting, Think-on-graph (ToG), MindMap and Graph-CoT.LLaMA2-13B (Touvron et al., 2023)</p>
<p>C Case Studies</p>
<p>In this section, we present a case analysis of the HotpotQA dataset to evaluate the performance of CogGRAG.As illustrated in Figure 5, CogGRAG decomposes the input question into two logically related sub-questions.Specifically, the complex question is broken down into "In which league cup" Decomposition { "Sub-question": "What league cup is the Wigan Athletic F.C. competing in during the 2017-18 seaso n?", "Sta te": "E nd." } {"Sub-question": "What is the name of the league cup from the 2017-18 Wigan Athletic F.C. seaso n?", "Sta te": "E nd." } Question: "The 2017-18 Wigan Athletic F.C. season will be a year in which the team competes in the league cup known as what for sp onso rsh ip rea sons ?"</p>
<p>{ "Question": "In which league cup did Wigan Athletic F.C. compete during the 2017-18 season?","Answer": "Wigan Athletic F.C. competed in the EFL Cup during the 2017-18 season."}{ "Question": "What was the sponsored name of the league cup identified in sub-question #1 during the 2017-18 season?","Answer": "The sponsored name of the league cup during the 2017-18 season was the Carabao Cup."}</p>
<p>[right]</p>
<p>[right]</p>
<p>[{ "Question": "The 2017-18 Wigan Athletic F.C. season will be a year in which the team competes in the league cup known as what for sponsorship reasons?", "Answer": "Carabao Cup."}]</p>
<p>Reasoning and Self-Verification</p>
<p>Entity: ["Wigan Athletic F.C.", "2017-18 season", "league cup", "sponsorship reasons" ] Triples: [ ("Wigan Athletic F.C.", "competes in", "league cup" ), ("league cup", "sponsorship name", "unknown" ), ("2017-18 season", "associated with", "Wigan Athletic F.C." ), ("league cup", "associated with", "2017-18 season" )] Subgraph: [ ("Wigan Athletic F.C.", "competes in", "league cup"), ("Wigan Athletic F.C.", "competes in", "2017-18 season"), ("Wigan Athletic F.C.", "has", "league cup name") ]</p>
<p>Retrieval</p>
<p>Retrieval and Prune</p>
<p>Triples: [("Wigan Athletic F.C.", "is a", "football club"), ("Wigan Athletic F.C.", "based in", "Wigan, England"), ("Wigan Athletic F.C.", "founded in", "1932"), ("Wigan Athletic F.C.", "competes in", "EFL Championship"), ("2017-18 season", "start date", "August 2017"), ("2017-18 season", "end date", "May 2018"), ("league cup", "official name", "EFL Cup"), ("league cup", "sponsored by", "Carabao"), ("league cup", "involves", "Wigan Athletic F.C."), ("league cup", "associated with", "EFL Championship"), ("league cup", "sponsorship name", "Carabao Cup")] and "What was the sponsored name", allowing the system to first identify the league cup and then determine its sponsored name based on the identified league.These sub-questions form a mind map, capturing the relationships between different levels of the problem.Next, CogGRAG extracts key information from all sub-questions, including both local level information within individual sub-questions and global level information across different subquestions.A subgraph is constructed to represent the interconnected triples within the subgraph, enabling a global perspective to model the relationships between different questions.The retrieved triples are pruned based on similarity metrics.All information retrieved from the KG is represented in the form of triples.Using this knowledge, Cog-GRAG prompts the LLM to perform bottom-up reasoning and self-verification based on the constructed mind map.Through this process, the system ultimately derives the answer to the target question: "Carabao Cup."This case demonstrates the effectiveness of CogGRAG in handling complex, multi-step reasoning tasks by leveraging hierarchical decomposition, structured knowledge retrieval, and self-verification mechanisms.</p>
<p>Additionally, Figure 6, Figure 8 and Figure 7 illustrate the process of prompting the large language model (LLM) to perform reasoning and selfverification, which provides a detailed breakdown of how the model generates and validates intermediate reasoning steps, ensuring the accuracy and reliability of the final output.</p>
<p>D Prompts in CogGRAG</p>
<p>In this section, we show all the prompts that need to be used in the main experiments as shown in Table 6, Table 7, Table 8, Table 9, Table 10 and  Table 11.</p>
<p>Input</p>
<p>Your task is to answer the questions with the provided completed reasoning and input knowledge.Please note that the response must be included in square brackets [xxx].</p>
<p>The completed reasoning: [ ]</p>
<p>The knowledge: Triples: [("Wigan Athletic F.C.", "is a", "football club"), ("Wigan Athletic F.C.", "based in", "Wigan, England"), ("Wigan Athletic F.C.", "founded in", "1932"), ("Wigan Athletic F.C.", "competes in", "EFL Championship"), ("2017-18 season", "start date", "August 2017"), ("2017-18 season", "end date", "May 2018"), ("league cup", "official name", "EFL Cup"), ("league cup", "sponsored by", "Carabao"), ("league cup", "involves", "Wigan Athletic F.C."), ("league cup", "associated with", "EFL Championship"), ("league cup", "sponsorship name", "Carabao Cup")]</p>
<p>The Input: ["In which league cup did Wigan Athletic F.C. compete during the 2017-18 season?","What was the sponsored name of the league cup identified in sub-question #1 during the 2017-18 season?"]Output</p>
<p>Response</p>
<p>[{ "Question": "In which league cup did Wigan Athletic F.C. compete during the 2017-18 season?","Answer": "Wigan Athletic F.C. competed in the EFL Cup during the 2017-18 season."}, { "Question": "What was the sponsored name of the league cup identified in subquestion #1 during the 2017-18 season?","Answer": "The sponsored name of the league cup during the 2017-18 season was the Carabao Cup." }]</p>
<p>Input</p>
<p>Your task is to answer the questions with the provided completed reasoning and input knowledge.Please note that the response must be included in square brackets [xxx].</p>
<p>The completed reasoning: [ { "Question": "In which league cup did Wigan Athletic F.C. compete during the 2017-18 season?","Answer": "Wigan Athletic F.C. competed in the EFL Cup during the 2017-18 season."}, { "Question": "What was the sponsored name of the league cup identified in sub-question #1 during the 2017-18 season?","Answer": "The sponsored name of the league cup during the 2017-18 season was the Carabao Cup." }] The knowledge: Triples: [("Wigan Athletic F.C.", "is a", "football club"), ("Wigan Athletic F.C.", "based in", "Wigan, England"), ("Wigan Athletic F.C.", "founded in", "1932"), ("Wigan Athletic F.C.", "competes in", "EFL Championship"), ("2017-18 season", "start date", "August 2017"), ("2017-18 season", "end date", "May 2018"), ("league cup", "official name", "EFL Cup"), ("league cup", "sponsored by", "Carabao"), ("league cup", "involves", "Wigan Athletic F.C."), ("league cup", "associated with", "EFL Championship"), ("league cup", "sponsorship name", "Carabao Cup")]</p>
<p>The Input: ["The 2017-18 Wigan Athletic F.C. season will be a year in which the team competes in the league cup known as what for sponsorship reasons?"]Output:</p>
<p>Response</p>
<p>[ { "Question": "The 2017-18 Wigan Athletic F.C. season will be a year in which the team competes in the league cup known as what for sponsorship reasons?", "Answer": "During the 2017-18 season, Wigan Athletic F.C. competed in the league cup known as the Carabao Cup for sponsorship reasons."} ]</p>
<p>Input</p>
<p>You are a logical verification assistant.Your task is to check whether the answer to a given question is logically consistent with the provided completed reasoning and input knowledge.If the answer is consistent, respond with "right".If the answer is inconsistent, respond with "wrong".Please note that the response must be included in square brackets [xxx].</p>
<p>The completed reasoning: [ ]</p>
<p>The knowledge: Triples: [("Wigan Athletic F.C.", "is a", "football club"), ("Wigan Athletic F.C.", "based in", "Wigan, England"), ("Wigan Athletic F.C.", "founded in", "1932"), ("Wigan Athletic F.C.", "competes in", "EFL Championship"), ("2017-18 season", "start date", "August 2017"), ("2017-18 season", "end date", "May 2018"), ("league cup", "official name", "EFL Cup"), ("league cup", "sponsored by", "Carabao"), ("league cup", "involves", "Wigan Athletic F.C."), ("league cup", "associated with", "EFL Championship"), ("league cup", "sponsorship name", "Carabao Cup")] The Input: ["In which league cup did Wigan Athletic F.C. compete during the 2017-18 season?"]The answer: [ { "Question": "In which league cup did Wigan Athletic F.C. compete during the 2017-18 season?","Answer": "Wigan Athletic F.C. competed in the EFL Cup during the 2017-18 season."}] Output:</p>
<p>Response</p>
<p>[right] The answers are logically consistent with the provided knowledge.The triples confirm that Wigan Athletic F.C. competed in the EFL Cup (league cup) during the 2017-18 season, and the sponsored name of the league cup was the Carabao Cup.Therefore, the answers are correct.</p>
<p>Decomposition prompt</p>
<p>Prompt head: "Your task is to decompose the given question Q into sub-questions.You should based on the specific logic of the question to determine the number of sub-questions and output them sequentially." Instruction: "Please only output the decomposed sub-questions as a string in list format, where each element represents the text of a sub-question, in the form of '[\"subq1\", \"subq2\", \"subq3\"]'.For each sub-questions, if you consider the sub-question to be sufficiently simple and no further decomposition is needed, then output \"End.\",otherwise, output \"Continue.\".Please strictly follow the format of the example below when answering the question.Here are some examples: " " Input: "What year did Guns N Roses perform a promo for a movie starring Arnold Schwarzenegger as a former New York Police detective?"Output: [ { "Sub-question": "What movie starring Arnold Schwarzenegger as a former New York Police detective is being referred to?", "State": "Continue."}, { "Sub-question": "In what year did Guns N Roses perform a promo for the movie mentioned in sub-question #1?", "State": "End."} ] Input: "What is the name of the fight song of the university whose main campus is in Lawrence, Kansas and whose branch campuses are in the Kansas City metropolitan area?" Output: [ { "Sub-question": "Which university has its main campus in Lawrence, Kansas and branch campuses in the Kansas City metropolitan area?", "State": "End."}, { "Sub-question": "What is the name of the fight song of the university identified in sub-question #1?", "State": "End."} ] Input: "Are the Laleli Mosque and Esma Sultan Mansion located in the same neighborhood?"Output: [ { "Sub-question": "Where is the Laleli Mosque located?", "State": "End."}, { "Sub-question": "Where is the Esma Sultan Mansion located?", "State": "End."}, { "Sub-question": "Are the locations of the Laleli Mosque and the Esma Sultan Mansion in the same neighborhood?","State": "End."} ] " "Input: Question Q" "Output: "</p>
<p>Extraction prompt</p>
<p>Prompt head: "Your task is to extract the entities (such as people, places, organizations, etc.) and relations (representing behaviors or properties between entities, such as verbs, attributes, or categories, etc.) involved in the input questions.These entities and relations can help answer the input questions.",Instruction: "Please extract entities and relations in one of the following forms: entity, tuples, or triples from the given input List.Entity means that only an entity, i.e. <entity>.Tuples means that an entity and a relation, i.e. <entity-relation>.Triples means that complete triples, i.e. <entity-relation-entity>.Please strictly follow the format of the example below when answering the question.", "Input: [The mind map M ]" "Output: "</p>
<p>Extraction prompt</p>
<p>Prompt head: "Your task is to extract the subgraphs involved in a set of input questions.",Instruction: "Please extract and organize information from a set of input questions into structured subgraphs.Each subgraph represents a group of triples (subject, relation, object) that share common entities and capture the logical relationships between the questions.Here are some examples:" " Input: ["What is the capital of France?", "Who is the president of France?", "What is the population of Paris?"] Output: [("France", "capital", "Paris"), ("France, "president", "Current President"), ("Paris", "population", "Population Number") ] " "Input: [The mind map M ]" "Output: "</p>
<p>Reasoning prompt</p>
<p>Prompt head: "Your task is to answer the questions with the provided completed reasoning and input knowledge.",Instruction: "Please note that the response must be included in square brackets [xxx].""The completed reasoning: [The reasoning process M ]" "The knowledge graph: [Knowledge T ]" "Input: [Subquestion q t j ] " "Output: "</p>
<p>Self-verification prompt</p>
<p>Prompt head: "You are a logical verification assistant.Your task is to check whether the answer to a given question is logically consistent with the provided completed reasoning and input knowledge.</p>
<p>If the answer is consistent, respond with "right".If the answer is inconsistent, respond with "wrong"."Instruction: "Please note that the response must be included in square brackets [xxx]."</p>
<p>"The completed reasoning: [The reasoning process M ]"</p>
<p>"The knowledge graph: [Knowledge T ]" "Input: [Subquestion q t j ] " "Answer: [Answer a t j ] "</p>
<p>"Output: "</p>
<p>Re-thinking prompt</p>
<p>Prompt head:"You are a reasoning and knowledge integration assistant.Your task is to re-think a question that was previously answered incorrectly by the self-verification model.Use the provided completed reasoning and input knowledge to generate a new answer."Instruction: "Please note, if the knowledge is insufficient to answer the question, respond with "Insufficient information, I don't know".The response must be included in square brackets [xxx]."</p>
<p>"The completed reasoning: [The reasoning process M ]" "The knowledge graph: [Knowledge T ]" "Input: [Subquestion q t j ] " "Output: "</p>
<p>Figure 1 :
1
Figure 1: Representative workflow of two Retrieval-Augmented Generation paradigms for enhancing LLMs.</p>
<p>Figure 2 :
2
Figure 2: The overall process of CogGRAG.Given a target question Q, CogGRAG first prompts the LLM to decompose it into a hierarchy of sub-problems in a top-down manner, constructing a structured mind map.Subsequently, CogGRAG prompts the LLM to extract both local and global key information from these subproblems.Finally, CogGRAG guides the LLM to perform bottom-up reasoning and verification based on the mind map and the retrieved knowledge, until the final answer is derived.</p>
<p>Figure 3 :
3
Figure 3: The key information extraction with local and global level.</p>
<p>Figure 4 :
4
Figure 4: Ablation study on the main components of CogGRAG.</p>
<p>Figure 5 :
5
Figure 5: Case of CogGRAG.</p>
<p>Figure 6 :
6
Figure 6: The prompt case of reasoning.</p>
<p>Figure 7 :
7
Figure 7: The prompt case of reasoning with the completed reasoning.</p>
<p>Figure 8 :
8
Figure 8: The prompt case of self-verification.</p>
<p>Table 1 :
1
Datasets statistics.
DomainDatasetData SplitTrainDev TestHotpotQA90564 7405WikipediaWebQSP30980CWQ27,734 3480AcademicGRBENCH-Academic850E-commerceGRBENCH-Amazon200LiteratureGRBENCH-Goodreads240HealthcareGRBENCH-Disease2705 Experiments5.1 Experimental SettingsDatasets and evaluation metrics. In order to testCogGRAG's complex problem-solving capabili-ties on KGQA tasks, we evaluate CogGRAG on
(Vrandei and Krtzsch, 2014) benchmarks: (1) HotpotQA(Yang et al., 2018), (2) WebQSP(Yih et al., 2016), (3) CWQ(Talmor and Berant, 2018).Following previous work(Li et al., 2023), full Wikidata(Vrandei and Krtzsch, 2014)is used</p>
<p>Table 2 :
2
Overall results of our CogGRAG on three KBQA datasets.The best score on each dataset is highlighted.
TypeMethodHotpotQACWQWebQSPRLEMRLEMRLEMWithout external knowledge graphLLM-onlyDirect CoT19.1% 23.3%17.3% 20.8%31.4% 35.1%28.8% 32.7%51.4% 55.2%47.9% 51.6%With external knowledge graphLLM+KGDirect+KG CoT+KG27.5% 28.7%23.7% 25.4%39.7% 42.2%35.1% 37.6%52.5% 52.8%49.3% 48.1%Graph-based RAGToG MindMap29.3% 27.9%26.4% 25.6%54.6% 46.7%49.1% 43.7%61.7% 56.6%57.4% 53.1%OursCogGRAG34.4%30.7%56.3%53.4%59.8%56.1%
(Touvron et al., 2023ge sources for all of these datasets.Considering that Wikidata is commonly used for training LLMs, there is a need for a domain-specific QA dataset that is not exposed during the pretraining process of LLMs in order to better evaluate the performance.Thus, we also test CogGRAG on a recently released domain-specific dataset GRBENCH(Jin et al., 2024b).All methods need interact with domain-specific graphs containing rich knowledge to solve the problem in this dataset.The statistics and details of these datasets can734 be found in Table1.For all datasets, we use two evaluation metrics: (1) Exact match(EM): measures whether the predicted answer or result matches the target answer exactly.(2)Rouge-L(RL):measures the longest common subsequence of words between the responses and the ground truth answers.Mindmap(Wen et al., 2023), Think-on-graph(Sun et al., 2023)andGraph-CoT (Jin et al., 2024b).Experimental Setup.We conduct experiments with three LLM backbones: LLaMA2-13B(Touvron et al., 2023), LLaMA3-8B (AI@Meta, 2024)</p>
<p>Table 3 :
3
Overall results of our CogGRAG with different backbone models on three KBQA datasets.We highlight the best score on each dataset in bold.
TypeMethodHotpotQACWQWebQSPRLEMRLEMRLEMQwen2.5-7B15.3%15.0%25.4%24.1%46.7%45.3%LLM-onlyLLaMA3-8B17.5%14.9%30.3%27.5%50.4%45.1%LLaMA2-13B19.1%17.3%31.4%28.8%51.4%47.9%Qwen2.5-7B+KG24.2%15.6%33.8%32.1%46.7%45.3%LLM+KGLLaMA3-8B+KG25.9%21.4%40.6%35.3%53.6%49.1%LLaMA2-13B+KG27.5%23.7%39.7%35.1%52.5%49.3%CogGRAG w/ Qwen2.5-7B28.4%27.1%50.5%45.7%53.2%51.6%Graph-Based RAGCogGRAG w/ LLaMA3-8B32.1%27.2%53.5%48.4%57.2%55.3%CogGRAG w/ LLaMA2-13B34.4%30.7%56.3%53.4%59.8%56.1%</p>
<p>Table 4 :
4
Overall results of our CogGRAG on GRBENCH dataset.We highlight the best score on each dataset in bold.
MethodE-commerceLiteratureAcademicHealthcareRLEMRLEMRLEMRLEMLLaMA2-13B7.1%6.8%5.4%5.1%5.4%4.7%4.3%3.1%Graph-CoT26.4%24.0%26.7%23.3%19.3%14.8%28.1%25.2%CogGRAG30.2%28.7%32.4%30.1%23.6%21.5%27.4%25.6%</p>
<p>Table 5 :
5
Overall results of our CogGRAG on GR-BENCH dataset.We highlight the best score on each dataset in bold.
MethodCorrect Missing HallucinationLLaMA2-13B 19.1%25.7%55.2%ToG29.3%20.2%50.5%MindMap27.9%22.4%49.7%CogGRAG34.4%40.6%44.9%</p>
<p>Table 6 :
6
The prompt template for decomposition.</p>
<p>Table 7 :
7
The prompt template for extraction on local level.</p>
<p>Table 8 :
8
The prompt template for extraction on global level.</p>
<p>Table 9 :
9
The prompt template for reasoning.</p>
<p>Table 10 :
10
The prompt template for self-verification.</p>
<p>Table 11 :
11
The prompt template for re-thinking.</p>
<p>https://huggingface.co
Our code is available at: https://anonymous.4open. science/r/RAG-5883</p>
<p>A I , Meta , Llama 3 model card. 2024</p>
<p>Text summarization using large language models: a comparative study of mpt-7b-instruct, falcon-7b-instruct, and openai chat-gpt models. Lochan Basyal, Mihir Sanghvi, arXiv:2310.104492023arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, Proceedings of the 2008 ACM SIG-MOD international conference on Management of data. the 2008 ACM SIG-MOD international conference on Management of data2008</p>
<p>From local to global: A graph rag approach to query-focused summarization. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson, arXiv:2404.161302024arXiv preprint</p>
<p>Cognitive reflection and decision making. Shane Frederick, Journal of Economic perspectives. 1942005</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. Xiaoxin He, Yijun Tian, Yifei Sun, V Nitesh, Thomas Chawla, Yann Laurent, Xavier Lecun, Bryan Bresson, Hooi, arXiv:2402.076302024arXiv preprint</p>
<p>Grag: Graph retrieval-augmented generation. Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, Liang Zhao, arXiv:2405.165062024arXiv preprint</p>
<p>Large language models on graphs: A comprehensive survey. Gang Bowen Jin, Chi Liu, Meng Han, Heng Jiang, Jiawei Ji, Han, IEEE Transactions on Knowledge and Data Engineering. 2024a</p>
<p>Graph chain-of-thought: Augmenting large language models by reasoning on graphs. Chulin Bowen Jin, Jiawei Xie, Kashob Zhang, Yu Kumar Roy, Zheng Zhang, Ruirui Li, Xianfeng Li, Suhang Tang, Yu Wang, Meng, arXiv:2404.071032024barXiv preprint</p>
<p>Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources. Xingxuan Li, Ruochen Zhao, Ken Yew, Bosheng Chia, Shafiq Ding, Soujanya Joty, Lidong Poria, Bing, arXiv:2305.132692023arXiv preprint</p>
<p>Think-on-graph 2.0: Deep and faithful large language model reasoning with knowledgeguided retrieval augmented generation. Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, Jian Guo, arXiv:2407.108052024Preprint</p>
<p>Costas Mavromatis, George Karypis, arXiv:2405.20139Gnnrag: Graph neural retrieval for large language model reasoning. 2024arXiv preprint</p>
<p>Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Randy Zhong, Juntong Song, Tong Zhang, arXiv:2401.00396Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models. 2023arXiv preprint</p>
<p>Unifying large language models and knowledge graphs: A roadmap. Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, arXiv:2306.083022023. 2023arXiv preprint</p>
<p>Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, Jian Guo, arXiv:2307.07697Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. 2023arXiv preprint</p>
<p>Alon Talmor, Jonathan Berant, arXiv:1803.06643The web as a knowledge-base for answering complex questions. 2018arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Motivation and justification: A dual-process model of culture in action. Stephen Vaisey, American journal of sociology. 11462009</p>
<p>Wikidata: a free collaborative knowledgebase. Denny Vrandei, Markus Krtzsch, Communications of the ACM. 57102014</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. Yilin Wen, Zifeng Wang, Jimeng Sun, arXiv:2308.097292023arXiv preprint</p>
<p>Retrieve-rewriteanswer: A kg-to-text enhanced llms framework for knowledge graph question answering. Yike Wu, Nan Hu, Sheng Bi, Guilin Qi, Jie Ren, Anhuan Xie, Wei Song, arXiv:2309.112062023arXiv preprint</p>
<p>. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, arXiv:2407.10671Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei ChuarXiv preprintYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024. Qwen2 technical report</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.09600Hotpotqa: A dataset for diverse, explainable multi-hop question answering. 2018arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>The value of semantic parse labeling for knowledge base question answering. Wen-Tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, Jina Suh, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsShort Papers20162</p>
<p>Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, Lei Li, arXiv:2304.04675Multilingual machine translation with large language models: Empirical results and analysis. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
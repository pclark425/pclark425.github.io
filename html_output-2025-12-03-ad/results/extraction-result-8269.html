<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8269 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8269</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8269</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-258840882</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.14333v1.pdf" target="_blank">Automatic Model Selection with Large Language Models for Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We introduce a model selection method to combine the best of both worlds by employing a large language model (LLM) to dynamically select between them. Our theoretical analysis underscores the feasibility of this method, which is further corroborated by empirical results. Our proposed method demonstrates significant performance improvements across eight reasoning datasets with Codex, ChatGPT, and GPT-4. Additionally, our method is complementary to self-consistency; when integrated, it can further enhance performance while significantly reducing computation costs. Moreover, we achieve new state-of-the-art results on GSM8K and SVAMP, with respective accuracies of 96.8% and 93.7%. Our code, data and prompts are available at https://github.com/XuZhao0/Model-Selection-Reasoning</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8269.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8269.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ModelSelection-CoT-PAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based model selection to choose between Chain-of-Thought and Program-Aided Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's principal method: use a large language model (few-shot in-context) to select between two distinct reasoning modalities — natural-language Chain-of-Thought (CoT) and programmatic Program-Aided Language (PAL) — by showing the LLM both solutions (labeled A/B) and asking it to pick which is correct (with an explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex / ChatGPT / GPT-4 (backbones used interchangeably)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Off-the-shelf large language models (OpenAI code-davinci-002 Codex, gpt-3.5-turbo ChatGPT, and GPT-4) used both to generate CoT/PAL solutions and to perform few-shot selection; same LLM is used for generation and selection in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT) — natural language multi-step reasoning', 'Program-Aided Language Model (PAL) — Python program representation of solution', 'LLM-based model selection with few-shot examples and explanation', 'Optional integration with self-consistency (majority voting across sampled outputs)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT: prompt an LLM to generate intermediate natural-language reasoning steps before final answer. PAL: prompt an LLM to produce a Python function whose execution yields the answer (computational correctness). Model selection: present pairs A/B (PAL vs CoT) plus few-shot examples (8-shot for Codex, 5-shot for ChatGPT/GPT-4 in arithmetic tasks; 6-shot for date tasks), ask the LLM to choose (A or B) and provide an explanation; if selection output is ambiguous (e.g. 'Both'/'Neither') the implementation chooses randomly. For generation/selection temperatures: greedy (temp=0) for selection; generation uses lower/higher temps for CoT/PAL and for self-consistency sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Directly compares combining two distinct reasoning methods (CoT vs PAL) via an LLM selector; used few-shot selection prompts showing paired reasoning chains and asking the LLM to pick correct one. Also compares against combining similar methods (CoT variants) and ablates use of selection explanations and self-consistency. Experiments run with three backbones across eight reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Eight reasoning datasets spanning arithmetic and symbolic reasoning: GSM8K, SVAMP, ASDIV, SingleOP, SingleEQ, AddSub, MultiArith (arithmetic) and Date Understanding (symbolic).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Combining CoT and PAL via LLM selection yields consistent improvements across datasets and backbones. Highlights: new SOTA on GSM8K (96.5% accuracy) and SVAMP (93.7%) reported (with GPT-4 + model selection + self-consistency). Without self-consistency GPT-4 + selection: GSM8K 95.6%, SVAMP 93.7% (reported). Selection success rate example: GPT-4 achieves 72.6% correct selection on GSM8K. Paper reports a 3.2% improvement over PAL's 71.5% on GSM8K in a setting where CoT alone was 64.4% (text reports the 3.2% improvement but does not name the exact combined numeric in that sentence). ChatGPT shows smaller gains (e.g., with SC@10 combined selection reached 88.9% on GSM8K), and Codex/GPT-4 show larger improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLMs can reliably spot differences between CoT and PAL chains and select the correct solving method; combining dissimilar methods produces larger per-question accuracy differences (|R(x)|) making selection more beneficial. Explanation prompts help weaker in-context learners (ChatGPT) more than strong ones (GPT-4, Codex). Model selection is robust to imperfect selection accuracy — theory and experiments show improvement even when overall selection probability ρ can be < 0.5 in some constructions. Combining CoT and PAL extracts complementary strengths: CoT's generality and interpretability vs PAL's computational accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Combining diverse reasoning methods (CoT and PAL) via an LLM-based model selector yields consistent performance gains; the magnitude of improvement depends jointly on (1) how dissimilar the methods are per instance (|R(x)|) and (2) the selection success probability (ρ_x). Even modest or imperfect selection can produce net gains if methods are sufficiently different.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8269.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8269.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT_similar_combo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selection between similar Chain-of-Thought variants (CoT, CoT' and ComplexCoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation testing whether selecting between two similar CoT-derived methods (temperature or prompt-variant CoT) yields performance gains compared to selecting between CoT and PAL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / GPT-4 / Codex (used in these ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLM backbones used to produce CoT variants by changing temperature (CoT') or using more complex prompt examples (ComplexCoT).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT) baseline', "CoT' (lower temperature variant)", 'ComplexCoT (more complex prompt examples)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate CoT reasoning chains with slight variations: modify sampling temperature (CoT') or modify prompt complexity (ComplexCoT). Then perform same LLM-based pairwise selection between two CoT-style solutions to see if selection improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compare model selection between CoT vs CoT' and CoT vs ComplexCoT (both similar natural-language reasoning styles) and contrast with CoT vs PAL (diverse). Evaluate on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (primary reported ablation dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Selection between similar CoT variants (CoT-CoT' or CoT-ComplexCoT) did not yield substantial performance gains, despite those variants having accuracy comparable to PAL in some cases. The ΔUpperBound (theoretical room for improvement) for CoT-PAL exceeds that for CoT-CoT' and CCoT-CoT, and empirical success-selection rates were lower for similar-method combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>When base methods are similar, per-question differences |R(x)| are small and the LLM selector has less signal to pick the better method, so selection provides little benefit; dissimilar methods (CoT vs PAL) produce clearer differences and higher selection success.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Combining similar reasoning approaches yields little improvement; the benefit of model selection primarily arises when the combined methods are behaviorally dissimilar on instances.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8269.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8269.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority voting on multiple sampled reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ensembling method that samples multiple reasoning paths/outputs from an LLM and selects the majority answer (used alone and in combination with LLM selection).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex / ChatGPT / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs sampled multiple times under nonzero temperature to produce diverse reasoning chains; majority voting (self-consistency) applied to outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Self-consistency: sample multiple CoT/PAL outputs and take majority answer', 'Combined with model selection: generate multiple samples per method and then apply selection + majority voting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Self-consistency sampling: run CoT with temperature (e.g., 0.5) and PAL with its own temperature (e.g., 0.8) to produce many candidate reasoning traces; perform majority vote over final answers (SC@5, SC@10 denote 5 or 10 samples). In combined experiments, selection was applied in each sampled run and majority voting used to produce final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (diverse paths within a single method, and diversity across methods when paired with selection)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Run multiple sampled traces per base method (5 or 10), use majority voting (self-consistency) and compare self-consistency alone vs self-consistency + model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (main reported), other arithmetic datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Self-consistency alone yields strong gains; combining self-consistency with model selection gives further improvements. Example: GPT-4 with model selection + self-consistency reaches GSM8K 96.5% (SOTA). Using 5 vs 10 samples produced similar results with GPT-4 in the paper; ChatGPT + selection + SC@10 reached 88.9% on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Self-consistency (diverse sampling) complements model selection: the two techniques are additive and produce higher final accuracy than either alone. The paper reports that selection still helps on top of self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Model selection and self-consistency are complementary; ensembling diverse reasoning paths within a method and selecting between different methods yields additive gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8269.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8269.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explanation_Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation of requesting explanations during LLM selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study of whether asking the selection LLM to provide explanations for its A/B choice affects selection success and downstream accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex / ChatGPT / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same backbones used; experiments remove or include explanation generation in the selection prompt/response.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Selection with explanations (ask the LLM to justify choice)', 'Selection without explanations (only A/B output)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>In prompts the model is asked both to pick which choice (A or B) is correct and to explain why the other is incorrect; ablation removes the explanation request and measures selection success and final task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (this ablation is about selection process rather than base reasoning style)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compare selection performance and final accuracies on GSM8K when selection LLM produces explanations vs when it does not.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (reported ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>For stronger in-context learners (Codex, GPT-4) including explanations has negligible effect on selection performance and final accuracy. For ChatGPT (weaker in-context learning), including explanations increases the selection success rate and yields larger accuracy improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Explanations help selection when the backbone has weaker in-context capabilities, likely by forcing more deliberate comparison; for strong LLMs, explanations add little because they already select well.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Prompting the selector to provide explanations improves selection for weaker LLMs (e.g., ChatGPT) but is unnecessary for stronger LLMs (e.g., GPT-4, Codex).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8269.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8269.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theoretical_Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theoretical analysis of model selection improvement conditions (Proposition 1, Theorem 1/2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Formal decomposition showing how overall error of a model-selection scheme depends on per-instance accuracy differences between methods (R(x)) and the per-instance probability of selecting the better method (ρ_x); constructs examples where selection helps even with low overall selection probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>theory (no specific LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Analytic results (Proposition 1 and Theorem 1/2) quantifying when and why combining methods via selection can reduce error relative to best base method.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Analytic decomposition of expected error under selection', 'Constructive existence proofs showing improvement possible with ρ < 0.5']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Proposition 1: err = err_1 - E_x[ |R(x)| (ρ_x - 1{R(x) < 0}) ], linking improvement to magnitude of per-instance differences and selection quality. Theorem 1/2: explicit constructions where err < err_1 even when the overall selection success probability ρ is arbitrarily small, emphasizing the role of distribution of R(x) and varying ρ_x.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (provides formal justification for benefits of combining diverse methods and explains limits when methods are similar)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Theory used to motivate experimental choices (prefer dissimilar base methods like CoT and PAL) and to explain observed robustness to imperfect selection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Theory applies generally; experiments validate on datasets like GSM8K/SVAMP etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Theorems do not give numeric benchmark accuracies but show that improvement depends on |R(x)| and ρ_x; they support empirical findings that selection between dissimilar methods (CoT vs PAL) yields gains and that imperfect selectors can still improve overall accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Key insight: choose base methods that behave differently per-instance (large |R(x)|) so that an imperfect yet targeted selector can yield benefit; combining too-similar methods gives small |R(x)| and little room for selection to help.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Performance improvement via model selection is driven by per-instance method differences and selection accuracy; it's both theoretically possible and empirically observed for CoT vs PAL combinations, and theory explains why combining similar methods is unlikely to help.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Program-Aided Language Models (PAL) <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>ComplexCoT: Prompt Engineering for Multi-step Reasoning <em>(Rating: 1)</em></li>
                <li>GSM8K: A Dataset for Math Word Problems <em>(Rating: 2)</em></li>
                <li>SVAMP: A Supervised Dataset for Math Word Problems (SVAMP) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8269",
    "paper_id": "paper-258840882",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "ModelSelection-CoT-PAL",
            "name_full": "LLM-based model selection to choose between Chain-of-Thought and Program-Aided Language Models",
            "brief_description": "This paper's principal method: use a large language model (few-shot in-context) to select between two distinct reasoning modalities — natural-language Chain-of-Thought (CoT) and programmatic Program-Aided Language (PAL) — by showing the LLM both solutions (labeled A/B) and asking it to pick which is correct (with an explanation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex / ChatGPT / GPT-4 (backbones used interchangeably)",
            "model_description": "Off-the-shelf large language models (OpenAI code-davinci-002 Codex, gpt-3.5-turbo ChatGPT, and GPT-4) used both to generate CoT/PAL solutions and to perform few-shot selection; same LLM is used for generation and selection in experiments.",
            "reasoning_methods": [
                "Chain-of-Thought (CoT) — natural language multi-step reasoning",
                "Program-Aided Language Model (PAL) — Python program representation of solution",
                "LLM-based model selection with few-shot examples and explanation",
                "Optional integration with self-consistency (majority voting across sampled outputs)"
            ],
            "reasoning_methods_description": "CoT: prompt an LLM to generate intermediate natural-language reasoning steps before final answer. PAL: prompt an LLM to produce a Python function whose execution yields the answer (computational correctness). Model selection: present pairs A/B (PAL vs CoT) plus few-shot examples (8-shot for Codex, 5-shot for ChatGPT/GPT-4 in arithmetic tasks; 6-shot for date tasks), ask the LLM to choose (A or B) and provide an explanation; if selection output is ambiguous (e.g. 'Both'/'Neither') the implementation chooses randomly. For generation/selection temperatures: greedy (temp=0) for selection; generation uses lower/higher temps for CoT/PAL and for self-consistency sampling.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Directly compares combining two distinct reasoning methods (CoT vs PAL) via an LLM selector; used few-shot selection prompts showing paired reasoning chains and asking the LLM to pick correct one. Also compares against combining similar methods (CoT variants) and ablates use of selection explanations and self-consistency. Experiments run with three backbones across eight reasoning datasets.",
            "task_or_benchmark": "Eight reasoning datasets spanning arithmetic and symbolic reasoning: GSM8K, SVAMP, ASDIV, SingleOP, SingleEQ, AddSub, MultiArith (arithmetic) and Date Understanding (symbolic).",
            "performance_results": "Combining CoT and PAL via LLM selection yields consistent improvements across datasets and backbones. Highlights: new SOTA on GSM8K (96.5% accuracy) and SVAMP (93.7%) reported (with GPT-4 + model selection + self-consistency). Without self-consistency GPT-4 + selection: GSM8K 95.6%, SVAMP 93.7% (reported). Selection success rate example: GPT-4 achieves 72.6% correct selection on GSM8K. Paper reports a 3.2% improvement over PAL's 71.5% on GSM8K in a setting where CoT alone was 64.4% (text reports the 3.2% improvement but does not name the exact combined numeric in that sentence). ChatGPT shows smaller gains (e.g., with SC@10 combined selection reached 88.9% on GSM8K), and Codex/GPT-4 show larger improvements.",
            "qualitative_findings": "LLMs can reliably spot differences between CoT and PAL chains and select the correct solving method; combining dissimilar methods produces larger per-question accuracy differences (|R(x)|) making selection more beneficial. Explanation prompts help weaker in-context learners (ChatGPT) more than strong ones (GPT-4, Codex). Model selection is robust to imperfect selection accuracy — theory and experiments show improvement even when overall selection probability ρ can be &lt; 0.5 in some constructions. Combining CoT and PAL extracts complementary strengths: CoT's generality and interpretability vs PAL's computational accuracy.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Combining diverse reasoning methods (CoT and PAL) via an LLM-based model selector yields consistent performance gains; the magnitude of improvement depends jointly on (1) how dissimilar the methods are per instance (|R(x)|) and (2) the selection success probability (ρ_x). Even modest or imperfect selection can produce net gains if methods are sufficiently different.",
            "uuid": "e8269.0"
        },
        {
            "name_short": "CoT_similar_combo",
            "name_full": "Selection between similar Chain-of-Thought variants (CoT, CoT' and ComplexCoT)",
            "brief_description": "An ablation testing whether selecting between two similar CoT-derived methods (temperature or prompt-variant CoT) yields performance gains compared to selecting between CoT and PAL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT / GPT-4 / Codex (used in these ablations)",
            "model_description": "Same LLM backbones used to produce CoT variants by changing temperature (CoT') or using more complex prompt examples (ComplexCoT).",
            "reasoning_methods": [
                "Chain-of-Thought (CoT) baseline",
                "CoT' (lower temperature variant)",
                "ComplexCoT (more complex prompt examples)"
            ],
            "reasoning_methods_description": "Generate CoT reasoning chains with slight variations: modify sampling temperature (CoT') or modify prompt complexity (ComplexCoT). Then perform same LLM-based pairwise selection between two CoT-style solutions to see if selection improves performance.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Compare model selection between CoT vs CoT' and CoT vs ComplexCoT (both similar natural-language reasoning styles) and contrast with CoT vs PAL (diverse). Evaluate on GSM8K.",
            "task_or_benchmark": "GSM8K (primary reported ablation dataset)",
            "performance_results": "Selection between similar CoT variants (CoT-CoT' or CoT-ComplexCoT) did not yield substantial performance gains, despite those variants having accuracy comparable to PAL in some cases. The ΔUpperBound (theoretical room for improvement) for CoT-PAL exceeds that for CoT-CoT' and CCoT-CoT, and empirical success-selection rates were lower for similar-method combinations.",
            "qualitative_findings": "When base methods are similar, per-question differences |R(x)| are small and the LLM selector has less signal to pick the better method, so selection provides little benefit; dissimilar methods (CoT vs PAL) produce clearer differences and higher selection success.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Combining similar reasoning approaches yields little improvement; the benefit of model selection primarily arises when the combined methods are behaviorally dissimilar on instances.",
            "uuid": "e8269.1"
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (majority voting on multiple sampled reasoning paths)",
            "brief_description": "Ensembling method that samples multiple reasoning paths/outputs from an LLM and selects the majority answer (used alone and in combination with LLM selection).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Codex / ChatGPT / GPT-4",
            "model_description": "LLMs sampled multiple times under nonzero temperature to produce diverse reasoning chains; majority voting (self-consistency) applied to outputs.",
            "reasoning_methods": [
                "Self-consistency: sample multiple CoT/PAL outputs and take majority answer",
                "Combined with model selection: generate multiple samples per method and then apply selection + majority voting"
            ],
            "reasoning_methods_description": "Self-consistency sampling: run CoT with temperature (e.g., 0.5) and PAL with its own temperature (e.g., 0.8) to produce many candidate reasoning traces; perform majority vote over final answers (SC@5, SC@10 denote 5 or 10 samples). In combined experiments, selection was applied in each sampled run and majority voting used to produce final answer.",
            "reasoning_diversity": "both (diverse paths within a single method, and diversity across methods when paired with selection)",
            "reasoning_diversity_experimental_setup": "Run multiple sampled traces per base method (5 or 10), use majority voting (self-consistency) and compare self-consistency alone vs self-consistency + model selection.",
            "task_or_benchmark": "GSM8K (main reported), other arithmetic datasets",
            "performance_results": "Self-consistency alone yields strong gains; combining self-consistency with model selection gives further improvements. Example: GPT-4 with model selection + self-consistency reaches GSM8K 96.5% (SOTA). Using 5 vs 10 samples produced similar results with GPT-4 in the paper; ChatGPT + selection + SC@10 reached 88.9% on GSM8K.",
            "qualitative_findings": "Self-consistency (diverse sampling) complements model selection: the two techniques are additive and produce higher final accuracy than either alone. The paper reports that selection still helps on top of self-consistency.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Model selection and self-consistency are complementary; ensembling diverse reasoning paths within a method and selecting between different methods yields additive gains.",
            "uuid": "e8269.2"
        },
        {
            "name_short": "Explanation_Ablation",
            "name_full": "Ablation of requesting explanations during LLM selection",
            "brief_description": "Study of whether asking the selection LLM to provide explanations for its A/B choice affects selection success and downstream accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex / ChatGPT / GPT-4",
            "model_description": "Same backbones used; experiments remove or include explanation generation in the selection prompt/response.",
            "reasoning_methods": [
                "Selection with explanations (ask the LLM to justify choice)",
                "Selection without explanations (only A/B output)"
            ],
            "reasoning_methods_description": "In prompts the model is asked both to pick which choice (A or B) is correct and to explain why the other is incorrect; ablation removes the explanation request and measures selection success and final task accuracy.",
            "reasoning_diversity": "both (this ablation is about selection process rather than base reasoning style)",
            "reasoning_diversity_experimental_setup": "Compare selection performance and final accuracies on GSM8K when selection LLM produces explanations vs when it does not.",
            "task_or_benchmark": "GSM8K (reported ablation)",
            "performance_results": "For stronger in-context learners (Codex, GPT-4) including explanations has negligible effect on selection performance and final accuracy. For ChatGPT (weaker in-context learning), including explanations increases the selection success rate and yields larger accuracy improvements.",
            "qualitative_findings": "Explanations help selection when the backbone has weaker in-context capabilities, likely by forcing more deliberate comparison; for strong LLMs, explanations add little because they already select well.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Prompting the selector to provide explanations improves selection for weaker LLMs (e.g., ChatGPT) but is unnecessary for stronger LLMs (e.g., GPT-4, Codex).",
            "uuid": "e8269.3"
        },
        {
            "name_short": "Theoretical_Framework",
            "name_full": "Theoretical analysis of model selection improvement conditions (Proposition 1, Theorem 1/2)",
            "brief_description": "Formal decomposition showing how overall error of a model-selection scheme depends on per-instance accuracy differences between methods (R(x)) and the per-instance probability of selecting the better method (ρ_x); constructs examples where selection helps even with low overall selection probability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "theory (no specific LLM)",
            "model_description": "Analytic results (Proposition 1 and Theorem 1/2) quantifying when and why combining methods via selection can reduce error relative to best base method.",
            "reasoning_methods": [
                "Analytic decomposition of expected error under selection",
                "Constructive existence proofs showing improvement possible with ρ &lt; 0.5"
            ],
            "reasoning_methods_description": "Proposition 1: err = err_1 - E_x[ |R(x)| (ρ_x - 1{R(x) &lt; 0}) ], linking improvement to magnitude of per-instance differences and selection quality. Theorem 1/2: explicit constructions where err &lt; err_1 even when the overall selection success probability ρ is arbitrarily small, emphasizing the role of distribution of R(x) and varying ρ_x.",
            "reasoning_diversity": "both (provides formal justification for benefits of combining diverse methods and explains limits when methods are similar)",
            "reasoning_diversity_experimental_setup": "Theory used to motivate experimental choices (prefer dissimilar base methods like CoT and PAL) and to explain observed robustness to imperfect selection.",
            "task_or_benchmark": "Theory applies generally; experiments validate on datasets like GSM8K/SVAMP etc.",
            "performance_results": "Theorems do not give numeric benchmark accuracies but show that improvement depends on |R(x)| and ρ_x; they support empirical findings that selection between dissimilar methods (CoT vs PAL) yields gains and that imperfect selectors can still improve overall accuracy.",
            "qualitative_findings": "Key insight: choose base methods that behave differently per-instance (large |R(x)|) so that an imperfect yet targeted selector can yield benefit; combining too-similar methods gives small |R(x)| and little room for selection to help.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Performance improvement via model selection is driven by per-instance method differences and selection accuracy; it's both theoretically possible and empirically observed for CoT vs PAL combinations, and theory explains why combining similar methods is unlikely to help.",
            "uuid": "e8269.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Program-Aided Language Models (PAL)",
            "rating": 2,
            "sanitized_title": "programaided_language_models_pal"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "ComplexCoT: Prompt Engineering for Multi-step Reasoning",
            "rating": 1,
            "sanitized_title": "complexcot_prompt_engineering_for_multistep_reasoning"
        },
        {
            "paper_title": "GSM8K: A Dataset for Math Word Problems",
            "rating": 2,
            "sanitized_title": "gsm8k_a_dataset_for_math_word_problems"
        },
        {
            "paper_title": "SVAMP: A Supervised Dataset for Math Word Problems (SVAMP)",
            "rating": 2,
            "sanitized_title": "svamp_a_supervised_dataset_for_math_word_problems_svamp"
        }
    ],
    "cost": 0.0148125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automatic Model Selection with Large Language Models for Reasoning</p>
<p>Xu Zhao xu.zhao@u.nus.edu 
National University of Singapore</p>
<p>Yuxi Xie 
National University of Singapore</p>
<p>Kenji Kawaguchi 
National University of Singapore</p>
<p>Junxian He 
Shanghai Jiao Tong University</p>
<p>Qizhe Xie 
National University of Singapore</p>
<p>Automatic Model Selection with Large Language Models for Reasoning</p>
<p>Chain-of-Thought and Program-Aided Language Models represent two distinct reasoning methods, each with its own strengths and weaknesses. We demonstrate that it is possible to combine the best of both worlds by using different models for different problems, employing a large language model (LLM) to perform model selection. Through a theoretical analysis, we discover that the performance improvement is determined by the differences between the combined methods and the success rate of choosing the correct model. On eight reasoning datasets, our proposed approach shows significant improvements. Furthermore, we achieve new state-of-the-art results on GSM8K and SVAMP with accuracies of 96.5% and 93.7%, respectively. Our code is publicly available at https://github.com/XuZhao0/Model-Selection-Reasoning.Leo Breiman. 2001. Random forests. Machine learning, 45:5-32.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have made impressive progresses in numerous fields (Devlin et al., 2019;Brown et al., 2020;OpenAI, 2023;Chowdhery et al., 2022;Bubeck et al., 2023;Wei et al., 2022a) and are often powerful enough to solve problems through a single unified method. While convenient, this approach tends to ignore the distinct structures and variations among the problems, which would benefit from using different methods. On the other hand, in human society, individuals with different skill sets excel in various roles, leading to a thriving world as a whole.</p>
<p>In the case of reasoning, Chain-of-Thought (CoT) (Wei et al., 2022b) and Program-Aided Language Models (PAL) (Gao et al., 2022) have emerged as two effective methods that offer different strengths and weaknesses. Essentially, CoT decomposes a reasoning problem into a series of intermediate steps using natural language. In contrast, PAL represents the solution to a reasoning problem through a Python function. CoT, which relies on natural language, is more general and offers clearer explanations, reducing ambiguity. Meanwhile, PAL, employing a programming language, enjoys guaranteed computation correctness. Intuitively, choosing between CoT and PAL based on their reasoning results to a specific problem would prove beneficial. However, without access to the ground-truth, choosing a better method itself becomes a machine learning problem.</p>
<p>In order to select among multiple results, previous studies have suggested training a ranker (Uesato et al., 2022). While training a dedicated model generally results in improved accuracy, it can also be somewhat cumbersome and entail significant costs. Conversely, large language models (LLMs) have demonstrated good calibration and have been used to assess the accuracy of their own outputs (Guo et al., 2017;Shinn et al., 2023;Xie et al., 2023). Therefore, in this study, we propose utilizing LLMs to perform model selection with few-shot learning. Additionally, we ask LLMs to provide explanations for their choices, ensuring they consider deliberate comparisons in the process.</p>
<p>Through a theoretical analysis, we discover that the effectiveness of the proposed method is mainly influenced by two factors: (1) the significance of the difference between two models, and (2) the probability of selecting the correct model. More specifically, we can attain a higher overall performance when there is a substantial difference between the models being considered and when the correct model is more likely to be chosen. However, we also observe that even without an exceptional model selection, we can still achieve improvement in certain cases. This validates our decision to simply employ an LLM for the purpose of model selection.</p>
<p>We evaluate the proposed method on eight reasoning tasks, with CoT and PAL serving as Figure 1: We propose to perform model selection to combine two distinct methods, CoT and PAL . The figure illustrates an example where PAL makes mistakes about crucial information and therefore fails to answer the question correctly. In contrast, CoT manages to correctly answer the same question. Our selection model successfully chooses the correct solution and provides a brief explanation to support its choice. the baseline methods. On these eight reasoning benchmarks, our method demonstrates substantial performance improvements when employing Codex, ChatGPT, and GPT-4 as the backbone LLMs. Moreover, our approach attains new state-of-the-art accuracies of 96.5% and 93.7% on GSM8K (Cobbe et al., 2021) and SVAMP (Patel et al., 2021), respectively.</p>
<p>Automatic Model Selection with Large</p>
<p>Language Models</p>
<p>Method</p>
<p>In this study, we examine reasoning tasks using two baseline models: CoT and PAL. To tackle complex reasoning tasks, CoT employs an LLM to generate several intermediate reasoning steps before arriving at a final conclusion. This approach has proven to be highly effective for various tasks. Due to its use of natural language, the reasoning steps are clearly explained, resulting in less ambiguity. Furthermore, natural language supports a broad range of reasoning that may involve common sense and confidence of the reasoning steps. In contrast, PAL breaks down a reasoning problem into computations using Python. Naturally, employing Python ensures a high level of computational accuracy. However, similar to how Python code can be challenging to comprehend, this approach also introduces ambiguity regarding the meaning of the program.</p>
<p>Considering the unique strengths and weaknesses of these two approaches, it would be advantageous to select between the two methods based on the problem at hand. Furthermore, model selection could be more precise if we incorporate the reasoning chains from CoT and PAL, as the model could potentially identify errors, gaps, or ambiguities within the chains and determine the correct one.</p>
<p>We employ an LLM to make the model selection. Specifically, we present the LLM with a few examples of choosing between PAL or CoT based on their reasoning chains. In all the included examples, either CoT or PAL is correct, i.e., we do not include instances where both are correct or both are incorrect. In our prompt, we present the choices as two options: (A) or (B), with the results of (A) and (B) being generated by PAL and CoT, respectively. By providing a few model selection examples in the context, an LLM learns to output either (A) or (B) to make a decision. However, there are instances where the model fails to generate (A) or (B); in these cases, we randomly choose a method.</p>
<p>Owing to the in-context learning capabilities of LLMs, we find that they exhibit reasonable accuracy in selecting the appropriate method. Additionally, as shown in our theoretical analysis, a highly accurate method selection is not actually necessary for the algorithm to perform well. We discover that the performance improvement is jointly determined by the significance of the difference between the two combined methods and the effectiveness of the model selection. Furthermore, there are situations where the model selection might be poor, yet the overall improvement remains substantial. In addition to requesting that the model selection identifies the correct method, we also ask it to provide explanations, which proves to be effective in our experiments. The proposed algorithm is illustrated in Figure 1.</p>
<p>Theoretical Analysis</p>
<p>In this section, we conduct a theoretical analysis to determine under which condition the proposed method could work (and fail).</p>
<p>Quantifying error rates Let us denote the error rates of the two base methods, m 1 and m 2 , by err 1 and err 2 , respectively. Without loss of generality, let m 1 be a better base method in the overall performance: i.e., err 1 ≤ err 2 . For a given question x, we define ρ x as the probability of choosing a more accurate method, either m 1 or m 2 , for the given x using the proposed approach. Define
R(x) = p(correct | x, m 2 ) − p(correct | x, m 1 )
where p(correct | x, m i ) represents the probability of outputting a correct prediction given input x with method m i . Then we can quantify the final error rate err of the proposed method as follows: Proposition 1. For any methods m 1 and m 2 with any combining probability function ρ x ,
err = err 1 − E x [|R(x)| (ρ x − 1{R(x) &lt; 0})]
We refer readers to Appendix B for the full proof. Proposition 1 decomposes the possible improvement (or deterioration) over base methods in terms of R(x) and ρ x . It quantitatively shows when and how we can expect improvement (or deterioration) based on these two factors. For example, to improve over the best base method m 1 , Proposition 1 suggests us to choose another base method m 2 such that |R(x)| is not too small and ρ x is high when |R(x)| is large. In other words, it discourages us from choosing too similar methods as base methods, because for similar methods, |R(x)| tends to be small and it is challenging to increase ρ x even when |R(x)| is large due to the similarity. This provides a theoretical motivation for us to use CoT and PAL, instead of combining CoT with another CoT.</p>
<p>On the accuracy of selection Define ρ to be the overall probability of selecting a better method:
ρ = E x [ρ x ].
Theorem 1 shows that ρ can be much worse than that of a random guess to achieve the improvement over the base methods m 1 and m 2 ; i.e., err &lt; err 1 and err 1 ≤ err 2 can happen with ρ &lt; 0.5: Theorem 1. For any ϵ &gt; 0, there exist data distribution over x, two base methods (m 1 , m 2 ), and combining probability (ρ x ) such that err &lt; err 1 , err 1 ≤ err 2 , and ρ &lt; ϵ.</p>
<p>We provide a stronger version of Theorem 1 in Appendix A (that implies Theorem 1) and its proof in Appendix B.</p>
<p>Theorem 1 supports our proposition that, despite not training a new model for the selection process and with the in-context learning limited to a fewshot prompt, it is possible to achieve improvement, even if we do not achieve ρ x &gt; 0.5 in some instances. This theoretical analysis offers deeper insights into the conditions and strategies for effective performance improvement with the proposed methodology.</p>
<p>Experiments</p>
<p>Setup</p>
<p>Datasets and backbones We conduct experiments on eight datasets that span a range of arithmetic and symbolic reasoning tasks. 7 of these datasets, including GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), ASDIV (Miao et al., 2020), Sin-gleOP, SingleEQ, AddSub and MultiArith (Koncel-Kedziorski et al., 2016), are about arithmetic reasoning, while Date Understanding (Srivastava et al., 2022) focuses on symbolic reasoning. To comprehensively evaluate the effectiveness of our approach, we employ three LLMs as backbone systems: Codex (code-davinci-002), ChatGPT (gpt-3.5-turbo) and . In our experiments, we always use the same LLM for the two base models and the model selection LLM. 1 .</p>
<p>Prompt design To effectively exploit the incontext learning abilities of the LLMs, we create a set of few-shot examples by manually creating an error in one model's reasoning chain. For arithmetic reasoning tasks, we employ an 8-shot example for Codex and 5-shot example for ChatGPT and GPT-4. For date understanding task, we use 6-shot  for all backbones. The used prompts can be found in Appendix C.</p>
<p>Hyperparameters In the solution generation and selection process, we set the temperature at 0 for greedy decoding. Some experiments also use self-consistency (Wang et al., 2022b) to further improve performance. In this context, we set the temperature for CoT at 0.5 and PAL at 0.8 during answer generation. For model selection, the temperature is set to 0 so that we always select the method that has a higher probability of being correct. However, it is worth noting that ChatGPT and GPT-4 do not always follow the few-shot instructions. As such, when these models output "Both" or "Neither" to a task, we randomly select one of the two choices as the final result for simplicity. In our initial experiments, we discovered that our method is highly robust with respect to hyperparameters. For instance, simply having the model selection LLM output the selection result leads to significant improvements, and there is no need to fine-tune the threshold for the selection probability.</p>
<p>Main Results</p>
<p>The results of our experiments with greedy decoding are shown in Table 1. First, we find that our proposed method effectively and robustly enhances performance on most settings with different datasets of different difficulties and different backbone LLMs, simply by combining two base models. For example, with GPT-4, we achieve an accuracy of 95.6% on GSM8K and 93.7% on SVAMP without self-consistency.</p>
<p>Second, our results show a considerable improvement even when one of the base models performs much worse than the other. For instance, we observe a significant 3.2% improvement over PAL's 71.5% accuracy on GSM8K, even though CoT has a lower accuracy of 64.4%. Third, our model's general applicability is further underscored by its 1.9% improvement on the symbolic date understanding task when we use Codex, illustrating its applicability not only to mathematical reasoning but also to other reasoning tasks. In fact, on this task, the accuracy difference between two base models is as large as 13%, our proposed method still improves the accuracy from 77.5% to 79.4%.</p>
<p>It is worth noting that the improvement on Codex and GPT-4 is larger than the improvement on Chat-GPT. As we will show in the analysis, this is due to ChatGPT has limited in-context learning capabilities and cannot choose the correct method based on the presented reasoning chains.</p>
<p>Experiments with self-consistency We aim to investigate the relationship between selfconsistency and model selection and whether they complement each other. To do this, we run our algorithm 5 or 10 times and use self-consistency to perform majority voting, arriving at the final answer.  Table 3: We define the following terms: ∆ UpperBound = Acc Upper Bound − Acc m1 , where Acc Upper Bound is the upper bound accuracy where we assume a perfect model selection and m 1 is the stronger one of the two base models. ∆ UpperBound reflects the expected performance difference between the two base models. Success Rate calculates the correct selection rates when either CoT is correct or PAL is correct, i.e., we ignore the cases where both methods are either correct or wrong. Improvement is the performance improvement achieved over the performance of the stronger base model. Table 2, we can achieve substantial improvements over self-consistency alone. With GPT-4, even though both base models already score around 95%, combining them leads to a new state-of-the-art performance on GSM8K at 96.5%. The performance of using 5 samples and 10 samples turn out to be the same with GPT-4. Additionally, using ChatGPT, we attain an accuracy of 88.9% when sampling 10 times.</p>
<p>As demonstrated in</p>
<p>Influencing Factors</p>
<p>We have shown that our model performs well across different datasets. To better understand the reasons for the performance improvement on different datasets and backbone LLMs, we present the performance improvement, and the associated influencing factors in Table 3. We can find that there is a high expected performance difference between the two base methods, reflected in the high ∆ Upper Bound , which means how different the two base models behave across questions. A larger ∆ Upper Bound indicates a larger room for potential improvement. Specifically, notice that on GSM8K, with ChatGPT, ∆ Upper Bound is 8.6% although the accuracy of CoT and PAL are very similar (80.8% and 79.2% respectively). Similarly, with GPT-4, ∆ Upper Bound is 2.5% while both the accuracy of the two base models are close (94.6% vs 94.0%).</p>
<p>In addition, we find that the success rate of model selection is relatively high with Codex and GPT-4. For example, with GPT-4, the success rate is 72.6% on GSM8K. In contrast, ChatGPT suffers on the success rate, which explains the relatively weaker performance improvement on ChatGPT.</p>
<p>This echoes with the theoretical analysis shown in Section 2.2. Essentially, to achieve better performance, we need a different behavior of model results, i.e., a high |R(x)| and a high success rate ρ, which jointly contribute more substantial improvements. Indeed, the study on ∆ Upper Bound and success rate explains the significant performance improvement on these datasets.</p>
<p>Analysis</p>
<p>In this section, we provide a few analysis to see when and how the method works.</p>
<p>Combination between Similar Methods</p>
<p>Backbone</p>
<p>ChatGPT  We choose CoT and PAL as our two base models due to the motivation of combining different strengths of distinct models. We conduct experiments to examine whether the performance improves when we combine two similar base models. We use two variants of CoT in this experiment: CoT ′ where we set the temperature at 0.1, ComplexCoT (Fu et al., 2022) where we use more complex examples in the prompt. Both of these methods' accuracies are similar or higher than the accuracy of PAL. The results are shown in Table 4. We have the following observations:
GPT-4 m1 CoT CoT m2 PAL CoT ′ PAL CCoT
• In our experiments, we found that model selection between CoT and CoT ′ , or CoT and ComplexCoT, does not lead to substantial performance gains, even though the accuracy of CoT ′ and ComplexCoT is on par with PAL.</p>
<p>On the other hand, model selection between CoT and PAL results in consistent performance improvements. To understand the reasons behind these outcomes, we further investigate the ∆ Upper Bound and the success selection rate.</p>
<p>• ∆ Upper Bound of CoT-PAL exceeds that of other combinations, CoT-CoT ′ and ComplexCoT-CoT, despite their employing two stronger or equivalent two base models. This observation suggests a larger absolute value of the accuracy difference per question for CoT-PAL. It indicates that CoT and PAL perform more dissimilarly than other model combinations. Theoretically, it represents a larger |R(x)|. As Proposition 1 highlights, without a substantial |R(x)|, it is unlikely to achieve significant performance gain since the improvement component is factored by |R(x)|.</p>
<p>• The success selection rate of CoT-PAL surpasses that of other model combinations. It means that the selection model is more likely to select the correct choice when one solution derives from CoT and the other from PAL. In theory, this higher success rate implies that when |R(x)| is high for a given question x, the success selection probability ρ x for CoT-PAL is higher than others.</p>
<p>These findings support our initial motivation and hypothesis. We choose CoT and PAL as our two base models because they represent distinct reasoning approaches using natural language and programming language. We expect these models to exhibit a significant difference in errors and accuracies, indicated by a high ∆ Upper Bound . Moreover, the considerable disparity in errors for a particular question makes it easier for large language models (LLMs) to select the correct option, leading to a  higher success rate compared to selecting between two similar base models like CoT-CoT ′ . This holds true even when different prompts or temperature settings are used.</p>
<p>Ablation Study on Explanation</p>
<p>To perform model selection, we provide explanations in the prompt and also ask the model to generate explanations after making a choice, as we expect to improve the model's selection ability by pointing out why the other choice is incorrect. To investigate the potential role of explanations in enhancing the model's selection capability, we conduct experiments on GSM8K by excluding explanations from the answer.</p>
<p>The results shown in Table 5 reveal that for backbone LLMs with more powerful in-context learning abilities, such as Codex and GPT-4, the inclusion of explanations within the answer seems to have negligible influence on the model's performance. In other words, for Codex and GPT-4, they have a relatively high success selection rate even when no explanation is given. However, for ChatGPT, which possesses relatively low in-context learning capability, the integration of explanations yields a larger improvement in the model's success selection rate. This observation suggests that for backbones with limited in-context learning abilities, explanations can serve as a valuable tool to improve the success selection rate.</p>
<p>Qualitative Studies</p>
<p>We are also interested in gaining an intuitive understanding of the algorithm, so we conduct some qualitative analysis. As displayed in Table 6, in the first example, both ChatGPT and GPT-4 can accurately identify that PAL is the correct solution. However, ChatGPT provides an incorrect Question: Courtney attended a concert and reported that the audience was 48 in number. However, Kelly went to the same concert and said that Courtney had made the mistake of overstating the number of people in attendance by 20%. If Kelly was right, how many people really attended the concert?</p>
<p>(A) Answer: Courtney reported 48 people in attendance. If this number is overstated by 20%, then we need to find the actual number of people. First, we need to find 20% of 48. 20% of 48 is (20/100) * 48 = 9.6. Now, we subtract this number from Courtney's reported number to find the actual number of people. 48 -9.6 = 38.4. Since we cannot have a fraction of a person, we round the number to the nearest whole number. So, the actual number of people in attendance was approximately 38. So the answer is 38. Question: How much does it cost you for lunch today at Subway if you pay $40 for a foot-long fish sub and thrice as much for a six-inch cold-cut combo sub?</p>
<p>(A)</p>
<p>Answer: If the foot-long fish sub costs $40, then the six-inch cold-cut combo sub costs 3 * $40 = $120. So the total cost for both subs is $40 + $120 = $160. Therefore, it costs $160 for lunch today at Subway. ChatGPT: (A) can correctly answer the math problem. Because (B) calculates the cost of the six-inch cold-cut combo sub as one-third of the cost of the foot-long fish sub instead of three times the cost of the foot-long fish sub.</p>
<p>GPT-4: (A) can correctly answer the math problem. Because (B) incorrectly calculates the cost of the six-inch cold-cut combo sub as one-third of the foot-long fish sub instead of thrice as much.</p>
<p>Table 6: In the first example, ChatGPT performs model selection correctly, but gives a wrong explanation, while GPT-4 gives a correct explanation. The second example shows where both ChatGPT and GPT-4 select correctly and give the correct explanation. explanation, while GPT-4 offers a valid one. In the second example, which is relatively simpler, both ChatGPT and GPT-4 successfully perform accurate model selection and provide valid explanations. In the first example, we can see that GPT-4 actually possesses exceptional reasoning capabilities and provide reliable explanations.</p>
<p>Related Work</p>
<p>Ensemble learning. In machine learning, the strategy of combining various models to address a single problem is exemplified in techniques such as bagging (Breiman, 1996), boosting (Freund and Schapire, 1997;Chen and Guestrin, 2016;Ke et al., 2017), and random forest (Ho, 1995;Breiman, 2001). The underlying idea in these methods is that a group of weak learners can collectively manifest as a strong learner. This concept has also found its place in deep learning through the use of ensembles. For reasoning, Self-consistency samples diverse reasoning paths and chooses the most consistent answer through majority voting, which can be regarded as ensembling (Wang et al., 2022b). Wang et al. (2022a) takes it a step further by introducing rationale-augmented ensembles, emphasizing rationale sampling in the output space as a critical component for enhancing performance robustly. However, typically, ensembling places equal weights on models through majority voting, which may restrict the full potential of the diverse strengths that each model offers.</p>
<p>Reasoning. The research community has made tremendous progress in the field of reasoning.  2022) simplifies complex problems by breaking them down into a series of subproblems solved in sequence. It has also been shown that prompts with higher complexity lead to superior performance on multi-step reasoning tasks (Fu et al., 2022). Kojima et al. (2022) shows that by simply adding "Let's think step by step" before each answer, LLMs can be competent zero-shot reasoners. Selection-Inference (Creswell et al., 2022) alternates between selection and inference stages, generating causal reasoning steps to the final answer. Kazemi et al. (2022) proposes a backward chaining algorithm that breaks reasoning down into four sub-models. REFINER (Paul et al., 2023) generates intermediate reasoning steps under the scrutiny of a critic model. Xie et al. (2023) delves into the exploration of the reasoning search space and generates reasoning chains by a self-evaluation guided stochastic beam search. Yao et al. (2023) introduces Tree of Thoughts (ToT), enabling exploration over "thoughts". It is important to mention that the contributions of these methods are distinct from our approach, and the progress made by them could potentially be seamlessly integrated using the method we propose.</p>
<p>Self-Evaluation. LLM calibration studies reveal that the probabilistic predictions made by current LLMs closely align with the actual frequencies of token occurrences, hence producing well-calibrated predictions for certain tasks (Guo et al., 2017;Kadavath et al., 2022;Jiang et al., 2020). As LLMs exhibit reliable calibration, there is a growing number of research emphasizing the use of self-evaluation for verification. Shinn et al. (2023) proposes an approach to provide an agent with dynamic memory and self-reflection capabilities. Madaan et al. (2023) proposes a method to generate outputs from LLMs and refine its previously generated output given its own feedback. Different from these works where the underlying method is the same, in this work, we are interested in combining systems with different strengths and weaknesses.</p>
<p>Conclusion</p>
<p>This study introduces a method that effectively combines two distinct base models, CoT and PAL, by enabling an LLM to make the correct selection. We provide a theoretical analysis that supports the feasibility of such a model combination, which is further validated through empirical results. Our method achieves performance improvements across eight datasets with various backbone LLMs. This research represents a significant step towards tapping into the potential of diversity and collaboration among models in LLMs.</p>
<p>In our future work, we aim to expand this framework to other domains. Another intriguing concept involves exploring the use of diverse system instructions to elicit varying model behaviors for model combinations. For example, we could prompt a model to provide reasoning steps in as much detail as possible or as succinctly as possible.</p>
<p>Limitation</p>
<p>• We focus on reasoning tasks in this work, but we posit that exploring model selection in other domains also presents a worthwhile direction for future research.</p>
<p>• As our approach solely relies on in-context learning capabilities of LLMs, it is sensitive to the prompts, which is a common issue with in-context learning.</p>
<p>• Our approach combines different models through a selection mechanism, which may not wholly leverage the unique strengths of each model. Future work can explore alternate methodologies for model collaboration.</p>
<p>A A detailed version of Theorem 1</p>
<p>In this appendix, we provide a detailed version of Theorem 1. Whereas Theorem 1 only states the existence of problem instances, Theorem 2 constructs such instances concretely: i.e., Theorem 2 implies Theorem 1. Define µ x [X ] to be the distribution for the expected errors: i.e., an expected error can be written by
E x∼µx[X ],y,f [1[y ̸ = f (x)]]
for some function f . Define S[X ] = {x ∈ X : R(x) &lt; 0}. Let us denote U [X ] as the uniform distribution over X . Given any X , we write n = |X |, T = |S[X ]|, α = T /n. Assume that 1 ≤ T &lt; n.</p>
<p>Theorem 2. Let µ x [X ] = U [X ] and X be given such that |X | &lt; ∞. Let ϵ, δ ∈ (0, 1) and λ ∈ (0, 1] such that β = ϵT n−T ∈ (0, 1) and λ ≥ 1 − β ϵT (n − T − δ). Let R and ρ x be set such that
R(x) = −ϵ for x ∈ S[X ], R(x) = β for x ∈ X \ S[X ], (1/T ) x∈S[X ] ρ x = λ, and (1/(n − T )) x∈X \S[X ] ρ x = ϵ(T /(n − T ))(1 − λ)β −1 + δ/(n − T )
. Then, we have that err &lt; err 1 , err 1 ≤ err 2 , and
ρ = 1 − α + λ[2α − 1] + δ n .
In particular, when α ≥ 0.5, we have ρ → 0 as α → 1 and (δ/(n−T )) → 0 (with λ = 1− β T ϵ (n−T −δ)); when α &lt; 0.5, we have ρ → 0 as α → 0 and (δ/n) → 0 (with λ = 1).</p>
<p>The proof of Theorem 2 is presented in Appendix B. Theorem 2 shows that the overall success probability of the selection process can be much worse than a random guess to achieve the improvement over the base methods m 1 and m 2 ; i.e., err &lt; err 1 and err 1 ≤ err 2 can happen with ρ &lt; 0.5. Indeed, it is possible to have ρ → 0 with the improvement (err &lt; err 1 and err 1 ≤ err 2 ) when the size of X is large: when α ≥ 0.5, we can choose λ = 1 − β T ϵ (n − T − δ) with which err &lt; err 1 , err 1 ≤ err 2 , and ρ → 0 as α → 1 and (δ/(n − T )) → 0. When α &lt; 0.5, we can choose λ = 1 with which err &lt; err 1 , err 1 ≤ err 2 , and ρ → 0 as α → 0 and (δ/n) → 0. This supports our proposition that, despite not training a new model for the selection process and with the in-context learning limited to a few-shot prompt, it is possible to achieve improvement, even if we do not achieve ρ x &gt; 0.5 in some instances.</p>
<p>Theorem 2 also suggests that if the overall performance of two base methods is similar, captured by ϵ, the overall selection process can be weak to achieve some improvement, as long as the success selection probability is relatively high when the two methods have very different expected errors (or accuracies) for a given question. In essence, Theorem 2 suggests a trade-off: we want |R(x)| to be larger when deciding which two base methods m 1 and m 2 to choose, implying that we prefer base methods to perform dissimilarly on X . On the other hand, if two base methods exhibit a substantial expected accuracy difference, then the selection process needs to be stronger to improve the performance (i.e., ρ needs to be larger). However, if the expected accuracy difference between two base methods is relatively small, increasing the power of the selection process is not that necessary to boost performance.</p>
<p>B Proofs</p>
<p>B.1 Proof of Proposition 1</p>
<p>Proof. Define acc = 1 − err and acc i = 1 − err i for i ∈ {1, 2}. Since expected error = E[1[incorrect prediction]] = P (incorrect prediction) = 1 − P (correct prediction), we have that
acc i = p(correct|m i ) = E x [p(correct|x, m i )]
where correct represents the event of the correct prediction. Similarly,
acc = E x 2 i=1 p(m i |x)p(correct|x, m i )
where p(m i |x) represents the probability of selecting method m i given x via the proposed method. Thus,
acc − acc 1 = E x [p(m 1 |x)p(correct|x, m 1 ) + p(m 2 |x)p(correct|x, m 2 ) − p(correct|x, m 1 )] = E x [(p(m 1 |x) − 1)p(correct|x, m 1 ) + p(m 2 |x)p(correct|x, m 2 )] = E x [(p(m 2 |x)p(correct|x, m 2 ) − (1 − p(m 1 |x))p(correct|x, m 1 )]
Since 1 − p(m 1 |x) = p(m 2 |x),
acc − acc 1 = E x [(p(m 2 |x)p(correct|x, m 2 ) − p(m 2 |x)p(correct|x, m 1 )] = E x [(p(m 2 |x)R(x)] .
Here, we notice that
p(m 2 |x) = ρ x if R(x) ≥ 0 1 − ρ x if R(x) &lt; 0 = 1{R(x) ≥ 0}ρ x + 1{R(x) &lt; 0}(1 − ρ x ).
By plugging this into the above equation,
acc − acc 1 = E x [(1{R(x) ≥ 0}ρ x + 1{R(x) &lt; 0}(1 − ρ x ))R(x)] = E x [R(x)1{R(x) ≥ 0}ρ x ] + E x [R(x)1{R(x) &lt; 0}(1 − ρ x )] Since R(x)1{R(x) ≥ 0} = |R(x)|1{R(x) ≥ 0} and R(x)1{R(x) &lt; 0} = −|R(x)|1{R(x) &lt; 0}, we have that acc − acc 1 = E x [|R(x)|1{R(x) ≥ 0}ρ x ] − E x [|R(x)|1{R(x) &lt; 0}(1 − ρ x )] = E x [|R(x)|((1{R(x) ≥ 0} + 1{R(x) &lt; 0})ρ x − 1{R(x) &lt; 0})] Since (1{R(x) ≥ 0} + 1{R(x) &lt; 0}) = 1 for any x, acc − acc 1 = E x [|R(x)|(ρ x − 1{R(x) &lt; 0})].</p>
<p>B.2 Proof of Theorem 2</p>
<p>Proof. We first confirm that R(x) and ρ x define valid probabilities under the condition of this statement. For R(x), since ϵ ∈ (0, 1) and β ∈ (0, 1), it defines valid probabilities for methods m 1 and m 2 . For ρ x , since λ ∈ [0, 1], it also defines valid probabilities for the case of x ∈ S[X ]. For the case of x ∈ X \ S[X ], since ϵ(T /(n − T ))(1 − λ)β −1 + δ/n ≥ 0, we need to show that ϵ(T /(n − T ))(1 − λ)β −1 + δ/n ≤ 1. That is,
ϵ(T /(n − T ))(1 − λ)β −1 + δ/(n − T ) ≤ 1 ⇐⇒ϵT (1 − λ)β −1 ≤ n − T − δ ⇐⇒1 − β(n − T − δ) ϵT ≤ λ,
which is satisfied by the condition on λ that λ ≥ 1 − β ϵT (n − T − δ). Thus, the condition on ρ x defines the valid probabilities for both cases of x ∈ S[X ] and x ∈ X \ S[X ].</p>
<p>We now show that err &lt; err 1 . Invoking Proposition 1,
err = err 1 − E x [|R(x)| (ρ x − 1{R(x) &lt; 0})] .
Thus, we have err &lt; err 1 if E x [|R(x)| (ρ x − 1{R(x) &lt; 0})] &gt; 0. This condition can be rewritten as
E x [|R(x)| (ρ x − 1{R(x) &lt; 0})] &gt; 0 ⇐⇒ 1 n x∈X [|R(x)| (ρ x − 1{R(x) &lt; 0})] &gt; 0 ⇐⇒ x∈X \S[X ] |R(x)|ρ x + x∈S[X ] |R(x)|ρ x &gt; x∈S[X ] |R(x)| ⇐⇒ x∈X \S[X ] |R(x)|ρ x + ϵT λ &gt; ϵT ⇐⇒β x∈X \S[X ] ρ x &gt; ϵT − ϵT λ = ϵT (1 − λ) ⇐⇒ x∈X \S[X ] ρ x &gt; ϵT (1 − λ) β
This is satisfied by the condition on ρ that
(1/(n − T )) x∈X \S[X ] ρ x = ϵ(T /(n − T ))(1 − λ)β −1 + δ/(n − T ) for some δ &gt; 0: i.e., x∈X \S[X ] ρ x = ϵT (1 − λ) β + δ.
Therefore, we have that err &lt; err 1 . We now show that err 1 ≤ err 2 . Similarly to the proof of Proposition 1, we define acc i = 1 − err i for i ∈ {1, 2}. Then, the inequality err 1 ≤ err 2 holds if acc 1 ≥ acc 2 . By using correct to represent the event of the correct prediction, this condition can be rewritten as
acc 1 ≥ acc 2 ⇐⇒ x∈X p(correct|x, m 1 )] ≥ x∈X p(correct|x, m 2 )] ⇐⇒0 ≥ x∈X R(x) = (n − T )β − ϵT ⇐⇒ ϵT n − T ≥ β
This is satisfied by β = ϵT n−T . Thus, we have that err 1 ≤ err 2 . Using these, we now compute the ρ as
ρ = E x∼µx[X ] [ρ x ] = 1 n x∈X \S[X ] ρ x + 1 n x∈S[X ] ρ x = 1 n ϵT (1 − λ) β + δ + αλ = ϵT (1 − λ) βn + αλ + δ n = ϵT (1 − λ)(n − T ) ϵT n + αλ + δ n = (1 − λ) − α(1 − λ) + αλ + δ n = 1 − α + λ[2α − 1] + δ n .
Finally, we prove the asymptotic behavior using this equation. When α &lt; 0.5, by setting λ = 1, we have that
ρ = 1 − α + λ[2α − 1] + δ n = 1 − α + 2α − 1 + δ n = α + δ n → 0
as α → 0 and (δ/n) → 0. When α ≥ 0.5, by setting λ = 1 − β T ϵ (n − T − δ), we have that
ρ = 1 − α + 1 − β T ϵ (n − T − δ) [2α − 1] + δ n = 1 − α + 2α − 1 − [2α − 1] β T ϵ (n − T − δ) + δ n = α − [2α − 1] β T ϵ (n − T − δ) + δ n By defining Q = β T ϵ (n − T − δ), we have ρ = α − [2α − 1]Q + δ n .
Here,
Q = β T ϵ (n − T − δ) = ϵT n − T 1 T ϵ (n − T − δ) = 1 n − T (n − T − δ) = 1 − δ n − T Thus, ρ = α − [2α − 1] 1 − δ n − T + δ n = α − 2α + 1 + δ 2α − 1 n − T + δ n = 1 − α + δ 2α − 1 n − T + 1 n → 0
as α = T /n → 1 and (δ/(n − T )) → 0: e.g., by setting δ = ζ(n − T ) and take ζ → 0, with which (δ/(n − T )) = ζ → 0.</p>
<p>C Prompts</p>
<p>We show examples for model selection prompts used on different tasks with different backbones. We only show a few examples for each case. Full prompts can be found in our code.</p>
<p>Math Problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?</p>
<p>Question: Which of the following two choices can correctly answer the math problem?   There are two choices to the same math problem. One uses natural language to answer the question, while the other uses Python program to answer it. Either of them can correctly answer the math problem. You need to identify which choice can correctly answer the math problem. Here is one example how to do it, Math problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?
(A)
Answer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 * 3 = 15 dollars. So she has 23 -15 = 8 dollars left. So the answer is 8.  Table 9: Two examples of 5-shot model selection prompts used on 7 arithmetic datasets with ChatGPT.</p>
<p>There are two choices to the same date understanding problem. One uses natural language to answer the question, while the other uses Python program to answer it. Either of them can correctly answer the date understanding problem. You need to identify which choice can correctly answer the problem. Here is one example how to do it, Date Understanding Problem: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?  There are two choices to the same math problem. One uses natural language to answer the question, while the other uses Python code to answer it. Either of them can correctly answer the math problem. You need to identify which choice can correctly answer the math problem. Here is one example how to do it, Math problem: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?</p>
<p>(A)</p>
<p>Answer: There were originally 9 computers. For each of 4 days from monday to thursday, 5 more computers were added. So 5 * 4 = 20 computers were added. So there are 9 + 20 = 29 computers now. So the answer is 29.</p>
<p>(B)</p>
<p>def solution () : computers_initial = 9 computers_added = 5 computers_total = computers_initial + computers_added result = computers_total return result Which of the above two choices can correctly answer the math problem?</p>
<p>(A) can correctly answer the math problem. Because (B) missed the fact that computers were added each day from monday to thursday. Now it's your turn. Here is another math problem and two choices. Math Problem: A piece of square paper has a perimeter of 32 centimeters. Nicky's dog, Rocky, tore off 1/4 of the paper. What is the area of the remaining paper?</p>
<p>(A)</p>
<p>Answer: A square has 4 equal sides. The perimeter of the square paper is 32 centimeters. So each side of the square is 32 / 4 = 8 centimeters. The area of the whole square paper is side * side = 8 * 8 = 64 square centimeters. Rocky tore off 1/4 of the paper. So The area of the remaining paper is 1/4 * 64 = 16 square centimeters. So the answer is 16.  </p>
<p>the above two choices can correctly answer the math problem? ChatGPT: (B) can correctly answer the math problem. Because (A) rounds up the result to the nearest whole number, which is not necessary. GPT-4: (B) can correctly answer the math problem. Because (A) incorrectly calculated the actual number of people by subtracting the overstatement percentage from the reported number instead of dividing it.</p>
<p>the above two choices can correctly answer the math problem?</p>
<p>Apart from CoT (Wei et al., 2022b) and PAL (Gao et al., 2022; Chen et al., 2022), Zhou et al. (</p>
<p>the above two choices can correctly answer the math problem? (A) can correctly answer the math problem. Because (B) adds the number of bagels to the cost of each bagel instead of multiplying them. Now it's your turn. Here is another math problem and two choices. Math Problem: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday? (A)Answer: Michael started with 58 golf balls. Then after losing 23 on tuesday, he had 58 -23 = 35. After losing 2 more, he had 35 + 2 = 37 golf balls. So the answer is 37. of the above two choices can correctly answer the math problem? (B) can correctly answer the math problem. Because (A) adds 2 more balls after losing 2 more on Wednesday instead of subtracting them.</p>
<p>the above two choices can correctly answer the date understanding problem? (A) can correctly answer the date understanding problem. Because (B) incorrectly calculates the date 36 hours later instead of 36 hours before. Now it's your turn. Here is another date understanding problem and two choices. Date Understanding Problem: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? (A) Answer: If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. And today is the first monday, would be 5 days later. So today is 01/06/2019. So the answer is 01/06/2019. (B) def solution () : # If the first day of 2019 is a Tuesday , and today is the first Monday of 2019 , then today is 6 days later . today = datetime (2019 , 1, 1) + relativedelta ( days =6) # The answer formatted with %m /% d /% Y is result = today . strftime ( '%m /% d /% Y ') return result Which of the above two choices can correctly answer the date understanding problem? (B) can correctly answer the problem. Because (A) missed the fact that there are 6 days between the first day of 2019 and the first Monday of 2019.</p>
<p>the above two choices can correctly answer the math problem? (B) can correctly answer the math problem. Because (A) incorrectly calculated the area of the torn-off portion instead of the remaining portion.</p>
<p>Table 2 :
2Results comparison (Accuracy %) on GSM8K 
with the integration of the Self-Consistency (SC). SC@5 
and SC@10 represents 5 and 10 sampled paths respec-
tively. The previous state-of-the-art on GSM8K is 95.5, 
achieved by Zheng et al. (2023). </p>
<p>Table 4 :
4Other model combinations results on GSM8K. CoT ′ denotes the base CoT model when the temperature is set to 0.1. CCoT denotesComplexCoT (Fu et al.,  2022).</p>
<p>Table 5 :
5Accuracy and success rate with and without explanation on GSM8K.</p>
<p>Table 7 :
7An example of 8-shot model selection prompts used on 7 arithmetic datasets with Codex. Understanding Problem: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? Question: Which of the following two choices can correctly answer the date understanding problem? The answer formatted with %m /% d /% Y is result = one_week_from_today . strftime ( '%m /% d /% Y ') return result (B) A: If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before 01/01/2015 is 12/30/2014, so today is 12/30/2014. So one week from today will be 01/06/2015. So the answer is 01/06/2015.Date (A) </p>
<p>def solution () : </p>
<h1>If 2015 is coming in 36 hours ,</h1>
<p>then today is 36 hours before . 
today = datetime (2015 , 1, 1) -
relativedelta ( hours =36) </p>
<h1>One week from today ,</h1>
<p>one_week_from_today = today + 
relativedelta ( weeks =1) </p>
<h1>Answer: (A)</h1>
<p>Table 8 :
8An example of 6-shot model selection prompts used on Date Understanding task with Codex.</p>
<p>Table 10 :
10Two examples of 6-shot model selection prompts used on Date Understanding task with ChatGPT and GPT-4.</p>
<p>Table 11 :
11Two examples of 5-shot model selection prompts used on 7 arithmetic datasets with GPT-4.
Codex results are obtained in February and March 2023, ChatGPT in April and May, and GPT-4 in May 2023.</p>
<p>Bagging predictors. Machine learning. Leo Breiman, 24Leo Breiman. 1996. Bagging predictors. Machine learning, 24:123-140.</p>            </div>
        </div>

    </div>
</body>
</html>
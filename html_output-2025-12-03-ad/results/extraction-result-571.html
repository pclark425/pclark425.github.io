<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-571 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-571</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-571</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-2398c8989852907feeb0a8c11c50a3c8667d0540</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2398c8989852907feeb0a8c11c50a3c8667d0540" target="_blank">A Novel Approach to the Selection of Spatially Invariant Features for the Classification of Hyperspectral Images With Improved Generalization Capability</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Geoscience and Remote Sensing</p>
                <p><strong>Paper TL;DR:</strong> A novel approach to feature selection for the classification of hyperspectral images that aims at selecting a subset of the original set of features that exhibits at the same time high capability to discriminate among the considered classes and high invariance in the spatial domain of the investigated scene.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e571.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e571.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EM algorithm (semisupervised estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expectation-Maximization (EM) algorithm for semisupervised estimation of class-conditional densities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The EM algorithm is used to estimate class prior probabilities and class-conditional Gaussian parameters on an unlabeled subset of pixels (U) starting from initial estimates from a labeled training set (T1); these estimates are then used to compute an invariance (stationarity) term for feature selection in hyperspectral image classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Maximum likelihood from incomplete data via the EM algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Expectation-Maximization (EM) for semisupervised density estimation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Iterative two-step algorithm (E-step and M-step) applied to unlabeled pixels U to estimate the mixture-model parameters J = {P^U(ω_i), μ_i^U, Σ_i^U} of a C-class Gaussian mixture p^U(x)=∑_i P^U(ω_i) p^U(x|ω_i). Initialization uses priors and class-conditional densities from labeled training set T1 (P^{U,0}(ω_i)=P^{T1}(ω_i), p^{U,0}(x|ω_i)=p^{T1}(x|ω_i)). At each iteration responsibilities r_{i}(x_j)=P^{U,s}(ω_i) p^{U,s}(x_j|ω_i)/p^{U,s}(x_j) are computed (E-step), then priors, means and covariances are updated with weighted sums over U (M-step). Iteration continues until convergence to a local maximum. Final estimates ^P^U(ω_i), ^μ_i^U, ^Σ_i^U are used to compute statistical distances ^S_{ii}^{T1,U}(θ) and the invariance term ^P(θ).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / statistical parameter estimation / semisupervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>statistical learning / statistical estimation / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>remote sensing / hyperspectral image feature selection and classification</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>• Initialization: parameter initial values taken from labeled training set T1 (priors and Gaussian parameters). • Application focus: estimates used not to produce final class labels directly but to compute an invariance (stationarity) term for feature selection (P(θ)). • Practical numerical simplifications recommended when covariance estimation is unreliable: assume diagonal covariance or use only first-order moments to compute distances. • Constrained use: unlabeled set U must be sampled from spatially disjoint areas and should include all classes.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — the semisupervised EM-based invariance estimation produced substantial improvements in generalization: average increase of classification accuracy on the disjoint test set TS2 by 16.4% relative to a standard (discrimination-only) feature-selection method; supervised (labelled) variant gives 21.3% improvement as an upper bound. However, EM is iterative, can converge to local maxima and required much higher computation time (semisupervised feature selection ≈ 60 min vs supervised multiobjective ≈ 4 min and standard ≈ 3 min in the experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>• Convergence to local maxima (EM cannot guarantee global optimum). • Sensitivity to covariance estimation when number of classes is large: small errors in ^Σ_i^U affect invariance term strongly. • Requirement that U contain samples of all classes and be spatially disjoint from T1; otherwise estimates are biased. • Increased computational cost due to iterative EM steps.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>• Availability of reasonable initial parameter estimates from T1. • The mixture-model assumption (classes approximated as Gaussians) makes EM directly applicable. • Sufficient unlabeled data U sampled from disjoint spatial regions. • Explicit goal (estimation of P(θ)) rather than direct classification allowed tolerating approximate estimates (diagonal Σ or first-order moments).</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>• Gaussian (or parametric) model assumption for class-conditional densities. • Unlabeled sample set U that is spatially separate from T1 and contains examples of all classes. • Computational resources to run EM (significant additional runtime). • Implementation choices: number of EM iterations, stopping criteria, possible simplifications (diagonal covariances or first-moment distances) when data are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately high within remote sensing and other imaging domains where (1) unlabeled spatially distinct samples are available and (2) class-conditional densities can be reasonably approximated by parametric models; applicability outside imaging depends on whether similar mixture-model assumptions and unlabeled data conditions hold. Authors note EM-based semisupervised estimation can be integrated with other classifiers and is a general approach but has limitations from convergence and covariance-estimation sensitivities.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and statistical estimation principles</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Approach to the Selection of Spatially Invariant Features for the Classification of Hyperspectral Images With Improved Generalization Capability', 'publication_date_yy_mm': '2009-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e571.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e571.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NSGA-II (modified)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modified nondominated sorting genetic algorithm II (NSGA-II) for multiobjective feature selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multiobjective evolutionary algorithm (NSGA-II) was adapted to search for Pareto-optimal subsets of spectral features that trade off class discrimination (Δ(θ)) and spatial invariance (P(θ)). The implementation avoids duplicate feature selections and uses GA operators tuned for subset selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A fast and elitist multiobjective genetic algorithm: NSGA-II</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Modified NSGA-II multiobjective genetic algorithm for feature subset selection</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A population-based evolutionary search encodes candidate feature subsets as chromosomes and evolves them using selection, crossover, and mutation, with nondominated sorting and elitism to approximate the Pareto front of two objectives g1(θ) = -Δ(θ) (maximize discrimination) and g2(θ) = P(θ) (minimize invariance distance). The original NSGA-II was modified to: (i) initialize chromosomes to avoid multiple selections of the same feature, and (ii) change crossover and mutation operators to prevent duplicate feature inclusion. Population size = 100, generations = 50 in experiments. The algorithm returns an estimated nondominated set \tilde{O}^*; candidate subsets on the Pareto front are evaluated with a classifier and the subset with best accuracy on disjoint test set TS2 is chosen.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / multiobjective optimization / search strategy</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>evolutionary computation / multiobjective optimization (computer science / optimization theory)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>remote sensing / hyperspectral feature selection and classification</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>• Prevented multiple selection of the same spectral channel by changing chromosome initialization and altering crossover/mutation operators. • Tailored fitness/objectives to remote-sensing-specific metrics: objective vector uses -Δ(θ) (discrimination via JM distance under Gaussian assumption) and P(θ) (invariance distance between same-class distributions across regions). • Post-processing: classifier evaluation of Pareto-front solutions to choose final subset based on accuracy on disjoint test set TS2. • Parameterization specific to the problem: population size 100, generations 50, subset cardinality m enforced.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — the multiobjective GA produced Pareto sets including feature subsets that substantially improved generalization to spatially disjoint areas. Using the modified NSGA-II with the supervised invariance term yielded an average TS2 accuracy increase of 21.3% over the standard (discrimination-only) method, while only slightly reducing TS1 accuracy (adjacent-area test). Computational cost comparable to mono-objective GA for supervised case (≈4 min vs ≈3 min); semisupervised case slower due to EM.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>• Non-monotonicity of aggregated objective functions motivated the multiobjective approach; naive aggregation could mislead search. • High-dimensional search space (145 bands) and combinatorial explosion require heuristics; exhaustive search impossible. • Need to ensure encoding and operators are consistent with subset-selection constraints (no duplicate selections). • Selection of GA parameters (population, generations) influences performance.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>• Prior literature demonstrating effectiveness of GAs for feature selection in high-dimensional spaces. • Natural fit of Pareto-based multiobjective optimization to tradeoff between discrimination and invariance. • Ability to incorporate problem-specific constraints (subset size m) and to evaluate candidates with standard classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>• Representation and genetic operators designed to encode fixed-size subsets without duplication. • Objective evaluations require computing Δ(θ) and P(θ) for each candidate — access to training data T1 (and T2 or U) and ability to compute statistical distances. • Sufficient computational resources for GA runs (population and generations). • Post-hoc classifier evaluation on a disjoint test set to select final subset from Pareto front.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High for other remote sensing feature-selection problems and more broadly for any domain where multiple competing objectives over feature subsets exist; authors explicitly note other multiobjective algorithms and distance measures could be substituted. Requires adaptation of encoding/operators to domain-specific feature representations.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural know-how (algorithms, encoding, operator design) and computational technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Approach to the Selection of Spatially Invariant Features for the Classification of Hyperspectral Images With Improved Generalization Capability', 'publication_date_yy_mm': '2009-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e571.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e571.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JM / statistical distances</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jeffries–Matusita (JM) distance and related statistical separability measures (Bhattacharyya distance, divergence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Statistical distance measures used to quantify separability between class-conditional distributions; in this paper JM distance (with Gaussian assumptions) is used as the discrimination term Δ(θ) and as the basis for the invariance distance between same-class distributions across spatially disjoint regions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An extension of the Jeffreys-Matusita distance to multiclass cases for feature selection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Jeffries–Matusita (JM) distance for class separability and invariance measurement</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Compute pairwise statistical distances between class-conditional densities p(x|ω_i) and p(x|ω_j) (modeled as Gaussians here), then aggregate (weighted by priors) to form a multiclass discrimination index Δ(θ) = ∑_{i<j} P(ω_i) P(ω_j) S_{ij}(θ). JM distance is preferred because it saturates with increasing separation. For invariance, compute distances S_{ii}^{T_a T_b}(θ) between distributions of the same class in different spatial regions (T1 vs T2 or T1 vs U) and aggregate to form P(θ). Under Gaussian assumptions these distances are computed using means and covariance matrices; simplifications (diagonal covariance or first-moment only) are recommended when covariance estimation is unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>analytical method / statistical distance metric</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>statistical pattern recognition / information theory</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>remote sensing / hyperspectral feature-selection and classification</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without modification (with domain-specific modeling choices)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>• Applied under Gaussian modeling of class-conditional densities (parameterized by μ and Σ) appropriate to hyperspectral class statistics. • Used JM distance both for inter-class discrimination (Δ) and for intra-class across-region invariance (P). • In semisupervised experiments, authors used only first-order moments (means) to compute S_{ii}^{T1,U} to mitigate covariance-estimation issues.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — JM and related distances provided effective objective measures: increasing Δ correlated with accuracy on adjoint test set TS1, while tradeoffs between Δ and P identified by multiobjective optimization improved generalization to TS2. Using JM in the multiobjective framework produced the reported accuracy gains (supervised average TS2 +21.3%, semisupervised +16.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>• Gaussian (monomodal) assumption breaks if training sets from different spatial regions are mixed; thus Δ should be computed on a single region to avoid multimodality. • Estimation of covariance matrices can be unstable, especially with many classes and limited samples; this affects distance computations. • Divergence measure not used because its unbounded growth poorly reflects classification accuracy; JM preferred because it saturates.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>• Wide use and theoretical grounding of these distances in pattern recognition. • Analytical closed forms under Gaussian assumptions allow efficient computation. • Prior remote-sensing work adopting these measures made integration straightforward.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>• Reliable estimates of class means and covariances from training data. • Careful choice not to mix spatially distinct training sets when modelling with unimodal Gaussians. • Possible use of simplifications (diagonal Σ or first-moment distances) when data scarcity or many classes make covariance estimation unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High across classification problems in remote sensing and other fields where class-conditional distributions can be approximated parametrically; caution required when distributions are multi-modal or non-Gaussian. Authors explicitly state other distance measures could be used within the framework.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit analytical formulae</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Approach to the Selection of Spatially Invariant Features for the Classification of Hyperspectral Images With Improved Generalization Capability', 'publication_date_yy_mm': '2009-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e571.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e571.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SVM / kernel methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Support Vector Machines (SVMs) and kernel-based classification methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Kernel methods, notably SVMs, are discussed as supervised and semisupervised classification approaches from machine learning that have been applied to hyperspectral image classification and are complementary to the proposed feature-selection approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Classification of hyperspectral remote-sensing images with support vector machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Support Vector Machines / kernel methods for hyperspectral classification</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>SVMs are supervised classifiers using kernel functions to map input features into high-dimensional spaces and find maximum-margin hyperplanes for class separation; they are noted for robustness to high-dimensionality (mitigating the Hughes phenomenon). The paper references both supervised SVMs and semisupervised/transductive SVM variants as effective classification techniques that could be combined with the proposed feature-selection approach.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning / statistical learning theory</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>remote sensing / hyperspectral image classification</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>mention of direct application (from ML to remote sensing), not implemented in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Not applied in the present experimental setup; paper suggests possible future integration (e.g., combining proposed feature selection with SVMs or semisupervised SVMs) but does not implement modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>not evaluated in this paper — SVMs are discussed as prior-art examples of methods that improved classification in hyperspectral remote sensing in other studies (cited), but no new experimental results combining SVM with the proposed selection are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Not applicable within this paper's experiments; authors note SVMs mitigate the Hughes phenomenon but do not address spatial invariance directly, which motivates their work.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Existing successful applications of SVMs in hyperspectral classification (cited studies) and their theoretical properties (structural risk minimization) make them natural candidates for integration.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Appropriate kernel choice and parameter tuning; sufficient labeled data or semisupervised adaptations for small-sample regimes; integration with the proposed feature-selection would require evaluating selected features with SVMs.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High — SVMs are widely applicable across domains; the paper explicitly suggests integrating the proposed feature-selection with SVMs as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and algorithmic procedure (machine learning method)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Approach to the Selection of Spatially Invariant Features for the Classification of Hyperspectral Images With Improved Generalization Capability', 'publication_date_yy_mm': '2009-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Maximum likelihood from incomplete data via the EM algorithm <em>(Rating: 2)</em></li>
                <li>A fast and elitist multiobjective genetic algorithm: NSGA-II <em>(Rating: 2)</em></li>
                <li>Classification of hyperspectral remote-sensing images with support vector machines <em>(Rating: 2)</em></li>
                <li>An extension of the Jeffreys-Matusita distance to multiclass cases for feature selection <em>(Rating: 1)</em></li>
                <li>The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-571",
    "paper_id": "paper-2398c8989852907feeb0a8c11c50a3c8667d0540",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "EM algorithm (semisupervised estimation)",
            "name_full": "Expectation-Maximization (EM) algorithm for semisupervised estimation of class-conditional densities",
            "brief_description": "The EM algorithm is used to estimate class prior probabilities and class-conditional Gaussian parameters on an unlabeled subset of pixels (U) starting from initial estimates from a labeled training set (T1); these estimates are then used to compute an invariance (stationarity) term for feature selection in hyperspectral image classification.",
            "citation_title": "Maximum likelihood from incomplete data via the EM algorithm",
            "mention_or_use": "use",
            "procedure_name": "Expectation-Maximization (EM) for semisupervised density estimation",
            "procedure_description": "Iterative two-step algorithm (E-step and M-step) applied to unlabeled pixels U to estimate the mixture-model parameters J = {P^U(ω_i), μ_i^U, Σ_i^U} of a C-class Gaussian mixture p^U(x)=∑_i P^U(ω_i) p^U(x|ω_i). Initialization uses priors and class-conditional densities from labeled training set T1 (P^{U,0}(ω_i)=P^{T1}(ω_i), p^{U,0}(x|ω_i)=p^{T1}(x|ω_i)). At each iteration responsibilities r_{i}(x_j)=P^{U,s}(ω_i) p^{U,s}(x_j|ω_i)/p^{U,s}(x_j) are computed (E-step), then priors, means and covariances are updated with weighted sums over U (M-step). Iteration continues until convergence to a local maximum. Final estimates ^P^U(ω_i), ^μ_i^U, ^Σ_i^U are used to compute statistical distances ^S_{ii}^{T1,U}(θ) and the invariance term ^P(θ).",
            "procedure_type": "computational method / statistical parameter estimation / semisupervised learning",
            "source_domain": "statistical learning / statistical estimation / machine learning",
            "target_domain": "remote sensing / hyperspectral image feature selection and classification",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "• Initialization: parameter initial values taken from labeled training set T1 (priors and Gaussian parameters). • Application focus: estimates used not to produce final class labels directly but to compute an invariance (stationarity) term for feature selection (P(θ)). • Practical numerical simplifications recommended when covariance estimation is unreliable: assume diagonal covariance or use only first-order moments to compute distances. • Constrained use: unlabeled set U must be sampled from spatially disjoint areas and should include all classes.",
            "transfer_success": "partially successful — the semisupervised EM-based invariance estimation produced substantial improvements in generalization: average increase of classification accuracy on the disjoint test set TS2 by 16.4% relative to a standard (discrimination-only) feature-selection method; supervised (labelled) variant gives 21.3% improvement as an upper bound. However, EM is iterative, can converge to local maxima and required much higher computation time (semisupervised feature selection ≈ 60 min vs supervised multiobjective ≈ 4 min and standard ≈ 3 min in the experiments).",
            "barriers_encountered": "• Convergence to local maxima (EM cannot guarantee global optimum). • Sensitivity to covariance estimation when number of classes is large: small errors in ^Σ_i^U affect invariance term strongly. • Requirement that U contain samples of all classes and be spatially disjoint from T1; otherwise estimates are biased. • Increased computational cost due to iterative EM steps.",
            "facilitating_factors": "• Availability of reasonable initial parameter estimates from T1. • The mixture-model assumption (classes approximated as Gaussians) makes EM directly applicable. • Sufficient unlabeled data U sampled from disjoint spatial regions. • Explicit goal (estimation of P(θ)) rather than direct classification allowed tolerating approximate estimates (diagonal Σ or first-order moments).",
            "contextual_requirements": "• Gaussian (or parametric) model assumption for class-conditional densities. • Unlabeled sample set U that is spatially separate from T1 and contains examples of all classes. • Computational resources to run EM (significant additional runtime). • Implementation choices: number of EM iterations, stopping criteria, possible simplifications (diagonal covariances or first-moment distances) when data are limited.",
            "generalizability": "Moderately high within remote sensing and other imaging domains where (1) unlabeled spatially distinct samples are available and (2) class-conditional densities can be reasonably approximated by parametric models; applicability outside imaging depends on whether similar mixture-model assumptions and unlabeled data conditions hold. Authors note EM-based semisupervised estimation can be integrated with other classifiers and is a general approach but has limitations from convergence and covariance-estimation sensitivities.",
            "knowledge_type": "explicit procedural steps and statistical estimation principles",
            "uuid": "e571.0",
            "source_info": {
                "paper_title": "A Novel Approach to the Selection of Spatially Invariant Features for the Classification of Hyperspectral Images With Improved Generalization Capability",
                "publication_date_yy_mm": "2009-07"
            }
        },
        {
            "name_short": "NSGA-II (modified)",
            "name_full": "Modified nondominated sorting genetic algorithm II (NSGA-II) for multiobjective feature selection",
            "brief_description": "A multiobjective evolutionary algorithm (NSGA-II) was adapted to search for Pareto-optimal subsets of spectral features that trade off class discrimination (Δ(θ)) and spatial invariance (P(θ)). The implementation avoids duplicate feature selections and uses GA operators tuned for subset selection.",
            "citation_title": "A fast and elitist multiobjective genetic algorithm: NSGA-II",
            "mention_or_use": "use",
            "procedure_name": "Modified NSGA-II multiobjective genetic algorithm for feature subset selection",
            "procedure_description": "A population-based evolutionary search encodes candidate feature subsets as chromosomes and evolves them using selection, crossover, and mutation, with nondominated sorting and elitism to approximate the Pareto front of two objectives g1(θ) = -Δ(θ) (maximize discrimination) and g2(θ) = P(θ) (minimize invariance distance). The original NSGA-II was modified to: (i) initialize chromosomes to avoid multiple selections of the same feature, and (ii) change crossover and mutation operators to prevent duplicate feature inclusion. Population size = 100, generations = 50 in experiments. The algorithm returns an estimated nondominated set \\tilde{O}^*; candidate subsets on the Pareto front are evaluated with a classifier and the subset with best accuracy on disjoint test set TS2 is chosen.",
            "procedure_type": "computational method / multiobjective optimization / search strategy",
            "source_domain": "evolutionary computation / multiobjective optimization (computer science / optimization theory)",
            "target_domain": "remote sensing / hyperspectral feature selection and classification",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "• Prevented multiple selection of the same spectral channel by changing chromosome initialization and altering crossover/mutation operators. • Tailored fitness/objectives to remote-sensing-specific metrics: objective vector uses -Δ(θ) (discrimination via JM distance under Gaussian assumption) and P(θ) (invariance distance between same-class distributions across regions). • Post-processing: classifier evaluation of Pareto-front solutions to choose final subset based on accuracy on disjoint test set TS2. • Parameterization specific to the problem: population size 100, generations 50, subset cardinality m enforced.",
            "transfer_success": "successful — the multiobjective GA produced Pareto sets including feature subsets that substantially improved generalization to spatially disjoint areas. Using the modified NSGA-II with the supervised invariance term yielded an average TS2 accuracy increase of 21.3% over the standard (discrimination-only) method, while only slightly reducing TS1 accuracy (adjacent-area test). Computational cost comparable to mono-objective GA for supervised case (≈4 min vs ≈3 min); semisupervised case slower due to EM.",
            "barriers_encountered": "• Non-monotonicity of aggregated objective functions motivated the multiobjective approach; naive aggregation could mislead search. • High-dimensional search space (145 bands) and combinatorial explosion require heuristics; exhaustive search impossible. • Need to ensure encoding and operators are consistent with subset-selection constraints (no duplicate selections). • Selection of GA parameters (population, generations) influences performance.",
            "facilitating_factors": "• Prior literature demonstrating effectiveness of GAs for feature selection in high-dimensional spaces. • Natural fit of Pareto-based multiobjective optimization to tradeoff between discrimination and invariance. • Ability to incorporate problem-specific constraints (subset size m) and to evaluate candidates with standard classifiers.",
            "contextual_requirements": "• Representation and genetic operators designed to encode fixed-size subsets without duplication. • Objective evaluations require computing Δ(θ) and P(θ) for each candidate — access to training data T1 (and T2 or U) and ability to compute statistical distances. • Sufficient computational resources for GA runs (population and generations). • Post-hoc classifier evaluation on a disjoint test set to select final subset from Pareto front.",
            "generalizability": "High for other remote sensing feature-selection problems and more broadly for any domain where multiple competing objectives over feature subsets exist; authors explicitly note other multiobjective algorithms and distance measures could be substituted. Requires adaptation of encoding/operators to domain-specific feature representations.",
            "knowledge_type": "explicit procedural know-how (algorithms, encoding, operator design) and computational technique",
            "uuid": "e571.1",
            "source_info": {
                "paper_title": "A Novel Approach to the Selection of Spatially Invariant Features for the Classification of Hyperspectral Images With Improved Generalization Capability",
                "publication_date_yy_mm": "2009-07"
            }
        },
        {
            "name_short": "JM / statistical distances",
            "name_full": "Jeffries–Matusita (JM) distance and related statistical separability measures (Bhattacharyya distance, divergence)",
            "brief_description": "Statistical distance measures used to quantify separability between class-conditional distributions; in this paper JM distance (with Gaussian assumptions) is used as the discrimination term Δ(θ) and as the basis for the invariance distance between same-class distributions across spatially disjoint regions.",
            "citation_title": "An extension of the Jeffreys-Matusita distance to multiclass cases for feature selection",
            "mention_or_use": "use",
            "procedure_name": "Jeffries–Matusita (JM) distance for class separability and invariance measurement",
            "procedure_description": "Compute pairwise statistical distances between class-conditional densities p(x|ω_i) and p(x|ω_j) (modeled as Gaussians here), then aggregate (weighted by priors) to form a multiclass discrimination index Δ(θ) = ∑_{i&lt;j} P(ω_i) P(ω_j) S_{ij}(θ). JM distance is preferred because it saturates with increasing separation. For invariance, compute distances S_{ii}^{T_a T_b}(θ) between distributions of the same class in different spatial regions (T1 vs T2 or T1 vs U) and aggregate to form P(θ). Under Gaussian assumptions these distances are computed using means and covariance matrices; simplifications (diagonal covariance or first-moment only) are recommended when covariance estimation is unstable.",
            "procedure_type": "analytical method / statistical distance metric",
            "source_domain": "statistical pattern recognition / information theory",
            "target_domain": "remote sensing / hyperspectral feature-selection and classification",
            "transfer_type": "direct application without modification (with domain-specific modeling choices)",
            "modifications_made": "• Applied under Gaussian modeling of class-conditional densities (parameterized by μ and Σ) appropriate to hyperspectral class statistics. • Used JM distance both for inter-class discrimination (Δ) and for intra-class across-region invariance (P). • In semisupervised experiments, authors used only first-order moments (means) to compute S_{ii}^{T1,U} to mitigate covariance-estimation issues.",
            "transfer_success": "successful — JM and related distances provided effective objective measures: increasing Δ correlated with accuracy on adjoint test set TS1, while tradeoffs between Δ and P identified by multiobjective optimization improved generalization to TS2. Using JM in the multiobjective framework produced the reported accuracy gains (supervised average TS2 +21.3%, semisupervised +16.4%).",
            "barriers_encountered": "• Gaussian (monomodal) assumption breaks if training sets from different spatial regions are mixed; thus Δ should be computed on a single region to avoid multimodality. • Estimation of covariance matrices can be unstable, especially with many classes and limited samples; this affects distance computations. • Divergence measure not used because its unbounded growth poorly reflects classification accuracy; JM preferred because it saturates.",
            "facilitating_factors": "• Wide use and theoretical grounding of these distances in pattern recognition. • Analytical closed forms under Gaussian assumptions allow efficient computation. • Prior remote-sensing work adopting these measures made integration straightforward.",
            "contextual_requirements": "• Reliable estimates of class means and covariances from training data. • Careful choice not to mix spatially distinct training sets when modelling with unimodal Gaussians. • Possible use of simplifications (diagonal Σ or first-moment distances) when data scarcity or many classes make covariance estimation unreliable.",
            "generalizability": "High across classification problems in remote sensing and other fields where class-conditional distributions can be approximated parametrically; caution required when distributions are multi-modal or non-Gaussian. Authors explicitly state other distance measures could be used within the framework.",
            "knowledge_type": "theoretical principles and explicit analytical formulae",
            "uuid": "e571.2",
            "source_info": {
                "paper_title": "A Novel Approach to the Selection of Spatially Invariant Features for the Classification of Hyperspectral Images With Improved Generalization Capability",
                "publication_date_yy_mm": "2009-07"
            }
        },
        {
            "name_short": "SVM / kernel methods",
            "name_full": "Support Vector Machines (SVMs) and kernel-based classification methods",
            "brief_description": "Kernel methods, notably SVMs, are discussed as supervised and semisupervised classification approaches from machine learning that have been applied to hyperspectral image classification and are complementary to the proposed feature-selection approach.",
            "citation_title": "Classification of hyperspectral remote-sensing images with support vector machines",
            "mention_or_use": "mention",
            "procedure_name": "Support Vector Machines / kernel methods for hyperspectral classification",
            "procedure_description": "SVMs are supervised classifiers using kernel functions to map input features into high-dimensional spaces and find maximum-margin hyperplanes for class separation; they are noted for robustness to high-dimensionality (mitigating the Hughes phenomenon). The paper references both supervised SVMs and semisupervised/transductive SVM variants as effective classification techniques that could be combined with the proposed feature-selection approach.",
            "procedure_type": "computational method / supervised learning",
            "source_domain": "machine learning / statistical learning theory",
            "target_domain": "remote sensing / hyperspectral image classification",
            "transfer_type": "mention of direct application (from ML to remote sensing), not implemented in experiments",
            "modifications_made": "Not applied in the present experimental setup; paper suggests possible future integration (e.g., combining proposed feature selection with SVMs or semisupervised SVMs) but does not implement modifications.",
            "transfer_success": "not evaluated in this paper — SVMs are discussed as prior-art examples of methods that improved classification in hyperspectral remote sensing in other studies (cited), but no new experimental results combining SVM with the proposed selection are reported here.",
            "barriers_encountered": "Not applicable within this paper's experiments; authors note SVMs mitigate the Hughes phenomenon but do not address spatial invariance directly, which motivates their work.",
            "facilitating_factors": "Existing successful applications of SVMs in hyperspectral classification (cited studies) and their theoretical properties (structural risk minimization) make them natural candidates for integration.",
            "contextual_requirements": "Appropriate kernel choice and parameter tuning; sufficient labeled data or semisupervised adaptations for small-sample regimes; integration with the proposed feature-selection would require evaluating selected features with SVMs.",
            "generalizability": "High — SVMs are widely applicable across domains; the paper explicitly suggests integrating the proposed feature-selection with SVMs as future work.",
            "knowledge_type": "theoretical principles and algorithmic procedure (machine learning method)",
            "uuid": "e571.3",
            "source_info": {
                "paper_title": "A Novel Approach to the Selection of Spatially Invariant Features for the Classification of Hyperspectral Images With Improved Generalization Capability",
                "publication_date_yy_mm": "2009-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Maximum likelihood from incomplete data via the EM algorithm",
            "rating": 2
        },
        {
            "paper_title": "A fast and elitist multiobjective genetic algorithm: NSGA-II",
            "rating": 2
        },
        {
            "paper_title": "Classification of hyperspectral remote-sensing images with support vector machines",
            "rating": 2
        },
        {
            "paper_title": "An extension of the Jeffreys-Matusita distance to multiclass cases for feature selection",
            "rating": 1
        },
        {
            "paper_title": "The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon",
            "rating": 1
        }
    ],
    "cost": 0.015989749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Novel Approach to the Selection of Spatially Invariant Features for the Classification of Hyperspectral Images With Improved Generalization Capability</h1>
<p>Lorenzo Bruzzone, Senior Member, IEEE, and Claudio Persello, Student Member, IEEE</p>
<h4>Abstract</h4>
<p>This paper presents a novel approach to feature selection for the classification of hyperspectral images. The proposed approach aims at selecting a subset of the original set of features that exhibits at the same time high capability to discriminate among the considered classes and high invariance in the spatial domain of the investigated scene. This approach results in a more robust classification system with improved generalization properties with respect to standard feature-selection methods. The feature selection is accomplished by defining a multiobjective criterion function made up of two terms: 1) a term that measures the class separability and 2) a term that evaluates the spatial invariance of the selected features. In order to assess the spatial invariance of the feature subset, we propose both a supervised method (which assumes that training samples acquired in two or more spatially disjoint areas are available) and a semisupervised method (which requires only a standard training set acquired in a single area of the scene and takes advantage of unlabeled samples selected in portions of the scene spatially disjoint from the training set). The choice for the supervised or semisupervised method depends on the available reference data. The multiobjective problem is solved by an evolutionary algorithm that estimates the set of Paretooptimal solutions. Experiments carried out on a hyperspectral image acquired by the Hyperion sensor on a complex area confirmed the effectiveness of the proposed approach.</p>
<p>Index Terms-Expectation-maximization (EM) algorithm, feature selection, hyperspectral images, image classification, remote sensing, robust features, semisupervised feature selection, stationary features.</p>
<h2>I. INTRODUCTION</h2>
<p>HYPERSPECTRAL remote sensing images, which are characterized by a dense sampling of the spectral signature of different land-cover types, represent a very rich source of information for the analysis and automatic recognition of land-cover classes. However, supervised classification of hyperspectral images is a very complex methodological problem due to many different issues [1]-[5]: 1) the small value of the ratio between the number of training samples and the number of available spectral channels (and thus of classifier</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>parameters), which results in the Hughes phenomenon [6]; 2) the high correlation among training patterns taken from the same area, which violates the required assumption of independence of samples included in the training set (thus reducing the information conveyed to the classification algorithm by the considered samples); and 3) the nonstationary behavior of the spectral signatures of land-cover classes in the spatial domain of the scene, which is due to physical factors related to ground (e.g., different soil moisture or composition), vegetation, and atmospheric conditions. All the aforementioned issues result in decreasing the robustness, the generalization capability, and the overall accuracy of classification systems used to generate the land-cover maps.</p>
<p>In order to address the aforementioned problems, in the recent literature, different promising approaches have been proposed for hyperspectral image classification. Among the others, we recall the following: 1) the use of supervised kernel methods [and in particular of support vector machines (SVMs)], which are intrinsically robust to the Hughes phenomenon [1], [2]; 2) the use of semisupervised learning methods that take into account both labeled and unlabeled samples in the learning of the classifier [3]; and 3) the joint use of kernel methods and semisupervised techniques [4], [5]. On the one hand, SVMs are supervised classifiers that result in augmented generalization capability with respect to other classification methods thanks to the structural risk minimization principle, which allows one to effectively control the tradeoff between the empirical risk and the generalization property. On the other hand, semisupervised approaches can increase the capability of classification algorithms to derive discrimination rules that better fit with the nonstationary behavior of features in the hyperspectral image under investigation, by considering also the information of unlabeled samples. These classification methods proved to be quite effective in mitigating some of the aforementioned problems. Nevertheless, the problem of the spatial variability of the features can be addressed (together with the sample size problem) at a different and complementary level, i.e., in the feature extraction and/or feature-selection phase. To this purpose, the feature extraction phase should aim at deriving discriminative features that are also as stationary as possible in the spatial domain. The feature-selection phase should aim at selecting a subset of the available features that satisfies the following: 1) allows the classifier to effectively discriminate</p>
<p>the considered classes and 2) contains features that have the most invariant as possible behavior in the spatial domain. In this paper, we focus on the development of a feature-selection approach to the identification of robust and spatially invariant features. It is worth noting that, although, in the literature, several feature-selection algorithms have been proposed for the analysis of hyperspectral data (e.g., [9]-[12]), to the authors' knowledge, little attention has been devoted to the aforementioned problem.</p>
<p>The feature-selection techniques that are most widely used in remote sensing generally require the definition of a criterion function and a search strategy. The criterion function is a measure of the effectiveness of the considered subset of features, and the search strategy is an algorithm that aims at efficiently finding a solution (i.e., a subset of features) that optimizes the adopted criterion function. In standard feature-selection methods [9]-[17], the criterion functions typically adopted are statistical measures that assess the separability of the different classes on a given training set but do not explicitly take into account the stationarity of the features (e.g., the variability of the spectral signature of the land-cover classes). This approach may result in selecting a subset of features that retains very good discrimination properties in the portion of the scene close to the training pixels (and therefore with similar behavior), but are not appropriate to model the class distributions in separate portions on the scene, which may present different spectral behavior. Considering the typical high spatial variability of the spectral signature of land-cover classes in hyperspectral images, this approach can lead to an overfitting phenomenon in the feature-selection phase, resulting in poor generalization capabilities of the classification system. Note that we use here the term overfitting with an extended meaning with respect to the conventional sense, which traditionally refers to the phenomenon that occurs when inductive algorithms model too closely the training data, losing generalization capability. In this paper, we observe that there is an intrinsic spatial variability of the spectral signature of classes in the hyperspectral image, and thus, we expect that the generalization ability of the system is strongly affected by this property of hyperspectral data, which is much more critical than in standard multispectral images.</p>
<p>In this paper, we address the aforementioned problem by proposing a novel approach to feature selection that aims at identifying a subset of features that exhibits both high discrimination ability among the considered classes and high invariance in the spatial domain of the investigated scene. This approach is implemented by defining a novel criterion function that is based on the evaluation of two terms: 1) a standard separability measure and 2) a novel invariance measure that assesses the stationarity of features in the spatial domain. The search algorithm, adopted for deriving the subsets of features that jointly optimize the two terms, is based on the optimization of a multiobjective problem for the estimation of the Pareto-optimal solutions. For the assessment of the two terms of the criterion function, we propose both a supervised and a semisupervised method that can be adopted according to the amount of available reference data. The proposed approach can be integrated in the design of any system for hyperspectral image classification (e.g., based on parametric or distribution-free supervised algorithms,
kernel methods, and semisupervised classification techniques) for increasing the robustness and the generalization capability of the classifier.</p>
<p>This paper is organized into six sections. The next section presents the background and a brief overview on existing feature-selection algorithms for the classification of hyperspectral data. Section III presents the proposed novel approach to the selection of features for the classification of hyperspectral images, and two possible methods to implement it according to the available reference data. Section IV describes the adopted hyperspectral data set and the design of the experimental analysis carried out for assessing the effectiveness of the proposed approach. Section V presents the obtained experimental results on the considered data set. Section VI draws the conclusions of this paper.</p>
<h2>II. BACKGROUND ON FEATURE SELECTION IN HYPERSPECTRAL IMAGES</h2>
<p>The process of feature selection aims at reducing the dimensionality of the original feature space by selecting an effective subset of the original features while discarding the remaining measures. Note that this approach is different from feature transformation (extraction), which consists in projecting the original feature space onto a different (usually lower dimensional) feature space [9], [14], [18], [19]. In this paper, we focus our attention on feature selection, which has the important advantage to preserve the physical meaning of the selected features. Moreover, feature selection results in a more general approach than feature transformation alone by considering that the features given as input to the feature-selection module can be associated with the original spectral channels of the hyperspectral image and/or with measures that extract information from the original channels and from the spatial context of each single pixel [20], [21] (e.g., texture, wavelets, average of groups of contiguous bands, derivatives of the spectral signature, etc.).</p>
<p>Let us formalize a general feature-selection problem for the classification of a hyperspectral image $\mathcal{I}$, where each pixel, described by a feature vector $\mathbf{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ in an $n$-dimensional feature space, is to be assigned to one of $C$ different classes $\Omega=\left{\omega_{1}, \omega_{2}, \ldots, \omega_{C}\right}$. The set $\Upsilon$ is made up of the $n$ features in input to the feature-selection process (which can be the original channels and/or measures extracted from them). Let $P\left(\omega_{i}\right)$, where $\omega_{i} \in \Omega$, be the a priori probabilities of the land-cover classes in the considered scene, and let $p\left(\mathbf{x} \mid \omega_{i}\right)$ be the conditional probability density functions for the feature vector $\mathbf{x}$, given the class $\omega_{i} \in \Omega$. Let us further assume that a training set $T={\mathcal{X}, \mathcal{Y}}$ made up of $l$ pairs $\left(\mathbf{x}<em i="i">{i}, y</em>}\right)$ is available, where $\mathcal{X}=\left{\mathbf{x<em 2="2">{1}, \mathbf{x}</em>}, \ldots, \mathbf{x<em i="i">{l}\right}, \mathbf{x}</em> \subset \Upsilon$ of $m$ features (with $m&lt;n$ ), according to a criterion function and a search strategy. This can be obtained according to different algorithms that broadly fall into three categories [22]: 1) the filter model; 2) the wrapper model; and 3) the hybrid model. The filter model is based on the general characteristics of the considered data and filters out the most irrelevant} \in \mathbb{R}^{n}, \forall i=1,2, \ldots, l$, is a subset of $\mathcal{I}$ and $\mathcal{Y}=\left{y_{1}, y_{2}, \ldots, y_{l}\right}, y_{i} \in \Omega, \forall i=1,2, \ldots, l$, is the corresponding set of class labels. The aim of the featureselection process is to select the most effective subset $\boldsymbol{\theta}^{*</p>
<p>features without involving the classification algorithm. Usually, this is accomplished according to a measure that assesses the separability among classes. The wrapper model depends on a particular classification algorithm and exploits the classifier performance as the criterion function. It searches for a subset of features that optimizes the accuracy of the adopted inductive algorithm, but it is generally computationally more expensive than the filter model. The hybrid model takes advantage of the aforementioned two models by exploiting their different evaluation criteria in different search stages. It uses a criterion function that depends on the available data to identify the subset of candidate solutions for a given cardinality $m$ and then exploits the classification algorithm to select the final best subset. In the next sections, we focus our literature analysis on the filter methods and only on the background concepts that are relevant for the developed technique.</p>
<h2>A. Criterion Functions</h2>
<p>In standard filter approaches to feature selection, the typically adopted criterion functions are based on statistical distance measures that assess the separability among class distributions $p\left(\mathbf{x} \mid \omega_{i}\right), \forall \omega_{i} \in \Omega$, on the basis of the available training set $T$. Statistical distance measures are usually adopted as they represent practical criteria to easily approximate the Bayes error. The commonly adopted measures to evaluate the separability between the distributions of two classes $\omega_{i}$ and $\omega_{j}$ are [9], [14]
Divergence:</p>
<p>$$
\operatorname{Div}<em _mathbf_x="\mathbf{x">{i j}(\boldsymbol{\theta})=\int</em>
$$}}\left{p\left(\mathbf{x} \mid \omega_{i}\right)-p\left(\mathbf{x} \mid \omega_{j}\right)\right} \ln \frac{p\left(\mathbf{x} \mid \omega_{i}\right)}{p\left(\mathbf{x} \mid \omega_{j}\right)} d \mathbf{x</p>
<p>Bhattacharyya distance:</p>
<p>$$
\mathrm{B}<em _mathbf_x="\mathbf{x">{i j}(\boldsymbol{\theta})=-\ln \left{\int</em>\right}
$$}} \sqrt{p\left(\mathbf{x} \mid \omega_{i}\right) p\left(\mathbf{x} \mid \omega_{j}\right)} d \mathbf{x</p>
<p>Jeffries-Matusita (JM) distance:</p>
<p>$$
\mathrm{JM}<em _mathbf_x="\mathbf{x">{i j}(\boldsymbol{\theta})=\left{\int</em>
$$}}\left[\sqrt{p\left(\mathbf{x} \mid \omega_{i}\right)}-\sqrt{p\left(\mathbf{x} \mid \omega_{j}\right)}\right]^{2} d \mathbf{x}\right}^{1 / 2</p>
<p>The JM distance can be rewritten according to the Bhattacharyya distance $B_{i j}$</p>
<p>$$
J M_{i j}(\boldsymbol{\theta})=\sqrt{2\left{1-\exp \left[-B_{i j}(\boldsymbol{\theta})\right]\right}}
$$</p>
<p>In multispectral and hyperspectral remote sensing images, the distributions of classes $p\left(\mathbf{x} \mid \omega_{i}\right), \omega_{i} \in \Omega$ are usually modeled with Gaussian functions with mean vectors $\mu_{i}$ and covariance matrices $\boldsymbol{\Sigma}_{i}$. Under this assumption, we can write</p>
<p>$$
\begin{aligned}
\operatorname{Div}<em i="i">{i j}(\boldsymbol{\theta})= &amp; \frac{1}{2} \operatorname{Tr}\left{\left(\boldsymbol{\Sigma}</em>}-\boldsymbol{\Sigma<em j="j">{j}\right)\left(\boldsymbol{\Sigma}</em>}^{-1}-\boldsymbol{\Sigma<em i="i">{i}^{-1}\right)\right} \
&amp; +\frac{1}{2} \operatorname{Tr}\left{\left(\boldsymbol{\Sigma}</em>}^{-1}-\boldsymbol{\Sigma<em i="i">{j}^{-1}\right)\left(\mu</em>\right}
\end{aligned}
$$}-\mu_{j}\right)\left(\mu_{i}-\mu_{j}\right)^{\mathrm{T}</p>
<p>$$
\begin{aligned}
\mathrm{B}<em i="i">{i j}(\boldsymbol{\theta})= &amp; \frac{1}{8}\left(\mu</em>}-\mu_{j}\right)^{\mathrm{T}}\left(\frac{\boldsymbol{\Sigma<em j="j">{i}+\boldsymbol{\Sigma}</em>\right) \
&amp; +\frac{1}{2} \ln \left(\frac{1}{2} \frac{\left|\boldsymbol{\Sigma}}}{2}\right)^{-1}\left(\mu_{i}-\mu_{j<em j="j">{i}+\boldsymbol{\Sigma}</em>}\right|}{\sqrt{\left|\boldsymbol{\Sigma<em j="j">{i}\right|\left|\boldsymbol{\Sigma}</em>\right)
\end{aligned}
$$}\right|}</p>
<p>where $\operatorname{Tr}{\cdot}$ is the trace of a matrix. An important drawback of the divergence is that its value quadratically increases with respect to the separation between the mean vectors of the class distributions. This behavior does not reflect the classification accuracy behavior, which asymptotically tends to one when the class distributions are perfectly separated. On the contrary, the JM distance exhibits a behavior that saturates when the separability between the two considered classes increases. For this reason, the JM distance is generally preferred to either the divergence or the Bhattacharyya distance.</p>
<p>The previously described measures evaluate the statistical distance between a pair of class distributions. In order to extend the separability measures to multiclass problems, a usually adopted separability indicator is obtained by computing the average distance among all pairwise distances. Thus, a multiclass separability measure can be defined as</p>
<p>$$
\Delta(\boldsymbol{\theta})=\sum_{i=1}^{C} \sum_{j&gt;i}^{C} P\left(\omega_{i}\right) P\left(\omega_{j}\right) S_{i j}(\boldsymbol{\theta})
$$</p>
<p>where $S_{i j}(\boldsymbol{\theta})$ is a statistical distance measure (e.g., Bhattacharyya distance, divergence, and JM distance) between the distributions $p\left(\mathbf{x} \mid \omega_{i}\right)$ and $p\left(\mathbf{x} \mid \omega_{j}\right)$ of the two classes $\omega_{i}$ and $\omega_{j}$, and $P\left(\omega_{i}\right)$ and $P\left(\omega_{j}\right)$ are the prior probabilities of the classes $\omega_{i}$ and $\omega_{j}$ in the considered scene, respectively.</p>
<p>Other measures adopted for feature selection are based on scatter matrices that allow one to characterize the variance within classes and between classes [14]. Using these measures, the canonical analysis aims at maximizing the ratio between among-class variance and within-class variance, resulting in the selection of features that simultaneously exhibit both requirements, i.e., high among-class variance and low within-class variance. Another example of indicator that can be adopted as criterion function is the mutual information, which measures the mutual dependence of two random variables. In the context of feature selection, the mutual information can be used to assess the capability of the considered feature vector $\mathbf{x}<em i="i">{i} \in \boldsymbol{\theta}$ to predict the correct class label $y</em> \in \Omega \forall i=1,2, \ldots, l$. To this purpose, a definition of the mutual information that considers the discrete nature of $y$ should be adopted (for deeper insight on feature selection based on mutual information, we refer the reader to [23] and [24]).</p>
<h2>B. Search Strategies</h2>
<p>In order to select the final subset of features that optimizes the adopted criterion function, a search strategy is needed. The search strategy generates possible solutions of the featureselection algorithm and compares them by applying the criterion function as a measure of the effectiveness of each solution. An exhaustive search for the optimal solution involves the evaluation and comparison of the criterion function for all</p>
<p>$\binom{n}{m}$ possible combinations of features. This is an intractable problem from a computational point of view, even for low numbers of features [17]. The branch-and-bound method proposed by Narendra and Fukunaga [14], [15] is a widely used approach to compute the globally optimum solution for monotonic criterion function without explicitly exploring all possible combinations of features. Nevertheless, the computational saving is not sufficient for treating problems with hundreds of features. Therefore, in the case of feature selection for hyperspectral data classification, suboptimal approaches should be adopted. Several suboptimal search strategies have been proposed in the literature. The simplest suboptimal search strategies are the sequential forward selection (SFS) and the sequential backward selection (SBS) techniques [16], [17]. A serious drawback of both algorithms is that they do not allow backtracking. In the case of the SFS algorithm, once the features have been selected, they cannot be discarded. Similarly, in the case of the SBS search technique, once the features have been discarded, they cannot be added again to the subset of selected features. Two effective sequential search methods are those proposed by Pudil et al. [16], namely, the sequential forward floating selection (SFFS) method and the sequential backward floating selection (SBFS) method. They improve the standard SFS and SBS techniques by dynamically changing the number of features included (SFFS) or removed (SBFS) to the subset of selected features at each step, thus allowing the reconsideration of the features included or removed at the previous steps. Other effective strategies are those proposed in [12], where two search algorithms are presented (i.e., the steepest ascent and the fast constrained search), which are based on the formalization of the feature-selection problem in the framework of a discrete optimization problem in an adequately defined binary multidimensional space.</p>
<p>An alternative approach to the exploration of the feature space that is relevant to this paper is that based on genetic algorithms (GAs), whose application to feature-selection problems was proposed in [25]. Genetic algorithms exploit an analogy with biology, in which a group of solutions, encoded as chromosomes, evolve via natural selection [26]. A standard GA starts by randomly creating an initial population (with a predefined size). Solutions are then combined via a crossover operator to produce offspring, thus expanding the current population. The individuals in the population are evaluated according to the criterion function, and the individuals that less fit such a function are discarded to return the population to its original size. A mutation operator is generally applied in order to increase individuals' variations. The processes of crossover, evaluation, and selection are repeated for a predetermined number of generations (if no other stop criterion is met before) in order to reach a satisfactory solution. Several papers confirmed the effectiveness of GAs for standard featureselection approaches (e.g., [27]-[29]), also for hyperdimensional feature space. Moreover, as it will be explained later, GAs become particularly relevant for this paper as they are effective when the criterion function involves multiple concurrent terms, and therefore, a multiobjective problem has to be optimized in order to estimate the Pareto-optimal solutions [30], [31].</p>
<h2>III. Proposed Feature-Selection Approach</h2>
<p>The main idea and novelty of the approach that we propose in this paper is to explicitly consider in the criterion function of the feature-selection process the spatial variability of the features (e.g., of the spectral signatures) on each land-cover class in the investigated scene, together with their discrimination capability. This results in the possibility to select a subset of features that exhibits both high capability to discriminate among different classes and high invariance in the spatial domain. The resulting subset of selected features implicitly improves the generalization capability in the classification process, which results in augmented robustness and accuracy in the classification of hyperspectral images with respect to feature subsets selected with standard methods. This property is particularly relevant when the considered scene is extended over large geographical areas and/or presents considerable intraclass variability of the spectral signatures.</p>
<p>From a formal viewpoint, the aim of the proposed approach is to select the subset $\boldsymbol{\theta}^{*} \subset \Upsilon$ of $m$ features (with $m&lt;n$ ) that optimizes a novel criterion function made up of two measures that characterize the following: 1) the capability of the subset of features to discriminate among the considered classes in $\Omega$ and 2) the spatial invariance (stationary behavior) of the selected features. The first measure can be evaluated with standard statistical separability indices (as described in the previous section), whereas the spatial invariance property is evaluated according to a novel invariance measure that represents an important contribution of this paper. In particular, we propose two possible methods to evaluate the invariance of a subset of features: 1) a supervised method and 2) a semisupervised method. The supervised method relies on the assumption that the available training set $T$ is made up of two subsets of labeled patterns $T_{1}$ and $T_{2}$ (such that $T_{1} \cup T_{2}=T$ and $T_{1} \cap$ $T_{2}=\varnothing$ ) collected on disjoint (separate) areas on the ground. This property of the training set is exploited for assessing the spatial variability of the spectral signatures of the land-cover classes. We successively relax this hypothesis by proposing a semisupervised method that does not require the availability of a training subset $T_{2}$ spatially disjoint from $T_{1}$ (only a standard training set $T \equiv T_{1}$ acquired in a single area of the scene is needed) and takes advantage of unlabeled samples. This second method is based on an estimation of the distributions of classes in portions of the image separate from $T$, which is carried out by exploiting the information captured from unlabeled pixels. The final subset of features is selected by jointly optimizing the two concurrent terms of the criterion function. This is done by defining a proper search strategy based on the optimization of a multiobjective problem for deriving the subsets of features that exhibit the best tradeoff between the two concurrent objectives.</p>
<p>In the following sections, we present the proposed supervised and semisupervised methods for the evaluation of the criterion function. Then, we describe the proposed multiobjective search strategy for deriving the final subsets of features that exhibit both the aforementioned properties (which can be assessed with either the supervised or the semisupervised method, depending on the available reference data).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Examples of feature subsets with different invariant (stationary) behaviors on two disjoint sets $T_{1}$ and $T_{2}$. (a) Feature subset that exhibits high separability and high invariance properties. (b) Feature subset with high separability on $T_{1}$ and high variability between $T_{1}$ and $T_{2}$.</p>
<h2>A. Supervised Formulation of the Proposed Criterion Function</h2>
<p>Let us first assume the availability of two subsets of labeled patterns $T_{1}$ and $T_{2}$ collected on disjoint areas on the ground (thus representing two different realizations of the class distributions). Under this assumption, we can define a novel criterion function that is based on two different terms: 1) a term that measures the class separability (discrimination term) and 2) a term that evaluates the spatial invariance of the investigated features (invariance term).</p>
<p>1) Discrimination Term $\Delta$ : This term is based on a standard feature-selection criterion function. In the proposed system, we adopt the definition given in (7), where the term $\Delta(\boldsymbol{\theta})$ evaluates the average measure of distance between all couples of class distributions $p\left(\mathbf{x} \mid \omega_{i}\right)$ and $p\left(\mathbf{x} \mid \omega_{j}\right) \forall \omega_{i}, \omega_{j} \in \Omega$ and $i&lt;j$. This term depends on the selected subset $\boldsymbol{\theta}$ of features, and the subset of $m$ features $\boldsymbol{\theta}^{*}$ that maximizes this distance results in the best potential for discriminating land-cover classes in the area modeled by the training samples. It is important to note that the evaluation of the aforementioned term is usually performed by assuming Gaussian distributions of classes for calculating the statistical distance $S_{i j}(\boldsymbol{\theta})$. Under this assumption, also in the presence of two disjoint training sets, it is preferable to evaluate the discrimination term by considering only one subset of the training set ( $T_{1}$ or $T_{2}$ ). This can be explained by considering that mixing up the two available training subsets $T_{1}$ and $T_{2}$ would result in mixing together two different realizations of the feature distributions, which, from a theoretical perspective, cannot be correctly modeled with Gaussian (monomodal) distributions.
2) Invariance Term P: In order to introduce the invariance term, let us first consider Fig. 1. This figure shows a qualitative example in a 2-D feature space of two subsets of features that exhibit different behavior of the samples extracted from different portions of a scene. The features of Fig. 1(a) present good capability to separate the class clusters and also exhibit high invariance on the two considered training sets. These properties allow the supervised algorithm to derive a robust classification rule, resulting in the capability to accurately classify samples that can be localized in both areas from which the samples of $T_{1}$ and $T_{2}$ are extracted. On the contrary, the features adopted in Fig. 1(b) exhibit good separability properties but low invariance. This feature subset leads the supervised learner to derive a classification rule that is not robust, resulting in poor classification accuracy in spatially disjoint areas.</p>
<p>The different behavior between the feature subsets in Fig. 1(a) and (b) can be modeled by considering the distance between the clusters that refer to the same land-cover class in the two disjoint training sets $T_{1}$ and $T_{2}$. Thus, we can introduce a novel term to explicitly measure the invariance (stationary behavior) of features on each class in the investigated image. It can be defined as</p>
<p>$$
\mathrm{P}(\boldsymbol{\theta})=\frac{1}{2} \sum_{i=1}^{C} P^{T_{1}}\left(\omega_{i}\right) P^{T_{2}}\left(\omega_{i}\right) S_{i i}^{T_{1} T_{2}}(\boldsymbol{\theta})
$$</p>
<p>where $S_{i i}^{T_{1} T_{2}}$ is a statistical distance measure between the distributions $p^{T_{r}}\left(\mathbf{x} \mid \omega_{i}\right), r=1,2$, of the class $\omega_{i}$ computed on $T_{1}$ and $T_{2}$, and $P^{T_{r}}\left(\omega_{i}\right)$ represents the prior probability of the class $\omega_{i}$ in $T_{r}, r=1,2$. This term evaluates the average distance between the distributions of the same class in different portions of the scene (i.e., on the two disjoint subsets of the training set). Unlike for $\Delta(\boldsymbol{\theta})$, we expect that a good (i.e., robust) subset of features should minimize the value of $\mathrm{P}(\boldsymbol{\theta})$. The computation of $\mathrm{P}(\boldsymbol{\theta})$ can be easily extended to more than two training subsets if labeled data collected on more than two disjoint regions are available. In the general case, when $R$ spatially disjoint training sets are available, the invariance term can be defined as follows:</p>
<p>$$
\mathrm{P}(\boldsymbol{\theta})=\frac{1}{R} \sum_{a=1}^{R} \sum_{b&gt;a}^{R} \sum_{i=1}^{C} P^{T_{a}}\left(\omega_{i}\right) P^{T_{b}}\left(\omega_{i}\right) S_{i i}^{T_{a} T_{b}}(\boldsymbol{\theta})
$$</p>
<p>The process of selection of features that jointly optimize the discrimination term $\Delta(\boldsymbol{\theta})$ and the invariance term $\mathrm{P}(\boldsymbol{\theta})$ will be described in Section III-C.</p>
<h2>B. Semisupervised Evaluation of the Criterion Function (Invariance Term Estimation)</h2>
<p>The collection of labeled training samples on two (or more) spatially disjoint areas from the site under investigation can be difficult and/or very expensive. This may compromise the applicability of the proposed supervised method in some real classification applications. In order to overcome this possible problem, in this section, we propose a semisupervised technique to estimate the invariance term defined in (8), which does not require the availability of a disjoint training subset $T_{2}$. Here, we only assume that a training set $T_{1}$ is available, and we consider a set of unlabeled pixels $U=\left{\mathbf{x}<em 2="2">{1}, \mathbf{x}</em>}, \ldots, \mathbf{x<em 1="1">{n}\right} \in \mathcal{I}$ (subset of the original image $\mathcal{I}$ ) that should satisfy two requirements: 1) $U$ contains samples of all the considered classes, and 2) samples in $U$ should be taken from portions of the scene separated from those on which the training samples $T</em>$. It is worth noting that, in the proposed algorithm, the labels of classes are not required. We only assume that the unlabeled samples are collected according to a strategy that can implicitly consider all classes present in the scene.}$ are collected. The set $U$ can be defined in either of the following ways: 1) by manually selecting clusters of pixels on a portion of the considered scene; 2) by randomly subsampling a set of pixels; or 3) by considering the whole image $\mathcal{I</p>
<p>The method is based on the semisupervised estimation of the terms $P^{U}\left(\omega_{i}\right)$ and $p^{U}\left(\mathbf{x} \mid \omega_{i}\right), \omega_{i} \in \Omega$, which, in this case, characterize the prior probabilities and the conditional probability density functions in the disjoint area corresponding to the pixels in $U$, respectively. The distribution of the samples in $U$ can be described by the following mixture model:</p>
<p>$$
p^{U}(\mathbf{x})=\sum_{i=1}^{C} P^{U}\left(\omega_{i}\right) p^{U}\left(\mathbf{x} \mid \omega_{i}\right)
$$</p>
<p>We assume that $P^{U}\left(\omega_{i}\right)$ and $p^{U}\left(\mathbf{x} \mid \omega_{i}\right)$ are not known, while $p^{U}(\mathbf{x})$ is given from the data distribution. However, despite the expected variability, for each class $\omega_{i} \in \Omega$, the initial values of both the prior probability $P^{U}\left(\omega_{i}\right)$ and the conditional density function $p^{U}\left(\mathbf{x} \mid \omega_{i}\right)$ can be roughly approximated by the prior and the conditional density function in $T_{1}$, i.e.,</p>
<p>$$
P^{U, 0}\left(\omega_{i}\right)=P^{T_{1}}\left(\omega_{i}\right) \quad p^{U, 0}\left(\mathbf{x} \mid \omega_{i}\right)=p^{T_{1}}\left(\mathbf{x} \mid \omega_{i}\right)
$$</p>
<p>The problem can be addressed by estimating the parameter vector $\mathbf{J}=\left[P^{U}\left(\omega_{i}\right), \delta_{i}\right]<em i="i">{i=1}^{C}$, where each component $\delta</em>)\right]$ defined as}$ represents the vector of parameters that characterize the density function $p^{U}\left(\mathbf{x} \mid \omega_{i}\right)$, which, given its dependence from $\delta_{i}$, can be rewritten as $p^{U}\left(\mathbf{x} \mid \omega_{i}, \delta_{i}\right)$. The components of $\mathbf{J}$ can be estimated by maximizing the pseudo log-likelihood function $L\left[p^{U}(\mathbf{x</p>
<p>$$
L\left[p^{U}(\mathbf{x}) \mid \mathbf{J}\right]=\sum_{j=1}^{m} \log \left{\sum_{i=1}^{C} P^{U}\left(\omega_{i} \mid \mathbf{J}\right) p^{U}\left(\mathbf{x} \mid \omega_{i}, \mathbf{J}\right)\right}
$$</p>
<p>The maximization of the log-likelihood function can be obtained with the expectation-maximization (EM) algorithm [32]. The EM algorithm consists of two main steps: an expectation step and a maximization step. The two steps are iterated, so that the value of the log-likelihood function $L\left[p^{U}(\mathbf{x})\right]$ increases at each iteration, until a local maximum is reached. For simplicity, let us consider that all the classes $\omega_{i} \in \Omega$ are Gaussian distributed. Under this assumption, the density function associated with each class $\omega_{i}$ can be completely described by the mean vector $\mu_{i}^{U}$ and the covariance matrix $\Sigma_{i}^{U}, i=1, \ldots, C$. Therefore, the parameter vector to be estimated becomes</p>
<p>$$
\mathbf{J}=\left[P^{U}\left(\omega_{i}\right), \mu_{i}^{U}, \Sigma_{i}^{U}\right]_{i=1}^{C}
$$</p>
<p>It can be proven that the equations to be used at iteration $s+1$ for estimating the statistical terms associated with a generic class $\omega_{i}$ are the following [3], [32], [33]:</p>
<p>$$
\begin{aligned}
P^{U, s+1}\left(\omega_{i}\right) &amp; =\frac{1}{m} \sum_{\mathbf{x}<em i="i">{j} \in U} \frac{P^{U, s}\left(\omega</em>}\right) p^{U, s}\left(\mathbf{x<em i="i">{j} \mid \omega</em>}\right)}{p^{U, s}\left(\mathbf{x<em i="i">{j}\right)} \
{\left[\mu</em>}^{U}\right]^{s+1}} &amp; =\frac{\sum_{\mathbf{x<em i="i">{j} \in U} \frac{P^{U, s}\left(\omega</em>}\right) p^{U, s}\left(\mathbf{x<em i="i">{j} \mid \omega</em>}\right)}{p^{U, s}\left(\mathbf{x<em j="j">{j}\right)} \mathbf{x}</em>}}{\sum_{\mathbf{x<em i="i">{j} \in U} \frac{P^{U, s}\left(\omega</em>}\right) p^{U, s}\left(\mathbf{x<em i="i">{j} \mid \omega</em>}\right)}{p^{U, s}\left(\mathbf{x<em i="i">{j}\right)}} \
{\left[\Sigma</em>}^{U}\right]^{s+1}} &amp; =\frac{\sum_{\mathbf{x<em i="i">{j} \in U} \frac{P^{U, s}\left(\omega</em>}\right) p^{U, s}\left(\mathbf{x<em i="i">{j} \mid \omega</em>}\right)}{p^{U, s}\left(\mathbf{x<em j="j">{j}\right)}\left{\mathbf{x}</em>}-\left[\mu_{i}^{U}\right]^{s+1}\right}^{2}}{\sum_{\mathbf{x<em i="i">{j} \in U} \frac{P^{U, s}\left(\omega</em>}\right) p^{U, s}\left(\mathbf{x<em i="i">{j} \mid \omega</em>
\end{aligned}
$$}\right)}{p^{U, s}\left(\mathbf{x}_{j}\right)}</p>
<p>where the superscripts $s$ and $s+1$ refer to the values of the parameters at the $s$ th and $s+1$ th iterations, respectively. The estimates of the statistical parameters that describe the class distributions in the disjoint areas are obtained starting from the initial values of the parameters [see (11)] and iterating (14)-(16) up to convergence. An important aspect of the EM algorithm concerns its convergence properties. It is not possible to guarantee that the algorithm will converge to the global maximum of the log-likelihood function, although convergence to a local maximum can be ensured. A detailed description of the EM algorithm is beyond the scope of this paper, so we refer the reader to the literature for a more detailed analysis of such an algorithm and its properties [3], [32]. The final estimates obtained at convergence for each class $\omega_{i} \in \Omega$, i.e., $\hat{P}^{U}\left(\omega_{i}\right)$, and $\hat{p}^{U}\left(\mathbf{x} \mid \omega_{i}\right)$ (which depend on the estimated parameters $\hat{\mu}<em i="i">{i}^{U}$ and $\hat{\Sigma}</em>$ considered. Thus, the semisupervised estimation of the invariance term becomes}^{U}$ ) can be used in place of $P^{T_{2}}\left(\omega_{i}\right)$ and $p^{T_{2}}\left(\mathbf{x} \mid \omega_{i}\right)$ to estimate the invariance term $\hat{\mathrm{P}}(\boldsymbol{\theta})$ for each subset of features $\boldsymbol{\theta</p>
<p>$$
\hat{\mathrm{P}}(\boldsymbol{\theta})=\frac{1}{2} \sum_{i=1}^{C} P^{T_{1}}\left(\omega_{i}\right) \hat{P}^{U}\left(\omega_{i}\right) \hat{S}<em 1="1">{i i}^{T</em>)
$$} U}(\boldsymbol{\theta</p>
<p>The discrimination term $\Delta(\boldsymbol{\theta})$ can be calculated as in (7) with no difference with respect to the supervised method.</p>
<p>It is worth noting that, depending on the adopted set $U$ of unlabeled pixels, the estimation of the prior probabilities and the class-conditional densities can reflect with different degree of accuracy the true values. In particular, the estimation of the elements of the covariance matrices $\Sigma_{i}^{U}, i=1, \ldots, C$, may become critical in some cases when the number of classes is high. Thus, in these cases, since small fluctuations in the accuracy of the estimation of the covariance terms $\hat{\Sigma}<em i="i">{i}^{U}, i=1, \ldots, C$, can strongly affect the invariance term values, the estimation of the invariance term can be simplified in the following ways: 1) by assuming that the covariance matrix is diagonal and 2) by considering only the first-order statistical moment (thus neglecting the second-order moments) for the evaluation of the statistical distance $\hat{S}</em>)$.}^{T_{1} U}(\boldsymbol{\theta</p>
<h2>C. Proposed Multiobjective Search Strategy</h2>
<p>Given the proposed criterion function that is made up of the discrimination term $\Delta(\boldsymbol{\theta})$ and the invariance term $\mathrm{P}(\boldsymbol{\theta})$ (which, depending on the available reference data, can be evaluated with the supervised or unsupervised methods, as described in the previous two sections), we address now the problem of defining a search strategy to select the subset (or the subsets) of features that (jointly) optimize(s) the two defined measures. To this purpose, one can define a global optimization function as</p>
<p>$$
V(\boldsymbol{\theta})=\Delta(\boldsymbol{\theta})+K \cdot f[\mathrm{P}(\boldsymbol{\theta})]
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Example of Pareto-optimal solutions and dominated solution in a twoobjective search space.
where $K$ tunes the tradeoff between discrimination ability and invariance of the selected subset of features and $f$ is the monotonic decreasing function of $\mathrm{P}(\boldsymbol{\theta})$. The subset $\boldsymbol{\theta}^{*}$ of $m$ features for which $V(\boldsymbol{\theta})$ has the maximum value represents the solution to the considered problem.</p>
<p>Nevertheless, the aforementioned formulation of the problem has two drawbacks: 1) the obtained criterion function is not monotonic (and thus, effective search algorithms based on this property cannot be used), and 2) the definition of $f$ and $K$ (which should be carried out empirically) affects significantly the final result. To overcome these drawbacks, we modeled this problem as a multiobjective minimization problem, where the multiobjective function $\mathbf{g}(\boldsymbol{\theta})$ is made up of two different (and possibly conflicting) objectives $g_{1}(\boldsymbol{\theta})$ and $g_{2}(\boldsymbol{\theta})$, which express the discrimination ability $\Delta(\boldsymbol{\theta})$ among the considered classes and the spatial invariance $\mathrm{P}(\boldsymbol{\theta})$ of the subset of features $\boldsymbol{\theta}$, respectively. The multiobjective problem can therefore be formulated as follows:</p>
<p>$$
\begin{gathered}
\min <em 1="1">{|\boldsymbol{\theta}|=m}{\mathbf{g}(\boldsymbol{\theta})} \
\text { where } \mathbf{g}(\boldsymbol{\theta})=\left[g</em>)]
\end{gathered}
$$}(\boldsymbol{\theta}), g_{2}(\boldsymbol{\theta})\right]=[-\Delta(\boldsymbol{\theta}), \mathrm{P}(\boldsymbol{\theta</p>
<p>where $|\boldsymbol{\theta}|$ is the cardinality of the subset $\boldsymbol{\theta}$, i.e., the number of features $m$ to be selected from the originally available $n$. This problem is solved in order to obtain a set of Pareto-optimal solutions $O^{<em>}$ instead of a single optimal one. In greater detail, a solution $\boldsymbol{\theta}^{</em>}$ is said to be Pareto optimal if it is not dominated by any other solution in the search space, i.e., there is no other $\boldsymbol{\theta}$ such that $g_{i}(\boldsymbol{\theta}) \leq g_{i}\left(\boldsymbol{\theta}^{<em>}\right)(\forall i=1,2)$ and $g_{j}(\boldsymbol{\theta})&lt;g_{j}\left(\boldsymbol{\theta}^{</em>}\right)$ for at least one $j(\forall j=1,2)$. This means that $\boldsymbol{\theta}^{<em>}$ is Pareto optimal if there exists no other subset of features $\boldsymbol{\theta}$ that would decrease an objective without simultaneously increasing the other one (Fig. 2 clarifies this concept with a graphical example). The set $O^{</em>}$ of all optimal solutions is called Pareto-optimal set. The plot of the objective function of all solutions in the Pareto-optimal set is called Pareto front $\mathrm{PF}^{<em>}=\left{\mathbf{g}(\boldsymbol{\theta}) \mid \boldsymbol{\theta} \in O^{</em>}\right}$. Because of the complexity of the search space, an exhaustive search of the set of optimal solutions $O^{<em>}$ is unfeasible. Thus, instead of identifying the true set of optimal solutions, we aim to estimate a set of nondominated solutions $\tilde{O}^{</em>}$ with objective values as close as possible to the Pareto front. This estimation can be achieved with different multiobjective optimization algorithms (e.g., multiobjective evolutionary algorithms).</p>
<p>The main advantage of the multiobjective approach is that it avoids aggregation of metrics capturing multiple objectives into a single measure. On the contrary, it allows one to effectively identify different possible tradeoffs between the values of $\Delta(\boldsymbol{\theta})$ and $\mathrm{P}(\boldsymbol{\theta})$. This results in the possibility to evaluate in a more flexible way the tradeoffs between discrimination ability among classes and spatial invariance of each feature subset and to identify the subsets of features that simultaneously exhibit both properties. In particular, we expect that the most robust subsets of features (which will result in the best generalization capability of the classification system) are represented by the solutions that are localized close to the knee of the estimated Pareto front (or the solutions closest to the origin of the search space).</p>
<h2>IV. Data Set Description and DESIGN OF EXPERIMENTS</h2>
<p>In order to assess the effectiveness of the presented approach (with both the proposed supervised and semisupervised methods), we carried out several experiments on a hyperspectral image acquired over an extended geographical area. We considered a data set that is increasingly used as a benchmark in the literature and consists of data acquired by the Hyperion sensor of the Earth Observing 1 (EO-1) satellite in an area of the Okavango Delta, Botswana. The Hyperion sensor on EO-1 acquired the hyperspectral image with a spatial resolution of 30 m over a $7.7-\mathrm{km}$ strip in 242 bands. Uncalibrated and noisy bands that cover water absorption range of the spectrum were removed, and the remaining 145 bands were given as input to the feature-selection technique. For more details on this data set, we refer the reader to [34]. The labeled reference samples were collected on two different and spatially disjoint areas (Area 1 and Area 2), thus representing possible spatial variabilities of the spectral signatures of classes. The samples taken on the first area were partitioned into a training set $T_{1}$ and a test set $T S_{1}$ by random sampling (these sets represent similar realizations of the spectral signatures of classes). The samples taken on the second area were used to derive a training set $T_{2}$ and a test set $T S_{2}$ according to the same procedure used for the samples of the first considered area (these two sets present possible variability in class distributions with respect to the first two sets). The numbers of labeled reference samples for each set and class are reported in Table I. After preliminary experiments were carried out in order to understand the size of the subset of features that led to the saturation of the classification accuracies, we performed different experiments (with both the supervised and semisupervised methods) by varying the size $m$ of the selected subset of features in a range between 6 and 14 with step 2. The obtained subsets of features were used to perform the classification with a Gaussian maximum-likelihood (ML) classifier. The training of the ML classifier (estimation of Gaussian parameters for class-conditional densities) was carried out using the training set $T_{1}$. We compared the classification accuracies obtained on both test sets $T S_{1}$ and $T S_{2}$ performing the feature selection with the following: 1) the proposed approach with the supervised method for the estimation of the invariance term; 2) the proposed semisupervised method</p>
<p>TABLE I
NUMBER OF TRAINING ( $T_{1}$ AND $T_{2}$ ) AND TEST ( $T S_{1}$ AND $T S_{2}$ ) PATTERNS ACQUIRED IN THE TWO SPATIALLY DISJOINT AREAS</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Class</th>
<th style="text-align: center;">Number of samples</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Area 1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Area 2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{T}_{\boldsymbol{I}}$</td>
<td style="text-align: center;">$\boldsymbol{T S}_{\boldsymbol{I}}$</td>
<td style="text-align: center;">$\boldsymbol{T}_{\boldsymbol{2}}$</td>
<td style="text-align: center;">$\boldsymbol{T S}_{\boldsymbol{2}}$</td>
</tr>
<tr>
<td style="text-align: center;">Water</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">213</td>
<td style="text-align: center;">57</td>
</tr>
<tr>
<td style="text-align: center;">Hippo grass</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: center;">Floodplain grasses1</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">199</td>
<td style="text-align: center;">52</td>
</tr>
<tr>
<td style="text-align: center;">Floodplain grasses2</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">169</td>
<td style="text-align: center;">46</td>
</tr>
<tr>
<td style="text-align: center;">Reeds1</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">219</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Riparian</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">109</td>
<td style="text-align: center;">221</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: center;">Firescar2</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">215</td>
<td style="text-align: center;">44</td>
</tr>
<tr>
<td style="text-align: center;">Island interior</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">166</td>
<td style="text-align: center;">37</td>
</tr>
<tr>
<td style="text-align: center;">Acacia woodlands</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">253</td>
<td style="text-align: center;">61</td>
</tr>
<tr>
<td style="text-align: center;">Acacia shrublands</td>
<td style="text-align: center;">101</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">202</td>
<td style="text-align: center;">46</td>
</tr>
<tr>
<td style="text-align: center;">Acacia grasslands</td>
<td style="text-align: center;">184</td>
<td style="text-align: center;">174</td>
<td style="text-align: center;">243</td>
<td style="text-align: center;">62</td>
</tr>
<tr>
<td style="text-align: center;">Short mopane</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">154</td>
<td style="text-align: center;">27</td>
</tr>
<tr>
<td style="text-align: center;">Mixed mopane</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">203</td>
<td style="text-align: center;">65</td>
</tr>
<tr>
<td style="text-align: center;">Exposed soil</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">1242</td>
<td style="text-align: center;">1252</td>
<td style="text-align: center;">2621</td>
<td style="text-align: center;">627</td>
</tr>
</tbody>
</table>
<p>for estimating the invariance term; and 3) a standard featureselection technique that considers only the discrimination term.</p>
<p>The experiments with the supervised feature-selection method were carried out by considering the training set $T_{1}$ for the evaluation of the discrimination term $\Delta(\boldsymbol{\theta})$ and both $T_{1}$ and $T_{2}$ for the evaluation of the invariance term $\mathrm{P}(\boldsymbol{\theta})$. In our implementation, we adopted the JM distance (under the Gaussian assumption for the distribution of classes) as a statistical distance measure for both considered terms. The second set of experiments was carried out with the proposed semisupervised feature-selection method. In these experiments, we considered the training set $T_{1}$ for the evaluation of the discriminative term $\Delta(\boldsymbol{\theta})$, while the invariance term $\hat{\mathrm{P}}(\boldsymbol{\theta})$ was estimated from $T_{1}$ and the samples of $T_{2}$, which were used without their class label information as set $U$. For simplicity, we considered only the first-order moment to evaluate the statistical distance $\hat{S}<em 1="1">{i i}^{T</em>$ (see Section II-A).} U}(\boldsymbol{\theta})$ (see the discussion in Section II-A). The standard feature selection was performed by selecting the subsets of features that maximize the JM distance on the training set $T_{1}$ with a (mono-objective) GA. Note that we did not mix up the two training sets $T_{1}$ and $T_{2}$ for both training the ML classifiers and evaluating the discrimination term, as the Gaussian approximation is no more reasonable for the two different Gaussian realizations of each class in $T_{1}$ and $T_{2</p>
<p>In order to solve the defined two-objective minimization problem for the proposed methods (i.e., estimating the Paretooptimal solutions), we implemented a modification of the "nondominated sorting in genetic algorithm II" (NSGA-II) [31]. The original algorithm was modified in order to avoid solutions with multiple selections of the same feature. This has been accomplished by changing the random initialization of the chromosome population and by modifying the crossover and mutation operators. In all the experiments, the population size was set equal to 100 , and the maximum number of generations was set equal to 50 . The classification was carried out using all combinations of features $\hat{\boldsymbol{\theta}}^{<em>} \in \hat{O}^{</em>}$ that lie on the estimated Pareto front, and the subset $\hat{\boldsymbol{\theta}}^{*}$ that resulted in the highest
accuracy on the disjoint test set $T S_{2}$ was finally selected. For the mono-objective GA, we adopted the same values for both the population size and the maximum number of generations as for the multiobjective GA.</p>
<h2>V. EXPERIMENTAL RESULTS</h2>
<h2>A. Results With the Supervised Method for the Estimation of the Invariance Term</h2>
<p>We first present the experimental results obtained with the proposed supervised method that allows us to derive important considerations about the validity of the proposed approach with respect to the standard one. In order to show the shortcomings of standard feature-selection algorithms for the classification of hyperspectral images, Fig. 3 shows the graphs of the accuracy obtained by the ML classifier on the adjoint $\left(T S_{1}\right)$ and disjoint $\left(T S_{2}\right)$ test sets versus the values of the discrimination term $\Delta(\boldsymbol{\theta})$ for different subsets of features. For the reported graphs, we used the solutions on the Pareto front estimated by the modified NSGA-II algorithm applied to the multiobjective minimization problem in (19), in the cases of six and eight features (these two cases are selected as examples; the other considered cases led to similar results). From this figure, it is possible to observe that the accuracy on $T S_{1}$ increases when the discrimination term increases, whereas the accuracy on $T S_{2}$ increases only until a certain value and then it decreases. Therefore, the simple maximization of the discrimination term (as standard approaches do) can lead to an overfitting phenomenon, which results in poor generalization capabilities, i.e., low capability to discriminate and correctly classify the landcover classes in areas of the scene different from that associated with the collected training data. This confirms the significant variability of the spectral signature of classes in hyperspectral images.</p>
<p>The aim of the proposed approach is to overcome this problem. Let us now consider Fig. 4 that shows the Pareto fronts estimated by the proposed approach (employing the modified NSGA-II algorithm) in the cases of the selection of six and eight features. This figure represents the information of the kappa coefficient of accuracy, which is obtained by the classification of the test sets $T S_{1}$ and $T S_{2}$ with the considered subset of features $\hat{\boldsymbol{\theta}}^{*}$, as the color of the point, according to the reported color scale bar. The diagrams in Fig. 4(a)-(c) show that, for the classification of $T S_{1}$, the solutions with higher discrimination capability [lower values of $-\Delta(\boldsymbol{\theta})$ ] result in better accuracies. This behavior reveals (as expected) that only the discrimination term is important for selecting the most effective feature subset for the classification of pixels acquired in a similar area of pixels in $T_{1}$ (in these conditions, training and test patterns represent the same realization of the statistical distributions of classes). On the contrary, the diagrams in Fig. 4(b)-(d) show that the most accurate solutions for the classification of the spatially disjoint samples of $T S_{2}$ (which result in the highest kappa coefficient of accuracy) are located in a middle region, close to the knee of the estimated Pareto front. This confirms the importance of the invariance term, and that tradeoff solutions between the two competing objectives $\Delta(\boldsymbol{\theta})$ and $\mathrm{P}(\boldsymbol{\theta})$ should be identified in order to select the subset of features that leads to</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Behaviors of the kappa coefficients of accuracy on the test sets $T S_1$ and $T S_2$ versus the values of the discrimination term $\Delta(\theta)$. Cases of (a) six and (b) eight features.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Pareto fronts estimated by the proposed approach with the supervised method. (a) and (b) Six-feature case. (c) and (d) Eight-feature case. The color indicates the kappa coefficient of accuracy on (a)–(c) $TS_1$ and (b)–(d) $TS_2$ according to the reported color scale bar.</p>
<p>better generalization capabilities and, thus, higher classification accuracy in areas of the hyperspectral image different from the training one.</p>
<p>Table II reports the comparison of the classification accuracies obtained on $TS_1$ and $TS_2$ by selecting the subset of features with the proposed multiobjective supervised and semisupervised methods, as well as the standard method. From this table, it is possible to observe that the obtained accuracy on the disjoint test set $TS_2$ are, in general, significantly lower than those obtained on the adjoint test set $TS_1$, confirming the presence of consistent variability in the spatial domain of the spectral signatures of the classes. This phenomenon severely challenges the generalization capability of the classification system. Nevertheless, we can observe that, for all considered cases, the proposed multiobjective feature-selection methods allowed one to significantly increase the accuracy on the test set $TS_2$ with respect to the standard method, while the accuracy on the adjoint test set $TS_1$ only slightly decreased. On average, the proposed supervised method resulted in an increase of the classification accuracy on the disjoint test set of 21.3% with respect to the standard approach, while it slightly decreased by 4.2% the accuracy on the adjoint test set.</p>
<p>The obtained results clearly confirm that the proposed approach is effective in exploiting the information of the two</p>
<p>TABLE II
Kappa Coefficient of Accuracies Obtained by the ML Classifier With the Features Selected by the Proposed Supervised and Semisupervised Methods and the Standard Approach</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Number of <br> features</th>
<th style="text-align: center;">Kappa coefficient of Accuracy on <br> (Test Set $\boldsymbol{T S}_{2}$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Kappa coefficient of Accuracy on <br> (Test Set $\boldsymbol{T S}_{3}$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Proposed <br> Semisup. <br> Method</td>
<td style="text-align: center;">Proposed <br> Supervised <br> method</td>
<td style="text-align: center;">Standard <br> method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Proposed <br> Semisup. <br> Method</td>
<td style="text-align: center;">Proposed <br> Supervised <br> method</td>
<td style="text-align: center;">Standard <br> method</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.931</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.939</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.938</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.942</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.954</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.953</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.944</td>
</tr>
</tbody>
</table>
<p>distinct available training sets to select subsets of robust and invariant features, which can improve the generalization capabilities of the classification system. We further observe that very few spectral channels (6-14 bands out of the originally available 145) are sufficient for effectively representing and discriminating the considered information classes, thus significantly reducing the problems associated with the Hughes phenomenon. The computational cost of the proposed supervised method is comparable with that of the standard mono-objective algorithm. In our experiments, which were carried out on a personal computer mounting an Intel Pentium D processor at 3.4 GHz and a 2-GB DDR2 RAM, the feature selection with the supervised multiobjective method took an average time of about 4 min , while the standard method took about 3 min . This is due to the fact that the evaluation of the discrimination term $\Delta(\boldsymbol{\theta})$ (which has to be computed also with standard feature-selection methods) requires a computational cost that is proportional to $C(C-1) / 2$, while the introduced invariance term $\mathrm{P}(\boldsymbol{\theta})$ has a computational cost that is proportional to $C$. Therefore, the additional cost due to the evaluation of the new term becomes lesser and lesser when the number of classes increases.</p>
<h2>B. Results With the Semisupervised Method for the Estimation of the Invariance Term</h2>
<p>Often, in real applications, a disjoint training set $T_{2}$ is not available to the user, and the proposed supervised method cannot be used. In these cases, the semisupervised approach can be adopted. It is worth noting that, from the perspective of the semisupervised method, the supervised technique represents an upper bound of the accuracy and generalization ability that can be obtained (if the same samples with and without labels are considered). Thus, in this case, the results presented in the previous section can be seen as the best performances that can be obtained on the considered samples.</p>
<p>As expected, the semisupervised method led to accuracies that were slightly lower than that of the supervised method, but it still maintained a significant improvement with respect to the traditional approach. On average, the semisupervised method increased the classification accuracy on $T S_{2}$ by $16.4 \%$ with respect to the standard feature-selection method, while it decreased the accuracy on $T S_{1}$ by $3.1 \%$. The small decrease in performances with respect to those obtained by the
supervised method is due to the approximate estimation of the invariance term carried out with the EM algorithm, which cannot ensure convergence to the optimal solution. However, the semisupervised method has the very important advantage to considerably increase the generalization capabilities of the classification systems with respect to the traditional approach without requiring additional reference data. The computation cost of this method is slightly higher with respect to the standard method, because of the time required by the EM algorithm to perform the estimation necessary to evaluate the invariance term. In our experiments, the average time for the feature selection with the semisupervised approach was about 60 min ( 15 times more than that with the supervised method).</p>
<h2>VI. CONCLUSION</h2>
<p>In this paper, we presented a novel feature-selection approach to the classification of hyperspectral images. The proposed approach aimed at selecting subsets of features that exhibited, at the same time, high discrimination ability and high spatial invariance, improving the robustness and the generalization properties of the classification system with respect to standard techniques. The feature selection was accomplished by defining a multiobjective criterion function that considered the evaluation of both a standard separability measure and a novel term that measured the spatial invariance of the selected features. In order to assess the invariance in the scene of the feature subset, we proposed both a supervised method (assuming the availability of training samples acquired in two or more spatially disjoint areas) and a semisupervised method (which required only a standard training set acquired in a single area of the scene and which exploited the information of unlabeled pixels in portions of the scene spatially disjoint from the training areas). The multiobjective problem was solved by an evolutionary algorithm for the estimation of the set of Pareto-optimal solutions.</p>
<p>Experimental results showed that the proposed featureselection approach selected subsets of the original features that sharply increased the classification accuracy on disjoint test samples, while it slightly decreased the accuracy on the adjoint test set with respect to standard methods. This behavior confirms that the proposed approach results in augmented generalization capability of the classification system. In this regard, we would like to stress the importance of evaluating the accuracy on a disjoint test set, because this allows one to estimate the</p>
<p>accuracy in the classification of the whole considered image. In particular, the proposed supervised method is effective in exploiting the information of the two available training sets, and the proposed semisupervised method can significantly increase the generalization capabilities of the classification system without requiring additional reference data with respect to traditional feature-selection algorithms. This can be achieved at the cost of an acceptable additional computational time.</p>
<p>It is important to note that the proposed approach is defined in a general way, thus allowing different possible implementations. For instance, the discrimination and invariance terms can be evaluated considering statistical distance measures that are different from those adopted in our experimental analysis, and other multiobjective optimization algorithms can be adopted as search strategy for estimating the Pareto-optimal solutions. This general definition of the approach results in the possibility of further developing the implementation that we adopted for our experimental analysis. As an example, as future developments of this paper, the proposed approach could be integrated with classification algorithms that are different from the adopted ML classifier, e.g., the SVM and/or other kernel-based classification techniques, for further improving the accuracy of the classification system. In addition, we think that the overall classification system can be further improved by jointly exploiting the proposed feature-selection approach and a semisupervised classification technique for a synergic and complete exploitation of the unlabeled-sample information.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>The authors would like to thank Prof. M. Crawford (Purdue University, West Lafayette, IN) for kindly providing the data set used in the experimental part of this paper. They would also like to thank Dr. A. Boni and Dr. A. Marconato for the valuable discussion on multiobjective optimization.</p>
<h2>REFERENCES</h2>
<p>[1] F. Melgani and L. Bruzzone, "Classification of hyperspectral remotesensing images with support vector machines," IEEE Trans. Geosci. Remote Sens., vol. 42, no. 8, pp. 1778-1790, Aug. 2004.
[2] G. Camps-Valls and L. Bruzzone, "Kernel-based methods for hyperspectral image classification," IEEE Trans. Geosci. Remote Sens., vol. 43, no. 6, pp. 1351-1362, Jun. 2005.
[3] B. M. Shahshahani and D. A. Landgrebe, "The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon," IEEE Trans. Geosci. Remote Sens., vol. 32, no. 5, pp. 1087-1095, Sep. 1994.
[4] L. Bruzzone, M. Chi, and M. Marconcini, "A novel transductive SVM for the semisupervised classification of remote-sensing images," IEEE Trans. Geosci. Remote Sens., vol. 44, no. 11, pp. 3363-3373, Nov. 2006.
[5] M. Chi and L. Bruzzone, "Semi-supervised classification of hyperspectral images by SVMs optimized in the primal," IEEE Trans. Geosci. Remote Sens., vol. 45, no. 6, pt. 2, pp. 1870-1880, Jun. 2007.
[6] G. F. Hughes, "On the mean accuracy of statistical pattern recognition," IEEE Trans. Inf. Theory, vol. IT-14, no. 1, pp. 55-63, Jan. 1968.
[7] V. N. Vapnik, The Nature of Statistical Learning Theory, 2nd ed. New York: Springer-Verlag, 2001.
[8] N. Cristianini and J. Shawe-Taylor, An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods. Cambridge, U.K.: Cambridge Univ. Press, 1995.
[9] J. A. Richards and X. Jia, Remote Sensing Digital Image Analysis, 4th ed. Berlin, Germany: Springer-Verlag, 2006.
[10] P. W. Mausel, W. J. Kramber, and J. K. Lee, "Optimum band selection for supervised classification of multispectral data," Photogramm. Eng. Remote Sens., vol. 56, no. 1, pp. 55-60, Jan. 1990.
[11] R. Archibald and G. Fann, "Feature selection and classification of hyperspectral images with support vector machines," IEEE Geosci. Remote Sens. Lett., vol. 4, no. 4, pp. 674-677, Oct. 2007.
[12] S. Serpico and L. Bruzzone, "A new search algorithm for feature selection in hyperspectral remote sensing images," IEEE Trans. Geosci. Remote Sens., vol. 39, no. 7, pp. 1360-1367, Jul. 2001.
[13] L. Bruzzone, F. Roli, and S. B. Serpico, "An extension of the Jeffreys-Matusita distance to multiclass cases for feature selection," IEEE Trans. Geosci. Remote Sens., vol. 33, no. 6, pp. 1318-1321, Nov. 1995.
[14] K. Fukunaga, Introduction to Statistical Pattern Recognition, 2nd ed. New York: Academic, 1990.
[15] P. M. Narendra and K. Fukunaga, "A branch and bound algorithm for feature subset selection," IEEE Trans. Comput., vol. C-26, no. 9, pp. 917-922, Sep. 1977.
[16] P. Pudil, J. Novovicova, and J. Kittler, "Floating search methods for feature selection," Pattern Recognit. Lett., vol. 15, no. 11, pp. 1119-1125, Nov. 1994.
[17] A. Jain and D. Zongker, "Feature selection: Evaluation, application, and small sample performance," IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 2, pp. 153-158, Feb. 1997.
[18] A. Hyvarinen and E. Oja, "Independent component analysis: Algorithms and applications," Neural Netw., vol. 13, no. 4/5, pp. 411-430, May/Jun. 2000.
[19] S. Serpico and G. Moser, "Extraction of spectral channels from hyperspectral images for classification purposes," IEEE Trans. Geosci. Remote Sens., vol. 45, no. 2, pp. 484-495, Feb. 2007.
[20] J. A. Benediktsson, M. Pesaresi, and K. Arnason, "Classification and feature extraction for remote sensing images from urban areas based on morphological transformations," IEEE Trans. Geosci. Remote Sens., vol. 41, no. 9, pp. 1940-1949, Sep. 2003.
[21] M. N. Do and M. Vetteri, "Wavelet-based texture retrieval using generalized Gaussian density and Kullback-Leibler distance," IEEE Trans. Image Process., vol. 11, no. 2, pp. 146-158, Feb. 2002.
[22] H. Liu and L. Yu, "Toward integrating feature selection algorithms for classification and clustering," IEEE Trans. Knowl. Data Eng., vol. 17, no. 4, pp. 491-502, Apr. 2005.
[23] H. Peng, F. Long, and C. Ding, "Feature selection based on mutual information: Criteria of max-dependency, max-relevance, and minredundancy," IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 8, pp. 1226-1238, Aug. 2005.
[24] B. Guo, R. I. Damper, S. R. Gunn, and J. D. B. Nelson, "A fast separability-based feature-selection method for high-dimensional remotely sensed image classification," Pattern Recognit., vol. 41, no. 5, pp. 1653-1662, May 2008.
[25] W. Siedlecki and J. Sklansky, "A note on genetic algorithms for large-scale feature selection," Pattern Recognit. Lett., vol. 10, no. 5, pp. 335-347, Nov. 1989.
[26] D. E. Goldberg, Genetic Algorithms in Search, Optimization and Machine Learning. Reading, MA: Addison-Wesley, 1989.
[27] F. Z. Brill, D. E. Brown, and W. N. Martin, "Fast genetic selection of features for neural network classifiers," IEEE Trans. Neural Netw., vol. 3, no. 2, pp. 324-328, Mar. 1992.
[28] J. H. Yang and V. Honavar, "Feature subset selection using a genetic algorithm," IEEE Intell. Syst., vol. 13, no. 2, pp. 44-49, Mar./Apr. 1998.
[29] M. L. Raymer, W. F. Punch, E. D. Goodman, L. A. Kuhn, and A. K. Jain, "Dimensionality reduction using genetic algorithms," IEEE Trans. Evol. Comput., vol. 4, no. 2, pp. 164-171, Jul. 2000.
[30] H. C. Lac and D. A. Stacey, "Feature subset selection via multi-objective genetic algorithm," in Proc. Int. Joint Conf. Neural Netw., Montreal, QC, Canada, Jul. 31-Aug. 4, 2005, pp. 1349-1354.
[31] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, "A fast and elitist multiobjective genetic algorithm: NSGA-II," IEEE Trans. Evol. Comput., vol. 6, no. 2, pp. 182-197, Apr. 2002.
[32] A. P. Dempster, N. M. Laird, and D. B. Rubin, "Maximum likelihood from incomplete data via the EM algorithm," J. R. Stat. Soc., vol. 39, no. 1, pp. 1-38, 1977.
[33] L. Bruzzone and D. F. Prieto, "Unsupervised retraining of a maximum likelihood classifier for the analysis of multitemporal remote sensing images," IEEE Trans. Geosci. Remote Sens., vol. 39, no. 2, pp. 456-460, Feb. 2001.
[34] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, "Investigation of the random forest framework for classification of hyperspectral data," IEEE Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492-501, Mar. 2005.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Lorenzo Bruzzone (S'95-M'98-SM'03) received the M.S. degree in electronic engineering (summa cum laude) and the Ph.D. degree in telecommunications from the University of Genoa, Genoa, Italy, in 1993 and 1998, respectively.</p>
<p>From 1998 to 2000, he was a Postdoctoral Researcher with the University of Genoa. Since 2000, he has been with the University of Trento, Trento, Italy, where he is currently a Full Professor of telecommunications and the Head of the Remote Sensing Laboratory, Department of Information Engineering and Computer Science. He teaches remote sensing, pattern recognition, radar, and electrical communications. His current research interests include remote sensing image processing and recognition (analysis of multitemporal data, feature extraction and selection, classification, regression and estimation, data fusion, and machine learning). He conducts and supervises research on these topics within the frameworks of several national and international projects. He is an Evaluator of project proposals for many different governments (including the European Commission) and scientific organizations. He is the author or coauthor of 74 scientific publications in referred international journals, more than 140 papers in conference proceedings, and seven book chapters.</p>
<p>Dr. Bruzzone ranked first place in the Student Prize Paper Competition of the 1998 IEEE International Geoscience and Remote Sensing Symposium (Seattle, July 1998). He was a recipient of the Recognition of IEEE Transactions on Geoscience and Remote Sensing Best Reviewers in 1999 and was a Guest Editor of a Special Issue of the IEEE Transactions on Geoscience and Remote Sensing on the subject of the analysis of multitemporal remote sensing images (November 2003). He was the General Chair and Cochair of the First and Second IEEE International Workshops on the Analysis of Multitemporal Remote Sensing Images (MultiTemp) and is currently a member of the Permanent Steering Committee of this series of workshops. Since 2003, he has been the Chair of the SPIE Conference on Image and Signal Processing for Remote Sensing. From 2004 to 2006, he was an Associate Editor of the IEEE Geoscience and Remote Sensing Letters and is currently an Associate Editor for the IEEE Transactions on Geoscience and Remote Sensing. He is a Referee for many international journals and has served on the Scientific Committees of several international conferences. He is a member of the Managing Committee of the Italian Inter-University Consortium for Telecommunications and a member of the Scientific Committee of the India-Italy Center for Advanced Research. Since 2009, he has been a member of the Administrative Committee of the IEEE Geoscience and Remote Sensing Society. He is also a member of the International Association for Pattern Recognition and the Italian Association for Remote Sensing (AIT).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Claudio Persello (S'07) received the B.S. and M.S. degrees in telecommunication engineering from the University of Trento, Trento, Italy, in 2003 and 2005, respectively, where he is currently working toward the Ph.D. degree in information and communication technologies.</p>
<p>He is with the Remote Sensing Group, Department of Information Engineering and Computer Science, University of Trento. His current research interests include remote sensing, image classification, pattern recognition, and machine learning. He is a Referee for the Canadian Journal of Remote Sensing and the IEEE Transactions on Geoscience and Remote Sensing.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Manuscript received November 18, 2008; revised January 30, 2009. First published July 10, 2009; current version published August 28, 2009.</p>
<p>The authors are with the Department of Information Engineering and Computer Science, University of Trento, 38050 Trento, Italy (e-mail: lorenzo. bruzzone@ing.unitn.it; claudio.persello@disi.unitn.it).</p>
<p>Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.</p>
<p>Digital Object Identifier 10.1109/TGRS.2009.2019636&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
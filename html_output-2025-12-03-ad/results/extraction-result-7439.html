<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7439 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7439</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7439</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-4610ffb1b016acaa82a2065ffd1a3adbae1ce722</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4610ffb1b016acaa82a2065ffd1a3adbae1ce722" target="_blank">Large Language Models Are Human-Level Prompt Engineers</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is shown that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.</p>
                <p><strong>Paper Abstract:</strong> By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the"program,"optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7439.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7439.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>APE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Prompt Engineer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm that treats natural-language instructions as programs and uses LLMs to propose, score, and (optionally) iteratively refine candidate instructions to maximize a chosen score on held-out examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large autoregressive transformer fine-tuned with human feedback to follow instructions (the InstructGPT family as reported by Ouyang et al.). Used both as the proposal model and as the target (execution) model in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction Induction (24-task suite)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given five input-output demonstration pairs from a task, synthesize a single natural-language instruction that, when prepended to a test input, causes the model to produce the correct output.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot instruction prompt: single natural-language instruction prepended to each test input (no in-context examples for zero-shot runs); also evaluated with instruction prepended to few-shot in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Selection of instruction candidates by sampling an LLM (default 50 samples), scoring via execution accuracy on held-out training subsets, adaptive filtering (multi-stage) of candidates, optional iterative resampling around high-scoring prompts; zero-shot evaluation on InstructGPT; experiments repeated 5 seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>execution accuracy (per-task accuracy; Interquartile Mean aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>IQM 0.810 (across 24 instruction-induction tasks, zero-shot on InstructGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Human-engineered prompts: IQM 0.749 (same 24 tasks, same evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.061 absolute IQM (APE vs human prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Default sample size 50; evaluation repeated 5 times with different random seeds; execution accuracy used as default score; InstructGPT (text-davinci-002) used as execution model.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Human-Level Prompt Engineers', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7439.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7439.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Forward vs Reverse proposal templates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forward (left-to-right) and reverse (infilling) proposal generation templates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two classes of prompt templates for generating instruction candidates from an LLM: forward templates produce the instruction at the end of left-to-right generation, while reverse/infilling templates use models with fill-in-the-middle capability to place the instruction anywhere in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (autoregressive and infilling models mentioned: InstructGPT, T5, GLM, InsertGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Forward generation: left-to-right autoregressive models (InstructGPT). Reverse generation: models with infilling capabilities (T5-like, GLM, InsertGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (examples include InstructGPT 175B and T5-family sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction candidate proposal</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate candidate natural-language instructions given example input-output pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Template-based natural-language completion (forward) vs infill template with a blank to insert an instruction (reverse).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt template / generation mode</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Forward template: supply demonstrations then ask 'The instruction was <COMPLETE>'; Reverse template: place <INSERT> marker where the instruction belongs and ask an infilling model to fill it; choice affects whether the instruction can appear in the middle or beginning of generated text and thus affects generation quality for different LLM architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>qualitative proposal quality (fraction of on-topic / high-execution-accuracy candidates)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Used reverse templates for tasks like TruthfulQA; forward templates for many instruction-induction tasks; sampling of hundreds of candidates in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Human-Level Prompt Engineers', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7439.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7439.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model size -> proposal quality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of LLM size and instruction-following fine-tuning on proposal distribution quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Larger and instruction-fine-tuned LLMs produce higher-quality instruction proposals (higher fraction of candidates with non-trivial execution accuracy) than smaller or unfine-tuned models when sampling candidate prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI models (ada, babbage, curie, davinci, text-ada-001, text-babbage-001, text-curie-001, text-davinci-002/InstructGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Family of OpenAI autoregressive transformer models of increasing size/capacity; text-davinci-002 is the largest/instruction-tuned variant used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>ranging from small (ada) to large (InstructGPT ≈175B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Proposal distribution quality (example: Pluralization and 'Start With')</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate 250 instruction candidates per model and measure the fraction that yield non-trivial test execution accuracy on 50 test points.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Sampling-based instruction generation from an LLM given demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>250 instruction samples per model; evaluate execution accuracy on 50 test points; plot survival function and accuracy histogram per model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>survival function / distribution of test execution accuracy across sampled prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: larger models (InstructGPT 175B) produce substantially more high-accuracy proposals; on a simple task all proposals from InstructGPT were reasonable, while smaller models produced many off-topic prompts on harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>250 samples per model; evaluation on 50 held-out test examples per sampled prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Human-Level Prompt Engineers', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7439.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7439.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sample size effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dependence of best-selected instruction quality on number of sampled instruction candidates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Increasing the number of sampled candidate instructions monotonically raises the expected test accuracy of the best instruction found, with diminishing returns and human-level performance achieved around 64 samples in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following autoregressive LLM used to sample candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction Induction (various tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure test execution accuracy of the best instruction selected from N sampled candidates (N varied from 4 to 128).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Sampling many candidate prompts from an LLM then selecting best by scoring</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt sampling parameter</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Sample sizes varied from 4 to 128 candidates; observed mean and std across six tasks; default chosen sample size = 50 for later experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>test execution accuracy of the best-selected instruction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Monotonic increase in best-instruction accuracy as sample size increases; human-level performance reached at ~64 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Small sample counts (e.g., 4) produce substantially lower best-instruction accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Improvement with larger sample sizes; diminishing returns beyond ~64 samples (qualitative description; absolute numbers not reported for all tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Sampling from InstructGPT; selection via execution-accuracy scoring on held-out subsets; mean/std reported over tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Human-Level Prompt Engineers', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7439.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7439.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-Shot-CoT (human prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Chain-of-Thought prompting: 'Let's think step by step.'</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A short human-authored answer-prefix that elicits chain-of-thought style intermediate reasoning in LLMs, dramatically improving zero-shot performance on multi-step reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (as evaluated in Kojima et al. and cited results)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned autoregressive transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>reported improvements often on large models such as InstructGPT-scale; Kojima et al. report results on InstructGPT-family models</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-step arithmetic and math reasoning (MultiArith, GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve arithmetic/word-problem tasks requiring multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot prompt with short chain-of-thought trigger inserted before the answer (single-sentence answer prefix)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (answer-prefix / CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prepending the phrase 'Let's think step by step.' to the model's answer; no few-shot chain-of-thought exemplars required.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (percent correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in Kojima et al.: MultiArith 17.7% -> 78.7%; GSM8K 10.4% -> 40.7% (when adding 'Let's think step by step.').</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>No-CoT zero-shot baseline: MultiArith 17.7%, GSM8K 10.4%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+61.0 and +30.3 percentage points absolute for MultiArith and GSM8K respectively (as cited from Kojima et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot evaluation with the chain-of-thought answer-prefix; exact model and settings as in Kojima et al.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Human-Level Prompt Engineers', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7439.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7439.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>APE-optimized Zero-Shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>APE-discovered chain-of-thought answer-prefix</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>APE was used to search over variants beginning with 'Let's' to maximize likelihood of correct reasoning steps; the discovered prompt ('Let's work this out in a step by step way to be sure we have the right answer.') improved on top of the human 'Let's think step by step.'</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer used to generate reasoning steps and to evaluate prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith and GSM8K (zero-shot chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot multi-step arithmetic reasoning by eliciting chain-of-thought via an answer-prefix.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot answer-prefix (chain-of-thought) prompt variant discovered by APE.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Generate dataset of correct reasoning steps using baseline CoT phrase, then use APE to search for best 'Let's...' prefix that maximizes likelihood of those correct steps; evaluate zero-shot accuracy with discovered prefix.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (percent correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MultiArith: 78.7% -> 82.0%; GSM8K: 40.7% -> 43.0% (APE prompt vs human 'Let's think step by step.' baseline, reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Human 'Let's think step by step.' (MultiArith 78.7%, GSM8K 40.7%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+3.3 and +2.3 percentage points absolute for MultiArith and GSM8K respectively (APE prompt vs human CoT prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Generate correct reasoning steps with InstructGPT using baseline CoT, filter to correct answers, optimize a prompt starting with 'Let's' using APE, evaluate zero-shot on test tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Human-Level Prompt Engineers', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7439.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7439.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction + In-context</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prepending an optimized instruction to few-shot in-context examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an instruction found by zero-shot selection (APE) and prepending it before in-context demonstrations can improve few-shot performance on most tasks but may harm performance when the selected instruction overfits the zero-shot regime or conflicts with the examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned autoregressive transformer; used for few-shot in-context evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot in-context learning on 24 instruction-induction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Insert the selected instruction before K-shot input-output demonstrations and evaluate on held-out test inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot in-context prompt: [Instruction] + K demonstrations (K=5 used in selection/varied), then test input.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Instructions selected by zero-shot execution accuracy and then prepended to standard in-context prompts (denoted 'Instruction + In-context'); compared against standard in-context learning without added instruction. Observed improved or comparable test performance on 21/24 tasks; for Rhymes, Large Animal, and Second Letters, adding instruction hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>test execution accuracy per task</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>APE-prepended instructions improved or matched standard few-shot in-context performance on 21 out of 24 tasks (aggregate per-task results shown in paper; individual task numbers in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Standard few-shot in-context learning without the added instruction (baseline per-task values in paper's Figure 8)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Improved or comparable on 21 tasks; decreased on Rhymes, Large Animal, Second Letters (qualitative counts provided; per-task deltas in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Instructions selected using zero-shot execution accuracy (default) and also evaluated selecting by few-shot execution accuracy as an alternative; 5 demonstrations used for selection in many experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Human-Level Prompt Engineers', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7439.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7439.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rhymes adversarial format effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Format interaction causing adversarial exploit in Rhymes task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When APE optimized instructions for zero-shot on the Rhymes task it sometimes discovered trivial instructions (e.g., 'echo the input'), which scored highly in zero-shot execution but when combined with few-shot examples the instruction-context mismatch caused a large performance drop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following autoregressive transformer used both to propose and execute instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Rhymes (instruction induction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce a word that rhymes with the given input word.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot vs few-shot: instruction alone vs instruction + in-context examples</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / instruction-example alignment</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot-selected instructions sometimes instructed the model to echo the input (trivial rhyme), which yields near-perfect zero-shot accuracy; adding in-context examples showing non-trivial rhymes caused a misalignment and performance drop. Scoring by few-shot execution accuracy instead of zero-shot can mitigate this.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>test execution accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: near-perfect zero-shot accuracy for trivial echoing instructions, but large drop when combined with in-context examples; exact per-task numbers in appendix Table 8.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Standard in-context few-shot without the added (overfitted) instruction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Substantial negative change for Rhymes when adding zero-shot-selected instruction to in-context examples (qualitative description; exact percent deltas provided in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Instructions selected on zero-shot metric; then combined with 5-shot examples; alternative selection by few-shot metric evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Human-Level Prompt Engineers', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7439.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7439.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scoring function choice</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution accuracy vs log-probability as score functions for prompt selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two candidate score functions were compared for selecting best instructions: execution accuracy (0-1 match or set-matching) and the log probability of the target answer under the model; execution accuracy correlated better with held-out test performance across tasks and was chosen as the default.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned autoregressive LLM used to compute both execution-accuracy and log-probability scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction selection correlation analysis (24 instruction-induction tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compute Spearman correlations between candidate score (execution accuracy or log-prob) and test execution accuracy across candidate prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Scoring candidate prompts by executing model on held-out subsets and computing either 0-1 execution accuracy or log P(target|[instruction; input]).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>scoring metric / evaluation signal</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Generate 250 instructions per task in forward mode; evaluate metric and test accuracy on 10 test data points; compute Spearman correlation between metric and test accuracy. Execution accuracy aligned better across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman correlation with test accuracy (qualitative outcome reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Execution accuracy score had higher correlation with test accuracy than log-prob score (qualitative conclusion; numeric correlations not tabulated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>250 sampled instructions per task; InstructGPT forward generation; test on 10 held-out points per candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Human-Level Prompt Engineers', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7439.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7439.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative Monte Carlo search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative LLM resampling around high-scoring prompts (iterative APE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative variant of APE that filters candidates and asks an LLM to propose semantically similar variants to top candidates; improves the quality of the overall proposal set but yields marginal improvement for the highest-scoring instruction in many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned autoregressive LLM used for resampling/generating new candidates near high-scoring ones.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction proposal refinement</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Iteratively refine candidate set by selecting top-k% and prompting the LLM to generate variations; evaluate resulting candidates' execution accuracy distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Iterated sample-filter-resample loop; candidate prompts are semantically paraphrased variants of top prompts</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>search algorithm / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Multiple rounds of filtering and resampling; survival function of candidate quality improves across rounds; diminishing returns after ~3 rounds; used as optional enhancement to default APE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>distribution of test execution accuracies across candidates; best-candidate test accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Iterative search increased the fraction of higher-quality candidates (improved survival curves) but only marginally improved the highest-scoring instruction for most tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Non-iterative (single-stage) APE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Marginal improvement in best-instruction accuracy on some tasks where initial proposals were poor; stabilized after ~3 rounds (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Iterative rounds with top-k% filtering; resampling prompt shown in paper Figure 3; default APE (non-iterative) used unless otherwise stated.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Human-Level Prompt Engineers', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7439.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7439.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA steering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction optimization for TruthfulQA trade-off between truthfulness and informativeness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using APE to search for instructions that bias model outputs along the trade-off frontier between truthfulness and informativeness on TruthfulQA, finding high-truth but low-info policies (e.g., 'No comment') and other prompts that improve both metrics relative to the human 'help' prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002) as execution model; GPT-judge and GPT-info used to evaluate truthfulness/informativeness</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>InstructGPT used for generation; automated evaluators (fine-tuned GPT-judge/GPT-info from Lin et al.) used as proxies for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B (InstructGPT); GPT-judge/info unspecified fine-tuned models from Lin et al.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA (steering style of answers)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate answers to adversarial/knowledge questions; measure %True, %Info, and combined %True+%Info under different instruction prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot instruction prompt applied across diverse question categories; reverse-mode proposal templates used to generate candidate instructions from demonstration pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / instruction steering</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Sampled 200 candidate instructions using reverse-mode generation; selected top-10 instructions by each metric on training set (100 examples) and evaluated on test (717 examples); used automated scoring metrics aligned with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>%True (truthfulness), %Info (informativeness), %True+%Info combined</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>APE achieved >40% answers that were both true and informative versus ~30% for the human 'help' prompt (aggregate across selected prompts on test set as shown in Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>'help' human prompt from Lin et al.: ~30% True+Info</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+≈10 percentage points absolute on combined True+Info metric for selected APE prompts (aggregate across top-10 candidates)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Training used 100 sampled QA pairs (not pure zero-shot), proposal set of 200 candidates from InstructGPT, selection by automated GPT-judge/GPT-info metrics; top-10 averaged for reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Human-Level Prompt Engineers', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Instruction induction: From few examples to natural language task descriptions <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>TruthfulQA: Measuring how models mimic human falsehoods <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Star: Bootstrapping reasoning with reasoning <em>(Rating: 1)</em></li>
                <li>AutoPrompt: Eliciting knowledge from language models with automatically generated prompts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7439",
    "paper_id": "paper-4610ffb1b016acaa82a2065ffd1a3adbae1ce722",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "APE",
            "name_full": "Automatic Prompt Engineer",
            "brief_description": "An algorithm that treats natural-language instructions as programs and uses LLMs to propose, score, and (optionally) iteratively refine candidate instructions to maximize a chosen score on held-out examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "A large autoregressive transformer fine-tuned with human feedback to follow instructions (the InstructGPT family as reported by Ouyang et al.). Used both as the proposal model and as the target (execution) model in experiments.",
            "model_size": "≈175B",
            "task_name": "Instruction Induction (24-task suite)",
            "task_description": "Given five input-output demonstration pairs from a task, synthesize a single natural-language instruction that, when prepended to a test input, causes the model to produce the correct output.",
            "problem_format": "Zero-shot instruction prompt: single natural-language instruction prepended to each test input (no in-context examples for zero-shot runs); also evaluated with instruction prepended to few-shot in-context examples.",
            "format_category": "prompt style / question type",
            "format_details": "Selection of instruction candidates by sampling an LLM (default 50 samples), scoring via execution accuracy on held-out training subsets, adaptive filtering (multi-stage) of candidates, optional iterative resampling around high-scoring prompts; zero-shot evaluation on InstructGPT; experiments repeated 5 seeds.",
            "performance_metric": "execution accuracy (per-task accuracy; Interquartile Mean aggregated)",
            "performance_value": "IQM 0.810 (across 24 instruction-induction tasks, zero-shot on InstructGPT)",
            "baseline_performance": "Human-engineered prompts: IQM 0.749 (same 24 tasks, same evaluation)",
            "performance_change": "+0.061 absolute IQM (APE vs human prompts)",
            "experimental_setting": "Default sample size 50; evaluation repeated 5 times with different random seeds; execution accuracy used as default score; InstructGPT (text-davinci-002) used as execution model.",
            "statistical_significance": null,
            "uuid": "e7439.0",
            "source_info": {
                "paper_title": "Large Language Models Are Human-Level Prompt Engineers",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Forward vs Reverse proposal templates",
            "name_full": "Forward (left-to-right) and reverse (infilling) proposal generation templates",
            "brief_description": "Two classes of prompt templates for generating instruction candidates from an LLM: forward templates produce the instruction at the end of left-to-right generation, while reverse/infilling templates use models with fill-in-the-middle capability to place the instruction anywhere in the text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs (autoregressive and infilling models mentioned: InstructGPT, T5, GLM, InsertGPT)",
            "model_description": "Forward generation: left-to-right autoregressive models (InstructGPT). Reverse generation: models with infilling capabilities (T5-like, GLM, InsertGPT).",
            "model_size": "various (examples include InstructGPT 175B and T5-family sizes)",
            "task_name": "Instruction candidate proposal",
            "task_description": "Generate candidate natural-language instructions given example input-output pairs.",
            "problem_format": "Template-based natural-language completion (forward) vs infill template with a blank to insert an instruction (reverse).",
            "format_category": "prompt template / generation mode",
            "format_details": "Forward template: supply demonstrations then ask 'The instruction was &lt;COMPLETE&gt;'; Reverse template: place &lt;INSERT&gt; marker where the instruction belongs and ask an infilling model to fill it; choice affects whether the instruction can appear in the middle or beginning of generated text and thus affects generation quality for different LLM architectures.",
            "performance_metric": "qualitative proposal quality (fraction of on-topic / high-execution-accuracy candidates)",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Used reverse templates for tasks like TruthfulQA; forward templates for many instruction-induction tasks; sampling of hundreds of candidates in some experiments.",
            "statistical_significance": null,
            "uuid": "e7439.1",
            "source_info": {
                "paper_title": "Large Language Models Are Human-Level Prompt Engineers",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Model size -&gt; proposal quality",
            "name_full": "Effect of LLM size and instruction-following fine-tuning on proposal distribution quality",
            "brief_description": "Larger and instruction-fine-tuned LLMs produce higher-quality instruction proposals (higher fraction of candidates with non-trivial execution accuracy) than smaller or unfine-tuned models when sampling candidate prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI models (ada, babbage, curie, davinci, text-ada-001, text-babbage-001, text-curie-001, text-davinci-002/InstructGPT)",
            "model_description": "Family of OpenAI autoregressive transformer models of increasing size/capacity; text-davinci-002 is the largest/instruction-tuned variant used.",
            "model_size": "ranging from small (ada) to large (InstructGPT ≈175B)",
            "task_name": "Proposal distribution quality (example: Pluralization and 'Start With')",
            "task_description": "Generate 250 instruction candidates per model and measure the fraction that yield non-trivial test execution accuracy on 50 test points.",
            "problem_format": "Sampling-based instruction generation from an LLM given demonstrations",
            "format_category": "input modality / prompt style",
            "format_details": "250 instruction samples per model; evaluate execution accuracy on 50 test points; plot survival function and accuracy histogram per model.",
            "performance_metric": "survival function / distribution of test execution accuracy across sampled prompts",
            "performance_value": "Qualitative: larger models (InstructGPT 175B) produce substantially more high-accuracy proposals; on a simple task all proposals from InstructGPT were reasonable, while smaller models produced many off-topic prompts on harder tasks.",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "250 samples per model; evaluation on 50 held-out test examples per sampled prompt.",
            "statistical_significance": null,
            "uuid": "e7439.2",
            "source_info": {
                "paper_title": "Large Language Models Are Human-Level Prompt Engineers",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Sample size effect",
            "name_full": "Dependence of best-selected instruction quality on number of sampled instruction candidates",
            "brief_description": "Increasing the number of sampled candidate instructions monotonically raises the expected test accuracy of the best instruction found, with diminishing returns and human-level performance achieved around 64 samples in their experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Instruction-following autoregressive LLM used to sample candidates.",
            "model_size": "≈175B",
            "task_name": "Instruction Induction (various tasks)",
            "task_description": "Measure test execution accuracy of the best instruction selected from N sampled candidates (N varied from 4 to 128).",
            "problem_format": "Sampling many candidate prompts from an LLM then selecting best by scoring",
            "format_category": "prompt sampling parameter",
            "format_details": "Sample sizes varied from 4 to 128 candidates; observed mean and std across six tasks; default chosen sample size = 50 for later experiments.",
            "performance_metric": "test execution accuracy of the best-selected instruction",
            "performance_value": "Monotonic increase in best-instruction accuracy as sample size increases; human-level performance reached at ~64 samples.",
            "baseline_performance": "Small sample counts (e.g., 4) produce substantially lower best-instruction accuracy",
            "performance_change": "Improvement with larger sample sizes; diminishing returns beyond ~64 samples (qualitative description; absolute numbers not reported for all tasks).",
            "experimental_setting": "Sampling from InstructGPT; selection via execution-accuracy scoring on held-out subsets; mean/std reported over tasks.",
            "statistical_significance": null,
            "uuid": "e7439.3",
            "source_info": {
                "paper_title": "Large Language Models Are Human-Level Prompt Engineers",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Zero-Shot-CoT (human prompt)",
            "name_full": "Zero-Shot Chain-of-Thought prompting: 'Let's think step by step.'",
            "brief_description": "A short human-authored answer-prefix that elicits chain-of-thought style intermediate reasoning in LLMs, dramatically improving zero-shot performance on multi-step reasoning tasks.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "mention",
            "model_name": "InstructGPT (as evaluated in Kojima et al. and cited results)",
            "model_description": "Instruction-tuned autoregressive transformer",
            "model_size": "reported improvements often on large models such as InstructGPT-scale; Kojima et al. report results on InstructGPT-family models",
            "task_name": "Multi-step arithmetic and math reasoning (MultiArith, GSM8K)",
            "task_description": "Solve arithmetic/word-problem tasks requiring multi-step reasoning.",
            "problem_format": "Zero-shot prompt with short chain-of-thought trigger inserted before the answer (single-sentence answer prefix)",
            "format_category": "prompt style (answer-prefix / CoT)",
            "format_details": "Prepending the phrase 'Let's think step by step.' to the model's answer; no few-shot chain-of-thought exemplars required.",
            "performance_metric": "accuracy (percent correct)",
            "performance_value": "Reported in Kojima et al.: MultiArith 17.7% -&gt; 78.7%; GSM8K 10.4% -&gt; 40.7% (when adding 'Let's think step by step.').",
            "baseline_performance": "No-CoT zero-shot baseline: MultiArith 17.7%, GSM8K 10.4%",
            "performance_change": "+61.0 and +30.3 percentage points absolute for MultiArith and GSM8K respectively (as cited from Kojima et al.)",
            "experimental_setting": "Zero-shot evaluation with the chain-of-thought answer-prefix; exact model and settings as in Kojima et al.",
            "statistical_significance": null,
            "uuid": "e7439.4",
            "source_info": {
                "paper_title": "Large Language Models Are Human-Level Prompt Engineers",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "APE-optimized Zero-Shot-CoT",
            "name_full": "APE-discovered chain-of-thought answer-prefix",
            "brief_description": "APE was used to search over variants beginning with 'Let's' to maximize likelihood of correct reasoning steps; the discovered prompt ('Let's work this out in a step by step way to be sure we have the right answer.') improved on top of the human 'Let's think step by step.'",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Instruction-tuned transformer used to generate reasoning steps and to evaluate prompt variants.",
            "model_size": "≈175B",
            "task_name": "MultiArith and GSM8K (zero-shot chain-of-thought)",
            "task_description": "Zero-shot multi-step arithmetic reasoning by eliciting chain-of-thought via an answer-prefix.",
            "problem_format": "Zero-shot answer-prefix (chain-of-thought) prompt variant discovered by APE.",
            "format_category": "prompt style / chain-of-thought",
            "format_details": "Generate dataset of correct reasoning steps using baseline CoT phrase, then use APE to search for best 'Let's...' prefix that maximizes likelihood of those correct steps; evaluate zero-shot accuracy with discovered prefix.",
            "performance_metric": "accuracy (percent correct)",
            "performance_value": "MultiArith: 78.7% -&gt; 82.0%; GSM8K: 40.7% -&gt; 43.0% (APE prompt vs human 'Let's think step by step.' baseline, reported in paper).",
            "baseline_performance": "Human 'Let's think step by step.' (MultiArith 78.7%, GSM8K 40.7%)",
            "performance_change": "+3.3 and +2.3 percentage points absolute for MultiArith and GSM8K respectively (APE prompt vs human CoT prompt)",
            "experimental_setting": "Generate correct reasoning steps with InstructGPT using baseline CoT, filter to correct answers, optimize a prompt starting with 'Let's' using APE, evaluate zero-shot on test tasks.",
            "statistical_significance": null,
            "uuid": "e7439.5",
            "source_info": {
                "paper_title": "Large Language Models Are Human-Level Prompt Engineers",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Instruction + In-context",
            "name_full": "Prepending an optimized instruction to few-shot in-context examples",
            "brief_description": "Using an instruction found by zero-shot selection (APE) and prepending it before in-context demonstrations can improve few-shot performance on most tasks but may harm performance when the selected instruction overfits the zero-shot regime or conflicts with the examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Instruction-tuned autoregressive transformer; used for few-shot in-context evaluation.",
            "model_size": "≈175B",
            "task_name": "Few-shot in-context learning on 24 instruction-induction tasks",
            "task_description": "Insert the selected instruction before K-shot input-output demonstrations and evaluate on held-out test inputs.",
            "problem_format": "Few-shot in-context prompt: [Instruction] + K demonstrations (K=5 used in selection/varied), then test input.",
            "format_category": "prompt style / few-shot vs zero-shot",
            "format_details": "Instructions selected by zero-shot execution accuracy and then prepended to standard in-context prompts (denoted 'Instruction + In-context'); compared against standard in-context learning without added instruction. Observed improved or comparable test performance on 21/24 tasks; for Rhymes, Large Animal, and Second Letters, adding instruction hurt performance.",
            "performance_metric": "test execution accuracy per task",
            "performance_value": "APE-prepended instructions improved or matched standard few-shot in-context performance on 21 out of 24 tasks (aggregate per-task results shown in paper; individual task numbers in appendix).",
            "baseline_performance": "Standard few-shot in-context learning without the added instruction (baseline per-task values in paper's Figure 8)",
            "performance_change": "Improved or comparable on 21 tasks; decreased on Rhymes, Large Animal, Second Letters (qualitative counts provided; per-task deltas in appendix).",
            "experimental_setting": "Instructions selected using zero-shot execution accuracy (default) and also evaluated selecting by few-shot execution accuracy as an alternative; 5 demonstrations used for selection in many experiments.",
            "statistical_significance": null,
            "uuid": "e7439.6",
            "source_info": {
                "paper_title": "Large Language Models Are Human-Level Prompt Engineers",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Rhymes adversarial format effect",
            "name_full": "Format interaction causing adversarial exploit in Rhymes task",
            "brief_description": "When APE optimized instructions for zero-shot on the Rhymes task it sometimes discovered trivial instructions (e.g., 'echo the input'), which scored highly in zero-shot execution but when combined with few-shot examples the instruction-context mismatch caused a large performance drop.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Instruction-following autoregressive transformer used both to propose and execute instructions.",
            "model_size": "≈175B",
            "task_name": "Rhymes (instruction induction)",
            "task_description": "Produce a word that rhymes with the given input word.",
            "problem_format": "Zero-shot vs few-shot: instruction alone vs instruction + in-context examples",
            "format_category": "prompt style / instruction-example alignment",
            "format_details": "Zero-shot-selected instructions sometimes instructed the model to echo the input (trivial rhyme), which yields near-perfect zero-shot accuracy; adding in-context examples showing non-trivial rhymes caused a misalignment and performance drop. Scoring by few-shot execution accuracy instead of zero-shot can mitigate this.",
            "performance_metric": "test execution accuracy",
            "performance_value": "Qualitative: near-perfect zero-shot accuracy for trivial echoing instructions, but large drop when combined with in-context examples; exact per-task numbers in appendix Table 8.",
            "baseline_performance": "Standard in-context few-shot without the added (overfitted) instruction",
            "performance_change": "Substantial negative change for Rhymes when adding zero-shot-selected instruction to in-context examples (qualitative description; exact percent deltas provided in appendix).",
            "experimental_setting": "Instructions selected on zero-shot metric; then combined with 5-shot examples; alternative selection by few-shot metric evaluated.",
            "statistical_significance": null,
            "uuid": "e7439.7",
            "source_info": {
                "paper_title": "Large Language Models Are Human-Level Prompt Engineers",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Scoring function choice",
            "name_full": "Execution accuracy vs log-probability as score functions for prompt selection",
            "brief_description": "Two candidate score functions were compared for selecting best instructions: execution accuracy (0-1 match or set-matching) and the log probability of the target answer under the model; execution accuracy correlated better with held-out test performance across tasks and was chosen as the default.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Instruction-tuned autoregressive LLM used to compute both execution-accuracy and log-probability scores.",
            "model_size": "≈175B",
            "task_name": "Instruction selection correlation analysis (24 instruction-induction tasks)",
            "task_description": "Compute Spearman correlations between candidate score (execution accuracy or log-prob) and test execution accuracy across candidate prompts.",
            "problem_format": "Scoring candidate prompts by executing model on held-out subsets and computing either 0-1 execution accuracy or log P(target|[instruction; input]).",
            "format_category": "scoring metric / evaluation signal",
            "format_details": "Generate 250 instructions per task in forward mode; evaluate metric and test accuracy on 10 test data points; compute Spearman correlation between metric and test accuracy. Execution accuracy aligned better across tasks.",
            "performance_metric": "Spearman correlation with test accuracy (qualitative outcome reported)",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": "Execution accuracy score had higher correlation with test accuracy than log-prob score (qualitative conclusion; numeric correlations not tabulated in main text).",
            "experimental_setting": "250 sampled instructions per task; InstructGPT forward generation; test on 10 held-out points per candidate.",
            "statistical_significance": null,
            "uuid": "e7439.8",
            "source_info": {
                "paper_title": "Large Language Models Are Human-Level Prompt Engineers",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Iterative Monte Carlo search",
            "name_full": "Iterative LLM resampling around high-scoring prompts (iterative APE)",
            "brief_description": "An iterative variant of APE that filters candidates and asks an LLM to propose semantically similar variants to top candidates; improves the quality of the overall proposal set but yields marginal improvement for the highest-scoring instruction in many tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Instruction-tuned autoregressive LLM used for resampling/generating new candidates near high-scoring ones.",
            "model_size": "≈175B",
            "task_name": "Instruction proposal refinement",
            "task_description": "Iteratively refine candidate set by selecting top-k% and prompting the LLM to generate variations; evaluate resulting candidates' execution accuracy distribution.",
            "problem_format": "Iterated sample-filter-resample loop; candidate prompts are semantically paraphrased variants of top prompts",
            "format_category": "search algorithm / prompt style",
            "format_details": "Multiple rounds of filtering and resampling; survival function of candidate quality improves across rounds; diminishing returns after ~3 rounds; used as optional enhancement to default APE.",
            "performance_metric": "distribution of test execution accuracies across candidates; best-candidate test accuracy",
            "performance_value": "Iterative search increased the fraction of higher-quality candidates (improved survival curves) but only marginally improved the highest-scoring instruction for most tasks.",
            "baseline_performance": "Non-iterative (single-stage) APE",
            "performance_change": "Marginal improvement in best-instruction accuracy on some tasks where initial proposals were poor; stabilized after ~3 rounds (qualitative).",
            "experimental_setting": "Iterative rounds with top-k% filtering; resampling prompt shown in paper Figure 3; default APE (non-iterative) used unless otherwise stated.",
            "statistical_significance": null,
            "uuid": "e7439.9",
            "source_info": {
                "paper_title": "Large Language Models Are Human-Level Prompt Engineers",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "TruthfulQA steering",
            "name_full": "Instruction optimization for TruthfulQA trade-off between truthfulness and informativeness",
            "brief_description": "Using APE to search for instructions that bias model outputs along the trade-off frontier between truthfulness and informativeness on TruthfulQA, finding high-truth but low-info policies (e.g., 'No comment') and other prompts that improve both metrics relative to the human 'help' prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002) as execution model; GPT-judge and GPT-info used to evaluate truthfulness/informativeness",
            "model_description": "InstructGPT used for generation; automated evaluators (fine-tuned GPT-judge/GPT-info from Lin et al.) used as proxies for human evaluation.",
            "model_size": "≈175B (InstructGPT); GPT-judge/info unspecified fine-tuned models from Lin et al.",
            "task_name": "TruthfulQA (steering style of answers)",
            "task_description": "Generate answers to adversarial/knowledge questions; measure %True, %Info, and combined %True+%Info under different instruction prompts.",
            "problem_format": "Zero-shot instruction prompt applied across diverse question categories; reverse-mode proposal templates used to generate candidate instructions from demonstration pairs.",
            "format_category": "prompt style / instruction steering",
            "format_details": "Sampled 200 candidate instructions using reverse-mode generation; selected top-10 instructions by each metric on training set (100 examples) and evaluated on test (717 examples); used automated scoring metrics aligned with human judgments.",
            "performance_metric": "%True (truthfulness), %Info (informativeness), %True+%Info combined",
            "performance_value": "APE achieved &gt;40% answers that were both true and informative versus ~30% for the human 'help' prompt (aggregate across selected prompts on test set as shown in Figure 5).",
            "baseline_performance": "'help' human prompt from Lin et al.: ~30% True+Info",
            "performance_change": "+≈10 percentage points absolute on combined True+Info metric for selected APE prompts (aggregate across top-10 candidates)",
            "experimental_setting": "Training used 100 sampled QA pairs (not pure zero-shot), proposal set of 200 candidates from InstructGPT, selection by automated GPT-judge/GPT-info metrics; top-10 averaged for reporting.",
            "statistical_significance": null,
            "uuid": "e7439.10",
            "source_info": {
                "paper_title": "Large Language Models Are Human-Level Prompt Engineers",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Instruction induction: From few examples to natural language task descriptions",
            "rating": 2,
            "sanitized_title": "instruction_induction_from_few_examples_to_natural_language_task_descriptions"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "rating": 2,
            "sanitized_title": "truthfulqa_measuring_how_models_mimic_human_falsehoods"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "Star: Bootstrapping reasoning with reasoning",
            "rating": 1,
            "sanitized_title": "star_bootstrapping_reasoning_with_reasoning"
        },
        {
            "paper_title": "AutoPrompt: Eliciting knowledge from language models with automatically generated prompts",
            "rating": 1,
            "sanitized_title": "autoprompt_eliciting_knowledge_from_language_models_with_automatically_generated_prompts"
        }
    ],
    "cost": 0.018348749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are Human-Level Prompt Engineers</h1>
<p>Yongchao Zhou ${ }^{1,2, <em>}$, Andrei Ioan Muresanu ${ }^{2,3, </em>}$, Ziwen Han ${ }^{1,2, <em>}$, Keiran Paster ${ }^{1,2}$, Silviu Pitis ${ }^{1,2}$, Harris Chan ${ }^{1,2}$, Jimmy Ba ${ }^{1,2}$<br>${ }^{1}$ University of Toronto ${ }^{2}$ Vector Institute ${ }^{3}$ University of Waterloo ${ }^{</em>}$ Equal contribution {yczhou,hanziwen,keirp,spitis,hchan,jba}@cs.toronto.edu {andrei.muresanu}@uwaterloo.ca</p>
<h4>Abstract</h4>
<p>By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer ${ }^{1}$ (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Extensive experiments show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 24/24 Instruction Induction tasks and 17/21 curated BIG-Bench tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts are able to improve few-shot learning performance (by simply prepending them to standard in-context learning prompts), find better zero-shot chain-ofthought prompts, as well as steer models toward truthfulness and/or informativeness. ${ }^{2}$</p>
<h2>1 INTRODUCTION</h2>
<p>The combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality (Kaplan et al., 2020; Vaswani et al., 2017). These so-called "large language models" (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups (Brown et al., 2020; Srivastava et al., 2022). With generality, however, there comes a question of control: how can we make LLMs do what we want them to do?</p>
<p>To answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), in-context learning (Brown et al., 2020), and several forms of prompt generation (Gao, 2021), including both differentiable tuning of soft prompts (Qin \&amp; Eisner, 2021; Lester et al., 2021) and natural language prompt engineering (Reynolds \&amp; McDonell, 2021). The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers (Rombach et al., 2022; Ramesh et al., 2022), for which public interest in prompt design and generation has also emerged (see Appendix A for examples).</p>
<p>Behind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model. We can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Our method, Automatic Prompt Engineer (APE), automatically generates instructions for a task that is specified via output demonstrations: it generates several instruction candidates, either via direct inference or a recursive process based on semantic similarity, executes them using the target model, and selects the most appropriate instruction based on computed evaluation scores. (b) As measured by the interquartile mean across the 24 NLP tasks introduced by <em>Honovich et al. (2022)</em>, APE is able to surpass human performance when using the InstructGPT model <em>(Ouyang et al., 2022)</em>. can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task <em>(Sanh et al., 2022; Wei et al., 2021)</em>.</p>
<p>To reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem <em>natural language program synthesis</em> and propose to address it as a black-box optimization problem using LLMs to generate and search over heuristically viable candidate solutions. In doing so, we leverage the generalist capabilities of LLMs in three ways. First, we use an LLM as an inference model <em>(Ellis et al., 2021; Honovich et al., 2022)</em> to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. Next, we guide the search process by computing a score for each instruction under the LLM we seek to control. Finally, we propose an iterative Monte Carlo search method where LLMs improve the best candidates by proposing semantically similar instruction variants. Intuitively, our algorithm asks LLMs to generate a set of instruction candidates based on demonstrations and then asks them to assess which instructions are more promising. We call our algorithm Automatic Prompt Engineer (APE). Our main contributions are:</p>
<ul>
<li>We frame instruction generation as natural language program synthesis, formulate it as a black-box optimization problem guided by LLMs, and propose both a naive and an iterative Monte Carlo search methods to approximate the solution.</li>
<li>Our proposed method, APE, achieves human-level performance on zero-shot learning with model-generated instructions on 24/24 Instruction Induction and 17/21 Big-Bench tasks.</li>
<li>We provide extensive qualitative and quantitative analyses exploring various facets of APE, and demonstrate applications of APE for improving few-shot learning, finding better zero-shot chain of thought prompts, and steering LLMs toward desired behaviors such as truthfulness and/or informativeness.</li>
</ul>
<h2>2 Related Work</h2>
<p>Large Language Models Scaling up transformer-based language models in terms of model size, training data, and training compute has been shown to predictably improve performance on a wide range of downstream NLP tasks <em>(Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020)</em>. Many emergent abilities <em>(Wei et al., 2022a)</em> of LLMs have been discovered as a result of this scaling, including few-shot in-context learning, zero-shot problem solving, chain of thought reasoning, instruction following, and instruction induction *(Cobbe et al., 2021; Wei et al., 2022b; Kojima et al.,</p>
<p>2022; Sanh et al., 2022; Wei et al., 2021; Ouyang et al., 2022; Honovich et al., 2022). In this paper, we view LLMs as black-box computers that execute programs specified by natural language instructions and investigate how to control an LLM's behavior using model-generated instructions.</p>
<p>Prompt Engineering Prompting offers a natural and intuitive interface for humans to interact with and use generalist models such as LLMs. Due to its flexibility, prompting has been widely used as a generic method for NLP tasks (Schick \&amp; Schütze, 2021; Brown et al., 2020; Sanh et al., 2022). However, LLMs require careful prompt engineering, either manually (Reynolds \&amp; McDonell, 2021) or automatically (Gao et al., 2021; Shin et al., 2020), as models do not seem to understand the prompts in the same way a human would (Webson \&amp; Pavlick, 2021; Lu et al., 2021). Though many successful prompt tuning methods perform optimization over a continuous space using gradient-based methods (Liu et al., 2021; Qin \&amp; Eisner, 2021; Lester et al., 2021), this becomes less practical with scale, as computing gradients becomes increasingly expensive and access to models shifts to APIs that may not provide gradient access. In our paper, we borrow components from discrete prompt search methods, such as prompt generation (Gao et al., 2021; Ben-David et al., 2021), prompt scoring (Davison et al., 2019) and prompt paraphrasing (Jiang et al., 2020; Yuan et al., 2021) to optimize instructions by searching directly in the natural language hypothesis space. As compared to this past work, which uses specialized models for each component and leans heavily on human templates, we show that the entire search can be conducted by a single LLM.</p>
<p>Program Synthesis Program synthesis involves the automatic search over a "program space" to find a program satisfying a particular specification (Gulwani et al., 2017). Modern program synthesis admits a wide variety of specifications, including input-output examples (Ellis et al., 2021; Wong et al., 2021) and natural language (Jain et al., 2022). The range of feasible program spaces to search over has also grown, from historically restrictive domain-specific languages to general-purpose programming languages (Austin et al., 2021). In contrast to prior approaches that require a suitable structured hypothesis space and library of components (Liang et al., 2010; Ellis et al., 2018), we leverage the structure provided by LLMs to search over the space of natural language programs. Using inference models is a standard practice to speed up the search by restricting the search space to a limited space of possible expressions (Menon et al., 2013; Lee et al., 2018; Devlin et al., 2017; Ellis et al., 2021). Inspired by this, we use LLMs as approximate inference models to generate program candidates based on a small set of demonstrations. Unlike classical program synthesis, our inference models do not require any training and generalize well to various tasks.</p>
<h1>3 Natural Language Program Synthesis using LLMs</h1>
<p>We consider a task specified by a dataset $\mathcal{D}_{\text {train }}={(Q, A)}$ of input/output demonstrations sampled from population $\mathcal{X}$, and a prompted model $\mathcal{M}$. The goal of natural language program synthesis is to find a single instruction $\rho$ such that, when $\mathcal{M}$ is prompted with the concatenation $[\rho ; Q]$ of instruction and a given input, $\mathcal{M}$ produces the corresponding output $A$. More formally, we frame this as an optimization problem, where we seek instruction $\rho$ that maximizes the expectation of some per-sample score $f(\rho, Q, A)$ over possible $(Q, A)$ :</p>
<p>$$
\rho^{\star}=\underset{\rho}{\arg \max } f(\rho)=\underset{\rho}{\arg \max } \mathbb{E}_{(Q, A)}[f(\rho, Q, A)]
$$</p>
<p>Note that in general, $Q$ may be the empty string, such that we are optimizing $\rho$ as a prompt that directly produces outputs ${A}$. While this task has been widely attempted by humans, we have little knowledge of how compatible any particular instruction is with model $\mathcal{M}$. Thus, we propose to treat this human-intractable question as a black-box optimization process guided by LLMs. Our algorithm, APE, uses LLMs in each of two key components, proposal and scoring. As shown in Figure 1 and summarized in Algorithm 1, APE first proposes a few candidate prompts, and then filters/refines the candidate set according to a chosen score function, ultimately choosing the instruction with the highest score. We discuss options for proposal and scoring next.</p>
<h3>3.1 Initial Proposal Distributions</h3>
<p>Due to the infinitely large search space, finding the right instruction can be extremely difficult, which has rendered natural language program synthesis historically intractable. Recent progress in NLP has shown language models are very good at generating diverse natural language text. Therefore, we</p>
<div class="codehilite"><pre><span></span><code>Algorithm <span class="mi">1</span> Automatic Prompt Engineer <span class="p">(</span>APE<span class="p">)</span>
    Require<span class="p">:</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>text <span class="p">{</span>train <span class="p">}}</span> <span class="err">\</span>leftarrow<span class="err">\</span><span class="p">{(</span>Q<span class="p">,</span> A<span class="p">)</span><span class="err">\</span><span class="p">}</span>_<span class="p">{</span>n<span class="p">}:</span><span class="err">\</span><span class="p">)</span> training examples<span class="p">,</span> <span class="err">\</span><span class="p">(</span>f<span class="p">:</span> <span class="err">\</span>rho <span class="err">\</span>times <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span> <span class="err">\</span>mapsto <span class="err">\</span>mathbb<span class="p">{</span>R<span class="p">}:</span><span class="err">\</span><span class="p">)</span> score function
    <span class="mi">1</span><span class="p">:</span> Use LLM to sample instruction proposals <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>U<span class="p">}</span> <span class="err">\</span>leftarrow<span class="err">\</span>left<span class="err">\</span><span class="p">{</span><span class="err">\</span>rho_<span class="p">{</span><span class="mi">1</span><span class="p">},</span> <span class="err">\</span>ldots<span class="p">,</span> <span class="err">\</span>rho_<span class="p">{</span>m<span class="p">}</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span> <span class="p">(</span>See Section <span class="mf">3.1</span><span class="p">)</span>
    <span class="mi">2</span><span class="p">:</span> while not converged do
    <span class="mi">3</span><span class="p">:</span> Choose a random training subset <span class="err">\</span><span class="p">(</span><span class="err">\</span>widetilde<span class="p">{</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}}</span>_<span class="p">{</span><span class="err">\</span>text <span class="p">{</span>train <span class="p">}}</span> <span class="err">\</span>subset <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>text <span class="p">{</span>train <span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>
    <span class="mi">4</span><span class="p">:</span> for all <span class="err">\</span><span class="p">(</span><span class="err">\</span>rho<span class="err">\</span><span class="p">)</span> <span class="k">in</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>U<span class="p">}</span><span class="err">\</span><span class="p">)</span> do
        Evaluate score on the subset <span class="err">\</span><span class="p">(</span><span class="err">\</span>widetilde<span class="p">{</span>s<span class="p">}</span> <span class="err">\</span>leftarrow f<span class="err">\</span>left<span class="p">(</span><span class="err">\</span>rho<span class="p">,</span> <span class="err">\</span>widetilde<span class="p">{</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}}</span>_<span class="p">{</span><span class="err">\</span>text <span class="p">{</span>train <span class="p">}}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> <span class="p">(</span>See Section <span class="mf">3.2</span> <span class="p">)</span>
        end for
        Filter the top <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathrm<span class="p">{</span>k<span class="p">}</span> <span class="err">\%\</span><span class="p">)</span> of instructions <span class="k">with</span> high scores <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>U<span class="p">}</span>_<span class="p">{</span>k<span class="p">}</span> <span class="err">\</span>subset <span class="err">\</span>mathcal<span class="p">{</span>U<span class="p">}</span><span class="err">\</span><span class="p">)</span> using <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span><span class="err">\</span>widetilde<span class="p">{</span>s<span class="p">}</span>_<span class="p">{</span><span class="mi">1</span><span class="p">},</span> <span class="err">\</span>ldots<span class="p">,</span> <span class="err">\</span>widetilde<span class="p">{</span>s<span class="p">}</span>_<span class="p">{</span>m<span class="p">}</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
        Update instructions <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>U<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>mathcal<span class="p">{</span>U<span class="p">}</span>_<span class="p">{</span>k<span class="p">}</span><span class="err">\</span><span class="p">)</span> <span class="ow">or</span> use LLM to resample <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>U<span class="p">}</span> <span class="err">\</span>leftarrow<span class="err">\</span><span class="p">)</span> resample <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>U<span class="p">}</span>_<span class="p">{</span>k<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> <span class="p">(</span>See Section <span class="mf">3.3</span><span class="p">)</span>
    end while
    Return instruction <span class="k">with</span> the highest score <span class="err">\</span><span class="p">(</span><span class="err">\</span>rho<span class="err">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>arg <span class="err">\</span>max _<span class="p">{</span><span class="err">\</span>rho <span class="err">\</span><span class="k">in</span> <span class="err">\</span>mathcal<span class="p">{</span>U<span class="p">}</span>_<span class="p">{</span>k<span class="p">}}</span> f<span class="err">\</span>left<span class="p">(</span><span class="err">\</span>rho<span class="p">,</span> <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>text <span class="p">{</span>train <span class="p">}}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>consider leveraging a pretrained LLM to propose a good set $\mathcal{U}$ of candidate solutions that will guide our search procedure. While random samples from LLMs are unlikely to produce the desired $(Q, A)$ pairs, we can instead ask the LLM to approximately infer the most likely instructions with a high score, given the input/output demonstrations; i.e., to approximately sample from $P\left(\rho \mid \mathcal{D}<em _text="\text" _train="{train">{\text {train }}, f(\rho)\right.$ is high $)$.
Forward Mode Generation We consider two approaches to generate high-quality candidates from $P\left(\rho \mid \mathcal{D}</em>}}, f(\rho)\right.$ is high). First, we adopt an approach based on "forward" mode generation by translating this distribution $P\left(\rho \mid \mathcal{D<em _text="\text" _train="{train">{\text {train }}, f(\rho)\right.$ is high) into words. For example, in our instruction induction experiments (Subsection 4.1), we follow Honovich et al. (2022) and prompt the LLM using Figure 2 (Top).
Reverse Mode Generation Although the "forward" model works out of the box for most of the pretrained LLMs, translating $P\left(\rho \mid \mathcal{D}</em>, f(\rho)\right.$ is high) by filling in the blank. We show an example of the such template in Figure 2 (Middle).
Customized Prompts Note that depending on the score function being used, there may exist more appropriate prompts than the samples above. For example, in our TruthfulQA experiments, we start with the human-designed instructions from the original dataset (Lin et al., 2022) and ask the the "reverse" model to propose initial instruction samples that fit the missing context (Figure 2 (Bottom)).}}, f(\rho)\right.$ is high) into words requires custom engineering across different tasks. This is because while instructions are typically found in the beginning of passages, the "forward" model only generates text from left to right, which requires the instruction to be predicted at the end of the prompt. Therefore, we desire a more flexible approach such that the instruction can be anywhere in the text. To address this, we consider "reverse" mode generation, which uses an LLM with infilling capabilities-e.g., T5 (Raffel et al., 2020), GLM (Du et al., 2022), and InsertGPT (Bavarian et al., 2022)-to infer the missing instructions. Our "reverse" model directly samples from $P\left(\rho \mid \mathcal{D}_{\text {train }</p>
<p>Forward Generation Template
I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs:
Input: $\left|Q_{1}\right| \quad$ Output: $\left|A_{1}\right|$
Input: $\left|Q_{2}\right| \quad$ Output: $\left|A_{2}\right|$
The instruction was $&lt;$ COMPLETE $&gt;$
Reverse Generation Template
I instructed my friend to $&lt;$ INSERT $&gt;$.
The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs:
Input: $\left|Q_{1}\right| \quad$ Output: $\left|A_{1}\right|$
Input: $\left|Q_{2}\right| \quad$ Output: $\left|A_{2}\right|$</p>
<p>Template for TruthfulQA
Professor Smith was given the following instructions: <INSERT></p>
<p>Here are the Professor's responses:
Input: $\left|Q_{1}\right| \quad$ Output: $\left|A_{1}\right|$
Input: $\left|Q_{2}\right| \quad$ Output: $\left|A_{2}\right|$</p>
<p>Figure 2: Prompts for LLMs</p>
<h1>3.2 Score Functions</h1>
<p>To cast our problem as black-box optimization, we choose a score function that accurately measures the alignment between the dataset and the data the model generates. In our instruction induction experiments, we consider two potential score functions, described below. In the TruthfulQA experiments, we focused primarily on automated metrics proposed in Lin et al. (2022), similar to the execution accuracy. In each case, we evaluate the quality of a generated instruction using Equation (1), and take the expectation over a held-out test dataset $\mathcal{D}_{\text {test }}$.</p>
<p>Execution accuracy First, we consider evaluating the quality of an instruction $\rho$ using the execution accuracy metric proposed by Honovich et al. (2022), which we denote as $f_{\text {exec }}$. In most cases,</p>
<p>execution accuracy is simply defined as the 0-1 loss, $f(\rho, Q, A)=\mathbb{1}[\mathcal{M}([\rho ; Q])=A]$. On some tasks, execution accuracy takes into account invariants; e.g., it may be an order invariant set matching loss, as described in Appendix A of Honovich et al. (2022).</p>
<p>Log probability We further consider a softer probabilistic score function, which we hypothesize might improve optimization by providing a more fine-grained signal when searching over low-quality instruction candidates. In particular, we consider the log probability of the desired answer given the instruction and question under the target model $\mathcal{M}$, which on a per sample basis, is $\log P(A|[\rho ; Q])$.</p>
<p>Efficient score estimation Estimating the score by computing the score over the entire training dataset for all instruction candidates can be expensive. To reduce the computation cost, we adopt a filtering scheme where a promising candidate receives more computation resources while a lowquality candidate receives less computation. It can be achieved by using a multi-stage computation strategy on lines 2-9 Algorithm 1. We first evaluate all candidates with a small subset of the training dataset. For the candidates with a score greater than a certain threshold, we sample and evaluate a new non-overlapping subset from the training dataset to update the moving average of the score. Then, we repeat this process until a small set of candidates is left, which are evaluated on the entire training dataset. This adaptive filtering scheme significantly improves the computation efficiency by keeping the exact computation costs for the high-quality samples and drastically reducing the computation costs for low-quality candidates. We note that a similar score estimation scheme has been used in previous works (Li et al., 2022; Maclaurin \&amp; Adams, 2015).</p>
<h1>3.3 Iterative Proposal Distributions</h1>
<p>Despite our attempt to directly sample high-quality initial instruction candidates, it could be the case that the method described in Subsection 3.1 fails to produce a good proposal set $\mathcal{U}$, either because it lacks of diversity or does not contain any candidates with a suitably high score. In case of such challenges, we explore an iterative process for resampling $\mathcal{U}$.</p>
<p>Iterative Monte Carlo Search Instead of only sampling from the initial proposal, we consider exploring the search space locally around the current best candidates. This allows us to generate new instructions that are more likely to be successful. We call this variant iterative APE. At each stage, we evaluate a set of instructions and filter out candidates with low scores. Then, an LLM is asked to generate new instructions similar to those with high scores. We provide the prompt used for resampling in Figure 3. Figure 6 (Right) shows that although this approach improves the overall quality of the proposal set $\mathcal{U}$, the highest scoring instruction tends to remain the same with more stages. We conclude iterative generation provides marginal improvement over the relative simplicity and effectiveness of the generative process described in Subsection 3.1. Therefore, we use APE without iterative search as default unless otherwise stated.</p>
<h2>4 Large Language Models are Human-Level Prompt Engineers</h2>
<p>This section examines how APE can guide LLMs to desired behaviors. We investigate from four perspectives: zero-shot performance, few-shot in-context learning performance, zero-shot chain-ofthought reasoning, and truthfulness. Our experiments show that APE can find prompts that improve task performance, performing equal to or even better than those authored by humans. APE also often produces insightful tricks for how to best prompt language models that can be successfully transferred to new tasks (see Section 4.3).</p>
<h3>4.1 INSTRUCTION INDUCTION</h3>
<p>We assess the effectiveness of zero-shot and few-shot in-context learning on 24 instruction induction tasks proposed in Honovich et al. (2022). The tasks span many facets of language understanding, from simple phrase structure to similarity and causality identification. We provide a detailed descriptions of each task in Appendix B. For each task, we sample five input-output pairs from the training data and select the best instruction using algorithm 1. Then, we evaluate the quality of the instruction</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Zero-shot test accuracy on 24 Instruction Induction tasks. APE achieves human-level or better performance on all 24 out of 24 tasks.
by executing the instruction on InstructGPT ${ }^{3}$. We repeat our experiments five times with different random seeds to report the mean and standard deviation. The exact templates for our experiments can be found in Appendix (Table 5).</p>
<p>Zero-shot Learning We compare our method against two baselines: human prompt engineers (Human) ${ }^{4}$ and the model-generated instruction algorithm proposed by Honovich et al. (2022). This algorithm can be thought of as a greedy version of APE, without a search and selection process; thus, we refer to it as "Greedy". Figure 4 shows the zero-shot performance of InstructGPT using human instructions and model generated instructions. Our algorithm outperforms "Greedy" on every task and achieves equal or better than human performance on 24 of 24 tasks. Moreover, the Interquartile Mean (IQM) (Agarwal et al., 2021) across all 24 tasks in Figure 1 suggests that APE with InstructGPT outperforms human-engineered prompts, obtaining an IQM of 0.810 vs humans' 0.749 . We summarize the instruction selected by APE for each task in Appendix (Table 12).</p>
<p>Few-shot In-context Learning We evaluated APE-generated instructions in few-shot in-context learning, where we insert the instruction before the in-context demonstrations. Those instructions are selected based on zero-shot execution accuracy, and we denote this setting as "Instruction + In-context" in Figure 8. As shown in Figure 8, adding an instruction achieves a comparable or better test performance than the standard in-context learning performance on 21 of 24 tasks. Counterintuitively, adding in-context examples for Rhymes, Large Animal, and Second Letters hurts model performance. We conjecture that it may be because the selected instructions overfit the zero-shot learning scenario and thus do not perform well on the few-shot case. Therefore, we experiment using few-shot execution accuracy as the selection metric. Figure 14 shows that the few-shot metric achieves comparable or slightly better than the zero-shot metric except for Rhymes. To have an intuitive understanding of what is happening, we provide a qualitative analysis in Appendix C.1.</p>
<h1>4.2 BIGBENCH</h1>
<p>To see whether APE can be applied to more challenging tasks, we propose and curate BIG-Bench Instruction Induction (BBII), a clean and tractable subset of 21 tasks that have a clear, human-written instruction that can be applied to all examples in the dataset. The selected tasks cover many facets of language understanding and includes all nine such problems from the BigBench-Hard Subset (Suzgun et al., 2022). In particular, it includes emotional understanding, context-free question answering, reading comprehension, summarization, algorithms, and various reasoning tasks (e.g., arithmetic, commonsense, symbolic, and other logical reasoning tasks). We provide a detailed description of the task and our selection criteria in Appendix B.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For each task, we used the reverse mode generation of InstructGPT to generate a set of instruction candidates and ranked the instructions based on their execution accuracy. Then, we executed the selected instruction on InstructGPT to compute the zero-shot performance on the test set and compared it with the default human prompt. As shown in Appendix Table 6, APE achieves comparable or better performance than the default human prompt on 17 out of 21 tasks.</p>
<h1>4.3 Zero-Shot Chain of Thought</h1>
<p>Chain-of-thought reasoning has been shown to dramatically improve the ability of LLMs to complete complex reasoning tasks, such as solving math problems that require multiple steps. Early works (Nye et al., 2021; Betz et al., 2021; Wei et al., 2022b) on chain-of-thought used fine-tuning or in-context learning to get LLMs to show their work for such problems. One of the most influential recent works of prompt engineering was the discovery (Kojima et al., 2022) that LLMs could be made to give chain-of-thoughts simply by prepending "Let's think step by step." to the beginning of the LLM's response. Known as Zero-Shot-CoT, this prompting strategy improves the zero-shot performance of InstructGPT on MultiArith (Roy \&amp; Roth, 2016) from 17.7 to 78.7 and improves performance on GSM8K(Cobbe et al., 2021) from 10.4 to 40.7. As shown in Table 7, Kojima et al. (2022) found their prompt was the best performing out of at least nine human-designed prompts.</p>
<p>We used APE to automatically search for the best answer-prefix across the suite of tasks used in Kojima et al. (2022). Our approach to optimizing this prompt was inspired by Zelikman et al. (2022). First, we generate a dataset of questions and reasoning steps generated using InstructGPT with "Let's think step by step." Then, we remove any data points that had incorrect answers. Finally, we use APE to find a prompt starting with "Let's" that maximizes the likelihood of these correct reasoning steps. See Table 5 for the template used for prompt generation and evaluation. APE produces the prompt "Let's work this out in a step by step way to be sure we have the right answer." This generated prompt further improves performance from 78.7 to 82.0 on MultiArith and from 40.7 to 43.0 on GSM8K. We believe this general workflow represents a common use-case for APE where prompt engineers use APE to optimize parts of their exiting templates to improve performance. See Figure 10 for details on the performance of this prompt on other reasoning tasks.</p>
<h3>4.4 TruthfulQA</h3>
<p>We apply our method on TruthfulQA (Lin et al., 2022) to see how APE-generated instructions can steer an LLM to generate answers with different styles, and study the trade-off between truthfulness and informativeness. Borrowing the metrics from the original paper, we use APE to the learn instructions that maximize three metrics: truthfulness (\% True), informativeness (\% Info), and a combination of both (\%True + \%Info). Lin et al. (2022) used human evaluation to assess the model performance, but they found their automated metrics align with human prediction over $90 \%$ of the time. In our experiments, we rely on their fine-tuned GPT-judge and GPT-info to evaluate the scores.
Prompt Engineering in TruthfulQA We want to stress that the TruthfulQA dataset is intended to test pretrained models in zero-shot settings. Our results are not in any way compatible with the original benchmarks. Because we have optimized the instructions using a small portion of the question and answer pairs as training demonstrations, our results are not "true few-shot learning" (Perez et al., 2021). We randomly sampled 100 out of 817 questions for the actual experiments to form training demonstrations $\mathcal{D}_{\text {train }}$. To sample the proposal set $\mathcal{U}$, we ask a "reverse" model to generate instructions based on six randomly chosen demonstration pairs, similar to our previous experiments. Unlike in Instruction Induction, in TruthfulQA, we aim to find a single best instruction prompt that works well across all 38 categories of questions spanning health, law, politics, and fiction. It is worth noting all our generated instructions are very generic, e.g., "You will be asked a series of questions. For each question, you must either answer the question or decline to answer, in which case you must state that you have no comment", and do not contain any examples from the dataset.
Truthfulness vs Informativeness Trade-off We found that APE outperforms the humanengineered prompt with only 200 candidates proposed by InstructGPT (175B), as seen in Figure 5. We compared our generated prompt with the "help" prompt from Lin et al. (2022). The training and test performance are shown in Figure 5(a)-(b). We found that choosing the top 10 of 200 candidates on the training set generalizes well to the test set. We report the average performance across the top 10 instructions for the three metrics. This result by itself is not surprising as the human baseline is</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Comparison of APE and "help" (human) prompt on the TruthfulQA task. (a) Percentage of answers that were either true ( $\%$ True), informative ( $\%$ Info), or both ( $\%$ True $+\%$ Info) on the 100 training examples. (b) Same data on the 717 test examples. (c) \%True-\%Info frontier computed on training data with top 10 instructions from each metric. (d) \%True-\%Info frontier on the test data.
not carefully chosen, as pointed out by Askell et al. (2021). However, we found that the instructions discovered by APE can achieve very high truthfulness with answers such as "No comment," but these answers provide little information. We used our top candidates to further investigate the trade-off between truthfulness and informativeness. We visualize the top 10 proposed samples across the three metrics on the truthfulness-informative plots shown in Figure 5(c) and Figure 5(d). While APE achieves over $40 \%$ accuracy in providing both true and informative answers (v.s. $30 \%$ by the "help" prompt from humans), the instructions discovered tend to target the two ends of this \%true-\%info Pareto frontier.</p>
<h1>5 Quantitative Analysis</h1>
<p>In this section, we conduct quantitative analyses to better understand the three main components of our method: proposal distribution, score functions, and iterative search. Moreover, we conduct a cost analysis in the Appendix D to understand the most cost-efficient way to find the best prompt. We observe the larger and more powerful language models are more cost-effective for generating the best prompt despite a higher per-token cost.</p>
<h3>5.1 LLMs for Proposal Distribution</h3>
<p>How does the proposal quality change as we increase the model size? To understand how the model size affects the quality of the initial proposal distribution, we examine eight different models ${ }^{5}$ available via the OpenAI API. To assess the quality of the proposal distribution, we generate 250 instructions per model and compute the execution accuracy on 50 test data points. We visualize the survival function (percentage of instructions with test accuracy greater than a certain threshold) and the histogram of test accuracy for a simple task (i.e., Pluralization) in Figure 6 (a) and include a similar plot for a more challenging task (Start With) in the Appendix (Figure 28). As shown in both figures (and unsurprisingly), larger models tend to produce better proposal distributions than smaller ones, as do the models that were fine-tuned to follow human instructions. On the simple task, all instructions generated by the best model, InstructGPT (175B), have reasonable test accuracy. In contrast, half of the instructions are off-topic and perform poorly on the more challenging task.</p>
<h3>5.2 LLMs FOR SELECTION</h3>
<p>Does proposal quality matter under selection? If we sample more instructions from the LLMs, then it becomes more likely for us to find better instructions. To verify this hypothesis, we increase the sample size from 4 to 128 and evaluate the test accuracy change. Figure 7 (Left) shows a monotonically increasing trend with a diminishing return, as human-level performance is achieved with 64 instruction samples. Thus, we choose 50 as our default sample size. Under this configuration, we investigate how the proposal distribution affects the test accuracy of the best instruction selected by our algorithm. Figure 1(b) shows that though the small models may be less likely to generate good instructions, they nonetheless generate some good ones if we sample enough candidates. Therefore,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: (Left) Quality of the proposal distribution of models with different size as assessed by test execution accuracy. (Right) Iterative Monte Carlo search improves the quality of the instruction candidates at each round.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: (Left) Test execution of the best instruction as we increase the number of instruction candidates. We report the mean and standard deviation across 6 different tasks. (Middle) Spearman Correlation between the test accuracy and two metrics on 24 tasks. (Right) Test execution accuracy of the best instruction selected using APE and iterative APE (APE (IT)).</p>
<p>We still find promising instructions with a small model by running our selection algorithm, explaining why our method outperforms the greedy approach <em>Honovich et al. (2022)</em> across all eight models.</p>
<p><strong>Which scoring function is better?</strong> We compute the correlation between the test accuracy and two metrics on 24 instruction induction tasks to study how good our proposed metrics are. We generate 250 instructions per task using InstructGPT (175B) in "forward" mode and compute the metric score and test accuracy on 10 test data points. We visualize the Spearman correlation between the test accuracy and two metrics. Figure 7 (Middle) shows that the execution accuracy aligns better with the test performance across the tasks. Thus, we choose it as our default metric unless otherwise stated.</p>
<h3>5.3 Iterative Monte Carlo Search</h3>
<p><strong>Does Iterative Search improve the instruction quality?</strong> We visualize the survival function and histogram of test accuracy on the "Passivization" task in Figure 6 (Right) and include five more tasks in the Appendix. The survival plot shows that the curves increase as the round goes up, which suggests that iterative search does result in a higher-quality proposal set. However, we observe diminishing returns to further selection rounds as the quality seems to stabilize after three rounds.</p>
<p><strong>Do we need Iterative Search?</strong> We compare APE and iterative APE on six tasks<sup>6</sup>. As shown in Figure 7, the iterative search marginally improves performance on tasks where APE underperforms humans but achieves similar performance on the other tasks. This is consistent with our hypothesis that iterative search would be most useful on tasks where generating a good initial <em>U</em> is challenging.</p>
<h2>6 Conclusion</h2>
<p>Large language models can be seen as general-purpose computers that execute programs specified by natural language prompts. We automate the prompt engineering process by formulating it as a black-box optimization problem, which we propose to solve using efficient search algorithms guided by LLMs. Our method achieves human-level performance on various tasks with minimum human inputs. As recent LLMs demonstrate an impressive ability to follow human instruction, we expect many future models, including those for formal program synthesis, to have a natural language interface. This work builds the foundation to control and steer generative artificial intelligence.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>We would like to thank Or Honovich and Michael Zhang for their help and valuable feedback. JB was supported by NSERC Grant [2020-06904], CIFAR AI Chairs program, Google Research Scholar Program and Amazon Research Award. KP was supported by NSERC PGS-D. SP was supported by NSERC CGS-D. HC was supported by NSERC CGS-D and RBC Graduate Fellowship. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute for Artificial Intelligence.</p>
<h2>REFERENCES</h2>
<p>Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems, 2021.</p>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022.</p>
<p>Eyal Ben-David, Nadav Oved, and Roi Reichart. Pada: A prompt-based autoregressive approach for adaptation to unseen domains. arXiv preprint arXiv:2102.12206, 2021.</p>
<p>Gregor Betz, Kyle Richardson, and Christian Voigt. Thinking aloud: Dynamic context generation improves zero-shot reasoning performance of gpt-2. arXiv preprint arXiv:2103.13033, 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Joe Davison, Joshua Feldman, and Alexander M Rush. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pp. 1173-1178, 2019.</p>
<p>Jacob Devlin, Rudy R Bunel, Rishabh Singh, Matthew Hausknecht, and Pushmeet Kohli. Neural program meta-induction. Advances in Neural Information Processing Systems, 30, 2017.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320-335, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/ v1/2022.acl-long.26. URL https://aclanthology.org/2022.acl-long. 26.</p>
<p>Kevin Ellis, Lucas Morales, Mathias Sablé-Meyer, Armando Solar-Lezama, and Josh Tenenbaum. Learning libraries of subroutines for neurally-guided bayesian program induction. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ 7aa685b3bldcld6780bf36f7340078c9-Paper.pdf.</p>
<p>Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning. In Proceedings of the 42nd acm sigplan international conference on programming language design and implementation, pp. 835-850, 2021.</p>
<p>Tianyu Gao. Prompting: Better ways of using language models for nlp tasks. The Gradient, 2021.
Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3816-3830, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.295. URL https://aclanthology.org/2021.acl-long. 295.</p>
<p>Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations and Trends ${ }^{\circledR}$ in Programming Languages, 4(1-2):1-119, 2017.</p>
<p>Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022.</p>
<p>Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma. Jigsaw: Large language models meet program synthesis. In Proceedings of the 44th International Conference on Software Engineering, pp. 1219-1231, 2022.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. Accelerating search-based program synthesis using learned probabilistic models. ACM SIGPLAN Notices, 53(4):436-449, 2018.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, 2021.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022.</p>
<p>Percy Liang, Michael I. Jordan, and Dan Klein. Learning programs: A hierarchical bayesian approach. In Johannes Fürnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 639-646. Omnipress, 2010. URL https://icml.cc/Conferences/2010/papers/568.pdf.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3214-3252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https: //aclanthology.org/2022.acl-long. 229.</p>
<p>Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. arXiv preprint arXiv:2103.10385, 2021.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.</p>
<p>Dougal Maclaurin and Ryan Prescott Adams. Firefly monte carlo: Exact mcmc with subsets of data. In Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015.</p>
<p>Aditya Menon, Omer Tamuz, Sumit Gulwani, Butler Lampson, and Adam Kalai. A machine learning framework for programming by example. In International Conference on Machine Learning, pp. 187-195. PMLR, 2013.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. Advances in Neural Information Processing Systems, 34:11054-11070, 2021.</p>
<p>Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5203-5212, 2021.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.</p>
<p>Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.</p>
<p>Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-7, 2021.</p>
<p>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684-10695, 2022.</p>
<p>Subhro Roy and Dan Roth. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413, 2016.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, 2022.</p>
<p>Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255-269, 2021.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. In Empirical Methods in Natural Language Processing (EMNLP), 2020.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? arXiv preprint arXiv:2109.01247, 2021.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.</p>
<p>Catherine Wong, Kevin M Ellis, Joshua Tenenbaum, and Jacob Andreas. Leveraging language to learn program abstractions and search heuristics. In International Conference on Machine Learning, pp. 11193-11204. PMLR, 2021.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263-27277, 2021.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D Goodman. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<p>Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.</p>
<h1>A Prompt Engineering in the Wild</h1>
<p>Large models with natural language interfaces, including models for text generation and image synthesis, have seen an increasing amount of public usage in recent years. As finding the right prompt can be difficult for humans, a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Among others, see, for example:</p>
<ul>
<li>https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/</li>
<li>https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/</li>
<li>https://news.ycombinator.com/item?id=32943224</li>
<li>https://promptomania.com/stable-diffusion-prompt-builder/</li>
<li>https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion</li>
</ul>
<p>In this paper we apply APE to generate effective instructions for steering LLMs, but the general framework Algorithm 1 could be applied to steer other models with natural language interfaces so long as an appropriate proposal method and scoring function can be designed.</p>
<h1>B IMPLEMENTATION DETAILS</h1>
<p>Table 1: Detailed description of 24 instruction induction tasks proposed in Honovich et al. (2022). For convenience, the original table from Honovich et al. (2022) is duplicated here.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Instruction</th>
<th style="text-align: center;">Demonstration</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">First Letter</td>
<td style="text-align: center;">Extract the first letter of the input word.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{c}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Second Letter</td>
<td style="text-align: center;">Extract the second letter of the input word.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{a}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">List Letters</td>
<td style="text-align: center;">Break the input word into letters, separated by spaces.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{c}$ a t</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Starting With</td>
<td style="text-align: center;">Extract the words starting with a given letter from the input sentence.</td>
<td style="text-align: center;">The man whose car I hit last week sued me. $[\mathrm{m}] \rightarrow$ man, me</td>
</tr>
<tr>
<td style="text-align: center;">Morphosyntax</td>
<td style="text-align: center;">Pluralization</td>
<td style="text-align: center;">Convert the input word to its plural form.</td>
<td style="text-align: center;">cat $\rightarrow$ cats</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Passivization</td>
<td style="text-align: center;">Write the input sentence in passive form.</td>
<td style="text-align: center;">The artist introduced the scientist. $\rightarrow$ The scientist was introduced by the artist.</td>
</tr>
<tr>
<td style="text-align: center;">Syntax</td>
<td style="text-align: center;">Negation</td>
<td style="text-align: center;">Negate the input sentence.</td>
<td style="text-align: center;">Time is finite $\rightarrow$ Time is not finite.</td>
</tr>
<tr>
<td style="text-align: center;">Lexical <br> Semantics</td>
<td style="text-align: center;">Antonyms</td>
<td style="text-align: center;">Write a word that means the opposite of the input word.</td>
<td style="text-align: center;">won $\rightarrow$ lost</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Synonyms</td>
<td style="text-align: center;">Write a word with a similar meaning to the input word.</td>
<td style="text-align: center;">alleged $\rightarrow$ supposed</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Membership</td>
<td style="text-align: center;">Write all the animals that appear in the given list.</td>
<td style="text-align: center;">cat, helicopter, cook, whale, frog, lion $\rightarrow$ frog, cat, lion, whale</td>
</tr>
<tr>
<td style="text-align: center;">Phonetics</td>
<td style="text-align: center;">Rhymes</td>
<td style="text-align: center;">Write a word that rhymes with the input word.</td>
<td style="text-align: center;">sing $\rightarrow$ ring</td>
</tr>
<tr>
<td style="text-align: center;">Knowledge</td>
<td style="text-align: center;">Larger Animal</td>
<td style="text-align: center;">Write the larger of the two given animals.</td>
<td style="text-align: center;">koala, snail $\rightarrow$ koala</td>
</tr>
<tr>
<td style="text-align: center;">Semantics</td>
<td style="text-align: center;">Cause Selection</td>
<td style="text-align: center;">Find which of the two given cause and effect sentences is the cause.</td>
<td style="text-align: center;">Sentence 1: The soda went flat. Sentence 2: The bottle was left open. $\rightarrow$ The bottle was left open.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Common <br> Concept</td>
<td style="text-align: center;">Find a common characteristic for the given objects.</td>
<td style="text-align: center;">guitars, pendulums, neutrinos $\rightarrow$ involve oscillations.</td>
</tr>
<tr>
<td style="text-align: center;">Style</td>
<td style="text-align: center;">Formality</td>
<td style="text-align: center;">Rephrase the sentence in formal language.</td>
<td style="text-align: center;">Please call once you get there $\rightarrow$ Please call upon your arrival.</td>
</tr>
<tr>
<td style="text-align: center;">Numerical</td>
<td style="text-align: center;">Sum</td>
<td style="text-align: center;">Sum the two given numbers.</td>
<td style="text-align: center;">$2210 \rightarrow 32$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Difference</td>
<td style="text-align: center;">Subtract the second number from the first.</td>
<td style="text-align: center;">$3222 \rightarrow 10$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number to Word</td>
<td style="text-align: center;">Write the number in English words.</td>
<td style="text-align: center;">$26 \rightarrow$ twenty-six</td>
</tr>
<tr>
<td style="text-align: center;">Multilingual</td>
<td style="text-align: center;">Translation</td>
<td style="text-align: center;">Translate the word into German / Spanish / French.</td>
<td style="text-align: center;">game $\rightarrow$ juego</td>
</tr>
<tr>
<td style="text-align: center;">GLUE</td>
<td style="text-align: center;">Sentiment <br> Analysis</td>
<td style="text-align: center;">Determine whether a movie review is positive or negative.</td>
<td style="text-align: center;">The film is small in scope, yet perfectly formed. $\rightarrow$ positive</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence <br> Similarity</td>
<td style="text-align: center;">Rate the semantic similarity of two input sentences on a scale of 0 - definitely not to 5 - perfectly.</td>
<td style="text-align: center;">Sentence 1: A man is smoking. Sentence 2: A man is skating. $\rightarrow$ 0 - definitely not</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Word in Context</td>
<td style="text-align: center;">Determine whether an input word has the same meaning in the two input sentences.</td>
<td style="text-align: center;">Sentence 1: Approach a task. Sentence 2: To approach the city. Word: approach $\rightarrow$ not the same</td>
</tr>
</tbody>
</table>
<p>Table 2: Detailed description of BIG-Bench Instruction Induction (BBII), a clean and tractable subset of 21 tasks that have a clear human written instruction that can be applied to all examples in the dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Keywords</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">causal judgment</td>
<td style="text-align: center;">Answer questions about causal attribution</td>
<td style="text-align: center;">causal reasoning, common sense, multiple choice, reading comprehension, social reasoning</td>
</tr>
<tr>
<td style="text-align: center;">disambiguation qa</td>
<td style="text-align: center;">Clarify the meaning of sentences with ambiguous pronouns</td>
<td style="text-align: center;">common sense, gender bias, many-shot, multiple choice</td>
</tr>
<tr>
<td style="text-align: center;">dyck languages</td>
<td style="text-align: center;">Correctly close a Dyck-n word</td>
<td style="text-align: center;">algebra, arithmetic, logical reasoning, multiple choice</td>
</tr>
<tr>
<td style="text-align: center;">epistemic reasoning</td>
<td style="text-align: center;">Determine whether one sentence entails the next</td>
<td style="text-align: center;">common sense, logical reasoning, multiple choice, social reasoning, theory of mind</td>
</tr>
<tr>
<td style="text-align: center;">gender inclusive sentences german</td>
<td style="text-align: center;">Given a German language sentence that does not use gender-inclusive forms, transform it to gender-inclusive forms</td>
<td style="text-align: center;">free response, grammar, inclusion, nonEnglish, paraphrase</td>
</tr>
<tr>
<td style="text-align: center;">implicatures</td>
<td style="text-align: center;">Predict whether Speaker 2's answer to Speaker 1 counts as a yes or as a no</td>
<td style="text-align: center;">contextual question-answering, multiple choice, reading comprehension, social reasoning, theory of mind</td>
</tr>
<tr>
<td style="text-align: center;">linguistics puzzles</td>
<td style="text-align: center;">Solve Rosetta Stone-style linguistics puzzles</td>
<td style="text-align: center;">free response, human-like behavior, linguistics, logical reasoning, reading comprehension</td>
</tr>
<tr>
<td style="text-align: center;">logical fallacy detection</td>
<td style="text-align: center;">Detect informal and formal logical fallacies</td>
<td style="text-align: center;">logical reasoning, multiple choice</td>
</tr>
<tr>
<td style="text-align: center;">movie recommendation</td>
<td style="text-align: center;">Recommend movies similar to the given list of movies</td>
<td style="text-align: center;">emotional intelligence, multiple choice</td>
</tr>
<tr>
<td style="text-align: center;">navigate</td>
<td style="text-align: center;">Given a series of navigation instructions, determine whether one would end up back at the starting point</td>
<td style="text-align: center;">arithmetic, logical reasoning, mathematics, multiple choice</td>
</tr>
<tr>
<td style="text-align: center;">object counting</td>
<td style="text-align: center;">Questions that involve enumerating objects of different types and asking the model to count them</td>
<td style="text-align: center;">free response, logical reasoning</td>
</tr>
<tr>
<td style="text-align: center;">operators</td>
<td style="text-align: center;">Given a mathematical operator definition in natural language, apply it</td>
<td style="text-align: center;">free response, mathematics, numerical response</td>
</tr>
<tr>
<td style="text-align: center;">presuppositions as nli</td>
<td style="text-align: center;">Determine whether the first sentence entails or contradicts the second</td>
<td style="text-align: center;">common sense, logical reasoning, multiple choice</td>
</tr>
<tr>
<td style="text-align: center;">question selection</td>
<td style="text-align: center;">Given a short answer along with its context, select the most appropriate question which to the given short answer</td>
<td style="text-align: center;">multiple choice, paraphrase, reading comprehension, summarization</td>
</tr>
<tr>
<td style="text-align: center;">ruin names</td>
<td style="text-align: center;">Select the humorous edit that 'ruins' the input movie or musical artist name</td>
<td style="text-align: center;">emotional understanding, multiple choice</td>
</tr>
<tr>
<td style="text-align: center;">snarks</td>
<td style="text-align: center;">Determine which of two sentences is sarcastic</td>
<td style="text-align: center;">emotional understanding, humor, multiple choice</td>
</tr>
<tr>
<td style="text-align: center;">sports understanding</td>
<td style="text-align: center;">Determine whether an artificially constructed sentence relating to sports is plausible or implausible</td>
<td style="text-align: center;">common sense, context-free question answering, domain specific, multiple choice</td>
</tr>
<tr>
<td style="text-align: center;">tense</td>
<td style="text-align: center;">Modify the tense of a given sentence</td>
<td style="text-align: center;">free response, paraphrase, syntax</td>
</tr>
<tr>
<td style="text-align: center;">winowhy</td>
<td style="text-align: center;">Evaluate the reasoning in answering Winograd Schema Challenge questions</td>
<td style="text-align: center;">causal reasoning, common sense, multiple choice, social reasoning</td>
</tr>
<tr>
<td style="text-align: center;">word sorting</td>
<td style="text-align: center;">Sort a list of words</td>
<td style="text-align: center;">algorithms, free response</td>
</tr>
<tr>
<td style="text-align: center;">word unscrambling</td>
<td style="text-align: center;">Unscramble the given letters to form an English word</td>
<td style="text-align: center;">free response, implicit reasoning, tokenization</td>
</tr>
</tbody>
</table>
<h1>B. 1 BIG-Bench Instruction Induction (BBII) Selection Process</h1>
<p>Step 1: BIG-Bench contains a large number of evaluation tasks with different level of quality. For example, some of the tasks only have the minimum number of examples needed to qualify for submission, while other tasks may lack an appropriate human baselines. Therefore, we follow Suzgun et al. (2022) to get a clean and tractable subset based on the following criteria.</p>
<p>Table 3: Filtering criteria to used to create the BIG-Bench Instruction Induction (BBII) subset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"># Tasks</th>
<th style="text-align: left;">Criteria</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">212</td>
<td style="text-align: left;">All BIG-Bench tasks</td>
</tr>
<tr>
<td style="text-align: left;">170</td>
<td style="text-align: left;">All JSON tasks</td>
</tr>
<tr>
<td style="text-align: left;">127</td>
<td style="text-align: left;">After filtering out tasks with more than one sub-task</td>
</tr>
<tr>
<td style="text-align: left;">74</td>
<td style="text-align: left;">After filtering out tasks with fewer than 150 examples</td>
</tr>
<tr>
<td style="text-align: left;">67</td>
<td style="text-align: left;">After filtering out tasks without human-rater baselines</td>
</tr>
<tr>
<td style="text-align: left;">57</td>
<td style="text-align: left;">After filtering out tasks that do not use multiple-choice or exact match as the evaluation metric</td>
</tr>
</tbody>
</table>
<h2>Criteria: JSON Tasks.</h2>
<p>Discarded tasks: abstraction and reasoning corpus, bbq lite, bias from probabilities, boolean expressions, com2sense, context definition alignment, convinceme, coqa conversational question answering, cycled letters, diverse social bias, dynamic counting, factuality of summary, forecasting subquestions, gender sensitivity chinese, gender sensitivity english, high low game, long context integration, multistep arithmetic, muslim violence bias, program synthesis, protein interacting sites, python programming challenge, question answer creation, roots optimization and games, self awareness, self evaluation courtroom, self evaluation tutoring, simple arithmetic, spelling bee, squad shifts, subject verb agreement, sudoku, taboo, talkdown, text navigation game, training on test set, truthful qa, twenty questions, unqover, web of lies, word problems on sets and graphs, yes no black white.</p>
<h2>Criteria: Tasks without sub-task.</h2>
<p>Discarded tasks: abstract narrative understanding, arithmetic, authorship verification, bbq lite json, cause and effect, chess state tracking, cifar10 classification, color, conceptual combinations, conlang translation, cs algorithms, elementary math qa, fact checker, gem, goal step wikihow, hhh alignment, indic cause and effect, intersect geometry, kanji ascii, key value maps, language games, linguistic mappings, list functions, logical deduction, metaphor understanding, minute mysteries qa, modified arithmetic, mult data wrangling, multiemo, natural instructions, periodic elements, physics, real or fake text, simp turing concept, simple arithmetic json subtasks, simple ethical questions, strange stories, symbol interpretation, tracking shuffled objects, undo permutation, unit conversion, unit interpretation, unnatural in context learning.</p>
<h2>Criteria: The task includes at least 150 examples with input-output pairs.</h2>
<p>Discarded tasks: analytic entailment, auto debugging, code line description, codenames, common morpheme, crash blossom, crass ai, cryobiology spanish, dark humor detection, emoji movie, emojis emotion prediction, empirical judgments, english proverbs, english russian proverbs, entailed polarity, entailed polarity hindi, evaluating information essentiality, figure of speech detection, general knowledge, gre reading comprehension, human organs senses, identify math theorems, identify odd metaphor, implicit relations, international phonetic alphabet nli, irony identification, known unknowns, logical args, logical sequence, mathematical induction, misconceptions russian, nonsense words grammar, novel concepts, odd one out, penguins in a table, persian idioms, phrase relatedness, physical intuition, physics questions, repeat copy logic, rephrase, riddle sense, scientific press release, sentence ambiguity, similarities abstraction, simple arithmetic json, simple arithmetic json multiple choice, simple arithmetic multiple targets json, simple text editing, sufficient information, suicide risk, swedish to german proverbs, what is the tao.</p>
<h2>Criteria: The task contains reported (average) human-rater or random performance.</h2>
<p>Discarded tasks: contextual parametric knowledge conflicts, hinglish toxicity, medical questions russian, parsinlu qa, swahili english proverbs, tellmewhy, which wiki edit.</p>
<h2>Criteria: The task is classification or uses exact match as the evaluation metric.</h2>
<p>Discarded tasks: auto categorization, few shot nlg, hindi question answering, international phonetic alphabet transliterate, polish sequence labeling, qa wikidata, semantic parsing in context sparc, semantic parsing spider, social support, topical chat.</p>
<p>Step 2: We do a manual inspection to divide the remaining tasks to the following three categories. In particular, Big-Bench Instruction Induction (BBII) subset is the subet we used to evaluate APE in Section 4.2.</p>
<ul>
<li>BBII Subset: A subset of Big Bench Tasks that satisfy the instruction induction format: each example in the dataset can be expressed as a question-answer pair, all examples focus on the same question that can be clearly described by a human instruction, and there is a human instruction available in the task JSON file.</li>
<li>Invalid Format: Tasks that do not match the instruction induction format: each example in the dataset asks a different question, or clear human instruction is not available.</li>
<li>Out of Scope: Tasks that are outside the scope of this work: not solvable by authors within 60 minutes, or requires specialized knowledge.</li>
</ul>
<p>Table 4: Filtering criteria to used to create the BIG-Bench Instruction Induction (BBII) subset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"># Category</th>
<th style="text-align: center;"># Tasks</th>
<th style="text-align: center;">Tasks Names</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BBII Subset</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">causal judgment, disambiguation qa, dyck language, epistemic reasoning, gender inclusive sentences german, implicatures, linguistics puzzles, logical fallacy detection, movie recommendation, navigate, object counting, operators, presuppositions as nli, question selection, ruin names, snarks, sports understanding, tense, winowhy, word sorting, word unscrambling.</td>
</tr>
<tr>
<td style="text-align: center;">Invalid Format</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">anachronisms, analogical similarity, bridging anaphora resolution barqa, data understanding, disfl qa, fantasy reasoning, formal fallacies syllogisms negation, hindu knowledge, hyperbaton, intent recognition, logic grid puzzle, paragraph segmentation, play dialog same or different, reasoning about colored objects, salient translation error detection, social iqa, strategyqa, temporal sequences, timedial, understanding fables, vitaminc fact verification.</td>
</tr>
<tr>
<td style="text-align: center;">Out of Scope</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">ascii word recognition, checkmate in one, chinese remainder theorem, cryptonite, discourse marker prediction, geometric shapes, kannada, language identification, matrixshapes, mnist ascii, moral permissibility, movie dialog same or different, parsinlu reading comprehension.</td>
</tr>
</tbody>
</table>
<p>Table 5: Raw templates used for model prompting in our experiments</p>
<table>
<thead>
<tr>
<th>Usage</th>
<th>Template</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zero-shot Evaluation</td>
<td>Instruction: [INSTRUCTION] <br> Input: $\left[Q_{\text {test }}\right.$ ] $]$ Output: $&lt;$ COMPLETE $&gt;$</td>
</tr>
<tr>
<td>Few-shot Evaluation</td>
<td>Instruction: [INSTRUCTION] <br> Input: $\left[Q_{1}\right.$ ] $]$ Output: $\left[A_{1}\right.$ ] $]$ Input: $\left[Q_{2}\right.$ ] $]$ Output: $\left[A_{2}\right] \ldots$ <br> Input: $\left[Q_{\text {test }}\right.$ ] $]$ Output: $&lt;$ COMPLETE $&gt;$</td>
</tr>
<tr>
<td>Forward Generation</td>
<td>I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. $\backslash$ Here are the input-output pairs: <br> Input: $\left[Q_{1}\right.$ ] $]$ Output: $\left[A_{1}\right.$ ] $]$ Input: $\left[Q_{2}\right.$ ] $]$ Output: $\left[A_{2}\right] \ldots$ <br> The instruction was $&lt;$ COMPLETE $&gt;$</td>
</tr>
<tr>
<td>Reverse Generation 1</td>
<td>I instructed my friend to<INSERT>. The friend read the instruction and wrote an output for every one of the inputs. $\backslash$ Here are the input-output pairs: <br> Input: $\left[Q_{1}\right.$ ] $]$ Output: $\left[A_{1}\right.$ ] $]$ Input: $\left[Q_{2}\right.$ ] $]$ Output: $\left[A_{2}\right] \ldots$</td>
</tr>
<tr>
<td>Reverse Generation 2</td>
<td>Professor Smith was given the following instructions:<INSERT> $\backslash$ Here are the Professor's responses: <br> $\mathbf{Q}:\left[Q_{1}\right]$ A: $\left[A_{1}\right.$ ] $\left.\mathbf{Q}:\left[Q_{2}\right]\right]$ A: $\left[A_{2}\right] \ldots$</td>
</tr>
<tr>
<td>Resample Instruction</td>
<td>Generate a variation of the following instruction while keeping the semantic meaning. <br> Input: [INSTRUCTION] $\$ Output:<COMPLETE></td>
</tr>
<tr>
<td>Zero-shot-CoT</td>
<td>Instruction: Answer the following question. <br> $\mathbf{Q}:[$ INPUT $] \backslash$ A: Let's $&lt;$ INSERT $&gt;$. [OUTPUT]</td>
</tr>
</tbody>
</table>
<h1>C ADDITIONAL RESULTS</h1>
<h2>C. 1 INSTRUCTION INDUCTION</h2>
<p>Few-shot In-context Learning We evaluated APE-generated instructions in the few-shot incontext learning, where we insert the instruction before the in-context demonstrations. Those instructions are selected based on zero-shot execution accuracy, and we denote this setting as "Instruction + In-context" in Figure 8. As shown in Figure 8, adding an instruction achieves a comparable or better test performance than the standard in-context learning performance on 21 of 24 tasks. Counter-intuitively, adding in-context examples for Rhymes, Large Animal, and Second Letters hurts model performance. We conjecture that it may be because the selected instructions overfit the zero-shot learning scenario and thus do not perform well on the few-shot case. Therefore, we experiment using few-shot execution accuracy as the selection metric. Figure 14 shows that the few-shot metric achieves comparable or slightly better than the zero-shot metric except for Rhymes. To have an intuitive understanding of what is happening, we provide a qualitative analysis below.</p>
<p>Few-shot Qualitative Analysis We find an adversarial case on Rhymes when combining the instruction and in-context prompts. Table 8 shows that 4 of 5 filtered instructions ask to echo the input word. These proposals effectively hack the evaluation with near-perfect test accuracy, as every word rhymes with itself. However, adding in-context examples for these instructions creates a misalignment between instruction (induces trivial rhymes) and context (induces non-trivial rhymes), resulting in a significant drop in performance. If we instead score the instructions based on the few-shot metric, this performance drop can be alleviated since the model can choose a more aligned instruction.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Few-shot in-context test accuracy on 24 Instruction Induction tasks. APE improves the few-shot in-context learning performance on 21 out of 24 tasks.</p>
<h1>C. 2 BIG-Bench Instruction Induction</h1>
<p>We use APE to generate new prompts for the tasks in BIG-Bench Instruction Induction (BBII). When compared to human prompts, APE-generated prompts improve or match zero-shot performance on 17 out of 21 tasks. We report the normalized preferred metric defined in Srivastava et al. (2022). Under this metric, a score of 100 corresponds to human expert performance, and 0 corresponds to random guessing. Note that a model can achieve a score less than 0 if it performs worse than random guessing on a multiple-choice task.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9: APE improves or matches normalized zero-shot performance on 17 out of 21 BIG-Bench Instruction Induction tasks.</p>
<p>Table 6: Zero-shot normalized test performance on 21 BIG-Bench Instruction Induction tasks. APE improves or matches performance on 17 out of 21 tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Normalized Performance</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">APE</td>
</tr>
<tr>
<td style="text-align: left;">causal judgment</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">18.0</td>
</tr>
<tr>
<td style="text-align: left;">disambiguation qa</td>
<td style="text-align: center;">-0.4</td>
<td style="text-align: center;">$\mathbf{5 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">dyck languages</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">$\mathbf{1 8 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">epistemic reasoning</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">$\mathbf{3 8 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">gender inclusive sentences german</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">$\mathbf{2 2 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">implicatures</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">60.0</td>
</tr>
<tr>
<td style="text-align: left;">linguistics puzzles</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">logical fallacy detection</td>
<td style="text-align: center;">$\mathbf{2 4 . 0}$</td>
<td style="text-align: center;">12.0</td>
</tr>
<tr>
<td style="text-align: left;">movie recommendation</td>
<td style="text-align: center;">-2.7</td>
<td style="text-align: center;">$\mathbf{1 2 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">navigate</td>
<td style="text-align: center;">-8.0</td>
<td style="text-align: center;">$\mathbf{1 2 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">object counting</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">$\mathbf{4 4 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">operators</td>
<td style="text-align: center;">$\mathbf{4 8 . 0}$</td>
<td style="text-align: center;">47.0</td>
</tr>
<tr>
<td style="text-align: left;">presuppositions as nli</td>
<td style="text-align: center;">$\mathbf{1 3 . 0}$</td>
<td style="text-align: center;">5.5</td>
</tr>
<tr>
<td style="text-align: left;">question selection</td>
<td style="text-align: center;">-2.6</td>
<td style="text-align: center;">$\mathbf{- 0 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">ruin names</td>
<td style="text-align: center;">$\mathbf{1 . 3}$</td>
<td style="text-align: center;">-14.7</td>
</tr>
<tr>
<td style="text-align: left;">snarks</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">$\mathbf{4 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">sports understanding</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">36.0</td>
</tr>
<tr>
<td style="text-align: left;">tense</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">$\mathbf{8 5 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">winowhy</td>
<td style="text-align: center;">-12.0</td>
<td style="text-align: center;">$\mathbf{1 2 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">word sorting</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">$\mathbf{3 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">word unscrambling</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">$\mathbf{1 5 . 0}$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ We use ada, babbage, curie, davinci, text-ada-001, text-babbage-001, text-curie-001, text-davanci-002&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
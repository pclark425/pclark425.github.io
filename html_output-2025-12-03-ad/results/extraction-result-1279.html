<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1279 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1279</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1279</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-48a8b3a0f39311a856ed3d0de84bc41c1fa386a3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/48a8b3a0f39311a856ed3d0de84bc41c1fa386a3" target="_blank">Dual Control for Exploitation and Exploration (DCEE) in Autonomous Search</a></p>
                <p><strong>Paper Venue:</strong> at - Automatisierungstechnik</p>
                <p><strong>Paper TL;DR:</strong> This new control theoretic approach achieves the optimal trade-off between exploitation and exploration in a unknown environment with an unknown target by driving the robot moving towards estimated target location while reducing its estimation uncertainty.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1279.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1279.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DCEE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual Control for Exploration and Exploitation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A control-theoretic adaptive experimental design algorithm that jointly optimises exploitation (drive the robot toward the current mean estimate of an unknown source) and exploration (reduce posterior uncertainty) by minimising the expected squared distance to the source after a hypothesised future measurement; implemented with particle filtering and one-step lookahead planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DCEE (Dual Control for Exploration and Exploitation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A planning-and-control agent combining Bayesian inference (particle filter) for source/environment parameter estimation with a stochastic MPC-style one-step (extendable to multistage) optimiser. Key components: (1) particle filter posterior over unknown parameters Θ (source position, release rate, wind speed/direction, diffusivity, particle lifetime), (2) generative dispersion + sensor models to simulate hypothesised future measurements for each candidate control action, (3) computation of a predicted posterior and its covariance for each action, and (4) selection of the action minimising expected squared distance = distance-to-predicted-mean + predicted-covariance.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian active experimental design with dual-control objective (expected posterior-aware planning); effectively an information-aware active learning / informative path planning embedded in a control-theoretic dual-control cost</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each step the agent (a) uses the particle filter posterior p(Θ|Z_k) as current belief; (b) for each candidate discrete action (finite set of 8 motion directions) it simulates hypothesised measurements by running the dispersion model with particles sampled from the posterior and applying the stochastic sensor model; (c) computes the predicted posterior mean and covariance given each hypothesised measurement; (d) evaluates the one-step expected cost J(u)=||p_{k+1}-E[s|Z_{k+1}]||^2 + trace(P_{k+1|k}) (distance term + predicted covariance); and (e) executes the action minimising J. Thus adaptation uses both current belief and predicted information gain (via predicted posterior covariance) to decide next sampling location.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Isotropic plume atmospheric dispersion (simulated and indoor experimental plume)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable and stochastic: continuous spatial field (3D agent and 3D source position and continuous environmental parameters), sparse and intermittent point observations (detection/non-detection; Poisson non-detection model; Gaussian noise when detected), unknown/uncertain environmental parameters (wind speed u_s, wind direction φ_s, diffusivity ζ_s1, particle lifetime ζ_s2), model mismatch in experiments relative to estimator model, limited sensing (threshold z_thr) and sampling intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Moderate-high: continuous 3D state for agent and source plus continuous nuisance parameters (Θ dimension ≥ 6 in extended experiments: s_x,s_y,s_z,q_s,φ_s,u_s,ζ_s1,ζ_s2); discrete action set of size 8 (cardinality = 8); sampling step size 2 m in UAV simulation; flight budget up to 900 s in simulation; per candidate action the algorithm generates ~40 hypothetical measurements per particle in simulation for prediction; computational cost increases with number of particles and number of unknown parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Simulation: across 120 runs (12 source configurations, 360 total runs described elsewhere), DCEE achieved convergence to RMSE ≈ 0.5 m at ≈ 500 s, 100% plume acquisition rate, and 100% source acquisition rate (success defined as final RMSE < 3 m) across the 120 simulation runs reported; Experiment (ground robot, 60 runs): qualitatively fastest localisation and best convergence of RMSE compared to baselines (numerical experimental RMSE curves reported but not exact tabulated final values).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines: MPC (stochastic MPC exploitation-only baseline) achieved ~80% plume acquisition and ~80% source acquisition in simulation and stalled after ~200 s in some cases (due to local minima); Entrotaxis (information-theoretic IPP) achieved 100% plume acquisition but only ~80% source acquisition and slower RMSE convergence, reaching similar accuracy to DCEE only near the end of the flight budget (~900 s). Exact numerical cumulative-reward style metrics not provided beyond these rates and RMSE-vs-time curves.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>In simulation, DCEE reached RMSE ≈ 0.5 m by ~500 s of mission time (sampling events spaced by movement step; per-step one measurement). For hypothetical-measurement computations, 40 measurement predictions per candidate location were used in simulation. No per-iteration particle count or exact sample-to-convergence counts are tabulated; however, DCEE required similar computational burden to Entrotaxis and significantly more than MPC due to particle-filter prediction of hypothesised measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Trade-off is explicit and derived from the objective: one-step expected cost decomposes into exploitation term (squared distance to predicted posterior mean) and exploration term (predicted posterior covariance, capturing expected uncertainty reduction). No manual weighting is required; the balance emerges naturally from minimising expected squared distance to the true source under hypothesised future measurements. This constitutes an implicit Bayesian active learning strategy with a dual-control probing effect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against: (1) Stochastic MPC (exploitation-focused baseline), (2) Entrotaxis (information-theoretic informative path planning / IPP baseline that minimises predicted entropy / maximises information gain).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>DCEE outperforms both MPC and Entrotaxis in simulated single-source plume localisation: faster RMSE reduction, 100% plume and source acquisition in the reported simulations (DCEE), where MPC suffered local minima and Entrotaxis converged slower (good exploration but weaker exploitation). In physical indoor experiments with more model mismatch and additional unknown environmental parameters, DCEE again showed superior and robust performance. The derived one-step cost yields an automatic, principled trade-off between exploration and exploitation without ad-hoc weighting, and the framework generalises to include environmental parameter estimation and multistage horizons (at extra computational cost).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computational burden: particle-filter based predicted-posterior simulation for each candidate action is costly, especially when estimating many environment parameters; multistage (N>1) planning greatly increases computation. Performance degrades or becomes more challenging under model mismatch, localisation/map uncertainty, and unmodelled dynamics (experiments highlighted greater model mismatch than simulation). The implemented system uses a discrete finite action set and one-step horizon (multistage extension acknowledged but not used in main experiments due to computational load). Sensitivity to prior mismatch can cause local minima for purely exploitation methods (MPC), while Entrotaxis can be slower in exploitation despite strong exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures_additional</strong></td>
                            <td>Exact particle count, timing per planning step, and wall-clock computational latency for on-board real-time operation are not fully quantified in the paper; these are practical limitations to scaling DCEE to higher-dimensional or faster problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Control for Exploitation and Exploration (DCEE) in Autonomous Search', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1279.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1279.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entrotaxis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entrotaxis (entropy-minimising informative path planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-theoretic autonomous search strategy that selects actions to maximise information gain / minimise predicted posterior entropy (active sensing/informative path planning) to reduce uncertainty about source location and environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Entrotaxis as a strategy for autonomous search and source reconstruction in turbulent conditions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Entrotaxis (information-theoretic IPP)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An informative path planning agent that chooses next sampling locations to minimise predicted entropy of the posterior (maximum-entropy-sampling principle) using the particle-filter belief over source/environment parameters; focuses on exploration (information gain) rather than direct task-driven exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information gain maximisation / entropy minimisation (informative path planning, active sensing)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each decision step the agent evaluates candidate moves by predicting their effect on posterior entropy (using current belief and the dispersion + sensor models), and selects moves that most reduce uncertainty (predicted entropy) of the estimated source/environment parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Isotropic plume atmospheric dispersion (same simulated and experimental plume environments as DCEE)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable, stochastic plume dispersion with sparse/intermittent detections, unknown source location and some environmental parameters; continuous state space for parameters; noisy sensor model with detection/non-detection events and thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same domain as DCEE: continuous multi-parameter space; action set used in experiments also discretised (8 directions); simulation used 12 source configurations and many random runs; episode horizon up to 900 s in UAV simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Simulation: Entrotaxis achieved 100% plume acquisition rate but only ~80% source acquisition rate across the reported simulation runs, with slower RMSE convergence than DCEE (converges to similar final accuracy only near end of 900 s flight budget). Experiment: Entrotaxis performed reasonably well but slower exploitation than DCEE; exact experimental success rates not tabulated numerically in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Entrotaxis exhibits strong exploration (rapid plume acquisition) but lower sample-efficiency for source localisation (slower RMSE reduction) compared to DCEE in these tasks; specific sample counts to reach given RMSE thresholds are not tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Biased heavily toward exploration (reducing entropy) and does not explicitly include an exploitation term; exploration-centric behaviour can delay precise localisation (exploitation) relative to DCEE but ensures high plume-acquisition rates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against DCEE (dual-control approach) and stochastic MPC in both simulation and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Entrotaxis is effective at exploration (100% plume acquisition in simulation), but because it focuses on entropy minimisation it is slower to converge to precise source location than DCEE and achieves lower source-acquisition rates (~80%). It demonstrates the expected behaviour of exploration-first strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Tends to travel perpendicular to wind to reduce entropy which can slow direct approach to source (exploitation) and therefore increase time-to-source localisation; less effective when exploitation (rapid convergence to source location) is required within limited mission budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Control for Exploitation and Exploration (DCEE) in Autonomous Search', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1279.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1279.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stochastic MPC (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic Model Predictive Control (exploitation-focused baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic control-theoretic planner that computes actions to minimise expected distance to the estimated source using the current posterior belief but does not account for the effect of actions on future posterior uncertainty (passive learning/certainty-equivalence style).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Stochastic MPC (exploitation-only baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An MPC-style agent that, at each step, minimises expected squared distance between the next robot position and the current posterior mean estimate of the source (uses current measurements only; does not simulate hypothesised future measurements for planning). Implemented with the same discrete action set for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses current posterior mean (estimate) to drive the agent toward the believed source location; adapts only because the posterior updates when real measurements are collected (passive learning), but actions are not chosen to actively reduce posterior uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Isotropic plume atmospheric dispersion (same as above)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable, stochastic, sparse/intermittent sensing; same sensor and dispersion complexities as other agents; environment/parameter uncertainty may cause prior mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as DCEE/Entrotaxis; action set size 8; simulation scenarios included 12 source configurations and up to 900 s flight budget.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Simulation baseline: MPC achieved ~80% plume acquisition and ~80% source acquisition; early RMSE reduction comparable to DCEE initially (due to central prior mean) but later often fails to converge further (stalls around ~200 s in some scenarios) due to local minima when prior mismatches ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>MPC can produce early accidental plume samples when prior mean lies along plume path, but it is less sample-efficient at ultimately reducing RMSE across the varied scenarios compared to DCEE; exact sample counts to convergence not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Pure exploitation (certainty-equivalence): focuses on minimising immediate distance-to-estimate, does not take actions explicitly to reduce uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Used as a baseline against DCEE and Entrotaxis in simulation and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>MPC (exploitation-only) can perform acceptably when priors align well with the true source, but is prone to local minima and fails to adequately explore when prior is wrong — resulting in lower plume and source acquisition rates compared with DCEE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Prone to failure when prior belief is inaccurate (local minima), lacks active probing so can miss plume entirely in some configurations; computationally cheaper but less robust in partially known environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Control for Exploitation and Exploration (DCEE) in Autonomous Search', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Entrotaxis as a strategy for autonomous search and source reconstruction in turbulent conditions <em>(Rating: 2)</em></li>
                <li>Information-based search for an atmospheric release using a mobile robot: Algorithm and experiments <em>(Rating: 2)</em></li>
                <li>Autonomous multi-robot search for a hazardous source in a turbulent environment <em>(Rating: 1)</em></li>
                <li>Stochastic model predictive control with active uncertainty learning: A survey on dual control <em>(Rating: 2)</em></li>
                <li>MPC-based dual control with online experiment design <em>(Rating: 1)</em></li>
                <li>Dual effect, certainty equivalence, and separation in stochastic control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1279",
    "paper_id": "paper-48a8b3a0f39311a856ed3d0de84bc41c1fa386a3",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "DCEE",
            "name_full": "Dual Control for Exploration and Exploitation",
            "brief_description": "A control-theoretic adaptive experimental design algorithm that jointly optimises exploitation (drive the robot toward the current mean estimate of an unknown source) and exploration (reduce posterior uncertainty) by minimising the expected squared distance to the source after a hypothesised future measurement; implemented with particle filtering and one-step lookahead planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DCEE (Dual Control for Exploration and Exploitation)",
            "agent_description": "A planning-and-control agent combining Bayesian inference (particle filter) for source/environment parameter estimation with a stochastic MPC-style one-step (extendable to multistage) optimiser. Key components: (1) particle filter posterior over unknown parameters Θ (source position, release rate, wind speed/direction, diffusivity, particle lifetime), (2) generative dispersion + sensor models to simulate hypothesised future measurements for each candidate control action, (3) computation of a predicted posterior and its covariance for each action, and (4) selection of the action minimising expected squared distance = distance-to-predicted-mean + predicted-covariance.",
            "adaptive_design_method": "Bayesian active experimental design with dual-control objective (expected posterior-aware planning); effectively an information-aware active learning / informative path planning embedded in a control-theoretic dual-control cost",
            "adaptation_strategy_description": "At each step the agent (a) uses the particle filter posterior p(Θ|Z_k) as current belief; (b) for each candidate discrete action (finite set of 8 motion directions) it simulates hypothesised measurements by running the dispersion model with particles sampled from the posterior and applying the stochastic sensor model; (c) computes the predicted posterior mean and covariance given each hypothesised measurement; (d) evaluates the one-step expected cost J(u)=||p_{k+1}-E[s|Z_{k+1}]||^2 + trace(P_{k+1|k}) (distance term + predicted covariance); and (e) executes the action minimising J. Thus adaptation uses both current belief and predicted information gain (via predicted posterior covariance) to decide next sampling location.",
            "environment_name": "Isotropic plume atmospheric dispersion (simulated and indoor experimental plume)",
            "environment_characteristics": "Partially observable and stochastic: continuous spatial field (3D agent and 3D source position and continuous environmental parameters), sparse and intermittent point observations (detection/non-detection; Poisson non-detection model; Gaussian noise when detected), unknown/uncertain environmental parameters (wind speed u_s, wind direction φ_s, diffusivity ζ_s1, particle lifetime ζ_s2), model mismatch in experiments relative to estimator model, limited sensing (threshold z_thr) and sampling intervals.",
            "environment_complexity": "Moderate-high: continuous 3D state for agent and source plus continuous nuisance parameters (Θ dimension ≥ 6 in extended experiments: s_x,s_y,s_z,q_s,φ_s,u_s,ζ_s1,ζ_s2); discrete action set of size 8 (cardinality = 8); sampling step size 2 m in UAV simulation; flight budget up to 900 s in simulation; per candidate action the algorithm generates ~40 hypothetical measurements per particle in simulation for prediction; computational cost increases with number of particles and number of unknown parameters.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Simulation: across 120 runs (12 source configurations, 360 total runs described elsewhere), DCEE achieved convergence to RMSE ≈ 0.5 m at ≈ 500 s, 100% plume acquisition rate, and 100% source acquisition rate (success defined as final RMSE &lt; 3 m) across the 120 simulation runs reported; Experiment (ground robot, 60 runs): qualitatively fastest localisation and best convergence of RMSE compared to baselines (numerical experimental RMSE curves reported but not exact tabulated final values).",
            "performance_without_adaptation": "Baselines: MPC (stochastic MPC exploitation-only baseline) achieved ~80% plume acquisition and ~80% source acquisition in simulation and stalled after ~200 s in some cases (due to local minima); Entrotaxis (information-theoretic IPP) achieved 100% plume acquisition but only ~80% source acquisition and slower RMSE convergence, reaching similar accuracy to DCEE only near the end of the flight budget (~900 s). Exact numerical cumulative-reward style metrics not provided beyond these rates and RMSE-vs-time curves.",
            "sample_efficiency": "In simulation, DCEE reached RMSE ≈ 0.5 m by ~500 s of mission time (sampling events spaced by movement step; per-step one measurement). For hypothetical-measurement computations, 40 measurement predictions per candidate location were used in simulation. No per-iteration particle count or exact sample-to-convergence counts are tabulated; however, DCEE required similar computational burden to Entrotaxis and significantly more than MPC due to particle-filter prediction of hypothesised measurements.",
            "exploration_exploitation_tradeoff": "Trade-off is explicit and derived from the objective: one-step expected cost decomposes into exploitation term (squared distance to predicted posterior mean) and exploration term (predicted posterior covariance, capturing expected uncertainty reduction). No manual weighting is required; the balance emerges naturally from minimising expected squared distance to the true source under hypothesised future measurements. This constitutes an implicit Bayesian active learning strategy with a dual-control probing effect.",
            "comparison_methods": "Compared against: (1) Stochastic MPC (exploitation-focused baseline), (2) Entrotaxis (information-theoretic informative path planning / IPP baseline that minimises predicted entropy / maximises information gain).",
            "key_results": "DCEE outperforms both MPC and Entrotaxis in simulated single-source plume localisation: faster RMSE reduction, 100% plume and source acquisition in the reported simulations (DCEE), where MPC suffered local minima and Entrotaxis converged slower (good exploration but weaker exploitation). In physical indoor experiments with more model mismatch and additional unknown environmental parameters, DCEE again showed superior and robust performance. The derived one-step cost yields an automatic, principled trade-off between exploration and exploitation without ad-hoc weighting, and the framework generalises to include environmental parameter estimation and multistage horizons (at extra computational cost).",
            "limitations_or_failures": "Computational burden: particle-filter based predicted-posterior simulation for each candidate action is costly, especially when estimating many environment parameters; multistage (N&gt;1) planning greatly increases computation. Performance degrades or becomes more challenging under model mismatch, localisation/map uncertainty, and unmodelled dynamics (experiments highlighted greater model mismatch than simulation). The implemented system uses a discrete finite action set and one-step horizon (multistage extension acknowledged but not used in main experiments due to computational load). Sensitivity to prior mismatch can cause local minima for purely exploitation methods (MPC), while Entrotaxis can be slower in exploitation despite strong exploration.",
            "limitations_or_failures_additional": "Exact particle count, timing per planning step, and wall-clock computational latency for on-board real-time operation are not fully quantified in the paper; these are practical limitations to scaling DCEE to higher-dimensional or faster problems.",
            "uuid": "e1279.0",
            "source_info": {
                "paper_title": "Dual Control for Exploitation and Exploration (DCEE) in Autonomous Search",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Entrotaxis",
            "name_full": "Entrotaxis (entropy-minimising informative path planning)",
            "brief_description": "An information-theoretic autonomous search strategy that selects actions to maximise information gain / minimise predicted posterior entropy (active sensing/informative path planning) to reduce uncertainty about source location and environment.",
            "citation_title": "Entrotaxis as a strategy for autonomous search and source reconstruction in turbulent conditions",
            "mention_or_use": "use",
            "agent_name": "Entrotaxis (information-theoretic IPP)",
            "agent_description": "An informative path planning agent that chooses next sampling locations to minimise predicted entropy of the posterior (maximum-entropy-sampling principle) using the particle-filter belief over source/environment parameters; focuses on exploration (information gain) rather than direct task-driven exploitation.",
            "adaptive_design_method": "Information gain maximisation / entropy minimisation (informative path planning, active sensing)",
            "adaptation_strategy_description": "At each decision step the agent evaluates candidate moves by predicting their effect on posterior entropy (using current belief and the dispersion + sensor models), and selects moves that most reduce uncertainty (predicted entropy) of the estimated source/environment parameters.",
            "environment_name": "Isotropic plume atmospheric dispersion (same simulated and experimental plume environments as DCEE)",
            "environment_characteristics": "Partially observable, stochastic plume dispersion with sparse/intermittent detections, unknown source location and some environmental parameters; continuous state space for parameters; noisy sensor model with detection/non-detection events and thresholds.",
            "environment_complexity": "Same domain as DCEE: continuous multi-parameter space; action set used in experiments also discretised (8 directions); simulation used 12 source configurations and many random runs; episode horizon up to 900 s in UAV simulation.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Simulation: Entrotaxis achieved 100% plume acquisition rate but only ~80% source acquisition rate across the reported simulation runs, with slower RMSE convergence than DCEE (converges to similar final accuracy only near end of 900 s flight budget). Experiment: Entrotaxis performed reasonably well but slower exploitation than DCEE; exact experimental success rates not tabulated numerically in the paper.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Entrotaxis exhibits strong exploration (rapid plume acquisition) but lower sample-efficiency for source localisation (slower RMSE reduction) compared to DCEE in these tasks; specific sample counts to reach given RMSE thresholds are not tabulated.",
            "exploration_exploitation_tradeoff": "Biased heavily toward exploration (reducing entropy) and does not explicitly include an exploitation term; exploration-centric behaviour can delay precise localisation (exploitation) relative to DCEE but ensures high plume-acquisition rates.",
            "comparison_methods": "Compared against DCEE (dual-control approach) and stochastic MPC in both simulation and experiments.",
            "key_results": "Entrotaxis is effective at exploration (100% plume acquisition in simulation), but because it focuses on entropy minimisation it is slower to converge to precise source location than DCEE and achieves lower source-acquisition rates (~80%). It demonstrates the expected behaviour of exploration-first strategies.",
            "limitations_or_failures": "Tends to travel perpendicular to wind to reduce entropy which can slow direct approach to source (exploitation) and therefore increase time-to-source localisation; less effective when exploitation (rapid convergence to source location) is required within limited mission budgets.",
            "uuid": "e1279.1",
            "source_info": {
                "paper_title": "Dual Control for Exploitation and Exploration (DCEE) in Autonomous Search",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Stochastic MPC (baseline)",
            "name_full": "Stochastic Model Predictive Control (exploitation-focused baseline)",
            "brief_description": "A classic control-theoretic planner that computes actions to minimise expected distance to the estimated source using the current posterior belief but does not account for the effect of actions on future posterior uncertainty (passive learning/certainty-equivalence style).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Stochastic MPC (exploitation-only baseline)",
            "agent_description": "An MPC-style agent that, at each step, minimises expected squared distance between the next robot position and the current posterior mean estimate of the source (uses current measurements only; does not simulate hypothesised future measurements for planning). Implemented with the same discrete action set for fair comparison.",
            "adaptive_design_method": null,
            "adaptation_strategy_description": "Uses current posterior mean (estimate) to drive the agent toward the believed source location; adapts only because the posterior updates when real measurements are collected (passive learning), but actions are not chosen to actively reduce posterior uncertainty.",
            "environment_name": "Isotropic plume atmospheric dispersion (same as above)",
            "environment_characteristics": "Partially observable, stochastic, sparse/intermittent sensing; same sensor and dispersion complexities as other agents; environment/parameter uncertainty may cause prior mismatch.",
            "environment_complexity": "Same as DCEE/Entrotaxis; action set size 8; simulation scenarios included 12 source configurations and up to 900 s flight budget.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Simulation baseline: MPC achieved ~80% plume acquisition and ~80% source acquisition; early RMSE reduction comparable to DCEE initially (due to central prior mean) but later often fails to converge further (stalls around ~200 s in some scenarios) due to local minima when prior mismatches ground truth.",
            "sample_efficiency": "MPC can produce early accidental plume samples when prior mean lies along plume path, but it is less sample-efficient at ultimately reducing RMSE across the varied scenarios compared to DCEE; exact sample counts to convergence not provided.",
            "exploration_exploitation_tradeoff": "Pure exploitation (certainty-equivalence): focuses on minimising immediate distance-to-estimate, does not take actions explicitly to reduce uncertainty.",
            "comparison_methods": "Used as a baseline against DCEE and Entrotaxis in simulation and experiments.",
            "key_results": "MPC (exploitation-only) can perform acceptably when priors align well with the true source, but is prone to local minima and fails to adequately explore when prior is wrong — resulting in lower plume and source acquisition rates compared with DCEE.",
            "limitations_or_failures": "Prone to failure when prior belief is inaccurate (local minima), lacks active probing so can miss plume entirely in some configurations; computationally cheaper but less robust in partially known environments.",
            "uuid": "e1279.2",
            "source_info": {
                "paper_title": "Dual Control for Exploitation and Exploration (DCEE) in Autonomous Search",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Entrotaxis as a strategy for autonomous search and source reconstruction in turbulent conditions",
            "rating": 2
        },
        {
            "paper_title": "Information-based search for an atmospheric release using a mobile robot: Algorithm and experiments",
            "rating": 2
        },
        {
            "paper_title": "Autonomous multi-robot search for a hazardous source in a turbulent environment",
            "rating": 1
        },
        {
            "paper_title": "Stochastic model predictive control with active uncertainty learning: A survey on dual control",
            "rating": 2
        },
        {
            "paper_title": "MPC-based dual control with online experiment design",
            "rating": 1
        },
        {
            "paper_title": "Dual effect, certainty equivalence, and separation in stochastic control",
            "rating": 1
        }
    ],
    "cost": 0.013521499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>This item was submitted to Loughborough's Research Repository by the author.
Items in Figshare are protected by copyright, with all rights reserved, unless otherwise indicated.</p>
<h1>Dual Control for Exploitation and Exploration (DCEE) in autonomous search</h1>
<p>PLEASE CITE THE PUBLISHED VERSION
https://doi.org/10.1016/j.automatica.2021.109851
PUBLISHER
Elsevier
VERSION
AM (Accepted Manuscript)
PUBLISHER STATEMENT
This paper was accepted for publication in the journal Automatica and the definitive published version is available at https://doi.org/10.1016/j.automatica.2021.109851.</p>
<h2>LICENCE</h2>
<p>CC BY-NC-ND 4.0</p>
<h2>REPOSITORY RECORD</h2>
<p>Chen, Wen-Hua, Callum Rhodes, and Cunjia Liu. 2021. "Dual Control for Exploitation and Exploration (DCEE) in Autonomous Search". Loughborough University. https://hdl.handle.net/2134/16577825.v1.</p>
<h1>Dual Control for Exploitation and Exploration (DCEE) in Autonomous Search ${ }^{\star}$</h1>
<p>Wen-Hua Chen ${ }^{a}$ Callum Rhodes ${ }^{a}$ Cunjia Liu ${ }^{a}$<br>*Department of Aeronautical and Automotive Engineering Loughborough University Leicestershire, LE11 3TU UK</p>
<h4>Abstract</h4>
<p>This paper proposes an optimal autonomous search framework, namely Dual Control for Exploration and Exploitation (DCEE), for a target at unknown location in an unknown environment. Source localisation is to find sources of atmospheric hazardous material release in a partially unknown environment. This paper proposes a control theoretic approach to this autonomous search problem. To cope with an unknown target location, at each step, the target location is estimated by Bayesian inference. Then a control action is taken to minimise the error between future robot position and the hypothesised future estimation of the target location. The latter is generated by hypothesised measurements at the corresponding future robot positions (due to the control action) with the current estimation of the target location as a prior. It shows that this approach can take into account both the error between the next robot position and the estimate of the target location, and the uncertainty of the estimate. This approach is further extended to the case with not only an unknown source location, but also an unknown local environment (e.g. wind speed and direction). Different from current information theoretic approaches, this new control theoretic approach achieves the optimal trade-off between exploitation and exploration in a unknown environment with an unknown target by driving the robot moving towards estimated target location while reducing its estimation uncertainty. This scheme is implemented using particle filtering on a mobile robot. Simulation and experimental studies demonstrate promising performance of the proposed approach. The relationships between the proposed approach, informative path planning, dual control, and classic model predictive control are discussed and compared. This work opens a door for further developing control systems operating in unknown environments, or performing tasks with unknown parameters.</p>
<p>Key words: Autonomous search, informative path planning, dual control, goal oriented control systems, exploration and exploitation</p>
<h2>1 Introduction</h2>
<p>Searching sources of airborne substance release could find a wide range of applications from disaster management and environment protection, to gas leakage detection in oil and gas industry Singh et al. (2015), Hutchinson et al. (2017). Chemical and biological materials could be released into the atmosphere deliberately (e.g. discharged by a plant, terrorism), naturally (e.g. methane emission, volcanic eruption), or accidentally (e.g. accidents in a chemical plant, Fukushima nuclear disaster).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>It can also be seen in natural behaviours from animals searching for food sources based on odours, to insects seeking mating in a large field or forest Vergassola et al. (2007). Quite often source localisation and quantification is referred to as a source term estimation (STE) problem in literature Bieringer et al. (2015), Platt \&amp; Deriggi (2010), Hutchinson et al. (2017). In the presence of hazardous material release events, there are two main approaches for existing emergency response practices: a static network of pre-deployed sensors and the manual collection of sensor measurements, e.g. using hand held devices and dedicated manned vehicles. The former is costly and requires substantial pre-planning while the latter puts people in harms way. After data are collected, estimation algorithms can be utilised to estimate the location of sources and release rates.</p>
<p>With the advances of robotics and autonomous system</p>
<p>technologies, there is a strong interest in developing a new STE solution using recently available mobile sensor platforms, where chemical or biological sensors are installed on a mobile ground robot or an unmanned aerial vehicle (UAV) Hutchinson et al. (2017). Essentially, this is to search a source in an unknown environment. It could be conducted through exhaustive search, so it can be considered as a coverage search problem. However, due to the time critical nature of this type of mission, there is a strong need in speeding up the search process, particularly for searching in a large area. To this end, various tools have been introduced including optimisation and Bayesian approaches Hutchinson et al. (2017). A key challenge arising in autonomous search using a mobile sensor platform is the deep interaction between estimation and planning - based on the current belief, a decision shall be made as to where to take the next measurement in order to maximise the chance of finding the source which, in turn, changes the belief since the new measurement is obtained at a new location. Driven by information-theoretic approaches, informative path planning (IPP) offers a promising solution to the STE problem Boström-Rost (2019). In an IPP approach, a mobile sensor platform plans a path that maximises the information gain about the source and local environment, based on its current belief, and updates its belief through incorporating new measurements of the dispersion environment using on-board sensors Ristic et al. (2010) Hutchinson (2019). A number of cognitive search strategies have been developed based on different reward functions of the information gain; for example, Ristic et al. (2017) Hutchinson, Oh \&amp; Chen (2018). A complete autonomous search system has been developed and tested in both indoor and outdoor environments with ground robots and unmanned aerial vehicles, respectively Hutchinson et al. (2019) Hutchinson, Liu \&amp; Chen (2018). In all the above studies, source localisation and search is considered as an estimation problem, or an exploration problem of unknown environment, so information theoretic approaches were advocated.</p>
<p>In this paper, instead of taking the information theoretic approach, we reconsider this autonomous search problem from a control theoretic perspective, and propose a new framework that is significantly different from the existing ones. Finding an unknown source is interpreted as controlling a mobile sensor platform approaching a target. That is, our goal is to design a control system that autonomously drives a robot equipped with chemical sensors to approach a target at an unknown location in an unknown environment. Different from a classic control setting, there is no reference trajectory or prescribed setpoint for the robot to follow. Indeed, the target location is unknown and the path to the target is to be defined in this autonomous search problem. To solve this problem, we formulate it as a stochastic model predictive control (MPC) problem with an undefined target location. It is shown that our approach is not only intuitive and promising, but also closely links with the dual
control concept Feldbaum (1960a) Feldbaum (1960b), Heirung et al. (2015), Mesbah (2018).</p>
<p>Different from a classic control setting, dual control considers that a control action affects not only future states of a system, but also uncertainty of its estimation BarShalom \&amp; Tse (1974). In general, dual control is intractable and computationally expensive, therefore it has found very few practical applications (Wittenmark (2008)). It shall be noted that there is a significant difference between the existing dual control formulation and our approach. In the current dual control setting, dual effects consider the effect of control on systems (e.g. state estimation in stochastic control, e.g.Mesbah (2018), Simpkins et al. (2008), or parameter estimation in adaptive control, e.g. Wittenmark (2008), Filatov \&amp; Unbehauen (2000), Heirung et al. (2015)). The dual control effect we consider in this paper is about the probing effect on the operational environment or the target, rather than on the uncertainty of the dynamic system itself (e.g. state or parameters). We aim to drive the robot to a believed target position (to be defined later) but at the same time reduce the uncertainty associated with this believed target position, and build up a better understanding of the operational environment. This very difference gives us the ability to deal with the problem of controlling a robot approaching a target at an unknown location and in an unknown environment.</p>
<p>The problem we are trying to solve also belongs to a much broader class of problems arising in machine learning and artificial intelligence - the trade-off between exploitation and exploration in an unknown environment. In an unknown environment, exploitation makes best choice based on current information which is biased to what you have known and may lead to local results. In contrast, exploration aims to gather more information and make the best overall choice, but may lead to wasted efforts. In our view, IPP in robotics and autonomous systems focuses on exploration to increase information gain so to reduce uncertainty, while traditionally control engineering mainly focuses exploitation, i.e. making use of information to derive a suitable control action/plan. It is shown that for an autonomous search of the source of airborne substance release, our approach provides an optimal trade-off between exploitation and exploration in the sense of Bellman's principle of optimality (1957) Bellman (1957).</p>
<p>This paper first formulates the autonomous search problem in an unknown environment into a control problem. The cost function is defined as the expected error between the robot's future position and the predicted estimation of the target location (after taking into account information gain due to hypothesised future measurements). Since the true chemical concentration measurements at future positions of the robot are unknown, this predicted estimation is generated by a dispersion model with the current estimated source location as a</p>
<p>prior and randomly simulating sensor readings under described sensor characteristics. It is shown that the cost function actually consists of two parts: the first part is about driving the robot moving towards the predicted estimated source location with hypothesised measurements, while the second term quantifies the uncertainty level of the predicted estimated source location. In other words, the first term is related to exploitation by making good use of the estimated source location in generating control actions, whereas the second term is about exploration by using control to probe the unknown environment in order to reduce the uncertainty level of the location estimation and the belief of the environment. By minimising this cost function, we are able to optimally trade-off between these two effects, i.e. simultaneously driving the robot towards the source and reduce the uncertainty in source location estimation. It is pointed out that, to a large extent, the current MPC and information theoretic approach could be considered as special cases of our framework.</p>
<p>Bayesian inference is used to develop the control algorithm. The whole control framework consists of two steps: Bayesian estimation of the source location (and other associated environmental parameters) after a new chemical concentration measurement is taken, and the calculation of a control action by minimising the above cost function where Bayesian estimation is also embedded for calculating the predicted posteriors. However, it is difficult to implement this Bayesian estimation based control framework, particularly when a large number of source and environment related parameters are unknown. We resort to particle filtering for implementing the proposed control algorithm in both simulation and experimental tests.</p>
<p>There are 4 main contributions in this paper. 1). This paper formulates the autonomous search of airborne hazardous substance release as a new type of control problem from a control theoretic perspective. The main feature of this control problem is that the target is at an unknown location and there is also no predefined reference trajectory. In other words, the goal of the control system is well defined, i.e. finding the source, but the information about the target and its operational environment is incomplete. 2). Inspired by a dual control concept, a new framework is proposed to trade-off between making use of the current estimate to driving a robot towards the believed location of the target (exploitation) and reducing the uncertainty of the estimation (exploration), namely Dual Control for Exploration and Exploitation (DCEE). It is shown that optimal autonomous search is realised through this approach (in the sense of Bellman optimality Bellman (1957)). This work is extended to deal with an unknown environment. 3). Simulation and physical experiments have been developed to demonstrate the performance of the proposed DCEE. Numerical implementation of Bayesian estimation and online optimisation have been presented. The comparison with
alternative approaches including information theoretic approaches and classic MPC clearly shows superior performance and effectiveness of the proposed DCEE. 4). This work brings insights into a number of related areas such as information theoretic approaches (e.g. IPP, active sensing), dual control, exploration and exploitation, autonomous search, and MPC. It shows that the existing approaches are biased to (or only focus on) either exploitation or exploration so may lead to less efficient or poorer searching performance.</p>
<p>The remainder of the paper is organised as follows. Section 2 introduces the autonomous search problem for atmospheric dispersion of hazardous substances. A isotropic plume dispersion model is presented and chemical sensor behaviour with misdetection and sensor errors are also introduced. The control algorithm development is presented in Section 3. An one-step-ahead cost function is proposed and the control algorithm is derived with discussions. Particle filtering implementation of the proposed control algorithm is presented to facilitate real-time applications. This work is further extended in several directions in Section 4 including operating in an unknown environment (for example, wind speed and direction has a significant effect on the dispersion) and a multi-stage cost function. The relationship between DCEE and other relevant control/search strategies is also discussed in this Section to provide more insight. Simulation is conducted in Section 5 and comparison with IPP and MPC clearly demonstrates superior performance of the proposed framework. Furthermore, physical experiments have been conducted on a ground robot in an indoor environment in Section 6. This paper ends with conclusion in Section 7.</p>
<h2>2 Autonomous search and its formulation</h2>
<h3>2.1 Agent modelling</h3>
<p>Consider an autonomous agent (e.g. a robot or a UAV) is tasked to search an area for finding a possible source of airborne hazardous substance release. It is supposed that there is a lower level controller that drives the agent to following any planned path so the detailed dynamics of the agent are ignored for the path planning purpose. The behaviour of the agent is modelled as Hutchinson (2019)</p>
<p>$$
\mathbf{p}<em k="k">{k+1}=\mathbf{p}</em>
$$}+\mathbf{u}_{k</p>
<p>where $\mathbf{p}<em k="k">{k} \in R^{3}$ is the current position of the agent, $\mathbf{u}</em>$ is a set of admissible control actions.} \in \mathcal{U}$ is the movement of the agent and $\mathcal{U</p>
<h3>2.2 Dispersion model and environment</h3>
<p>The dispersion model of an airborne chemical release can be formulated as an Isotropic plume model Holmes</p>
<p>$\&amp;$ Morawska (2006). In this model, the expected concentration at the robot position $\mathbf{p}<em k="k" x_="x,">{k}=\left[p</em>$ is given by:}, p_{y, k}, p_{z, k}\right]^{\mathrm{T}}$ from a release source positioned at $\mathbf{s}=\left[s_{x}, s_{y}, s_{z}\right]^{\mathrm{T}}$ with a releasing rate of $q_{s</p>
<p>$$
\begin{aligned}
&amp; \mathcal{M}\left(\mathbf{p}<em s="s">{k}, \Theta\right)=\frac{q</em>}}{4 \pi \zeta_{s 1}\left|\mathbf{p<em k="k">{k}-\mathbf{s}\right|} \exp \left[\frac{-\left|\mathbf{p}</em>\right] \times \
&amp; \exp \left[\frac{-\left(p_{x, k}-s_{x}\right) u_{s} \cos \phi_{s}}{2 \zeta_{s 1}}\right] \exp \left[\frac{-\left(p_{y, k}-s_{y}\right) u_{s} \sin \phi_{s}}{2 \zeta_{s 1}}\right]
\end{aligned}
$$}-\mathbf{s}\right|}{\lambda</p>
<p>where $\lambda=\sqrt{\frac{\zeta_{s 1} \zeta_{s 2}}{1+\left(\alpha\left(\zeta_{s 2}\right) /\left(4 \zeta_{s 1}\right)}}$. A number of parameters are involved to characterise this dispersion, including the average particle lifetime $\zeta_{s 2}$, the diffusivity $\zeta_{s 1}$, the mean wind speed $u_{s}$ and wind direction $\phi_{s}$. These parameters of the source and local environment (e.g. wind field, temperature) are assumed be unknown but may have certain prior information.</p>
<h3>2.3 Sensor modelling</h3>
<p>When an agent moves to a location, a point-wise measurement of chemical/biological concentration at the current location is taken by onboard chemical (or biological) sensors/detectors. Due to the nature of these sensors, an agent needs to hold at the location for a short period in order to get a reliable reading, which is referred to as the sampling interval. This is also why the detailed dynamics of an agent are not required in plan planning. Due to the power, size and payload of mobile platforms (e.g. small UAVs), only portable chemical detectors could be installed on the agent which quite often implies a limited accuracy and poor dynamic measurement capability. Furthermore, local turbulence in airflow has a significant effect on the distribution of concentration, causing quick perturbation in the dispersion, which causes intermittent sensor readings. Therefore, it is important to model sensor characteristics and the influence of the local environment on its behaviour. Both detection and non-detection events are considered due to the sparsity of the measurements and complication in local flow. A non-detection event may be caused by three hypothesised scenarios:</p>
<ul>
<li>The source is not present at all or present but not within range of the chemical detector. Any concentration measurement recorded is only a result of background and instrument noise.</li>
<li>A source is present and in detectable concentrations but there is a non-detection as a result of intermittency caused by turbulence or a missed detection, typically exacerbated by the short sampling intervals of the agent.</li>
<li>The source is present but the concentration (plus any addition from background and instrument noise) is below a pre-specified concentration threshold $z_{t h r}$. The
threshold is set high enough to minimise false detections, whilst maintaining sufficient sensitivity Yee (2017).</li>
</ul>
<p>In a detection event, the sensor reading consists of the true concentration and sensor noises. In summary, the sensor behaviour is modelled as</p>
<p>$$
z_{k}= \begin{cases}\mathcal{M}\left(\mathbf{p}<em k="k">{k}, \Theta\right)+v</em>
$$} ; &amp; D=1 \ \bar{v}_{k} ; &amp; D=0\end{cases</p>
<p>where $D$ denote a detection event. $P_{d}=\operatorname{Pr}{D=1}$ is the detection probability encapsulating all the three hypothesised scenarios. A Poisson distribution is used to represent the non-detection event in this study with its non-detection probability as the sum of all these three scenarios. $\bar{v}<em k="k">{k}$ and $v</em>$ satisfying Gaussian distribution represent the noise in a non-detection event and detection event, respectively.</p>
<h3>2.4 Autonomous search as a control problem</h3>
<p>Traditionally the source search problem is referred to as an STE problem. This is to estimate the key parameter related to release so emergency responders or disaster management teams could be well informed. Broadly speaking, there are two approaches in developing information theoretic based search strategies to this problem: optimisation based approaches where gradients of the concentration are exploited and Bayesian approaches which is based on a probability framework Hutchinson et al. (2017). Various information metrics such as mutual information on entropy, Kullback-Leibler divergence and variance have been used as a reward function to develop cognitive search strategies such as 'infotaxis' Ristic et al. (2017), Ristic et al. (2016), 'entrotaxis' Hutchinson, Oh $\&amp;$ Chen (2018). They provide a more reliable and robust solution with a shorter search time than the optimisation based search methods and traditional search methods such as 'chemotaxis' Adler (1966). In essence, these information theoretic methods mainly focus on exploration, i.e., to explore the unknown environment to find more information about sources, although some inherent trade-off behaviour between exploitation and exploration has been observed since it also drives the agent towards the target Hutchinson, Liu \&amp; Chen (2018).</p>
<p>Departing from the information theoretic approaches, we reformulate this problem from a control theoretic perspective. Basically, we consider the autonomous search problem as a process to driving the agent from its start location to a target. Once the target is reached, the mission is completed. However one challenge is that this is a non-conventional control problem since the location of the target is unknown and the environment is also unknown. It cannot be formulated in the traditional control framework such as as a regulation or tracking problem</p>
<p>as there is no pre-defined reference trajectory. Another challenge from the control system point of view is that measurements are sporadic and intermittent. In actuality, for the majority of the search process, there is no sensor reading. To best of the authors' knowledge, there is no work in investigating this kind of autonomous search from a control theoretic perspective. We will show our new formulation provides a much better way to balance exploitation (make use of information collected) and exploration (reduce uncertainty in information by exploring the environment) as to lead to a substantially improved search performance.</p>
<h2>3 Control algorithm development</h2>
<p>We will start control algorithm development with only unknown parameters related to the source but will extend the work to deal with an unknown environment in Section 4. Consequently, the unknown or uncertain parameters we consider at this stage are the location of the source and its release rate, i.e. $\Theta=\left[\mathbf{s}^{\top} q_{s}\right]^{\top}$.</p>
<p>The goal of the control system for autonomous search is to drive the agent to the source at an unknown location. Up until now, a typical approach in the current control system setting would be to drive the robot towards the believed location derived from prior information and collect measurements. However, this would not be able to take into account the current level of uncertainty in the belief and how the future move could reduce it as illustrated in Section 4.2.2. This motivates our DCEE framework.</p>
<h3>3.1 DCEE framework</h3>
<p>When the source term $\Theta$ is unknown, a probability density function $p\left(\Theta_{k}\right)$ can be used to represent the belief of $\Theta$ at time $k$. Let $\mathcal{Z}<em k="k">{k}$ denote the vector of measurements collected at different locations by the robot up to time step $k$, i.e. $\mathcal{Z}</em>}:=\left{z_{1}\left(\mathbf{p<em 2="2">{1}\right), z</em>}\left(\mathbf{p<em k="k">{2}\right), \ldots, z</em>\right)\right}$.}\left(\mathbf{p}_{k</p>
<p>We define $\rho_{k \mid k}:=p\left(\Theta \mid \mathcal{Z}<em k="k">{k}\right)$ as the posterior distribution at time $k$. Now we consider any control input $\mathbf{u}</em>}$, which moves the robot to a new location where a future new measurement $\hat{z<em k_1="k+1">{k+1}$ could be made at time $k+1$ (where $\hat{z}$ is used to distinguish a predicted variable from a real variable). This future measurement can be considered as a random variable given the control input, i.e. $\hat{z}</em>} \sim p\left(\hat{z<em k="k">{k+1} \mid \mathbf{u}</em>}\right)$. We define a hypothetical posterior distribution $\hat{\rho<em k_1="k+1">{k+1 \mid k}$ conditioned on this possible future measurement $\hat{z}</em>}$, i.e. $\hat{\rho<em k="k">{k+1 \mid k}:=p\left(\Theta \mid \mathcal{Z}</em>\right)$. Therefore, the control input not only affects the future location of the robot but also affects its future belief about where the target might be.}, \hat{z}_{k+1</p>
<p>Inspired by the above discussion, we would like to move the robot's future position closer to the predicted pos-
terior of the target's location, $\hat{\rho}<em k="k">{k+1 \mid k}$, conditional upon the control input $\mathbf{u}</em>}$. Given a possible future measurement $\hat{z<em k="k">{k+1}$ induced by control input $\mathbf{u}</em>$, the conditional cost function for approaching a target at an unknown location s, can be formulated as follows</p>
<p>$$
J\left(\mathbf{u}<em _Theta="\Theta">{k}\right)=\mathbb{E}</em>}\left[\mathbb{E<em k_1="k+1">{\hat{z}</em>}}\left[\left|\mathbf{p<em k="k">{k+1 \mid k}-\mathbf{s}\right|^{2} \mid \mathcal{Z}</em>\right]\right]
$$}, \hat{z}_{k+1</p>
<p>In this formulation, the control input $\mathbf{u}<em _mid="\mid" k="k" k_1="k+1">{k}$ not only determines the future robot position $\mathbf{p}</em>}$, but also affects the possible future measurement $\hat{z<em _mid="\mid" k="k" k_1="k+1">{k+1}\left(\mathbf{p}</em>}\right)$. Let $\Theta_{k+1 \mid k}=\left[\mathbf{s<em _mid="\mid" k="k" k_1="k+1" s_="s,">{k+1 \mid k}^{\top}, q</em>}\right]^{\top}$ denote the estimate of the source location and the release rate at time $k$ with the measurements $\mathcal{Z<em k_1="k+1">{k}$ and the virtual measurement $\hat{z}</em>}\left(\mathbf{p<em k="k">{k+1 \mid k}\right)$. It is clear that the move $\mathbf{u}</em>}$ affects the predicted posterior $\hat{\rho<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}:=p\left(\Theta</em>\right)$ of the source location and the release rate. Taking the expectation with respect to the future measurement rewards the exploration effect in reducing the level of uncertainty in unknown source parameter estimation, as we will show later.</p>
<p>The optimisation problem for DCEE can be formulated as follows:</p>
<p>$$
\min <em k="k">{\mathbf{u}</em>}} J\left(\mathbf{u<em _mathbf_u="\mathbf{u">{k}\right)=\min </em><em _Theta="\Theta">{k}} \mathbb{E}</em>}\left[\mathbb{E<em k_1="k+1">{\hat{z}</em>}}\left[\left|\mathbf{p<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}-\mathbf{s}\right|^{2} \mid \mathcal{Z}</em>\right]\right]
$$</p>
<p>subject to</p>
<p>$$
\begin{aligned}
&amp; \mathbf{p}<em k="k">{k+1 \mid k}=\mathbf{p}</em>}+\mathbf{u<em _mathbf_k="\mathbf{k">{k} \
&amp; \mathbf{u}</em>
\end{aligned}
$$}} \in \mathcal{U</p>
<p>where $\mathcal{Z}<em k="k">{k+1 \mid k}:=\left{\mathcal{Z}</em>}, \hat{z<em _mid="\mid" k="k" k_1="k+1">{k+1}\right}$.
We define the nominal estimated source location as the mean of the posterior distribution of the source location estimation. Similarly, the nominal predicted estimation of the source location $\hat{\mathbf{s}}</em>}$ is defined as the mean of the predicted distribution, i.e. $\hat{\rho<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}$, with $\mathcal{Z}</em>$, which is given by</p>
<p>$$
\overline{\mathbf{s}}<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}:=\mathbb{E}\left[\mathbf{s}</em>\right]
$$}\right]=\mathbb{E}\left[\mathbf{s} \mid \mathcal{Z}_{k+1 \mid k</p>
<p>With $\overline{\mathbf{s}}<em _mid="\mid" k="k" k_1="k+1">{k+1}$, we define $\overline{\mathbf{s}}</em>}$ conditional on $\mathcal{Z<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}$ as $\hat{\mathbf{s}}</em>$. Therefore, the cost function (5) can be reformulated as}=\mathbf{s}-\overline{\mathbf{s}}_{k+1 \mid k</p>
<p>$$
J\left(\mathbf{u}<em _Theta_="\Theta," _hat_z="\hat{z">{k}\right)=\mathbb{E}</em><em _mid="\mid" k="k" k_1="k+1">{k+1}}\left[\left|\mathbf{p}</em>}-\overline{\mathbf{s}<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}-\hat{\mathbf{s}}</em>\right]
$$}\right|^{2} \mid \mathcal{Z}_{k+1 \mid k</p>
<p>Theorem 1 The cost function of DCEE defined in (4) is equivalent to the following cost function</p>
<p>$$
J\left(\mathbf{u}<em _mid="\mid" k="k" k_1="k+1">{k}\right)=\left|\mathbf{p}</em>}-\overline{\mathbf{s}<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}\right|^{2}+P</em>
$$</p>
<p>where $P_{k+1 \mid k}=\mathbb{E}<em k_1="k+1">{\Theta, \hat{z}</em>}}\left[\overline{\mathbf{s}<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}^{\top} \overline{\mathbf{s}}</em>} \mid \mathcal{Z<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}\right]$ is the predicted covariance matrix of $\mathbf{s}</em>$.</p>
<p>PROOF. Expanding the square and collecting terms of the right hand side of (7) gives</p>
<p>$$
\begin{aligned}
J\left(\mathbf{u}<em _mid="\mid" k="k" k_1="k+1">{k}\right)= &amp; \mathbb{E}\left[\left|\mathbf{p}</em>}-\overline{\mathbf{s}<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}\right|^{2} \mid \mathcal{Z}</em>\right] \
&amp; -2 \mathbb{E}\left[\overline{\mathbf{s}}<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}^{T}\left(\mathbf{p}</em>}-\overline{\mathbf{s}<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}\right) \mid \mathcal{Z}</em>\right] \
&amp; +\mathbb{E}\left[\overline{\mathbf{s}}<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}^{T} \overline{\mathbf{s}}</em>\right]
\end{aligned}
$$} \mid \mathcal{Z}_{k+1 \mid k</p>
<p>Given that both $\mathbf{p}<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}$ and $\overline{\mathbf{s}}</em>}$ are deterministic and that $\mathbb{E}\left[\overline{\mathbf{s}<em _mid="\mid" k="k" k_1="k+1">{k+1 \mid k}^{T} \mid \mathcal{Z}</em>\right]=0$, results in (8) can be concluded.</p>
<p>Remark 2 The cost function in the form of (8) clearly reveals the dual control effect of our approach. The first term in the cost function (8) drives the robot to the estimated location of the source, which is related to exploitation. The second term is about the level of uncertainty of the estimated target location, captured by the predicted covariance of the source location in the next time step. The influence of future control action on both the distance to a believed target location and the current information uncertainty is quantified. It is quite intuitive and natural. Optimising this cost function over admissible control actions gives the best next move that balances the probing effect and performing certain task.</p>
<p>Remark 3 This optimal control problem gives the best strategy in trade-off between exploration and exploitation so it is the optimal autonomous search strategy for this problem in the sense of the principle of optimality (Bellmman, 1957). The existing IPP based autonomous search is only concerned about the second term while the classic MPC approach only considers the first term. In many areas, there is a big challenge in how to balance exploration and exploitation, particularly in artificial intelligence, optimisation and decision making for complicated problems. Normally weights on the cost/reward functions have to be introduced to balance these two effects. Sometimes, in order to reflect the dual effect, a related term is also artificially added to the cost function Heirung et al. (2015). There is no such a requirement in choosing weights to trade-off in our formulation. They are derived from a physically meaningful cost function in (4). The balance between them is naturally embedded. More discussion about this approach and the existing work will be made in Section 4.</p>
<p>Bayesian estimation plays a key role both in the inference engine for estimating the parameters related to the source and in the planning loop for calculating predicted posteriors. It is implemented using using particle filtering while the control a finite set of one-step action, i.e. $\mathbf{u}_{k} \in \mathcal{U}:=\left{\uparrow, \downarrow, \leftarrow, \rightarrow, \nwarrow, \nearrow, \swarrow, \searrow\right}$ Hutchinson, Liu \&amp; Chen (2018). In the following sections, we will discuss the implementation issues related to this framework.</p>
<h3>3.2 Implementation of Bayesian estimation</h3>
<p>Bayesian estimation plays a key role both in the inference engine for estimating the parameters related to the source and in the planning loop for calculating predicted posteriors.</p>
<p>The conditional probability of the source terms can be obtained via recursive Bayesian estimation, such that</p>
<p>$$
p\left(\Theta_{k} \mid \mathcal{Z}<em k="k">{k}\right)=\frac{p\left(z</em>} \mid \Theta_{k}\right) p\left(\Theta_{k} \mid \mathcal{Z<em k="k">{k-1}\right)}{p\left(z</em>
$$} \mid \mathcal{Z}_{k-1}\right)</p>
<p>where</p>
<p>$$
p\left(z_{k} \mid \mathcal{Z}<em k="k">{k-1}\right)=\int p\left(z</em>} \mid \Theta_{k}\right) p\left(\Theta_{k} \mid \mathcal{Z<em k="k">{k-1}\right) \mathrm{d} \Theta</em>
$$</p>
<p>The initial prior distribution $p\left(\Theta_{0} \mid \mathcal{Z}<em 0="0">{0}\right)=p\left(\Theta</em>\right)$ is determined by using the dispersion model (2) and the sensor model (3).}\right)$ is assumed to be given. In this work, the parameters associated with the source term are assumed to be unknown but fixed. The likelihood function in the Bayesian estimation $p\left(z_{k} \mid \Theta_{k</p>
<p>Given the nonlinear nature of this Bayesian estimation problem, a particle filtering approach is used to approximate the Bayesian estimation. In the particle filter, the posterior distribution from Eq. (10) is approximated by a set of weighted random samples $\left{\Theta_{k}^{(i)}, w_{k}^{(i)}\right}_{i=1}^{N}$ such that</p>
<p>$$
p\left(\Theta_{k} \mid \mathbf{z}<em i="1">{1: k}\right) \approx \sum</em>\right)
$$}^{N} w_{k}^{(i)} \delta\left(\Theta_{k}-\Theta_{k}^{(i)</p>
<p>where $\delta(\cdot)$ is a Dirac delta function, $\Theta_{k}^{(i)}$ is a sample representing a potential source term realisation and $w_{k}^{(i)}$ is the corresponding normalised weighting such that $\sum_{i=1}^{N} w_{k}^{(i)}=1$.</p>
<p>The process of recursively calculating the posterior distribution at sampling instance $k$ is summarised in Algorithm 1 and more details can be found in Hutchinson, Liu \&amp; Chen (2018).</p>
<p>A similar process is employed in calculating predicted posteriors with hypothesised measurements under a candidate control action but with some simplifications to reduce online computational load.</p>
<h3>3.3 Implementation of optimisation</h3>
<p>To reduce the computational load, in this paper we choose the optimal control input from a finite set of onestep action, i.e. $\mathbf{u}_{k} \in \mathcal{U}:=\left{\uparrow, \downarrow, \leftarrow, \rightarrow, \nwarrow, \nearrow, \swarrow, \searrow\right}$. The step size can be determined based on the operation environment.</p>
<p>Require: prior samples: $\left{\Theta_{k-1}^{(i)}, \omega_{k-1}^{(i)}\right}<em k="k">{i=1}^{N}$; sensor measurement $z</em>}$ at location $\mathbf{p<em k="k">{k}$;
for $i=1,2, \ldots, N$ do
draw sample $\Theta</em>\right)$
Assign weight $\bar{w}}^{(i)} \sim q\left(\Theta_{k-1}^{(i)<em k-1="k-1">{k}^{(i)}=w</em>}^{(i)} \cdot \frac{p\left(z_{k} \mid \Theta_{k}^{(i)}\right) p\left(\Theta_{k}^{(i)} \mid \Theta_{k-1}^{(i)}\right)}{q\left(\Theta_{k}^{(i)} \mid \Theta_{k-1}^{(i)}, \boldsymbol{x<em k="k">{1: k}\right)}$
end for
5: normalise weight $w</em>}^{(i)}=\bar{w<em i="1">{k}^{(i)} /\left(\Sigma</em>}^{N} \bar{w<em e="e" f="f">{k}^{(i)}\right)$
calculate effective sample size $N</em>$
if $N_{e f f}&lt;N_{T}$ then
resample $\left{\Theta_{k}^{(i)}, \omega_{k}^{(i)}\right}}=1 / \Sigma_{i=1}^{N}\left(w_{k}^{(i)}\right)^{2<em k="k">{i=1}^{N}$
apply a MCMC (Monte Carlo Markov Chain) move
end if
Ensure: posterior samples: $\left{\Theta</em>\right}}^{(i)}, \omega_{k}^{(i)<em k="k">{i=1}^{N}$
Algorithm 1. Particle filter for source parameter estimation
The possible future measurements for a given control input $\mathbf{u}</em>}$ can be generated using the agent model (1), the dispersion model (2) and the sensor model (3) with $\Theta_{k} \sim$ $\rho_{k \mid k}$. Based on (12), a set of samples to represent the distribution of $\rho(k \mid k)$ are generated. For each sample $\Theta^{i}$, the dispersion model (2) is run to generate the chemical concentration at the new agent location $\mathbf{p<em k="k">{k+1 \mid k}$ due to the move $\mathbf{u}</em>}$. A set of measurements $\hat{z<em k="k">{k+1}$ are obtained by randomising the measurements using the sensor model (3) with the described noise characters $v</em>$. The optimal control action is selected as the one minimising the cost (4) or (8).}$ and $\bar{v}_{k</p>
<h2>4 Extension and relationships with other methods</h2>
<p>This section first extends the basic DCEE framework presented in Section 3 in a number of directions (including the incorporation of an unknown environment). The relationships with several other methods will then be discussed. It is shown that DCEE covers the cost/reward functions in both classic MPC and IPP, and is better fitting with autonomous search and control system design for autonomous systems in general.</p>
<h3>4.1 Extension</h3>
<h3>4.1.1 Unknown environment</h3>
<p>We have only considered unknown parameters related to the target such as its location in Section 3. In real operation, the operation environment is also more likely to be partially known. For example, we may only know wind direction and speed within a certain range from weather forecast or meteorologic data. We now extend our DCEE algorithm to cope with uncertainty in both the source and environment. Physical parameters in the dispersion model depend on the type of chemical release and the
environment. We now consider that all the related parameters in the Isotropic plumemodel (2) are unknown but with certain prior information. More specifically, in addition to the position and release rate of the source, the parameters to be estimated during the search process also include wind direction $\phi_{s}$, wind speed $u_{s}$, the diffusivity $\zeta_{s 1}$, and the average particle lifetime $\zeta_{s 2}$. That is, $\Theta=[\mathbf{s}, q_{s}, \phi_{s}, u_{s}, \zeta_{s 1}, \zeta_{s 2}] \Upsilon$. These parameters are related to the environment (e.g. wind field, temperature) and specific chemicals. It is assumed that all the parameters are unknown but constant during the search process.</p>
<p>By the virtue of the Bayesian estimate framework, we are able to make use of any prior information of these unknown parameters in our estimation. This will improve the accuracy of the parameter estimation of the target. There are no major changes to the structure of Algorithm 1 except the computational burden. This extended version of DCEE will be implemented and tested in real-time search operation on a robot in Section 6.</p>
<h3>4.1.2 Multistage DCEE</h3>
<p>For the convenience of illustrating the basic concept of DCEE, only a one-step-ahead cost function is considered in Section 3. It is straightforward to extend it to DCEE with a multistage cost function as</p>
<p>$$
J\left(\mathbf{U}<em _Theta="\Theta">{k}\right)=\mathbb{E}</em>}\left[\mathbb{E<em k_1="k+1">{\hat{z}</em>}, \ldots, \hat{z<em _mid="\mid" k="k" k_N="k+N">{k+N}}\left[\left|\mathbf{p}</em>\right]\right]
$$}-\mathbf{s}\right|^{2} \mid \mathcal{Z}_{k+N \mid k</p>
<p>where $\mathbf{U}<em k="k">{k}=\left[\mathbf{u}</em>}, \ldots, \mathbf{u<em _mid="\mid" k="k" k_N="k+N">{k+N-1}\right], \mathcal{Z}</em>}=\left[\mathcal{Z<em k_1="k+1">{k}, \hat{z}</em>}\left(\mathbf{p<em k_N="k+N">{k+1 \mid k}\right), \ldots, \hat{z}</em>}\right]$ where $\mathbf{P<em k="k">{k+i \mid k}, i=1, \ldots, N$, are the predicted locations of the agent under the control sequence $\mathbf{U}</em>}$ with the agent model (1). This implies that the conditional distance between the robot position and the location of the source in the next $N$ th step is of interest. In other words, the predicted posterior of $N$ th step with the hypothesised future $N$ step measurements under possible control sequence $\mathbf{U<em k="k">{k}$ is used to improve our autonomous search strategy. Then this cost function is minimised over the set of admissible control $\mathbf{U}</em>^{}$ to give the optimal control sequence $\mathbf{U}_{k<em>}$ but only the first move $\mathbf{u}_{k}^{</em>}$ is implemented and the process is repeated in a receding horizon fashion. Although the multistage strategy may improve the performance of the autonomous search further, it brings a significant increase of computation burden with it.</p>
<h3>4.2 Relationship with other methods</h3>
<h3>4.2.1 Information theoretic approaches</h3>
<p>As briefly discussed in Section 3, information theoretic approaches have been used to develop autonomous search methods where the search process is treated as an information gathering problem. More specifically, the aim of motion planning is to reduce the uncertainty of</p>
<p>the estimation of the target location and other unknown environmental factors. Therefore, the next move of the robot is determined to maximise a reward function related to information. Mutual information on entropy, Kullback-Leibler divergence and other metrics are used to measure the uncertainty of the information and the corresponding search schemes have been presented. More specifically, Entrotaxis enforces the maximum entropy sampling principle which dictates the agent moves to the position that is least certain Hutchinson, Oh \&amp; Chen (2018). Instead of reducing predicted entropy, infotaxis aims to minimise the predicted posterior variance of the source location Ristic et al. (2016) Ristic et al. (2017). This directly links to exploration effect of DCEE, i.e. the second term of (8).</p>
<p>Since Entrotaxis exhibits a slightly better performance than Infotaxis Hutchinson, Oh \&amp; Chen (2018), we implement Entrotaxis as a benchmark of IPP for comparison with DCEE in our simulation and experimental study.</p>
<h3>4.2.2 Model Predictive Control (MPC)</h3>
<p>As discussed in Introduction, currently there is little work looking into autonomous search from a control engineering perspective. If following our idea of considering the search as a process to drive the agent towards the source, it could be formulated in a classic stochastic MPC framework as</p>
<p>$$
J\left(\mathbf{u}<em _Theta="\Theta">{k}\right)=\mathbb{E}</em>}\left[\left|\mathbf{p<em k="k">{k+1 \mid k}-\mathbf{s}\right|^{2} \mid \mathcal{Z}</em>\right]
$$</p>
<p>where only the measurements up to the current time are used in estimating the location of the source. A simple interpretation of this cost function is that a right strategy is to drive the robot towards the best estimate of the source location with all the information we have so far.</p>
<p>In a similar fashion, this cost function could be rewritten as</p>
<p>$$
J\left(\mathbf{u}<em _mid="\mid" k="k" k_1="k+1">{k}\right)=\left|\mathbf{p}</em>}-\hat{\mathbf{s}<em _mid="\mid" k="k">{k \mid k}\right|^{2}+P</em>
$$</p>
<p>where $\hat{\mathbf{s}}<em k="k">{k \mid k}$ is the mean estimated location of the source at time $k$ conditional on $\mathcal{Z}</em>}$, and $P_{k \mid k}$ is the covariance matrix of the estimation $\mathbf{s<em _mid="\mid" k="k">{k \mid k}$ defined as $P</em>}=\mathbb{E}\left[\hat{\mathbf{s}<em _mid="\mid" k="k">{k \mid k}^{T} \hat{\mathbf{s}}</em>\right]$.</p>
<p>Since the control $\mathbf{u}_{k}$ does not affect the current estimation so its uncertainty, the second term in (15) is not relevant. The control action only affects the future position of the agent. So broadly speaking, the cost function (14) in classic stochastic MPC corresponds to the exploitation effect of DCEE, i.e. the first term of (8), i.e., to drive the robot to the best estimate location of the source based on all the current available information. This is not surprising since control is about to make use of information to take action. During the process of driving towards the believed source location, the robot also collects new measurements so its belief about the source
location is updated at each step with these new measurements. However, this is accidental or passive learning to reduce the uncertainty. It shall be highlighted that recently there are works in MPC with active learning using dual control effort that will be discussed in the next section Mesbah (2018).</p>
<p>In summary, the MPC and information theoretic approaches can be considered as special cases of the the proposed DCEE: the former is biased to exploitation while the latter exploration. The simulation and experimental comparisons between MPC, Entrotaxis and DCEE will be presented in Section 5 and 6.</p>
<h3>4.2.3 Dual control of uncertain systems</h3>
<p>According to Bar-Shalom and Tse Bar-Shalom \&amp; Tse (1974), a control input is said to have dual control effect if it can affect, with nonzero probability, at least one $r$ thorder central moment of a state variable $(r&gt;1)$. In a series of seminal papers, Feldaum first recognised when controlling an uncertain system, control inputs have a dual effect; i.e. not only a directing effect on system states but also a probing effect on system uncertainty Feldbaum (1960a) Feldbaum (1960b) Feldbaum (1961a) and Feldbaum (1961b). Most of the work in dual control is devoted to a system with unknown or immeasurable states. It has been shown in the so-called separation principle of control engineering, that a state estimator and a controller could be designed separately does not hold for most of systems, except, i.e. Linear Quadratic Gaussian (LQG). Various works have been presented to exploit the dual effect of control inputs. However, although it is very conceptually attractive, dual control is computationally intractable even for moderately sized systems Wittenmark (2008). Recently triggered by the advances in MPC and active learning, there is a renewed interest in dual control in the context of stochastic MPC with active learning Mesbah (2018). The probing effect of control inputs is investigated to actively learn the system state so to reduce the level of uncertainty in state estimation in an MPC setting.</p>
<p>Another area concerning the dual effect of control signals is adaptive dual control, e.g. Wittenmark (2008), Filatov \&amp; Unbehauen (2000), Heirung et al. (2017). Most of the adaptive control methods are developed based on the Certainty Equivalence (CE) principle where a control law is developed by treating the estimated parameters as the true parameters and a parameter updating mechanism is adopted to online estimate the unknown parameters. However, except in very special cases, control signal affects not only the output but also the quality of the parameter estimation. This separation principle implied by CE does not hold for most of adaptive control. There is also an intrinsic conflict between them since one side, information about the process increases with the level of perturbation and on the other side, the</p>
<p>system output shall vary as little as possible. Adaptive dual control was proposed to address these issues; see Wittenmark (2008), Filatov \&amp; Unbehauen (2000).</p>
<p>To address the computationally intractable challenge of dual control, a number of methods have been proposed to find suboptimal solutions. Broadly speaking, there are two types of MPC with dual control effect: indirect dual control and explicit dual control. The former is to approximate the dynamic programming Bertsekas (2005) while the latter adds the probing activity by deliberately modifying the cost function. A number of suboptimal approaches have been developed, but so far there are only a few reports about the applications of adaptive dual control or MPC with active learning Mesbah (2018).</p>
<p>Although DCEE is similar to these dual control methods that explicitly exploit the dual effects of control signals, there are clear differences between them. In the MPC with active learning or other dual control methods, the dual effect concerns the probing effect on a control system itself (i.e. unknown states or parameters).</p>
<p>Rather than about a control systems itself, DCEE exploits the probing effect on unknown parameters of the tasks a control system is designed to perform (e.g. the source location in this study), or an unknown environment that a control system operates in. So it goes beyond the traditional use of dual control effects, e.g. Wittenmark (2008), Filatov $\mathcal{E}$ Unbehauen (2000), Heirung et al. (2015), Heirung et al. (2017), Mesbah (2018). DCEE aims to increase the level of autonomy, or deal with control problems where only high level specifications (goals) are defined. The control system itself has to find more information about the goals and the environment in order to perform the tasks by probing the environment.</p>
<h2>5 Simulation study</h2>
<p>To test the efficacy of the proposed solution, a simulation study is performed which compares DCEE against the competitive solutions, most notably, MPC described in Section 4.2.2 and IPP. It shall be highlighted that there is no existing work about applying MPC in autonomous search which itself also forms a new contribution of this paper. For IPP, we implement a typical search algorithm, Entrotaxis proposed in Hutchinson, Oh \&amp; Chen (2018). In all the simulation studies and the experiment studies in the next section, we keep everything the same except the cost/reward function. By testing in this manner, the joint exploitation/exploration characteristics of DCEE can be thoroughly evaluated.</p>
<h3>5.1 Simulation Scenario</h3>
<p>The simulation is performed under the setting of a single source release within an open bounded environment.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. 12 sources differing in $\mathbf{s}<em y="y">{x}, \mathbf{s}</em>$ (indicated by the black arrow) within the bounded search domain. Three example plumes are shown with a coloured scale. Red ' X ' represents the UAV's start location.}$ and their respective wind directions $\phi_{x</p>
<p>The agent deployed to search the area is a UAV. The simulated source is modelled with a constant release rate using the IP model (2). The gas sensing model (3) outlined in Section 2 is used for taking simulated measurements from the IP ground truth model. To make the results meaningful, in total 360 runs across 12 source configurations have been conducted (see Fig. 1).</p>
<p>Prior values of $\Theta$ are initialised using simulation model parameters fitted with a PDF to encompass initial uncertainty. The prior $\theta_{x y}$ area is set uniformly as the entire domain of Fig. 1. UAV operational parameters are defined as a 2 m sampling step size with directions at $45^{\circ}$ intervals. At each potential sampling location, 40 measurement predictions are made. In this simulation study, we follow Section 3 that only the location $\left(\mathbf{s}<em y="y">{x}, \mathbf{s}</em>}, \mathbf{s<em s="s">{z}\right)$ and the release rate $q</em>$ of the source are considered to be unknown.</p>
<h3>5.2 Results and discussion</h3>
<p>To measure the performance of each search method over time, the RMSE of estimated $\left(\mathbf{s}<em y="y">{x}, \mathbf{s}</em>\right)$ against that of the true source is recorded at each sampling event. The average RMSE for each method across all sources is shown in Figure 2.}, \mathbf{s}_{z</p>
<p>Initially, MPC demonstrates a comparable gradient of RMSE reduction as DCEE. This is because in MPC, the location estimation is given by the mean of the probability distribution $p\left(\Theta_{k}\right)$, i.e. $\hat{\mathbf{s}}_{k \mid k}$, and at the beginning, a uniform distribution of the possible locations of the source is used. Its mean is therefore located in the middle of the search area so naturally MPC drives the UAV towards the middle of the area. Given a plume intersects with this path, then early sampling can occur.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Average RMSE over time for all 120 simulation runs. Also pictured, each method's source acquisition rate and plume acquisition rate respectively.</p>
<p>Evidently, it accidentally occurs that early sampling is achieved with MPC, not intentionally. Entrotaxis, however, shows a shallower gradient at the beginning due to the tendency to travel perpendicular to wind direction in order to reduce the entropy of the estimation in the particle filter. At approximately 200s, MPC fails to continue to adequately resolve the source whereas DCEE continues gathering useful data and convergence to a RMSE of 0.5 m at approximately 500 s. Entrotaxis also manages to converge to a similar accuracy but at the end of the UAV budgeted flight time of 900s.</p>
<p>The source acquisition rate and the plume acquisition rate are also recorded in Fig. 2. For a control method to be defined as having acquired the source, the final RMSE of an individual run must be below 3 m . The UAV is deemed to have acquired the plume when there is a sampling event yielding a reading greater than the minimum threshold $h_{t h r}$ of the sensor model. These are both important metrics to consider during evaluation as although performance is primarily dictated by an ability to reduce the distance to the true source quickly, the success rate of finding the plume (exploration) and source (exploitation) gives a greater insight as to operational qualities of each control method. Both DCEE and Entrotaxis achieve $100 \%$ plume acquisition (therefore good exploration characteristics) but MPC only achieves $80 \%$. DCEE achieves $100 \%$ success in source acquisition rate in all 120 simulations (thus also showing good exploitation characteristics), compared to $80 \%$ of Entrotaxis and MPC. The lack of plume acquisition for MPC is caused by local minimum during the search due to a large mismatch between the prior information and the ground truth of certain source configurations.</p>
<p>It is clear that DCEE outperform these two methods significantly in all aspects. This is because DCEE is able to
generate the best autonomous search strategy in trading off between exploration and exploitation in all the scenarios. It shall be mentioned that DCEE and Entrotaxis has a similar computational burden but MPC has a much less computational demand than these two other methods.</p>
<h2>6 Experimental Study with a ground robot</h2>
<p>To further validate the findings in the simulation, these three autonomous search algorithms are implemented in an indoor environment using a ground robot. They are tested and evaluated through a large number of experiments with the system setup remaining largely unchanged (changes are only made in appropriate operational parameters due to the size of the search area and the agent model from UAV of the third order to the ground robot of a second order). However, one major difference between the simulation and experimental tests is that all parameters associated with the sources and the environment are now treated as unknown and their estimation uncertainty is considered through dual control effect, i.e. the algorithm presented in Section 4 is applied. More specifically, the parameters estimated online in the search process consist of the source location ( $\mathbf{s}<em y="y">{x}, \mathbf{s}</em>$ across the test area. A fresh air intake as well as exhaust fans are used to ensure that the plume is stable over the 60 runs for a fair comparison.}$ ), the release rate $q_{s}$, wind direction $\phi_{s}$, wind speed $u_{s}$, the diffusivity $\zeta_{s 1}$, and the average particle lifetime $\zeta_{s 2}$. The source of the airborne release used in the experiments is a small beaker of acetone, which is agitated by the air flow of two fans that create a wind field of $\sim 1.5 \mathrm{~m} / \mathrm{s</p>
<p>An equipped ground robot is used to collect concentration measurements of the acetone vapour (see Fig. 4). Since one of the aims of the experimental study is to closely recreate real-world deployment, it is also not assumed that localisation and low-level control/planner are known or perfect. Mapping and localisation are performed using the popular simultaneous localisation and mapping (SLAM) technique of Hector SLAM Kohlbrecher et al. (2011), whilst the low-level planner is achieved by using a dynamic window approach (DWA) algorithm Fox et al. (1997).</p>
<p>The main functional difference between the simulation and experiments is that there is greater model uncertainty between the dispersion model used in Bayesian estimation and the real dispersion event. Whereas the estimation algorithm could guarantee convergence in the simulations (due to using the same plume model, certain localisation and perfect control), the source parameters in the experimental scenario are unknown and values have to be estimated during the autonomous search. These factors make the online estimation aspect much more challenging. Furthermore, since the DCEE strategy is deeply coupled with the Bayesian estimation to</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Experimental scenario. Red boundary shows the prior search boundary limits, green arrows show the wind field source and direction, the yellow star marks the acetone source location and the red X marks the downwind start location for the search.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Data flow for the experimental testing of autonomous search algorithms using a ground robot.</p>
<p>Decide the best next move based on the level of estimation uncertainty, it is important to investigate the performance of DCEE under dispersion model mismatching and unmodelled dynamics in real applications. Given these factors (plus the new uncertainty in localisation, mapping and low-level planning/control actions), the experiment provides a challenging stage upon which any weaknesses of DCEE will become apparent. Successful application in the experimental trial will also show that the DCEE framework is suitable for real-world deployment on autonomous search systems.</p>
<p>Since the indoor experiment proposed is relatively constricted in the number of configurations that can be tested, two start locations of the robot are explored. The first is downwind of the source but close to the plume and the second is situated away from the plume in the perpendicular wind direction. Testing these two extremes will show the relative merits and demerits of each control method and also test the versatility of DCEE. Each location is tested 10 times per control method (60 total runs).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Average RMSE over time for each control method across both experimental source configurations.</p>
<h3>6.1 Results</h3>
<p>The RMSE over time combined from the two source configurations are shown in Figure 5. The trends echo those seen in the simulation tests, with DCEE showing its ability to quickly localise and converge on the source. Due to the constricted nature of the test (and therefore certainty of sampling the plume), MPC shows more competitive performance however even in this small scenario, it still lacks the ability fully converge to the same degree as DCEE and Entrotaxis.</p>
<p>The DCEE framework has now been proven to be suitable for real-world deployment on autonomous search systems.</p>
<h3>7 Conclusion</h3>
<p>For an autonomous search problem, a control theoretic approach is proposed in this paper. Inspired by the dual control concept, DCEE is able to take into account both exploitation and exploration effect of control/decision/planning actions. It is suitable for designing high level control systems for a system operating in an unknown environment and (or) with unknown parameters associated with a task (i.e. unknown location of a source in this paper). Simulation and experiment results show this new framework outperforms classic MPC, a popular method in control engineering, and IPP, an information theory approach widely used in robotics and autonomous systems. For the specific autonomous search of hazardous sources, it is shown that DCEE maintains an optimal balance (in the sense of the principle of optimality (Bellman, Bellman (1957))) between the probing activity and control activity of control inputs, which are naturally in conflict. In the DCEE framework, stochastic MPC, active learning,</p>
<p>IPP, exploitation and exploration, and autonomous search are closely related and integrated. Much more work is required in further exploring and exploiting this new framework.</p>
<p>The level of autonomy of a system can be measured in terms of the level of complexity of tasks it is able to perform and the level of uncertainty it is able to cope with Antsaklis (2020). Hence it could be argued that any autonomous system shall be able to perform certain tasks and achieve defined goals based on its belief, but also update its belief and reduce the level of uncertainty by actively exploring the environment. Consequently any decision making/planning/control action shall have (dual) effect in these two aspects. The DCEE framework proposed in this paper provides a promising vehicle in realising these desirable properties for autonomous systems. It will inspire more research into developing similar goaloriented control systems.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the UK Engineering and Physical Sciences Research Council (EPSRC) Established Career Fellowship "Goal-Oriented Control Systems: Disturbance, Uncertainty and Constarints" under the grant number EP/T005734/1.</p>
<h2>References</h2>
<p>Adler, J. (1966), 'Chemotaxis in bacteria', Science 153(3737), 708-716.
Antsaklis, P. (2020), 'Autonomy and metrics of autonomy', Annual Reviews in Control 49, 15-26.
Bar-Shalom, Y. \&amp; Tse, E. M. (1974), 'Dual effect, certainty equivalence, and separation in stochastic control', IEEE Transactions on Automatic Control 19, $494-500$.
Bellman, R. E. (1957), Dynamic Programming, Princeton University Press, New Jersey.
Bertsekas, D. P. (2005), 'Dynamic programming and suboptimal control: a survey from adp to mpc', European Journal of Control 11, 310 - 334.
Bieringer, P. E., Rodriguez, L. M., Vandenberghe, F., Hurst, J. G., Bieberbach, G., Sykes, I., Hannan, J. R., Zaragoza, J. \&amp; Fry, R. N. (2015), 'Automated source term and wind parameter estimation for atmospheric transport and dispersion applications', Atmospheric Environment 122, 206-219.
Boström-Rost, P. (2019), On Informative Path Planning for Tracking and Surveillance, Vol. 1838, Linköping University Electronic Press.
Feldbaum, A. A. (1960a), 'Dual control theory. i', $A u$ tomation Remote Control 21, 874 - 880.
Feldbaum, A. A. (1960b), 'Dual control theory. ii', $A u$ tomation Remote Control 21, 1033 - 1039.
Feldbaum, A. A. (1961a), 'Dual control theory. iii', $A u$ tomation Remote Control 22, 1 - 12.</p>
<p>Feldbaum, A. A. (1961b), 'Dual control theory. iv', $A u$ tomation Remote Control 22, 109 - 121.
Filatov, N. M. \&amp; Unbehauen, H. (2000), 'Survey of adaptive dual control methods', IEE Proc Control Theory Application 147, 118 - 128.
Fox, D., Burgard, W. \&amp; Thrun, S. (1997), 'The dynamic window approach to collision avoidance', IEEE Robotics $\mathcal{G}$ Automation Magazine 4(1), 23-33.
Heirung, T. A. N., Foss, B. \&amp; Ydstie, B. E. (2015), 'Mpcbased dual control with online experiment design', Journal of Process Control 32, 64-76.
Heirung, T. A. N., Ydstie, B. E. \&amp; Foss, B. (2017), 'Dual adaptive model predictive control', Automatica 80, 340-348.
Holmes, N. S. \&amp; Morawska, L. (2006), 'A review of dispersion modelling and its application to the dispersion of particles: an overview of different dispersion models available', Atmospheric environment 40(30), 59025928.</p>
<p>Hutchinson, M. (2019), On the use of autonomous unmanned vehicles in response to hazardous atmospheric release incidents, PhD thesis, Loughborough University.
Hutchinson, M., Liu, C. \&amp; Chen, W.-H. (2018), 'Information-based search for an atmospheric release using a mobile robot: Algorithm and experiments', IEEE Transactions on Control Systems Technology 27(6), 2388-2402.
Hutchinson, M., Liu, C., Thomas, P. \&amp; Chen, W.H. (2019), 'Unmanned aerial vehicle-based hazardous materials response: Information-theoretic hazardous source search and reconstruction', IEEE Robotics $\mathcal{G}$ Automation Magazine .
Hutchinson, M., Oh, H. \&amp; Chen, W.-H. (2017), 'A review of source term estimation methods for atmospheric dispersion events using static or mobile sensors', Information Fusion 36, 130-148.
Hutchinson, M., Oh, H. \&amp; Chen, W.-H. (2018), 'Entrotaxis as a strategy for autonomous search and source reconstruction in turbulent conditions', Information Fusion 42, 179-189.
Kohlbrecher, S., Von Stryk, O., Meyer, J. \&amp; Klingauf, U. (2011), A flexible and scalable slam system with full 3d motion estimation, in '2011 IEEE international symposium on safety, security, and rescue robotics', IEEE, pp. 155-160.
Mesbah, A. (2018), 'Stochastic model predictive control with active uncertainty learning: A survey on dual control', Annual Reviews in Control 45, 107 - 117.
Platt, N. \&amp; Deriggi, D. (2010), Comparative investigation of source term estimation algorithms using fusion field trial 2007 data, in '8th Conference on Artificial Intelligence Applications to Environmental Sciences at AMS Annual Meeting, Atlanta, GA, Jan', Vol. 1.
Ristic, B., Angley, D., Moran, B. \&amp; Palmer, J. (2017), 'Autonomous multi-robot search for a hazardous source in a turbulent environment', Sensors 17(4), 918.
Ristic, B., Morelande, M. \&amp; Gunatilaka, A. (2010), 'In-</p>
<p>formation driven search for point sources of gamma radiation', Signal Processing 90(4), 1225-1239.
Ristic, B., Skvortsov, A. \&amp; Gunatilaka, A. (2016), 'A study of cognitive strategies for an autonomous search', Information Fusion 28, 1-9.
Simpkins, A., De Callafon, R. \&amp; Todorov, E. (2008), Optimal trade-off between exploration and exploitation, in '2008 American Control Conference', IEEE, pp. 33-38.
Singh, S. K., Sharan, M. \&amp; Issartel, J.-P. (2015), 'Inverse modelling methods for identifying unknown releases in emergency scenarios: an overview', International Journal of Environment and Pollution 57(1-2), 68 91 .
Vergassola, M., Villermaux, E. \&amp; Shraiman, B. I. (2007), "infotaxis' as a strategy for searching without gradients', Nature 445(7126), 406-409.
Wittenmark, B. (2008), Adaptive dual control, in H. Unbehauen, ed., 'Control systems, robotics and automation', Vol. X, EOLSS Publisher, pp. 122 - 132.
Yee, E. (2017), 'Automated computational inference engine for bayesian source reconstruction: Application to some detections/non-detections made in the ctbt international monitoring system', Applied Mathematical Sciences 11(32), 1581-1618.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>We-Hua Chen holds a Chair in Autonomous Vehicles with the Department of Aeronautical and Automotive Engineering, Loughborough University, U.K. He is interested in control, signal processing and artificial intelligence and their applications in robots, aerospace, and automotive systems. He is a Chartered Engineer, a Fellow of IEEE, the Institution of Mechanical Engineers and the Institution of Engineering and Technology, U.K. He has authored or coauthored near 300 papers and 2 books.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Callum Rhdoes received the MEng (Hons) degree in automotive engineering from Loughborough University in 2018. He is currently pursuing his PhD within the Loughborough University Centre for Autonomous Systems research team, with a research interest in path planning techniques for autonomous robots operating in hazardous environments.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Cunjia Liu received a Ph.D. in autonomous vehicle control from Loughborough University in 2011. He was a research associate with the Department of Aeronautical and Automotive Engineering at Loughborough University, where he is currently
a Reader in Unmanned Vehicles. He has considerable research experience in autonomous vehicles and Bayesian estimation and their applications in different domains.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>This paper has not been presented on any IFAC conference. Corresponding author: Wen-Hua Chen. w.chen@lboro.ac.uk</li>
</ul>
<p>Email addresses: W.Chen@lboro.ac.uk (Wen-Hua Chen), C.Rhodes@lboro.ac.uk (Callum Rhodes), C.Liu5@lboro.ac.uk (Cunjia Liu).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
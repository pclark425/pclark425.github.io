<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-833 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-833</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-833</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f" target="_blank">Attention is not Explanation</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work performs extensive experiments across a variety of NLP tasks to assess the degree to which attention weights provide meaningful “explanations” for predictions, and finds that they largely do not.</p>
                <p><strong>Paper Abstract:</strong> Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations” for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e833.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e833.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention-Explanation Gap (correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weak correspondence between attention weights and feature-importance explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper demonstrates that attention weight heatmaps (commonly presented in natural language descriptions as explanations) often do not correlate strongly with model-intrinsic feature importance measures such as input gradients or leave-one-out (LOO) attribution, revealing a misalignment between the explanatory claim and the implemented attention mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Attention-equipped neural NLP models (BiLSTM encoder experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A suite of attention-based classifiers and QA/NLI models (primarily BiLSTM encoders with additive or scaled-dot attention) implemented in PyTorch, trained and evaluated on multiple datasets (SST, IMDB, MIMIC, CNN QA, SNLI, bAbI, AG News, 20News, ADR tweets).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>implicit/explicit research-paper claim that attention heatmaps afford interpretability (research paper narrative and figure captions)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model implementation and experiment scripts (PyTorch attention module + evaluation code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>interpretability misalignment / ambiguous claim ('attention-as-explanation' vs implementation behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language presentations (figures and common claims in papers) often treat attention weight heatmaps as explanations indicating which input tokens are responsible for outputs; empirical experiments on implemented attention modules show weak and inconsistent correlation between these attention weights and independent feature-importance measures (gradients and leave-one-out), demonstrating that the natural-language claim is not faithfully realized by the code/implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model interpretation layer (attention mechanism and evaluation/visualization pipeline); also in documentation/figures that present attention maps as explanations</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical reproducibility-style experiments implemented by the authors: compute attention weights from trained models and compare to independent importance measures; disconnect gradient flow through attention for gradient attribution; statistical analysis across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Kendall τ correlations between attention and (a) gradient-based importance (τ_g) and (b) leave-one-out importance (τ_loo); reporting mean ± std and fraction of instances with statistically significant correlation (Sig. Frac). Example metrics: τ_g often around 0.2–0.45 for BiLSTM across corpora; specific instance cited τ_g=0.29; Sig. Frac values varied by dataset (e.g., some datasets show Sig. Frac ≈ 0.36, others ≈ 1.00).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Attention heatmaps can be misleading as explanations: many instances show weak correlation with gradient/LOO attributions, undermining claims that attended tokens are responsible for predictions. Quantitative examples from the paper: median output change after permuting attention often ≈ 0.006 (TVD); correlations (τ) centered ≲ 0.5 for BiLSTM; many datasets exhibit low Sig. Frac values meaning correlations are not consistently significant. This affects interpretability assertions and how practitioners communicate model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed broadly across evaluated corpora and tasks (classification, QA, NLI). Table 2 and histograms show weak/variable correlations for many datasets: some datasets (e.g., MIMIC, IMDB) show stronger/significant correlations, but across the board the correspondence is inconsistent — not a rare edge case.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or overstated natural-language framing of attention as explanation; attention weights are not causally tied to outputs and interact with contextualized encoders (BiLSTMs) such that attention values do not reliably reflect feature importance; implicit assumptions omitted from publications (that attention is a faithful explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Evaluate attention against independent attribution methods (gradients, LOO), report correlation statistics, and avoid presenting attention heatmaps as sole explanations; consider alternative attention designs aiming for interpretability (sparse/hard attention, structured attention, tying attention to human rationales); provide code and metrics that quantify explanation reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>The paper demonstrates diagnostics (correlations) and advises alternatives, but does not present a validated, fully effective mitigation; structured/sparse attention and rationale-anchoring are recommended directions (cited works), but their effectiveness is not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / natural language processing / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Attention is not Explanation', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e833.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e833.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Counterfactual Attention Gap (adversarial)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-uniqueness of attention-based explanations revealed by permutation and adversarial attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors show that radically different attention distributions (generated by permutation or adversarial optimization) can produce effectively identical model outputs, demonstrating that attention heatmaps as presented in papers do not uniquely explain predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Attention-based decoder + adversarial attention search (experimental evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Experimental pipeline that (1) computes the model's original attention and output for a test instance, (2) (a) permutes attention or (b) optimizes adversarial attention distributions to maximize divergence from the original while constraining output change within ε, and (3) measures divergence and output change.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>figures and narrative in research papers that imply a single attention heatmap explains a prediction</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts and optimization code (adversarial attention search implemented with Adam in PyTorch; attention module treated as tunable input)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete/ambiguous specification of explanation semantics; implementation allows non-unique attention-to-output mappings</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although papers and figure captions suggest that a single learned attention distribution pinpoints explanatory inputs, the implemented attention mechanism and decoder permit multiple, very different attention distributions to produce the same output; experiments using random permutations and explicit adversarial optimization (maximizing JSD subject to TVD(ŷ,ŷ_orig) ≤ ε) find diverse attention distributions that leave outputs within small ε.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model architecture and evaluation procedure (attention-to-output mapping; explanation-generation code and visualization)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Two experimental strategies implemented in code: (1) permute the observed attention weights repeatedly and measure the median change in output (Algorithm 2); (2) optimize for k adversarial attention distributions maximizing JSD from original (and mutual JSD) while constraining output TVD ≤ ε using Adam (Algorithm 3).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Jensen-Shannon Divergence (JSD) between original and adversarial attention distributions to quantify attention dissimilarity; Total Variation Distance (TVD) between output distributions to quantify permitted output change; thresholds ε set (0.01 for classification, 0.05 for QA); report ε-max JSD, median Δŷ under permutations, and distributions of JSD/TVD. Notable numeric facts: JSD upper bound ≈ 0.69; many adversaries achieved JSD near upper bound while Δŷ ≤ ε; example median Δŷ under permutation ≈ 0.006.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Demonstrates that attention-based visual explanations are not unique or necessarily causative: alternative attention maps (sometimes maximally different, JSD ≈ 0.69) can produce outputs within ε of the original, so heatmaps can mislead users about what contributed to prediction. This undermines trust in attention visualizations and any downstream conclusions drawn from them.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Adversarial and permutation tests produced many instances with high attention divergence and small output change across multiple datasets and tasks; not isolated — observed 'in many cases' and visualized in histograms (Figure 4, Figure 3, Figure 5). Some datasets (e.g., MIMIC positive examples) are exceptions where perturbing attention does change output substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>The attention mechanism and decoder do not enforce a unique, causal mapping from attention distributions to outputs; contextualized encoders produce representations where the decoder can rely on encoded features irrespective of attention reweighting; natural-language claims omit this modeling nuance.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use adversarial-attention diagnostics (compute ε-max JSD) as a metric of explanation reliability; report such metrics when publishing attention-based visualizations; prefer attention variants that enforce selection (hard/sparse attention), structured attention with probabilistic semantics, or models trained with rationale supervision to tie attention to human-meaningful features.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>The paper introduces the ε-max JSD diagnostic and demonstrates it uncovers unreliability, but does not empirically validate that alternate attention designs (e.g., sparse/hard attention) fully resolve the issue within this work; suggested mitigations are proposals supported by related literature rather than results here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / natural language processing / explainable AI</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Attention is not Explanation', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Understanding neural networks through representation erasure <em>(Rating: 2)</em></li>
                <li>Pathologies of neural models make interpretation difficult <em>(Rating: 2)</em></li>
                <li>Right for the right reasons: Training differentiable models by constraining their explanations <em>(Rating: 1)</em></li>
                <li>A causal framework for explaining the predictions of black-box sequence-to-sequence models <em>(Rating: 1)</em></li>
                <li>Deriving machine attention from human rationales <em>(Rating: 1)</em></li>
                <li>Interpretable neural models for natural language processing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-833",
    "paper_id": "paper-1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Attention-Explanation Gap (correlation)",
            "name_full": "Weak correspondence between attention weights and feature-importance explanations",
            "brief_description": "The paper demonstrates that attention weight heatmaps (commonly presented in natural language descriptions as explanations) often do not correlate strongly with model-intrinsic feature importance measures such as input gradients or leave-one-out (LOO) attribution, revealing a misalignment between the explanatory claim and the implemented attention mechanism.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Attention-equipped neural NLP models (BiLSTM encoder experiments)",
            "system_description": "A suite of attention-based classifiers and QA/NLI models (primarily BiLSTM encoders with additive or scaled-dot attention) implemented in PyTorch, trained and evaluated on multiple datasets (SST, IMDB, MIMIC, CNN QA, SNLI, bAbI, AG News, 20News, ADR tweets).",
            "nl_description_type": "implicit/explicit research-paper claim that attention heatmaps afford interpretability (research paper narrative and figure captions)",
            "code_implementation_type": "model implementation and experiment scripts (PyTorch attention module + evaluation code)",
            "gap_type": "interpretability misalignment / ambiguous claim ('attention-as-explanation' vs implementation behavior)",
            "gap_description": "Natural-language presentations (figures and common claims in papers) often treat attention weight heatmaps as explanations indicating which input tokens are responsible for outputs; empirical experiments on implemented attention modules show weak and inconsistent correlation between these attention weights and independent feature-importance measures (gradients and leave-one-out), demonstrating that the natural-language claim is not faithfully realized by the code/implementation.",
            "gap_location": "model interpretation layer (attention mechanism and evaluation/visualization pipeline); also in documentation/figures that present attention maps as explanations",
            "detection_method": "empirical reproducibility-style experiments implemented by the authors: compute attention weights from trained models and compare to independent importance measures; disconnect gradient flow through attention for gradient attribution; statistical analysis across datasets.",
            "measurement_method": "Kendall τ correlations between attention and (a) gradient-based importance (τ_g) and (b) leave-one-out importance (τ_loo); reporting mean ± std and fraction of instances with statistically significant correlation (Sig. Frac). Example metrics: τ_g often around 0.2–0.45 for BiLSTM across corpora; specific instance cited τ_g=0.29; Sig. Frac values varied by dataset (e.g., some datasets show Sig. Frac ≈ 0.36, others ≈ 1.00).",
            "impact_on_results": "Attention heatmaps can be misleading as explanations: many instances show weak correlation with gradient/LOO attributions, undermining claims that attended tokens are responsible for predictions. Quantitative examples from the paper: median output change after permuting attention often ≈ 0.006 (TVD); correlations (τ) centered ≲ 0.5 for BiLSTM; many datasets exhibit low Sig. Frac values meaning correlations are not consistently significant. This affects interpretability assertions and how practitioners communicate model behavior.",
            "frequency_or_prevalence": "Observed broadly across evaluated corpora and tasks (classification, QA, NLI). Table 2 and histograms show weak/variable correlations for many datasets: some datasets (e.g., MIMIC, IMDB) show stronger/significant correlations, but across the board the correspondence is inconsistent — not a rare edge case.",
            "root_cause": "Ambiguous or overstated natural-language framing of attention as explanation; attention weights are not causally tied to outputs and interact with contextualized encoders (BiLSTMs) such that attention values do not reliably reflect feature importance; implicit assumptions omitted from publications (that attention is a faithful explanation).",
            "mitigation_approach": "Evaluate attention against independent attribution methods (gradients, LOO), report correlation statistics, and avoid presenting attention heatmaps as sole explanations; consider alternative attention designs aiming for interpretability (sparse/hard attention, structured attention, tying attention to human rationales); provide code and metrics that quantify explanation reliability.",
            "mitigation_effectiveness": "The paper demonstrates diagnostics (correlations) and advises alternatives, but does not present a validated, fully effective mitigation; structured/sparse attention and rationale-anchoring are recommended directions (cited works), but their effectiveness is not quantified here.",
            "domain_or_field": "machine learning / natural language processing / deep learning",
            "reproducibility_impact": true,
            "uuid": "e833.0",
            "source_info": {
                "paper_title": "Attention is not Explanation",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "Counterfactual Attention Gap (adversarial)",
            "name_full": "Non-uniqueness of attention-based explanations revealed by permutation and adversarial attention",
            "brief_description": "The authors show that radically different attention distributions (generated by permutation or adversarial optimization) can produce effectively identical model outputs, demonstrating that attention heatmaps as presented in papers do not uniquely explain predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Attention-based decoder + adversarial attention search (experimental evaluation)",
            "system_description": "Experimental pipeline that (1) computes the model's original attention and output for a test instance, (2) (a) permutes attention or (b) optimizes adversarial attention distributions to maximize divergence from the original while constraining output change within ε, and (3) measures divergence and output change.",
            "nl_description_type": "figures and narrative in research papers that imply a single attention heatmap explains a prediction",
            "code_implementation_type": "experiment scripts and optimization code (adversarial attention search implemented with Adam in PyTorch; attention module treated as tunable input)",
            "gap_type": "incomplete/ambiguous specification of explanation semantics; implementation allows non-unique attention-to-output mappings",
            "gap_description": "Although papers and figure captions suggest that a single learned attention distribution pinpoints explanatory inputs, the implemented attention mechanism and decoder permit multiple, very different attention distributions to produce the same output; experiments using random permutations and explicit adversarial optimization (maximizing JSD subject to TVD(ŷ,ŷ_orig) ≤ ε) find diverse attention distributions that leave outputs within small ε.",
            "gap_location": "model architecture and evaluation procedure (attention-to-output mapping; explanation-generation code and visualization)",
            "detection_method": "Two experimental strategies implemented in code: (1) permute the observed attention weights repeatedly and measure the median change in output (Algorithm 2); (2) optimize for k adversarial attention distributions maximizing JSD from original (and mutual JSD) while constraining output TVD ≤ ε using Adam (Algorithm 3).",
            "measurement_method": "Jensen-Shannon Divergence (JSD) between original and adversarial attention distributions to quantify attention dissimilarity; Total Variation Distance (TVD) between output distributions to quantify permitted output change; thresholds ε set (0.01 for classification, 0.05 for QA); report ε-max JSD, median Δŷ under permutations, and distributions of JSD/TVD. Notable numeric facts: JSD upper bound ≈ 0.69; many adversaries achieved JSD near upper bound while Δŷ ≤ ε; example median Δŷ under permutation ≈ 0.006.",
            "impact_on_results": "Demonstrates that attention-based visual explanations are not unique or necessarily causative: alternative attention maps (sometimes maximally different, JSD ≈ 0.69) can produce outputs within ε of the original, so heatmaps can mislead users about what contributed to prediction. This undermines trust in attention visualizations and any downstream conclusions drawn from them.",
            "frequency_or_prevalence": "Adversarial and permutation tests produced many instances with high attention divergence and small output change across multiple datasets and tasks; not isolated — observed 'in many cases' and visualized in histograms (Figure 4, Figure 3, Figure 5). Some datasets (e.g., MIMIC positive examples) are exceptions where perturbing attention does change output substantially.",
            "root_cause": "The attention mechanism and decoder do not enforce a unique, causal mapping from attention distributions to outputs; contextualized encoders produce representations where the decoder can rely on encoded features irrespective of attention reweighting; natural-language claims omit this modeling nuance.",
            "mitigation_approach": "Use adversarial-attention diagnostics (compute ε-max JSD) as a metric of explanation reliability; report such metrics when publishing attention-based visualizations; prefer attention variants that enforce selection (hard/sparse attention), structured attention with probabilistic semantics, or models trained with rationale supervision to tie attention to human-meaningful features.",
            "mitigation_effectiveness": "The paper introduces the ε-max JSD diagnostic and demonstrates it uncovers unreliability, but does not empirically validate that alternate attention designs (e.g., sparse/hard attention) fully resolve the issue within this work; suggested mitigations are proposals supported by related literature rather than results here.",
            "domain_or_field": "machine learning / natural language processing / explainable AI",
            "reproducibility_impact": true,
            "uuid": "e833.1",
            "source_info": {
                "paper_title": "Attention is not Explanation",
                "publication_date_yy_mm": "2019-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Understanding neural networks through representation erasure",
            "rating": 2
        },
        {
            "paper_title": "Pathologies of neural models make interpretation difficult",
            "rating": 2
        },
        {
            "paper_title": "Right for the right reasons: Training differentiable models by constraining their explanations",
            "rating": 1
        },
        {
            "paper_title": "A causal framework for explaining the predictions of black-box sequence-to-sequence models",
            "rating": 1
        },
        {
            "paper_title": "Deriving machine attention from human rationales",
            "rating": 1
        },
        {
            "paper_title": "Interpretable neural models for natural language processing",
            "rating": 1
        }
    ],
    "cost": 0.01162975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Attention is not Explanation</h1>
<p>Sarthak Jain<br>Northeastern University<br>jain.sar@husky.neu.edu</p>
<h2>Byron C. Wallace</h2>
<p>Northeastern University
b.wallace@northeastern.edu</p>
<h4>Abstract</h4>
<p>Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful "explanations" for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code to reproduce all experiments is available at https://github.com/successar/ AttentionExplanation.</p>
<h2>1 Introduction and Motivation</h2>
<p>Attention mechanisms (Bahdanau et al., 2014) induce conditional distributions over input units to compose a weighted context vector for downstream modules. These are now a near-ubiquitous component of neural NLP architectures. Attention weights are often claimed (implicitly or explicitly) to afford insights into the "inner-workings" of models: for a given output one can inspect the inputs to which the model assigned large attention weights. Li et al. (2016) summarized this commonly held view in NLP: "Attention provides an important way to explain the workings of neural models". Indeed, claims that attention provides</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Heatmap of attention weights induced over a negative movie review. We show observed model attention (left) and an adversarially constructed set of attention weights (right). Despite being quite dissimilar, these both yield effectively the same prediction ( 0.01 ).
interpretability are common in the literature, e.g., (Xu et al., 2015; Choi et al., 2016; Lei et al., 2017; Martins and Astudillo, 2016; Xie et al., 2017). ${ }^{1}$</p>
<p>Implicit in this is the assumption that the input units (e.g., words) accorded high attention weights are responsible for model outputs. But as far as we are aware, this assumption has not been formally evaluated, and our findings here suggest that it is problematic. More specifically, we empirically investigate the relationship between attention weights, inputs, and outputs. Assuming attention provides an explanation for model predictions, we might expect the following properties to hold. (i) Attention weights should correlate with feature importance measures (e.g., gradient-based measures); (ii) Alternative (or counterfactual) attention weight configurations ought to yield corresponding changes in prediction (and if they do not then are equally plausible as explanations). We report that neither property is consistently observed by standard attention mechanisms in the context of text classification, question answering (QA), and Natural Language Inference (NLI) tasks when RNN encoders are used.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Consider Figure 1. The left panel shows the original attention distribution $\alpha$ over the words of a particular movie review using a standard attentive BiLSTM architecture for sentiment analysis. It is tempting to conclude from this that the token waste is largely responsible for the model coming to its disposition of 'negative' $(\hat{y}=0.01)$. But one can construct an alternative attention distribution $\tilde{\alpha}$ (right panel) that attends to entirely different tokens yet yields an essentially identical prediction (holding all other parameters of $f, \theta$, constant).</p>
<p>Such counterfactual distributions imply that explaining the original prediction by highlighting attended-to tokens is misleading insofar as alternative attention distributions would have yielded an equivalent prediction (e.g., one might conclude from the right panel that model output was due primarily to was rather than waste). Further, the attention weights in this case correlate only weakly with gradient-based measures of feature importance $\left(\tau_{g}=0.29\right)$. And arbitrarily permuting the entries in $\alpha$ yields a median output difference of 0.006 with the original prediction.</p>
<p>These and similar findings call into question the view that attention provides meaningful insight into model predictions. We thus caution against using attention weights to highlight input tokens "responsible for" model outputs and constructing just-so stories on this basis, particularly with complex encoders.
Research questions and contributions. We examine the extent to which the (often implicit) narrative that attention provides model transparency ${ }^{2}$ holds across tasks by exploring the following empirical questions.</p>
<ol>
<li>To what extent do induced attention weights correlate with measures of feature importance - specifically, those resulting from gradients and leave-one-out methods?</li>
<li>Would alternative attention weights (and hence distinct heatmaps/"explanations") necessarily yield different predictions?</li>
</ol>
<p>Our findings with respect to these questions (assuming a BiRNN encoder) are summarized as follows: (1) Only weakly and inconsistently, and, (2) No; it is very often possible to construct adversarial attention distributions that yield effectively</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>equivalent predictions as when using the originally induced attention weights, despite attending to entirely different input features. Further, randomly permuting attention weights often induces only minimal changes in output.</p>
<h2>2 Preliminaries and Assumptions</h2>
<p>We consider exemplar NLP tasks for which attention mechanisms are commonly used: classification, natural language inference (NLI), and question answering. ${ }^{3}$ We adopt the following general modeling assumptions and notation.</p>
<p>We assume model inputs $\mathbf{x} \in \mathbb{R}^{T \times|V|}$, composed of one-hot encoded words at each position. These are passed through an embedding matrix $\mathbf{E}$ which provides dense ( $d$ dimensional) token representations $\mathbf{x}<em e="e">{e} \in \mathbb{R}^{T \times d}$. Next, an encoder Enc consumes the embedded tokens in order, producing $T m$-dimensional hidden states: $\mathbf{h}=\operatorname{Enc}\left(\mathbf{x}</em>$}\right) \in$ $\mathbb{R}^{T \times m}$. We predominantly consider a Bi-RNN as the encoder module, but for completeness we also analyze convolutional and (unordered) 'average embedding' variants. ${ }^{4</p>
<p>A similarity function $\phi$ maps $\mathbf{h}$ and a query $\mathbf{Q} \in \mathbb{R}^{m}$ (e.g., hidden representation of a question in QA, or the hypothesis in NLI) to scalar scores, and attention is then induced over these: $\hat{\boldsymbol{\alpha}}=$ $\operatorname{softmax}(\phi(\mathbf{h}, \mathbf{Q})) \in \mathbb{R}^{T}$. In this work we consider two common similarity functions: Additive $\phi(\mathbf{h}, \mathbf{Q})=\mathbf{v}^{T} \tanh \left(\mathbf{W}<em _mathbf_2="\mathbf{2">{\mathbf{1}} \mathbf{h}+\mathbf{W}</em>}} \mathbf{Q}\right)$ (Bahdanau et al., 2014) and Scaled Dot-Product $\phi(\mathbf{h}, \mathbf{Q})=$ $\frac{\mathrm{hQ}}{\sqrt{\mathrm{m}}}$ (Vaswani et al., 2017), where $\mathbf{v}, \mathbf{W<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \mathbf{W}</em>$ are model parameters.}</p>
<p>Finally, a dense layer Dec with parameters $\boldsymbol{\theta}$ consumes a weighted instance representation and yields a prediction $\hat{y}=\sigma\left(\boldsymbol{\theta} \cdot h_{\alpha}\right) \in \mathbb{R}^{|\mathcal{Y}|}$, where $h_{\alpha}=\sum_{t=1}^{T} \hat{\alpha}<em t="t">{t} \cdot h</em>|$ denotes the label set size.} ; \sigma$ is an output activation function; and $|\mathcal{Y</p>
<h2>3 Datasets and Tasks</h2>
<p>For binary text classification, we use:
Stanford Sentiment Treebank (SST) (Socher et al., 2013). 10,662 sentences tagged with sentiment on a scale from 1 (most negative) to 5 (most positive). We filter out neutral instances and dichotomize the remaining sentences into positive $(4,5)$ and negative $(1,2)$.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>| Dataset | $|\boldsymbol{V}|$ | Avg. length | Train size | Test size | Test performance |
| :-- | :-- | :-- | --: | --: | :--: |
| SST | 16175 | 19 | $3034 / 3321$ | $863 / 862$ | 0.81 |
| IMDB | 13916 | 179 | $12500 / 12500$ | $2184 / 2172$ | 0.88 |
| ADR Tweets | 8686 | 20 | $14446 / 1939$ | $3636 / 487$ | 0.61 |
| 20 Newsgroups | 8853 | 115 | $716 / 710$ | $151 / 183$ | 0.94 |
| AG News | 14752 | 36 | $30000 / 30000$ | $1900 / 1900$ | 0.96 |
| Diabetes (MIMIC) | 22316 | 1858 | $6381 / 1353$ | $1295 / 319$ | 0.79 |
| Anemia (MIMIC) | 19743 | 2188 | $1847 / 3251$ | $460 / 802$ | 0.92 |
| CNN | 74790 | 761 | 380298 | 3198 | 0.64 |
| bAbI (Task 1 / 2 / 3) | 40 | $8 / 67 / 421$ | 10000 | 1000 | $1.0 / 0.65 / 0.64$ |
| SNLI | 20982 | 14 | $182764 / 183187 / 183416$ | $3219 / 3237 / 3368$ | 0.78 |</p>
<p>Table 1: Dataset characteristics. For train and test size, we list the cardinality for each class, where applicable: $0 / 1$ for binary classification (top), and $0 / 1 / 2$ for NLI (bottom). Average length is in tokens. Test metrics are F1 score, accuracy, and micro-F1 for classification, QA, and NLI, respectively; all correspond to performance using a BiLSTM encoder. We note that results using convolutional and average (i.e., non-recurrent) encoders are comparable for classification though markedly worse for QA tasks.</p>
<p>IMDB Large Movie Reviews Corpus (Maas et al., 2011). Binary sentiment classification dataset containing 50,000 polarized (positive or negative) movie reviews, split into half for training and testing.</p>
<p>Twitter Adverse Drug Reaction dataset (Nikfarjam et al., 2015). A corpus of $\sim 8000$ tweets retrieved from Twitter, annotated by domain experts as mentioning adverse drug reactions.</p>
<p>20 Newsgroups (Hockey vs Baseball). Collection of $\sim 20,000$ newsgroup correspondences, partitioned (nearly) evenly across 20 categories. We extract instances belonging to baseball and hockey, which we designate as 0 and 1 , respectively, to derive a binary classification task.</p>
<p>AG News Corpus (Business vs World). ${ }^{5}$ 496,835 news articles from 2000+ sources. We follow (Zhang et al., 2015) in filtering out all but the top 4 categories. We consider the binary classification task of discriminating between world ( 0 ) and business (1) articles.</p>
<p>MIMIC ICD9 (Diabetes) (Johnson et al., 2016). A subset of discharge summaries from the MIMIC III dataset of electronic health records. The task is to recognize if a given summary has been labeled with the ICD9 code for diabetes (or not).</p>
<p>MIMIC ICD9 (Chronic vs Acute Anemia) (Johnson et al., 2016). A subset of discharge summaries from MIMIC III dataset (Johnson et al., 2016) known to correspond to patients with anemia. Here the task to distinguish the type of anemia for each report - acute ( 0 ) or chronic (1).</p>
<h2>For Question Answering (QA):</h2>
<p>CNN News Articles (Hermann et al., 2015). A corpus of cloze-style questions created via auto-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>matic parsing of news articles from CNN. Each instance comprises a paragraph-question-answer triplet, where the answer is one of the anonymized entities in the paragraph.
$b A b I$ (Weston et al., 2015). We consider the three tasks presented in the original bAbI dataset paper, training separate models for each. These entail finding (i) a single supporting fact for a question and (ii) two or (iii) three supporting statements, chained together to compose a coherent line of reasoning.</p>
<p>Finally, for Natural Language Inference (NLI):
The SNLI dataset (Bowman et al., 2015). 570k human-written English sentence pairs manually labeled for balanced classification with the labels neutral, contradiction, and entailment, supporting the task of natural language inference (NLI). In this work, we generate an attention distribution over premise words conditioned on the hidden representation induced for the hypothesis.</p>
<p>We restrict ourselves to comparatively simple instantiations of attention mechanisms, as described in the preceding section. This means we do not consider recently proposed 'BiAttentive' architectures that attend to tokens in the respective inputs, conditioned on the other inputs (Parikh et al., 2016; Seo et al., 2016; Xiong et al., 2016).</p>
<p>Table 1 provides summary statistics for all datasets, as well as the observed test performances for additional context.</p>
<h2>4 Experiments</h2>
<p>We run a battery of experiments that aim to examine empirical properties of learned attention weights and to interrogate their interpretability and transparency. The key questions are: Do</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Gradient (BiLSTM) $\tau_{g}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Gradient (Average) $\tau_{g}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Leave-One-Out (BiLSTM) $\tau_{l o o}$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">Class</td>
<td style="text-align: center;">Mean $\pm$ Std.</td>
<td style="text-align: center;">Sig. Frac.</td>
<td style="text-align: center;">Mean $\pm$ Std.</td>
<td style="text-align: center;">Sig. Frac.</td>
<td style="text-align: center;">Mean $\pm$ Std.</td>
<td style="text-align: center;">Sig. Frac.</td>
</tr>
<tr>
<td style="text-align: center;">SST</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.34 \pm 0.21$</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">$0.61 \pm 0.20$</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">$0.27 \pm 0.19$</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$0.36 \pm 0.21$</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">$0.60 \pm 0.21$</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">$0.32 \pm 0.19$</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;">IMDB</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.44 \pm 0.06$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.67 \pm 0.05$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.34 \pm 0.07$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$0.43 \pm 0.06$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.68 \pm 0.05$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.34 \pm 0.07$</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;">ADR Tweets</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.47 \pm 0.18$</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">$0.73 \pm 0.13$</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">$0.29 \pm 0.20$</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$0.49 \pm 0.15$</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">$0.72 \pm 0.12$</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">$0.44 \pm 0.16$</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;">20News</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.07 \pm 0.17$</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">$0.79 \pm 0.07$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.06 \pm 0.15$</td>
<td style="text-align: center;">0.29</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$0.21 \pm 0.22$</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">$0.75 \pm 0.08$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.20 \pm 0.20$</td>
<td style="text-align: center;">0.62</td>
</tr>
<tr>
<td style="text-align: center;">AG News</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.36 \pm 0.13$</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">$0.78 \pm 0.07$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.30 \pm 0.13$</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$0.42 \pm 0.13$</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">$0.76 \pm 0.07$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.43 \pm 0.14$</td>
<td style="text-align: center;">0.91</td>
</tr>
<tr>
<td style="text-align: center;">Diabetes</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.42 \pm 0.05$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.75 \pm 0.02$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.41 \pm 0.05$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$0.40 \pm 0.05$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.75 \pm 0.02$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.45 \pm 0.05$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;">Anemia</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.47 \pm 0.05$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.77 \pm 0.02$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.46 \pm 0.05$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$0.46 \pm 0.06$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.77 \pm 0.03$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.47 \pm 0.06$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;">CNN</td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">$0.24 \pm 0.07$</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">$0.50 \pm 0.10$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.20 \pm 0.07$</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: center;">bAbI 1</td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">$0.25 \pm 0.16$</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">$0.72 \pm 0.12$</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">$0.16 \pm 0.14$</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: center;">bAbI 2</td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">$-0.02 \pm 0.14$</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">$0.68 \pm 0.06$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$-0.01 \pm 0.13$</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: center;">bAbI 3</td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">$0.24 \pm 0.11$</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">$0.61 \pm 0.13$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$0.26 \pm 0.10$</td>
<td style="text-align: center;">0.89</td>
</tr>
<tr>
<td style="text-align: center;">SNLI</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.31 \pm 0.23$</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">$0.59 \pm 0.18$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">$0.16 \pm 0.26$</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$0.33 \pm 0.21$</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">$0.58 \pm 0.19$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">$0.36 \pm 0.19$</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$0.31 \pm 0.21$</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">$0.57 \pm 0.19$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">$0.34 \pm 0.20$</td>
<td style="text-align: center;">0.40</td>
</tr>
</tbody>
</table>
<p>Table 2: Mean and std. dev. of correlations between gradient/leave-one-out importance measures and attention weights. Sig. Frac. columns report the fraction of instances for which this correlation is statistically significant; note that this largely depends on input length, as correlation does tend to exist, just weakly. Encoders are denoted parenthetically. These are representative results; exhaustive results for all encoders are available to browse online.
learned attention weights agree with alternative, natural measures of feature importance? And, Had we attended to different features, would the prediction have been different?</p>
<p>More specifically, in Section 4.1, we empirically analyze the correlation between gradientbased feature importance and learned attention weights, and between 'leave-one-out' (LOO) measures and the same. In Section 4.2 we then consider counterfactual (to those observed) attention distributions. Under the assumption that attention weights are explanatory, such counterfactual distributions may be viewed as alternative potential explanations; if these do not correspondingly change model output, then the original attention weights do not provide unique explanation for predictions, i.e., attending to other features could have resulted in the same output.</p>
<p>To generate counterfactual attention distributions, we first consider randomly permuting observed attention weights and recording associated changes in model outputs (4.2.1). We then propose explicitly searching for "adversarial" attention weights that maximally differ from the observed attention weights (which one might show in a heatmap and use to explain a model prediction), and yet yield an effectively equivalent prediction (4.2.2). The latter strategy also provides a useful potential metric for the reliability of attention weights as explanations: we can report a measure
quantifying how different attention weights can be for a given instance without changing the model output by more than some threshold $\epsilon$.</p>
<p>All results presented below are generated on test sets. We present results for Additive attention below. The results for Scaled Dot Product in its place are comparable. We provide a web interface to interactively browse the (very large set of) plots for all datasets, model variants, and experiment types: https://successar.github. io/AttentionExplanation/docs/.</p>
<p>In the following sections, we use Total Variation Distance (TVD) as the measure of change between output distributions, defined as follows. $\operatorname{TVD}\left(\hat{y}<em 2="2">{1}, \hat{y}</em>}\right)=\frac{1}{2} \sum_{i=1}^{|2|}\left|\hat{y<em 2="2" i="i">{1 i}-\hat{y}</em>\right|\right.$.}\right|$. We use the Jensen-Shannon Divergence (JSD) to quantify the difference between two attention distributions: $\operatorname{JSD}\left(\alpha_{1}, \alpha_{2}\right)=\frac{1}{2} \operatorname{KL}\left[\alpha_{1}\left|\frac{\alpha_{1}+\alpha_{2}}{2}\right|\right]+$ $\frac{1}{2} \operatorname{KL}\left[\alpha_{2}\left|\frac{\alpha_{1}+\alpha_{2}}{2</p>
<h3>4.1 Correlation Between Attention and Feature Importance Measures</h3>
<p>We empirically characterize the relationship between attention weights and corresponding feature importance scores. Specifically we measure correlations between attention and: (1) gradient based measures of feature importance $\left(\tau_{g}\right)$, and, (2) differences in model output induced by leaving features out $\left(\tau_{l o o}\right)$. While these measures are themselves insufficient for interpretation of neu-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Histogram of Kendall τ between attention and gradients. Encoder variants are denoted parenthetically; colors indicate predicted classes. Exhaustive results are available for perusal online. Best viewed in color.</p>
<p>ral model behavior (Feng et al., 2018), they do provide measures of individual feature importance with known semantics (Ross et al., 2017). It is thus instructive to ask whether these measures correlate with attention weights.</p>
<p>The process we follow to quantify this is described in Algorithm 1. We denote the input resulting from removing the word at position t in <strong>x</strong> by <strong>x</strong>(<em>t</em>). Note that we disconnect the computation graph at the attention module so that the gradient does not flow through this layer.</p>
<p>Algorithm 1 Feature Importance Computations</p>
<p>$$
\begin{array}{l}
\mathbf{h} \leftarrow \text{Enc}(\mathbf{x}), \quad \hat{\alpha} \leftarrow \text{softmax}(\phi(\mathbf{h}, \mathbf{Q})) \
\hat{y} \leftarrow \text{Dec}(\mathbf{h}, \alpha) \
g_t \leftarrow |\sum_{w=1}^{|V|} \mathbb{1}[\mathbf{x}<em tw="tw">{tw} = 1] \frac{\partial g}{\partial \mathbf{x}</em>|, \forall t \in [1, T] \
\tau_g \leftarrow \text{Kendall-τ}(\alpha, g) \
\Delta \hat{y}}<em -t="-t">t \leftarrow \text{TVD}(\hat{y}(\mathbf{x}</em>)) \ , \forall t \in [1, T] \
\tau_{\text{loo}} \leftarrow \text{Kendall-τ}(\alpha, \Delta \hat{y})
\end{array}
$$}), \hat{y}(\mathbf{x</p>
<p>Table 2 reports summary statistics of Kendall τ correlations for each dataset. Full distributions are shown in Figure 2, which plots histograms of τ<em>g</em> for every data point in the respective corpora. (Corresponding plots for τ<em>loo</em> are similar and the full set can be browsed via the online supplement.) We plot these separately for each class: orange (■) represents instances predicted as positive, and purple (■) those predicted to be negative. For SNLI, colors ■, ■ and ■ code for contradiction, entailment, and neutral respectively.</p>
<p>In general, observed correlations are modest (recall: 0 indicates no correspondence, 1 implies perfect concordance) for the BiRNN encoder. The centrality of observed densities hovers around or below 0.5 in most of the corpora considered. Moreover, as per Table 2, correlation is sufficiently weak that a statistically significant correlation between attention weights and feature importance scores (both gradient and feature erasure based) cannot consistently be established across corpora.</p>
<p>In contrast, gradients in "average" embedding based models show very high degree of correspondence with attention weights – on average across corpora, correlation between LOO scores and attention weights is ~0.375 points higher for this encoder, compared to the BiLSTM. These results suggest that, in general, attention weights do not strongly or consistently agree with such feature importance scores in models with contextualized embeddings. This is problematic for the view of attention weights as explanatory, given the face validity of input gradient/erasure based explanations (Ross et al., 2017; Li et al., 2016). On some datasets — notably the MIMIC tasks, and to a lesser extent the QA corpora — this correlation is consistently significant but remains relatively weak. This could be attributed to increased length of documents for these datasets providing stronger signal to standard hypothesis testing methods.</p>
<p>For reference we report correlations between gradients and LOO scores in the Appendix and online materials; these are consistently stronger than the correlation between attention weights and</p>
<p>either feature importance score for the recurrent (BiLSTM) encoder. These exhibit, on average, a (i) 0.2 and (ii) $\sim 0.25$ greater correlation with each other than BiLSTM attention and (i) LOO and (ii) gradient scores.</p>
<h3>4.2 Counterfactual Attention Weights</h3>
<p>We next consider what-if scenarios corresponding to alternative (counterfactual) attention weights. The idea is to investigate whether the prediction would have been different, had the model emphasized (attended to) different input features. More precisely, suppose $\hat{\alpha}=\left{\hat{\alpha_{t}}\right}_{t=1}^{T}$ are the attention weights induced for an instance, giving rise to model output $\hat{y}$. We then consider counterfactual distributions over $y$, under alternative $\alpha$.</p>
<p>We experiment with two means of constructing such distributions. First, we simply scramble the original attention weights $\hat{\alpha}$, re-assigning each value to an arbitrary, randomly sampled index (input feature). Second, we generate an adversarial attention distribution: this is a set of attention weights that is maximally distinct from $\hat{\alpha}$ but that nonetheless yields an equivalent prediction (i.e., prediction within some $\epsilon$ of $\hat{y}$ ).</p>
<h3>4.2.1 Attention Permutation</h3>
<p>To characterize model behavior when attention weights are shuffled, we follow Algorithm 2.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 2 Permuting attention weights
    \(\mathbf{h} \leftarrow \operatorname{Enc}(\mathbf{x}), \hat{\alpha} \leftarrow \operatorname{softmax}(\phi(\mathbf{h}, \mathbf{Q}))\)
    \(\hat{y} \leftarrow \operatorname{Dec}(\mathbf{h}, \hat{\alpha})\)
    for \(p \leftarrow 1\) to 100 do
        \(\alpha^{p} \leftarrow \operatorname{Permute}(\hat{\alpha})\)
        \(\hat{y}^{p} \leftarrow \operatorname{Dec}\left(\mathbf{h}, \alpha^{p}\right) \quad \triangleright\) Note : \(\mathbf{h}\) is not changed
        \(\Delta \hat{y}^{p} \leftarrow \operatorname{TVD}\left[\hat{y}^{p}, \hat{y}\right]\)
    end for
    \(\Delta \hat{y}^{m e d} \leftarrow \operatorname{Median}_{p}\left(\Delta \hat{y}^{p}\right)\)
</code></pre></div>

<p>Figure 3 depicts the relationship between the maximum attention value in the original $\hat{\alpha}$ and the median induced change in model output ( $\Delta \hat{y}^{\text {med }}$ ) across instances in the respective datasets. Colors again indicate class predictions, as above.</p>
<p>We observe that there exist many points with small $\Delta \hat{y}^{\text {med }}$ despite large magnitude attention weights. These are cases in which the attention weights might suggest explaining an output by a small set of features (this is how one might reasonably read a heatmap depicting the attention weights), but where scrambling the attention
makes little difference to the prediction.
In some cases, such as predicting ICD codes from notes using the MIMIC dataset, one can see different behavior for the respective classes. For the Diabetes task, e.g., attention behaves intuitively for at least the positive class; perturbing attention in this case causes large changes to the prediction. We again conjecture that this is due to a few tokens serving as high precision indicators for the positive class; in their absence (or when they are not attended to sufficiently), the prediction drops considerably. However, this is the exception rather than the rule.</p>
<h3>4.2.2 Adversarial Attention</h3>
<p>We next propose a more focused approach to counterfactual attention weights, which we will refer to as adversarial attention. The intuition is to explicitly seek out attention weights that differ as much as possible from the observed attention distribution and yet leave the prediction effectively unchanged. Such adversarial weights violate an intuitive property of explanations: shifting model attention to very different input features should yield corresponding changes in the output. Alternative attention distributions identified adversarially may then be viewed as equally plausible explanations for the same output.</p>
<p>Operationally, realizing this objective requires specifying a value $\epsilon$ that defines what qualifies as a "small" difference in model output. Once this is specified, we aim to find $k$ adversarial distributions $\left{\alpha^{(1)}, \ldots, \alpha^{(k)}\right}$, such that each $\alpha^{(i)}$ maximizes the distance from original $\hat{\alpha}$ but does not change the output by more than $\epsilon$. In practice we simply set this to 0.01 for text classification and 0.05 for QA datasets. ${ }^{6}$</p>
<p>We propose the following optimization problem to identify adversarial attention weights.</p>
<p>$$
\begin{array}{ll}
\underset{\alpha^{(i)}, \ldots, \alpha^{(k)}}{\operatorname{maximize}} &amp; f\left(\left{\alpha^{(i)}\right}_{i=1}^{k}\right) \
\text { subject to } &amp; \forall i \operatorname{TVD}\left[\hat{y}\left(\mathbf{x}, \alpha^{(i)}\right), \hat{y}(\mathbf{x}, \hat{\alpha})\right] \leq \epsilon
\end{array}
$$</p>
<p>Where $f\left(\left{\alpha^{(i)}\right}_{i=1}^{k}\right)$ is:</p>
<p>$$
\sum_{i=1}^{k} \operatorname{JSD}\left[\alpha^{(i)}, \hat{\alpha}\right]+\frac{1}{k(k-1)} \sum_{i&lt;j} \operatorname{JSD}\left[\alpha^{(i)}, \alpha^{(j)}\right]
$$</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Median change in output ( $\Delta \hat{y}^{\text {med }}$ ) (x-axis) densities in relation to the max attention (max $\hat{\alpha}$ ) (y-axis) obtained by randomly permuting instance attention weights. Encoders denoted parenthetically. Plots for all corpora and using all encoders are available online.</p>
<p>In practice we maximize a relaxed version of this objective via the Adam SGD optimizer (Kingma and Ba, 2014): $f\left(\left{\alpha^{(i)}\right}<em i="1">{i=1}^{k}\right)+$ $\frac{1}{k} \sum</em>$}^{k} \max \left(0, \operatorname{TVD}\left[\hat{y}\left(\mathbf{x}, \alpha^{(i)}\right), \hat{y}(\mathbf{x}, \hat{\alpha})\right]-\epsilon\right){ }^{7</p>
<p>Equation 1 attempts to identify a set of new attention distributions over the input that is as far as possible from the observed $\alpha$ (as measured by JSD) and from each other (and thus diverse), while keeping the output of the model within $\epsilon$ of the original prediction. We denote the output obtained under the $i^{\text {th }}$ adversarial attention by $\hat{y}^{(i)}$. Note that the JS Divergence between any two categorical distributions (irrespective of length) is bounded from above by 0.69 .</p>
<p>One can view an attentive decoder as a function that maps from the space of latent input representations and attention weights over input words $\Delta^{T-1}$ to a distribution over the output space $\mathcal{Y}$. Thus, for any output $\hat{y}$, we can define how likely each attention distribution $\alpha$ will generate the output as inversely proportional to $\operatorname{TVD}(y(\alpha), \hat{y})$.</p>
<p>Figure 4 depicts the distributions of max JSDs realized over instances with adversarial attention weights for a subset of the datasets considered. Colors again indicate predicted class. Mass toward the upper-bound of 0.69 indicates that we are frequently able to identify maximally different attention weights that hardly budge model output. We observe that one can identify adversarial attention weights associated with high JSD for a significant number of examples. This means that is often the</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Algorithm 3 Finding adversarial attention weights
$\mathbf{h} \leftarrow \operatorname{Enc}(\mathbf{x}), \hat{\alpha} \leftarrow \operatorname{softmax}(\phi(\mathbf{h}, \mathbf{Q}))$
$\hat{y} \leftarrow \operatorname{Dec}(\mathbf{h}, \hat{\alpha})$
$\alpha^{(1)}, \ldots, \alpha^{(k)} \leftarrow$ Optimize Eq 1
for $i \leftarrow 1$ to $k$ do
$\hat{y}^{(i)} \leftarrow \operatorname{Dec}\left(\mathbf{h}, \alpha^{(i)}\right) \quad \triangleright \mathbf{h}$ is not changed
$\Delta \hat{y}^{(i)} \leftarrow \operatorname{TVD}\left[\hat{y}, \hat{y}^{(i)}\right]$
$\Delta \alpha^{(i)} \leftarrow \operatorname{JSD}\left[\hat{\alpha}, \alpha^{(i)}\right]$
end for
$\epsilon-\max \mathrm{JSD} \leftarrow \max _{i} \mathbb{1}\left[\Delta \hat{y}^{(i)} \leq \epsilon\right] \Delta \alpha^{(i)}$
case that quite different attention distributions over inputs would yield essentially the same (within $\epsilon$ ) output.</p>
<p>In the case of the diabetes task, we again observe a pattern of low JSD for positive examples (where evidence is present) and high JSD for negative examples. In other words, for this task, if one perturbs the attention weights when it is inferred that the patient is diabetic, this does change the output, which is intuitively agreeable. However, this behavior again is an exception to the rule.</p>
<p>We also consider the relationship between max attention weights (indicating strong emphasis on a particular feature) and the dissimilarity of identified adversarial attention weights, as measured via JSD, for adversaries that yield a prediction within $\epsilon$ of the original model output. Intuitively, one might hope that if attention weights are peaky, then counterfactual attention weights that are very different but which yield equivalent predictions</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Histogram of maximum adversarial JS Divergence (ε-max JSD) between original and adversarial attentions over all instances. In all cases shown, |<em>ŷndν</em> − <em>ŷ</em>| &lt; ε. Encoders are specified in parantheses. Best viewed in color.</p>
<p>would be more difficult to identify.</p>
<p>Figure 5 illustrates that while there is a negative trend to this effect, it is realized only weakly. Put another way: there exist many cases (in all datasets) in which despite a high attention weight, an alternative and quite different attention configuration over inputs yields effectively the same output. In light of this, presenting a heatmap implying that a particular set of features is primarily responsible for an output would seem to be misleading.</p>
<h2>5 Related Work</h2>
<p>We have focused on attention mechanisms and the question of whether they afford transparency, but a number of interesting strategies unrelated to attention mechanisms have been recently proposed to provide insights into neural NLP models. These include approaches that measure feature importance based on gradient information (Ross et al., 2017; Sundararajan et al., 2017) (aligned with the gradient-based measures that we have used here), and methods based on <em>representation erasure</em> (Li et al., 2016), in which dimensions are removed and then the resultant change in output is recorded (similar to our experiments with removing tokens from inputs, albeit we do this at the input layer).</p>
<p>Comparing such importance measures to attention scores may provide additional insights into the working of attention based models (Ghaeini et al., 2018). Another novel line of work in this direction involves explicitly identifying explanations of black-box predictions via a causal framework (Alvarez-Melis and Jaakkola, 2017). We also note that there has been complementary work demonstrating correlation between human attention and induced attention weights, which was relatively strong when humans agreed on an explanation (Pappas and Popescu-Belis, 2016). It would be interesting to explore if such cases present explicit 'high precision' signals in the text (for example, the positive label in diabetes dataset).</p>
<p>More specific to attention mechanisms, recent promising work has proposed more principled attention variants designed explicitly for interpretability; these may provide greater transparency by imposing <em>hard</em>, <em>sparse</em> attention. Such instantiations explicitly select (modest) subsets of inputs to be considered when making a prediction, which are then by construction responsible for model output (Lei et al., 2016; Peters et al., 2018). <em>Structured attention</em> models (Kim et al., 2017) provide a generalized framework for describing and fitting attention variants with explicit probabilistic semantics. Tying attention weights to human-provided rationales is another potentially promising avenue (Bao et al., 2018). We hope our work motivates further development of these methods, resulting in attention variants that both improve predictive performance and provide insights into model predictions.</p>
<h2>6 Discussion and Conclusions</h2>
<p>We have provided evidence that correlation between intuitive feature importance measures (in-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Densities of maximum JS divergences (ε-max JSD) (x-axis) as a function of the max attention (y-axis) in each instance for obtained between original and adversarial attention weights.</p>
<p>cluding gradient and feature erasure approaches) and learned attention weights is weak when using a BiRNN encoder (Section 4.1). We also established that counterfactual attention distributions — which would tell a different story about why a model made the prediction that it did — often have no effect on model output (Section 4.2).</p>
<p>These results suggest that while attention modules consistently yield improved performance on NLP tasks, their ability to provide transparency for model predictions is (in the sense of pointing to inputs responsible for outputs) questionable. More generally, how one is meant to interpret the 'heatmaps' of attention weights placed over inputs that are commonly presented is unclear. These seem to suggest a story about how a model arrived at a particular disposition, but the results here indicate that the relationship between this and attention is not obvious, at least for RNN encoders.</p>
<p>There are important <strong>limitations</strong> to this work and the conclusions we can draw from it. We have reported the (generally weak) correlation between learned attention weights and various alternative measures of feature importance, e.g., gradients. We do not imply that such alternative measures are necessarily ideal or should be considered 'ground truth'. While such measures do enjoy a clear intrinsic (to the model) semantics, their interpretation for non-linear neural networks can nonetheless be difficult for humans (Feng et al., 2018). Still, that attention consistently correlates poorly with <em>multiple</em> such measures ought to give pause to practitioners. That said, exactly how strong such correlations 'should' be to establish reliability as explanation is an admittedly subjective question.</p>
<p>We note that the counterfactual attention experiments demonstrate the existence of alternative heatmaps that yield equivalent predictions; thus one cannot conclude that the model made a particular prediction <em>because</em> it attended over inputs in a specific way. But these adversarial weights may themselves be unlikely under the attention module parameters. Further, it may be that multiple plausible explanations exist, complicating interpretation. We would maintain that in such cases the model should highlight all plausible explanations, but one may instead view a model that provides 'sufficient' explanation as reasonable.</p>
<p>An additional limitation is that we have only considered a handful of attention variants, selected to reflect common module architectures for the respective tasks included in our analysis. Alternative attention specifications may yield different conclusions; and indeed we hope this work motivates further development of principled attention mechanisms (or encoders). Finally, we have limited our evaluation to tasks with unstructured output spaces, i.e., we have not considered seq2seq tasks, which we leave for future work. However we believe interpretability is more often a consideration in, e.g., classification than in translation.</p>
<h3>Acknowledgements</h3>
<p>We thank Zachary Lipton for insightful feedback on a preliminary version of this manuscript.</p>
<p>This work was supported by the Army Research Office (ARO), award W911NF1810328.</p>
<h2>References</h2>
<p>David Alvarez-Melis and Tommi S Jaakkola. 2017. A causal framework for explaining the predictions of black-box sequence-to-sequence models. arXiv preprint arXiv:1707.01943.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</p>
<p>Yujia Bao, Shiyu Chang, Mo Yu, and Regina Barzilay. 2018. Deriving machine attention from human rationales. arXiv preprint arXiv:1808.09367.</p>
<p>Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326.</p>
<p>Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, and Walter Stewart. 2016. Retain: An interpretable predictive model for healthcare using reverse time attention mechanism. In Advances in Neural Information Processing Systems, pages 3504-3512.</p>
<p>Shi Feng, Eric Wallace, Alvin Grissom II, Pedro Rodriguez, Mohit Iyyer, and Jordan Boyd-Graber. 2018. Pathologies of neural models make interpretation difficult. In Empirical Methods in Natural Language Processing.</p>
<p>Reza Ghaeini, Xiaoli Fern, and Prasad Tadepalli. 2018. Interpreting recurrent and attention-based neural models: a case study on natural language inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4952-4957.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 16931701.</p>
<p>Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016. Mimic-iii, a freely accessible critical care database. Scientific data, 3:160035.</p>
<p>Yoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. 2017. Structured attention networks. arXiv preprint arXiv:1702.00887.</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing neural predictions. arXiv preprint arXiv:1606.04155.</p>
<p>Tao Lei et al. 2017. Interpretable neural models for natural language processing. Ph.D. thesis, Massachusetts Institute of Technology.</p>
<p>Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding neural networks through representation erasure. arXiv preprint arXiv:1612.08220.</p>
<p>Zachary C Lipton. 2016. The mythos of model interpretability. arXiv preprint arXiv:1606.03490.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA. Association for Computational Linguistics.</p>
<p>Andre Martins and Ramon Astudillo. 2016. From softmax to sparsemax: A sparse model of attention and multi-label classification. In International Conference on Machine Learning, pages 1614-1623.</p>
<p>Azadeh Nikfarjam, Abeed Sarker, Karen Oǎ̌̌̌Connor, Rachel Ginn, and Graciela Gonzalez. 2015. Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features. Journal of the American Medical Informatics Association, 22(3):671-681.</p>
<p>Nikolaos Pappas and Andrei Popescu-Belis. 2016. Human versus machine attention in document classification: A dataset with crowdsourced annotations. In Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages $94-100$.</p>
<p>Ankur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. arXiv preprint arXiv:1606.01933.</p>
<p>Ben Peters, Vlad Niculae, and André FT Martins. 2018. Interpretable structure induction via sparse attention. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 365-367.</p>
<p>Andrew Slavin Ross, Michael C Hughes, and Finale Doshi-Velez. 2017. Right for the right reasons: Training differentiable models by constraining their explanations. arXiv preprint arXiv:1703.03717.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on</p>
<p>empirical methods in natural language processing, pages $1631-1642$.</p>
<p>Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</p>
<p>Qizhe Xie, Xuezhe Ma, Zihang Dai, and Eduard Hovy. 2017. An interpretable knowledge transfer model for knowledge base completion. arXiv preprint arXiv:1704.05908.</p>
<p>Caiming Xiong, Victor Zhong, and Richard Socher. 2016. Dynamic coattention networks for question answering. arXiv preprint arXiv:1611.01604.</p>
<p>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pages 2048-2057.</p>
<p>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649-657.</p>
<p>Ye Zhang, Iain Marshall, and Byron C Wallace. 2016. Rationale-augmented convolutional neural networks for text classification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), volume 2016, pages 795-804.</p>
<h2>Appendices</h2>
<h2>A Model details</h2>
<p>For all datasets, we use spaCy for tokenization. We map out of vocabulary words to a special <unk> token and map all words with numeric characters to 'qqq'. Each word in the vocabulary was initialized to pretrained embeddings. For general domain corpora we used either (i) FastText Embeddings (SST, IMDB, 20News, and CNN) trained on Simple English Wikipedia, or, (ii) GloVe 840B embeddings (AGNews and SNLI). For the MIMIC dataset, we learned word embeddings using Gensim over all discharge summaries in the corpus. We initialize words not present in the vocabulary using samples from a standard Gaussian $\mathcal{N}\left(\mu=0, \sigma^{2}=1\right)$.</p>
<h2>A. 1 BiLSTM</h2>
<p>We use an embedding size of 300 and hidden size of 128 for all datasets except bAbI (for which we use 50 and 30, respectively). All models were regularized using $\ell_{2}$ regularization $\left(\lambda=10^{-5}\right)$ applied to all parameters. We use a sigmoid activation functions for binary classification tasks, and a softmax for all other outputs. We trained the model using maximum likelihood loss using the Adam Optimizer with default parameters in PyTorch.</p>
<h2>A. 2 CNN</h2>
<p>We use an embedding size of 300 and 4 kernels of sizes $[1,3,5,7]$, each with 64 filters, giving a final hidden size of 256 (for bAbI we use 50 and 8 respectively with same kernel sizes). We use ReLU activation function on the output of the filters. All other configurations remain same as BiLSTM.</p>
<h2>A. 3 Average</h2>
<p>We use the embedding size of 300 and a projection size of 256 with ReLU activation on the output of the projection matrix. All other configurations remain same as BiLSTM.</p>
<h2>B Further details regarding attentional module of gradient</h2>
<p>In the gradient experiments, we made the decision to cut-off the computation graph at the attention module so that gradient does not flow through this layer and contribute to the gradient feature importance score. For the sake of gradient calculation this effectively treats the attention as a separate input to the network, independent of the input. We argue that this is a natural choice to make for our analysis because it calculates: how much does the output change as we perturb particular inputs (words) by a small amount, while paying the same amount of attention to said word as originally estimated and shown in the heatmap?</p>
<h2>C Correlations between Feature Importance measures</h2>
<p>A question one might have here is how well correlated LOO and gradients are with one another. We report such results in their entirety on the paper website, and we summarize their correlations relative to those realized by attention in a BiLSTM model with LOO measures in Figure 6. This reports the mean differences between (i) gradient</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Mean difference in correlation of (i) LOO vs. Gradients and (ii) Attention vs. LOO scores using BiLSTM Encoder + Tanh Attention. On average the former is more correlated than the latter by $&gt;0.2 \tau_{l o o}$.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Mean difference in correlation of (i) LOO vs. Gradients and (ii) Attention vs. Gradients using BiLSTM Encoder + Tanh Attention. On average the former is more correlated than the latter by $\sim 0.25 \tau_{g}$.
and LOO correlations, and (ii) attention and LOO correlations. As expected, we find that these exhibit, in general, considerably higher correlation with one another (on average) than LOO does with attention scores. (The lone exception is on SNLI.) Figure 7 shows the same for gradients and attention scores; the differences are comparable. In the $A D R$ and Diabetes corpora, a few high precision tokens indicate (the positive) class, and in these cases we see better agreement between LOO/gradient measures with attention; this is consistent with Figure 4 which shows that it is difficult for the BiLSTM variant to find adversarial attention distributions for Diabetes.</p>
<p>A potential issues with using Kendall $\tau$ as our metric here is that (potentially many) irrelevant features may add noise to the correlation measures. We acknowledge that this as a shortcoming of the metric. One observation that may mitigate this concern is that we might expect such noise to depress the LOO and gradient correlations to the same extent as they do the correlation between at-
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Difference in mean correlation of attention weights vs. LOO importance measures for (i) Average (feed-forward projection) and (ii) BiLSTM Encoders with Tanh attention. Average correlation (vertical bar) is on average $\sim 0.375$ points higher for the simple feedforward encoder, indicating greater correspondence with the LOO measure.
tention and feature importance scores; but as per Figure 7, they do not. We also note that the correlations between the attention weights on top of feedforward (projection) encoder and LOO scores are much stronger, on average, than those between BiLSTM attention weights and LOO. This is shown in Figure 8. Were low correlations due simply to noise, we would not expect this. ${ }^{8}$</p>
<h2>D Graphs</h2>
<p>To provide easy navigation of our (large set of) graphs depicting attention weights on various datasets/tasks under various model configuration we have created an interactive interface to browse these results, accessible at: https://successar.github.io/ AttentionExplanation/docs/.</p>
<h2>E Adversarial Heatmaps</h2>
<h2>SST</h2>
<p>Original: reggio falls victim to relying on the very digital technology that he fervently scorns creating a meandering inarticulate and ultimately disappointing film</p>
<p>Adversarial: reggio falls victim to relying on the very digital technology that he fervently scorns creating a meandering inarticulate and ultimately disappointing film $\Delta \ddot{y}: 0.005$</p>
<h2>IMDB</h2>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Original: fantastic movie one of the best film noir movies ever made bad guys bad girls a jewel heist a twisted morality a kidnapping everything is here jean has a face that would make bogart proud and the rest of the cast is is full of character actors who seem to to know they're onto something good get some popcorn and have a great time</p>
<p>Adversarial: fantastic movie one of the best film noir movies ever made bad guys bad girls a jewel heist a twisted morality a kidnapping everything is here jean has a face that would make bogart proud and the rest of the cast is is full of character actors who seem to to know they're onto something good get some popcorn and have a great time $\Delta \ddot{y}: 0.004$</p>
<h2>20 News Group - Sports</h2>
<p>Original:i meant to comment on this at the time there ' s just no way baserunning could be that important if it was runs created would $\mathrm{n}^{\prime} \mathrm{t}$ be nearly as accurate as it is runs created is usually about qqq qqq accurate on a team level and there ' s a lot more than baserunning that has to account for the remaining percent .</p>
<p>Adversarial:i meant to comment on this at the time there ' s just no way baserunning could be that important if it was runs created would $\mathrm{n}^{\prime} \mathrm{t}$ be nearly as accurate as it is runs created is usually about qqq qqq accurate on a team level and there ' s a lot more than baserunning that has to account for the remaining percent . $\Delta \ddot{y}: 0.001$</p>
<h2>ADR</h2>
<p>Original:meanwhile wait for DRUG and DRUG to kick in first co i need to prep dog food etc . co omg $&lt;$ UNK $&gt;$.</p>
<p>Adversarial:meanwhile wait for DRUG and DRUG to kick in first co i need to prep dog food etc . co omg $&lt;$ UNK $&gt;. \Delta \ddot{y}: 0.002$</p>
<h2>AG News</h2>
<p>Original:general motors and daimlerchrysler say they # qqq teaming up to develop hybrid technology for use in their vehicles . the two giant automakers say they have signed a memorandum of understanding</p>
<p>Adversarial:general motors and daimlerchrysler say they # qqq teaming up to develop hybrid technology for use in their vehicles . the two giant automakers say they have signed a memorandum of understanding . $\Delta \ddot{y}$ : 0.006</p>
<h2>SNLI</h2>
<p>Hypothesis:a man is running on foot
Original Premise Attention:a man in a gray shirt and blue shorts is standing outside of an old fashioned ice cream shop named sara 's old fashioned ice cream, holding his bike up, with a wood like table, chairs, benches in front of him .</p>
<p>Adversarial Premise Attention:a man in a gray shirt and blue shorts is standing outside of an old fashioned ice cream shop named sara 's old fashioned ice cream, holding his bike up, with a wood like table, chairs, benches in front of him . $\Delta \ddot{y}: 0.002$</p>
<h2>Babi Task 1</h2>
<p>Question: Where is Sandra?
Original Attention:John travelled to the garden . Sandra travelled to the garden</p>
<p>Adversarial Attention:John travelled to the garden. Sandra travelled to the garden $\Delta \ddot{y}: 0.003$</p>
<h2>CNN-QA</h2>
<p>Question:federal education minister @placeholder visited a @entity 15 store in @entity 17 , saw cameras</p>
<p>Original:@entity1 , @entity2 ( @entity3 ) police have arrested four employees of a popular @entity2 ethnic - wear chain after a minister spotted a security camera overlooking the changing room of one of its stores. federal education minister @entity13 was visiting a @entity15 outlet in the tourist resort state of @entity17 on friday when she discovered a surveillance camera pointed at the changing room , police said . four employees of the store have been arrested, but its manager - herself a woman - was still at large saturday, said @entity17 police superintendent @entity25 . state authorities launched their investigation right after @entity13 levied her accusation. they found an overhead camera that the minister had spotted and determined that it was indeed able to take photos of customers using the store 's changing room, according to @entity 25 . after the incident , authorities sealed off the store and summoned six top officials from @entity15, he said . the arrested staff have been charged with voyeurism and breach of privacy, according to the police . if convicted, they could spend up to three years in jail, @entity25 said . officials from @entity 15 - which sells ethnic garments, fabrics and other products - are heading to @entity17 to work</p>
<p>with investigators, according to the company . " @entity15 is deeply concerned and shocked at this allegation , " the company said in a statement . " we are in the process of investigating this internally and will be cooperating fully with the police . "</p>
<p>Adversarial:@entity1 , @entity2 ( @entity3 ) police have arrested four employees of a popular @entity2 ethnic - wear chain after a minister spotted a security camera overlooking the changing room of one of its stores . federal education minister @entity13 was visiting a @entity15 outlet in the tourist resort state of @entity17 on friday when she discovered a surveillance camera pointed at the changing room , police said . four employees of the store have been arrested, but its manager - herself a woman - was still at large saturday, said @entity17 police superintendent @entity25 . state authorities launched their investigation right after @entity13 levied her accusation. they found an overhead camera that the minister had spotted and determined that it was indeed able to take photos of customers using the store 's changing room, according to @entity25 . after the incident , authorities sealed off the store and summoned six top officials from @entity15, he said . the arrested staff have been charged with voyeurism and breach of privacy, according to the police . if convicted, they could spend up to three years in jail, @entity25 said . officials from @entity15 - which sells ethnic garments, fabrics and other products - are heading to @entity17 to work with investigators, according to the company . " @entity15 is deeply concerned and shocked at this allegation , " the company said in a statement . " we are in the process of investigating this internally and will be cooperating fully with the police . " $\Delta \ddot{y}: 0.005$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ The same contrast can be seen for the gradients, as one would expect given the direct gradient paths in the projection network back to individual tokens.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ While attention is perhaps most common in seq2seq tasks like translation, our impression is that interpretability is not typically emphasized for such tasks, in general.
${ }^{4}$ In the latter case, $h_{t}$ is the embedding of token $t$ after being passed through a linear layer and ReLU activation.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
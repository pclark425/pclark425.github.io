<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7631 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7631</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7631</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-261125320</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.15472v1.pdf" target="_blank">Towards Consistent Language Models Using Declarative Constraints</a></p>
                <p><strong>Paper Abstract:</strong> Large language models have shown unprecedented abilities in generating linguistically coherent and syntactically correct natural language output. However, they often return incorrect and inconsistent answers to input questions. Due to the complexity and uninterpretability of the internally learned representations, it is challenging to modify language models such that they provide correct and consistent results. The data management community has developed various methods and tools for providing consistent answers over inconsistent datasets. In these methods, users specify the desired properties of data in a domain in the form of high-level declarative constraints. This approach has provided usable and scalable methods to delivering consistent information from inconsistent datasets. We aim to build upon this success and leverage these methods to modify language models such that they deliver consistent and accurate results. We investigate the challenges of using these ideas to obtain consistent and relevant answers from language models and report some preliminary empirical studies.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7631",
    "paper_id": "paper-261125320",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004096,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Consistent Language Models Using Declarative Constraints
24 Dec 2023</p>
<p>Jasmin Mousavi mousavij@oregonstate.edu 
Arash Termehchy Oregon State University</p>
<p>Towards Consistent Language Models Using Declarative Constraints
24 Dec 2023F0D5E4D726B4BD6FB0F376A90B004581arXiv:2312.15472v1[cs.DB]
Large language models have shown unprecedented abilities in generating linguistically coherent and syntactically correct natural language output.However, they often return incorrect and inconsistent answers to input questions.Due to the complexity and uninterpretability of the internally learned representations, it is challenging to modify language models such that they provide correct and consistent results.The data management community has developed various methods and tools for providing consistent answers over inconsistent datasets.In these methods, users specify the desired properties of data in a domain in the form of high-level declarative constraints.This approach has provided usable and scalable methods to delivering consistent information from inconsistent datasets.We aim to build upon this success and leverage these methods to modify language models such that they deliver consistent and accurate results.We investigate the challenges of using these ideas to obtain consistent and relevant answers from language models and report some preliminary empirical studies.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown unprecedented abilities in processing natural languages [32,40].They effectively generalize to perform various tasks with few or no training examples.Thus, there is a rapidly growing interest in using them to solve data-driven problems, such as, interactive question answering.</p>
<p>Nonetheless, LLMs often provide incorrect answers to input queries and perform inaccurate inferences [18,32].Several studies indicate the recent LLMs provide up to 40% erroneous answers to factual questions [32].These erroneous results are important obstacle for wide-spread use of LLMs in real-world applications.</p>
<p>To address the problem of inaccurate answers returned by LLMs, we should recognize that LLMs are not knowledge bases, but rather approximate models of factual in-formation.Due to this approximate nature, they may represent inaccurate and inconsistent patterns.They may overgeneralize relationships in the pretraining data, which leads to returning spurious relationships and inaccurate results.The uninterpretable mixture of learned linguistic patterns and factual information has made it challenging to eliminate incorrect information from LLMs.</p>
<p>Nevertheless, we may be able to restrict LLMs' pretrained representation or decoding to adhere to semantic constraints in the domain to avoid generating incorrect results.This is akin to the problem of data cleaning and answering queries over inconsistent databases [1,3,9,14,51].Databases often contain data that does not comply with the semantic constraints in their domains.For example, a person might not have any social security number or have more than one in a human resource database.The usual query processing methods might return inaccurate results over incomplete or inconsistent databases.The data management community has developed a unified, usable, and scalable approach to repairing and querying inconsistent data based on declarative semantic constraints [3,37,41,49].Hence, instead of writing long and complex imperative programs, users specify the properties of the consistent dataset succinctly in high-level declarative languages.They are usually subsets of first order logic that are sufficiently expressive to capture important knowledge in the domain yet not too expressive to make reasoning intractable.Hence, data systems may check incompatibilities or redundancies in constraints efficiently.They can also be learned from data in an unsupervised manner [4,33].This approach offers an end-to-end and unified method: data systems use these constraints both to clean or return reliable answers over inconsistent data.</p>
<p>There has been recent effort on limiting the decoded output of LLMs to follow some syntactical patterns, e.g., contain certain keywords [21,26].In these systems, users often write (imperative) programs that detect some invalid patterns in the output of LLMs.These systems, then, use constrained optimization or probabilistic inference over the sequences generated by the LLM to reduce the probability of the outputs with invalid patterns.These efforts are steps in the right direction but fall short of providing a usable and scalable method to deliver consistent information over LLMs.First, they do not generally support semantic constraints.Second, users may have to write multiple and possibly long programs to clean up the output of the model.As some domain may have numerous constraints, it is challenging to develop and maintain these programs.Users must check manually whether these programs are consistent with each other and there is no redundancy across different programs.Third, they are applied during the decoding stage, therefore, they can detect and eliminate only a limited set of inconsistencies.Specifically, it is difficult for them to control all the implications of imprecise learned information in LLMs.For instance, the learned spurious relationships about one entity e 1 may impact how the LLM answers a question about a different but related entity e 2 .It is challenging to understand and apply implicit implications of imprecise information about e 1 when they analyze the output about e 2 during decoding.Finally, modifying LLMs' outputs during decoding often reduces their (linguistic) coherency [21,26].</p>
<p>We believe that the success using of high-level semantic constraints in data management suggests that applying semantic constraints could similarly offer a practical and scalable approach for developing and maintaining reliable and consistent LLMs.In this paper, we propose an endto-end framework to provide a usable and unified approach to reduce inconsistencies in LLMs using high-level declarative constraints.We leverage concepts and methods of data cleaning and querying or learning over inconsistent data in the data management community.We also investigate the challenges of using declarative constraints to pre-train accurate representations and infer precise answers from LLMs.We discuss how to use current work on using declarative constraints to learn accurate ML models over inconsistent data [38,53] and techniques to embed structured queries in vector space [17,20,42] to address these problems.We also report preliminary results for integrating constraints in Llama-2 [46].</p>
<p>Declarative Constraints</p>
<p>The properties of entities and relationships in a domain of interest are often expressed as a set of (semantic) constraints.Constraints take various forms depending on the data model used to represent information in the underlying domain.For example, in relational data, constraints are typically expressed as logical sentences involving relational predicates, such as functional dependencies [1].</p>
<p>Ontologies describe constraints on concepts, i.e., sets of entities, members of concepts (entities), and binary relationships between concepts (predicates) [8,17,19,20].The simplicity of ontologies makes them a convenient choice for representing semantic properties of entities and relationships frequently found in textual data.Moreover, ontologies have been developed and are available across various domains, e.g., healthcare [31].Thus, in this paper, we assume that the semantic constraints are expressed in form of ontologies.Nevertheless, we believe that the challenges and approaches discussed in this paper can be extended to other types of constraints.</p>
<p>To better understand this representation, let us consider an ontology for the domain of people.Concepts refer to sets of individuals, binary predicates denote relationships between people, and members of concepts represent individual names.For instance, the concept President represents the set of presidents, while Democrat represents people registered as Democrat.Binary predicates like partyOf establish relationships between individuals, indicating which presidents belong to the Democratic Party.</p>
<p>Learning Consistent Representations</p>
<p>Due to the lack of interoperability in a standard offthe-shelf LLM's learned representation, it becomes unclear whether these representations respect the semantic constraints of a given domain.To address this problem, the LLM could be fine-tuned with the objective of learning a more consistent representation that encompasses semantic constraints.</p>
<p>The large number of parameters and intricate internal representations of LLMs make it difficult to directly enforce declarative constraints.The challenge lies in effectively incorporating these constraints into the pre-training or fine-tuning process without compromising the overall performance and generalization capabilities of the model.To address this challenge, we need novel techniques that can include declarative constraints in an LLM learned representation.</p>
<p>Encoding Constraints in Input Sequences</p>
<p>As declarative constraints are typically represented as ontologies, one approach to incorporating them into an LLM is by augmenting the context of the pre-training or fine-tuning input sequences.By including this as part of the input context, the LLM can leverage the additional information to generate semantically constrained responses.</p>
<p>To extract information from an ontology, it is necessary to first identify the entities that are relevant to the finetuning data.These selected entities then serve as a foundation for retrieving constraint information from the ontology.Incorporating information from an ontology into LLMs presents challenges, as LLMs are fine-tuned on unstructured data, while ontologies convey structured information.To overcome this, one may simply supplement the training data with information from the ontology in textual form, e.g., Obama is a President.However, in domains containing several semantic constraints, the augmented training data may exceed the maximum sequence lengths (commonly restricted to 512 in most models).</p>
<p>Another approach is to encode the ontology information into an embedded representation using an LSTM [25].The embedding is integrated through the use of a gating function, allowing the language model to control what information augments the input.While this method successfully limits sequence length, it may not be optimal for incorporating constraints, as it could lead to information loss.Moreover, this method is more suitable for enriching the input with additional context rather than excluding incorrect information.Unfortunately, both these approaches fall short of incorporating the ontology in a manner that retains the structured information.</p>
<p>Structural Embedding</p>
<p>Ideally, the representation learned by an LLM should capture the structural information in the constraints.Geometric embeddings (e.g.box, circle, cone) have been widely explored for learning representations of graph structures such as ontologies and knowledge bases [15,17,20,42,43,50].These embeddings aim to capture the inherent geometry and structure of the entities and relationships within the graph, including the set-based rules and logic constraints.By constructing relational geometric embeddings, the resulting entity representations exhibit convex regions that not only model the similarity between entities but also encode set-based rules and logic constraints.These embeddings preserve the structural properties in an embedded space, ensuring that the output representations maintain the specified constraints.For instance, if an ontology has the constraint that every employee is a person, the geometric embedding for person should contain the geometric embedding for employee, reflecting that employees are a subset of persons.The objective of learning geometric embeddings is to associate geometric points and concepts with geometric shapes in R n such that the constraints of the ontology are preserved.Each constraint is associated with a loss function, and the overall objective is to minimize the total loss, which is the summation of the losses for each constraint.These embeddings preserve the structural properties in an embedded space, ensuring that the output representations maintain the specified constraints.</p>
<p>Geometric embeddings have primarily been utilized in the context of ontology and knowledge bases [17,20,43].These applications focus on modeling structured data, while LLMs model unstructured data.It is not clear how to directly extend these concepts to LLMs.</p>
<p>Intuitively, the LLM should generate a probability distribution over a sequence of words in such a way that the probability of the next word, given the preceding context, lies within a predefined geometric shape that represents the desired constraints.To achieve this, we propose incorporating geometric embeddings into the LLM, enabling us to directly optimize the objective function with respect to the specified constraints.In addition to the traditional masked language modeling objective, we can introduce additional objective tasks that align with the constraints.One such objective task is type modeling [35], which involves predicting the type of the next word given a set of ontology concepts c, a word w, and a type function c(w) that indicates whether the word represents a concept entity (the word can also extend to a span of words).The goal is to predict the type of the next word based on this context.Another objective task is entity composite modeling [35].In this task, given the previous context w and the type function c(w), the objective is to predict the next word, which could be an entity name or any other relevant information related to the specified constraints.</p>
<p>During inference, the generated probabilities from the autoregressive language modeling, type modeling, and entity composite modeling tasks are combined to compute the final probability distribution over all possible words.This distribution represents the model's output and can be used to generate coherent and constraint-conserving responses.By adding geometric embeddings and incorporating additional objective tasks, the model can be optimized to generate outputs that adhere to the desired constraints and exhibit meaningful behavior.</p>
<p>Enforcing Constraints Using Prefix-Tuning</p>
<p>Optimizing over multiple constraints can become computationally expensive, especially in the context of pretraining or fine-tuning of LLMs with numerous parameters.To address this challenge, one may employ a prefixtuning technique, which offers a lightweight strategy for fine-tuning [22].</p>
<p>Prefix-tuning involves freezing the gradients for most parameters during the fine-tuning process.It serves as a continuous method for tuning the prompt, which is otherwise discrete since it is represented in the word space.The core idea is to fine-tune the model for a specific task or constraint by adding a prefix vector to the input sequence.The prefix functions similar to prompt-tuning, effectively adding extra words to guide the model's behavior.</p>
<p>The key distinction is that the prefix is treated as virtual tokens that do not have associated parameters.The transformer can attend to the prefix as if it were actual tokens, allowing it to influence the model's behavior.The objective of prefix-tuning is to optimize the prefix specifically for the desired task or constraint.Since the gradients are turned off for the majority of the LLM, the computational overhead for prefix optimization is minimal.</p>
<p>By using prefix-tuning, an LLM can effectively support multiple tasks or constraints simultaneously.Each task can be associated with its own optimized prefix, allowing the model to adapt its behavior accordingly.This lightweight fine-tuning strategy allows the model to effectively model constraint-specific behavior while mitigating the computational complexity that arises when optimizing over a large number of constraints and parameters.</p>
<p>Model Repair</p>
<p>To ensure that a database complies with a constraint, we often find the information in the database that do not follow the constraint and update them so the database satisfies the constraint.we may adopt this approach to repair a pretrained model so it satisfies a set of given constraints.In other words, we may find the portion of the model responsible for representing a constraint or lack thereof and update them if necessary so that the resulting model satisfies the constraint.Fact-based Repair.There has been some recent success in updating facts represented in an LLM by modifying its weights [28].Each update aims at changing the object in a given triples in form of (subject (s), relation (r), object (o)).These methods find and modify the weights responsible for encoding o so it represents the new object with high probability.</p>
<p>Building upon this work, one may check whether a model satisfies a constraint by finding and modifying the pre-trained weights that represent the facts that violate the constraints.We may represent each constraint as a set of facts that follow the constraint.</p>
<p>It is known that there are often many possible modifications of an inconsistent dataset to satisfy a set of constraints.It is challenging to maintain and query all these repairs of databases.Hence, researchers have proposed heuristics to choose a few of these repairs, e.g., the ones that differ the least from the original database.The same problem might also happen in repairing models.One may use similar semantics and heuristics to maintain just a few repaired models.Constraint-based Repair.It may take a long time to update a large number of facts in a model [28].Thus, the approach of fact-based repair may efficiently modify the model to satisfy constraints with a relatively few instances, e.g., facts in the ontology.However, it might be computationally challenging to do for constraints with many instances.Additionally, if a constraint has many instances, this approach might deliver many possible model repairs even after applying the aforementioned heuristics to reduce the space of possible repairs.It will be challenging to query or train these models for a given task.</p>
<p>LLMs generalize input data during pretraining.They have also been successfully used to generate data that closely resembles real-world data and train accurate models using a relatively few training examples for various tasks.Hence, we hypothesize that they might represent some constraints in the domain in whole or in part.If this hypothesis is true, an LLM does not satisfy some constraints because the LLM might represent them incompletely or erroneously.Hence, to ensure that the model satisfy a constraint, instead of repairing all facts that violate the constraint, one might change directly the portion of the model that represents a constraint.This portion might be significantly smaller than the parts that represent the violating facts.Thus, it might be substantially faster and easier to find the weights in the model responsible for incomplete or erroneous representation of the constraint than doing the same for all facts that violate that constraint.</p>
<p>Consistent Querying and Prompting</p>
<p>Since it is often resource-intensive to (re)train a large language model, users may like to use an available off-theshelf language model.Hence, they might not (re)train a large language model to apply semantic constraints.Additionally, as the learned representation in a language model contains a low-dimensional approximation of the training information, it is not clear whether it precisely and faithfully represent the relationships between entities and concepts according to the input constraints.</p>
<p>To address this problem, one can use off-the-shelf language models and enforce the domain constraints during decoding.A similar problem arises in querying inconsistent datasets.Users often would like to avoid cleaning an inconsistent dataset due to its costs and overhead and prefer that the system returns answers to an input query that are consistent with the data constraints over the inconsistent dataset, i.e., consistent query answering [3].It has been shown that consistent answers can be computed efficiently for some classes of formal queries languages and constraints [5].</p>
<p>There are, however, some challenges for adapting the idea of consistent query answering to return accurate results of language models.First, consistent query answering is done via rewriting the input query using the available constraints such that the resulting query encapsulates precisely the properties of answers to the query that adhere to the constraints.The modified query is executed over the dataset and its answers are returned to the user.As opposed to a database whose content is interpretable, it is challenging to interpret the learned representation of a language model.Thus, it is not clear what the modified query will return over the language model.</p>
<p>Second, questions or training information over language models are often in form of natural languages rather than formal query languages, such as SQL.There have been some effort to develop formal query languages to prompt or ask questions from language models [6,30].Nonethe-less, users often would like to ask their questions in natural language.In what follows, we explain how to address these challenges.</p>
<p>Query Rewriting As Chain-of-Thought Prompting</p>
<p>Rewriting queries using constraints has generally two benefits for querying datasets.First, the modified query may contain more information about the underlying domain, which are not available in the database [8].This may lead to more informative answers.For example, assume that there are some domain constraints in form of an ontology.One might be able to find new information about the relationships of entities, e.g., each patient is a human, which might not be explicitly available in the database.Hence, the rewritten query might convey more information about the domain than the original one.Second, the modified query expresses properties of the data items that are consistent with the domain constraints [3,5].Thus, it does not return inconsistent answers and improves precision of the original query.There are algorithms to rewrite formal queries over relational databases in both cases.As we explained before, there are challenges of extending these ideas for querying language models.</p>
<p>Recently, researchers have observed that explaining the properties of the desired answers gradually, i.e., chain of thoughts, improves the accuracy of answering questions over language models [48].One may use this property and provides the language model with step by step explanation of the modified query.In particular, if the original query is written in a formal query language, such as SQL [6,30], it lends itself to clear step by step explanation using its operators [44].</p>
<p>If the original query is in a formal programming language [6,13,30], the current ideas in query rewriting using constraints might be used to modify the original query.If the input query is in natural language, one should first identify the entities that appear in the query to find relevant constraints.Since constraints are usually written in subsets of first-order logic, one can express them in form of natural language, e.g., by translating ⊂ to is a subset of.The final query will be a composition of the original one and natural language translation of its relevant constrains.</p>
<p>Checking and Repairing Generated Sequences</p>
<p>Due to the uninterpretablity of the learned representation in a language model, it is difficult to ensure that rewriting the query will return accurate answers.Hence, one would also like to check whether the returned answers from the language model comply with the constraints in the domain.If the answers do not follow some constraints, we would like to repair and modify them so that they adhere to the constraints while still being sufficiently relevant to the input question.</p>
<p>Checking the Compliance of Output Sequences</p>
<p>Data management community has provided several methods to check whether a structured dataset comply with a set of semantic constraints [5,14,41,51].There are two important differences between checking the compliance of structured datasets, e.g., relational data, and sequences of tokens generated by a language model.First, the output of popular language models is often in form of natural language to make it usable for end-users.To check whether a sequence returned by a language model violates a constraint, one has to find tokens in the sequence that are members of concepts or mentions to entities or relationships in constraints.This might be difficult as each entity might appear in different forms in a sequence of natural language tokens.Second, checking the consistency of a dataset is often done offline and before query time.Nonetheless, we have to check the generated sequences of language model during query time.Hence, it is essential to perform it very quickly.</p>
<p>As explained in Section 1, researchers have proposed methods that check the sequence of tokens generated by language models to satisfy lexical patterns [21,26,52].In particular, authors in [21] frame this as a probabilistic inference problem.To answer a question, a language model may produce a set of candidate sequences and will return the one with largest probability of matching the input.This method sequentially checks the output sequences as they are generated.Given the probability of returning a sequence by the language model and the degree by which the sequence might violate the lexical patterns, this method computes posterior probability of satisfying the patterns for generated sequences and returns the one with the largest posterior probability.This approach provides a clear semantic for the degree by which a sequence satisfies given patterns.</p>
<p>Nonetheless, it might be challenging to extend this approach for semantic constraints.Some domains may have many constraints, it might be too time-consuming to check to what degree a set of possible sequences satisfy all constraints in these domains during query time.One might check constraints in order of their importance or probability of being violated based on previous observations to save some time.We can also use current research on reasoning over constraints to find a minimal set of constraints that imply the entire set of reduce the number of constraints [1].</p>
<p>Another possible challenge is that one might have to check relatively long sequences of text to detect violations of semantic constraints.For example, the relationship between two entities might be represented in a relatively long sentence (paragraph) with each entity is placed in one end of the sentence (paragraph).The longer the size of examined sequences gets, the more generated sequences must be checked to compute the one with the largest posterior probability.This may significantly increase the time of returning an answer to the user.To speed up this process, one might test the constraints in the order of how close the mentions to the concepts or relationships usually appear in the text to prune some candidate sequences early.Another useful technique is to consider a relatively small sample of possible sequences instead of the entire set to return the final consistent result.</p>
<p>Semantic Constraint Detection in Text</p>
<p>To check whether a given sequence violates a constraint, we must identify in the sequence of generated tokens by the language model the mentions to the entities, concepts, or relationships in the constraint.One might assume that there are knowledge bases that contains names of members of a concept or relationship in some domains [25].Also, natural language processing and information extraction communities have developed effective techniques to discover mentions to concepts, entities, or relationship in text [36,54].These methods are often not exact and return a probability distribution over a set of possible concepts or relationships tokens represent.One can consider this source of uncertainty during computing the posterior probability of satisfying constraints by sequence.</p>
<p>Repairing Output Sequences</p>
<p>If none of the output sequences satisfy the constraints to a sufficiently large degree, one might try to modify sequences so that they satisfy the domain constraints while still being relevant to the input question according to the language model.The database community has proposed several approaches to repair an inconsistent structured dataset to comply with a set of semantic constraints efficiently [5,14,41,51].These methods define a set of operations, e.g., insertion or deletion, to modify the input dataset so that the resulting dataset(s) satisfies the given constraints.Since there are often various ways to repair an inconsistent dataset, researchers have proposed methods to return a reasonable subset of all possible repairs, e.g., that ones with minimal amounts of modifications.</p>
<p>One might extend the ideas to repair the decoded sequences of a language model by defining a set of operations, e.g, replacing or dropping tokens.However, due to the complexity and heterogeneity of natural language, it is not clear whether we can precisely define the exact and generalizable conditions of applying a set of abstract operations under which the resulting sequence always satisfy the input constraints.For example, the repair operations that make a sentence consistent with a given constraint might not do the same for another sentence.It is also challenging to ensure that the repaired sequences are still syntactically correct, linguistically coherent, and relevant to the original question.</p>
<p>To address this problem, one approach is to iteratively and incrementally modify the sequence in a greedy manner.After each modification, the resulting sequence can be checked to determine if it satisfies the input constraints.Afterward, we might prompt the language model again with the repaired sequence and original question to return a linguistically coherent version of the sequence that are relevant to the question.The prompt might explicitly ask the language model not to regenerate the original inconsistent sequence.One might also provide additional restrictions to the set of repair operations to increase the degree of relevance and coherency for the repaired sequence.To generate relevant repairs efficiently, we can generate the repairs for a small number of promising original sequences, e.g., the ones with highest posterior probabilities.</p>
<p>Preliminary Empirical Results</p>
<p>In this section we present our preliminary results for integrating lexical constraints with LLMs using the Common-Gen benchmark [23].We identify the risks and trade-offs of augmenting LLMs with lexical constraints for the input and output layers, i.e., prompt only and decoder only, in terms of generation quality, constraint satisfaction, and efficiency.We also explore whether injecting constraints in multiple layers, i.e., prompt + decoder, will help or hurt any risks and trade-offs that exist in single layer augmentation.</p>
<p>LLM Implementation Details</p>
<p>We use Llama-2 [46] as our pretrained language model across all experiments.Llama-2 was pretrained over 2 trillion tokens of data between January 2023 and July 2023.Llama-2 consists of 7 billion parameters, 32 layers, 4096 hidden representation size, 32 attention heads, a 4096 token context window size.</p>
<p>Dataset</p>
<p>The CommonGen dataset [23] is a benchmark designed for controlling language model generation with lexical constraints, i.e., contain certain keywords.Given a set of keywords, e.g., "dog run field", the goal is to generate a sentence using all the keywords or the infections of the keywords, e.g., "dogs" or "dog".Each set contains a minimum of 3 keywords and a maximum of 5 keywords.The dataset is split into train, validation, and test sets of sizes 64.7k, 4.02k, and 1.5k rows, respectively.Typically, those using the CommonGen dataset would first fine-tune their language model using the training set.However, given the size of modern LLMs, users may not have the resources for fine-tuning an LLM.Hence, we focus on using inferencebased algorithms that can be used with off-the-shelf models  [26], and hard constraint decoding, i.e., masked-based Sequential Monte Carlo (SMC) [21].Two prompting strategies were conducted: abstract based (ABS) and conjunctive normal form based (CNF).Each prompting strategy leveraged 0-shot, 1-shot, and 2-shot in-context examples.With the exception of efficiency (lower is better), a perfect score is 100.</p>
<p>without fine-tuning.Our results are conducted over the test set.</p>
<p>Lexical Constraints.</p>
<p>In the CommonGen dataset, lexical constraints can be defined for a set of keywords [w1, w2, w3] as follows.If S is a sentence, then S must contain w1 or one of its inflections, w2 or one of its inflections, and w3 or one of its inflections.The objective here is to generate sentences that adhere to this constraint.A key characteristic of this constraint is its allowance for multiple valid outputs, stemming from the underspecificity of the input.This leads to a wide array of possible sentences that represent instances of the constraint.Data Leakage.In recent years, researchers have become concerned with data leakage in LLMs [11,12].Due to their ability to memorize training data [11], benchmark performance is often inflated.Given that Commongen is a public dataset, it is likely that Llama-2 [46] has seen this dataset and even memorized ground truth sentences for the train and validation sets.However, since ground truth sentences for the test set are not publicly available, it is unlikely that Llama-2 memorized them.</p>
<p>Input Layer Prompting Strategies</p>
<p>We test two prompting techniques with varying representation sizes for lexical constraints on the CommonGen dataset [23].We also supply the prompt with additional in-context examples, i.e. 0-shot, 1-shot, and 2-shot.Examples were extracted from the training set.</p>
<p>Conjunctive Normal Form (CNF) prompting style models lexical constraints, i.e., keywords, in conjunctive normal form.For example, if the given concepts are "dog run field", the lexical constraint in conjunctive normal form is (dog ∨ dogs ∨ ... ) ∧ (run ∨ running ∨ ... ) ∧ (field ∨ fields).We can translate this constraint to text by converting ∨ to or and ∧ to and.Hence, our final prompt is "Write a sentence using the words (dog or dogs or ... ) and (run or running or ... ) and (field or fields)".</p>
<p>Abstract (ABS) prompting style describes an abstract instance of a lexical constraint, e.g., "Given a set of words x, write a sentence using all words in x or inflections of x".Since ABS prompts do not include specific instances of keyword inflections, it is more compressed CNF style prompts.</p>
<p>Output Layer Decoding Strategies</p>
<p>We test two decoding strategies with varying levels of satisfaction: soft constraint decoding with NeuroLogic [26] and hard constraint decoding with Sequential Monte Carlo [21].</p>
<p>NeuroLogic (NL) [26] is an inference time decoding algorithm that uses a variant of beam-search.The objective is to optimize the probability of generating sequences while also steering towards lexical constraints using a penalty term.Due to the interest of using off-the-shelf models, we chose not to fine-tune an LLM for using the NeuroLogic decoder.It is important to note, however, their experiments were conducted using a fine-tuned model.</p>
<p>Sequential Monte Carlo (SMC) [21] is an inference time masked decoding algorithm.They model sequence generation as a probabilistic inference problem using a variant of Sequential Monte Carlo with particle filtering.In SMC, a user writes a program that specifies the desired constraints in a sequential manner.The user may also specify the number of particles used, where each particle acts as a weighted sample of the posterior distribution.We programmed the task in CommonGen as an infilling problem, where keywords are sampled with a masked vocabulary.</p>
<p>Metrics</p>
<p>Generation Quality is measured using automatic metrics, such as ROUGE [24], BLEU [34], CIDEr [47], and SPICE [2].These metrics generate a quality score for the generated sentence based on human generated reference sentences, where a perfect score is 100.</p>
<p>ROUGE-L is a precision and recall based metric that identifies the longest common co-occurring n-grams and sentence-level similarity by calculating the weighted harmonic mean.BLEU-4 is a precision based metric that counts the matching 4-grams between the generated and ref-erence sentences.CIDEr is a consensus based metric that takes the average cosine similarity of Term Frequency -Inverse Document Frequency weighted n-grams.SPICE is a semantic propositional based metric that establishes syntactic dependencies between words, then maps the syntactic dependencies using logical rules, and finally computes the F-score defined over the logical rules.</p>
<p>Constraint Satisfaction measures the method's ability to fully satisfy the constraint (generated sequence used all keywords or their inflections), i.e., satisfied.We also calculate coverage, which is an average over the percentage of keywords (or their inflections) used in the generated sequence.</p>
<p>Efficiency is computed as the time taken (in seconds) for generating a sequence, i.e., inference time.</p>
<p>Results &amp; Analysis</p>
<p>Results over all experiments can be found on Table 1.</p>
<p>Prompt Only</p>
<p>We aim to understand how varying the constraint representation in the prompt, i.e., ABS vs. CNF and in-context examples, i.e., n-shot, impact generation quality, constraint satisfaction, and efficiency.</p>
<p>Across most experiments ABS prompting achieves higher satisfaction than CNF prompting.This suggests that LLMs can understand abstract, high-level descriptions of constraints.Given the fact that CNF prompts include all the inflections, one would expect higher constraint satisfaction across all experiments, however, this is not the case.With the exception of CNF 1-shot, ABS style prompting obtains higher satisfaction than CNF.</p>
<p>ABS prompting outperforms CNF prompting in terms of generation quality across all experiments.CNF style prompts are inherently more structured and further from 'natural language' compared to ABS style prompts.This suggests structured prompts are less beneficial and may require a fine-tuning strategy.</p>
<p>Increasing input length does not have significant impacts on efficiency.Despite ABS prompts having a smaller constraint representation size than CNF prompts, there is little change in inference time across all n-shot experiments.</p>
<p>In-context examples boosts quality in both prompting strategies, but hurts satisfaction in CNF 2-shot.Including more than one in-context example worsens constraint satisfaction for CNF style prompts.This suggests that extending the input context with inflections for every in-context example may lead to noisy, sub-optimal distributions during generation.</p>
<p>Decoder Only</p>
<p>In this section we discover the impacts of the output layer, i.e., decoder, on generation quality, constraint satisfaction, and efficiency.We compare two decoding strategies: soft constraint decoding, i.e., beam-based NL and hard constraint decoding, i.e., masked-based SMC.</p>
<p>In the absence of fine-tuning, beam-based/soft constraint decoding, i.e., NL, encounters challenges in both generation quality and constraint satisfaction.Compared with SMC, NL is more dependent on a high quality output distributions.This suggest that soft constraint decoding may require higher quality output distributions from the LLM.</p>
<p>Increasing the number of particles for SMC decoding does not yield quality or satisfaction improvements while increasing inference time.This observation indicates that the underlying distribution may be of low quality, as increasing the number of particles does not enhance performance.Moreover, in cases of uncertainty, the decoder will not see benefits by increasing computational resources.</p>
<p>Although the SMC decoder achieves 100% constraint satisfaction, this achievement comes at the cost of significantly reduced efficiency.For example, the longest inference time recorded among the tested prompting strategies was only 1.85 seconds, in contrast, SMC with 8 particles required a considerably longer duration of 22.92 seconds for generation.This indicates a substantial increase in computational time required to achieve complete constraint satisfaction with masked decoding strategies, such as SMC.</p>
<p>Despite the improvements in constraint satisfaction, decoder only strategies tend to degrade generation quality and reduce efficiency.</p>
<p>Prompt &amp; Decoder</p>
<p>Although prompt only strategies have higher performance on generation quality and efficiency, they cannot provide any guarantees on constraint satisfaction.Conversely, decoding strategies optimize over constraint satisfaction, but at the cost of generation quality and efficiency.In this section we aim answer whether these two layers can work together to improve the disadvantages of single layer augmentation with a multi-layer approach, i.e., an end-to-end system.More specifically, we would like to understand how different strategies work together and whether they induce any trade-offs between our metrics.</p>
<p>Augmenting the prompt with constraints enhances generation quality and constraint satisfaction, indicating that prompting results in a higher-quality output distribution for the decoder to operates on.Notably, the NL decoder, although underperforming as a standalone decoder, shows remarkable improvement in quality metrics when combined with prompts.This demonstrates that soft constraint decoding performance depends on the quality of the output distribution.</p>
<p>Although prompting improves quality in SMC, it has significant impacts on efficiency.Checking for hard constraints within the SMC decoding strategy is less scalable when compared to the implementation of soft constraints in NL.In contrast, the NL decoder benefits in both quality and efficiency.Due to the higher quality output distributions produced with prompting, the NL decoder spent less time searching, leading to reductions in inference time.</p>
<p>Across most experiments, the NL decoder achieves higher generation quality than SMC.The use of soft constraints in NL results in less drastic distribution changes compared to SMC, allowing for higher quality generation, albeit with a trade-off in constraint satisfaction.</p>
<p>Despite the NL decoder leveraging CNF formula, it exhibits higher satisfaction levels with ABS style prompts.This indicates that structured prompts could potentially limit the model's performance by producing sub-optimal output distributions for the decoder.It suggests that highlevel concepts and relationships might be more effective inputs to the model when optimizing the output distribution for decoding.</p>
<p>Related Work</p>
<p>Domain Specific Fine-Tuning.One can fine-tune an LLM on a set of domain-specific data sources to improve the quality of its answers for questions in a given domain [16].Nonetheless, it has shown that these methods may also lead to many inaccurate answers [26].This is, in part, due to the fact that fine-tuning is inherently under-specified and may not sufficiently modify the model to eliminate its already learned spurious information.</p>
<p>Retrieval-Augmented LLMs.Researchers have augmented LLMs with additional and potentially relevant information from external data sources [10,25,29].These methods often add extra information to the context considered during pretraining.This line of research have improved the accuracy of LLMs to a limited degree, as it does not address the core issue of having spurious and incorrect information in LLMs.It is unclear whether adding more relevant information eliminate inaccurate information stored in the model.Moreover, finding sufficiently many relevant data sources, particularly for long-tail entities, may pose challenges.Compared to retrieval-based augmentation, we argue that constraints offer a more robust and adaptable framework for reducing inconsistencies in LLMs.Constraints encapsulate rules governing the underlying domain, thereby enabling a system to generalize beyond particular instances in a dataset, i.e., out of distribution generalization.They also are a generalization of retrieval-based approaches that augment LLMs with facts extracted from external sources, as each fact is a special case of a constraint.Moreover, they extend beyond mere facts by representing more expressive relationships, such as instances where certain entities lack connections with other concepts.Constraints are also a form of high-level knowledge and effectively abstract large quantities of data.Their compressed representation offers a flexible and efficient method of augmenting LLMs by (1) allowing for soft incorporation of constraints (e.g.adhere to a constraint with 80% probability), ( 2) reducing the size of information used as context to LLMs, and (3) providing a structured way to control the output of LLMs.</p>
<p>Semantic Parsing.LLMs have been an effective approach for program synthesis [39,45].These methods employ constrained decoding techniques to guarantee that the generated output aligns with the syntax and grammar of the target programming language.It is essential to note that the concept of "semantic" in these contexts differs from our work.In our work, semantic constraints serve as high-level rules that define relationships and characteristics between concepts.</p>
<p>Self-Consistency of Language Models.It is known that language models produce contradictory answers to the questions that seek the same information but phrased differently.Researchers have proposed methods to address this issue by prompting the language model to critique and refine its own output during inference [27].This method prompts the language model with differently phrased questions and builds a (weighted) model over answers to infer the most likely result.We, however, mainly focus on ensuring that the language model follows semantic constraints.</p>
<p>Extracting Knowledge from Language Models.Researchers have proposed methods to extract generic statements or factual knowledge from language models using prompt engineering and human supervision [7].The prompts are constructed in a way that encourages succinct factual statements.They use human labeled data to detect inaccurate outputs and fine-tune the language model.However, it might be challenging to collect a sufficient amount of training data to extract accurate statements.</p>
<p>Querying Language Models.There has been some recent effort to design programming languages for prompting large language models, i.e., language model programming [6,13,30].There are generally domain-specific programming languages to extract information from and control the output of a large language model to satisfy the users' input hard constraints, akin to where conditions in SQL queries.Some of these languages resemble database query languages, e.g., SQL [30].These languages aim at making it easier to query and prompt and optimize the number of calls to large language models.However, these languages do not generate consistent results conditioned on domain constraints.Thus, they may return answers that violate semantic constraints in the domain.</p>
<p>Table 1 .
1
Performance results on generation quality, constraint satisfaction, and efficiency over the CommonGen test set for different generation methods: decoder only, prompt only, and prompt + decoder.Results include a comparison against soft constraint decoding, i.e., beam-based NeuroLogic (NL)
Generation QualityConstraint Satisfaction EfficiencyMethodROUGE-L BLEU-4 CIDEr SPICE CoverageSatisfiedTime(s)Prompt OnlyABS 0-shot25.3706.2904.3413.8151.6118.8401.56ABS 1-shot29.4608.4106.2218.4974.4935.3401.85ABS 2-shot31.3410.6007.3320.0676.7438.7401.83CNF 0-shot22.8203.7402.3712.1342.1711.0901.84CNF 1-shot29.6008.0705.8818.7777.4738.8801.51CNF 2-shot30.9309.9206.8119.2274.4834.3401.46Decoder OnlyNL [26], beam=810.0500.0000.0802.6702.4100.0003.46NL [26], beam=3210.3600.0000.0602.3101.1200.0012.47NL [26], beam=649.7400.2300.0402.5900.9600.0024.01SMC [21], particle=823.1002.6001.7115.37100.0100.022.92SMC [21], particle=1622.8602.5201.6215.55100.0100.022.96SMC [21], particle=3222.9202.6401.6915.26100.0100.023.17Prompt &amp; DecoderABS 0-shot + NL [26], beam=836.5414.8210.7220.6595.9383.4305.30ABS 1-shot + NL [26], beam=839.0719.2512.1323.2594.1376.5504.61ABS 2-shot + NL [26], beam=839.3919.7612.2623.6593.8175.4805.11CNF 0-shot + NL [26], beam=815.4103.9801.5606.4209.3801.0007.47CNF 1-shot + NL [26], beam=839.6625.7313.3024.1879.9139.0809.37CNF 2-shot + NL [26], beam=839.8125.3512.8423.5775.4927.9911.30ABS 0-shot + SMC [21], particle=825.8604.0002.7918.80100.0100.025.33ABS 1-shot + SMC [21], particle=827.8605.6604.4620.27100.0100.025.14ABS 2-shot + SMC [21], particle=828.6206.1704.9020.55100.0100.029.96CNF 0-shot + SMC [21], particle=826.2704.0703.1419.85100.0100.027.51CNF 1-shot + SMC [21], particle=827.4004.6503.8420.29100.0100.034.10CNF 2-shot + SMC [21], particle=828.4405.9304.5420.70100.0100.048.76</p>
<p>Foundations of Databases: The Logical Level. Serge Abiteboul, Richard Hull, Victor Vianu, 1994Addison-Wesley15</p>
<p>Spice: Semantic propositional image caption evaluation. Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould, Computer Vision-ECCV 2016: 14th European Conference. AmsterdamSpringerOctober 11-14, 2016. 2016Proceedings, Part V 14</p>
<p>Consistent query answers in inconsistent databases. Marcelo Arenas, Leopoldo E Bertossi, Jan Chomicki, PODS. 199915</p>
<p>Efficient discovery of ontology functional dependencies. Sridevi Baskaran, Alexander Keller, Fei Chiang, Lukasz Golab, Jaroslaw Szlichta, Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. the 2017 ACM on Conference on Information and Knowledge Management2017</p>
<p>Database repairs and consistent query answering: Origins and further developments. Leopoldo Bertossi, Proceedings of the 38th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS '19. the 38th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS '19New York, NY, USAAssociation for Computing Machinery201946</p>
<p>Prompting is programming: A query language for large language models. Luca Beurer-Kellner, Marc Fischer, Martin Vechev, Proceedings of the ACM on Programming Languages. 710jun 2023</p>
<p>I2d2: Inductive knowledge distillation with neurologic and self-imitation. Chandra Bhagavatula, Jena D Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Lianhui Qin, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, Yejin Choi, 2023</p>
<p>Ontology-based data access: A study through disjunctive datalog, csp, and mmsnp. Meghyn Bienvenu, Ten Balder, Carsten Cate, Frank Lutz, Wolter, ACM Trans. Database Syst. 3945dec 2015</p>
<p>Tractable approximations of consistent query answering for robust ontology-based data access. Meghyn Bienvenu, Riccardo Rosati, Twenty-Third International Joint Conference on Artificial Intelligence. 2013</p>
<p>Improving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las, Aurelia Casas, Jacob Guy, Roman Menick, Tom Ring, Saffron Hennigan, Loren Huang, Chris Maggiore, Albin Jones, Andy Cassirer, Michela Brock, Geoffrey Paganini, Irving, Oriol Vinyals. Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre92022</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, arXiv:2202.076462022arXiv preprint</p>
<p>Extracting training data from large language models. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, 30th USENIX Security Symposium (USENIX Security 21). 2021</p>
<p>Outlines: Generative model programming. Normal Computing, Remi Louf, 2023510</p>
<p>Foundations of Data Quality Management. Wenfei Fan, Floris Geerts, Synthesis Lectures on Data Management. 2012. 1, 5, 6Morgan &amp; Claypool Publishers</p>
<p>Quantum embedding of knowledge for reasoning. Dinesh Garg, Shajith Ikbal, K Santosh, Harit Srivastava, Hima Vishwakarma, Karanam, Subramaniam Venkata, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc2019323</p>
<p>Recipegpt: Generative pre-training based cooking recipe generation and evaluation system. Helena H Lee, Ke Shu, Palakorn Achananuparp, Kokoh Philips, Yue Prasetyo, Ee-Peng Liu, Lav R Lim, Varshney, Companion Proceedings of the Web Conference 2020, WWW '20. New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Box 2 el: Concept and role box embeddings for the description logic el++. Mathias Jackermeier, Jiaoyan Chen, Ian Horrocks, 202323</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 5512mar 2023. 1</p>
<p>Markus Krotzsch, arXiv:1201.4089Frantisek Simancik, and Ian Horrocks. A description logic primer. 2012arXiv preprint</p>
<p>El embeddings: Geometric construction of models for the description logic el++. Maxat Kulmanov, Wang Liu-Wei, Yuan Yan, Robert Hoehndorf, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19201973</p>
<p>Sequential monte carlo steering of large language models using probabilistic programs. Alexander K Lew, Tan Zhi-Xuan, Gabriel Grand, Vikash K Mansinghka, 2023. 1, 2, 5, 7, 8</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021arXiv preprint</p>
<p>Commongen: A constrained text generation challenge for generative commonsense reasoning. Wangchunshu Bill Yuchen Lin, Ming Zhou, Pei Shen, Chandra Zhou, Yejin Bhagavatula, Xiang Choi, Ren, arXiv:1911.03705201967arXiv preprint</p>
<p>Automatic evaluation of summaries using n-gram co-occurrence statistics. Chin-Yew Lin, Eduard Hovy, Proceedings of the 2003 human language technology conference of the North American chapter of the association for computational linguistics. the 2003 human language technology conference of the North American chapter of the association for computational linguistics2003</p>
<p>Relational memory augmented language models. Qi Liu, Dani Yogatama, Phil Blunsom, 202269</p>
<p>Neu-roLogic decoding: (un)supervised neural text generation with predicate logic constraints. Ximing Lu, Peter West, Rowan Zellers, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1, 2, 5, 7, 8, 9OnlineJune 2021</p>
<p>Self-refine: Iterative refinement with selffeedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, 2023</p>
<p>Mass-editing memory in a transformer. Kevin Meng, Sen Arnab, Alex Sharma, Yonatan Andonian, David Belinkov, Bau, ICLR, 2023. 4</p>
<p>. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, 2023Augmented language models: a survey</p>
<p>Guidance: A guidance language for controlling large language models. Scott Microsoft, Lundberg, 510</p>
<p>. NIH. Medical ontology research. 22023</p>
<p>. OpenAI. Gpt-4 technical report. 12023</p>
<p>Functional dependency discovery: An experimental evaluation of seven algorithms. Thorsten Papenbrock, Jens Ehrlich, Jannik Marten, Tommy Neubert, Jan-Peer Rudolph, Martin Schönberg, Jakob Zwiener, Felix Naumann, Proceedings of the VLDB Endowment. the VLDB Endowment20158</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Saikat Md Rizwan Parvez, Baishakhi Chakraborty, Kai-Wei Ray, Chang, arXiv:1805.04836Building language models for text with named entities. 2018arXiv preprint</p>
<p>Relation extraction : A survey. Sachin Pawar, K Girish, Pushpak Palshikar, Bhattacharyya, 2017</p>
<p>Learning over dirty data without cleaning. Jose Picado, John Davis, Arash Termehchy, Ga Young, Lee , Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, SIG-MOD '20. the 2020 ACM SIGMOD International Conference on Management of Data, SIG-MOD '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Learning over dirty data without cleaning. Jose Picado, John Davis, Arash Termehchy, Ga Young, Lee , Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. the 2020 ACM SIGMOD International Conference on Management of Data2020</p>
<p>Synchromesh: Reliable code generation from pre-trained language models. Gabriel Poesia, Oleksandr Polozov, Ashish Vu Le, Gustavo Tiwari, Christopher Soares, Sumit Meek, Gulwani, arXiv:2201.112272022arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>HoloClean: Holistic data repairs with probabilistic inference. I Theodoros, Xu Rekatsinas, Ihab F Chu, Christopher Ilyas, Ré, PVLDB. 102017. 1, 5, 6</p>
<p>Query2box: Reasoning over knowledge graphs in vector space using box embeddings. Weihua Hongyu Ren, Jure Hu, Leskovec, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. 202020203OpenReview.net</p>
<p>Beta embeddings for multi-hop logical reasoning in knowledge graphs. Hongyu Ren, Jure Leskovec, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20. the 34th International Conference on Neural Information Processing Systems, NIPS'20Red Hook, NY, USACurran Associates Inc2020</p>
<p>Querying large language models with sql. Mohammed Saeed, Nicola De Cao, Paolo Papotti, 2023</p>
<p>Mukul Singh, José Cambronero, Sumit Gulwani, Carina Vu Le, Negreanu, arXiv:2310.17306Elnaz Nouri, Mohammad Raza, and Gust Verbruggen. Format5: Abstention and examples for conditional table formatting with natural language. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and finetuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288202367arXiv preprint</p>
<p>Consensus-based image description evaluation. Lawrence Vedantam, Zitnick, Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition8</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Condensed representation of database repairs for consistent query answering. Jef Wijsen, Proceedings of the 9th International Conference on Database Theory. the 9th International Conference on Database TheoryBerlin, HeidelbergSpringer-Verlag2003393</p>
<p>Bo Xiong, Mojtaba Nayyeri, Ming Jin, Yunjie He, Michael Cochez, Shirui Pan, Steffen Staab, arXiv:2304.11949Geometric relational embeddings: A survey. 2023arXiv preprint</p>
<p>Mohamed Yakout, Ahmed K Elmagarmid, Jennifer Neville, Mourad Ouzzani, Ihab F Ilyas, Guided data repair. Proc. VLDB Endow. Feb. 2011. 1, 5, 64</p>
<p>Nanyun Peng, and Guy Van den Broeck. Tractable control for autoregressive language generation. Honghua Zhang, Meihua Dang, Proceedings of the 40th International Conference on Machine Learning (ICML). the 40th International Conference on Machine Learning (ICML)july 2023. 5</p>
<p>When can we ignore missing data in model training?. Amandeep Cheng Zhen, Arash Singh Chabada, Termehchy, Proceedings of the Seventh Workshop on Data Management for End-to-End Machine Learning, DEEM 2023. the Seventh Workshop on Data Management for End-to-End Machine Learning, DEEM 2023Seattle, WA, USAACM18 June 2023. 20234</p>
<p>A survey on neural open information extraction: Current status and future directions. Shaowen Zhou, Bowen Yu, Aixin Sun, Cheng Long, Jingyang Li, Haiyang Yu, Jian Sun, Yongbin Li, 2022</p>            </div>
        </div>

    </div>
</body>
</html>
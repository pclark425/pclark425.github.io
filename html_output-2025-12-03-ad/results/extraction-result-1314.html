<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1314 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1314</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1314</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-221082834</p>
                <p><strong>Paper Title:</strong> Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?</p>
                <p><strong>Paper Abstract:</strong> Does progress in simulation translate to progress on robots? If one method outperforms another in simulation, how likely is that trend to hold in reality on a robot? We examine this question for embodied PointGoal navigation – developing engineering tools and a research paradigm for evaluating a simulator by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy), a library for seamless execution of identical code on simulated agents and robots – transferring simulation-trained agents to a LoCoBot platform with a one-line code change. Second, we investigate the sim2real predictivity of Habitat-Sim M. Savva et al., for PointGoal navigation. We 3D-scan a physical lab space to create a virtualized replica, and run parallel tests of 9 different models in reality and simulation. We present a new metric called Sim-vs-Real Correlation Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as used for the CVPR19 challenge is low (0.18 for the success metric), suggesting that performance differences in this simulator-based challenge do not persist after physical deployment. This gap is largely due to AI agents learning to exploit simulator imperfections – abusing collision dynamics to ‘slide’ along walls, leading to shortcuts through otherwise non-navigable space. Naturally, such exploits do not work in the real world. Our experiments show that it is possible to tune simulation parameters to improve sim2real predictivity (e.g. improving SRCC$_{\text{Succ}}$ from 0.18 to 0.844) – increasing confidence that in-simulation comparisons will translate to deployed systems in reality.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1314.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1314.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Habitat-Sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Habitat-Sim (part of the Habitat platform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performance photorealistic 3D simulator for embodied AI research used for training and evaluating visual navigation agents; supports importing scanned 3D environments and configurable physics/actuation models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Habitat-Sim</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Photorealistic 3D indoor environment simulator (part of Habitat) that can import reconstructed meshes (e.g. Matterport/Matterport Pro2 scans), render RGB and depth observations, expose GPS+Compass sensors, and provide a discrete action interface (turn/forward/STOP).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied navigation / robotics (visual navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high visual fidelity, medium/approximate physical fidelity (photorealistic rendering; approximate physics rather than full real-world dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>High-fidelity visual rendering from scanned meshes; discrete action semantics (deterministic by default); configurable actuation noise model (can sample Gaussian displacement noise); depth sensor range/clipping (e.g. clipped to 10 m); collision handling that by default allows 'sliding' along obstacles (an approximation of contact physics). Does not simulate perfect contact/bump-stop behavior unless configured; physics approximations can enable non-physical shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DD-PPO PointGoal navigation agents (variants: RGB, Depth, RGB→PredictedDepth)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement-learning agents trained with DD-PPO; visual encoder (ResNet-50) + policy (2-layer LSTM, 512-dim state); trained from scratch on Gibson environments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>PointGoal visual navigation: navigate from spawn to a relative goal location specified by GPS+Compass (r, θ) using egocentric RGB or depth inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Agents trained for 500M steps on 64 V100 GPUs; in many simulation settings success rates in-sim were near 1.0 (exact numeric training curves not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world LoCoBot platform in a scanned lab (CODA) via the HaPy bridge</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Raw sim-vs-real predictivity quantified by SRCC: under Habitat Challenge 2019 default settings SRCC_SPL = 0.603 and SRCC_Succ = 0.18 (low predictivity); after tuning simulation parameters (disable sliding, actuation noise multiplier 0.0) SRCC_SPL improved to 0.875 and SRCC_Succ to 0.844, indicating strong alignment between sim and real performance.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Grid search over 'sliding' (on/off) and actuation-noise multiplier (0.0–1.0) showed that disabling sliding and using 0.0 multiplier on the chosen actuation noise model produced the highest sim2real predictivity. Sliding-on simulations produced optimistic in-sim advantages that did not transfer; disabling sliding removed the exploit and improved SRCC dramatically.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue simulator need not be a perfect replica but must be predictive: accurate collision behavior (no unrealistic sliding) and appropriately matched actuation noise are necessary for predictive evaluation. They found that disabling sliding (so collisions stop the agent) was necessary to prevent learned agents exploiting simulator artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Sliding-on collision model enabled learned agents to 'slide' around obstacles and take effective straight-line shortcuts measured by Euclidean distance (used in SPL), producing high in-sim success/SPL that failed in reality where robots stop on contact; actuation-noise model mismatch also affected predictivity (optimal grid-search found 0.0 multiplier, suggesting the used noise model did not reflect reality).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1314.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1314.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Habitat-PyRobot Bridge (HaPy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Habitat-PyRobot Bridge (HaPy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A software bridge that makes it trivial to execute identical embodied agent code in Habitat simulation and on PyRobot-enabled physical robots (LoCoBot) by swapping simulator backends with a single config change.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>HaPy (switches between Habitat-Sim-v0 and PyRobot-Locobot-v0)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Not a simulator per se but an integration layer: unifies observation and action APIs between Habitat-Sim and real robot interfaces exposed by PyRobot, enabling identical agent code to run in simulation or reality.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied navigation / robotics (enables sim2real experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>NA (bridge/interface rather than physics simulator); enables use of real-world sensor/actuator fidelity via PyRobot/LoCoBot when targeting reality.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides uniform observation preprocessing (resizing, normalization), unified discrete action abstraction across sim and robot, and allows swapping the simulator backend to the real robot (PyRobot-LoCoBot) with minimal code changes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Same DD-PPO PointGoal agents trained in Habitat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same reinforcement-learning agent architectures as in simulation; HaPy ensures inputs/actions are consistent across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Run/evaluate PointGoal navigation policies in both simulation and reality without code changes.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real LoCoBot robot via PyRobot API (enables direct evaluation of simulation-trained agents on hardware)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Enables transfer evaluations reported in the paper (parallel sim vs real experiments); specific per-model transferred performance reported elsewhere in paper (see Habitat-Sim entry and SRCC numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>HaPy facilitates isolating simulation fidelity factors by enabling identical code across sim/real; authors emphasize this reduces software-stack inconsistencies that could confound sim2real evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1314.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1314.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gibson (env / dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gibson environment / dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of real scanned indoor spaces and an environment suite used for training visual navigation agents; used as the training distribution for the DD-PPO agents in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gibson (training scenes imported into Habitat)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Collection of real-world scanned indoor spaces (apartments, houses, offices) whose meshes/environments are used as training domains for simulators like Habitat; provides realistic layout/visuals but typically uses approximate physics in the simulator layer.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied navigation / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high visual fidelity for environment geometry/appearance (real scans); physical interactions remain simulated/approximate depending on simulator (here used via Habitat).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Real-scanned geometry and textures; agents trained on Gibson scenes inside Habitat; physics and action execution fidelity depend on Habitat settings (see Habitat-Sim entry).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DD-PPO PointGoal agents trained on Gibson training splits</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agents (ResNet50 encoder + 2-layer LSTM) trained end-to-end with DD-PPO on Gibson scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>PointGoal navigation training (generalization to unseen scenes tested in Gibson-val and then transferred to CODA real lab).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Model selection used Gibson-val performance; in some sim settings model achieved higher SPL (e.g. model outperforming a wall-following oracle on Gibson-val: model 0.59 SPL vs oracle 0.46 SPL under a particular sim setting).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>CODA real lab via Habitat->HaPy->LoCoBot pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Transfer quality depends on simulation test settings; naive Habitat-Challenge settings had low SRCC to reality, but tuning (disable sliding, adjust noise) improved predictive transfer (see Habitat-Sim entry).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Using real scanned environments reduces semantic/visual domain gaps, but physical-interaction fidelity (collision response, actuation noise) remains critical for predictive transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Even when using scanned Gibson environments, agents exploited simulator-specific collision dynamics (sliding) producing policies that failed on the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1314.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1314.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MINOS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MINOS (Multimodal INdoor Simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An indoor navigation simulator referenced as an example where sliding-on-collision behavior is enabled by default; cited in relation to simulators whose contact approximations can be exploited by learned agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MINOS</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Multimodal indoor navigation simulator (cited) that supports navigation research; in the paper MINOS is mentioned as an example where sliding-on-collision behavior is enabled by default.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied navigation / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>approximate physics typical of fast navigation simulators; visual fidelity and physics approximations not deeply detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Paper notes MINOS (like several other simulators) permits sliding along obstacles on collision by default — an approximation aiding smooth control but not reflective of real robot bump-stop behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Mentioned as an example supporting the claim that many simulators adopt sliding/collision approximations; authors highlight that such features can undermine sim2real predictivity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Sliding-on-collision is called out as a simulator artifact that can be exploited by agents and cause transfer failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1314.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1314.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI2-THOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI2-THOR (Interactive 3D environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D interactive environment for visual AI cited as another simulator where sliding/approximate collision behavior is prevalent; mentioned in the context of simulator imperfections exploited by agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>AI2-THOR</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Interactive 3D environment for visual AI research; provides objects, interactions and physics approximations for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied navigation / robotics / interactive environments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>photoreal/interactive visuals; approximate physics and contact modeled for interactive gameplay and agent control rather than exact physical realism.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Paper states AI2-THOR (and other simulators) exhibit sliding-on-collision behavior due to approximate physics/engine settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Used as a cited example for simulation artifacts; no direct experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Listed among simulators where collision approximations allow non-physical agent behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1314.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1314.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepMind Lab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepMind Lab</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A research platform for 3D reinforcement learning cited as another environment where sliding-on-collision occurs; mentioned in the discussion of physics approximations in simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>DeepMind Lab</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A 3D environment used for reinforcement learning research; known to prioritize fast, controllable interactions over perfect physical realism.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>reinforcement learning / embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-to-medium physical fidelity (focus on fast RL research), medium visual fidelity depending on scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Paper cites DeepMind Lab as another platform where sliding on collision is enabled by default; physics are approximated to favor smooth control.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Imperfect collision dynamics cited as potential source of sim2real mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1314.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1314.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open urban driving simulator cited in related work as an example of increasingly realistic simulators used for control and navigation research; mentioned but not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Driving simulator focused on urban autonomy research; provides photorealistic rendering and simulated vehicle dynamics for autonomous driving experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>autonomous driving / vehicle control</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high visual fidelity; approximate vehicle dynamics and environment physics tuned for driving research.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Not experimented with in this paper; cited as an example of domain-specific high-fidelity simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1314.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1314.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAD2RL (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CAD2RL: real single-image flight without a single real image</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work cited as a successful demonstration of training entirely in simulation and deploying on aerial drones in reality; referenced in related work on sim2real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CAD2RL: real single-image flight without a single real image</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>CAD2RL (system trained in simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>System that trains perception/policy modules in synthetic rendered environments (CAD-based) and transfers to real drones; not run in this paper but cited as evidence of successful sim2real.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>aerial robotics / collision avoidance / control</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>varies by rendering and physics used in CAD-based sim; typically approximated physics with domain randomization used to improve transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Paper cites CAD2RL as an example of sim-only training transferred to real drones; specifics not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Habitat: A platform for embodied AI research <em>(Rating: 2)</em></li>
                <li>DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames <em>(Rating: 2)</em></li>
                <li>Gibson env: Real-world perception for embodied agents <em>(Rating: 2)</em></li>
                <li>MINOS: Multimodal indoor simulator for navigation in complex environments <em>(Rating: 2)</em></li>
                <li>AI2-THOR: An interactive 3D environment for visual AI <em>(Rating: 2)</em></li>
                <li>CAD2RL: real single-image flight without a single real image <em>(Rating: 1)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 1)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1314",
    "paper_id": "paper-221082834",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Habitat-Sim",
            "name_full": "Habitat-Sim (part of the Habitat platform)",
            "brief_description": "A high-performance photorealistic 3D simulator for embodied AI research used for training and evaluating visual navigation agents; supports importing scanned 3D environments and configurable physics/actuation models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Habitat-Sim",
            "simulator_description": "Photorealistic 3D indoor environment simulator (part of Habitat) that can import reconstructed meshes (e.g. Matterport/Matterport Pro2 scans), render RGB and depth observations, expose GPS+Compass sensors, and provide a discrete action interface (turn/forward/STOP).",
            "scientific_domain": "embodied navigation / robotics (visual navigation)",
            "fidelity_level": "medium-to-high visual fidelity, medium/approximate physical fidelity (photorealistic rendering; approximate physics rather than full real-world dynamics).",
            "fidelity_characteristics": "High-fidelity visual rendering from scanned meshes; discrete action semantics (deterministic by default); configurable actuation noise model (can sample Gaussian displacement noise); depth sensor range/clipping (e.g. clipped to 10 m); collision handling that by default allows 'sliding' along obstacles (an approximation of contact physics). Does not simulate perfect contact/bump-stop behavior unless configured; physics approximations can enable non-physical shortcuts.",
            "model_or_agent_name": "DD-PPO PointGoal navigation agents (variants: RGB, Depth, RGB→PredictedDepth)",
            "model_description": "Reinforcement-learning agents trained with DD-PPO; visual encoder (ResNet-50) + policy (2-layer LSTM, 512-dim state); trained from scratch on Gibson environments.",
            "reasoning_task": "PointGoal visual navigation: navigate from spawn to a relative goal location specified by GPS+Compass (r, θ) using egocentric RGB or depth inputs.",
            "training_performance": "Agents trained for 500M steps on 64 V100 GPUs; in many simulation settings success rates in-sim were near 1.0 (exact numeric training curves not provided here).",
            "transfer_target": "Real-world LoCoBot platform in a scanned lab (CODA) via the HaPy bridge",
            "transfer_performance": "Raw sim-vs-real predictivity quantified by SRCC: under Habitat Challenge 2019 default settings SRCC_SPL = 0.603 and SRCC_Succ = 0.18 (low predictivity); after tuning simulation parameters (disable sliding, actuation noise multiplier 0.0) SRCC_SPL improved to 0.875 and SRCC_Succ to 0.844, indicating strong alignment between sim and real performance.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Grid search over 'sliding' (on/off) and actuation-noise multiplier (0.0–1.0) showed that disabling sliding and using 0.0 multiplier on the chosen actuation noise model produced the highest sim2real predictivity. Sliding-on simulations produced optimistic in-sim advantages that did not transfer; disabling sliding removed the exploit and improved SRCC dramatically.",
            "minimal_fidelity_discussion": "Authors argue simulator need not be a perfect replica but must be predictive: accurate collision behavior (no unrealistic sliding) and appropriately matched actuation noise are necessary for predictive evaluation. They found that disabling sliding (so collisions stop the agent) was necessary to prevent learned agents exploiting simulator artifacts.",
            "failure_cases": "Sliding-on collision model enabled learned agents to 'slide' around obstacles and take effective straight-line shortcuts measured by Euclidean distance (used in SPL), producing high in-sim success/SPL that failed in reality where robots stop on contact; actuation-noise model mismatch also affected predictivity (optimal grid-search found 0.0 multiplier, suggesting the used noise model did not reflect reality).",
            "uuid": "e1314.0",
            "source_info": {
                "paper_title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Habitat-PyRobot Bridge (HaPy)",
            "name_full": "Habitat-PyRobot Bridge (HaPy)",
            "brief_description": "A software bridge that makes it trivial to execute identical embodied agent code in Habitat simulation and on PyRobot-enabled physical robots (LoCoBot) by swapping simulator backends with a single config change.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "HaPy (switches between Habitat-Sim-v0 and PyRobot-Locobot-v0)",
            "simulator_description": "Not a simulator per se but an integration layer: unifies observation and action APIs between Habitat-Sim and real robot interfaces exposed by PyRobot, enabling identical agent code to run in simulation or reality.",
            "scientific_domain": "embodied navigation / robotics (enables sim2real experiments)",
            "fidelity_level": "NA (bridge/interface rather than physics simulator); enables use of real-world sensor/actuator fidelity via PyRobot/LoCoBot when targeting reality.",
            "fidelity_characteristics": "Provides uniform observation preprocessing (resizing, normalization), unified discrete action abstraction across sim and robot, and allows swapping the simulator backend to the real robot (PyRobot-LoCoBot) with minimal code changes.",
            "model_or_agent_name": "Same DD-PPO PointGoal agents trained in Habitat",
            "model_description": "Same reinforcement-learning agent architectures as in simulation; HaPy ensures inputs/actions are consistent across domains.",
            "reasoning_task": "Run/evaluate PointGoal navigation policies in both simulation and reality without code changes.",
            "training_performance": null,
            "transfer_target": "Real LoCoBot robot via PyRobot API (enables direct evaluation of simulation-trained agents on hardware)",
            "transfer_performance": "Enables transfer evaluations reported in the paper (parallel sim vs real experiments); specific per-model transferred performance reported elsewhere in paper (see Habitat-Sim entry and SRCC numbers).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "HaPy facilitates isolating simulation fidelity factors by enabling identical code across sim/real; authors emphasize this reduces software-stack inconsistencies that could confound sim2real evaluation.",
            "failure_cases": null,
            "uuid": "e1314.1",
            "source_info": {
                "paper_title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Gibson (env / dataset)",
            "name_full": "Gibson environment / dataset",
            "brief_description": "A dataset of real scanned indoor spaces and an environment suite used for training visual navigation agents; used as the training distribution for the DD-PPO agents in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Gibson (training scenes imported into Habitat)",
            "simulator_description": "Collection of real-world scanned indoor spaces (apartments, houses, offices) whose meshes/environments are used as training domains for simulators like Habitat; provides realistic layout/visuals but typically uses approximate physics in the simulator layer.",
            "scientific_domain": "embodied navigation / robotics",
            "fidelity_level": "high visual fidelity for environment geometry/appearance (real scans); physical interactions remain simulated/approximate depending on simulator (here used via Habitat).",
            "fidelity_characteristics": "Real-scanned geometry and textures; agents trained on Gibson scenes inside Habitat; physics and action execution fidelity depend on Habitat settings (see Habitat-Sim entry).",
            "model_or_agent_name": "DD-PPO PointGoal agents trained on Gibson training splits",
            "model_description": "RL agents (ResNet50 encoder + 2-layer LSTM) trained end-to-end with DD-PPO on Gibson scenes.",
            "reasoning_task": "PointGoal navigation training (generalization to unseen scenes tested in Gibson-val and then transferred to CODA real lab).",
            "training_performance": "Model selection used Gibson-val performance; in some sim settings model achieved higher SPL (e.g. model outperforming a wall-following oracle on Gibson-val: model 0.59 SPL vs oracle 0.46 SPL under a particular sim setting).",
            "transfer_target": "CODA real lab via Habitat-&gt;HaPy-&gt;LoCoBot pipeline",
            "transfer_performance": "Transfer quality depends on simulation test settings; naive Habitat-Challenge settings had low SRCC to reality, but tuning (disable sliding, adjust noise) improved predictive transfer (see Habitat-Sim entry).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Using real scanned environments reduces semantic/visual domain gaps, but physical-interaction fidelity (collision response, actuation noise) remains critical for predictive transfer.",
            "failure_cases": "Even when using scanned Gibson environments, agents exploited simulator-specific collision dynamics (sliding) producing policies that failed on the real robot.",
            "uuid": "e1314.2",
            "source_info": {
                "paper_title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "MINOS",
            "name_full": "MINOS (Multimodal INdoor Simulator)",
            "brief_description": "An indoor navigation simulator referenced as an example where sliding-on-collision behavior is enabled by default; cited in relation to simulators whose contact approximations can be exploited by learned agents.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "MINOS",
            "simulator_description": "Multimodal indoor navigation simulator (cited) that supports navigation research; in the paper MINOS is mentioned as an example where sliding-on-collision behavior is enabled by default.",
            "scientific_domain": "embodied navigation / robotics",
            "fidelity_level": "approximate physics typical of fast navigation simulators; visual fidelity and physics approximations not deeply detailed in this paper.",
            "fidelity_characteristics": "Paper notes MINOS (like several other simulators) permits sliding along obstacles on collision by default — an approximation aiding smooth control but not reflective of real robot bump-stop behavior.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Mentioned as an example supporting the claim that many simulators adopt sliding/collision approximations; authors highlight that such features can undermine sim2real predictivity.",
            "failure_cases": "Sliding-on-collision is called out as a simulator artifact that can be exploited by agents and cause transfer failures.",
            "uuid": "e1314.3",
            "source_info": {
                "paper_title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "AI2-THOR",
            "name_full": "AI2-THOR (Interactive 3D environment)",
            "brief_description": "A 3D interactive environment for visual AI cited as another simulator where sliding/approximate collision behavior is prevalent; mentioned in the context of simulator imperfections exploited by agents.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "AI2-THOR",
            "simulator_description": "Interactive 3D environment for visual AI research; provides objects, interactions and physics approximations for embodied tasks.",
            "scientific_domain": "embodied navigation / robotics / interactive environments",
            "fidelity_level": "photoreal/interactive visuals; approximate physics and contact modeled for interactive gameplay and agent control rather than exact physical realism.",
            "fidelity_characteristics": "Paper states AI2-THOR (and other simulators) exhibit sliding-on-collision behavior due to approximate physics/engine settings.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Used as a cited example for simulation artifacts; no direct experiments in this paper.",
            "failure_cases": "Listed among simulators where collision approximations allow non-physical agent behaviors.",
            "uuid": "e1314.4",
            "source_info": {
                "paper_title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "DeepMind Lab",
            "name_full": "DeepMind Lab",
            "brief_description": "A research platform for 3D reinforcement learning cited as another environment where sliding-on-collision occurs; mentioned in the discussion of physics approximations in simulators.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "DeepMind Lab",
            "simulator_description": "A 3D environment used for reinforcement learning research; known to prioritize fast, controllable interactions over perfect physical realism.",
            "scientific_domain": "reinforcement learning / embodied agents",
            "fidelity_level": "low-to-medium physical fidelity (focus on fast RL research), medium visual fidelity depending on scenarios.",
            "fidelity_characteristics": "Paper cites DeepMind Lab as another platform where sliding on collision is enabled by default; physics are approximated to favor smooth control.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": null,
            "failure_cases": "Imperfect collision dynamics cited as potential source of sim2real mismatch.",
            "uuid": "e1314.5",
            "source_info": {
                "paper_title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "CARLA",
            "name_full": "CARLA",
            "brief_description": "An open urban driving simulator cited in related work as an example of increasingly realistic simulators used for control and navigation research; mentioned but not used in experiments here.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "CARLA",
            "simulator_description": "Driving simulator focused on urban autonomy research; provides photorealistic rendering and simulated vehicle dynamics for autonomous driving experiments.",
            "scientific_domain": "autonomous driving / vehicle control",
            "fidelity_level": "medium-to-high visual fidelity; approximate vehicle dynamics and environment physics tuned for driving research.",
            "fidelity_characteristics": "Not experimented with in this paper; cited as an example of domain-specific high-fidelity simulators.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": null,
            "failure_cases": null,
            "uuid": "e1314.6",
            "source_info": {
                "paper_title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "CAD2RL (cited)",
            "name_full": "CAD2RL: real single-image flight without a single real image",
            "brief_description": "A prior work cited as a successful demonstration of training entirely in simulation and deploying on aerial drones in reality; referenced in related work on sim2real transfer.",
            "citation_title": "CAD2RL: real single-image flight without a single real image",
            "mention_or_use": "mention",
            "simulator_name": "CAD2RL (system trained in simulation)",
            "simulator_description": "System that trains perception/policy modules in synthetic rendered environments (CAD-based) and transfers to real drones; not run in this paper but cited as evidence of successful sim2real.",
            "scientific_domain": "aerial robotics / collision avoidance / control",
            "fidelity_level": "varies by rendering and physics used in CAD-based sim; typically approximated physics with domain randomization used to improve transfer.",
            "fidelity_characteristics": "Paper cites CAD2RL as an example of sim-only training transferred to real drones; specifics not evaluated here.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": null,
            "failure_cases": null,
            "uuid": "e1314.7",
            "source_info": {
                "paper_title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Habitat: A platform for embodied AI research",
            "rating": 2,
            "sanitized_title": "habitat_a_platform_for_embodied_ai_research"
        },
        {
            "paper_title": "DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames",
            "rating": 2,
            "sanitized_title": "ddppo_learning_nearperfect_pointgoal_navigators_from_25_billion_frames"
        },
        {
            "paper_title": "Gibson env: Real-world perception for embodied agents",
            "rating": 2,
            "sanitized_title": "gibson_env_realworld_perception_for_embodied_agents"
        },
        {
            "paper_title": "MINOS: Multimodal indoor simulator for navigation in complex environments",
            "rating": 2,
            "sanitized_title": "minos_multimodal_indoor_simulator_for_navigation_in_complex_environments"
        },
        {
            "paper_title": "AI2-THOR: An interactive 3D environment for visual AI",
            "rating": 2,
            "sanitized_title": "ai2thor_an_interactive_3d_environment_for_visual_ai"
        },
        {
            "paper_title": "CAD2RL: real single-image flight without a single real image",
            "rating": 1,
            "sanitized_title": "cad2rl_real_singleimage_flight_without_a_single_real_image"
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 1,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 1,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        }
    ],
    "cost": 0.015275999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?
OCTOBER 2020</p>
<p>Abhishek Kadian 
Joanne Truong 
Aaron Gokaslan 
Alexander Clegg 
Erik Wijmans 
Stefan Lee 
Manolis Savva 
Sonia Chernova 
Dhruv Batra 
Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?</p>
<p>IEEE ROBOTICS AND AUTOMATION LETTERS
54OCTOBER 202010.1109/LRA.2020.30138486670
Does progress in simulation translate to progress on robots? If one method outperforms another in simulation, how likely is that trend to hold in reality on a robot? We examine this question for embodied PointGoal navigation -developing engineering tools and a research paradigm for evaluating a simulator by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy), a library for seamless execution of identical code on simulated agents and robots -transferring simulation-trained agents to a LoCoBot platform with a one-line code change. Second, we investigate the sim2real predictivity of Habitat-Sim M. Savva et al., for PointGoal navigation. We 3D-scan a physical lab space to create a virtualized replica, and run parallel tests of 9 different models in reality and simulation. We present a new metric called Sim-vs-Real Correlation Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as used for the CVPR19 challenge is low (0.18 for the success metric), suggesting that performance differences in this simulator-based challenge do not persist after physical deployment. This gap is largely due to AI agents learning to exploit simulator imperfections -abusing collision dynamics to 'slide' along walls , leading to shortcuts through otherwise non-navigable space. Naturally, such exploits do not work in the real world. Our experiments show that it is possible to tune simulation parameters to improve sim2real predictivity (e.g. improving SRCC Succ from 0.18 to 0.844) -increasing confidence that in-simulation comparisons will translate to deployed systems in reality.Index Terms-Visual-based navigation, reinforcement learning.</p>
<p>T HE VISION, language, and learning communities have recently witnessed a resurgence of interest in studying integrative robot-inspired agents that perceive, navigate, and interact with their environment. For a variety of reasons, such work has commonly been carried out in simulation rather than in real-world environments. Simulators can run orders of magnitude faster than real-time [1], can be highly parallelized, and enable decades of agent experience to be collected in days [2]. Moreover, evaluating agents in simulation is safer, cheaper, and enables easier benchmarking of scientific progress than running robots in the real-world. Consequentially, these communities have rallied around simulation as a testbed -developing several increasingly realistic indoor/outdoor navigation simulators [1], [3]- [9], designing a variety of tasks set in them [3], [10], [11], holding workshops about such platforms [12], and even running challenges in these simulated worlds [13]- [15]. As a result, significant progress has been made in these settings. For example, agents can reach point goals in novel home environments with near-perfect efficiency [2], control vehicles in complex, dynamic city environments [4], follow natural-language instructions [10], and answer questions [11].</p>
<p>However, no simulation is a perfect replica of reality, and AI systems are known to exploit imperfections and biases to achieve strong performance in simulation which may be unrepresentative of performance in reality. Notable examples include evolving tall creatures for locomotion that fall and somersault instead of learning active locomotion strategies [16] and OpenAI's hide-and-seek agents abusing their physics engine to 'surf' on top of obstacles [17].</p>
<p>This raises several fundamental questions of deep interest to the scientific and engineering communities: Do comparisons drawn from simulation translate to reality for robotic systems? Concretely, if one method outperforms another in simulation, will it continue to do so when deployed on a robot? Should we trust the outcomes of embodied AI challenges (e.g. the AI Habitat Challenge at CVPR 2019) that are performed entirely in simulation? These are questions not only of simulator fidelity, but rather of predictivity.</p>
<p>In this work, we examine the above questions in the context of visual navigation -focusing on measuring and optimizing the predictivity of a simulator. High predictivity enables researchers to use simulation for evaluation with confidence that the performance of different models will generalize to real robots. Given this focus, our efforts are orthogonal to techniques for sim2real transfer, including those based on adjusting simulator parameters. To answer these questions, we introduce engineering tools and a research paradigm for performing simulation-to-reality (sim2real) indoor navigation studies, revealing surprising findings about prior work.</p>
<p>First, we develop the Habitat-PyRobot Bridge (HaPy), a software library that enables seamless sim2robot transfer. HaPy is an interface between (1) Habitat [1], a high-performance photorealistic 3D simulator, and (2) PyRobot [18], a high-level python library for robotics research. Crucially, HaPy makes it trivial to execute identical code in simulation and reality. Sim2robot transfer with HaPy involves only a single line edit to the code (changing the config.simulator variable from Habitat-Sim-v0 to PyRobot-Locobot-v0), essentially treating reality as just another simulator! This reduces code duplication, provides an intuitive high-level abstraction, and allows for rapid prototyping with modularity (training a large number of models in simulation and 'tossing them over' for testing on the robot). In fact, all experiments in this article were conducted by a team of researchers physically separated by thousands of miles -one set training and testing models in simulation, another conducting on-site tests with the robot, made trivial due to HaPy. We will open-source HaPy so that everyone has this ability.</p>
<p>Second, we propose a general experimental paradigm for performing sim2real studies, which we call sim2real predictivity. Our thesis is that simulators need not be a perfect replica of reality to be useful. Specifically, we should primarily judge simulators not by their visual or physical realism, but by their sim2real predictivity -if method A outperforms B in simulation, how likely is the trend to hold in reality? To answer this question, we propose the use of a quantity we call Sim2Real Correlation Coefficient (SRCC).</p>
<p>We prepare a real lab space within which the robot must navigate while avoiding obstacles. We then virtualize this lab space (under different obstacle configurations) by 3D scanning the space and importing it in Habitat. Armed with the power to perform parallel trials in reality and simulation, we test a suite of navigation models both in simulation and in the lab with a real robot. We then produce a scatter plot where every point is a navigation model, the x-axis is the performance in simulation, and the y-axis is performance in reality. SRCC is shown in a box at the top. If SRCC is high (close to 1), this is a 'good' simulator setting in the sense that we can conduct scientific development and testing purely in simulation, with confidence that we are making 'real' progress because the improvements in simulation will generalize to real robotic testbeds. If SRCC is low (close to 0), this is a 'poor' simulator, and we should have no confidence in results reported solely in simulation. We apply this methodology in the context of PointGoal Navigation (PointNav) [19] with Habitat and the LoCoBot robot [20] as our simulation and reality platforms -our experiments made easy with HaPy. These experiments reveal a number of surprising findings:</p>
<p>1) We find that SRCC for Habitat as used for the CVPR19 challenge is 0.603 for the Success weighted by (normalized inverse) Path Length (SPL) metric and 0.18 for agent success. When ranked by SPL, we observe 9 relative ordering reversals from simulation to reality, suggesting that the results/winners may not be the same if the challenge were run on LoCoBot. 2) We find that large-scale RL trained models can learn to 'cheat' by exploiting the way Habitat allows for 'sliding' along walls on collision. Essentially, the virtual robot is capable of cutting corners by sliding around obstacles, leading to unrealistic shortcuts through parts of nonnavigable space and 'better than optimal' paths. Naturally, such exploits do not work in the real world where the robot stops on contact with walls. 3) We optimize SRCC over Habitat design parameters and find that a few simple changes improve SRCC SPL from 0.603 to 0.875 and SRCC Succ from 0.18 to 0.844. The number of rank reversals nearly halves to 5 (13.8%). Furthermore, we identify highly-performant agents in both this new simulation and on LoCoBot in real environments. While our experiments are conducted on the PointNav task, we believe our software (HaPy), experimental paradigm (sim2real predictivity and SRCC), and take-away messages are useful to the broader community. While we believe our controlled environment is complex enough to robustly estimate sim2real predictivity, we do not believe it should be used to measure navigation performance. Our work is complementary to ongoing efforts to improve sim2real performance and our metric is independent of the simulator implementation.</p>
<p>II. RELATED WORK</p>
<p>Embodied AI tasks: Given the emergence of several 3D simulation platforms, it is not surprising that there has been a surge of research activity focusing on investigation of embodied AI tasks. One early example leveraging simulation is the work of Zhu et al. [21] on target-driven navigation using deep reinforcement learning in synthetic environments within AI2 THOR [7]. Follow up work by Gupta et al. [22] demonstrated an end-to-end learned joint mapping and planning method evaluated in simulation using reconstructed interior spaces. More recently, Gordon et al. [23] showed that decoupling perception and policy learning modules can aid in generalization to unseen environments, as well as between different environment datasets. Beyond these few examples, a breadth of recent work on embodied AI tasks demonstrates the acceleration that 3D simulators have brought to this research area. In contrast, deployment on real robotic platforms for similar AI tasks still incurs significant resource overheads and is typically only feasible with large, well-equipped teams of researchers. One of the most prominent examples is the DARPA Robotics Challenge (DRC) [24]. Another example of real-world deployment is the work of Gandhi et al. [25] who trained a drone to fly in reality by locking it in a room. Our goal is to characterize how well a model trained in simulation can generalize when deployed on a real robot.</p>
<p>Simulation-to-reality transfer: Due to the logistical limitations of physical experimentation, transfer of agents trained in simulation to real platforms is a topic of much interest. There have been successful demonstrations of sim2real transfer in several domains. The CAD2RL [8] system of Sadeghi and Levine trained a collision avoidance policy entirely in simulation and deployed it on real aerial drones. Similarly, Muller et al. [26] show that driving policies can be transferred from simulated cars to real remote-controlled cars by leveraging modularity and abstraction in the control policy. Tan et al. [27] train quadruped locomotion policies in simulation by leveraging domain randomization and demonstrate robustness when deployed to real robots. Chebotar et al. [28] improve the simulation using the difference between simulation and reality observations. Lastly, Hwangbo et al. [29] train legged robotic systems in simulation and transfer the learned policies to reality. The goal of this work is to enable researchers to use simulation for evaluation with confidence that their results will generalize to real robots. This brings to the forefront the key question: can we establish a correlation between performance in simulation and in reality? We focus on this question in the domain of indoor visual navigation.</p>
<p>III. HABITAT-PYROBOT BRIDGE: SIMPLE SIM2REAL</p>
<p>Deploying AI systems developed in simulation to physical robots presents significant financial, engineering, and logistical challenges -especially for non-robotics researchers. Approaching this directly requires researchers to maintain two parallel software stacks, one typically based on ROS [30] for the physical robot and another for simulation. In addition to requiring significant duplication of effort, this model can also introduce inconsistencies between agent details and task specifications in simulation and reality.</p>
<p>To reduce this burden and enable our experiments, we introduce the Habitat-PyRobot Bridge (HaPy). As its name suggests, HaPy integrates the Habitat [1] platform with PyRobot APIs [18] -enabling identical agent and evaluation code to be executed in simulation with Habitat and on a PyRobot-enabled physical robot. Habitat is a platform for embodied AI research that aims to standardize the different layers of the embodied agent software stack, covering 1) datasets, 2) simulators, and 3) tasks. This enables researchers to cleanly define, study, and share embodied tasks, metrics, and agents. For deploying simulation-trained agents to reality, we replace the simulator layer in this stack with 'reality' while maintaining task specifications and agent interfaces. Towards this end, we integrate Habitat with Py-Robot [18], a recently released high-level API that implements simple interfaces to abstract lower-level control and perception for mobile platforms (LoCoBot), and manipulators (Sawyer), and offers a seamless interface for adding new robots. HaPy is able to benefit from the scalability and generalizability of both Habitat and PyRobot. Running an agent developed in Habitat on the LoCoBot platform requires changing a single argument,
Habitat-Sim-v0→PyRobot-Locobot-v0.
At a high level, HaPy enables the following: 1) A uniform observation space API across simulation and reality. Having a shared implementation ensures that observations from simulation and reality sensors go through the same transformations (e.g. resizing, normalization). 2) A uniform action space API for agents across simulation and reality. Habitat and PyRobot differ in their agent action spaces. Our integration unifies the two action spaces and allows an agent model to remain agnostic between simulation and reality. 3) Integration at the simulator layer in the embodied agent stack allows reuse of functionalities offered by task layers of the stack -task definition, metrics, etc. stay the same across simulation and reality. This also opens the potential for jointly training in simulation and reality. 4) Containerized deployment for running challenges where participants upload their code for seamless deployment to mobile robot platforms such as LoCoBot. We hope this contribution will allow the community to easily build agents in simulation and deploy them in the real world.</p>
<p>IV. VISUAL NAVIGATION IN SIMULATION &amp; REALITY</p>
<p>Recall that our goal is to answer an ostensibly straightforward scientific question -is performance in simulated environments predictive of real-world performance for visual navigation? Let us make this more concrete.</p>
<p>First, note this question is about testing in simulation vs reality. It does not require us to take a stand on training in simulation vs reality (or the need for training at all). For a comparison between simulation-trained and non-learning-based navigation methods, we refer the reader to previous studies [1], [31], [32]. We focus on test-time discrepancies between simulation and reality for learning-based methods.</p>
<p>Second, even at test-time, many variables contribute to the sim2real gap. The real-world test environment may include objects or rooms which visually differ from simulation, or may present a differing task difficulty distribution (due to unmodeled physics or rendering in simulation). To isolate these factors as much as possible, we propose a direct comparison -evaluating agents in physical environments and in corresponding simulated replicas. We construct a set of physical lab environment configurations for a robot to traverse and virtualize each by 3D scanning the space, thus controlling for semantic domain gap. We then evaluate agents in matching simulated and real configurations to characterize the sim-vs-real gap in visual navigation.</p>
<p>In this section, we first recap the PointNav task [19] from the recent AI Habitat Challenge [13]. Then, we compare agents in simulation and robots in reality.</p>
<p>A. Task: PointGoal Navigation (PointNav)</p>
<p>In this task, an agent is spawned in an unseen environment and asked to navigate to a goal location specified in relative coordinates. We start from the agent specification and observation settings from [13]. Specifically, agents have access to an egocentric RGB (or RGBD) sensor and accurate localization and heading via a GPS+Compass sensor. The goal is specified using polar coordinates (r, θ), where r is the Euclidean distance to the goal and θ is the azimuth to the goal. The action space for the agent is: turn-left 30°1, turn-right 30°1, forward 0.25 m, and STOP. An episode is considered successful if the agent issues the STOP command within 0.2 meters of the goal. Episodes lasting longer than 200 steps or calling the STOP command &gt;0.2 m from goal are declared unsuccessful. We set the step threshold to 200 (compared to 500 in the challenge) because our testing lab is small and we found that episodes longer than 200 actions are likely to fail. We also limit collisions to 40 to prevent damage to the robot. We find that &gt;40 collisions in an episode typically occur when the robot is stuck and likely to fail.</p>
<p>B. Agent in Simulation</p>
<p>Body: The experiments by Savva et al. [1] and the Habitat Challenge 2019 [13] model the agent as an idealized cylinder of radius 0.1m and height 1.5˜m. As shown in Fig. 3, we configure the agent to match the robot used in our experiments (LoCoBot) as closely as possible. Specifically, we configure the simulated agent's base-radius and height to be 0.175 m and 0.61 m respectively to match LoCoBot dimensions.</p>
<p>Sensors: We set the agent camera field of view to 45 degrees to match images from the Intel D435 camera on LoCoBot. We match the aspect ratio and resolution of the simulated sensor frames to real sensor frames from LoCoBot using square center cropping followed by image resizing to a height and width of 256 pixels. To mimic the depth camera's limitations, we clip simulated depth sensing to 10 m.</p>
<p>Actions: In [1], [13], agent actions are deterministici.e. when the agent executes turn-left 30°, it turns exactly 30 • , and forward 0.25 m moves the agent exactly 0.25 m forward (modulo collisions). However, no robot moves deterministically due to real-world actuation noise. To model the actions on LoCoBot, we leverage an actuation noise model derived from mocap-based benchmarking by the PyRobot authors [18]. Specifically, when the agent calls (say) forward, we sample from an action-specific 2D Gaussian distribution over relative displacements. Fig. 2 shows trajectory rollouts sampled from this noise model. As shown, identical action sequences can lead to vastly different final locations.</p>
<p>Finally, in contrast to [1], [13], we increase the angles associated with turn-left and turn-right actions from 10 • to 30 • degrees. The reason is a fundamental discrepancy between simulation and reality -there is no 'ideal' GPS+compass sensor in reality. Perfect localization in indoor environments is an open research problem. In our preliminary experiments, we found that localization noise was exacerbated by the 'move, stop, turn, move' behavior of the robot, which is a result of a discrete action space (as opposed to continuous control via velocity or acceleration actions). We strike a balance between staying comparable to prior work (that uses discrete actions) and reducing localization noise by increasing the turn angles (which decreases the number of robot restarts). In the longer term, we believe the community should move towards continuous control to overcome this issue. All the modeling parameters are easily adaptable to different robots.</p>
<p>C. LoCoBot in Reality</p>
<p>Body: LoCoBot is designed to provide easy access to a robot with basic grasping, locomotion, and perception capabilities. It is a modular robot based on a Kobuki YMR-K01-W1 mobile base with an extensible body.</p>
<p>Sensors: LoCoBot is equipped with a Intel D435 RGB+depth camera. While LoCoBot possesses on-board IMUs and motor encoders (which can provide the GPS+Compass sensor observations required by this task), the frequent stopping and starting from our discrete actions resulted in significant error accumulation. To provide precise localization, we mounted a Hokuyo UTM-30LX LIDAR sensor in place of the robot's grasping arm (seen in Fig. 3). We run the LIDAR-based Hector SLAM [33] algorithm to provide the location+heading for the GPS+Compass sensor and for computing success and SPL of tests in the lab.</p>
<p>At this point, it is worth asking how accurate the LIDAR based localization is. To quantify localization error, we ran a total of 45 tests across 3 different room configurations in the lab, and manually measured the error with measuring tape. On average, we find errors of approximately 7 cm with Hector SLAM, compared to 40 cm obtained from wheel odometry and onboard IMU (combined with an Extended Kalman Filter implementation in ROS). Note that 7 cm is significantly lower than the 0.2 m = 20 cm criteria used to define success in PointNav, providing us confidence that we can use LIDAR-based Hector SLAM to judge success in our real-world experiments. More importantly, we notice that the LIDAR approach allows the robot to reliably relocalize using its surroundings, and thus error does not accumulate over long trajectories, or with consecutive runs, which is important for running hundreds of real-world experiments.</p>
<p>D. Evaluation Environment</p>
<p>Our evaluation environment is a controlled 6.5 m by 10 m interior room called CODA. Note that the agent was trained entirely in simulation and has never seen our evaluation environment room during training. The grid-like texture of the floor is purely coincidental, and not relied upon by the agent for navigation. We create 3 room configurations (easy, medium, hard difficulty) with increasing number of tall, cardboard obstacles spread throughout the room. These 'boxy' obstacles can be sensed easily by both the LIDAR and camera despite the two being vertically separated by ∼0.5 m. Objects like tables or chairs have narrow support at the LIDAR's height. For each room configuration, we define a set of 5 waypoints to serve as the start and end locations for navigation episodes. Fig. 4 shows top-down views of these room configurations.</p>
<p>Virtualization: We digitize each environment configuration using a Matterport Pro2 3D camera to collect 360 • scans at multiple points in the room, ensuring full coverage. These scans are used to reconstruct 3D meshes of the environment which can be directly imported into Habitat. This streamlined process is easily scalable and enables quick virtualization of new physical spaces. On average, each configuration was reconstructed from 7 panoramic captures and took approximately 10 minutes. We also evaluated how the reconstruction quality in simulation affects the transfer, this was done by dropping 5% of mesh triangles in simulation which led to a 23% drop in SRCC (defined in Sec. IV.E).</p>
<p>Test protocol: We run parallel episodes in both simulation and reality. The agent navigates sequentially through the waypoints shown in Fig. 4
(A → B → C → D → E → A)
for a total of 5 navigation episodes per room configuration. The starting points, starting rotations, and goal locations are identical across simulation and reality. In total, we test 9 navigation models (described in the next section), in 3 different room configurations, each with 5 spawn-to-goal waypoints, and 3 independent trials, for a total of 810 runs in simulation and reality combined. Each spawn-to-goal navigation with LoCoBot takes approximately 6 minutes, corresponding to 40.5 hours of real-world testing. Safety guidelines require that a human monitor the experiments and at 8 hours a day, these experiments would take 5 days. With such long turn-around times, it is essential that we use a robust pipeline to automate (or semi-automate) our experiments and reduce the cognitive load on the human supervisor. After each episode, the robot is automatically reset and has no knowledge from its previous run. For unsuccessful episodes, the robot uses a prebuilt environment map to navigate to the next episode start position. The room is equipped with a wireless camera to remotely track the experiments. In future, we plan to connect this automated evaluation setup to a docker2robot challenge, where participants can push code to a repository, which is then automatically evaluated on a real robot in this lab environment.</p>
<p>E. Sim2Real Correlation Coefficient</p>
<p>To quantify the degree to which performance in simulation translates to performance in reality, we use a measure we call Sim2Real Correlation Coefficient (SRCC). Let (s i , r i ) denote accuracy (episode success rate, SPL [10], etc.) of navigation method i in simulation and reality respectively.</p>
<p>Given a paired dataset of accuracies for n navigation methods  {(s 1 , r 1 ), . . . , (s n , r n )}, SRCC is the sample Pearson correlation coefficient (bivariate correlation). 2 SRCC values close to +1 indicate high linear correlation and are desirable, insofar as changes in simulation performance metrics correlate highly with changes in reality performance metrics. Values close to 0 indicate low correlation and are undesirable as they indicate changes of performance in simulation is not predictive of real world changes in performance. Note that this definition of SRCC also suggests an intuitive visualization: by plotting performance in reality against performance in simulation as a scatterplot we can reveal the existence of performance correlation and detect outlier simulation settings or evaluation scenarios.</p>
<p>Beyond the utility of SRCC as a simulation predictivity metric, we can also view it as an optimization objective for simulation parameters. Concretely, let θ denote parameters controlling the simulator (amount of actuation noise, lighting, etc.). We can view simulator design as optimization problem: max θ SRCC(S n (θ), R n ) where S n (θ) = {s 1 (θ), . . . , s n (θ)} is the set of accuracies in simulation with parameters θ and R n is the same performance metric computed on equivalent episodes in reality. Note that θ affects performance in simulation S n (θ) but not R n since we are only changing test-time parameters. The specific navigation models themselves are held fixed. Overall, this gives us a formal approach to simulator design instead of operating on intuitions and qualitative assessments.</p>
<p>In contrast, if a simulator has low SRCC but high mean real world performance, researchers will not be able to use this simulator to make decisions (e.g. model selection) because they cant know if changes to performance in simulation will have a positive or negative effect on real-world performance. Every change will have to be tested on the physical robot.</p>
<p>V. MEASURING THE SIM2REAL GAP Navigation Models: We experiment with learning-based navigation models. Specifically, we train for PointGoal in Habitat on the 72 Gibson environments that were rated 4+ for quality in [1]. For consistency with [1], we use the Gibson dataset [5] for training. Agents are trained from scratch with reinforcement learning using DD-PPO [2] -a decentralized, distributed proximal policy optimization [34] algorithm that is well-suited for GPU-intensive simulator-based training. Each model is trained for 500 million steps on 64 Tesla V100 s. For evaluation, we select the model with best Gibson-val performance. We use the agent architecture from [2] composed of a visual encoder (ResNet50 [35]) and policy network (2-layer LSTM [36] with 512-dim state).</p>
<p>Consistent with prior work, we train agents with RGB and depth sensors. Real-world depth sensors exhibit significant noise and are limited in range. Thus, inspired by the winning entry in the RGB track of the Habitat Challenge 2019 [13], we also test an agent that uses a monocular depth estimator [37] to predict depth from RGB, which is then fed to the navigation 2 Other metrics such as rank correlation can also be used. model. In total, we train 9 different agents by varying sensor modalities (RGB, Depth, RGB→Predicted Depth) and training simulator configurations (e.g. actuation noise levels). The exact settings are listed in Table I. Note that the simulator parameters used for training these models may differ from the simulator parameters used for testing (θ). Our goal is to span the spectrum of performance at test-time.</p>
<p>A. Revisiting the AI Habitat Challenge 2019</p>
<p>Fig. 5 plots sim-vs-real performance of these 9 navigation models w.r.t. success rate (right) and SPL [19] (left). Horizontal and vertical bars on a symbol indicate the standard error in simulation and reality respectively. SRCC SPL is 0.60, which is reasonably high but far from a level where we can be confident about evaluations in simulation alone. Problematically, there are 9 relative ordering reversals from simulation to reality. The success scatterplot (right) shows an even more disturbing trend -nearly all methods (except one) appear to be working exceedingly well in simulation with success rates close to 1. However, there is a large dynamic range in success rates in reality. This is summarized by a low SRCC Succ of 0.18, suggesting that improvements in performance in simulation are not predictive of performance improvements on a real robot.</p>
<p>Note that other than largely cosmetic adjustments to the robot size, sensor and action space specification, this simulation setting is not fundamentally different from the Habitat Challenge 2019. Upon deeper investigation, we discovered that one factor leading to this low sim2real predictivity is due to a 'sliding' behavior in Habitat.</p>
<p>Cheating by sliding: In Habitat-Sim [1], when the agent takes an action that results in a collision, the agent slides along the obstacle as opposed to stopping. This behavior is also seen in many other simulators -it is enabled by default in MINOS [9], and Deepmind Lab [6], and is also prevalent in simulators and video game engines that employ physics backend, such as Gibson [5], and AI2 THOR [7]. This is because there is no perfect physics simulator, only approximations; this type of physics allows for smooth human control, but does not accurately reflect real-world physics nor safety precautions the robot platform may employ (i.e. stopping on collision). We find that this enables 'cheating' by learned agents. As illustrated by an example in Fig. 6, the agent exploits this sliding mechanism to take an effective path that appears to travel through non-navigable regions of the environment (like walls). Let s t denote agent position at time t, where the agent is already in contact with an obstacle. The agent executes a forward action, collides, and slides along the obstacle to state s t+1 . The path taken during this maneuver is far from a straight line, however for the purposes of computing SPL (the metric the agent optimizes), Habitat calculates the Euclidean distance travelled ||s t − s t+1 || 2 . This is equivalent to taking a straight line path between s t and s t+1 that goes outside the navigable regions of the environment (appearing to go through obstacles). On the one hand, the emergence of such exploits is a sign of success of large-scale reinforcement learning -clearly, we are maximizing reward. On the other hand, this is a problem for sim2real transfer. Such policies fail disastrously in the real world where the robot bump sensors force a stop on contact with obstacles. To rectify this issue, we modify Habitat-Sim to disable sliding on collisions. The discovery of this issue motivated our investigation into optimizing simulation parameters.</p>
<p>B. Optimizing Simulation With SRCC</p>
<p>We perform grid-search over simulation parameters -sliding (off vs on) and a scalar multiplier on actuation noise (varying from 0 to 1, in increments of 0.1). We find that sliding off and 0.0 actuation noise lead to the highest SRCC. Fig. 7 shows a remarkable alignment in the SPL scatter-plot (left) -nearly all models lie close to the diagonal, suggesting that we fairly accurately predict how a model is going to perform on the robot by testing in simulation. Recall that the former takes 40.5 hours and significant human effort, while the latter is computed in under 1 minute. SRCC SPL improves from 0.603 (Fig. 5) to 0.875 (0.272 gain). SRCC Succ shows an even stronger improvementfrom 0.18 to 0.844 (0.664 gain)! From Table I, we can see that the best performance in reality (Reality SPL) is achieved by row 2, which is confidently predicted by CODA Test-Sim SPL (col 4) by a strong margin. The fact that no actuation noise is the optimal setting suggests that our chosen actuation noise model (from PyRobot [18]) may not reflect conditions in reality.</p>
<p>To demonstrate the capabilities of our models in navigating cluttered environments, we also evaluate performance on the Gibson dataset, which contains 572 scanned spaces (apartments, multi-level homes, offices, houses, hospitals, gyms), containing furniture (chairs, desks, sofas, tables), for a total of 1447 floors. We use the 'Gibson-val' navigation episodes from Savva et al. [1]. We observe a similar drop between performance between sliding on (Chall-Sim col 5, 7 in Table I) vs sliding off (Test-Sim col 4,6 in Table I). This further suggests that the participants in the Habitat Challenge 2019 would not achieve similar performance on a real robot.</p>
<p>We also implement a wall-following oracle, and compare its performance to our learned model (Depth, actuation noise=0.5, sliding=off). The oracle receives full visibility of the environment via a top-down map of the environment. Note that this is significantly stronger input than our learned models (which operate on egocentric depth frames). The oracle navigates to the goal by following a straight path towards the goal and follows walls upon coming in contact with obstacles. We endow the oracle with perfect wall-following (i.e. it follows the wall along the shortest direction to the goal), and it never actually collides or gets stuck on obstacles. This oracle achieves 0.46 SPL on Gibson-val, compared to 0.59 SPL achieved by our model under the Gibson Test-Sim setting. For episodes longer than 10 m, the oracle performance drops to 0.05 SPL, compared to our model, which achieves a 0.23 SPL. These experiments show the need for a learning based approach in complex and cluttered environments.</p>
<p>VI. CONCLUSION</p>
<p>We introduce the Habitat-PyRobot Bridge (HaPy) library that allows for seamless deployment of visual navigation models. Using Matterport scanning, the Habitat stack, the HaPy library, and LoCoBot we benchmark the correlation between reality and simulation performance with the SRCC metric. We find that naive simulation parameters lead to low correlation between performance in simulation and performance in reality. We then optimize the simulation parameters for SRCC and obtain high predictivity of real world performance without running new evaluations in the real world. We hope that the infrastructure we introduce and the conceptual framework of optimizing simulation for predictivity will enable sim2real transfer in a variety of navigation tasks.</p>
<p>ACKNOWLEDGMENT</p>
<p>We thank the reviewers for their helpful suggestions. We are grateful to Kalyan Alwala, Dhiraj Gandhi and Wojciech Galuba for their help and support. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.</p>
<p>Licenses for referenced datasets:</p>
<p>Gibson: https://storage.googleapis.com/gibson_material/ Agreement%20GDS%2006-04-18.pdf Matterport3D: http://kaldir.vc.in.tum.de/matterport/MP_ TOS.pdf</p>
<p>Fig. 1 .
1We measure the correlation between visual navigation performance in simulation and in reality by virtualizing reality and executing parallel experiments. (a) Navigation trajectory in a real space with obstacles. (b) virtualized replica in simulation. (c) we propose the Sim2Real Correlation Coefficient (SRCC) as a measure of simulation predictivity. By optimizing for SRCC, we arrive at simulation settings that are highly predictive of real-world performance.</p>
<p>Fig. 2 .
2Effect of actuation noise. The black line is a trajectory from an action sequence with perfect actuation. In red are trajectories from this sequence with actuation noise.</p>
<p>Fig. 3 .
3Simulation vs. reality. Shading on the trajectory in reality represents uncertainty in the robot's location (±7 cm).</p>
<p>Fig. 4 .
4Top-down view of one of our testing environments. White boxes are obstacles. The robot navigates sequentially through the waypointsA → B → C → D → E → A.</p>
<p>Fig. 6 .
6Sliding behavior leading to 'cheating' agents. At time t, the agent at s t executes a forward action, and slides along the wall to state s t+1 . The resulting straight-line path (used to calculate SPL) goes outside the environment. Gray denotes navigable space while white is non-navigable.</p>
<p>Fig. 7 .
7Optimized SRCC SPL (left) and SRCC Succ (right) scatterplots in the CODA environment. Comparing withFig. 5we see improvements, indicating better predictivity of real-world performance.</p>
<p>TABLE I
IAVERAGE PERFORMANCE IN REALITY (COL. 3), CODA AND GIBSON SCENES 
UNDER HABITAT CHALLENGE 2019 SETTINGS [SLIDING=ON, NOISE=0.0] 
(COL. 5, COL. 7), AND TEST-SIM [SLIDING=OFF, NOISE=0.0] (COL. 6, COL. 8) 
ACROSS DIFFERENT TRAIN-SIM CONFIGURATIONS (COL. 1-2) </p>
<p>Fig. 5. SRCC SPL (left) and SRCC Succ (right) plots for AI Habitat Challenge 
2019 test-sim setting in the CODA environment. We note a relatively low 
correlation between real and simulated performance. </p>
<p>Originally 10 • .</p>
<p>Habitat: A platform for embodied AI research. M Savva, Proc. Int. Conf. Comput. Vision. Int. Conf. Comput. Vision67M. Savva et al., "Habitat: A platform for embodied AI research," in Proc. Int. Conf. Comput. Vision, 2019, pp. 1-4, 6, 7.</p>
<p>DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. E Wijmans, Proc. Int. Conf. Learn. Representations, 2020. Int. Conf. Learn. Representations, 2020E. Wijmans et al., "DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames," in Proc. Int. Conf. Learn. Representations, 2020, pp. 1, 6.</p>
<p>Cognitive mapping and planning for visual navigation. S Gupta, J Davidson, S Levine, R Sukthankar, J Malik, Proc. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern Recognit. Conf1S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik, "Cognitive mapping and planning for visual navigation," in Proc. Comput. Vision Pattern Recognit. Conf., 2017, p. 1.</p>
<p>CARLA: An open urban driving simulator. A Dosovitskiy, G Ros, F Codevilla, A Lopez, V Koltun, Proc. 1st Annu. Conf. Robot Learn. 1st Annu. Conf. Robot LearnA. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, "CARLA: An open urban driving simulator," in Proc. 1st Annu. Conf. Robot Learn., 2017, pp. 1-16.</p>
<p>Gibson env: Real-world perception for embodied agents. F Xia, A R Zamir, Z He, A Sax, J Malik, S Savarese, Proc. Comput. Vision Pattern Recognit. Comput. Vision Pattern RecognitF. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese, "Gibson env: Real-world perception for embodied agents," in Proc. Comput. Vision Pattern Recognit. Conf., 2018, pp. 1-7.</p>
<p>Deepmind lab. C Beattie, arXiv:1612.03801C. Beattie et al., "Deepmind lab," 2016, arXiv:1612.03801.</p>
<p>AI2-THOR: An interactive 3D environment for visual AI. E Kolve, arXiv:1712.05474E. Kolve et al., "AI2-THOR: An interactive 3D environment for visual AI," 2017, arXiv:1712.05474.</p>
<p>CAD2RL: real single-image flight without a single real image. F Sadeghi, S Levine, Robotics: Science and Systems XIII. Cambridge, Massachusetts, USAF. Sadeghi and S. Levine, "CAD2RL: real single-image flight without a single real image," in Robotics: Science and Systems XIII, Cambridge, Massachusetts, USA, Jul. 2017.</p>
<p>MINOS: Multimodal indoor simulator for navigation in complex environments. M Savva, A X Chang, A Dosovitskiy, T Funkhouser, V Koltun, arXiv:1712.03931M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun, "MINOS: Multimodal indoor simulator for navigation in complex envi- ronments," 2017, arXiv:1712.03931.</p>
<p>Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. P Anderson, Proc. Comput. Vision Pattern Recognit. Comput. Vision Pattern Recognit5P. Anderson et al., "Vision-and-language navigation: Interpreting visually- grounded navigation instructions in real environments," in Proc. Comput. Vision Pattern Recognit. Conf., 2018, pp. 1, 5.</p>
<p>Embodied Question Answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, Proc. Comput. Vision Pattern Recognit. Comput. Vision Pattern Recognit1A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, "Embodied Question Answering," in Proc. Comput. Vision Pattern Recognit. Conf., 2018, p. 1.</p>
<p>. Visual Learning and Embodied Agents in Simulation Environments @. "Visual Learning and Embodied Agents in Simulation Environments @ ECCV 2018," 2018. [Online]. Available: https://eccv18-vlease.github.io/</p>
<p>Habitat Challenge 2019 @ Habitat Embodied Agents Workshop. "Habitat Challenge 2019 @ Habitat Embodied Agents Workshop. CVPR 2019," 2019. [Online]. Available: https://aihabitat.org/challenge/2019/</p>
<p>CARLA autonomous driving challenge @ CVPR 2019. "CARLA autonomous driving challenge @ CVPR 2019," 2019. [Online]. Available: https://carlachallenge.org/</p>
<p>Robothor challenge @ CVPR 2020. "Robothor challenge @ CVPR 2020," 2020. [Online]. Available: https: //ai2thor.allenai.org/robothor/challenge/</p>
<p>The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities. J Lehman, arXiv:1803.03453J. Lehman et al., "The surprising creativity of digital evolution: A col- lection of anecdotes from the evolutionary computation and artificial life research communities," 2018, arXiv:1803.03453.</p>
<p>Emergent tool use from multi-agent autocurricula. B Baker, arXiv:1909.07528B. Baker et al., "Emergent tool use from multi-agent autocurricula," 2019, arXiv:1909.07528.</p>
<p>Pyrobot: An open-source robotics framework for research and benchmarking. A Murali, arXiv:1906.08236A. Murali et al., "Pyrobot: An open-source robotics framework for research and benchmarking," 2019, arXiv:1906.08236.</p>
<p>On Evaluation of Embodied Navigation Agents. P Anderson, arXiv:1807.06757P. Anderson et al., "On Evaluation of Embodied Navigation Agents," 2018, arXiv:1807.06757.</p>
<p>Locobot: An open source low cost robot. "Locobot: An open source low cost robot," 2019. [Online]. Available: https://locobot-website.netlify.com/</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, Proc. IEEE Int. Conf. Robot. Autom. IEEE Int. Conf. Robot. Autom2Y. Zhu et al., "Target-driven visual navigation in indoor scenes using deep reinforcement learning," in Proc. IEEE Int. Conf. Robot. Autom., May 2017, p. 2.</p>
<p>Cognitive mapping and planning for visual navigation. S Gupta, J Davidson, S Levine, R Sukthankar, J Malik, Proc. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern Recognit. Conf2S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik, "Cognitive mapping and planning for visual navigation," in Proc. Comput. Vision Pattern Recognit. Conf., 2017, p. 2.</p>
<p>Splitnet: Sim2sim and task2task transfer for embodied visual navigation. D Gordon, A Kadian, D Parikh, J Hoffman, D Batra, Proc. Int. Conf. Comput. Vision. Int. Conf. Comput. Vision2D. Gordon, A. Kadian, D. Parikh, J. Hoffman, and D. Batra, "Splitnet: Sim2sim and task2task transfer for embodied visual navigation," in Proc. Int. Conf. Comput. Vision, 2019, p. 2.</p>
<p>The darpa robotics challenge finals: results and perspectives. E Krotkov, J. Field Robot. 342E. Krotkov et al., "The darpa robotics challenge finals: results and per- spectives," J. Field Robot., vol. 34, no. 2, pp. 229-240, 2017.</p>
<p>Learning to fly by crashing. D Gandhi, L Pinto, A Gupta, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. IEEE/RSJ Int. Conf. Intell. Robots SystD. Gandhi, L. Pinto, and A. Gupta, "Learning to fly by crashing," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2017, pp. 3948-3955.</p>
<p>Driving policy transfer via modularity and abstraction. M Müller, A Dosovitskiy, B Ghanem, V Koltun, arXiv:1804.09364M. Müller, A. Dosovitskiy, B. Ghanem, and V. Koltun, "Driving policy transfer via modularity and abstraction," 2018, arXiv:1804.09364.</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, arXiv:1804.10332J. Tan et al., "Sim-to-real: Learning agile locomotion for quadruped robots," 2018, arXiv:1804.10332.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, Proc. Int. Conf. Robot. Autom. 2019. Int. Conf. Robot. Autom. 2019Y. Chebotar et al., "Closing the sim-to-real loop: Adapting simulation randomization with real world experience," in Proc. Int. Conf. Robot. Autom. 2019, pp. 8973-8979.</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, arXiv:1901.08652J. Hwangbo et al., "Learning agile and dynamic motor skills for legged robots," 2019, arXiv:1901.08652.</p>
<p>ROS: An open-source robot operating system. M Quigley, Proc. ICRA Workshop Open Source Softw. ICRA Workshop Open Source Softw33M. Quigley et al., "ROS: An open-source robot operating system," in Proc. ICRA Workshop Open Source Softw., 2009, vol. 3, p. 3.</p>
<p>Benchmarking classic and learned navigation in complex 3D environments. D Mishkin, A Dosovitskiy, V Koltun, arXiv:1901.10915D. Mishkin, A. Dosovitskiy, and V. Koltun, "Benchmarking clas- sic and learned navigation in complex 3D environments," 2019, arXiv:1901.10915.</p>
<p>To learn or not to learn: Analyzing the role of learning for navigation in virtual environments. N Kojima, J Deng, arXiv:1907.11770N. Kojima and J. Deng, "To learn or not to learn: Analyzing the role of learning for navigation in virtual environments," 2019, arXiv:1907.11770.</p>
<p>A flexible and scalable slam system with full 3d motion estimation. S Kohlbrecher, J Meyer, O Stryk, U Klingauf, Proc. IEEE Int. Symp. Safety, Secur. Rescue Robot. IEEE Int. Symp. Safety, Secur. Rescue RobotS. Kohlbrecher, J. Meyer, O. von Stryk, and U. Klingauf, "A flexible and scalable slam system with full 3d motion estimation," in Proc. IEEE Int. Symp. Safety, Secur. Rescue Robot., 2011, pp. 155-160.</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," 2017, arXiv:1707.06347.</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. Comput. Vision Pattern Recognit. Conf. Comput. Vision Pattern Recognit. Conf6K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proc. Comput. Vision Pattern Recognit. Conf., 2016, p. 6.</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 98S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural Computation, vol. 9, no. 8, pp. 1735-1780, 1997.</p>
<p>FastDepth: Fast monocular depth estimation on embedded systems. D Wofk, F Ma, T.-J Yang, S Karaman, V Sze, Proc. ICRA. ICRA6D. Wofk, F. Ma, T.-J. Yang, S. Karaman, and V. Sze, "FastDepth: Fast monocular depth estimation on embedded systems," in Proc. ICRA, 2019, p. 6.</p>            </div>
        </div>

    </div>
</body>
</html>
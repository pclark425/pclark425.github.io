<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7846 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7846</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7846</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-278207672</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.21202v1.pdf" target="_blank">Automatic Legal Writing Evaluation of LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Despite the recent advances in Large Language Models, benchmarks for evaluating legal writing remain scarce due to the inherent complexity of assessing open-ended responses in this domain. One of the key challenges in evaluating language models on domain-specific tasks is finding test datasets that are public, frequently updated, and contain comprehensive evaluation guidelines. The Brazilian Bar Examination meets these requirements. We introduce oab-bench, a benchmark comprising 105 questions across seven areas of law from recent editions of the exam. The benchmark includes comprehensive evaluation guidelines and reference materials used by human examiners to ensure consistent grading. We evaluate the performance of four LLMs on oab-bench, finding that Claude-3.5 Sonnet achieves the best results with an average score of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can serve as reliable automated judges for evaluating legal writing. Our experiments show that frontier models like OpenAI's o1 achieve a strong correlation with human scores when evaluating approved exams, suggesting their potential as reliable automated evaluators despite the inherently subjective nature of legal writing assessment. The source code and the benchmark -- containing questions, evaluation guidelines, model-generated responses, and their respective automated evaluations -- are publicly available.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7846.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7846.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-vs-Human (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate comparison of LLM judges versus human examiners on oab-bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summary of the paper's empirical comparisons between LLMs used as automated judges and human examiners on Brazilian Bar Exam open-ended responses (oab-bench), reporting MAE-based agreement, observed failure modes, and limitations of the approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Legal Writing Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automatic Legal Writing Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal writing evaluation (Brazilian Bar Exam second phase: essays and discursive questions)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>oab-bench (105 questions from 3 recent OAB editions) and a small collected set of real human-written, human-graded exam responses (3 exams used in human-LLM comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>o1, GPT-4o, DeepSeek-R1 (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Multiple judge models evaluated: o1-2024-12-17 (OpenAI reasoning model with chain-of-thought emphasis, higher compute), GPT-4o (OpenAI multimodal/omni model), DeepSeek-R1 (reasoning-focused model described by authors); different temperatures used per model.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Official human examiners' grades extracted from preparatory-course-shared materials (handwritten answer booklets manually transcribed); these are human expert/official grades.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Mean Absolute Error (MAE)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.2466666666666667</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>small human-sample size (only approved/passing responses); lack of low-grade examples; high evaluation cost for frontier models; lack of automated validation (no automatic checks for score summation or range); possible transcription noise from manual transcription; judge verbosity (no enforced line limits)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Overall reasonable alignment between LLM judges and human scores on approved exams; LLMs can approximate human scoring patterns but show variability across models and question types; human graders are also error-prone (handwriting misreadings, fatigue, inconsistent scoring); more extensive validation needed.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Scalability and repeatability; potential to provide cost-effective, automated evaluation signals (e.g., for RL or benchmarking) and reduce reliance on large human-annotator pools; consistent application of provided grading rubric when constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Judge prompt provided question statement, max score, commented answer and score distribution table (reference-guided); modes: single-answer + reference-guided grading and a multi-turn judge prompt for evaluating sub-question B with access to A; manual transcription of three human exams (one per area) chosen to span score ranges; MAE used to quantify discrepancy; manual inspection/validation of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Legal Writing Evaluation of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7846.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7846.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1-vs-Human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1-2024-12-17 judge model compared to human examiners on oab-bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison showing o1's automated grades compared to human grades on three real human-written OAB exams, reporting small MAE values and systematic tendencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Legal Writing Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automatic Legal Writing Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal essay and discursive question scoring (OAB second phase)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>oab-bench; three manually transcribed human exam responses (Criminal, Civil, Labor)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>o1 (o1-2024-12-17)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI 'o1' reasoning model (released 2024-12-17), designed to produce long chains of thought; higher compute/cost; temperature used = 1.0 for reasoning models in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Official human examiners' scores from preparatory-course-shared materials (handwritten); treated as ground truth for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Mean Absolute Error (MAE)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.18666666666666668</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>tendency to assign higher scores to legal essays relative to human examiners; small-sample evaluation limited to passing/approved responses (no failing cases); occasional variations in per-item distribution though total scores often close</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>o1 demonstrated the best alignment among tested judges: MAE per-exam reported as 0.04 (Criminal), 0.28 (Civil), and 0.24 (Labor); in one case o1 matched human perfect score (Criminal); in another it assigned a higher total (Civil). Authors describe o1 as having 'reasonable alignment' and slightly better consistency than other judges.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Highest consistency and closest numerical agreement with human scores among evaluated models; reliable handling of multi-turn / reference-guided prompts when configured properly.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Reference-guided single-answer grading and multi-turn judge prompts used; judge provided with question, max score, commented answer, score distribution table, and candidate answer; three human exams evaluated (one per area), MAE computed per exam; manual verification of some outputs; o1 judge cost contributed to per-model evaluation cost estimate ($50–$55 per model run).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Legal Writing Evaluation of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7846.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7846.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-vs-Human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4o judge model compared to human examiners on oab-bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of GPT-4o automated grading with human examiners showing larger discrepancies and several concrete failure modes (arithmetic and out-of-range scoring, multi-turn prompt noncompliance).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Legal Writing Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automatic Legal Writing Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal essay and discursive question scoring (OAB second phase)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>oab-bench; three manually transcribed human exam responses (Criminal, Civil, Labor)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o (version used in experiments); temperature used = 0 for GPT-4o judgments; observed to sometimes output invalid/out-of-range scores and arithmetic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Official human examiners' scores from preparatory-course-shared materials (handwritten); treated as ground truth for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Mean Absolute Error (MAE)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.32666666666666666</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>arithmetic/summation errors when computing totals; assigned out-of-range scores (exceeded per-item maximums); failed to follow multi-turn instruction (combined sub-question A+B instead of evaluating only B); larger MAE than o1</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4o showed more inconsistency and concrete operational failures (e.g., reporting 2.0 on a 1.25-max item, incorrect total sums). Although it sometimes agreed with o1 on item-level judgments, its output formatting, summation errors, and failure to respect explicit prompt constraints reduce reliability as an automated judge in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>When functioning correctly, capable of reasonable grading, but in this study GPT-4o's failures limited practical advantage; still potentially useful with stricter output validation and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same reference-guided and multi-turn judge prompts as other models; GPT-4o used temperature 0 for judgments; evaluation uncovered model-specific failures (out-of-range scores, summation mistakes); MAE per-exam reported as 0.23 (Criminal), 0.55 (Civil), and 0.20 (Labor).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Legal Writing Evaluation of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7846.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7846.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1-vs-Human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1 judge model compared to human examiners on oab-bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of DeepSeek-R1 as a judge showing comparable MAE to o1 in some cases but occasional failures in multi-turn focus and sub-question evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Legal Writing Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automatic Legal Writing Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal essay and discursive question scoring (OAB second phase)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>oab-bench; three manually transcribed human exam responses (Criminal, Civil, Labor)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>DeepSeek-R1 (reasoning-focused model described by authors as a promising open-source alternative), temperature used = 1.0 (only supported value for reasoning models).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Official human examiners' scores from preparatory-course-shared materials (handwritten); treated as ground truth for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Mean Absolute Error (MAE)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.22</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>occasional failure to restrict evaluation to the intended sub-question in multi-turn mode; occasional need to rescale scores to fit maximum limits; small-sample evaluation limitations</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>DeepSeek-R1's overall performance was comparable to o1 on some metrics, but authors observed it sometimes failed at focusing only on the last sub-question in multi-turn evaluations; when disagreements with humans occurred, LLM interpretations were often reasonable and justifiable per rubric.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Potential cost-effective alternative to frontier models; comparable performance in some scenarios and promising for future open-source, lower-cost judging pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluated using the same judge prompt templates (reference-guided, single-answer, multi-turn for sub-question B); DeepSeek-R1 used temperature 1.0; MAE per-exam reported as 0.12 (Criminal), 0.27 (Civil), and 0.27 (Labor); authors manually noted specific multi-turn focus failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Legal Writing Evaluation of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can Large Language Models Be an Alternative to Human Evaluations <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA <em>(Rating: 2)</em></li>
                <li>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7846",
    "paper_id": "paper-278207672",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "LLM-vs-Human (aggregate)",
            "name_full": "Aggregate comparison of LLM judges versus human examiners on oab-bench",
            "brief_description": "Summary of the paper's empirical comparisons between LLMs used as automated judges and human examiners on Brazilian Bar Exam open-ended responses (oab-bench), reporting MAE-based agreement, observed failure modes, and limitations of the approach.",
            "citation_title": "Automatic Legal Writing Evaluation of LLMs",
            "mention_or_use": "use",
            "paper_title": "Automatic Legal Writing Evaluation of LLMs",
            "evaluation_task": "Legal writing evaluation (Brazilian Bar Exam second phase: essays and discursive questions)",
            "dataset_name": "oab-bench (105 questions from 3 recent OAB editions) and a small collected set of real human-written, human-graded exam responses (3 exams used in human-LLM comparison)",
            "judge_model_name": "o1, GPT-4o, DeepSeek-R1 (aggregate)",
            "judge_model_details": "Multiple judge models evaluated: o1-2024-12-17 (OpenAI reasoning model with chain-of-thought emphasis, higher compute), GPT-4o (OpenAI multimodal/omni model), DeepSeek-R1 (reasoning-focused model described by authors); different temperatures used per model.",
            "human_evaluator_type": "Official human examiners' grades extracted from preparatory-course-shared materials (handwritten answer booklets manually transcribed); these are human expert/official grades.",
            "agreement_metric": "Mean Absolute Error (MAE)",
            "agreement_score": 0.2466666666666667,
            "reported_loss_aspects": "small human-sample size (only approved/passing responses); lack of low-grade examples; high evaluation cost for frontier models; lack of automated validation (no automatic checks for score summation or range); possible transcription noise from manual transcription; judge verbosity (no enforced line limits)",
            "qualitative_findings": "Overall reasonable alignment between LLM judges and human scores on approved exams; LLMs can approximate human scoring patterns but show variability across models and question types; human graders are also error-prone (handwriting misreadings, fatigue, inconsistent scoring); more extensive validation needed.",
            "advantages_of_llm_judge": "Scalability and repeatability; potential to provide cost-effective, automated evaluation signals (e.g., for RL or benchmarking) and reduce reliance on large human-annotator pools; consistent application of provided grading rubric when constrained.",
            "experimental_setting": "Judge prompt provided question statement, max score, commented answer and score distribution table (reference-guided); modes: single-answer + reference-guided grading and a multi-turn judge prompt for evaluating sub-question B with access to A; manual transcription of three human exams (one per area) chosen to span score ranges; MAE used to quantify discrepancy; manual inspection/validation of outputs.",
            "uuid": "e7846.0",
            "source_info": {
                "paper_title": "Automatic Legal Writing Evaluation of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "o1-vs-Human",
            "name_full": "OpenAI o1-2024-12-17 judge model compared to human examiners on oab-bench",
            "brief_description": "Empirical comparison showing o1's automated grades compared to human grades on three real human-written OAB exams, reporting small MAE values and systematic tendencies.",
            "citation_title": "Automatic Legal Writing Evaluation of LLMs",
            "mention_or_use": "use",
            "paper_title": "Automatic Legal Writing Evaluation of LLMs",
            "evaluation_task": "Legal essay and discursive question scoring (OAB second phase)",
            "dataset_name": "oab-bench; three manually transcribed human exam responses (Criminal, Civil, Labor)",
            "judge_model_name": "o1 (o1-2024-12-17)",
            "judge_model_details": "OpenAI 'o1' reasoning model (released 2024-12-17), designed to produce long chains of thought; higher compute/cost; temperature used = 1.0 for reasoning models in experiments.",
            "human_evaluator_type": "Official human examiners' scores from preparatory-course-shared materials (handwritten); treated as ground truth for comparison.",
            "agreement_metric": "Mean Absolute Error (MAE)",
            "agreement_score": 0.18666666666666668,
            "reported_loss_aspects": "tendency to assign higher scores to legal essays relative to human examiners; small-sample evaluation limited to passing/approved responses (no failing cases); occasional variations in per-item distribution though total scores often close",
            "qualitative_findings": "o1 demonstrated the best alignment among tested judges: MAE per-exam reported as 0.04 (Criminal), 0.28 (Civil), and 0.24 (Labor); in one case o1 matched human perfect score (Criminal); in another it assigned a higher total (Civil). Authors describe o1 as having 'reasonable alignment' and slightly better consistency than other judges.",
            "advantages_of_llm_judge": "Highest consistency and closest numerical agreement with human scores among evaluated models; reliable handling of multi-turn / reference-guided prompts when configured properly.",
            "experimental_setting": "Reference-guided single-answer grading and multi-turn judge prompts used; judge provided with question, max score, commented answer, score distribution table, and candidate answer; three human exams evaluated (one per area), MAE computed per exam; manual verification of some outputs; o1 judge cost contributed to per-model evaluation cost estimate ($50–$55 per model run).",
            "uuid": "e7846.1",
            "source_info": {
                "paper_title": "Automatic Legal Writing Evaluation of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT-4o-vs-Human",
            "name_full": "OpenAI GPT-4o judge model compared to human examiners on oab-bench",
            "brief_description": "Comparison of GPT-4o automated grading with human examiners showing larger discrepancies and several concrete failure modes (arithmetic and out-of-range scoring, multi-turn prompt noncompliance).",
            "citation_title": "Automatic Legal Writing Evaluation of LLMs",
            "mention_or_use": "use",
            "paper_title": "Automatic Legal Writing Evaluation of LLMs",
            "evaluation_task": "Legal essay and discursive question scoring (OAB second phase)",
            "dataset_name": "oab-bench; three manually transcribed human exam responses (Criminal, Civil, Labor)",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o (version used in experiments); temperature used = 0 for GPT-4o judgments; observed to sometimes output invalid/out-of-range scores and arithmetic errors.",
            "human_evaluator_type": "Official human examiners' scores from preparatory-course-shared materials (handwritten); treated as ground truth for comparison.",
            "agreement_metric": "Mean Absolute Error (MAE)",
            "agreement_score": 0.32666666666666666,
            "reported_loss_aspects": "arithmetic/summation errors when computing totals; assigned out-of-range scores (exceeded per-item maximums); failed to follow multi-turn instruction (combined sub-question A+B instead of evaluating only B); larger MAE than o1",
            "qualitative_findings": "GPT-4o showed more inconsistency and concrete operational failures (e.g., reporting 2.0 on a 1.25-max item, incorrect total sums). Although it sometimes agreed with o1 on item-level judgments, its output formatting, summation errors, and failure to respect explicit prompt constraints reduce reliability as an automated judge in this setup.",
            "advantages_of_llm_judge": "When functioning correctly, capable of reasonable grading, but in this study GPT-4o's failures limited practical advantage; still potentially useful with stricter output validation and calibration.",
            "experimental_setting": "Same reference-guided and multi-turn judge prompts as other models; GPT-4o used temperature 0 for judgments; evaluation uncovered model-specific failures (out-of-range scores, summation mistakes); MAE per-exam reported as 0.23 (Criminal), 0.55 (Civil), and 0.20 (Labor).",
            "uuid": "e7846.2",
            "source_info": {
                "paper_title": "Automatic Legal Writing Evaluation of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "DeepSeek-R1-vs-Human",
            "name_full": "DeepSeek-R1 judge model compared to human examiners on oab-bench",
            "brief_description": "Evaluation of DeepSeek-R1 as a judge showing comparable MAE to o1 in some cases but occasional failures in multi-turn focus and sub-question evaluation.",
            "citation_title": "Automatic Legal Writing Evaluation of LLMs",
            "mention_or_use": "use",
            "paper_title": "Automatic Legal Writing Evaluation of LLMs",
            "evaluation_task": "Legal essay and discursive question scoring (OAB second phase)",
            "dataset_name": "oab-bench; three manually transcribed human exam responses (Criminal, Civil, Labor)",
            "judge_model_name": "DeepSeek-R1",
            "judge_model_details": "DeepSeek-R1 (reasoning-focused model described by authors as a promising open-source alternative), temperature used = 1.0 (only supported value for reasoning models).",
            "human_evaluator_type": "Official human examiners' scores from preparatory-course-shared materials (handwritten); treated as ground truth for comparison.",
            "agreement_metric": "Mean Absolute Error (MAE)",
            "agreement_score": 0.22,
            "reported_loss_aspects": "occasional failure to restrict evaluation to the intended sub-question in multi-turn mode; occasional need to rescale scores to fit maximum limits; small-sample evaluation limitations",
            "qualitative_findings": "DeepSeek-R1's overall performance was comparable to o1 on some metrics, but authors observed it sometimes failed at focusing only on the last sub-question in multi-turn evaluations; when disagreements with humans occurred, LLM interpretations were often reasonable and justifiable per rubric.",
            "advantages_of_llm_judge": "Potential cost-effective alternative to frontier models; comparable performance in some scenarios and promising for future open-source, lower-cost judging pipelines.",
            "experimental_setting": "Evaluated using the same judge prompt templates (reference-guided, single-answer, multi-turn for sub-question B); DeepSeek-R1 used temperature 1.0; MAE per-exam reported as 0.12 (Criminal), 0.27 (Civil), and 0.27 (Labor); authors manually noted specific multi-turn focus failures.",
            "uuid": "e7846.3",
            "source_info": {
                "paper_title": "Automatic Legal Writing Evaluation of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can Large Language Models Be an Alternative to Human Evaluations",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
            "rating": 2,
            "sanitized_title": "retrievalbased_evaluation_for_llms_a_case_study_in_korean_legal_qa"
        },
        {
            "paper_title": "G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment",
            "rating": 1,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        }
    ],
    "cost": 0.012532749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automatic Legal Writing Evaluation of LLMs
29 Apr 2025</p>
<p>Roseval Malaquias roseval@maritaca.ai 
AI</p>
<p>Rodrigo Nogueira rodrigo@maritaca.ai 
AI</p>
<p>AI</p>
<p>Ramon Pires 
AI</p>
<p>Roseval Malaquias Junior 
AI</p>
<p>Automatic Legal Writing Evaluation of LLMs
29 Apr 20250F9EF900DAA4F8D1FB4FFEBFA0C24BEB10.1145/nnnnnnn.nnnnnnnarXiv:2504.21202v1[cs.CL]Open-ended TasksLegal WritingAutomatic EvaluationBrazilian Bar ExamLLM JudgeLarge Language Models
Figure 1: Overview of the oab-bench evaluation framework.The left shows the stage of generating responses to exam questions before the assessment.The right shows the collection and conversion of human-written responses from approved candidates to machine-readable format.The middle shows our automated evaluation pipeline that uses LLM judges following official grading guidelines to assess both human and model-generated responses.</p>
<p>ACM Reference Format: Ramon Pires, Roseval Malaquias Junior, and Rodrigo Nogueira.2025.Automatic Legal Writing Evaluation of LLMs.In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (ICAIL 2025 -Preprint).ACM, New York, NY, USA, 8 pages.https://doi.org/10.1145/nnnnnnn.nnnnnnn</p>
<p>Introduction</p>
<p>Recent advances in large-scale reinforcement learning have enabled Large Language Models (LLMs) to achieve PhD-level performance in standardized exams through reasoning during inference time [12,24].However, these emerging capabilities have led to concerns about benchmark saturation, prompting the development of more complex evaluation benchmarks [19,28].</p>
<p>These efforts mainly focus on STEM benchmarks, where evaluation requires minimal human intervention, typically involving Questions Answering (QA) or short-text generation tasks [17,20].Nonetheless, certain domains remain underexplored for LLM applications due to their inherent evaluation complexity, such as essay writing in specialized domains like the legal domain.In particular, this domain is characterized by the subjective and interpretative nature of some tasks.Evaluating LLM performance in this context often requires a large number of human experts, demanding an expensive interdisciplinary collaboration between machine learning and legal expertise to ensure inter-annotator agreement [15].</p>
<p>In this work, we investigate the automatic evaluation of legal open-ended questions using LLMs as judges.We postulate that frontier models can achieve a high correlation with human judges on standardized legal writing exams.To investigate this, we propose the oab-bench, as illustrated in Figure 1.This benchmark comprises 105 questions across seven areas of law, derived from the written portion of the Brazilian Bar Exam.Using the frontier model o1 [24] as a LLM judge, we generated scores for human responses and compared them with scores provided by human judges for the same questions.Our results demonstrated a strong correlation between the scores assigned by the LLM and those assigned by human judges.</p>
<p>The main contributions of this work are twofold: (1) the oabbench, a benchmark for evaluating legal writing skills of LLMs, addressing the growing need for challenging benchmarks, particularly in the legal domain given the emergence of specialized LLMs [5,8,9]; and (2) an automated evaluation pipeline for this benchmark, supported by our findings that show frontier LLMs, despite the inherently subjective and interpretative nature of the legal writing task, can serve as reliable judges closely aligned with human expert judgment.</p>
<p>Related Work</p>
<p>In this section, we present studies related to our research proposal.First, we discuss legal benchmarks, focusing on test suites commonly used to evaluate models in closed-ended tasks.We then present studies on the evaluation of open-ended tasks, highlighting their key applications in education and industry with LLM as a judge.</p>
<p>Automatic evaluation of LLMs on legal domain</p>
<p>The rise of domain-specific LLMs trained on legal data [5,8,9] has created a growing need for benchmarks that can automatically evaluate their performance in the legal domain [4,15,18].Among these, LegalBench [18] stands out for its interdisciplinary collaboration between machine learning experts and legal professionals, ensuring the inclusion of tasks that reflect real-world legal practice.</p>
<p>This benchmark comprises 162 tasks, each with at least 50 samples.</p>
<p>In particular, seven of these tasks involve open-ended generation, which required evaluation by legal professionals.However, human evaluation can be inconsistent across different test scenarios, making it labor-intensive to standardize performance assessment.In Chiang et.al [6], the use of LLMs as judges for openended story generation and adversarial attacks has been explored.The authors found a correlation between the scores assigned by human evaluators and those given by LLM judges, suggesting that LLMs could replace human evaluation in certain scenarios.</p>
<p>In this context, does this correlation hold for specialized domains such as the legal field?The experiments conducted in this work aim to address this gap.Additionally, to the best of our knowledge, we are the first to provide a dataset of open-ended questions of standardized exams in the Brazilian legal domain.</p>
<p>Automatic evaluation of open-ended tasks</p>
<p>While previous studies have identified a correlation between human evaluators and LLM judges in open-ended task evaluations [6,16,21], automatic evaluation still presents limitations that may introduce inconsistencies, such as position bias [30] and hallucinations [29].To address these challenges, recent research has explored techniques to enhance the correlation between human and LLM judges through prompting techniques, such as chain of thought prompting [21] and Retrieval-Augmented Generation (RAG) [29].</p>
<p>In particular, Ryu et al. [29] introduced a framework that uses RAG for the automatic evaluation of open-ended legal QA.Their findings indicated that incorporating relevant legal documents into the evaluation process enhanced the LLM judge alignment with human legal experts.Notably, the LLM judge successfully identified factual errors that were previously overlooked without RAG.This work explores a similar context by evaluating open-ended legal exam questions.However, we hypothesize that alignment with human scores can be further improved by just employing a robust model, such as o1.</p>
<p>Beyond the legal domain, the automatic evaluation of openended tasks impacts educational research, where LLMs are used to automatically evaluate essays [22,31] and provide continuous feedback to learners of a new language [2,14].Additionally, LLMs have been used to evaluate open-ended tasks generated by other LLMs [23], similar to this work.The evaluation of LLM performance as chatbots also relies on the assessment of open-ended tasks, as demonstrated in chatbot benchmarks such as AlpacaEval [13].This approach proves particularly effective in generating cost-effective performance signals for reinforcement learning, contributing to the development of post-training datasets such as UltraFeedback [10].</p>
<p>Methodology</p>
<p>One of the key challenges in evaluating LLMs on domain-specific tasks is finding suitable test datasets that are publicly available, frequently updated to avoid potential training data contamination, and contain comprehensive answer keys and evaluation guidelines to enable automatic evaluations.The Brazilian Bar Association (OAB) Examination meets these requirements.In this section, we describe our methodology for leveraging these exams and evaluating LLMs as automated judges of legal responses.</p>
<p>OAB exam</p>
<p>The OAB examination is a mandatory professional qualification test that all law graduates must pass to practice law in Brazil.The exam is conducted three times a year and consists of two phases: the first phase is an objective test with 80 multiple-choice questions, and the second phase is a dissertative test, requiring candidates to demonstrate their practical legal knowledge through open-ended responses.More than 100 thousand candidates take the exam every edition, and 20% to 45% are approved. 3The OAB examination is organized by the Fundação Getúlio Vargas (FGV).More information on all editions can be accessed through the FGV's official website for the OAB 4 .</p>
<p>In this work, we focus on the second phase of the OAB exam, which comprises a legal essay (peça prático-profissional) worth 5 points and four discursive questions worth 1.25 points each, totaling 10 points.For the legal essay, the candidates must first correctly identify the appropriate type of legal document required based on the case presented, and then write that document, which could be a lawsuit, an appeal, a contract, or any other legal instrument relevant to their chosen law area.The structure of discursive questions comprises two sub-questions labeled A and B.</p>
<p>The exam is offered in seven areas of law, with candidates choosing their specialization during registration: Administrative, Civil, Constitutional, Labor, Business, Criminal, and Tax law.The candidates must achieve a minimum score of 6 points to be approved.</p>
<p>To ensure a standardized evaluation, the examination board provides two key evaluation guidelines: (1) a commented answer with comprehensive explanations of expected legal reasoning and relevant legislation, and (2) a score distribution table that breaks down how scores should be allocated for specific elements in both the essay and the questions.These materials guide examiners in maintaining consistency across evaluations while allowing for flexibility in recognizing alternative valid legal arguments.Figure 2 presents an example of those evaluation guidelines (translated into English).</p>
<p>The oab-bench</p>
<p>We introduce oab-bench, a benchmark comprising questions and reference materials from three recent editions of the OAB exam (39th, 40th, and 41st), spanning from 2023.3 to 2024.2.We emphasize that the second phase of these three editions was administered in the year 2024; for instance, the second phase of the 39th edition, which is associated with 2023.3, was actually held at the beginning of 2024.We chose recent editions to minimize the risk of data contamination in open-source and proprietary LLMs, as these exams were conducted after the known training cutoff dates of the models.</p>
<p>The dataset includes exams from all the seven areas of law covered in the tests.The total number of questions is 105, covering 3 editions × 7 areas × 5 questions per exam.The oab-bench includes all the evaluation guidelines needed to judge each question:</p>
<p>• The complete question statement • The maximum score possible for that question • Official commented answers and score distribution table</p>
<p>Commented Answer</p>
<p>The question requires the examinee to demonstrate knowledge about extinction of punishability and criminal review.</p>
<p>A) No, given the principle of non-transferability of punishment or personal responsibility or personality or non-transmissibility of punishment, the death of the convicted person extinguishes punishability, according to Art. 107, item I, of the Criminal Code, or Art. 5, item XLV, of the Federal Constitution or Article 5, item 3, of the American Convention on Human Rights -the Pact of San José, Costa Rica (approved by Decree 678/92).B) Janaína can request a criminal review on behalf of her father, according to Art. 621, item III, or Art.623, both from the Criminal Procedure Code.The oab-bench is available at https://huggingface.co/datasets/ maritaca-ai/oab-bench.</p>
<p>Score Distribution</p>
<p>LLM as a Candidate.</p>
<p>The first stage of our evaluation pipeline involves using LLMs to generate answers to the OAB exam questions.We provide to the model the same instructions given to actual exam takers.Figure 3 presents the prompt translated into English.</p>
<p>In our experiments we use the temperature of 0.7 to generate model answers.</p>
<p>LLM as a Judge.</p>
<p>In this work we use a strong LLM as an examiner to judge model answers [7,32] following the analytical grading process applied in the real world.The judge model receives:</p>
<p>(1) the question statement, (2) the maximum possible score for the question, (3) the reference material with the commented answer and the score distribution table, and (4) the answer to be evaluated.The judge then analyzes the candidate's answer against these reference materials to assign appropriate scores based on the demonstrated knowledge and reasoning.</p>
<p>To ensure that the LLM judge behaves similarly to human examiners in the grading process, we investigated online sources (primarily videos) created and shared by legal experts, preparatory course instructors, and candidates approved in previous editions.From these sources, we extracted valuable information to understand how the grading process works in practice, and prepare the judge prompts reflecting those details. Figure 4 presents the resulting prompt template translated into English.</p>
<p>We use the combination of two judge modes initially proposed by Zheng et al. [32]: single-answer and reference-guided grading modes.In this scenario, the LLM judge is asked to directly assign a score to a single answer, with access to all the available guidelines or You are a law graduate taking the second phase of the Brazilian Bar Association (OAB) exam, organized by FGV.Your task is to answer the essay questions and prepare a legal document, demonstrating your legal knowledge, reasoning ability, and skill in applying relevant legislation and jurisprudence to the presented case.</p>
<p>ATTENTION</p>
<p>When preparing the texts for the practical-professional document and answers to the essay questions, you must include all necessary data without producing any identification or information beyond what is provided and permitted in the statements contained in the exam booklet.The omission of data that is legally required or necessary for the correct solution of the proposed problem will result in point deductions.You must be careful not to generate any different data that could create an identifying mark.</p>
<p>The detection of any identifying mark in the space designated for the transcription of the final texts will result in the annulment of the practical-professional exam and your elimination.For example, when closing the document, you should opt to use only "ellipsis" or "XXX", that is: date "..." or Date "XXX", location "..." or Location "XXX", Attorney "..." or Attorney "XXX", OAB registration "..." or OAB Registration "XXX".Note that in the body of your answers, you should not create any data that generates an identification mark.OBSERVATIONS PRACTICAL-PROFESSIONAL DOCUMENT: The document must cover all legal grounds that can be used to support the claim.Simply mentioning or transcribing the legal provision does not earn points.QUESTION: You must provide reasoning for your answers.Merely citing the legal provision does not earn points.</p>
<p>From now on, all your answers will compose the final text (not the draft booklet) reference answers.Here we use the commented answers and score distribution table to guide the judge (see Section 3.1 for details).</p>
<p>Additionally, to evaluate the second sub-question (labeled as B) of each discursive question, we apply a multi-turn judge prompt.This is important because sub-questions are often interconnected, where the answer to item B may build upon or reference information provided in item A. By implementing a multi-turn approach, we ensure the judge has the complete context -both the statement and response from item A -when evaluating item B.</p>
<p>The multi-turn mode is complementary to the single-answer mode, as the model still gives a score to a unique answer; and to reference-guided mode, as the judge receives the evaluation guidelines.</p>
<p>The multi-turn judge prompt is similar to the one illustrated in Figure 4, except it emphasizes that the model must judge only the candidate's response to the second sub-question, and both the candidate's responses and evaluation guidelines are presented as two separate conversation histories -one containing the candidate's responses to parts A and B, and another containing the corresponding evaluation guidelines.</p>
<p>We use o1-2024-12-17 [26] as the LLM judge, a reasoning model released by OpenAI.O1 was designed to solve complex and critical problems by producing a long chain of thought before responding to the user.The disadvantage is that the model requires more compute to generate several tokens in the thinking stage, reflecting in the cost to the final users.</p>
<p>We experimented with GPT-4o [25] as an alternative judge model.However, we observed several limitations that made it unsuitable: it assigned out-of-range scores, failed in computing final scores, and
[Instruction]
You are part of the FGV examining board responsible for evaluating law graduates' answers in the second phase of the OAB exam.The OAB exam grading is analytical.Assign points for each topic covered, considering not only the final result but also legal reasoning, legal basis, and clarity in presenting arguments.If the candidate presents a partially correct solution but hasn't fully developed the reasoning, it's possible to award a portion of the points for the correct part of the work.This principle aims to value demonstrated legal knowledge, even if the answer isn't perfect.As a reference to evaluate the answer, use the OAB answer standard, composed of the commented answer and the points distribution table.The commented answer provides examples of valid answers and criteria to be met.The table shows possible scores for each item according to the parts covered.Each item consists of parts, and each part has a score.For example, items representing legal theses are composed of: presentation, argumentation, and legal basis.The SCORE column presents the range of values, from minimum to maximum, for each item.Begin your evaluation by comparing the candidate's answer with the commented answer.Identify and correct any divergences, striving to be as objective as possible.</p>
<p>Then, analyze each item in the points distribution table, verifying the convergence between the table and the candidate's answer.For each item, check if each part was addressed in the answer, assign 0 or the full score for the respective part (binary evaluation), and provide the accumulated score for the item.Try not to penalize the candidate when they include articles beyond those required.If the participant references the correct article but their argumentation is incorrect, it's prudent to assign a score of 0.0 for both parts.On the other hand, if the argumentation is coherent and well-constructed but supported by an incorrect article, the argumentation score should be counted.After analyzing all items, sum up the candidate's total score.Provide the final score, between 0.0 and {max_score}, using the following format: "[[score]]".For example, if you assign a score of {max_score}, the format would be " [  applied inconsistent score scaling that did not match the required intervals.DeepSeek-R1 [12], another reasoning model, could be a good alternative as its performance is comparable with o1 and it is open source.However, we observed that it occasionally failed at evaluating the two sub-questions when it should focus only on the last one in multi-turn evaluation.These issues led us to choose o1 as our judge model.We show some of those errors in Section 4 and Appendix A.</p>
<p>Evaluating a model on oab-bench requires 189 API requests, since we have 21 exams, and each exam needs 9 individual evaluations (1 for the essay and 2 for each discursive question).The cost with the o1 judge model ranges from $50 to $55 USD.</p>
<p>To generate model judgments, for GPT-4o we use temperature 0, while reasoning models (o1 and DeepSeek-R1) require temperature 1.0 as their only supported value.</p>
<p>All the code needed to reproduce the experiments, including the judge prompts and the evaluation pipeline, are available at https://github.com/maritaca-ai/oab-bench.</p>
<p>Comparison of Judges: Human versus LLM</p>
<p>To assess whether the LLM judge effectively performs its role as an examiner, we evaluated real human-written responses using the model.We collected a set of actual answers from law students, already evaluated by human examiners, manually transcribed them from handwritten format, and assessed them independently with the LLM judge.This allowed us to measure the correlation between human and automated scoring, providing insights into the reliability of our automated evaluation approach.</p>
<p>The main challenge in this evaluation was obtaining real exam responses, as there is a scarcity of publicly available graded exams.We addressed this by collecting student responses and their corresponding official scores from materials voluntarily shared on a preparatory course website 5 by candidates approved in the exam.These materials were available as photographs of handwritten answer booklets, which required careful manual transcription to create machine-readable versions.</p>
<p>While these exams are from older editions, this does not significantly impact our evaluation.The main reason is that the content was shared as images in handwritten format, making it unlikely that the judge model was exposed to this content during training.Even if the model had seen these materials, the risk would be limited to potentially giving higher scores to answers it recognized, but the handwritten image format significantly reduces this possibility.</p>
<p>Despite the relatively small number of exams, we applied two main selection criteria: (1) using at most one exam per area, and ( 2 4 Results</p>
<p>Table 1 presents the results of four open-source and proprietary</p>
<p>LLMs [1,3,25,27] on the oab-bench.For each of the 21 exams, the table shows the total score achieved by each model, with scores ranging from 0 to 10. Scores in red indicate failing grades (below 6.0), which is the minimum passing score required in the actual OAB exam.Additionally, Figure 5 shows the average performance of each model for each law area.</p>
<p>It is important to note that these scores reflect the LLM judge's evaluation and may not align perfectly with how human examiners would grade the same responses.A more comprehensive study involving official examiners would be needed to validate the alignment between LLM and human scoring.</p>
<p>Claude-3-5 Sonnet outperformed other models, achieving an average score of 7.93 across all exams and passing all 21 evaluations.The model excelled particularly in Constitutional law and Criminal law, with average scores of 8.43 and 8.33, respectively.</p>
<p>GPT-4o performed second best, with an average score of 6.87 and approval in 18 exams.The model performed particularly well in Civil and Constitutional law, scoring an average of 7.42 in both areas.GPT-4o struggled in Business law, with a marginal score of 6.02.Sabiá-3 ranked third with an average score of 6.55 (0.32 below GPT-4o), passing 16 of 21 exams.The model showed consistent performance across different law areas, with strong results in Labor law (7.17average).However, similarly to GPT-4o, Sabiá-3 struggled in Business law, achieving an average score of 5.82 -the only area below the approval threshold.</p>
<p>Qwen2.5-72B Instruct showed the lowest performance, with an average score of 5.21 and passing only on 5 exams.</p>
<p>Evaluating Human Responses</p>
<p>This section applies the evaluation pipeline of the oab-bench benchmark over real exams collected from the internet (see Section 3.3).</p>
<p>Here we evaluate the performance with LLM Judges and compare with human examinees.Table 2 presents the comparison between human and LLM judges across three different exams.We use Mean Absolute Error (MAE) to During multi-turn evaluation, two LLM judges showed scoring inconsistencies.In the Criminal law exam (question 1), GPT-4o assigned 2.0 points, exceeding the maximum possible score of 1.25.Similarly, in the Labor law exam (question 2), DeepSeek-R1 had to scale its scores to fit within the maximum limit.These inconsistencies occurred because both models failed to follow the explicit instructions in the multi-turn prompt.Instead of evaluating only the response to sub-question B within its specified maximum score, the models incorrectly evaluated responses from both turns A and B combined.</p>
<p>Overall, the o1 judge demonstrated reasonable alignment with human scoring, though with some notable variations.O1 model showed slightly better consistency, with MAE ranging from 0.04 to 0.28, while GPT-4o's MAE ranged from 0.20 to 0.55, and DeepSeek-R1's MAE ranged from 0.12 to 0.27.</p>
<p>In the Criminal law exam, the o1 model closely matched the human-graded perfect score of 10.0, while for the Civil law exam, it assigned 7.50, outperforming the human grade of 6.10.Finally, for the Labor law exam, the o1 model achieved identical total scores, although with slight variations in the distribution across individual questions resulting in a MAE of 0.24.</p>
<p>Considering only the legal essays, the o1 judge tends to assign higher scores than human examiners.These results suggest that while LLM judges can approximate human scoring patterns, they may require additional constraints or calibration to consistently respect scoring boundaries and maintain human-level precision in borderline cases.</p>
<p>Appendix A provides a more detailed comparison of judges behavior focused on legal essays.</p>
<p>Limitations</p>
<p>Our examinee prompt contains only the standard instructions found in the exam booklet.However, we did not enforce line limits in our model prompts that match the constraints given to human candidates (150 lines for essays and 30 lines per question).Incorporating these constraints could help reduce model verbosity.</p>
<p>Additionally, consulting with specialists who are part of the examination body, rather than relying solely on video materials on internet, would enhance the judging criteria with valuable domain expertise.This direct expert input could lead to more precise evaluation guidelines and ensure alignment with current examiners.</p>
<p>With regard to automatic judging, the high cost is a significant obstacle.As of this publication, evaluating a single model on oabbench costs between 50 and 55 USD, which limited our analysis to only four LLMs.The DeepSeek-R1 could be a promising costeffective alternative, and with the rapid development of open-source models, we anticipate that future versions may offer even better performance-to-cost ratios.</p>
<p>Although we measure the correlation of human and LLM judges on real human-written exam responses, the analysis lacks the evaluation of tests that would be reproved.</p>
<p>Furthermore, the current LLM judge pipeline lacks automated validation mechanisms for several critical aspects: there is no automatic verification of score summation accuracy, no system to check if individual item scores fall within their permitted ranges, and no validation to ensure the model properly accounts for allowed variations in article combinations.Implementing such verification systems would add an important layer of quality control to the evaluation process.Our experiments revealed that GPT-4o occasionally failed in some of these aspects.While such errors were not observed with the o1 judge, we acknowledge that our manual validation was not exhaustive enough to guarantee the complete absence of errors.</p>
<p>Conclusion</p>
<p>In this work, we introduced oab-bench, a benchmark comprising 105 questions across seven areas of law from recent editions of the Brazilian Bar Exam for evaluating the capacity of LLMs in legal writing tasks.Our experiments demonstrated that strong LLMs can achieve considerable performance in this domain, with Claude-3-5 Sonnet passing all 21 exams with an average score of 7.93.</p>
<p>We also investigated the feasibility of using LLMs as automated judges for evaluating open-ended tasks on legal domain.Our results showed that the o1 model achieved reasonable alignment with human scoring when evaluating approved exams across different areas of law.This finding is particularly relevant, as subjective tasks in this domain require a large number of human judges to achieve high inter-annotator agreement.</p>
<p>However, we acknowledge that a more extensive study would be required to fully validate LLM judges as reliable examiners or potential copilots for human evaluators.For example, testing with low-grade responses, which were not available in our collected dataset, could provide more robust comparisons between human and LLM judges.Furthermore, having official examiners to evaluate model-generated responses would improve judge guidelines and achieve a more accurate assessment of LLM performance.</p>
<p>Also, the high cost of using frontier models like o1 remains a significant limitation, although the rapid development of opensource alternatives like DeepSeek-R1 suggests more cost-effective solutions may emerge.</p>
<p>Future work should focus on implementing automated validation mechanisms to ensure scoring accuracy and developing more sophisticated and domain-related prompting techniques that incorporate valuable specialist's feedback.The oab-bench and our findings contribute to the broader field of automated evaluation of open-ended tasks in specialized domains.</p>
<p>A Disagreement Analysis in Legal Essays</p>
<p>Tables 3 and 4 show, respectively, a summarized outline of a grading rubric for Civil Law and Labor Law documents written by humans.In addition to the scores assigned by human evaluators, the table also shows the scores given by LLM judges.We emphasize that the official point distribution table, and consequently the official grading rubric, are more detailed.Here we present a simplified version due to space constraints.In comparing LLM evaluations, we observed that GPT-4o made significant arithmetic errors when summing individual scores in both analyses.Although GPT-4o agreed with o1 on all items of the Criminal law essay, it incorrectly reported a total of 4.60 instead of 4.80.Similarly for the Labor Law essay, it reported 3.50 when the correct sum was 3.70.These basic calculation errors raise concerns about using GPT-4o as a reliable judge for oab-bench model responses.</p>
<p>When comparing Human vs. LLM judgments (o1), we found divergence on only one item per essay.DeepSeek-R1 agreed with o1's assessment on these two discrepancies.Upon analyzing these disagreements, the LLM judges' interpretations appear reasonable and well-justified based on the grading criteria.</p>
<p>Like LLMs, human evaluators are also susceptible to errors in their assessments.The subjective nature of grading legal essays makes them particularly vulnerable to human factors, such as misinterpreting handwriting, evaluating while fatigued, or confusing elements due to the high volume of exams they must grade.These human limitations can lead to inconsistent or incorrect evaluations, potentially impacting candidates' outcomes.</p>
<p>According to a private law preparatory course that offers a service for grade increase analysis and personalized appeals, 41% of appeals filed through their service for the 40th bar exam were successful.In other words, 1 in 3 appeals reversed a failing grade [11].These appeals stem from human grading errors.</p>
<p>Figure 3 :
3
Figure 3: Prompt used to instruct the LLM to act as an examinee for the OAB Exam.The prompt includes the same guidelines given to the candidates in the application of the exam.</p>
<p>Figure 4 :
4
Figure 4: Prompt template used to instruct the LLM to act as an examiner for the OAB Exam.The prompt explains the analytical grading process and stablishes the format of a final score.</p>
<p>) seeking responses with different score ranges.Our final selection included: • A Criminal law exam from the 15th edition with perfect scores (10.0/10.0)• A Civil law exam from the 27th edition with an intermediate passing score (6.1/10.0)• A Labor law exam from the 28th edition with a comfortable passing score (8.15/10.0)</p>
<p>Figure 5 :
5
Figure 5: Performance of each model across different areas of law.</p>
<p>No, given the principle of non-transferability of punishment or personal responsibility or personality or non-transmissibility of punishment (0.20), the death of the convicted person extinguishes punishability (0.35), according to Art. 107, item I, of the Criminal Code, or Art. 5, item XLV, of the Federal Constitution or Article 5, item 3, of the American Convention on Human Rightsthe Pact of San José, Costa Rica (approved by Decree 678/92) (0.10).
ITEMSCOREA) 0.00/0.20/0.30 0.35/0.45/0.550.65
B) Criminal review is admissible even after the death of the convicted person (0.50), based on Art.621, item III, or Art.623, both from the Criminal Procedure Code (0.10).0.00/0.50/0.60Figure2:Example of a commented answer and a score distribution table from the Criminal Law exam (edition 41, question 4).The table shows how the scores are allocated for each item and its parts, with some items accepting multiple valid legal articles as basis.</p>
<p>Table 1 :
1
Results of LLMs on the oab-bench benchmark.Scores in red indicate failing grades.
Edition ExamQwen2.5-72BClaude-3-5GPT-4oSabiá-3InstructSonnetAdministrative law7.008.908.307.35Civil law5.308.357.006.20Constitutional law4.358.056.456.2039thLabor law4.507.456.106.55Business law4.757.054.656.20Criminal law5.757.958.157.80Tax law6.558.406.957.15Administrative law4.908.007.005.50Civil law7.108.907.758.25Constitutional law4.809.308.407.0540thLabor law6.108.307.108.10Business law3.956.906.005.90Criminal law3.757.505.354.95Tax law3.456.756.104.75Administrative law4.206.953.606.15Civil law4.057.557.506.55Constitutional law5.807.957.406.9041stLabor law5.057.657.006.85Business law5.958.607.405.36Criminal law6.859.558.607.25Tax law5.256.507.556.45Mean5.217.936.876.55</p>
<p>Table 2 :
2
Results Breakdown  − ŷ |, where   represents the human score and ŷ represents the LLM judge score for the -th item, providing a straightforward measure of scoring discrepancy.
JudgeLegal essay Question 1 Question 2 Question 3 Question 4 TotalMAE5.001.251.251.251.2510.00Criminal lawHuman5.001.251.251.251.2510.00-(15th edition)LLM (o1)4.801.251.251.251.259.80 (-0.2)0.04LLM (GPT-4o)4.602.001.251.251.2510.35 (+0.35) 0.23LLM (DeepSeek-R1)4.401.251.251.251.259.40 (-0.6)0.12Civil lawHuman3.800.550.500.500.756.10-(27th edition)LLM (o1)4.900.650.500.500.957.50 (+1.4)0.28LLM (GPT-4o)5.000.001.050.750.957.75 (+1.65)0.55LLM (DeepSeek-R1)4.300.550.501.250.857.45 (+1.35)0.27Labor lawHuman3.901.250.601.151.258.15-(28th edition)LLM (o1)4.301.250.701.250.658.15 (+0)0.24LLM (GPT-4o)3.501.250.701.150.757.35 (-0.8)0.20LLM (DeepSeek-R1)4.301.250.341.250.657.79 (+0.36)0.272measure the average magnitude of differences between human andLLM judge scores. MAE is calculated as 1 𝑛𝑛 𝑖=1 |𝑦</p>
<p>Table 3 :
3
Analytical Evaluation of Criminal Law.The GPT-4o judge made an error in summing the individual scores, reporting 4.60 when the correct total would be 4.80.
ItemValue Humano1GPT-4o DeepSeek-R11. Correct addressing: Criminal Special Court of Niterói0.100.100.100.100.102. Correct indication of legal provision supporting the crim-0.100.100.100.100.10inal complaint3.1. Qualification of complainant and defendant0.200.200.200.200.203.2. Existence of Power of Attorney with special powers0.300.300.300.300.304.1. Exposition of criminal facts0.600.600.600.600.604.2. Description of defamation offense and typical classifi-0.600.600.600.600.60cation4.3. Incidence of penalty enhancement cause0.300.300.300.300.304.4. Incidence of formal concurrence of crimes0.400.400.400.400.105.a. Designation of preliminary or conciliation hearing0.200.200.000.000.005.b. Summons of the defendant0.200.200.200.200.205.c. Acceptance of the complaint0.200.200.200.200.205.d. Hearing of listed witnesses0.200.200.200.200.205.e. Conviction of the defendant0.900.900.900.900.805.f. Setting minimum compensation value0.400.400.400.400.406. Witness list: List Carlos, Miguel and Ramirez0.200.200.200.200.207. Correct structure0.100.100.100.100.10Total5.005.004.804.604.40</p>
<p>Table 4 :
4
Analytical Evaluation of Labor Law.The GPT-4o judge made an error in summing the individual scores, reporting 3.50 when the correct total would be 3.70.
ItemValue Humano1GPT-4o DeepSeek-R11. Complaint addressed to Criminal Court of Cuiabá0.100.100.100.100.102. Qualification of parties0.200.200.200.200.203. Indication of Art. 847 of CLT0.100.100.100.100.104. Preliminary motion of ineptitude for hazard pay claim0.500.500.500.500.505. Partial statute of limitations0.500.500.500.500.506. Denial of occupational illness compensation0.600.000.000.000.007. Dental plan not part of salary0.500.500.500.500.508. Food allowance not due for lack of ultra-activity0.500.500.500.500.509. Time at disposal (ecumenical service)0.500.500.500.500.5010. Resignation request / Coercion allegation0.500.000.400.000.4011. Functional accumulation0.500.500.500.500.5012. Renewal of preliminary motion in closing0.100.100.100.000.1013. Renewal of partial statute of limitations0.100.100.100.000.1014. General request for dismissal and evidence indication0.200.200.200.200.2015. Closing0.100.100.100.100.10Total5.003.904.303.504.30
https://github.com/maritaca-ai/oab-bench
https://huggingface.co/datasets/maritaca-ai/oab-bench
https://oab.estrategia.com/portal/estatisticas-completas-do-exame-de-ordem-daoab/
4 https://oab.fgv.br/
https://www.provadaordem.com.br/blog/post/provas-resolvidas-2a-fase-edepoimentos-de-aprovados-na-oab/</p>
<p>Roseval Malaquias Junior, Giovana Kerche Bonás, Rodrigo Nogueira, and Ramon Pires. Hugo Abonizio, Thales Sales Almeida, Thiago Laitz, arXiv:2410.12049[cs.CL2024Sabiá-3 Technical Report</p>
<p>ChatGPT over my friends: Japanese English-as-a-Foreign-Language learners' preferences for editing and proofreading strategies. Allen Todd, Atsushi Mizumoto, RELC Journal. 003368822412625332024. 2024</p>
<p>. Anthropic, 2024Introducing Claude 3.5 Sonnet</p>
<p>BLT: Can Large Language Models Handle Basic Legal Text. Andrew Blair-Stanek, Nils Holzenberger, Benjamin Van Durme, Proceedings of the Natural Legal Language Processing Workshop 2024. the Natural Legal Language Processing Workshop 20242024</p>
<p>Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. Ilias Chalkidis, Manos Fergadiotis, Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics2020LEGAL-BERT: The Muppets straight out of Law School</p>
<p>Can Large Language Models Be an Alternative to Human Evaluations. Cheng- , Han Chiang, Hung-Yi Lee, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, 2023. 14 April 2023. 2023</p>
<p>Pierre Colombo, Telmo Pires, Malik Boudiaf, Rui Melo, Dominic Culver, Sofia Morgado, Etienne Malaboeuf, Gabriel Hautreux, Johanne Charpentier, Michael Desa, arXiv:2407.19584[cs.CL]SaulLM-54B &amp; SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain. 2024</p>
<p>Pierre Colombo, Pessoa Telmo, Malik Pires, Dominic Boudiaf, Rui Culver, Caio Melo, Corro, F T Andre, Fabrizio Martins, Vera Esposito, Sofia Lúcia Raposo, Michael Morgado, Desa, arXiv:2403.03883[cs.CL]SaulLM-7B: A pioneering Large Language Model for Law. 2024</p>
<p>Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, Maosong Sun, arXiv:2310.01377[cs.CL]UltraFeedback: Boosting Language Models with Scaled AI Feedback. 2024</p>
<p>Prova Da, Ordem , Recurso Personalizado para 2ª Fase OAB 41º Exame. 2025</p>
<p>-Ai Deepseek, Daya Guo, arXiv:2501.12948[cs.CL]DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. 2025</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv:2404.04475[cs.LG]Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators. 2024</p>
<p>AI-generated feedback on writing: insights into efficacy and ENL student preference. Juan Escalante, Austin Pack, Alex Barrett, International Journal of Educational Technology in Higher Education. 20572023. 2023</p>
<p>Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, Jidong Ge, arXiv:2309.16289[cs.CL]LawBench: Benchmarking Legal Knowledge of Large Language Models. 2023</p>
<p>GPTScore: Evaluate as You Desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American Chapterthe Association for Computational Linguistics20241</p>
<p>Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily De Oliveira, Santos, arXiv:2411.04872Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai. 2024. 2024arXiv preprint</p>
<p>LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models. Neel Guha, Julian Nyarko, Daniel Ho, Christopher Ré, Adam Chilton, Aditya K , Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, Dmitry Talisman, Enam Hoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gregory Dickinson, Haggai Porat, Jason Hegland, Jessica Wu, Joe Nudell, Joel Niklaus, John Nay, Jonathan Choi, Kevin Tobia, Margaret Hagan, Megan Ma, Michael Livermore, Nikon Rasumov-Rahe, Nils Holzenberger, Noam Kolt, Peter Henderson, Sean Rehaag, Sharad Goel, Shang Gao, Spencer Williams, Sunny Gandhi, Tom Zur, Varun Iyer, Zehua Li, Advances in Neural Information Processing Systems. 202336</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300[cs.CY]Measuring Massive Multitask Language Understanding. 2021</p>
<p>SWE-bench: Can Language Models Resolve Real-world Github Issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, ; Karthik R Narasimhan, The Twelfth International Conference on Learning Representations. Ofir Press2024</p>
<p>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Exploring the potential of using an AI language model for automated essay scoring. Atsushi Mizumoto, Masaki Eguchi, Research Methods in Applied Linguistics. 21000502023. 2023</p>
<p>Aidar Myrzakhan, Sondos , Mahmoud Bsharat, Zhiqiang Shen, arXiv:2406.07545[cs.CL]Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena. 2024</p>
<p>Introducing OpenAI o1. 2024. 26 January 2025OpenAI</p>
<p>GPT-4o System Card. Aaron Openai, Hurst, arXiv:2410.21276[cs.CL2024. 2024</p>
<p>OpenAI o1 System Card. Aaron Openai, Jaech, arXiv:2412.16720[cs.AI2024. 2024</p>
<p>. An Qwen, Yang, arXiv:2412.15115[cs.CL2025. 2025Qwen2.5 Technical Report</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.12022[cs.AI]GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark. 2023</p>
<p>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA. Cheol Ryu, Seolhwa Lee, Subeen Pang, Chanyeol Choi, Hojun Choi, Myeonggee Min, Jy-Yong Sohn, Proceedings of the Natural Legal Language Processing Workshop 2023. the Natural Legal Language Processing Workshop 20232023</p>
<p>Large Language Models are not Fair Evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, Zhifang Sui, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Rating Short L2 Essays on the CEFR Scale with GPT-4. Kevin P Yancey, Geoffrey Laflair, Anthony Verardi, Jill Burstein, Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications. the 18th Workshop on Innovative Use of NLP for Building Educational Applications2023. 2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362023. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
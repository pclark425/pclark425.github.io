<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6484 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6484</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6484</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-3e0b72b9b61114d97095d49931cb7620d81fc29c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3e0b72b9b61114d97095d49931cb7620d81fc29c" target="_blank">Probing Large Language Models in Reasoning and Translating Complex Linguistic Puzzles</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper Abstract:</strong> This paper investigates the utilization of Large Language Models (LLMs) for solving complex linguistic puzzles, a domain requiring advanced reasoning and adept translation capabilities akin to human cognitive processes. We explore specific prompting techniques designed to enhance ability of LLMs to reason and elucidate their decision-making pathways, with a focus on Input-Output Prompting (IO), Chain-of-Thought Prompting (CoT), and Solo Performance Prompting (SPP). Utilizing datasets from the Puzzling Machine Competition and various Linguistics Olympiads, we employ a comprehensive set of metrics to assess the performance of GPT-4 0603, a prominent LLM, across these prompting methods. Our findings illuminate the potential of LLMs in linguistic reasoning and complex translation tasks, highlighting their capabilities and identifying limitations in the context of linguistic puzzles. This research contributes significantly to the broader field of Natural Language Processing (NLP) by providing insights into the optimization of LLM applications for improved reasoning and translation accuracy, thereby enriching the ongoing dialogue in NLP advancements.</p>
                <p><strong>Cost:</strong> 0.002</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6484",
    "paper_id": "paper-3e0b72b9b61114d97095d49931cb7620d81fc29c",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00242925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Probing Large Language Models in Reasoning and Translating Complex Linguistic Puzzles</h1>
<p>Zheng-Lin Lin, Yu-Fei Shih, Shu-Kai Hsieh<br>National Taiwan University<br>b09208026@ntu.edu.tw, yfshih@nlg.csie.ntu.edu.tw, shukaihsieh@ntu.edu.tw</p>
<h4>Abstract</h4>
<p>This paper investigates the utilization of Large Language Models (LLMs) for solving complex linguistic puzzles, a domain requiring advanced reasoning and adept translation capabilities akin to human cognitive processes. We explore specific prompting techniques designed to enhance LLMs' ability to reason and elucidate their decision-making pathways, with a focus on Input-Output Prompting (IO), Chain-of-Thought Prompting (CoT), and Solo Performance Prompting (SPP). Utilizing datasets from the Puzzling Machine Competition and various Linguistics Olympiads, we employ a comprehensive set of metrics to assess the performance of GPT-4 0603, a prominent LLM, across these prompting methods. Our findings illuminate the potential of LLMs in linguistic reasoning and complex translation tasks, highlighting their capabilities and identifying limitations in the context of linguistic puzzles. This research contributes significantly to the broader field of Natural Language Processing (NLP) by providing insights into the optimization of LLM applications for improved reasoning and translation accuracy, thereby enriching the ongoing dialogue in NLP advancements.</p>
<p>Keywords: Machine Translation, Machine Reasoning, Large Language Models, Explainability, Linguistic Olympiad, Text Analytics</p>
<h2>1. Introduction</h2>
<p>The exploration of human cognitive systems has unveiled a fascinating dichotomy: System 1, responsible for quick, intuitive reactions, and System 2, which governs our capacity for complex reasoning, thereby operating at a slower pace and demanding more energy (Daniel, 2017). This bifurcation in cognitive processes is starkly evident in the realm of language translation. While the task of translating a single word may lean heavily on the rapid, reflexive capabilities of System 1, akin to a simple lookup in a dictionary, the translation of entire sentences plunges into the realm of System 2, requiring a deeper analytical engagement. This engagement is especially crucial in contexts where references are scant and the linguistic puzzle complex.</p>
<p>Within this framework, the Rosetta Stone Problem emerges as a quintessential challenge, embodying the essence of System 2 reasoning within linguistic puzzles. Featured prominently in the International Linguistics Olympiad ${ }^{1}$, this problem demands from its solvers not only the translation of texts between two languages with limited references but also the construction and application of a mini-grammar and vocabulary deduced from the given material (Bozhanov and Derzhanski, 2013). Such tasks underscore the profound complexity and the intricate cognitive engagement required, mirroring the deliberative, analytical processes characteristic of System 2 thought.</p>
<p>The advent of Large Language Models (LLMs)
has opened new vistas in addressing complex linguistic challenges, such as the Rosetta Stone Problem. This paper delves into the potential of LLMs to navigate these intricacies, with a particular focus on specific prompting techniques believed to significantly enhance the models' ability to translate with greater accuracy and elucidate their reasoning paths, paralleling human cognitive processes. By exploring the efficacy of these prompting strategies, we aim to illuminate the nuances of LLM reasoning, their potential for error detection and correction, and the implications for complex translation tasks. Through our examination, we seek not only to advance our understanding of LLM capabilities in complex linguistic reasoning and translation but also to contribute to the broader field of Natural Language Processing (NLP) by fostering innovative developments and optimizing LLM applications, particularly in linguistic reasoning and machine translation, thus enriching the ongoing dialogue in NLP advancements.</p>
<h2>2. Background</h2>
<p>The Puzzling Machine Challenge ${ }^{2}$ has been a significant landmark in showcasing the capabilities of LLMs, particularly with the application of methodology by Vamvas (2022) that utilized ChatGPT in conjunction with Input Output Prompting (IO) to achieve unprecedented success in solving Rosetta Stone Problems (İşgüder et al., 2020). This success highlights the potential of LLMs to outperform</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of Rule Contradiction in language Kabyle: GPT-4 breaches its own established rule, wherein 'gh' is designated to signify the first person past tense.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An example of dictionary contradiction within GPT-4's reasoning process using CoT Prompting on the Rosetta Stone Problem of Choctaw.
traditional methods in linguistic puzzles, setting a new benchmark for future research and application.</p>
<p>Translating within the constraints of Rosetta Stone problems, however, introduces substantial challenges. The structures of languages often diverge significantly, a fact that can lead to inaccuracies in the mini-grammars and dictionaries derived by solvers. This discrepancy poses a notable challenge, emphasizing the necessity for LLMs to not
only generate translations but also to navigate and correct errors in their initial linguistic assumptions (Bozhanov and Derzhanski, 2013).</p>
<p>Recent developments in prompting techniques have brought forth a variety of methods aimed at eliciting more sophisticated reasoning from LLMs. Among these, Chain-of-Thought (CoT), SelfConsistency (SC-CoT), Tree-of-Thought (ToT), and Solo Performance Prompting (SPP) stand out for</p>
<p>their potential to facilitate System 2-like cognitive processes in machines (Wei et al., 2023a; Wang et al., 2023a; Yao et al., 2023; Wang et al., 2023b). These techniques represent a significant stride toward enhancing the depth of reasoning LLMs can exhibit, providing a new lens through which the complexities of linguistic puzzles can be approached.</p>
<h2>3. Experiments</h2>
<p>In this section, we delineate our experimental setup designed to evaluate the reasoning capabilities of Large Language Models (LLMs) in addressing Rosetta Stone Problems. Our investigation centers on the application of GPT-4 0603 (hereafter GPT-4), a specific iteration of the GPT-4 model, to explore its performance across a carefully curated dataset using a variety of prompting methods.</p>
<h3>3.1. Dataset</h3>
<p>We utilized two datasets for our evaluation: one from the Puzzling Machine Challenge, as detailed by İşgüder et al. (2020), and another compiled from the United Kingdom Linguistics Olympiad (UKLO) ${ }^{3}$ and the North American Computational Linguistics Open Competition (NACLO) ${ }^{4}$. The Puzzling Machine Challenge dataset, featuring around 100 problems in 81 languages (İşgüder et al., 2020), was our primary source, from which we selected 86 unlabeled problems to test GPT-4's accuracy.</p>
<p>Our second dataset consists of 28 problems either directly obtained or adapted into Rosetta Stone puzzles from LO competitions. These problems were modified to match the format used in the Puzzling Machine Challenge, ensuring consistency in evaluation. Each problem is structured into a Meta section, providing crucial information on the foreign language, a Train Set of translation pairs for deriving rules, and a Test Set where one side of the pair is missing. An illustrative example of this format is shown in Figure 3.</p>
<h3>3.2. Prompting Methods</h3>
<p>Input-Output Prompting (IO) and Zero-Example Prompting (ZeroEx): IO prompting, as used by Vamvas (2022) with ChatGPT, introduces the task without detailed solving instructions, aiming for GPT-4 to generate answers without reasoning paths. ZeroEx Prompting, a variant of IO we modified which excludes example pairs to test if GPT-4 recognizes the language, focusing on raw answer generation.</p>
<p>Two-Phase reasoning strategy. Tailored to the unique challenges of Rosetta Stone Problems,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>this strategy directs GPT-4 through two reasoning phases. Initially, the model analyzes rules and vocabularies from examples, followed by rule application and potential revision in the test phase. This setup ensures comprehensive discussion of all language pairs and necessitates explanations for assumptions, facilitating an in-depth analysis of GPT-4's reasoning.</p>
<p>Chain-of-Thought Prompting: Enhancing the IO method, CoT introduces a step-by-step reasoning instruction, based on findings by Wei et al. (2023a) that such an approach improves LLM performance on reasoning-intensive tasks. We integrated CoT with the two-phase strategy, incorporating organized reasoning directives.</p>
<p>Multi-Experts Self-Collaboration: Drawing from Solo Performance Prompting (SPP) by Wang et al. (2023b), which simulates discussions among various personas, this method assigns GPT-4 as a facilitator to enhance engagement. Proven effective in pretests, it's adapted here to align with the two-phase reasoning approach, aiming to extract GPT-4's internal knowledge while maintaining its reasoning capabilities.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Problem format example of northern Algeria language, Kabyle, collected and refined from UKLO.</p>
<h2>4. Evaluation</h2>
<p>This section outlines the methodology adopted to evaluate GPT-4's proficiency in solving Rosetta Stone Problems, utilizing two distinct datasets: the Puzzling Machine Competition data and a dataset compiled from various Linguistic Olympiads (LO). Central to our investigation are two primary objectives: firstly, to assess the impact of various</p>
<p>prompting techniques on GPT-4's ability to generate reasoning paths that parallel human cognitive processes, and secondly, to illuminate the nuances of LLM reasoning, including its potential for error detection and correction. Given the Puzzling Machine provides an online judging system and the answers to all problems are not publicly accessible, our analysis will not examine the reasoning path of GPT-4 on this dataset.</p>
<p>We leveraged the evaluation tool made available on their official website ${ }^{5}$, as recommended by İşgüder et al. (2020). This approach ensures our assessment aligns with the competition's established metrics, facilitating a standardized evaluation of GPT-4's performance without delving into the reasoning paths due to the aforementioned constraints.</p>
<p>For translations from English to the Unknown Language:</p>
<ul>
<li>BLEU-2: A word-level based metric that assesses the quality of the generated text by comparing it with reference texts, using bigrams to provide a balance between precision and recall (Papineni et al., 2002).</li>
<li>characTER: A character-level metric designed to evaluate translation accuracy by considering edits at the character level, which is particularly useful for capturing finer linguistic nuances (Wang et al., 2016).</li>
<li>chrF: Another character-level metric that calculates F-scores based on character n-grams, facilitating a detailed assessment of translation quality (Popović, 2015).</li>
</ul>
<p>For translations from the Unknown Language to English: We utilized embeddings generated by the all-MiniLM-L6-v2 model from Sentence Transformers (Reimers and Gurevych, 2019) for the English sentences. The evaluation score is derived by calculating the cosine similarity (CosSim) between the embedding vectors of the generated and reference texts, offering a measure of semantic similarity.</p>
<h2>Overall Performance Evaluation:</h2>
<ul>
<li>Exact Match (EM): This metric assigns a score of 1 if GPT-4's generated expression exactly matches the reference expression, and 0 otherwise. EM is used to evaluate the model's performance across both translation directions, providing a direct measure of accuracy.</li>
</ul>
<p>By employing these metrics, our evaluation framework aims to comprehensively assess the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>capabilities of GPT-4 in solving Rosetta Stone Problems, taking into account both the precision of translation and the semantic accuracy of the generated text.</p>
<h2>5. Result</h2>
<p>In this section, we delve into the performance evaluation of GPT-4 regarding its ability to solve Rosetta Stone Problems across varying datasets and experimental conditions.</p>
<h3>5.1. Performance Analysis Using the Linguistics Olympiad Dataset</h3>
<p>The performance of different methodologies in the Linguistics Olympiad dataset is quantitatively assessed through Table 2, which presents the average scores computed by dividing the total scores for each query by the number of queries. This analysis reveals that the Information Ordering (IO) scores consistently surpass those achieved by other methods, while the ZeroEx approach lags behind in all metrics. A comparative evaluation of the SelfPaced Learning (SPP) and Chain of Thought (CoT) methods shows similar overall performance, with CoT yielding higher Exact Match (EM) scores and better performance in metrics related to the translation from English to unknown languages (CharacTER, ChF-3, and BLEU-2). Conversely, SPP performs better in the CosSim metric, indicating a superior ability to translate from unknown languages to English.</p>
<p>To mitigate potential biases introduced by languages with a higher query count in the LO dataset, we refer to Table 3. This table adjusts for language representation by first averaging the scores within each language and then averaging these across all languages. The findings align with those in Table 2, highlighting the superior performance of IO and the comparatively lower scores of ZeroEx. CoT demonstrates an advantage in EM and English-tounknown language translation metrics over SPP, while both methods show comparable performance in the CosSim score.</p>
<h3>5.2. Analysis Based on the Puzzling Machine Competition Dataset</h3>
<p>Table 1 details the performance metrics for SPP, CoT, and IO using data from the Puzzling Machine Competition. Despite differences in the metrics compared to those used for the LO dataset analysis, they convey analogous insights. Metrics prefixed with "FE" are designed to evaluate translations from unknown languages to English, whereas "EF" metrics assess translations from English to unknown languages. According to Table 1, IO outperforms</p>
<p>both SPP and CoT across all metrics. Interestingly, the dataset reveals a reversal in the comparative performance of SPP and CoT observed in the LO dataset analysis: SPP marginally outperforms CoT in EM and English-to-unknown language translation metrics, while CoT exhibits superior performance in translating from unknown languages to English.</p>
<h3>5.3. Analysis of GPT-4's Linguistic Proficiency Using the Linguistic Olympics Dataset</h3>
<p>In this study, we evaluated GPT-4's linguistic capabilities across various languages represented in the Linguistic Olympics Dataset, utilizing the ZenoEx metric as a primary analytical tool. Our analysis, grounded in five distinct metrics, revealed a consistent distribution pattern across all languages assessed. Consequently, we chose to highlight the CharacTER scores of the ZeroEx method for all 28 languages in Figure 4, as a representative illustration of the overarching trends observed with other metrics. This graphical representation allows for categorization of languages into three distinct tiers based on GPT-4's proficiency:</p>
<ul>
<li>Limited Proficiency Languages: This category includes Arhuaco, Iyo'awujwa, Paiwan, Sauk, and Yukhagir. For these languages, GPT-4 exhibited a complete lack of translation capability, indicating minimal to no knowledge.</li>
<li>High Proficiency Languages: Italian and Maori are placed within this tier. GPT-4 demonstrated a comprehensive understanding of these languages, accurately fulfilling translation requests without significant errors.</li>
<li>Moderate Proficiency Languages: The remainder of the languages fall into this intermediate category. While GPT-4 is capable of generating translations for these languages, the accuracy and correctness of the output vary, indicating a level of proficiency that falls between the two extremes outlined above.</li>
</ul>
<h2>6. Discussion</h2>
<p>The results from our experiment presented a surprising observation: IO consistently outperformed CoT and SPP on all metrics assessed. Before providing the potential reason for this deviation, it's pertinent to discuss the challenges that reduce the performance of CoT and SPP.</p>
<p>The strategy in solving Rosetta Stone Problems entails recognizing underlying linguistic rules and then creating a dictionary to bridge English with
the unknown language. This demands iterative refinement as the example language pairs are crossreferenced.</p>
<p>In the rule identification process, CoT frequently makes assumptions about linguistic rules and potential dictionary pairings without supplying adequate justification. As this process iteratively go through the following example language pairs, there is almost no retrospective corrections to these assumptions. Instead, CoT often reaffirms the preestablished rules and dictionary pairings as correct. See Figure 5 for the example.</p>
<p>Additionally, CoT's reasoning process revealed a bias. When confronted with linguistic challenges unfamiliar to GPT-4, CoT regularly relied on implicit grammar rules stemming from English. In contrast to the familiar language of GPT-4 like Italian, CoT clearly explained the derivation and application of language rules that determine word choices based on grammatical gender. However, the reasoning process was not comprehensive enough to provide the complete solution to the puzzle.</p>
<p>Challenges also emerge with the SPP method. Conversations between expert personas seldom added depth to the discussion. Out of 28 language puzzles, expert personas challenged each other's assumptions in just one instance. The dialogue tends to remain overly harmonious, whereas reallife interactions among diverse roles often elicit broader perspectives.</p>
<p>A closer look at the second-phase GPT-4's translation capabilities revealed another limitation. The model frequently outputs translations without elucidating the reasoning behind these choices. Furthermore, because of the incomplete rules from the first phase, GPT-4 always applied rules or vocabulary not discussed in the first phase. It also occurred when translating words that varied from those in the pre-established dictionary, both CoT and SPP provide alternate translations without justification.</p>
<p>A pivotal observation from our study involves the contradictory nature of GPT-4's reasoning process. Specifically, we have identified instances where the answers provided by GPT-4 were at odds with the reasoning framework it initially established. We have classified these contradictions into two categories: dictionary contradictions and rule contradictions. This classification delineates instances where GPT-4's conclusions either contravene the lexical parameters (dictionary contradictions) or the logical premises (rule contradictions) it had previously set. See Figure 2 for examples of dictionary contradictions and Figure 1 for rule contradictions encountered in our analysis. This suggests GPT4's generated reasoning might not genuinely mirror its internal thought process when tackling the Rosetta Stone problems, leading to varying outcomes. However, it's essential to highlight that the</p>
<p>Table 1: Puzzling Machine Competition Result</p>
<table>
<thead>
<tr>
<th style="text-align: left;">prompting method</th>
<th style="text-align: center;">EM</th>
<th style="text-align: center;">FE_CTER $^{\text {a }}$</th>
<th style="text-align: center;">FE_CHRF</th>
<th style="text-align: center;">FE_BLEU</th>
<th style="text-align: center;">EF_CTER $^{\text {b }}$</th>
<th style="text-align: center;">EF_CHRF</th>
<th style="text-align: center;">EF_BLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SPP</td>
<td style="text-align: center;">31.83</td>
<td style="text-align: center;">66.92</td>
<td style="text-align: center;">70.99</td>
<td style="text-align: center;">56.91</td>
<td style="text-align: center;">69.74</td>
<td style="text-align: center;">72.13</td>
<td style="text-align: center;">39.83</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">31.37</td>
<td style="text-align: center;">70.64</td>
<td style="text-align: center;">73.78</td>
<td style="text-align: center;">60.19</td>
<td style="text-align: center;">68.64</td>
<td style="text-align: center;">71.25</td>
<td style="text-align: center;">38.39</td>
</tr>
<tr>
<td style="text-align: left;">IO</td>
<td style="text-align: center;">33.79</td>
<td style="text-align: center;">73.14</td>
<td style="text-align: center;">75.79</td>
<td style="text-align: center;">61.37</td>
<td style="text-align: center;">74.86</td>
<td style="text-align: center;">75.56</td>
<td style="text-align: center;">42.65</td>
</tr>
</tbody>
</table>
<p>${ }^{a}$ FE denotes translation from an unknown language to English
${ }^{\text {b }}$ EF denotes translation from English to an unknown language
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: characTER score of zero example
identified contradictions in the second phase don't necessarily correlate with output quality. There were instances where GPT-4 disregarded incorrect pre-established rules, resulting in the correct answer, as well as cases where it overlooked the correct rules, producing an erroneous response. The relationship between these inconsistencies and performance outcomes warrants further exploration.</p>
<p>In summary, GPT-4's challenges in addressing the Rosetta Stone Problems primarily revolve around its inability to provide a thorough reasoning pathway and accurate translation pairs consistently. This leads us to the assumption that the superior performance of the IO method could stem from GPT-4's incomplete rules and dictionary causing distractions or adding noise to the translation process. In contrast, the IO method, by relying on example language pairs only, offers answers without the convoluted reasoning process.</p>
<h2>7. Conclusion</h2>
<p>In light of our study, which delved into GPT-4's reasoning capabilities concerning linguistic puzzles, several limitations emerged, emphasizing the need for continued exploration in this area.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The figure shows the baseless assumption occurring in the CoT discussion on Kiche language. Linguistic anthropologists first propose baseless vocabulary pairs and lexicographers reaffirm the opinion.</p>
<p>We conducted a detailed analysis of GPT-4's reasoning process primarily using the LO dataset, which comprises only 28 language puzzles. Given the limited size of this dataset, there's a potential for bias on the puzzles we selected. Future stud-</p>
<p>Table 2: Average Performance of all Queries</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompting Method</th>
<th style="text-align: center;">EM</th>
<th style="text-align: center;">CosSim</th>
<th style="text-align: center;">CharacTER</th>
<th style="text-align: center;">ChF-3</th>
<th style="text-align: right;">BLEU-2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SPP</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: right;">0.29</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.618</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: right;">0.351</td>
</tr>
<tr>
<td style="text-align: left;">IO</td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: right;">0.384</td>
</tr>
<tr>
<td style="text-align: left;">ZeroEx</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: right;">0.19</td>
</tr>
</tbody>
</table>
<p>Table 3: Average Performance of all Languages</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompting Method</th>
<th style="text-align: center;">EM</th>
<th style="text-align: center;">CosSim</th>
<th style="text-align: center;">CharacTER</th>
<th style="text-align: center;">ChF-3</th>
<th style="text-align: center;">BLEU-2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SPP</td>
<td style="text-align: center;">0.077</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.249</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">0.085</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.296</td>
</tr>
<tr>
<td style="text-align: left;">IO</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.334</td>
</tr>
<tr>
<td style="text-align: left;">ZeroEx</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.142</td>
</tr>
</tbody>
</table>
<p>ies should consider a more expansive and diverse dataset to ensure comprehensive insights.</p>
<p>Furthermore, our results(see figure) indicate that GPT-4 may have varying levels of familiarity with different languages. Throughout our investigation, we employed both SPP and CoT prompting approaches for the entire dataset. It's important to recognize that identifying various familiarity levels for GPT-4 and designing separate experiments for each level might yield more detailed insights into GPT-4's reasoning process.</p>
<p>Our findings highlight challenges in GPT-4's ability to generate linguistic rules and dictionaries and subsequently apply them to unknown language translations. An avenue for future research could explore alternative prompting methods that not only guide GPT-4 to produce accurate answers but also elucidate the comprehensive reasoning processes leading to those conclusions from the given example language pairs only.</p>
<p>Our analysis shows that the IO method consistently surpassed both SPP and CoT in performance. It remains crucial to delve deeper into the reasons behind the superior efficacy of the direct IO method. Moreover, a comprehensive examination is required to assess our assumption that the incomplete or errors in the reasoning processes of GPT-4's response could impact its output quality and performance.</p>
<h2>8. References</h2>
<p>Bozhidar Bozhanov and Ivan Derzhanski. 2013. Rosetta stone linguistic problems. pages 1-8.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel</p>
<p>Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.</p>
<p>Kahneman Daniel. 2017. Thinking, fast and slow.
Ivan Derzhanski and Thomas Payne. 2010. The linguistics olympiads: Academic competitions in linguistics for secondary school students. Linguistics at school: language awareness in primary and secondary education, pages 213-26.</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey on in-context learning.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.</p>
<p>Chuanyang Jin, Songyang Zhang, Tianmin Shu, and Zhihan Cui. 2023. The cultural psychology of large language models: Is chatgpt a holistic or analytic thinker?</p>
<p>Zheng-Lin Lin, Chiao-Han Yen, Jia-Cheng Xu, Deborah Watty, and Shu-Kai Hsieh. 2023. Solving linguistic olympiad problems with tree-of-thought prompting. In Proceedings of the 35th Conference on Computational Linguistics and Speech</p>
<p>Processing (ROCLING 2023), pages 262-269, Taipei City, Taiwan. The Association for Computational Linguistics and Chinese Language Processing (ACLCLP).</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. 2022. Formal mathematics statement curriculum learning.</p>
<p>Stanislas Polu and Ilya Sutskever. 2020. Generative language modeling for automated theorem proving.</p>
<p>Maja Popović. 2015. chrF: character n-gram Fscore for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. Sentencebert: Sentence embeddings using siamese bertnetworks.</p>
<p>Jannis Vamvas. 2022. Translation puzzles are in-context learning tasks.</p>
<p>Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. 2016. CharacTer: Translation edit rate on character level. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 505-510, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023a. Self-consistency improves chain of thought reasoning in language models.</p>
<p>Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2023b. Unleashing cognitive synergy in large language models: A task-solving agent through multipersona self-collaboration.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023a. Chain-ofthought prompting elicits reasoning in large language models.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023b. Chain-ofthought prompting elicits reasoning in large language models.</p>
<p>Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023a. An empirical study on challenging math problem solving with gpt-4.</p>
<p>Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023b. An empirical study on challenging math problem solving with gpt-4.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models.</p>
<p>Gözde İşgüder, Yova Kementchedjhieva, Phillip Rust, and Iryna Gurevych. 2020. Puzzling machines: A challenge on learning from small data. pages 1241-1254.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://eval.ai/web/challenges/ challenge-page/2150/overview&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
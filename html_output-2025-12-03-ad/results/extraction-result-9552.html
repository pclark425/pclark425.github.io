<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9552 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9552</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9552</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-259063913</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.01388v2.pdf" target="_blank">From Large Language Models to Databases and Back: A discussion on research and education</a></p>
                <p><strong>Paper Abstract:</strong> This discussion was conducted at a recent panel at the 28th International Conference on Database Systems for Advanced Applications (DASFAA 2023), held April 17-20, 2023 in Tianjin, China. The title of the panel was"What does LLM (ChatGPT) Bring to Data Science Research and Education? Pros and Cons". It was moderated by Lei Chen and Xiaochun Yang. The discussion raised several questions on how large language models (LLMs) and database research and education can help each other and the potential risks of LLMs.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9552",
    "paper_id": "paper-259063913",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0031887499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Large Language Models to Databases and Back A discussion on research and education
8 Jul 2023</p>
<p>Sihem Amer-Yahia (CNRS
Univ. Grenoble Alpes, France)
Angela Bonifati (Univ. Lyon 1</p>
<p>CNRS Liris, France)
Lei Chen (Hong Kong University of Science and Technology, China)
Guoliang Li (Tsinghua University
China)</p>
<p>Kyuseok Shim (Seoul National University
Korea)</p>
<p>Jianliang Xu (Hong Kong Baptist University)
Xiaochun Yang (Northeastern University
China</p>
<p>From Large Language Models to Databases and Back A discussion on research and education
8 Jul 2023B27098B31BD0A79D805BDF00A079F2D3arXiv:2306.01388v2[cs.DB]
The title of the panel was "What does LLM (ChatGPT) Bring to Data Science Research and Education?Pros and Cons".It was moderated by Lei Chen and Xiaochun Yang.The discussion raised several questions on how large language models (LLMs) and database research and education can help each other and the potential risks of LLMs.</p>
<p>LLMS AND DATABASES</p>
<p>In recent years, large language models (LLMs) have garnered increasing attention from both academia and industry due to their potential to facilitate natural language processing (NLP) and generate high-quality text.Despite their benefits, however, the use of LLMs is raising concerns about the reliability of knowledge extraction.The combination of database research and data science has advanced the state of the art in solving real-world problems, such as merchandise recommendation and hazard prevention.In this discussion, we explore the challenges and opportunities related to LLMs in database and data science research and education.</p>
<p>LLMs for database research.LLMs have proven to be highly useful for language writing, as they can identify and correct grammar errors.Additionally, LLMs can serve as a valuable resource for knowledge acquisition and analysis.However, the accuracy of extracted knowledge is not guaranteed, and bias can be introduced, leading to potential inaccuracies in the data analysis process.</p>
<p>LLMs can be highly effective in data preparation and labeling tasks, such as text mining, text parsing, keyword extraction, and sentiment analysis.It also has great potential to improve feature extraction, selection, and parameter tuning.</p>
<p>Database research for LLMs.Database research can support LLM development from data cleaning and preprocessing to training and optimization.For instance, domain-specific knowledge can be incorporated into training data to create more accurate and reliable models.Furthermore, database research can be used to optimize prompt engineering to improve the effectiveness of LLM.</p>
<p>LLMs for education.LLMs can be used to reform database education and address the challenges and concerns related to their use.LLMs can provide students with a wealth of knowledge and practical skills, such as techniques for handling dirty data.However, care must be taken to ensure that the information and programming styles provided by LLMs are accurate and free from bias or misinformation.As such, it is crucial to consider how to detect plagiarism when people use LLMs to generate scientific articles, papers, or assignments.We conclude that, although there are challenges associated with the use of LLMs in database research and education, these can be addressed through careful research and thoughtful integration of LLMs into the data science curriculum.</p>
<p>ChatGPT, today's famous LLM.A Generative Pre-Trained Transformer (GPT) is a language model relying on deep learning that is designed to take a text-based input and generate a natural human-like text.Chat Generative Pre-trained Transformer (Chat-GPT) is a chatbot released by OpenAI in November 2022 and is built on top of OpenAI's GPT-3.5 large language model (LLM).ChatGTP has been trained on a large body of text from a variety of sources in 2020 and can write any form of text such as essays, poems, paragraphs and computer programs.It is able to understand and generate human-like natural language with a high level of accuracy and fluency.A new version based on GPT-4 was released on March 2023 and is available for paid subscribers on a limited basis.</p>
<p>While ChatGPT can help people with a lot of well-suited tasks such translation of foreign languages, summarization of a text and generation of human-like conversational responses, it has several drawbacks.Since large language models perform the task of predicting the next word in a series of words by replicating common patterns of trained texts, it produces the output text without any concern of originality, plagiarism and privacy.Furthermore, since the training text data is derived from publicly available data before 2021, it cannot provide accurate information in a timely manner and may not know the most up-to-date information.Moreover, ChatGPT does not seem to be yet able to perform well complicated mathematical calculations or high-level problem-solving tasks.Since its outputs may contain false or outdated information, we should carefully evaluate the outputs of ChatGPT and use them cautiously.</p>
<p>PROS AND CONS OF LLMS FOR RESEARCH AND EDUCATION</p>
<p>LLMs for Data Science Research.Calculators and word processors are useful tools for people that allow not to worry about complex arithmetic calculations and incorrect spellings/grammars as well as citation labels of their writing, respectively.The positive implications of using both tools are that people can focus and concentrate more on the content of their work without worrying about inaccurate calculations or spelling/grammatical errors.Similarly, data scientists can utilize ChatGPT for their data science research to focus more on high-level creative thinking including getting the big picture, original idea generation, analytical thinking and problem solving.For example, they can utilize it to summarize the texts about related works, learn about a particular research topic, brainstorm about research directions for their new project and improve their writing skills of technical papers.Chat-GPT can also assist non-native data scientists in understanding, interpreting and writing English texts.On the other hand, they can use ChatGPT to produce a high-quality code with explanations and improve their coding skills.It can even help a data scientist rewrite his old code in a programming language to an equivalent code in another programming language.Note that an important skill required for data scientists is the ability to produce a good quality of code.Thus, instead of spending time in learning how to code or producing code for data analysis, data scientists can concentrating more on their research by utilizing ChatGPT.</p>
<p>While more training data is likely to produce a more accurate model [6], there are many applications such as named entity recognition [33], relation extraction [23] and image classification [36] where producing a large-scale training data by manual labeling is expensive and time-consuming.To quickly obtain a large-scale training data with low cost, one approach is to use weak supervision that automatically annotates unlabeled data by heuristic rules or machine learning models.For example, one of the most popular techniques for weak supervision is distant supervision that utilizes external knowledge bases to produce weak labels [27].We can also utilize ChatGPT as an alternative method for weak supervision.For instance, ChatGPT was recently investigated to augment training data for few-shot classification by rephrasing each sentence in the training data into multiple similar sentences [17].</p>
<p>Data Science Research for LLMs.Since an LLM model is only as knowledgeable as the training texts that have been provided for learning, its knowledge is limited according to the training data and it may become unfair by absorbing the biases from the training data.Furthermore, it lacks the capability of ethical thinking too.Thus, developing learning techniques to overcome such handicaps of LLM models will be very helpful to LLMs.Publicly available text data on the Web has a lot of sensitive information and training LLMs with public data can thus disclose sensitive and private information of people.On the other hand, as users input more data with conversations into ChatGPT, it may potentially leak the sensitive information to other users of ChatGPT.Thus, developing the privacy preserving schemes, such as the differential privacy, with high utility for training LLM modles will help LLMs to protect the privacy of individuals.</p>
<p>LLMs for Computer Science and Data Science Education.ChatGPT can be a useful tool for both disciplines and we need to reform the curricular to include ChatGPT.In [22], opportunities of utilizing ChatGPT are addressed and several ChatGPT-based tasks are suggested for computer science education.For instance, teachers can ask students to generate a code for a given problem, and then explain, analyze and improve the code.In addition, we can also ask students to write their own code for the same problem and find the similarities as well as differences between two codes.</p>
<p>Students can utilize ChatGPT to summarize/understand/learn the texts about existing works for a particular research topic, enhance their coding as well as debugging skills to generate a high quality code, brainstorm about research topics and improve their writing skills of technical papers.Thus, students can utilize Chat-GPT to focus more on high-level creative thinking including getting the big picture, original idea generation, analytical thinking and the detailed steps of their methods to solve a given problem.To do so, since the outputs of ChatGPT may contain false or outdated information, students should learn how to use ChatGPT effectively and cautiously.</p>
<p>While there are many advantages of including ChatGPT in the curriculum, students who consistently depend on ChatGPT may lose or cannot improve their skills of summarizing the texts, searching for relevant materials about a particular topic and writing technical papers by themselves.Furthermore, while ChatGPT is proficient in generating fluent text, it may produce the contents with lack of clarity as well as originality.Moreover, the outputs of ChatGPT may contain false or outdated information, and may even present a plagiarized writing from another source without citing properly.Thus, we need to provide precise guidelines of using ChatGPT to students so that they can learn how to use ChatGPT effectively and utilize the ChatGPT outputs with caution.</p>
<p>Is ChatGPT charming?Have you ever watched the movie "Cyrano"?It is a story about a man who sent love letters to a woman that were actually written by another man with a good skill of writing romantic love letters to a woman.When you start chatting with someone for the first time on an online dating site or dating app, if you feel too attracted to the person, watch out -you may actually be talking with ChatGPT!</p>
<p>WHAT CAN AND CANNOT LLM DO FOR DATABASES?</p>
<p>As we all know, LLMs typically report probabilistic results but cannot be used to report fully deterministic results [20,29].Therefore, LLM can be leveraged to handle inexact data/query processing problems that can tolerate approximate results, e.g., approximate query processing and data integration.However, it is hard to use LLMs to support exact data/query processing components, e.g., query answering and query rewriting.In the following, we discuss how to use LLMs to support exact and inexact data/query processing.</p>
<p>LLMs for Database Research.For most of database problems, e.g., data discovery, data cleansing, and data integration, the users are satisfied with approximate results.The optimization goal is to improve the generalizability, efficiency and quality.Intuitively, we can utilize LLMs to improve the generalizability.However, several challenges arise.The first challenge is automatic prompt engineering that automatically generates appropriate prompts to guide LLMs to find correct answers.LLMs have limitations on the number of token constraints and long latency, and the automatic prompt engineering tool should optimize these two factors.The second challenge is how to integrate domain knowledge into LLMs.Current LLMs use open Web corpus to pretrain a large language model and thus can well support data cleaning and integration on Web data but cannot effectively support vertical domains that are absent on open Web.Hence, the challenge is to fine-tune LLMs to support domain knowledge or use prompt engineering to teach LLMs to do this.The third challenge is how to combine LLMs and existing data science tools, as it is expensive to call LLMs and it is beneficial to utilize some existing tools to reduce the cost.For example, there are many good database tools, e.g., blocking tools and entity-matching tools, and we can design tool learning that enable LLMs to call effective tools to reduce the cost.</p>
<p>Database Research for LLMs.The theory and model architecture of LLMs are almost mature, and the researchers and scientists that are working on LLMs focus on providing high-quality data to train LLMs, e.g., discovering data, cleaning data and integrating data.There exists a plethora of tools for the above database tasks that can be used to prepare the data on which LLMs are trained.A challenge is how to make a win-win loop between database systems and LLMs, which uses database techniques to provide high-quality of LLMs and uses LLMs to optimize the database tools.</p>
<p>Querying data with natural language.An interesting application is Text-to-SQL, which converts natural language queries into SQL statements.This has been a long-studied research problem.The state-of-the-art is currently a work presented at AAAI 2023 [26], which achieved an accuracy of 79.9% using a seq2seq pre-trained language model.With continuous prompts, ChatGPT enables users to interactively refine the generated SQL queries and could further improve their accuracy.This unique feature presents a potential opportunity to integrate LLMs with existing Text-to-SQL techniques to generate more precise SQL statements.</p>
<p>Additionally, ChatGPT allows users to query a dataset with natural language.This could eliminate the need for SQL queries and increase the efficiency of data retrieval and analysis for certain applications, which poses the question of whether SQL remains necessary or if it is possible to translate text into a query evaluation plan for database result evaluation.The integration of LLMs with database techniques has the potential to open up new opportunities for research and advancement in the field of data science.</p>
<p>LLMs for Logical Query Optimization.Logical query optimization (e.g., query rewriting) aims to translate a query plan to an optimized query plan (possibly with a lower cost but without guarantee).It seems that LLMs can be used to support this problem.But the key challenge is that the translated query plan should be exactly equivalent to the original plan.Therefore, there are several possible solutions.The first uses LLMs to obtain an optimized query plan and then verifies the equivalence using existing techniques (and then keeps the equivalent query and drops the in-equivalent one).The second uses LLMs to optimize the using of query rewriting rules, including discovering new query-rewrite rules and judiciously using the rules (including whether to use a rule and to determine the order of using different rules).</p>
<p>LLMs for Physical Query Optimization.Different from logical query optimization, physical query optimization should utilize the physical database statistics and it is vital to provide these information to LLMs.However, the current LLMs cannot effectively support numerical values.Two challenges arise in this context.The first is to fine-tune the LLMs that enables LLMs to support numerical statistics.The second is to embed database statistics into the prompt to facilitate that LLMs can use such information to get an optimized physical plan.There are also some other similar problems that should utilize physical statistics, e.g., knob tuning, index/view advisor, and query diagnosis.</p>
<p>Database Tuning for LLMs.Database knob tuning [19] is important to achieve high performance of database systems.Traditionally, database administrators (DBAs) tune database systems.Since the number of knobs in database systems increases as database systems become more sophisticated, it is difficult for DBAs to tune the database systems by considering all possible values of knobs [32].</p>
<p>To overcome the drawbacks, auto-tuning methods were proposed to find an optimal configuration of database systems without human intervention [19,32].Database knob tuning techniques developed for database systems can be useful to find an optimal configuration of an LLM for applications.</p>
<p>LLMs for Database Storage and Transactions.Both database storage and transactions are deterministic and we are pessimistic that LLMs cannot be used to optimize them.</p>
<p>LLMS (BADLY) NEED INTEGRATED DATA AND REASONING</p>
<p>LLMs and data integration To understand the differences between LLMs and databases, let us compare them with the process of integrating heterogeneous data sources.Data integration is a longstanding research problem in data management [7,31].Schema mapping, data deduplication, schema and data fusion are all tasks that involve considering up-to-date data sources, as well as personal and proprietary data.By leveraging the inherent semantics of mappings (correspondences between queries or views on different data sources), these tasks do not need re-training on large corpuses of data and can easily capture new incoming data.As recently argued in a vision paper, these data of different nature are not considered so far in the LLM [21].Moreover, integrated data can easily cater for privacy constraints and become trustworthy, for instance by blending mappings with policy views [9] or by having humans as first-class citizens in the data integration process [5,8,15].Provenance and lineage information, in particular why, how and where provenance characterizing query results [14], could serve the need of filling an existing gap of large language models, missing the key capability of locating the sources of information.</p>
<p>But what is missing in LLMs to take advantage of databases and integrated data sources?The following is a non-exhaustive list of missing features (and the reader is invited to add more):</p>
<p>-Need to retain provenance and schema information as well as other kinds of metadata, which is not merely raw data and should not be unified with raw data; -Need to process and compute provenance throughout the learning process and be able to annotate the results with provenance information (and, thus, citation sources); -Need to capture privacy constraints and privacy policies in the data acquisition and data fusion process.</p>
<p>LLMs and Graphs.Graphs are a great source of knowledge, which is typically curated by humans and, as such, can be seen as high-quality and trustworthy integrated information.Examples of such highly curated graphs are Wikidata and DBPedia, that are typically used by search engines [34], whereas LLM are trained on large text corpora, such as Wikipedia, books, news and open datasets.Knowledge graphs such as DBPedia and Wikidata can be navigated and queried by leveraging query endpoints.Queries collected at the endpoints allow to understand what the users search within the knowledge graphs and thus indirectly to characterize the underlying structure of the data.</p>
<p>To illustrate the difference between semantics in graph databases and language models, we choose to confront a query from the DBPedia graph query logs with question answering in ChatGPT. Figure 1: The Henry VIII query, a 7-clique containing one constant and six variables.All edges between Henry VIII and the variables are labeled "dbpedia-owl:spouse" and all edges between variables are labeled with the property path "!dbpedia-owl:sameAs" [11].</p>
<p>Figure 1 shows the shape of a 7-node clique query involving Henry The VIII to his 6 wifes where all edges between Henry The VII and his wifes are labeled as "dbpedia-owl:spouse" and the edges between each pair of wifes are labeled with a property path1 , namely "!dbpedia-owl:same-As" (to retrieve his distinct wifes).</p>
<p>Figure 2 shows question answering on ChatGPT on the possible relationships (in terms of RDF properties) between Henry The VIII and his 6 wives.It shows the RDF syntax of the "spouse" relationship but it does substantiate the result with the information about the source from which this information has been retrieved.ChatGPT cannot access databases such as DBPedia or Wikidata at the time being and, as text-based AI model is unable to draw diagrams on query shapes such as the one in Figure 1.</p>
<p>LLMs and Reasoning.Contrarily to LLMs, graphs enable symbolic reasoning, when logic-based existential rules and path queries are used to augment existing knowledge graphs with additional inferred data [13].Moreover, database queries including graph queries return certain answers [10], whereas output of LLM is highly uncertain depending on the frequency of values in the training data.</p>
<p>Combining results from LLMs with graphs could improve the results of LLMs by unifying machine reasoning with symbolic and logic-based reasoning [30].The knowledge graph lifecycle with maintenance operations, update propagation and error fixing is nonexistent for LLMs.On the other hand, the outputs of LLM could be used to enrich knowledge graphs for question answering tasks by leveraging graph attention networks [35].</p>
<p>Concluding, we cannot disagree with Gary Markus' blog 'Hoping for the Best as AI Evolves' [28]: "Large language models lack mechanisms for verifying truth; we need to find new ways to integrate them with the tools of classical AI, such as databases, Webs of knowledge, and reasoning.".</p>
<p>LLMS AS A RESEARCH ASSISTANT</p>
<p>Helping with Scientific Writing.LLMs like ChatGPT can help with scientific writing in several ways, such as proofreading, rewriting, summarization, and even suggesting titles for research papers.It can also help improve the language to better communicate research ideas and results, so as to facilitate research collaborations.However, the key challenge lies in creating effective prompts that generate high-quality responses.Below are some examples:</p>
<p>• Revise the following paragraph from the introduction of a Computer Science academic paper so the citations are kept and the text has a clear sentence structure.• Here is the abstract of a paper.Suggest five creative titles.</p>
<p>• Write a 1-page sensational press release for this research.</p>
<p>With well-crafted prompts, ChatGPT can deliver results that rival those of paid editing services.</p>
<p>Assisting with Data Analytics.LLMs can assist with data creation.It can generate data based on input parameters, which can be useful in situations where large amounts of synthetic data are needed for research.Additionally, LLMs can help generate code for data analytics tasks [18].One such task is exploratory data analysis (EDA).For example, suppose we have a loan dataset that we want to perform EDA on.We can simply prompt ChatGPT with a request to write Python code to load and perform EDA on the loan dataset, and it will provide us with a code that we can use to analyze and visualize the data.ChatGPT can also assist with other data analytics tasks such as data cleaning and preprocessing, feature engineering, hyperparameter tuning, and model selection and evaluation.It can help save time and effort by automating some of these tedious tasks and focus on more complex aspects of data analytics.</p>
<p>Limitation of Hallucination in LLMs.Despite their many benefits, LLMs do have some limitations that need to be considered.One of these limitations is their potential to generate incorrect Figure 3: Overview of AIED research, focusing on the examination of underlying data and applications [24].content that appears plausible, known as "hallucination".This is particularly relevant in research paper writing, where ChatGPT may suggest non-existing references or provide inaccurate information.For example, when asked to recommend a paper on the topic of data science authored by Lei Chen, ChatGPT suggested a paper titled "Crowdsourced Data Management: A Survey" authored by Lei Chen, Reynold Cheng, Silviu Maniu, and Wang-Chien Lee and published in IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 9, pp.1959-1977, September 2013.However, although this paper title does exist, it was not authored by Lei, rather by Guoliang et al. and published in 2016 (see [3]).In fact, Lei, Sihem, and Anand did author a survey paper on a related topic but their title is different (see [4]).To address this issue, one possible solution is to integrate ChatGPT with a knowledge graph and ground truth facts, which can help verify the accuracy of the generated information by crossreferencing it with existing data.</p>
<p>Incorporating External Data into LLMs.While LLMs have access to a vast amount of data, their knowledge is still limited by the data they have been trained on.There are some recent efforts to incorporate external data into LLMs through the use of prompts.However, LLMs may have a limitation on the length of input they can process, which can impact their ability to understand complex or lengthy inputs.For example, the GPT-4 base version allows up to 8,192 tokens, which may not be sufficient for processing longer texts.One approach to overcome this limitation, known as chunking, is to split the longer text into smaller segments and process each segment separately [25].It can be combined with traditional data science techniques such as embedding and indexing to improve the model's performance on longer inputs.Yet, chunking can also introduce challenges such as maintaining coherence and consistency between segments, which may require further research.</p>
<p>Ethical and legal issues.Ethical concerns such as bias, plagiarism, and data privacy and security are also significant issues when using LLMs.LLMs may be biased towards the data they were trained on, which can lead to unfair or inaccurate results.Additionally, there is a risk of privacy breaches if the input contains sensitive information.Moreover, legal and copyright issues must also be considered when using LLMs.If the model generates copyrighted material without proper licensing, it could lead to legal repercussions.Thus, it is essential to responsibly and ethically use LLMs by thoroughly vetting and verifying generated content before using or publishing it.The new ACM policy requires disclosure of the use of generative AI tools for content generation in published work, with specific details regarding their usage provided in the acknowledgments section or elsewhere in the work [1].</p>
<p>LLMS AND EDUCATION</p>
<p>One area that is receiving both scientific and media coverage these days is the impact of LLMs on education.Several concerns have been raised about students using ChatGPT to complete tests, ChatGPT passing bar exams and professors using it to devise quizzes.As scientists, we focus on discussing LLMs in teaching and LLMs for doing research on education, that we refer to as LLM4ED.</p>
<p>Teaching LLMs.First of all, we need to teach LLMs just like other models.This will contribute to demystifying them and to raising awareness about their lack of transparency as well as their benefits and pitfalls.We also need to encourage our students to treat LLMs just like other recommendation engines.Their prediction accuracy should be tested keeping in mind that the best paper award at RecSys in 2019 showed that KNN outperformed 6 Deep Learning recommendation methods on MovieLens data [16].These includes Collaborative Variational Autoencoder and and Neural Collaborative Filtering methods.Students need to learn to build on top of LLMs and treat them just like other models.In recent work, we built a meta-recommender that learns the best algorithm to apply given a (user,dataset) or a (user,question) pair [12].This approach could integrate LLMs as a recommendation option.</p>
<p>LLM4ED.There are many challenges and opportunities of database research in education [2,24].An LLM could be modeled as a learner or to support learners, teachers or administrators.</p>
<p>LLMs as Learners.This would require to model learners' data and behavior.Figure 3 represents data about learners and learning artefacts that can be found in most education systems.This data is highly diverse.Student and curriculum records capture individual learners' records such as their demographics which are usually provided by learners at registration time as well as information on learning material such as artefacts, and assessment and outcome requirements.Learning records capture data on learners' achievements such as grades and assessment outcomes.Learning logs record learners' engagement with artefacts, feedback to learners and collaboration.This would encourage students to see LLMs as a peer from which to learn and to criticize.</p>
<p>LLMs for Learners, Teachers and Administrators.This opens new opportunities for LLM-in-the-loop research in education.Figure 3 summarizes some research directions.For instance, in the context of collaborative learning, team formation algorithms could be revisited to consider LLMs as team members.In the context of developing an intelligent teaching assistant, LLMs are already in use for grading students which raises the question of accuracy and fairness.They are also used help teachers create content.In both grading and content creation, ensuring teachers' agency will allow them to guide the process and override automatic decisions.LLMs can also be used for admission support and student dropout prediction analytics.A particular point of attention in all these applications is the study of bias in ranking (student ranking) and in classification (student dropout prediction).Adding provenance to LLMs to better identify their sources of flaws would also address some of these concerns.</p>
<p>CONCLUSION</p>
<p>Databases and LLMS are on either side of the spectrum of data science research.Databases are collections of data, while LLMS are viewed as summaries and profiles of experiences based on data.Databases can help data scientists to query and statistically analyze the stored data accurately and efficiently.LLMs, on the other hand, are learned from textual data and can help data scientists to solve semantic application problems related to natural language.However, LLMs cannot guarantee an accurate or complete answer.</p>
<p>We discussed the advantages and limitations that LLMs brought to data science and database research and education.Regarding the discussion of LLMs, the optimistic view is LLMs can facilitate most of the data science and data management tasks, including data discovery, data cleaning, data integration, and data visualization, and LLMs can bring great benefit to education.While the pessimistic view is LLMs are hard for data modeling, data analytics, and data interpretation, meanwhile, they might weaken learning skills and training LLMs could bring bias, plagiarism, privacy, legal, and copyright issues.People should be careful about the results obtained from an AI model and take them with a grain of salt.</p>
<p>Figure 2 :
2
Figure 2: Snapshot of question answering on Henry The VIII's wives on ChatGPT.</p>
<p>A property path is a path with a regular expression allowing to navigate graph data.
We also showed our insights that data science and database research could also help LLMs, including how to use provenance and lineage information to fill the existing gap of LLMs, how to take advantage of databases and integrated data sources, how to unify machine reasoning with symbolic and logic-based reasoning by combining results from LLM with graphs, and how to combine model-centric and data-centric approaches altogether.
. ACM. ACM Policy on Authorship. 2023. May 20, 2023</p>
<p>Towards ai-powered data-driven education. Sihem Amer-Yahia, Proc. VLDB Endow. VLDB Endow202215</p>
<p>Crowdsourced data management: A survey. Guoliang Li, Jiannan Wang, Yudian Zheng, Michael J Franklin, IEEE Trans. Knowl. Data Eng. 2892016</p>
<p>A survey of general-purpose crowdsourcing techniques. Anand Inasu, Chittilappilly , Lei Chen, Sihem Amer-Yahia, IEEE Trans. Knowl. Data Eng. 2892016</p>
<p>User-guided repairing of inconsistent knowledge bases. Abdallah Arioua, Angela Bonifati, Proceedings of the 21st International Conference on Extending Database Technology, EDBT 2018. H Michael, Reinhard Böhlen, Norman Pichler, Erhard May, Shan-Hung Rahm, Katja Wu, Hose, the 21st International Conference on Extending Database Technology, EDBT 2018Vienna, AustriaMarch 26-29. 2018. 2018</p>
<p>Scaling to very very large corpora for natural language disambiguation. Michele Banko, Eric Brill, Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics. the 39th Annual Meeting of the Association for Computational LinguisticsToulouse, FranceAssociation for Computational LinguisticsJuly 2001</p>
<p>Schema Matching and Mapping. Data-Centric Systems and Applications. Zohra Bellahsene, Angela Bonifati, Erhard Rahm, Springer2011</p>
<p>Interactive mapping specification with exemplar tuples. Angela Bonifati, Ugo Comignani, Emmanuel Coquery, Romuald Thion, Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD Conference 2017. Semih Salihoglu, Wenchao Zhou, Rada Chirkova, Jun Yang, Dan Suciu, the 2017 ACM International Conference on Management of Data, SIGMOD Conference 2017Chicago, IL, USAACMMay 14-19, 2017. 2017</p>
<p>Exchanging data under policy views. Angela Bonifati, Ugo Comignani, Efthymia Tsamoura, Proceedings of the 24th International Conference on Extending Database Technology, EDBT 2021. Yannis Velegrakis, Demetris Zeinalipour-Yazti, Panos K Chrysanthis, Francesco Guerra, the 24th International Conference on Extending Database Technology, EDBT 2021Nicosia, CyprusMarch 23 -26, 2021. 2021</p>
<p>Querying Graphs. Synthesis Lectures on Data Management. Angela Bonifati, H L George, Hannes Fletcher, Nikolay Voigt, Yakovets, 2018Morgan &amp; Claypool Publishers</p>
<p>An analytical study of large SPARQL query logs. Angela Bonifati, Wim Martens, Thomas Timm, VLDB J. 292-32020</p>
<p>How useful is metarecommendation? an empirical investigation. Nassim Bouarour, Idir Benouaret, Sihem Amer-Yahia, 2021 IEEE International Conference on Big Data (Big Data). Orlando, FL, USADecember 15-18, 2021. 2021</p>
<p>Chasing sets: How to use existential rules for expressive reasoning. David Carral, Irina Dragoste, Markus Krötzsch, Christian Lewe, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019. Sarit Kraus, the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019Macao, ChinaAugust 10-16, 2019. 2019</p>
<p>James Cheney, Laura Chiticariu, Wang Chiew, Tan , Provenance in databases: Why, how, and where. Found. Trends Databases. 20091</p>
<p>Debugging schema mappings with routes. Laura Chiticariu, Wang Chiew, Tan , Proceedings of the 32nd International Conference on Very Large Data Bases. Umeshwar Dayal, Kyu-Young Whang, David B Lomet, Gustavo Alonso, Guy M Lohman, Martin L Kersten, Sang Kyun Cha, Young-Kuk Kim, the 32nd International Conference on Very Large Data BasesSeoul, KoreaACMSeptember 12-15, 2006. 2006</p>
<p>Are we really making much progress? A worrying analysis of recent neural recommendation approaches. Maurizio Ferrari Dacrema, Paolo Cremonesi, Dietmar Jannach, CoRR, abs/1907.069022019</p>
<p>Auggpt: Leveraging chatgpt for text data augmentation. Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, Hongmin Cai, Lichao Sun, Quanzheng Li, Dinggang Shen, Tianming Liu, Xiang Li, 2023</p>
<p>A guide to using ChatGPT for data science projects. Datacamp, 2023. May 4, 2023</p>
<p>Tuning database configuration parameters with ituned. Songyun Duan, Vamsidhar Thummala, Shivnath Babu, Proceedings of the VLDB Endowment. the VLDB Endowment20092</p>
<p>Probabilistic machine learning and artificial intelligence. Zoubin Ghahramani, Nature. 52112015</p>
<p>Learnings from data integration for augmented language models. Y Alon, Jane Halevy, Dwivedi-Yu, CoRR, abs/2304.045762023</p>
<p>Chatgpt in computer science education. Orit Hazzan, BLOG@CACM on. 2023. January 23, 2023</p>
<p>Dual supervision framework for relation extraction with distant supervision and human annotation. Woohwan Jung, Kyuseok Shim, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, SpainDecember 2020International Committee on Computational Linguistics</p>
<p>Data management of ai-powered education technologies: Challenges and opportunities. Hassam Khosravi, Shazia Sadiq, Sihem Amer-Yahia, 2023In Learning Letters, 0., page (to appear</p>
<p>. LangChain. LangChain Chat. 2023. May 4, 2023</p>
<p>RESDSQL: Decoupling schema linking and skeleton parsing for text-to-SQL. Haoyang Li, Jing Zhang, Cuiping Li, Hong Chen, Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI). the Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI)2023</p>
<p>BOND: bert-assisted open-domain named entity recognition with distant supervision. Chen Liang, Yue Yu, Haoming Jiang, Siawpeng Er, Ruijia Wang, Tuo Zhao, Chao Zhang, KDD 2020. ACM2020</p>
<p>Hoping for the best as ai evolves. Gary Marcus, Commun. ACM. 664mar 2023</p>
<p>Probabilistic machine learning: An introduction. Kevin Patrick, Murphy , March, 2022MIT Press</p>
<p>The future is big graphs: a community view on graph processing systems. Angela Sherif Sakr, Hannes Bonifati, Alexandru Voigt, Khaled Iosup, Renzo Ammar, Angles, G Walid, Marcelo Aref, Maciej Arenas, Peter A Besta, Khuzaima Boncz, Emanuele Della Daudjee, Stefania Valle, Olaf Dumbrava, Bernhard Hartig, Tim Haslhofer, Jan Hegeman, Katja Hidders, Adriana Hose, Vasiliki Iamnitchi, Hugo Kalavri, Wim Kapp, M Martens, Tamer, Eric Özsu, Stefan Peukert, Mohamed Plantikow, Matei Ragab, Semih Ripeanu, Christian Salihoglu, Petra Schulz, Juan F Selmer, Joshua Sequeda, Gábor Shinavier, Riccardo Szárnyas, Antonino Tommasini, Alexandru Tumeo, Ana Lucia Uta, Varbanescu, Hsiang-Yun, Nikolay Wu, Da Yakovets, Eiko Yan, Yoneki, Commun. ACM. 6492021</p>
<p>Data integration: The current status and the way forward. Michael Stonebraker, Ihab F Ilyas, IEEE Data Eng. Bull. 4122018</p>
<p>Automatic database management system tuning through large-scale machine learning. Dana Van Aken, Andrew Pavlo, Geoffrey J Gordon, Bohan Zhang, Proceedings of the 2017 ACM International Conference on Management of Data. the 2017 ACM International Conference on Management of Data2017</p>
<p>Meta self-training for few-shot neural sequence labeling. Yaqing Wang, Subhabrata Mukherjee, Haoda Chu, Yuancheng Tu, Ming Wu, Jing Gao, Ahmed Hassan, Awadallah , KDD 2021. 2021</p>
<p>Knowledge graphs 2021: A data odyssey. Gerhard Weikum, Proc. VLDB Endow. VLDB Endow202114</p>
<p>QA-GNN: reasoning with language models and knowledge graphs for question answering. Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, Jure Leskovec, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, Yichao Zhou, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021Association for Computational LinguisticsJune 6-11, 2021. 2021</p>
<p>Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, Takahiro Shinozaki, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, J Wortman Vaughan, Curran Associates, Inc202134</p>            </div>
        </div>

    </div>
</body>
</html>